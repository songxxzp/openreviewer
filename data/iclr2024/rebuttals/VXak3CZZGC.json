[
    {
        "title": "Provable Out-of-Distribution Generalization in Hypersphere"
    },
    {
        "review": {
            "id": "oB6RJNMRrW",
            "forum": "VXak3CZZGC",
            "replyto": "VXak3CZZGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the challenge of out-of-distribution (OOD) generalization. Building upon previous research, it introduces the HYPO learning algorithm aimed at reducing intra-class variation while increasing inter-class separation. Notably, the paper establishes a connection between the loss function and the von Mises-Fisher (vMF) distribution. Subsequently, it provides a generalization upper bound of variation. These set HYPO apart from an existing work PCL. Extensive experimentation on OOD benchmarks showcases the superior performance of the HYPO algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written and well-organized.\n- The problem studied in this paper is interesting and important.\n- The authors have provided a clear discussion of the relation to previous work, PCL."
                },
                "weaknesses": {
                    "value": "1. The theoretical result appears to have limitations. \n- Although Theorem 5.1 provides insights into the upper bound of generalization variation, it does not conclusively demonstrate the superiority of the proposed method or loss, since the theorem directly assumes that the variation can be optimized to a small value under the proposed loss, i.e., $\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j\\ge 1-\\varepsilon$. If one were to substitute an alternative loss, such as changing the prototype to another sample within the same class (e.g., employing the SupCon loss) or directly using PCL's loss, it would also yield a generalization bound. Consequently, the question arises: How can we establish that the proposed loss is indeed superior, provably?\n- Theorem 5.1 cannot be valid unless we explicitly specify the distribution distance  $\\rho$.\n- Theorem 5.1 does not account for the influence of inter-class separation, a key aspect that this paper seeks to enhance through the second term in loss eq. (5). I notice that in Ye et al (2021)'s Theorem 4.1, function O(.) also depends on additional factors beyond just the variation.\n\n2. Training Loss.\n- Since prototypes $\\mu_i$ are updated in an EMA manner, it's worth noting that the second term in eq. (5) will not generate a gradient for $h$. Consequently, the second term of the loss becomes devoid of meaning.\n\n3. The idea is quite straightforward and shares many similarities with proxy-based contrastive learning methods. Is there any additional insight that I might have overlooked?\n\n4. The empirical improvements appear to be marginal, as indicated by the data in Tables 1 and 2.\n\nOverall, I think the theoretical contribution and empirical enhancements appear to have room for further development and strengthening."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697984992791,
            "cdate": 1697984992791,
            "tmdate": 1700395894611,
            "mdate": 1700395894611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vHiK7hYO1p",
                "forum": "VXak3CZZGC",
                "replyto": "oB6RJNMRrW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kMBq"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your comments and questions, which we address in detail below.\n\n> *W1.1. Although Thm 5.1 provides insights into the upper bound of generalization variation, it does not conclusively demonstrate the superiority of the proposed method or loss.*\n\nWe would like to clarify that our goal is not to theoretically establish HYPO's superiority over SupCon and PCL. Our primary contribution is to provide the **first theoretical justification** of prototype-based learning algorithms for OOD generalization. This is also acknowledged by Reviewer W1ps, who commented:\n\n> _\"...several learning methods have previously been proposed that utilize hyperspherical embeddings. But this paper is the first to provide a theoretical justification angled at OOD generalization.\"_\n\nUnlike PCL, our learning framework HYPO is motivated and guided by theory (Section 3.1), and provably guaranteed to reduce OOD generalization error (Thm 3.1 & Thm 5.1). Indeed, there may exist several alternative proxy-based learning algorithms such as PCL. However, it remains unclear if PCL's performance can be rigorously interpreted and bounded. We believe that our theoretical framework takes an important first step towards understanding domain generalization via hyperspherical learning formally, paving the way for more effective and theoretically grounded approaches in the field.\n\nEmpirically, unlike PCL which relies on SWAD optimization, HYPO is able to achieve strong performance by simple SGD optimization and the performance can be further improved with SWAD. Compared to PCL, HYPO achieves superior performance of an average accuracy of 89% on PACS, further demonstrating its effectiveness in practice. \n\n> *W1.2. Thm 5.1 cannot be valid unless we explicitly specify the distribution distance $\\rho$.*\n\nIndeed! We specified the distribution distance $\\rho$ as shown in **Definition 3.1** in the main manuscript. We have also added a footnote in the updated main manuscript for clarity.\n\n> *W1.3. Thm 5.1 does not account for the influence of inter-class separation. Besides, in Ye et al (2021)'s Thm 4.1, function O(.) also depends on additional factors beyond just the variation.*\n\nWe address the effect of inter-class separation in our theoretical analysis presented in **Appendix J**, which links to the second term in our loss eq. (5). Additionally, for the discussion on regularity conditions in Thm 4.1, please see our response to Reviewer W1ps. \n\n> *W2. Clarification on the prototype update rule via EMA and the meaning of the second term in the loss.*\n\nThe separation term's primary purpose is to maintain distinctiveness between class prototypes in the embedding space. While the separation term may not directly affect the gradients of $\\mathbf{z}$, it indirectly influences how $\\mathbf{z}$ is learned. By keeping the prototypes separate, the variation term has to adapt $\\mathbf{z}$ in a way that aligns with these well-separated prototypes, leading to better OOD generalization. As detailed in Algorithm 1 in Appendix A, in this work, we choose to update the prototypes $\\boldsymbol{\\mu}_i$ via EMA, which leads to stable updates of prototypes. Alternatively, one can set class prototypes as learnable parameters and update prototypes via gradients. We compare the performance of our method with EMA update and learnable prototypes (LP) in Appendix H (P24). We observe our method with EMA achieves superior performance compared to LP, which empirically verifies the effectiveness of EMA-style update of prototypes in practice."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110211408,
                "cdate": 1700110211408,
                "tmdate": 1700110211408,
                "mdate": 1700110211408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SiFgtqwTbv",
                "forum": "VXak3CZZGC",
                "replyto": "oB6RJNMRrW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Detailed Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive and detailed rebuttal. I have carefully read through your responses to the comments and truly appreciate the time and effort you have put into addressing each concern. My further comments are listed as follows.\n\n---\n\n(1) This paper claims at the beginning of section 5 that \n> Our main Theorem 5.1 gives a\nprovable understanding of **how the learning objective effectively reduces the variation estimate**.\n\nHowever, Theorem 5.1 presupposes that the variation can be optimized to a small value under the proposed loss, specifically $\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j\\ge 1-\\varepsilon$. \nNotably, the paper lacks a clear explanation of how the learning objective in Eq (5) upper bounds $1-\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j$, which is crucial for justifying the claimed reduction in variation.\nEven for a special and simpler case where infinity samples are accessible $N\\rightarrow\\infty$, the theorem can still not tell why the learning objective is able to effectively reduce the variation estimate.\n\nTherefore, \nthe theoretical justification of prototype-based learning algorithms for OOD generalization is not sufficiently informative.\n\nMoreover, it seems under this assumption, PCL and other methods can  also offer the same theoretical insights.\n\n---\n\n\n(2) Why is there only a footnote directing to Appendix C for the theorem in the revised paper? Firstly, while Appendix C is extensive, it would be more helpful if the authors could provide a more specific reference within it for easier navigation. Additionally, why do the authors not directly include the distance $\\rho$ used in Definition 3.1, as metentioned in the rebuttal?\n\nSecondly, for the sake of rigor, it is essential for the theorem to be self-contained. Including specific details, such as the distance $\\rho$ from Definition 3.1, within the theorem would enhance clarity and completeness.\n\n---\n\n(3) I appreciate the effort made to demonstrate that a simplex ETF can minimize the separation part of the proposed loss. However, my initial concern remains unanswered: how does the separation part of the proposed loss impact the OOD error?\n\n\n---\n\n(4) I cannot understand how the separation term can indirectly influence $\\mathbf{z}$ if it does not directly affect the gradients of $\\mathbf{z}$, as the authors mentioned in the rebuttal. A more detailed and mathematical explanation would be appreciated to clarify this aspect.\n\nMoreover, my initial concern revolves around the potential redundancy of the separation term when EMA is employed. In the rebuttal, the experiments compare the loss with EMA and without EMA, which certainly introduces a distinction between the two settings. To address this concern directly, it would be beneficial to conduct experiments comparing the loss with the separation term and without the separation term specifically when employing EMA.\n\n---\n\nThe manuscript would benefit from further refinement to fully meet the standards of ICLR. Thus, I believe this submission will be a good paper, but for now, it is not ready. I have decided to maintain my original score, and thank you for your effort once again."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395756839,
                "cdate": 1700395756839,
                "tmdate": 1700396019181,
                "mdate": 1700396019181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Aae0JduPcW",
                "forum": "VXak3CZZGC",
                "replyto": "oB6RJNMRrW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_kMBq"
                ],
                "content": {
                    "title": {
                        "value": "Maintain My Original Score"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nI have reviewed your latest feedback, including the appendix, and I want to express my gratitude for your diligent response. My further comments are as follows.\n\n(1) This paper highlights its theoretical significance as a main contribution (page 2, also the paper title), **claiming to provide theoretical justification for how the proposed loss guarantees improved OOD generalization**.\nConsequently, I expected a direct theorem demonstrating that **the OOD error can be bounded by the proposed loss**. However, this paper fails to provide this crucial argument, which largely limits its novelty.\n\n(2) The rebuttal claims that minimizing **the variation loss** is \"equivalent\" to maximizing the negative **first term of variation loss**. This claim seems to lack sufficient mathematical rigor.\n\n(3) In fact, the proposed loss comprises three components: `L = first term of variation loss + second term of variation + separation loss`.\nWhile minimizing the first term of the variation alone ensures that $\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j$ is close to 1, **the presence of the last two parts tends to deviate it from 1**. Therefore, **the gap between $\\frac{1}{N}\\sum_j\\mu_{c(j)}^T z_j$ and one should be the most important part of this paper** and needs to be thoroughly investigated, instead of assuming its proximity to 1 directly (while this paper also claims that the introduction of separation loss is essential). \n\n(4) Otherwise, if one uses only the first term of variation loss as the total loss, according to your theorem, the ood performance should also be good or even better (since smaller $\\varepsilon$), which contradicts observed facts. This challenges the applicability of the theorem.\n\n(5) Furthermore, based on your assumption, it appears that PCL and other methods could potentially provide similar theoretical insights, thereby largely diminishing the contribution of your approach compared to PCL.\n\n(6) Similarly, since you claim that the proposed loss guarantees improved OOD generalization, you should point out **how the separation loss affects the ood error in your theorem** (the necessity of the separation loss). Otherwise, why can't one remove the separation loss? This concern does not appear to be adequately addressed in Appendix J.\n\n(7) About EMA: The proposed loss can be written as $L(z,\\mu)=variation(z,\\mu)+separation(\\mu)$. Since $\\mu$ is updated by EMA, it is fixed when computating the gradients, i.e., $\\nabla_z L(z,\\mu)=\\nabla_z variation(z,\\mu)+\\nabla_z separation(\\mu)=\\nabla_z variation(z,\\mu)$. Then the encoder updates its weight based on $\\nabla_z variation(z,\\mu)$. After that, $\\mu$ is updated by EMA, depending on the previous $\\mu$ and current $z$, not depending on the loss. Therefore, the separation loss is not used in the training procedure. \n\n\n---\n\nIn summary, this paper appears to overstate its **theoretical** contributions and demonstrates only marginal **empirical** improvements, possibly with technical flaws. Consequently, I am inclined to reject it."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459755394,
                "cdate": 1700459755394,
                "tmdate": 1700460330991,
                "mdate": 1700460330991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "scL2oRt9Rc",
            "forum": "VXak3CZZGC",
            "replyto": "VXak3CZZGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a pracical algorithm for achieving provable out-of-distribution (OOD) generalization. The proposed approach is motivated by recent theoretical work that decomposes OOD generalization into two measurable quantities: intra-class variation and inter-class separation. This paper designs a training objective (and representation space) where these terms can be optimized to achieve low OOD generalization error.\n\nSpecifically, the proposed method learns representations for each data point that lie on a hypersphere. The goal is to encourage data points belonging to the same class to lie close together on the hypersphere (in terms of cosine distance) but to have the centroids of each class lie far apart. This approach itself is not particularly novel, as several learning methods have previously been proposed that utilize hyperspherical embeddings. But this paper is the first to provide a theoretical justification angled at OOD generalization.\n\nThe paper provides a formal theoretical proof that bounds the OOD generalization error via a standard PAC-like learning bound. The proof leans on the prior theoretical results that motivated this work."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposed a simple algorithm that is easy to implement. The loss terms can be computed efficiently and are easy to mini-batch for SGD. The authors provide a clear description of the algorithm and even include pseudo-code. It would be easy to reproduce the proposed method.\n\nThe paper is well-written and easy to follow. Motivation is laid out clearly and the paper accurately describes its contributions relative to prior work. I was able to find all of the information that I wanted while reading the paper either within the main text or the appendices.\n\nI see the primary contribution of this work to be the formal theoretical guarantee on the generalization performance of the proposed method. The theoretical results presented in this work are environment agnostic in the sense that they only depend on the environments through the ability to fit the training data effectively and reduce the intra-class variation. This is a valuable contribution.\n\nThe empirical results are relatively thorough and compare HYPO (the proposed method) against a wide range of baseline methods across several tasks. The results show that HYPO performs well consistently, and is on average the best OOD classifier.\n\nI liked the simple theoretical exploration in Appendix J. This was a valuable inclusion that helped to give some intuition for the class separation loss component."
                },
                "weaknesses": {
                    "value": "The paper lacks quantitative verification of the theoretical result. I think that this would be a valuable contribution to help give an idea of how tight/vacuous the bound is. I am mostly curious about the $\\epsilon$ term that appears in Theorem 5.1 and can be easily computed in practice.\n\nThe theoretical result shown gives a bound on the intra-class variation. This is a useful component of producing an OOD generalization bound, but it is not sufficient by itself. The results in Ye et al. require some regularity conditions that depend on the distribution over the learned representations --- this is difficult to compute in this case. From my point of view, the theoretical results in this paper provide a strong intuition for the success of the method but have not yet been demonstrated to produce a tractable OOD generalization bound.\n\nSpurious correlations are ignored in this work, though are one of the more challenging aspects of OOD generalization in practice. However, I think that this is a reasonable compromise to make at this stage.\n\nI feel that the novelty is slightly limited here. The proposed learning algorithm is a form of prototypical learning on a hypersphere. The specific loss is, to my knowledge, novel but is made up of fairly standard components. The theoretical results are novel and interesting, but are essentially an instantiation of results from prior work. Indeed, the contribution of the training loss to the generalization error is largely captured in an assumption within the theoretical statement. I do consider the overall novelty of this paper to be sufficient for me to recommend acceptance, but it has affected my overall judgment so I am including this as a weakness."
                },
                "questions": {
                    "value": "- I'd appreciate it if the authors could explain the motivation behind Equation 6 a little more.  Is the primary goal to improve on the computational efficiency of computing the average across all training data points? Or is there another benefit to adopting an exponential moving average? This also ties loosely into my next question.\n\n- How strong is the assumption that the samples are aligned? Intuitively, the intra-class variation measures how much the features vary across environments for a single class. The alignment assumption is an assumption over all of the training data in the available environments. Consequently, Theorem 5.1 consists of a term that depends on epsilon, and a generalization term that (intuitively) describes generalization to the unavailable environments. I think it would be more valuable if one could show that the alignment assumption is satisfied by reducing the training loss directly, bringing the result more in line with typical PAC generalization bounds.\n\n- The epsilon factor could potentially make the bound very loose if it is too close to 1. Given that this value is easy to compute, I would be curious to know what epsilon looks like for some of the models trained in the experiments.\n\n- Ye et al. provide a specialized result for linear models (Theorem 4.2 in their work). I see this as a justification that the theoretical framework can be realized by some model. However, in the present work, it is unclear whether the vMF distribution can satisfy the regularity conditions for some choice of environment distribution(s). In other words, how do we know that the OOD generalization bounds can actually be computed for the choice of model used?\n\n\nMinor comments:\n\n- In the introduction, I'd recommend replacing the four lines of citations with a survey paper, for example [1]. The full list of references could be included in the related work, or even as an extended discussion of related work in the supplementary material.\n- [2] is another reference that explores a contrastive metric learning approach for hyperspherical embeddings. The goal here is not to do OOD generalization, but the algorithm is modestly similar.\n- In proof of Theorem 5.1, \"at last $1 - \\delta$\" -> \"at least $1-\\delta$\".\n- It would be nice if Table 1 were sorted by ascending average accuracy.\n\n\n[1]: Domain Generalization: A Survey, Zhou et al.\n[2]: Video Face Clustering with Unknown Number of Clusters, Tapaswi et al. ICCV 2019"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839729003,
            "cdate": 1698839729003,
            "tmdate": 1700477983057,
            "mdate": 1700477983057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GgxxIjKWsO",
                "forum": "VXak3CZZGC",
                "replyto": "scL2oRt9Rc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W1ps"
                    },
                    "comment": {
                        "value": "We are grateful for your insightful comments and appreciation of our work. We address each question in detail and provide further clarifications below.\n\n\n> *Q1. The motivation behind Equation 6*\n\nAs you concur, we choose to update the class prototypes using exponential-moving-average (EMA) due to computational efficiency. Otherwise, computing the average over the entire dataset would have been very expensive. This practice has been commonly used in prior literature, see for example [3].\n\n\n> *C1. Clarification on the significance of Thm 5.1*\n\nWe would like to clarify that in Thm 5.1, the impact of sample-to-prototype alignment on the intra-class variation can be explicitly characterized via the first term $\u03b5^{1\\over 3}$ in the upper bound.  In particular, $\u03b5^{1\\over 3}$ measures how sample embeddings are aligned with their class prototypes on the hyperspherical space (as we have $\\frac{1}{N} \\sum_{j=1}^N \\boldsymbol{\\mu}_{c(j)}^{\\top} \\mathbf{z}_j \\geq 1-\\epsilon$), which is optimized by our proposed loss.  This Theorem implies that improved alignment (an illustration is shown in Figure 4(b)) leads to smaller upper bound of the intra-class variation $\\mathcal{V}^{\\text{sup}}$, a key term to upper bound the OOD generalization error by Thm 3.1. We provide empirical verifications below which further verify that the core assumption in our theoretical framework is satisfied in practice via our loss.\n\n>  Q2 Empirical verification of Thm 5.1 and $\\epsilon$ factor.\n\n\n- **Quantitative verification**: Following the suggestion, we calculate the average intra-class variation over data from all environments $\\frac{1}{N} \\sum_{j=1}^N \\boldsymbol{\\mu}\\_{c(j)}^{\\top} \\mathbf{z}\\_j$ (Thm 5.1) models trained with HYPO. Then we obtain $\\hat{\\epsilon} := 1 - \\frac{1}{N} \\sum_{j=1}^N \\boldsymbol{\\mu}\\_{c(j)}^{\\top} \\mathbf{z}\\_j$. Due to the constraint of time, we evaluated on PACS, VLCS, and OfficeHome and summarize the results in the table below. We observe that training with HYPO significantly reduces the average intra-class variation, resulting in a small epsilon ($\\hat{\\epsilon} < 0.1$) in practice. This suggests that the first term $O(\\epsilon^{1\\over 3})$ in Thm 5.1 is indeed small for models trained with HYPO. \n\n| Dataset |$\\hat{\\epsilon}$ |\n|-----|-----|\n| PACS | 0.06|\n| VLCS| 0.08|\n| OfficeHome| 0.09 | \n\n\n- **Qualitative verification**: We visualized the feature representations via UMAP in Figure 4. We can see that representations learned by HYPO become significantly more aligned with class prototypes across environments. This alignment further confirms the low variation in our learned representations, which is in close agreement with the regularity conditions on the density function required for the main theorem in Ye et al.\n\n\n> *Q3. Ye et al. provide a specialized result for linear models (Theorem 4.2 in their work). I see this as a justification that the theoretical framework can be realized by some model...How do we know that the OOD generalization bounds can actually be computed for the choice of model used?*\n\nGreat question! As specified in P9, we perform classification by identifying the closest class prototype: $\\hat y = \\text{argmax}_{c \\in[C]} f_c$ where $f_c= \\mathbf{z}^\\top\\boldsymbol{\\mu}_c$ for embedding $\\mathbf{z}$.  This is equivalent to the linear top model as in Ye et al. (without the bias term), as we can reformulate as $f_c= (W\\mathbf{z})_c$, where the $c$-th row of $W$ is $\\boldsymbol{\\mu}_c$. In this case, we have a linear convergence rate without the regularity conditions in Thm 4.1 (as shown in P7 in Ye et al.). In a more general case, the generalization bound in Thm 4.1 (pasted below for easy reference) depends on additional factors\n$$\\operatorname{err}(f) \\leq O\\left(s\\left(\\mathcal{V}\\_\\rho^{\\text {sup }}\\left(h, \\mathcal{E}\\_{\\text {avail }}\\right)\\right)^{\\frac{\\alpha^2}{(\\alpha+d)^2}}\\right) $$\n\nsuch as $\\alpha$ and $d$, where $d$ denotes the dimension of feature embeddings, and $\\alpha$ is closely correlated to the density concentration. Such factors mainly affect the convergence rate of the error upper bound. The Theorem shows that, for any model, the generalization gap depends largely on the model\u2019s variation captured by $\\mathcal{V}^{\\text {sup}}$. As HYPO aims to reduce the intra-class variation (verified in the previous response), it effectively reduces the OOD generalization bound."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110238253,
                "cdate": 1700110238253,
                "tmdate": 1700110272903,
                "mdate": 1700110272903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9S9uhbWrw1",
                "forum": "VXak3CZZGC",
                "replyto": "scL2oRt9Rc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_W1ps"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response.\n\nI have taken the time to read through the discussions with other reviewers, in addition to your response here.\n\nI appreciate the additional experiments that you've included. I feel that this helps to solidify the relevance of Theorem 5.1. The $\\hat{\\epsilon}$ values reported lead to an approximate additive error of $\\epsilon^{1/3} \\approx 0.4$ for the datasets explored. I'm having a hard time qualifying how large this value is relative to the loss bound (`B`) as I couldn't easily see how this boundedness assumption impacts the generalization error upper bound. It would also help to compare this value of $\\epsilon$ achieved for other baseline methods --- to verify that an improvement is obtained from the proposed method.\n\nI also share some of the concerns of kMBq, which I discussed in my original review: \"I think it would be more valuable if one could show that the alignment assumption is satisfied by reducing the training loss directly, bringing the result more in line with typical PAC generalization bounds.\" I tend to agree with their final conclusion that this hasn't been adequately demonstrated; though the analysis you gave in response is a good start and I'd recommend that this be included.\n\nOverall, I feel that the paper has been improved in rebuttal. The addition of new empirical results helps to support the claims of the paper. I believe that the theoretical contribution could be made more complete, but I think on balance there are some good contributions in this work. While I broadly agree with Reviewer kMBq, I have chosen to maintain my current score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650567469,
                "cdate": 1700650567469,
                "tmdate": 1700650608097,
                "mdate": 1700650608097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z6SZLWi3WN",
            "forum": "VXak3CZZGC",
            "replyto": "VXak3CZZGC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new loss function tailored to the out-of-distribution problem, where generalisation of the algorithm is required across multiple (and sometimes unseen) environments. Inspired by prior theoretical work, the authors devise an algorithm that encourages samples with the same label to be learnt by features that are as stable as possible across environments, while at the same time encouraging embeddings to look very dissimilar for data points with different labels. They achieve this by embedding points on the sphere and introducing class centroids per label (shared among environments), where points are encouraged to lie close to their corresponding centroid, and centroids themselves are pushed apart. The authors derive a theoretical guarantee for their algorithm and demonstrate its empirical success on CIFAR10-C, PACS and similar."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very well-written which made it (mostly) easy to follow as well as a pleasure to read for me. \n2. The suggested loss function is very intuitive and I like the geometric interpretation the authors provide in terms of the Mises-Fisher model. The visualisation in Fig 4 is also very neat. Empirical performance is also very strong across the different explored tasks."
                },
                "weaknesses": {
                    "value": "1. I struggle to see how Theorem 5.1 connects back to the proposed loss function. From Theorem 3.1 we know that $\\nu^{\\text{sup}}$ serves as an upper bound to the OOD error, and then Theorem 5.1 in-turn provides an upper bound for $\\nu^{\\text{sup}}$ in terms of the  Rademacher complexity and some additive constants. Which term here is the loss trying to minimise here? The Rademacher complexity is over any $\\sigma_i$, so its sign has nothing to do with the true labels. I don\u2019t see how the developed loss would encourage to minimise this quantity. It\u2019s also a worst-case bound in terms of the hypothesis $h$, so again I don\u2019t see how that could be minimised. I hope the authors can elaborate on this connection.\n2. The CIFAR10-C results look strong but only naive ERM is provided as a baseline. How does the approach fair against more specialised algorithms. I don\u2019t expect this novel approach to be state-of-the-art but it would be nice to know where it stands among more modern algorithms.\n3. While the authors do compare against [1], I think the paper would benefit from a more in-depth comparison of the two losses. I\u2019m also a bit confused as to why the results of [1] are not reported in Table 1 but only in a separate ablation in Table 2. Could the authors clarify this?\n\n\\\n\\\n[1] Yao et al, Pcl: Proxy-based contrastive learning for domain generalization"
                },
                "questions": {
                    "value": "1. I think Equation (5) has some typos, shouldn\u2019t the embedding $z_i$ also depend on the environment $e$, i.e. $z_i^e$? If one interprets this equation \u201cliterally\u201d you would be summing over the same $z_i$ over and over again."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699089379667,
            "cdate": 1699089379667,
            "tmdate": 1699636093419,
            "mdate": 1699636093419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xHtNbTyMrC",
                "forum": "VXak3CZZGC",
                "replyto": "z6SZLWi3WN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HW2k"
                    },
                    "comment": {
                        "value": "We are grateful for your insightful comments and appreciation of our work. We address each question in detail and provide further clarifications below.\n\n> *Q1: How does Thm 5.1 connect back to the proposed loss function?*\n\nIn Thm 5.1, we can see that the upper bound consists of three factors: (1) the optimization error $\u03b5^{1\\over 3}$, (2) the Rademacher complexity of the given neural network, and (3) the estimation error which goes down as the number of samples $N$ increases. While (2) and (3) are standard, the critical term, (1) $\u03b5^{1\\over 3}$, measures how sample embeddings are aligned with their class prototypes on the hyperspherical space (as we have $\\frac{1}{N} \\sum_{j=1}^N \\boldsymbol{\\mu}_{c(j)}^{\\top} \\mathbf{z}_j \\geq 1-\\epsilon$), which is optimized by our proposed loss. \n\nOur Theorem implies that improved alignment (as illustrated in Figure 4(b)) leads to smaller upper bound of the intra-class variation $\\mathcal{V}^{\\text{sup}}$, a key term to upper bound the OOD generalization error by Thm 3.1. \n\n> *Q2: How does our loss compare with more specialized algorithms on CIFAR10-C?*  \n\nThanks for the suggestion! Here we compare our loss (HYPO) with very recent algorithms: EQRM (NeurIPS'22) [3] and SharpDRO (SOTA from CVPR'23) [4], on the CIFAR10-C dataset (Gaussian noise). All three methods are trained on CIFAR-10 using ResNet-18, consistent with the architecture used in our main paper. The results are presented below. \n\n| Method | OOD Acc. (%) |\n|-------|-------|\n| EQRM       | 77.06 |\n| SharpDRO   | 81.61 |\n| HYPO (ours)| 85.21 |\n\n> *Q3: A more in-depth comparison with the PCL loss*\n\nWe provide a comparison in the paragraph *Relation to PCL [1]* on page 7. Here we would like to highlight notable distinctions and provide further clarifications:\n- [**Theoretical grounding**] PCL offers no theoretical insights, while our loss HYPO is guided by theory. We provide a formal theoretical justification that our method reduces intra-class variation which is essential to bounding OOD generalization error (Section 5). \n- [**Statistical interpretation via vMF**] While PCL and our loss share high-level intuitions, our loss formulation can be rigorously interpreted as shaping vMF distributions of hyperspherical embeddings (Section 4.2). In contrast, it remains unclear if PCL can be statistically interpreted.\n- [**Strong performance w/o specialized optimization technique SWAD**] Unlike PCL which relies on SWAD [2], a dense and overfit-aware stochastic weight sampling strategy for OOD generalization, HYPO is able to achieve strong performance by simple SGD optimization (Avg Acc 88.0\\% without SWAD) and the performance can be further improved with SWAD.\n\n> *Q4: Why the comparison of HYPO vs. PCL is in not included in Table 1?*\n\nTable 1 intends to contrast the effect of different loss functions **under the standard SGD optimization**. We exclude PCL from Table 1 since the performance reported in the original PCL paper (Table 3) is implicitly based on SWAD. \n\nDue to the relevance of two losses and the choice of optimization strategies, we believe it is worthwhile to provide a detailed comparison of HYPO vs. PCL under four optimization strategies: PCL w/o SWAD, HYPO w/o SWAD, PCL w/ SWAD, and PCL w/ SWAD. Therefore, we summarize the results in a separate table (Table 2), as opposed to including them in Table 1 for better clarity. As shown in the table, compared to PCL, HYPO achieves superior performance with (Avg Acc 88.7\\% $\\rightarrow$ 89\\%) and without SWAD (Avg Acc 86.3\\% $\\rightarrow$ 88\\%), which further demonstrates its effectiveness and generality. \n\nTable 2 in the main paper is pasted here for easy reference:\n\n| Model | Art painting | Cartoon | Photo | Sketch | Average Acc. (%)|\n|-----|-----|-----|-----|-----|-----|\n| PCL w/o SWAD  | 88.0 | 78.8 | 98.1 | 80.3 | 86.3 |\n| HYPO w/o SWAD (Ours) | 87.2 | 82.3 | 98.0 | 84.5 | 88.0 |\n| PCL w/ SWAD   | 90.2 | 83.9 | 98.1 | 82.6 | 88.7 |\n| HYPO w/ SWAD (Ours) | 90.5 | 84.6 | 97.7 | 83.2 | 89.0 |\n\n**References**\n\n[1] Yao et al., PCL: Proxy-based Contrastive Learning for Domain Generalization, CVPR 2022.\n\n[2] Cha et al., SWAD: Domain Generalization by Seeking Flat Minima, NeurIPS 2021. \n\n[3] Eastwood et al., Probable domain generalization via quantile risk minimization. NeurIPS 2022.\n\n[4] Huang et al., Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization. CVPR 2023.\n\n> *Q5: Typos*\n\nThanks for pointing out! We have fixed this typo in the updated manuscript by adding a superscript that explicitly denotes the environment ($\\mathbf{z}_i \u2192 \\mathbf{z}_i^e$) to avoid potential misunderstanding."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108469711,
                "cdate": 1700108469711,
                "tmdate": 1700108469711,
                "mdate": 1700108469711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HkcYQEzcrP",
                "forum": "VXak3CZZGC",
                "replyto": "xHtNbTyMrC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1657/Reviewer_HW2k"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I thank the authors for the additional experiments!\n\n**Loss function:** I missed the dependence of $\\epsilon$ on the alignment, thank you for the clarification. I have a follow-up question; if the insights from this theoretical statement are driving the performance of the algorithm, shouldn't the same loss function *without* the separation term also work? Why do you need the separation term in the first place? How are the two terms competing with each other, i.e. is the introduction of the separation term even negatively affecting the alignment (which is supposed to predict performance). Could the authors clarify this relationship?\n\n**CIFAR10-C:** How does PCL perform?\n\n**HYPO vs PCL:** Thanks for the clarification!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485363664,
                "cdate": 1700485363664,
                "tmdate": 1700485363664,
                "mdate": 1700485363664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]