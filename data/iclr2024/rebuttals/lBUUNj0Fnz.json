[
    {
        "title": "Active Learning for Image Segmentation with Binary User Feedback"
    },
    {
        "review": {
            "id": "hKypAdfhFI",
            "forum": "lBUUNj0Fnz",
            "replyto": "lBUUNj0Fnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission752/Reviewer_eZUf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission752/Reviewer_eZUf"
            ],
            "content": {
                "summary": {
                    "value": "**This paper proposes an active learning algorithm that simplifies the task of data labeling in image segmentation. The proposed algorithm utilizes binary queries, asking about the existence of semantic classes in images, which appears to be a streamlined approach compared to conventional detailed annotation processes.** However, while the methodology is novel and the binary query concept intriguing, the paper might benefit from a more robust evaluation of its practical implications and limitations. \n\nThe empirical studies presented seem promising but might not fully encapsulate the algorithm's efficacy and applicability. Additionally, while the focus on reducing human effort is commendable, there appears to be **a critical assumption that binary queries are sufficient for image segmentation tasks, which might not always hold**. A crucial aspect to consider is the algorithm's reliance on an initial dataset. The effectiveness of the proposed method may be compromised if not furnished with a large number of images initially. This significant dependence on initial data volume underscores a vulnerability in its design, potentially restricting its practical applicability where acquiring ample relevant images is problematic. While the paper's innovative approach to reducing manual labeling efforts is noteworthy, its successful implementation seems contingent upon the availability of a robust initial image dataset. Such a requirement necessitates a thoughtful appraisal of its practical adaptability and overall efficacy in real-world scenarios.\n\nFurthermore, the generalizability of this algorithm to other domains, such as object detection, is mentioned as a future direction, but it remains speculative without empirical validation. In conclusion, the paper presents an active learning for image segmentation but leaves room for a more comprehensive exploration of its practical potential and limitations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presents a new active learning algorithm for image segmentation in deep learning, boasting several notable strengths. \n\n- The algorithm is characterized by a thoughtfully designed linear programming formulation. However, it's important to note that this design is primarily grounded in heuristics. This basis may influence the robustness and predictability of the algorithm, possibly affecting its overall efficacy and application in various contexts.\n\n- The algorithm is user-friendly, making the annotation process considerably more manageable for humans. Simplifying the labeling task through binary queries regarding semantic classes in images fosters an environment where annotators can work more efficiently and effectively, reducing the complexity and burden of the annotation process.\n\n- Performance-wise, the algorithm exhibits reasonable outcomes when furnished with an ample initial set of labeled images. With a sufficient starting dataset, the algorithm illustrates a notable level of effectiveness and utility, positioning itself as a potent tool for image segmentation tasks in deep learning. These observations affirm the viability and functionality of the proposed concept, indicating that the underlying idea holds merit and applicability."
                },
                "weaknesses": {
                    "value": "This paper presents several notable areas for improvement and consideration. \n\n- There is a concern regarding the Linear Programming (LP) formulation, as opposed to some aspects of my statements in strengths, which, while crucial, seems to necessitate significant memory, especially when handling large datasets with extensive semantic classes. This could potentially hinder the algorithm\u2019s efficiency and applicability in broader contexts.\n\n- The theoretical foundation of the proposed framework seems somewhat lacking. Since the formulations primarily rely on heuristics-based LP formulation, there\u2019s an inherent uncertainty regarding the algorithm\u2019s reliability and the specific conditions under which it might fail. A more robust theoretical backing would enhance the algorithm\u2019s credibility and predictability.\n\n- It\u2019s essential to highlight the algorithm\u2019s pronounced dependence on a considerable initial dataset for optimal functionality. This dependency could limit its utility in scenarios where access to such extensive initial data is restricted or impractical.\n\n- The algorithm presented in this paper aims to reduce human labeling efforts, enhancing ease and efficiency in the data annotation process. However, it appears that this simplification comes at a cost, with the algorithm showing a certain level of compromised performance according to Figure 2 and Figure 3. The effort to make the labeling process more user-friendly and less labor-intensive seems to inadvertently lead to a trade-off, affecting the overall efficacy of the algorithm. This aspect suggests a delicate balance between facilitating human involvement and optimizing algorithm performance.\n\n- The paper appears to omit consideration of recent advancements in active learning techniques. Notably, very few pixel annotation strategies, such as \"PixelPick\" [1], which emphasizes minimal pixel-based annotations in each image, and \"BADGE\" [2], \"Balanced Entropy\" [3], or \"PowerBALD/PowerEntropy\" [4], potentially offering a more effective approach than CoreSet, are overlooked. These examples, while not exhaustive, underscore the breadth of contemporary methodologies that could be instrumental in enriching the algorithm\u2019s framework and applicability. By integrating these current developments, the paper could ensure a comprehensive alignment with the evolving landscape of the field, thereby bolstering the algorithm\u2019s relevance and efficacy in the context of modern active learning paradigms.\n\n[1] All you need are a few pixels: semantic segmentation with PIXELPICK, ICCVW 2021 - https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.pdf\n\n[2] Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds, ICLR 2020 - https://openreview.net/forum?id=ryghZJBKPS\n\n[3] Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle, ICLR 2023 - https://openreview.net/forum?id=ZTMuZ68B1g\n\n[4] Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning, TMLR 2023 - https://openreview.net/forum?id=vcHwQyNBjW\n\n**Minor Comment**\n\nPage 4, Eq (1): The sign of entropy has been reversed. It should be $H_{ij}=-p_{ij}\\log p_{ij} - (1-p_{ij})\\\\log (1-p_{ij})$. Otherwise, $H_{ij}$ would be a negative value."
                },
                "questions": {
                    "value": "1. The proposed algorithm necessitates significant memory resources for the Linear Programming (LP) formulation. **Could the authors provide clarification regarding the memory consumption associated with the algorithm, particularly as the volume of unlabeled images increases?** Understanding how the algorithm's memory usage scales in response to larger datasets would be crucial for assessing its practical applicability and efficiency.\n\n2. In evaluating Image Redundancy, i.e., Eq (3) on page 4, the authors have assigned a value of $0$ to negative cosine similarity values. However, it is worth considering whether this approach effectively captures the essence of redundancy. Negative cosine similarity values indicate an inverse redundancy, which suggests that setting these values to $0$ might not be the most informative choice. Using the absolute value of the cosine similarity could be a more reasonable alternative, as it would allow for a better understanding of redundancy relationships. **Could the authors please clarify the rationale behind assigning a $0$ value in this context and why it is deemed essential for accurately capturing redundancy in images?**\n\n3. **Please elucidate the impact of the number of initially labeled images on the algorithm's performance.** This aspect is a pivotal assumption in this study, playing a crucial role in the algorithm\u2019s effectiveness. It is essential to either validate this assumption or provide recommendations regarding the requisite quantity of initial images necessary for the algorithm to function optimally. Clarifying this matter will enhance the algorithm\u2019s practical applicability and reliability in real-world scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Reviewer_eZUf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698464419698,
            "cdate": 1698464419698,
            "tmdate": 1699636002556,
            "mdate": 1699636002556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ph7fjirES6",
                "forum": "lBUUNj0Fnz",
                "replyto": "hKypAdfhFI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eZUf: Part 1"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable comments. We have revised the manuscript based on your feedback, and also provide responses here. All the revisions have been marked in red in the manuscript. \n\n**Q. There is a concern regarding the Linear Programming (LP) formulation, as opposed to some aspects of my statements in strengths, which, while crucial, seems to necessitate significant memory, especially when handling large datasets with extensive semantic classes. This could potentially hinder the algorithm\u2019s efficiency and applicability in broader contexts.**\nWe agree that larger datasets with extensive semantic classes will necessitate more memory. We propose two techniques to address this challenge: \n\n(1) Apply an uncertainty criterion to select a subset of images with the highest uncertainties. Then, apply the LP based selection only on the selected subset of images. Since the uncertainty of an image can be easily computed using entropy, this can substantially reduce the memory and computational overhead. Such a strategy has been used in previous AL research to deal with large datasets [1]. \n\n(2) GPU-based parallel algorithms have been proposed to solve large-scale LP problems [2]. This can greatly reduce the computation time and is an effective way to solve large scale LP problems. \n\nBoth of these will be investigated as part of future research. \n\n[1] S. Chakraborty, V. Balasubramanian, Q. Sun, S. Panchanathan, J. Ye. Active Batch Selection via Convex Relaxations with Guaranteed Solution Bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2015\n\n[2] J. Li, R. Lv, X. Hu, Z. Jiang. A GPU-Based Parallel Algorithm for Large Scale Linear Programming Problem. Intelligent Decision Technologies, 2011\n\n**We have mentioned this in the future work section of the revised paper.**\n\n**Q. The theoretical foundation of the proposed framework seems somewhat lacking. Since the formulations primarily rely on heuristics-based LP formulation, there\u2019s an inherent uncertainty regarding the algorithm\u2019s reliability and the specific conditions under which it might fail. A more robust theoretical backing would enhance the algorithm\u2019s credibility and predictability.**\nWe agree that the solution to the LP problem may sometimes fail (if the problem is infeasible / unbounded). However, in our wide range of experiments (across different datasets, query budgets, backbone network architectures and different initial training set sizes) we never encountered a situation where the solution to the LP problem failed. \n\nIn case the solution to the LP problem fails in a particular AL iteration, a possible strategy is to use the class presence uncertainty matrix $H$ (in Equation 1) and query the $B$ image-class pairs with the highest class presence uncertainty; that is, for that AL iteration, we will be querying samples based on only the uncertainty criterion and not the redundancy criterion. \n\nDeriving the specific conditions under which the LP might fail is a topic of theoretical optimization research, and is outside the scope of this paper. Linear programming (LP) and quadratic programming (QP) solvers have been extensively used in AL research [3 ,4, 5]. \n\n[3] Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Joint transfer and batch-mode active learning. In International Conference on Machine Learning (ICML), 2013.\n\n[4] Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active learning. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2013.\n\n[5] Rita Chattopadhyay, Zheng Wang, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Batch Mode Active Sampling Based on Marginal Probability Distribution Matching. ACM Transactions on Knowledge Discovery from Data, 7(3), 2013. \n\n\n**Q. It\u2019s essential to highlight the algorithm\u2019s pronounced dependence on a considerable initial dataset for optimal functionality. This dependency could limit its utility in scenarios where access to such extensive initial data is restricted or impractical.**\nWe agree this is a very valid concern. To address this, we have conducted two experiments on the Cityscapes dataset, where we reduced the size of the initial training set to **300** and **500** images with pixel-level annotations. **The results are presented in Figure 11 in Section G of the Appendix**. The results depict a very similar trend as Figure 2 in the paper. From the results we note that the mIoU furnished by our binary query based AL method is still very much competitive to the pixel-level and region-level annotation methods, and it outperforms the other binary level annotation methods. Thus, our method is still effective when the size of the initial training set is much smaller."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219420880,
                "cdate": 1700219420880,
                "tmdate": 1700219795063,
                "mdate": 1700219795063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gz8yzIgh4G",
                "forum": "lBUUNj0Fnz",
                "replyto": "hKypAdfhFI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eZUf: Part 2"
                    },
                    "comment": {
                        "value": "**Q. The algorithm presented in this paper aims to reduce human labeling efforts, enhancing ease and efficiency in the data annotation process. However, it appears that this simplification comes at a cost, with the algorithm showing a certain level of compromised performance according to Figure 2 and Figure 3. The effort to make the labeling process more user-friendly and less labor-intensive seems to inadvertently lead to a trade-off, affecting the overall efficacy of the algorithm. This aspect suggests a delicate balance between facilitating human involvement and optimizing algorithm performance.**\nThank you for this insightful comment. As you have correctly pointed out, the main goal of this research is to study the trade-off between the human labeling effort and the overall efficiency of the algorithm. It is expected that, as we attempt to develop novel query mechanisms to reduce the labeling burden on the human oracles, we have to compromise on the performance of the algorithm. Since an AL algorithm with binary annotation did not exist, this trade-off could not be studied for image segmentation applications; to our knowledge, the proposed framework is the first of its kind to develop a binary query based annotation mechanism for AL in image segmentation, which enables us to study this trade-off. Our analysis revealed that we can often save substantial amounts of human annotation effort, in exchange for a minor loss in accuracy. For instance, from the results in Figure 3, we note that the final mIoU achieved by our binary query method is very close to that achieved by Coreset (less than 0.5% difference), which requires pixel-level annotations for the queried images; however, from Table 3, we note that the total annotation time required by Coreset is $134.89$ times greater than our method. We believe that the study of such a trade-off can provide valuable insights during the design and development of image segmentation applications.  \n\n**Q. The paper appears to omit consideration of recent advancements in active learning techniques. Notably, very few pixel annotation strategies, such as \u201cPixelPick\u201d [1], which emphasizes minimal pixel-based annotations in each image, and \u201cBADGE\u201d [2], \u201cBalanced Entropy\u201d [3], or \u201cPowerBALD/PowerEntropy\u201d [4], potentially offering a more effective approach than CoreSet, are overlooked. These examples, while not exhaustive, underscore the breadth of contemporary methodologies that could be instrumental in enriching the algorithm\u2019s framework and applicability. By integrating these current developments, the paper could ensure a comprehensive alignment with the evolving landscape of the field, thereby bolstering the algorithm\u2019s relevance and efficacy in the context of modern active learning paradigms.**\nThank you for this suggestion. We have added the mentioned references in the revised version of the paper. AL techniques like BADGE, BALD and Balanced Entropy require all the pixels to be annotated in the queried images. Since our annotation mechanism is fundamentally different (binary annotation), we did not perform an exhaustive comparison with state-of-the-art AL techniques. Rather, we selected our comparison baselines to cover a wide variety of annotation strategies: pixel-level annotation, region-level annotation and binary level annotation. Within the first category, we used Coreset (a state-of-the-art AL technique) and Entropy (a widely used AL technique) as our comparison baselines. Since our main contribution is a novel annotation mechanism for AL in image segmentation, we selected the comparison baselines to cover different types of annotation strategies, rather than only the state-of-the-art AL methods, which require pixel-level annotation. \n\n**Q. Entropy equation** \nThank you very much for pointing this out. **We have corrected Equation (1) in the revised version of the paper.** \n\n**Q. Image Redundancy computation** \nWe agree that there may be better ways to handle the situation where the cosine similarity produces a negative value. We assign a value of $0$ to entries where the cosine similarity is negative, so that the matrix $R$ does not contain any negative entries; this is critical in the proof of Theorem 1. \n\nAn alternative idea may be to compute the redundancy matrix R and normalize all the values to be within a given range (say, $0$ to $1$) by applying an appropriate transformation. This will be investigated as part of our ongoing work. \n\n**Q. Generalizability to object detection** \nWe agree with your concern and have removed this statement from the future work section.\n\nWe hope these answer your questions. If you have any additional questions, we will be happy to answer."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219652940,
                "cdate": 1700219652940,
                "tmdate": 1700219701539,
                "mdate": 1700219701539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bcgwV8yOa6",
                "forum": "lBUUNj0Fnz",
                "replyto": "Gz8yzIgh4G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Reviewer_eZUf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Reviewer_eZUf"
                ],
                "content": {
                    "comment": {
                        "value": "I am thankful to the authors for taking the time to address my questions and concerns. In light of their responses, I maintain my initial score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537371249,
                "cdate": 1700537371249,
                "tmdate": 1700537371249,
                "mdate": 1700537371249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A7EE2Upo3x",
            "forum": "lBUUNj0Fnz",
            "replyto": "lBUUNj0Fnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission752/Reviewer_ZnPV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission752/Reviewer_ZnPV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an active learning algorithm for image segmentation. The new method identifies a batch of informative images and a list of semantic classes for each image by a constrained optimization problem. The human annotator merely needs to answer whether a given semantic class is present or absent in a given image. The experimental results show that the proposed method consumes less time than pixel-level annotations and region-level annotations, and its annotation results are better than other comparative binary-level annotations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper applies an active learning framework to image segmentation, which poses only binary (yes/no) queries to the users."
                },
                "weaknesses": {
                    "value": "(1) Algorithm 1 does not include all the details of the method.\n\n(2) The article focuses on how to sample images and provide object classes through optimization models, and there is less and vague introduction on how to iteratively achieve pixel level annotation of images."
                },
                "questions": {
                    "value": "(1)\t Algorithm 1 does not include all the details of the method, that is, Algorithm 1 only introduces the method of selecting the unlabeled images and the corresponding semantic classes, but does not mention how the annotator answers yes/no, nor does it mention how to achieve higher precision pixel level annotation iteratively after obtaining M.\n\n(2)\tHow to annotate an image at the pixel level only by determining the classes of the objects in the image? In other words, if all the classes of the objects in the image are correct, how to accurately locate these objects and achieve pixel level annotation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Reviewer_ZnPV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583176819,
            "cdate": 1698583176819,
            "tmdate": 1699636002485,
            "mdate": 1699636002485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CfklrM14K6",
                "forum": "lBUUNj0Fnz",
                "replyto": "A7EE2Upo3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZnPV"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable comments. \n\n**Q1. Algorithm 1 does not include all the details of the method, that is, Algorithm 1 only introduces the method of selecting the unlabeled images and the corresponding semantic classes, but does not mention how the annotator answers yes/no, nor does it mention how to achieve higher precision pixel level annotation iteratively after obtaining M**\n\n**Q2. How to annotate an image at the pixel level only by determining the classes of the objects in the image? In other words, if all the classes of the objects in the image are correct, how to accurately locate these objects and achieve pixel level annotation?**\n\nFor both these questions, we would request the reviewer to kindly refer to Sections F.1 and F.2 in the Appendix, where we have explained in detail how we locate the objects in a given image and achieve pixel level predictions with binary feedback. We also have a couple of visual illustrations in Section F.2, where we have demonstrated how an object gradually shifts to its correct location in a particular test image, as the model receives more and more binary feedback through the AL iterations. \n\nWe hope these answer your questions. If you have any additional questions, we will be happy to answer."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219047311,
                "cdate": 1700219047311,
                "tmdate": 1700219047311,
                "mdate": 1700219047311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "siVNbegjKP",
                "forum": "lBUUNj0Fnz",
                "replyto": "CfklrM14K6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Reviewer_ZnPV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Reviewer_ZnPV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors\u2018 explanation. The complete algorithm process should be reflected in the main text. This article lacks a certain theoretical foundation and analysis. Therefore, I maintain my previous opinion."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573094826,
                "cdate": 1700573094826,
                "tmdate": 1700573094826,
                "mdate": 1700573094826,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yRGHdUIG59",
            "forum": "lBUUNj0Fnz",
            "replyto": "lBUUNj0Fnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission752/Reviewer_cBzD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission752/Reviewer_cBzD"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed an active learning algorithm for image segmentation, which queries binary feedback to ascertain the presence or absence of a semantic class within an unlabeled image. The authors identified the informative image-class pairs by considering both the class presence uncertainty and image redundancy. Furthermore, they conduct experiments and user studies on three image segmentation benchmarks to assess the efficacy of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Compared to pixel-wise or region-wise annotations, binary (yes/no) queries are more time-efficient and less labor-intensive."
                },
                "weaknesses": {
                    "value": "Weakness\n1.Both metrics of uncertainty and diversity are lack of novelty. Besides, in computing class presence uncertainty, the calculation of the probability that image i contains the semantic class j remains unclear. Is it the average probability of pixels belonging to the semantic class j within image i?\n2.The rationale of selecting informative image-class pairs through an optimization problem, rather than designing a sampling strategy, lacks clarity.\n3.The authors employed 1,500 images with pixel-wise annotations to construct the initial training set. Despite the efficiency gains from binary queries in subsequent AL rounds, the annotation cost for the initial training set remains substantial. Moreover, I have reservations about the impact of the initial training set size on AL performance. In other words, if there are fewer pixel-wise annotated samples in the initial training set, will the binary-query-based AL still yield effective results?\n4.The authors did not investigate recent advancements in active learning (AL), and the related work section should be updated accordingly. Besides, the authors did not compare with state-of-the-art AL methods."
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Reviewer_cBzD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743773086,
            "cdate": 1698743773086,
            "tmdate": 1699636002406,
            "mdate": 1699636002406,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eyDy9xUKpF",
                "forum": "lBUUNj0Fnz",
                "replyto": "yRGHdUIG59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cBzD"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable comments. We have revised the manuscript based on your feedback, and also provide responses here. All the revisions have been marked in red in the manuscript. \n\n**Q. Both metrics of uncertainty and diversity are lack of novelty. Besides, in computing class presence uncertainty, the calculation of the probability that image i contains the semantic class j remains unclear. Is it the average probability of pixels belonging to the semantic class j within image i?** \nWe agree that the uncertainty and diversity metrics have been explored in AL research before and are not novel. Our primary novelty in this research is the formulation of an AL technique for image segmentation which poses only binary queries to the human annotators. To the best of our knowledge, such a query technique has not been explored before in the context of image segmentation. Since our main contribution is not the development of an active sampling criterion, rather it is the development of a novel query strategy, we used an existing criterion based on uncertainty and diversity in our work. \n\nYou are absolutely correct. The probability that image i contains the semantic class $j$ is the average probability of pixels belonging to the semantic class $j$ within image $i$. **We have clarified this in the revised version of our paper, just before Equation (1)**. \n\n**Q. The rationale of selecting informative image-class pairs through an optimization problem, rather than designing a sampling strategy, lacks clarity.**\nOur objective was to select a batch of image-class pairs that are both informative individually, and mutually diverse (non-redundant). It may be a little challenging to design a sampling strategy to simultaneously optimize two criteria. A possible sampling strategy may be to first select $K$ (where $K > B$, the actual batch size) image-class pairs based on the informativeness criterion only, and then select the $B$ most diverse image-class pairs from the selected subset. This would be a two-step solution process. \n\nIn contrast, in our algorithm, we devise two separate terms for the informativeness and redundancy criteria and formulate an optimization problem to optimize them simultaneously. Solving the optimization problem directly gives us the $B$ image-class pairs that are both informative and non-redundant, in a single step. \n\n**Q. The authors employed 1,500 images with pixel-wise annotations to construct the initial training set. Despite the efficiency gains from binary queries in subsequent AL rounds, the annotation cost for the initial training set remains substantial. Moreover, I have reservations about the impact of the initial training set size on AL performance. In other words, if there are fewer pixel-wise annotated samples in the initial training set, will the binary-query-based AL still yield effective results?** \nWe agree that this is a very valid concern. We have conducted two experiments on the Cityscapes dataset, where we reduced the size of the initial training set to **300** and **500** images with pixel-level annotations. **The results are presented in Figure 11 in Section G of the Appendix.** The results depict a very similar trend as Figure 2 in the paper. From the results we note that the mIoU furnished by our binary query based AL method is still very much competitive to the pixel-level and region-level annotation methods, and it outperforms the other binary level annotation methods. Thus, our method is still effective when the size of the initial training set is much smaller.  \n\n**Q. The authors did not investigate recent advancements in active learning (AL), and the related work section should be updated accordingly. Besides, the authors did not compare with state-of-the-art AL methods.**\nWe have updated the related work section to include a few recent references on active learning. \n\nState-of-the-art AL methods require the human annotators to annotate all the pixels in the queried images. Since our annotation mechanism is fundamentally different (binary annotation), we did not perform an exhaustive comparison with state-of-the-art AL techniques. Rather, we selected our comparison baselines to cover a wide variety of annotation strategies: pixel-level annotation, region-level annotation and binary level annotation. Within the first category, we used Coreset (a state-of-the-art AL technique) and Entropy (a widely used AL technique) as our comparison baselines. Since our main contribution is a novel annotation mechanism for AL in image segmentation, we selected the comparison baselines to cover different types of annotation strategies, rather than only the state-of-the-art AL methods, which require pixel-level annotation. \n\nWe hope these answer your questions. If you have any additional questions, we will be happy to answer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218596105,
                "cdate": 1700218596105,
                "tmdate": 1700218912913,
                "mdate": 1700218912913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iStcGk24v3",
                "forum": "lBUUNj0Fnz",
                "replyto": "yRGHdUIG59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer cBzD,\n\nWe wanted to check back with you to see if you got a chance to review our rebuttal and the revised version of the paper, and whether you had any further questions. If not, we would like to request you to kindly consider increasing your score. \n\nBest regards,\n - Authors of Paper 752."
                    },
                    "title": {
                        "value": "Requesting Reviewer cBzD to kindly check our response"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704955803,
                "cdate": 1700704955803,
                "tmdate": 1700705090925,
                "mdate": 1700705090925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rC8M2NYo1H",
            "forum": "lBUUNj0Fnz",
            "replyto": "lBUUNj0Fnz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission752/Reviewer_nZpY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission752/Reviewer_nZpY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use Active Learning (AL) to query for weak labels for the task of semantic segmentation. The proposed approach is evaluated on three different datasets with two different backbones for the chosen architecture. Additionally a small user study is performed to motivate the benefits provided by the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Interesting idea to combine existing AL techniques with weak labels. \n\nVery helpful to motivate the proposed approach with a user study and actual labelling effort measured in annotation time. \n\nThe proposed approach is evaluated on multiple datasets of varying difficulty/scope, additionally two different backbones are used in the experiments."
                },
                "weaknesses": {
                    "value": "# General comments\nOverall the idea is interesting, while incremental in novelty. The presentation and language of the submission have room for improvement. As the paper touches weak-labelling and active learning, it could be better embedded especially in the literature/related work on semi-supervised learning, self-supervised learning and especially learning from weak labels.\n\nMinor hints on language, there are more of those, i suggest to consider an additional proof reading step by a native speaker:\n* \"to induce a neural network\" not sure what this formulation refers to, it is used at multiple places.\n* \"which entails much lesser annotation effort\" consider rephrasing\n* \"furnishing the highest prediction entropy\" not sure 'furnishing' is an optimal choice of wording here\n\n# Related work\nWhile (Settles, 2010) is certainly an important reference, however, there are more current survey papers that can be cited here, e.g. https://arxiv.org/pdf/2203.13450.pdf\n\n\"Active learning for image segmentation has been comparatively less explored than other applications.\"\nI disagree with that statement, with semantic segmentation being algorithmically very close to classification, I'd argue semantic segmentation is one of the prime applications of AL methods. See e.g. the references the authors themselves use, (Casanova et al., 2020), (Kasarla et al., 2019), (Mackowiak et al., 2018), (Vezhnevets et al., 2012), (Golestaneh & Kitani, 2020) and more, like\n\nSiddiqui, Yawar, Julien Valentin, and Matthias Nie\u00dfner. \"Viewal: Active learning with viewpoint entropy for semantic segmentation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\nXie, Shuai, et al. \"Deal: Difficulty-aware active learning for semantic segmentation.\" Proceedings of the Asian conference on computer vision. 2020.\n\nLearning from weak labels seems to be missing from the comparisons, i suggest adding a discussion or better experimental comparison, some suggestions below. \n\nOlmin, Amanda, et al. \"Active Learning with Weak Labels for Gaussian Processes.\" arXiv preprint arXiv:2204.08335 (2022).\n\nWu, Jian, et al. \"Weak-labeled active learning with conditional label dependence for multilabel image classification.\" IEEE Transactions on Multimedia 19.6 (2017): 1156-1169.\n\nYounesian, Taraneh, et al. \"Qactor: Active learning on noisy labels.\" Asian Conference on Machine Learning. PMLR, 2021.\n\nWu, Jian, et al. \"Weak-labeled active learning with conditional label dependence for multilabel image classification.\" IEEE Transactions on Multimedia 19.6 (2017): 1156-1169.\n\nLu, Zhiwu, et al. \"Learning from weak and noisy labels for semantic segmentation.\" IEEE transactions on pattern analysis and machine intelligence 39.3 (2016): 486-500.\n\n# Implementation Details\n\n## User Study\nVery valuable to provide some evidence on the time required for manual labelling.\nUnfortunately the user study has been conducted with a relatively low number of Annotators (3) and images (10) and only the mean was reported, potentially averaging over very extreme differences in annotator performance.\n\nThe reported numbers are still valuable, but the study could be improved. To understand the results better it would help to elaborate on the proficiency of the annotators (un-trained?) and give a reference to the annotation tool used. Different tools can have very varying suitability for the three different tasks that were compared.\n\nWith the low number of images and annotators exhaustive statistics seem non-appropriate, but providing some guidance on the spread of the values, e.g., standard-deviation (across all dimensions or across images), would help in interpreting them.\n\n## Experimental setup\nFor a study as the presented one the split into train, test and pool is actually very important, however details on how the split was realized are lacking. Additionally there is no validation set mentioned, usually i'd assume the validation was used to tune the training of the chosen architecture on the chosen dataset. \n\nCityscapes provides 20k weakly annotated frames, given, that the authors propose a AL + weak label scheme, those would be a prime candidate for more exhaustive experiments. \n\nSide-note: i find the graphs a bit hard to read due to the chosen style of presentation, different line styles and markers, as well as larger images could help."
                },
                "questions": {
                    "value": "* Which labelling tool was used for the user study?\n* Were the annotators trained at the task or did they do labelling for the first time?\n* How was the train, test, pool split realized? Same strategy for the different datasets?\n* Why is the test set rather small?\n* Was there no validation set? \n* How were the parameters for the training of the DNN chosen? \n* The reported numbers do not seem to match the full size of e.g., Cityscapes, any reason to not using the full dataset for the experiments?\n* How do the results compare to a random baseline?\n* How do the results compare to a baseline (same architecture) trained on the full set (train + pool) evaluated on the custom test split?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission752/Reviewer_nZpY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699451556923,
            "cdate": 1699451556923,
            "tmdate": 1700833621005,
            "mdate": 1700833621005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hmcZEtuz3c",
                "forum": "lBUUNj0Fnz",
                "replyto": "rC8M2NYo1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nZpY"
                    },
                    "comment": {
                        "value": "Thank you very much for your comments. We have revised the manuscript based on your feedback, and also provide responses here. All the revisions have been marked in red in the manuscript. \n\n# General Comments\n\n**Q. \u201cto induce a neural network\u201d not sure what this formulation refers to, it is used at multiple places.**\nWe have updated this statement and have replaced \u201cinduce\u201d with \u201ctrain\u201d.\n\n**Q. \u201cwhich entails much lesser annotation effort\u201d consider rephrasing**\nWe have updated this statement and have replaced \u201centails\u201d with \u201crequires\u201d.\n\n**Q. \u201cfurnishing the highest prediction entropy\u201d not sure 'furnishing' is an optimal choice of wording here**\nWe have updated this statement and have replaced \u201cfurnishing\u201d with \u201cproducing\u201d.\n\n\n\n# Related Work\n\nWe have included all the mentioned references in the revised version of the paper. We have removed the statement: \"Active learning for image segmentation has been comparatively less explored than other applications.\"\n\n\n\n\n# Implementation Details\n \n**Q. Annotation tool used**\nWe used the LabelMe annotation tool for the user study. We have mentioned this in Section 4.5 of the revised version of the paper and have also included a reference. \n\n**Q. Proficiency of the annotators**\nThe annotators did not have any training specifically for the annotation task. This was done to mimic a real-world scenario where the annotators may not have specific knowledge / training about the application in question. \n\n**Q. Standard deviation of the values** \nWe have included the standard deviation values in Table 1 of the revised version. \n\n**Q. How was the train, test, pool split realized? Same strategy for the different datasets?**\nFollowing the convention in AL research, a given dataset was split at random to generate the training, unlabeled and test sets. We repeated the experiment for three different random splits and averaged the results, to rule out any effects of randomness. \n\n**Q. Was there no validation set?**\nWe did not use any separate validation set. This is because, the main premise of AL is that we do not have a large amount of labeled data; using a separate validation set (besides the training set) will be contradictory to the conventional AL setup. During training the model, we randomly used 10% of the training set itself as the validation set. \n\n**Q. Cityscapes provides 20k weakly annotated frames, given, that the authors propose a AL + weak label scheme, those would be a prime candidate for more exhaustive experiments.**\nWe are aware that the Cityscapes dataset provides several frames that are weakly annotated. However, the annotation scheme that we proposed in this paper is binary; that is, annotators are asked whether a particular semantic class is present in a particular image, and they simply have to answer YES / NO. This form of annotation is much weaker than the weakly annotated frames. \n\n**Q. How were the parameters for the training of the DNN chosen?**\nWe followed the training parameter settings in the following papers in our work:\n\n[1] L. Chen, G. Papandreou, F. Schroff, H. Adam. Rethinking Atrous Convolution for Semantic Image Segmentation. arXiv:1706.05587v3, 2017\n\n[2] B. Kim, H. Choi, H. Jang, S. Kim. Resolution-Aware Design of Atrous Rates for Semantic Segmentation Networks. arXiv:2307.14179, 2023\n\n[3] F. Lin, T. Wu, S. Wu, S. Tian, G. Guo. Feature Selective Transformer for Semantic Image Segmentation. arXiv:2203.14124v3, 2022\n\n**Q. Why is the test set rather small? The reported numbers do not seem to match the full size of e.g., Cityscapes, any reason to not using the full dataset for the experiments?**\nNo particular reason. We plan to conduct experiments on the full dataset as part of our ongoing research. \n\n**Q. How do the results compare to a random baseline?**\nRandom sampling was included as a comparison baseline in all our experiments; the method RR denotes the baseline where the images, as well as the semantic classes were selected at random for active query. \n\n**Q. How do the results compare to a baseline (same architecture) trained on the full set (train + pool) evaluated on the custom test split?**\nWe conducted an experiment to study the performance of our method compared to a fully supervised baseline (trained on the training set + the complete unlabeled pool). **The results are presented in Figure 12 and Table 9 in Section H of the Appendix.** These results further corroborate the usefulness of our framework in drastically reducing human annotation effort, with a minor loss in performance, for image segmentation applications. Please refer to section H of the Appendix for more details.\n\nWe hope these answer your questions. If you have any additional questions, we will be happy to answer."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217694885,
                "cdate": 1700217694885,
                "tmdate": 1700218416950,
                "mdate": 1700218416950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RF894jbgBt",
                "forum": "lBUUNj0Fnz",
                "replyto": "rC8M2NYo1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission752/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer nZpY,\n\nWe wanted to check back with you to see if you got a chance to review our rebuttal and the revised version of the paper, and whether you had any further questions. If not, we would like to request you to kindly consider increasing your score.\n\nBest regards,\n - Authors of Paper 752."
                    },
                    "title": {
                        "value": "Requesting Reviewer nZpY to kindly check our response"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705056624,
                "cdate": 1700705056624,
                "tmdate": 1700705142315,
                "mdate": 1700705142315,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]