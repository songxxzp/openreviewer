[
    {
        "title": "Graph Clustering with Masked AutoEncoders"
    },
    {
        "review": {
            "id": "WVprpuc5qe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_yz1d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_yz1d"
            ],
            "forum": "NvQ4kzcRSL",
            "replyto": "NvQ4kzcRSL",
            "content": {
                "summary": {
                    "value": "This paper studies the problem of graph clustering and proposes a new framework named Graph Clustering with Masked Autoencoders (GCMA). It involves a graph masking into an auto-encoder framework. Extensive experiments demonstrate the superiority of GCMA over state-of-the-art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies a practical problem.\n- The main idea of the paper is simple and intuitive.\n- The proposed method achieves superior performance on various datasets for different benchmark tasks."
                },
                "weaknesses": {
                    "value": "- The evaluation is not sufficient. The method only involves one large-scale datasets, i.e., Ogbn-arxiv, which is not sufficient to support the claim.\n- More result analysis should be included in Sec. 4.4. \n- There are some missing prior works about graph clustering in 2022-2023, e.g., [1], which should be included in performance comparison. \n- It seems that your self-optimization modules is similar to Deep Embedding Clustering, which should be discussed.  \n\n[1] Yi et al., Redundancy-Free Self-Supervised Relational Learning for Graph Clustering"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Reviewer_yz1d"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697269340287,
            "cdate": 1697269340287,
            "tmdate": 1699636295413,
            "mdate": 1699636295413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NAipY7rSdU",
                "forum": "NvQ4kzcRSL",
                "replyto": "WVprpuc5qe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1.**\n\nFor large-scale datasets, our experimental results on ogbn-arxiv have demonstrated our advantage as a non-parametric method, capable of predicting the number of clusters in large datasets. However, the disadvantage is that, as the data scale increases, hindered by noise interference and dimensionality growth, GCMA, like most methods, experiences a gradual decline in performance. Due to space limitations, we believe it is unnecessary to elaborate excessively on this universally acknowledged aspect in the main text. However, we will provide corresponding experimental results below as supporting evidence. We ran our model on the larger dataset Reddit. While other baseline such as EGAE will have out of memory problem. There is only one way to get the result for the parameter class method S3GC[4].SHOT represents the accuracy of the predicted k-value.\n\n|         | S3GC | GCMA   |\n|---------|--------|--------|\n| Shot    | -   | 2/10  |\n| NMI     | 80.70  | 69.10  |\n| ARI     | 74.50  | 58.98  |\n\n**Q2.**\n\nAdditional result analyses have been incorporated into the original text. In Section 4.4, our intention was to demonstrate the superiority of the GCMA method and the impact of integrating AE encoding, namely the GCMA-A model. Therefore, visual results for only two datasets are presented along with brief explanations. From another perspective, Section 4.4 serves as a visual representation of Table 3. It can be observed from the graphs that the separation effect between clusters is worse for GCMA-A, proving the effectiveness of integrating AE encoding. The integrated graph embedding makes full use of node information to correct the clustering results.\n\n**Q3.**\n\nWe did not have the methodology of the literature [1] when we started writing this paper, so we added additional experiments. The experimental dataset is cite. However, it is clear that this is a parametric class method that requires the correct number of clusters k to be entered in advance for optimal performance.\n\n|         | R2FGC | GCMA   |\n|---------|--------|--------|\n| Shot    | -   | 10/10  |\n| NMI     | 45.39  | 44.30  |\n| ARI     | 47.07  | 46.07  |\n\n**Q4.**\n\nThe self-optimization method we applied indeed stems from deep clustering research, as seen in references [2] and [3]. This method has become nearly a universal approach within the clustering domain for enhancing the quality of autoencoder encodings. It achieves this by compelling better representations through self-supervised training. Subsequent ablation experiments substantiate this point, and for this aspect, we conducted more in-depth experimental research, specifically investigating the impact of the number q on the performance of the self-supervised module (refer to Table 4). The results of these experiments consistently indicate that our application of the optimal number q for the self-supervised optimization module yields the best performance.\n\n[1] Yi et al., Redundancy-Free Self-Supervised Relational Learning for Graph Clustering\n\n[2] Hong Yuan\uff0cet al.. Embedding graph auto-encoder for graph clustering\n\n[3]Chun Wang, et al.Attributed graph clustering: A deep attentional embedding approach.\n\n[4]Fnu Devvrit, et al.S3GC: Scalable Self-Supervised Graph Clustering"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059585477,
                "cdate": 1700059585477,
                "tmdate": 1700059585477,
                "mdate": 1700059585477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wloQPnaxVB",
                "forum": "NvQ4kzcRSL",
                "replyto": "WVprpuc5qe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer, I have submitted my reply a few days ago. Now the deadline for public comment is approaching, but I observe that you have not replied to my comment. I would be happy if you let me know if you still have some questions or replies. If so, I will reply promptly. Looking forward to your reply."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478468166,
                "cdate": 1700478468166,
                "tmdate": 1700478468166,
                "mdate": 1700478468166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AEYnWC1ZhJ",
                "forum": "NvQ4kzcRSL",
                "replyto": "wloQPnaxVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_yz1d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_yz1d"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. It seems that the paper was not revised.  I will keep my score."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623709855,
                "cdate": 1700623709855,
                "tmdate": 1700623709855,
                "mdate": 1700623709855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U3cU8qzEzz",
            "forum": "NvQ4kzcRSL",
            "replyto": "NvQ4kzcRSL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_CdSX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_CdSX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework called Graph Clustering with Masked Autoencoders, i.e., GCMA, for unsupervised graph clustering. GCMA employs a fusion autoencoder based on graph masking and an improved density-based clustering algorithm to improve the generalization ability of graph autoencoder clustering algorithms. One advantage of the proposed model is that it can automatically determine the number of clusters k. The authors demonstrate that GCMA can outperform other graph clustering baselines on citation graphs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides a clear introduction to the problem of unsupervised graph clustering and the motivation for the proposed model.\n\n2. A detailed explanation of the fusion autoencoder based on graph masking used in GCMA.\n\n3. GCMA can automatically determine the number of clusters k."
                },
                "weaknesses": {
                    "value": "1. The novelty contribution is incremental. This paper applies graph MAE for graph clustering. However, MAE is well-known and graph MAE has been used for various graph tasks, the idea is not novel.  \n\n2. The method description is not entirely clear. As a major difference from the parametric baselines, automatically determine k is a claimed advantage, but how it can be done is not clear from the current writing.\n\n3. Empirical evaluation is not sufficient. Only citation graphs are considered. Does the proposed method also work for other types of graphs?"
                },
                "questions": {
                    "value": "1. For Table 3, are the ground truth k used as input for the parametric algorithms? \n\n2. How do the non-parametric methods perform in terms of those metrics in Table 3, e.g., ACC, NMI, and ARI? \n\n3. In section 3.2.2, how exactly was k generated? The paper says \"Thus the clustering center can be clearly separated from the rest of the data points to get the best k value\". The authors may illustrate more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Reviewer_CdSX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563960970,
            "cdate": 1698563960970,
            "tmdate": 1699636295279,
            "mdate": 1699636295279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q47GNtZiTX",
                "forum": "NvQ4kzcRSL",
                "replyto": "U3cU8qzEzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our work. In response to your questions, we provide detailed explanations below.\n\n**W1.**\n\nCertainly, as the reviewer pointed out, graph masking methods have become increasingly prevalent. However, there hasn't been a well-performing non-parametric method for graph clustering. In earlier works like MGAE, edge masking was applied to the graph, followed by decoding through cross-correlation. Subsequent methods such as MaskGAE utilized single-edge masking and path masking. More recent approaches like GiGaMAE and GraphMAE introduced feature masking and re-masking to enhance training data for better results. However, current research predominantly focuses on the study of graph masking encoding performance, with downstream tasks mainly oriented towards link prediction and node classification. There is a lack of a dedicated deep learning model applied to clustering tasks. The only existing small-scale downstream experiment on deep clustering with graph masking is found in GiGaMAE, but this model did not incorporate specific improvements for clustering, resulting in relatively poor experimental performance. In light of these research developments, GCMA innovatively combines graph masking technology with clustering. We seamlessly integrate graph masking technology, utilizing single-edge, node, and re-masking, and incorporating AE encoding. We introduce a multi-objective decoder specifically tailored for clustering tasks. In terms of graph encoding technology, our differentiator lies in the modifications made specifically for clustering tasks. We inherit multi-objective reconstruction and the second CFSFDP decoder, achieving better clustering performance while keeping training costs low.\n\n**W2.**\n\nNext, we provide a detailed explanation of the process for predicting the clustering number, k. In the main text, especially in Figure 2, we have depicted the relevant flowchart. The algorithm operates based on two fundamental assumptions. First, data points near the cluster center have lower density, and second, data points are farther away from other centers with higher density. Sample point density is defined as $\\rho$, and the minimum distance between a data point and points with higher density is defined as $\\delta$. Among all data points, there are points with a product $\\gamma_i$ that is larger, representing points with higher density $\\rho$. These points are the centers of clusters. After determining several centers, the remaining data points are assigned in order of density, belonging to the point with higher density closest to them. This way, we can automatically obtain the clustering number, k. We integrate this process into our end-to-end GCMA model, optimizing the distance between sample points and cluster centers, further enhancing the algorithm's performance.\n\n**W3.**\n\nRegarding the consideration for dataset selection, our primary concern is the distinction between graph and non-graph data. This allows for better validation of our model's generalization capability. For graph data, most clustering algorithms primarily utilize referenced graph datasets, such as in the literature SDFN, DFCN, S3GC. Rarely do we see other types of graph data in related research.\n\n**Q1.**\n\nFor other baselines in Table 3, we provided the correct k value as input. Otherwise, it would be unfair to compare with other parameter-based methods. As mentioned in our Figure 1, the overall clustering performance of parameter-based methods would significantly decline.\n\n**Q2.**\n\nWe did not find a non-parametric method that is highly suitable for graph data. Therefore, we had to use non-parametric methods from Table 2 as baselines for performance comparison. However, we acknowledge that this might be unfair to methods not specifically designed for graph data, as they may struggle to showcase their performance on non-specialized data. Here are the additional experimental results we have supplemented. We choose the Cora dataset.\n\n|       | DeepDPM | DED    |\n|-------|---------|--------|\n| ACC   | 11.23   | 4.26   |\n| NMI   | 15.91   | 3.32   |\n| ARI   | 8.10   | 2.13   |\n\n**Q3.**\n\nAs mentioned in our response in section \"W2,\" the algorithmic process of CFSFDP allows us to obtain points with larger $\\gamma_i$ values, which we consider as cluster centers, thus determining the number of clusters, k. It is essential to highlight the improvements we introduced in our work. First, in our enhancements, we employed a Gaussian kernel to determine the truncation distance. Second, the original CFSFDP, being an independent algorithm, was integrated into the end-to-end GCMA, where we designed a unified loss function for optimization. These improvements position CFSFDP as the second decoding module within GCMA.\n\nFinally, thank you again for taking the time to review my paper and I look forward to hearing from you again."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055161809,
                "cdate": 1700055161809,
                "tmdate": 1700055161809,
                "mdate": 1700055161809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IMnKKDMuts",
                "forum": "NvQ4kzcRSL",
                "replyto": "U3cU8qzEzz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer, I have submitted my reply a few days ago. Now the deadline for public comment is approaching, but I observe that you have not replied to my comment. I would be happy if you let me know if you still have some questions or replies. If so, I will reply promptly. Looking forward to your reply."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478461497,
                "cdate": 1700478461497,
                "tmdate": 1700478461497,
                "mdate": 1700478461497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wT8mwPW86Q",
                "forum": "NvQ4kzcRSL",
                "replyto": "IMnKKDMuts",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_CdSX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_CdSX"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank the authors for their rebuttal regarding my questions. I partially agree with the response regarding novelty, dataset selection, and non-parametric method comparison. I will keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520924524,
                "cdate": 1700520924524,
                "tmdate": 1700520924524,
                "mdate": 1700520924524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c0OafZYU7D",
            "forum": "NvQ4kzcRSL",
            "replyto": "NvQ4kzcRSL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_5Nk3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_5Nk3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose Graph Clustering with Masked Autoencoders (GCMA). They claim three different contributions: \n1. the first method to determine the number of clusters specifically for graph data.\n2. using the mask graph mechanism allows learned representations to be applied to multiple types of downstream tasks\n3. Extensive experiments on five datasets demonstrate that our model outperforms existing state-of-the-art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Sorry, I really cannot get significant strengths."
                },
                "weaknesses": {
                    "value": "Originality:\n     The whole method is based on GraphMAE (or modified version). The only significant difference is \u201cCFSFDP DECODER\u201c. But it is not new and the authors also point out the citation. \nNovelty:\n    The novelty is also an issue. Self-supervised learning on graphs + a specific loss on clustering and no experiment on comparison between SSL on graphs and the proposed method.\nPerformance:\n    The comparison only picks up old graph clustering methods and new general clustering methods, not graphs. In my opinion, the performance is fair but not a SOTA one in the graph-based methods. Even I think this method cannot outperform some graph convolution-only methods like \n1. NAFS: A Simple yet Tough-to-beat Baseline for Graph Representation Learning"
                },
                "questions": {
                    "value": "I have no question. But I am still open to observing how the authors convince all reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641208749,
            "cdate": 1698641208749,
            "tmdate": 1699636295205,
            "mdate": 1699636295205,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w0RdW8GRsQ",
                "forum": "NvQ4kzcRSL",
                "replyto": "c0OafZYU7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our work. In response to your questions, we provide detailed explanations below.\n\nFirstly, we want to clarify the distinctions between our approach and GraphMAE. GraphMAE primarily masks nodes and their corresponding row vectors, replacing the decoder with a GNN layer (GAT/GIN) instead of an MLP. In contrast, our method masks weights, including nodes and edges, and employs a multi-objective decoding approach to address decoding challenges.\n\nRegarding the previously proposed CFSFDP method, it had three main issues. Firstly, the algorithm's truncation distance, dc, originally required human experience for determination. In our enhancement, we utilize a Gaussian kernel, enabling automatic determination of the truncation distance. Secondly, the original algorithm necessitated the semi-automatic input of the cluster number, k, based on observed built-in parameters. We, however, directly obtain cluster centers by calculating parameters \u03b3i and point density. Lastly, the original CFSFDP operated as an independent algorithm, but we integrated it into the end-to-end GCMA, designing a unified optimized loss function. These improvements position CFSFDP as the second decoding module in GCMA. Consequently, our method operates as a non-parametric approach, eliminating the need for pre-inputting the cluster number k. The original method fails to achieve comparable experimental results, and notably, few researchers in the field of graph clustering have employed this method.\n\nAdditionally, concerning your remark about not conducting comparative experiments between SSL and the proposed method, this decision stems from the fact that, as a deep clustering model, the SSL module has become a standard component, as seen in AGC and EGAE within our model. Its coercive learning approach has been validated to optimize embeddings to some extent. Therefore, our comparative experiments focused solely on parameter selection within SSL, as detailed in Table 4.\n\nFinally, we would like to explain our motivation for choosing baselines. Our experiments aim to investigate the clustering performance and the accuracy of predicting k for GCMA. The logical design of our experiments is as follows. Two performance metrics are considered: 1) the accuracy of predicting clustering numbers, k, and 2) graph clustering performance. For the former, our experimental baseline is a non-parametric clustering method capable of predicting k. As there are few methods specifically designed for graph clustering, we introduce some deep clustering methods with SOTA performance as benchmarks. For the latter, we need to compare graph clustering performance, thus referencing baseline methods 1, 2, and 3, which are recent SOTA methods. We believe applying graph data to new experimental results obtained through clustering methods would be unfair. The selected graph clustering methods are also among the most effective in recent years, as indicated by various references, including the latest literature [1], which cites these baselines. Experimental results demonstrate that we still achieve considerable performance in graph clustering.\n\nOnce again, we appreciate your time in reviewing our paper and look forward to your further feedback.\n\n[1] Yi et al., Redundancy-Free Self-Supervised Relational Learning for Graph Clustering"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053700051,
                "cdate": 1700053700051,
                "tmdate": 1700053700051,
                "mdate": 1700053700051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UMYWVzD7w6",
                "forum": "NvQ4kzcRSL",
                "replyto": "c0OafZYU7D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer, I have submitted my reply a few days ago. Now the deadline for public comment is approaching, but I observe that you have not replied to my comment. I would be happy if you let me know if you still have some questions or replies. If so, I will reply promptly. Looking forward to your reply."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478435188,
                "cdate": 1700478435188,
                "tmdate": 1700478435188,
                "mdate": 1700478435188,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SOqXLjTe2e",
            "forum": "NvQ4kzcRSL",
            "replyto": "NvQ4kzcRSL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_sCEC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_sCEC"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes an unsupervised method for graph clustering based on Graph Masked AutoEncoder. The motivation behind this work is that existing methods struggle to automatically select the number of clusters and lack good generalization ability. To address this issue, the paper introduces a density-based clustering algorithm as the Decoder module. The proposed approach can end-to-end select the number of clusters while improving generalization performance. Extensive experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This article addresses a significant drawback of previous graph clustering algorithms, which is the need to pre-determine the number of clusters."
                },
                "weaknesses": {
                    "value": "-**Writing Quality**: The writing quality of this article needs improvement. The language used in the article feels difficult to understand. Additionally, the quality of some of the figures, such as Figure 1, could be enhanced.\n\n-**Method is not clearly presented**:  The introduction of the proposed method in this article is quite unclear. For instance, in Section 3.2.2, the authors abruptly introduce a new concept, CFSFDP, making it challenging for readers unfamiliar with this concept to grasp the purpose of this section. It seems that the authors did not effectively emphasize the main focus of this paper. In my view, since Masked Graph Autoencoders are already established content, the author's primary contribution should lie in the clustering part. However, the authors dedicate a significant amount of content to the introduction of Graph masking and AutoEncoder, which, in my opinion, is unnecessary. While the authors provide Algorithm 1 as a summary of the entire model, this algorithm appears overly concise. Furthermore, in Equation (3), the authors propose using mutual information to calculate the loss. However, I fail to discern the connection between Equation (3) and Equation (4) with mutual information.\n\n-**Rationality for the designs**: Some of the designs proposed in this paper appear to rely heavily on intuition, and certain claims lack robust supporting evidence. For example, in Section 3.1.1, it is not clear why masking both modal information at the same time is considered harmful. In Section 3.1.2, the reasoning behind why a graph autoencoder would overemphasize proximity information needs further explanation. In 3.2.1, the motivation behind adding a mask to Z_m is not well-justified.  I hope the authors can provide their own analysis rather than simply following others' claims."
                },
                "questions": {
                    "value": "Most of my questions have been presented in the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3432/Reviewer_sCEC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697600047,
            "cdate": 1698697600047,
            "tmdate": 1699636295112,
            "mdate": 1699636295112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FwRjnyngcf",
                "forum": "NvQ4kzcRSL",
                "replyto": "SOqXLjTe2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weaknesses:Writing Quality"
                    },
                    "comment": {
                        "value": "Thank you very much for your evaluation of our work. We will now provide detailed responses to the raised questions.\n\nWe have made several modifications to the original text, improving the grammar and sentences to enhance the overall readability of the article. For instance, in the introduction of the first chapter, we strengthened the logical flow to make the development of graph masking technology more comprehensible. Regarding Figure 1, we redesigned the information within the image. We added a grid to the background and clarified the numerical values of each point, allowing readers to clearly observe the overall trend."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056710075,
                "cdate": 1700056710075,
                "tmdate": 1700056710075,
                "mdate": 1700056710075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7BznlhWIS4",
                "forum": "NvQ4kzcRSL",
                "replyto": "SOqXLjTe2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rationality for the designs"
                    },
                    "comment": {
                        "value": "First and foremost, it is crucial to clarify that the viewpoints presented in the article are derived from prior relevant research and our experimental results. The perspectives in sections 3.1.1 and 3.1.2 are based on references [1] and [2], respectively. These sections were briefly mentioned in Chapter 1 without elaboration, as they are not the focus of this section, to avoid unnecessary repetition. Therefore, in these subsections, we only provide brief mentions. A more detailed explanation is warranted, as we believe in the viewpoint presented in 3.1.1. This is because, when both node features and edges of the same node are masked, we lack direct information about this node and can only obtain its information from the hidden representations of other nodes. This poses a greater obstacle to the originally intended reconstruction decoding. Without alternative decoding methods, it would have a detrimental impact on the model's performance. To validate this point, we conducted experiments. Specifically, we changed the decoding method to reconstruct the graph structure traditionally. This approach did not consider the harmful effects of simultaneously blocking two modal information types. From the experimental results, it is evident that the clustering performance of this reconstruction method significantly deteriorated. We experimented with the Cora dataset.GCMA-r stands for using the traditional reconstruction graph structure as the decoder, and SHOT stands for the number of correct k-values predicted.\n\n|         | GCMA-r | GCMA   |\n|---------|--------|--------|\n| Shot    | 7/10   | 10/10  |\n| NMI     | 45.00  | 59.16  |\n| ARI     | 36.00  | 55.41  |\n\nRegarding the point mentioned in Section 3.1.2 about GAE overly emphasizing proximity information, this is because previous GAE clustering methods mostly used the link reconstruction graph method to construct loss. In each node's hidden representation, there is information about other neighboring nodes, once again emphasizing the impact of proximity information on overall reconstruction.\n\nAs for the point in Section 3.2.1 about re-masking Zm to obtain more compressed representations, this is done because a single masked encoding may not provide comprehensive information for multi-target reconstruction. As the masks are random, using different masked compression representations for the reconstruction of different targets allows our model to learn more diverse modal knowledge content. Additional experiments have been added to demonstrate a significant decrease in experimental performance if we only use a single masked embedding. GCMA-one represents the result under just one mask, and shot represents the number of times the correct k-value was predicted.\n\n|         | GCMA-one | GCMA   |\n|---------|--------|--------|\n| Shot    | 8/10   | 10/10  |\n| NMI     | 57.39  | 59.16  |\n| ARI     | 54.10  | 55.41  |\n\nFinally, thank you again for taking the time to review my paper and I look forward to hearing from you again.\n\n[1] Jintang Li, et al.What\u2019s behind the mask: Understanding masked graph modeling for graph autoencoders.\n\n[2] Yucheng Shi, et al.Gigamae: Generalizable graph masked autoencoder via collaborative latent space reconstruction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057943644,
                "cdate": 1700057943644,
                "tmdate": 1700058129510,
                "mdate": 1700058129510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rTTKWkwbzo",
                "forum": "NvQ4kzcRSL",
                "replyto": "SOqXLjTe2e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer, I have submitted my reply a few days ago. Now the deadline for public comment is approaching, but I observe that you have not replied to my comment. I would be happy if you let me know if you still have some questions or replies. If so, I will reply promptly. Looking forward to your reply."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478358401,
                "cdate": 1700478358401,
                "tmdate": 1700478358401,
                "mdate": 1700478358401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OtOStDFcii",
                "forum": "NvQ4kzcRSL",
                "replyto": "QKHKnIQQmy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_sCEC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_sCEC"
                ],
                "content": {
                    "title": {
                        "value": "Why Equation 3 is related to mutual information?"
                    },
                    "comment": {
                        "value": "Thanks for the response. I am still confused about the relationship between Eq.3, Eq.4 and mutual information. I am familiar with MI estimators such as InfoNCE loss, MINE loss, while Eq.3 and Eq.4 belong to none of them."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583073526,
                "cdate": 1700583073526,
                "tmdate": 1700583073526,
                "mdate": 1700583073526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wfHuwZG6Q5",
            "forum": "NvQ4kzcRSL",
            "replyto": "NvQ4kzcRSL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents GCMA, a framework addressing challenges in graph clustering, specifically generalization and automatic cluster number determination. GCMA utilizes a fusion autoencoder based on graph masking for encoding, combined with an improved density-based clustering algorithm as a secondary decoder. This allows the model to capture more generalized knowledge by decoding mask embeddings. The work emphasizes the importance of determining the correct number of clusters in unsupervised learning and highlights issues with existing methods that overemphasize proximity in graph structures. The authors introduce the graph masking autoencoder to clustering tasks, offering enhanced generalization and interpretability, and through extensive experiments, demonstrate its superiority over existing methods"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  The research problem is significant, as node clustering is a fundamental topic in the graph learning domain. \n2. Overall, the paper is well-structured and well-motivated. \n3. Extensive baselines are compared in the experimental section."
                },
                "weaknesses": {
                    "value": "1.  The technique novelty is limited.  As they claimed in the introduction, the main contribution of this paper is the usage of graph masking autoencoder for clustering analysis. However, this technique has been well studied in self-supervised learning on graphs/ pretrain models on graphs.  It is not clear why the model can improve the generalization ability.  Another important technique,  density-based clustering algorithm, has also been well-studied in both graph and non-graph domains. \n\n2. Some important claims are not well verified. It claims that the model has better generalization ability. They conduct experiments on noisy/incomplete datasets to verify these claims.  This is confusing.  It is more like robustness instead of generalization.  It is not clear why the model has better interpretability.  \n\n3. Some experiments are confusing. For example,  \"but both NMI and ARI values are significantly decreased. This means that the interpretability and generalization performance of the results decreased.\"  How to infer interpretability and generalization from NMI and ARI  are not clear.  From table 3, GCMA is better than GCMA-A in COra. However, the visualization results in Figure 5 show that GCMA-A is better. There is no explanation or description of that. \n\n4. The presentation is messy, especially table 4 to table 8."
                },
                "questions": {
                    "value": "Please check the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3432/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783887060,
            "cdate": 1698783887060,
            "tmdate": 1699636295002,
            "mdate": 1699636295002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xTzV0fEFxK",
                "forum": "NvQ4kzcRSL",
                "replyto": "wfHuwZG6Q5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your evaluation of our work. We will now provide detailed responses to the raised questions.\n\nOur primary contribution lies in the application of graph mask autoencoders for clustering analysis. However, we have not identified any existing non-parametric methods utilizing graph mask autoencoders for clustering analysis. Most clustering models employing self-supervised learning on graphs utilize single-class-based autoencoders, as documented in references [1] and [2]. These references respectively employ GAT and GCN as encoders to construct autoencoder-based deep clustering models. Additionally, certain methods leverage fusion encoders, combining GNN networks and DNN networks, as illustrated in reference [3]. This alignment is also evident in our Table 3, where our chosen baselines are consistent with these methods.\n\nPresently, various masking strategies have evolved in graph mask techniques. The earliest approach, as demonstrated in MGAE, involves initially applying edge masks to the graph followed by decoding through cross-linking. Subsequent methods such as MaskGAE utilize single-edge masks and path masks. More recent models like GiGaMAE and GraphMAE introduce feature masks and heavy masks to enhance training data and achieve superior results. However, current research primarily focuses on investigating the encoding performance of graph mask techniques, with most studies concentrating on downstream tasks such as link prediction and node classification. Notably, there is a dearth of deep learning models specifically applied to clustering tasks. The available data on graph mask methods for deep clustering is limited to a small downstream experiment in GiGaMAE. Unfortunately, this model has not made specific improvements for clustering and exhibits suboptimal experimental performance. Its performance on the Cora dataset is as follows.\n\n|         | GiGaMAE | GCMA   |\n|---------|--------|--------|\n| Shot    | -   | 10/10  |\n| NMI     | 56.39  | 59.16  |\n| ARI     |50.02  | 55.41  |\n\nAs a deep graph clustering model, we innovatively integrate graph mask technology with clustering. Our end-to-end integration of graph mask techniques, utilizing single-edge, node, and heavy masks while incorporating AE encoding, involves setting up a multi-target decoder specifically tailored for clustering tasks. In terms of graph encoding techniques, our distinctiveness lies in the modifications made for clustering tasks. We have incorporated multi-target reconstruction and a second CFSFDP decoder, achieving superior clustering performance while keeping training costs economical. For a specialized deep clustering model, our approach marks the first application of graph mask technology, resulting in more compact graph embeddings.\n\nRegarding the density-based clustering algorithm CFSFDP, despite its earlier proposal, it faces three primary challenges. First, the original algorithm's truncation distance traditionally required manual determination based on empirical experience. Our enhancement addresses this by employing a Gaussian kernel, enabling automatic determination of the truncation distance. Second, the original algorithm necessitates the manual input of the cluster number (k) through observation of internal parameters, constituting a semi-automatic determination of k. In contrast, we directly derive cluster centers by computing parameters $\\gamma_i$ and point density. Finally, the original CFSFDP is a standalone algorithm, whereas we seamlessly integrate it into the end-to-end GCMA framework and design a unified loss function for optimization. These improvements position CFSFDP as the second decoder module in GCMA. Consequently, our approach, as a non-parametric method, eliminates the need for pre-inputting the cluster number k. The original method fails to yield results as robust as our improved algorithm. It is noteworthy that in the field of graph clustering, few researchers have explored the application of this method.\n\n[1] Chun Wang, et al.Attributed graph clustering: A deep attentional embedding approach.\n\n[2] Hong Yuan\uff0cet al.. Embedding graph auto-encoder for graph clustering\n\n[3] Deyu Bo, et al.. Structural deep clustering network."
                    },
                    "title": {
                        "value": "Weaknesses:1"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052949280,
                "cdate": 1700052949280,
                "tmdate": 1700060648606,
                "mdate": 1700060648606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HmldLMZX8S",
                "forum": "NvQ4kzcRSL",
                "replyto": "wfHuwZG6Q5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer, I have submitted my reply a few days ago. Now the deadline for public comment is approaching, but I observe that you have not replied to my comment. I would be happy if you let me know if you still have some questions or replies. If so, I will reply promptly. Looking forward to your reply."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478322866,
                "cdate": 1700478322866,
                "tmdate": 1700478322866,
                "mdate": 1700478322866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8bcYOFUtAD",
                "forum": "NvQ4kzcRSL",
                "replyto": "HmldLMZX8S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for the efforts.  I am still not fully convinced by the response.   Specifically, \n\n1.  As the authors have summarized the novelty lies in two aspects. \n1) \"application\" of graph mask autoencoders for clustering analysis.\n2) determination of k in deep end-to-end clustering methods.\nI still believe that the technique's novelty is limited.\n\n2.  The interpretation of model generalization capacity is misleading. Model generalization capacity refers to the ability of a machine learning model to perform well on new, previously unseen data, beyond the specific examples it was trained on. This concept is a crucial and well-established concept in machine learning. Using a model designed for graph-structured data on non-graph structured data and achieving good performance does not necessarily equate to having better model generalization capacity in the traditional sense. This situation is more about the versatility or adaptability of the model to different types of data structures rather than its generalization capacity. \n\n3. \"visually more separated clusters in GCMA-A represent misclustering of some points.\" It is unclear why this conclusion has been drawn."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534177678,
                "cdate": 1700534177678,
                "tmdate": 1700534177678,
                "mdate": 1700534177678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKEIVfF92Q",
                "forum": "NvQ4kzcRSL",
                "replyto": "Rm0qhpc2Lo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3432/Reviewer_BL5K"
                ],
                "content": {
                    "title": {
                        "value": "After author's reponse"
                    },
                    "comment": {
                        "value": "Thank the author for their response. \n\nHowever, I am not convinced by the author's response.\n\n1. ``Our innovations are distinctive in terms of precision and end-to-end integration''.  This is quite limited. \n2.  The usage of generalization is misleading.  I suggest the authors change to another term. The provided results cannot verify the generalization capacity of a model. \n3. For the third question, to me, from the visualization results, GCMA-A is better.  It seems that the authors don't agree with that and argue that  \"visually more separated clusters in GCMA-A represent misclustering of some points.\". So my question is how you draw this conclusion from the figure.  I don't find the authors' direct response to my question. It is just an intuitive analysis of the method but not an analysis of the experimental result. \n\nAfter reviewing the authors' responses, I keep my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3432/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579930952,
                "cdate": 1700579930952,
                "tmdate": 1700579930952,
                "mdate": 1700579930952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]