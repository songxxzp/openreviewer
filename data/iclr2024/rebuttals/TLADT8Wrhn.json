[
    {
        "title": "TiC-CLIP: Continual Training of CLIP Models"
    },
    {
        "review": {
            "id": "DyeQ1Ui6lQ",
            "forum": "TLADT8Wrhn",
            "replyto": "TLADT8Wrhn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_R93w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_R93w"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a study on training CLIP with time-evolving data in an incremental manner. The authors timestamp the training data spanning 2014-2022, treating each timestamp as a distinct incremental learning step. Their analysis delves into the backward and forward compatibility of CLIP as it undergoes training, leading to some findings.\n\nFirstly, the research demonstrates that employing simple small replay techniques effectively mitigate forgetfulness, an insight in the context of continual learning. Secondly, the study reveals an unsurprising yet noteworthy performance gap: CLIP underperforms on future, unseen data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1:\nThis paper addresses a critical and, to the best of my knowledge, an open issue in continual foundation model training. The challenge lies in the impracticality of re-training large foundation models like ChatGPT or CLIP, highlighting the necessity for continual learning solutions. Despite this urgency, there's a notable absence of organized datasets for the community to tackle this problem. While the CLEAR benchmark exists, it has not gained traction and lacks essential components like Web captions. This benchmark, being a captioned version of CLEAR that evolves over time, fills a gap, offering a valuable resource for researchers in the continual learning domain.\n\nS2:\nThe authors conduct insightful analyses using fundamental regularization and replay techniques. Unsurprisingly, the results show that preserving and replaying a portion of the data during incremental learning effectively mitigates forgetfulness. This empirical validation underscores the importance of such techniques in preserving model performance over time, providing a practical and valuable contribution to the field.\n\nS3:\nThe paper's central discussion on the fine-tuning cost in terms of compute MAC carries some value. This perspective equips researchers with a framework to explore low MAC solutions, emphasizing a nuanced approach rather than blindly minimizing the forget ratio."
                },
                "weaknesses": {
                    "value": "W1: No method, only benchmarking\n\nThe paper is commendable in its focus as a benchmark study, emphasizing data and existing baselines without introducing new methodologies. The utilization of well-known and straightforward baselines is executed competently. However, the paper could have significantly bolstered its strength by incorporating benchmarks from related works such as Continual-CLIP [1] and the methods outlined in \"Robust fine-tuning of zero-shot models\" [2]. Including these comparisons would have provided a more comprehensive evaluation, highlighting the benchmark's effectiveness in contrast to existing state-of-the-art approaches.\n\nW2: No fine-tuning, only pre-training\n\nAn additional area of improvement lies in the paper's scope, particularly concerning the fine-tuning stage. While the study adeptly focuses on the continual pre-training phase, it would have been more insightful and impactful to extend the analysis to the fine-tuning stage. Specifically, investigating how well the obtained checkpoint at a given time transfers to standard vision benchmarks, beyond simple retrieval tasks, would have provided a more nuanced understanding of the model's performance. This consideration is essential for gauging the real-world applicability and adaptability of the continual learning approach outlined in the paper.\n\n\n[1] Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models\n\n[2] Robust fine-tuning of zero-shot models"
                },
                "questions": {
                    "value": "No questions so far."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Reviewer_R93w"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698411130140,
            "cdate": 1698411130140,
            "tmdate": 1699636690062,
            "mdate": 1699636690062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SGRn2AIjvV",
                "forum": "TLADT8Wrhn",
                "replyto": "DyeQ1Ui6lQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R93w"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback on our work. We are glad to see the reviewer appreciating the importance of our continual learning benchmark and our experimental protocol. \n\n> **No method, only benchmarking. The paper is commendable in its focus as a benchmark study \u2026 However,... incorporating \u2026  related works such as Continual-CLIP [1] and  \u2026 \"Robust fine-tuning of zero-shot models\" [2]. Including these comparisons \u2026 highlighting \u2026  effectiveness in contrast state-of-the-art approaches.**\n\nWe thank the reviewer for their appreciation of our benchmark study. We would like to clarify that we indeed perform experiments with methods close to both the references the reviewer pointed out. In particular, we perform experiments with Patching (Ilharco et al., 2022) and LwF (Li & Hoiem, 2017; Ding et al., 2022). To clarify: (i) We employ Patching which is a follow up of Wise-FT that adapts Wise-FT to continual learning settings where we observe more than 1 task in sequence; (ii)  We adapt LwF for multimodal models which closely resembles the method in Zheng et al. 2023 (equation 4 performs LwF on both text and vision backbone). \n\n> **No fine-tuning, only pre-training.  An additional area of improvement lies in the paper's scope, particularly concerning the fine-tuning stage \u2026 Specifically, investigating how \u2026 checkpoint at a given time transfers to standard vision benchmarks, beyond simple retrieval tasks \u2026 This consideration is essential for gauging the real-world applicability and adaptability \u2026 .**\n\nWe perform zero-shot evaluations with 28 existing standard vision benchmarks to evaluate our CLIP models (results are in Table 2 and the standard 28 datasets are listed in Table 4). Our observations highlight that continual pretraining improves on standard tasks (see Figure 1-right and Table 2). Moreover, existing work highlights that zero-shot evaluation is strongly correlated with linear-probe performance (Gadre et al. 2023; Figures 16,17 in Section O). This finding, together with our zero shot evaluation results on 28 datasets, highlight the real-world applicability of continual pretraining for linear probing evaluations. \n\nIf we misunderstood your comment, we request the reviewer to provide clarification. We would be happy to run additional experiments if the reviewer has suggestions. We are also happy to run linear probing experiments if the reviewer suggests that these comparisons would strengthen the paper. \n\nWe would also be happy to answer any further questions you have. If you do not have any further questions, we hope that you might consider raising your score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993390326,
                "cdate": 1699993390326,
                "tmdate": 1699993402293,
                "mdate": 1699993402293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TjiRBlTTIy",
            "forum": "TLADT8Wrhn",
            "replyto": "TLADT8Wrhn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_RTur"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_RTur"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark for continual learning using large pretrained vision-language models, e.g., CLIP. The authors construct multiple datasets with timestamp information to evaluate the performance of existing methods on time-continuous data. The experimental results show that the rehearsal-based approach can reduce computation while achieving similar performance as the oracle algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors construct multiple datasets with time information based on existing datasets for continual learning settings. This is an non-trivial contribution for continue learning to evaluate the effectiveness of algorithms when facing natural distribution shifts.\n2. This paper is well-written and easy-to-follow."
                },
                "weaknesses": {
                    "value": "1. This benchmark lacks various types of continual learning methods [1]: elastic weights consolidation methods, progressive neural network methods, dynamic architecture methods, etc. Therefore, the experiments of this benchmark is relative weak and insufficient. \n2. This paper lacks some in-depth analysis of vision-language models solving continual learning. Vision-language models enable various novel model tuning paradigms, such as prompt tuning, vision prompt tuning, parameter-efficient tuning, etc. If these aspects are ignored when discussing the solving of continual learning with the CLIP model, then this benchmark is a bit over-claimed.\n\n**References:**\n\n[1] Da-Wei Zhou,\u00a0Fu-Yun Wang,\u00a0Han-Jia Ye,\u00a0De-Chuan Zhan: PyCIL:\u00a0a\u00a0Python\u00a0toolbox\u00a0for\u00a0classincremental\u00a0learning.\u00a0Sci. China Inf. Sci.\u00a066(9)\u00a0(2023)"
                },
                "questions": {
                    "value": "Please refer to questions in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Reviewer_RTur",
                        "ICLR.cc/2024/Conference/Submission6288/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728249817,
            "cdate": 1698728249817,
            "tmdate": 1700320428798,
            "mdate": 1700320428798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TL7u0Uy5ms",
                "forum": "TLADT8Wrhn",
                "replyto": "TjiRBlTTIy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RTur"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad to see that the reviewer finds our continual learning benchmarks as non-trivial contribution.   \n\n> **This benchmark lacks various types of continual learning methods [1]: elastic weights consolidation methods, progressive neural network methods, dynamic architecture methods, etc. Therefore, the experiments\u2026is relative weak and insufficient.**\n\nWhile we included references to these papers, we do not compare with other methods such as  elastic weights consolidation methods and  progressive neural network methods because these methods significantly increase the computation cost (refer to Appendix A.1). Several existing works argue that in constrained-compute scenarios, these methods perform poorly than other CL methods, e.g., LwF and Patching, that are computationally more efficient (Ilharco et al. 2022, Wallingford et al. 2023). We did compare with LwF which often performs empirically better than or similar to EWC (Zheng et al. 2023, Ni et al. 2023). Our findings highlight that this method increases the compute and doesn't improve over our baselines. \n\nMoreover, in our work, we show that simply by increasing the compute budget by 1.2$\\times$ for cumulative baseline, we can bridge the gap completely and achieve performance close to or better than the Oracle method. In contrast, methods like EWC significantly increase the compute budget, i.e., at least by a factor of 2 (since we need to compute gradients for two models) excluding the time to estimate the eigenvalues which can also incur significant costs (Ding et al. 2022). \nWe also thank the reviewer for sharing the reference on the toolbox. We have now included this in our work. \n\n> **This paper lacks  \u2026 vision-language models solving continual learning. Vision-language models enable various novel model tuning paradigms, such as prompt tuning, parameter-efficient tuning, \u2026 aspects are ignored \u2026  then this benchmark is a bit over-claimed**.\n\nWe would like to clarify and further highlight the distinct nature of our work in comparison to existing research. Unlike the majority of prior studies which primarily focus on task/domain/class incremental setups, our work diverges significantly. In these previous works, such as those by Ni et al. 2023, Ding et al. 2022, and Zheng et al. 2023, the emphasis has been on handling small-scale downstream task data, typically of a very small order than pretraining dataset size. These methods tend to produce models specialized in individual domains. However, in our work, we focus on pre-training (general knowledge acquisition) of the base model with continually evolving internet data. We create the first benchmark with 12.7 billion timestamped img-text pairs that enables systematic study of continual pretraining of CLIP models. This benchmark paves the way for a systematic exploration of the continual pretraining methods  in CLIP models. \n\nWhile fine-tuning a limited set of parameters may be an effective strategy for tailoring a model to a confined dataset from a specific domain, this approach is less viable for maintaining and updating \"general-purpose\" foundational models (e.g., CLIP) with dynamically evolving internet data. In our setup, the proportional increase in new data relative to the pre-existing dataset necessitates comprehensive model updates.\n\n*(New result)* As per your suggestion, we performed an additional experiment to assess the efficacy of updating selected model parameters versus full-scale fine-tuning with sequential training on the TiC-YFCC dataset. In particular, we compare LiT [1] where we perform full-finetuning on the first time step and then perform LiT for subsequent time steps on Tic-Datacomp (M). We observed that while updating a few parameters does avoid catastrophic forgetting on old timestamps, this happens at the cost of reduced performance on all tasks. For instance, the final Imagenet performance drops substantially from 24.0 (w full finetuning) to 7.3 (w LiT).  This highlights limitations of finetuning only a small number of parameters while trying to keep general purpose foundation models up to date. \n\nMoreover, we would like to emphasize that the main goal of the paper is NOT to introduce a new continual learning technique, rather introduce the first benchmark for continual pretraining where we pretrain foundation models as internet data evolves. For this setup, we present extensive results using standard continual learning approaches that are suitable for pre-training and highlight a simple method that enables continual pretrain of CLIP models without retraining models from scratch when new data arrives.\n\nWe would be happy to answer any further questions you have. If you do not have any further questions, we hope that you might consider raising your score.\n\n----\n[1] Zhai et al. 2021. LiT: Zero-Shot Transfer with Locked-image text Tuning    \n\n[2] Wallingford et al. 2023. Fluid: A Unified Evaluation Framework for Flexible Sequential Data"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993299597,
                "cdate": 1699993299597,
                "tmdate": 1699993299597,
                "mdate": 1699993299597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n2fKLesqnt",
                "forum": "TLADT8Wrhn",
                "replyto": "TL7u0Uy5ms",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_RTur"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_RTur"
                ],
                "content": {
                    "comment": {
                        "value": "I understand your difficulties with comparison methods, but these efforts are necessary as this is a paper releasing a benchmark. I decided to raise my score to show my appreciation for your work but I still can not fully support accepting this paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320410984,
                "cdate": 1700320410984,
                "tmdate": 1700320410984,
                "mdate": 1700320410984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gxl3kLyVzu",
                "forum": "TLADT8Wrhn",
                "replyto": "TjiRBlTTIy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Checking in"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their engagement in the rebuttal process. Since the discussion period is coming to an end today, we wanted to follow up on our response from yesterday. While we understand that it has been only several hours since our last response, we wanted to ask if you have any major additional questions or concerns left, and if not, we hope you might consider raising your score."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694121523,
                "cdate": 1700694121523,
                "tmdate": 1700694146076,
                "mdate": 1700694146076,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9QbPMldLYj",
            "forum": "TLADT8Wrhn",
            "replyto": "TLADT8Wrhn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces new pretraining datasets and dynamic downstream tasks for web-scale continual learning using CLIP on natural distribution shifts.\n\nDatasets introduced by ordering metadata by time:\n- **TiC-DataComp**: 12.7 billion image-text pairs from datacomp-xlarge ordered by first seen snapshot (monthly increments available from 2014-2022), with 7 timesteps from (2014-2016) and every subsequent year.\n- **TiC-YFCC**: 15M image-text pairs on subset of YFCC100M which have captions and timestamps. Data from 2008\u20132014, with 4 timesteps involving (2008-2011) and every subsequent year.\n- **TiC-RedCaps**: 12M image-text curated from Reddit in 2011-2020 ordered by creation timestamps of posts, with 4 timesteps (2011-2017) and three subsequent years.\n\nEvaluation\n\n**Static Tasks**: Zero-shot evaluation on a set of 28 downstream tasks similar to (Radford et al 2021)\n\n**Dynamic Tasks**: On a small subset of samples reserved for testing at each timestep,\n- (i) T2I retrieval for samples in a given timestep\n- (ii) Classification on a LAIONNet-like 1000 class dataset (filtered by sentence embedding). Similar in nature to a scaled up CLEAR dataset. \n\n**Metric**: Use the checkpoint after every timestep and perform classification across different timesteps to evaluate In-domain Acc, Backward Transfer and Forward Transfer.\n\n**Primary Findings**:\n1) Continual training saves Cumulative* ~3x the cost for TiC-Datacomp compared to training-from-scratch (Oracle**), with a similar savings  shown for ImageNet IID-incremental.\n\n2) Comparison between Sequential and Cumulative-all on TiC-Datacomp: This highlights sequential has\n- significant catastrophic forgetting (low backward transfer performance)\n- similar forward transfer performance \u2013 indicating catastrophic forgetting does not impact generalization to new distributions\n\nThis entails:\n- poor performance static benchmarks, having poor backward transfer performance leads to a performance hit on static benchmarks which have old data (supported by other evidence: Fig 9&11)\n- Consequently adding a memory buffer seems to help static tasks.\n3) Patching helps a lot more than traditional continual learning methods like LwF when training without past data, indicating different continual learning approaches might help in these settings.\n\n4) Different models work better for static and dynamic evaluation tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1) **Tackles an important problem** [Critical]: This work correctly highlights the need to shift focus in continual learning and introduces time-evolving benchmarks for evaluating continual pretraining which turns out is quite important. I really liked the dynamic retrieval and the classification task design. Retrieval captures performance shifts in time by new concepts and distribution shifts, whereas classification task ablates the performance gap caused due to new things (e.g. covid) by choosing the same 1000 classes\n\nS2) **Insightful analysis** [Critical]: This work was a delight to read. I liked this work motivating a case where optimizing models for old datasets might lead to continuously worse performance on current-day tasks. Similarly, summary (4) indicates picking continual SSL training strategies based on downstream tasks might lead to poor design choices (best models worse than intended), shown in experiments on best-pool filtering.\n\nS3) **High-quality work** [Critical]: The contributions seem fairly clear, experiments investigate interesting related claims well and experimentation is quite extensive."
                },
                "weaknesses": {
                    "value": "W1) **Sequential and cumulative models behave quite differently between TiC-YFCC15M and TiC-Datacomp** [Critical]\n- The paper nicely illustrates that YFCC15M has strong distribution shifts in Figure 15. \n- However, does TiC-DataComp have significant distribution shifts?\n    - The case for continually training CLIP primarily relies on Datacomp-like data having strong distribution shifts. \n    - I suspect the case there is far weaker than YFCC15M (I am worried it's too small to make this setting exciting). \n- Can the authors create a plot similar to Figure 15 for TiC-DataComp or some other mechanism to analyze distribution shift in TiC-Datacomp?\n\nW1.1) Further unexplained variations between Sequential and -All:\n- In TiC-YFCC15M and TiC-RedCaps, the performance of forward transfer is significantly affected, but that is not the case in TiC-Datacomp. \n- Similarly, the gap between backward transfer and ID retrieval is far smaller between them in TiC-Datacomp. Why is this the case?\n\nW1.2): Fig1 (mid) and Figure 10 present a different picture than Table 2.\n- Comparing backward transfer retrieval to ID performance for Cumulative exp (oracle) in Table 2\n    - Intuitively, ID performance should be higher (equal). \n- To what degree is the drop in forward transfer attributable to tasks getting harder vis-a-vis encountering new distributions.\n\nHypothesis: Past tasks are easier! Can the authors help shed some light on why the difference?\n\nW2) **Major results simply not present** [Critical]\n\nI cannot find any results for LAIONNet-like dynamic classification task. Did I miss something?\n\nOther missing ablations:\n- a) \u201cSame maximum LR works best across all runs when using cosine schedule\u201d \u2013 Didn't find supporting evidence.\n- b) \u201cWarm up helps training on data from the first time step, but hurts on subsequent time steps.\u201d \u2013 After correcting the minor shift issue, the table seems to robustly support the opposite conclusion. This seems surprising to me! I would have not expected warmup to help as the init is a very good one. Warmup mostly mitigates over-updating to high gradients when starting from a poor (random) init in my view.\n- c) \u201cGiven a fixed buffer size for each past step, we observe minimal to no difference between random subsampling and other strategies.\u201d \u2013 Didn't find supporting evidence.\n\nW3) **Serious Design Flaws in Continual Learning Setup** [Critical]\n\nW3.1) *4-7 timesteps are too few*\n- I am worried the major findings would significantly change when tested on 20/50 timesteps but with the same computational cost per timestep, as the streams become far more non-i.i.d in that case. \n- Why? Because of memory buffers become far smaller, that it becomes very hard in my experience to bridge the gap between CL methods and -all. \n- Similarly, I am concerned there will be a far larger gap between -all and Oracle. \n\nThis critically affects some of the core findings presented in this work.\n\nW3.2) *Is memory constrained?*\n- For TiC-RedCaps and TiC-YFCC15M, all and exp/equal only differ at last timestep (Until timestep 3, they all can store all the data). This grossly undermines the comparisons as even at the last timestep, they still store 2/3rd of the data. It is very surprising that the difference in training for timestep causes such a big gap! If this is caused by D/3 data missing at the last timestep, the findings given 20-50 will probably be quite different!   \n- This issue is mitigated in TiC-DataComp as at last timestep, equal would store D/3 samples per timestep (2D/6D) which is indeed much  smaller! (Minor note: Am I correct that buffer conserved here is 2.3x smaller, not the claimed 3.5x smaller? -- The replay+current is 3D/7D). \n\nW3.3) *Replay buffers not utilized effectively, computation implicitly constrains memory.*\n- I worry most of the decline in performance is due to random sampling, and hence wanted to see supporting evidence for W2.c). I make my case below:\n- *Why?* This seems to be a case of imbalance data across timesteps due to the buffer constraints. \n   - If trained without oversampling less-represented data, I am not surprised to see a bias against replay data (i.e. poor backward transfer). \n- *Why not data loss constraining replayable sample?* I focus on TiC-Datacomp because there is a significant gap in replay buffers here\n   - In TiC-Datacomp-medium, 35k iterations with 4096 samples barely allows ~1 pass through the 128M samples.\n   - Hence, there seems to be a far stronger implicit memory constraint, as even after storing kD samples at timestep k, the network can only pass through 1/k fraction of samples from each past timestep at timestep k (training with oversampling). \n   - Given a fixed buffer size, retaining equal samples of past data still stores 2/(k-1) samples which is far higher than the limitation introduced by computational constraints. It seems possible to train on mostly unique samples, mimicking -all.\n\nHence, the gap in performance between -equal and -all should have been minimal (which seems like a serious design issue-- alleviated by far smaller buffers in the 20-50 timestep setup discussed above). However, the fact that gap still exists seems to compound the issue, and I worry it is due to poor sampling.\n\nW4) **Little discussion of past work despite similar findings/parallels** [Important]\n\nAlthough I fully admit the dominant setups in CL are quite synthetic and need improvement, as correctly pointed out by the authors.\n- (Cai et al., 2021) also has analysis covering several aspects of interest here, e.g. YFCC distribution shift plot (see their Fig 2), learning rate modulation, comparing impacts of replay buffer, web-scale training and similar compute constraints as here but on 600K timesteps. This avoids promoting CL methods which work only on a short timescale and degrade quickly when scaled beyond 5-20 timesteps (i.e. my worry about the 4-7 timesteps). \n- Finding (1) on Imagenet IID seems out-of-place here, and has detailed precisely in several works, e.g. \u201cComputationally Budgeted Continual Learning: What Does Matter?\u201d, CVPR23. Other works like \u201cOne Pass ImageNet\u201d NeurIPS-W 2021 have analysis on Sequential/Cumulative variants as discussed here on Imagenet but these papers are not discussed.\n- Other works which have interesting investigations with computational limits (Bornschein et al., 2022) and (Jang et al., 2022) have nice analysis quite relevant to this work. E.g. hyperparamter optimization as a cost in the compute budget C (will be required when training in the unknown)"
                },
                "questions": {
                    "value": "Q1) What is the size of dynamic retrieval and LAIONNet test sets per chunk?\n\nMinor Comments: (Not considered for score)\n\nC1) Metric\n- The backward and forward transfer metric utilized here seems to be from (Lin et. al., 2021).\n- (Lopez-Paz & Ranzato, 2017) have a very different metric for backward and forward transfer, while (D\u00edaz-Rodr\u00edguez et al., 2018) do not introduce any modification to this metric.\n\nOverall, I strongly agree with the motivations and findings from analysis (filtering, CLIP vs OpenCLIP) presented. I worry the continual learning experiments made poor design choices which might seriously impact most of the major findings in this work (W1, W3). If addressed, I will be very happy to increase my score. \n\nNote: I tried to detail why the ask, primarily to minimize superfluous asks which I find annoying from reviewers on my submissions (Did not mean to sound pretentious).\n\n[Edit]: Increased score from 3 to 8 and soundness from 1 to 4, as additional analysis resolve my pointed concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h",
                        "ICLR.cc/2024/Conference/Submission6288/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698954720894,
            "cdate": 1698954720894,
            "tmdate": 1700676836813,
            "mdate": 1700676836813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "856gee4nDx",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fC5h (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their extremely detailed constructive feedback on our work.  It is encouraging to note that the reviewer recognizes the high quality of our paper, its relevance in addressing an important problem, and the depth of our analysis.\n\nWe provide detailed answers to your concerns below and we believe that by addressing these concerns we have significantly improved the manuscript.  \n\n\n> **W1) Sequential and cumulative models behave quite differently between TiC-YFCC15M and TiC-Datacomp**\n\nWe give a brief answer here and provide a details as we answer the sequence of questions below. The primary reasons are two (see Sec B.7 for detailed discussion):\n\n(i) the nature of the distribution shift observed in YFCC and its constructed dynamic retrieval task; we observe that models trained on Tic-YFCC suffer from relatively larger drops due to catastrophic forgetting than Tic-Datacomp (see updated plots for Sequential in Fig 6 in App B.1).\n\n(ii) compute used at each time step per data available at each time step. Overall YFCC is 2x smaller than Tic-Datacomp (M) but the compute we used in both TiC-YFCC and TiC-Datacomp setup is of similar order (in fact, it is slightly higher in TiC-YFCC). We re-ran the experiments for Tic-YFCC by reducing the compute. In the updated runs, we observe that the gap between ID performances of Sequential and Cumulative-All vanishes .  \n\nWe also emphasize that the goal of our study is to highlight the problem with the distribution shift for data collected from different data sources (e.g., Flickr, Common Crawl, Reddit). While we agree that the distribution shift severity is different in different data sources, we show that the problem exists for all different data sources. Similar to Figure 4 (right), we have now added heatmaps with other methods and datasets on dynamic retrieval evaluation tasks (Figure 6). For a fixed training time (a specific row), we clearly see performance drop on evaluation time steps after the training time step, highlight distribution shift in these benchmarks.\n\n- > **\u2026 does TiC-DataComp have significant distribution shift** \n\nYes, we observed that TiC-Datacomp has distribution shift problems as shown by the performance difference between forward transfer and backward transfer (in Table 2, Figure 4 (right) and Figure 6 (c,d)). We clearly observe that performance on dynamic tasks from future time steps is worse than current or previous time steps. Figure 3 presents some examples highlighting how distribution evolves over time in TiC-Datacomp. For example, with time images from novel concepts appear (e.g., COVID-19). \n\n- > **Can the authors create a plot similar to Figure 15 for TiC-DataComp or some other mechanism to analyze distribution shift in TiC-Datacomp?**\n\nWe have included a plot highlighting the nature of the shift in TiC-Datacomp in updated Figure 16. We observe similar trends for FID score on feature distribution for TIc-Datacomp. For TV distance, the distance increases initially and then saturates for later timesteps. \n \n- > **In TiC-YFCC15M and TiC-RedCaps, \u2026  forward transfer is significantly affected, but \u2026 not \u2026 in TiC-Datacomp. Similarly, gap between backward transfer and ID retrieval is far smaller between them in TiC-Datacomp. Why is this the case?** \n\nWe primarily attribute this to the data source differences between these constructed benchmarks. While we agree that the distribution shift severity is higher in TiC-YFCC and TiC-Redcaps, we show that this problem exists even for TiC-Datacomp. We have now included Figure 6 where we plot the performance evaluation matrix $\\mathcal{E}$ for all of these datasets. \n\n- > **Fig1 (mid) and Figure 10 present a different picture than Table 2. \u2026 In Table 2, intuitively, ID performance should be higher (equal)** \n\nYour observation about the discrepancy in the forward and backward transfer metrics is insightful. Thank you for highlighting this. Originally, our methodology involved averaging the entries in the upper and lower diagonal of our performance matrix $\\mathcal{E}. This approach inadvertently results in the backward transfer metric being influenced by later evaluation time steps resulting in backward transfer performance numbers slightly larger than ID performance.\n\nTo address this issue, we've revised our metric calculation method by deviating from prior work Lin et al 2021. Now, we normalize the data in each row by subtracting the ID performance as in D\u00edaz-Rodr\u00edguez et al., 2018) This adjustment ensures a more balanced and accurate representation across all training time steps. We have also adjusted the forward transfer metric similarly (Table in Appendix ). We have updated the draft with these change (App E). In the final version, we will include these results in addition to the existing results.  \n\nWe have also included additional plots that capture the performance matrix \\mathcal{E} as in Figure 4 (right) on all datasets in the appendix (Figure 6 in App B.1)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992691449,
                "cdate": 1699992691449,
                "tmdate": 1699992691449,
                "mdate": 1699992691449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2V0MwLPWek",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fC5h (Part 2)"
                    },
                    "comment": {
                        "value": "- > **To what degree is the drop in forward transfer attributable to tasks getting harder vis-a-vis encountering new distributions.**\n\nThis is a great question. To elucidate the differences, we compared performance of the Oracle method trained on data from all time steps on evaluation sets from different time steps, i.e., The last row in dynamic evaluation of Oracle method in Figure 4 (right) and in Figure 6 (App B). \n\nSince the models evaluated in the last row of Oracle method are trained on all training data, differences in performance of the Oracle model at the last training time step primarily stem from hardness of distributions at every time step. \n\nWe make the following observations. For TiC-YFCC and TiC-Redcaps, we observe an increasing trend in dynamic retrieval performance highlighting that the task becomes easier for future time steps. For TiC-Datacomp, the hardness of the task remains almost the same (slight increase) as the retrieval performance remains almost the same. These observations hint that while we encounter different distributions for all datasets, for TiC-Datacomp the hardness of the tasks in future timesteps remains almost the same, whereas, for TiC-YFCC and TiC-Redcaps, the tasks become easier as time progresses.  This observation hints that for TiC-YFCC and TiC-Redcaps, the quality of images and their corresponding captions improve over time. For example, the average quality of cameras on mobile phones have improved.\n\n> **W2) Major results simply not present**\n\nWe have now included all the missing tables in the appendix or references to already existing tables in the appendices.\n\n- >**Same maximum LR works best across all runs when using cosine schedule**\n\nTable 7 in the updated draft provides evidence to this finding (we have added the missing reference in the main paper). Earlier draft also included this table with a typo in one column \u2013 we have fixed that. \n\n- >**Warm up helps training on data from the first time step, but hurts on subsequent time steps \u2013 After correcting the minor shift issue, the table  \u2026 support the opposite conclusion.**\n\nSorry for the confusion here. We copied the column names incorrectly (you can check the mismatch between column names and the corresponding results between Table 6 and Table 2). We have now fixed this table in the updated draft which matches the stated finding in our paper. \n\n- > **Given a fixed buffer size for each past step, we observe minimal to no difference between random subsampling and other strategies. \u2013 Didn't find supporting evidence**\n\nWe only performed preliminary analysis for this. In our preliminary experiments, we explored the efficacy of subsampling old data based on the alignment between text and image content from previous time steps. Specifically, when training a model at time step t+1 , we used the model from the end of time step t to assess this alignment. We employed two distinct subsampling methods: \n1. Retaining half of the data with the lowest alignment scores, based on the premise that these data points might be more challenging to learn and require additional gradient steps.\n2. Retaining half of the data with the highest alignment scores, under the assumption that these represent higher quality data, as indicated by the stronger alignment between text and image pairs.\n\nWe applied these methods to the TiC-YFCC dataset and evaluated their performance against a baseline of random sampling. The outcomes revealed minimal differences: less than 0.2% variation in Imagenet performance and under 0.5% in dynamic retrieval performance across different time steps. Given that these minor improvements came with a significant computational cost\u2014requiring a full forward pass to compute alignment post each training epoch\u2014they exceeded our compute budget constraints. As a result, we opted for random sampling in our research. These findings have been included in the appendix for further reference (App B.4). We leave investigation on improved subsampling techniques for future work. \n\n- > **I cannot find any results for LAIONNet-like dynamic classification task.**\n\nWe apologize for missing the table for this result. We have included this in the updated draft and have added a reference to that in the main paper.  The trends described in Sec 3.4 hold true. We have added Table 8 in App B.6 and Figure 15 in C.6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992825620,
                "cdate": 1699992825620,
                "tmdate": 1699993172684,
                "mdate": 1699993172684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kPSYJuh58P",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fC5h (Part 3)"
                    },
                    "comment": {
                        "value": "> **W3) Serious Design Flaws in Continual Learning Setup**\n- > **4-7 timesteps are too few \u2026 major findings would significantly change when tested on 20/50 timesteps**\n\nWe acknowledge the reviewer's perspective that extending our analysis to 20/50 timesteps could yield different findings. However, we contend that our chosen 4-7 timestep range (one model a year for up to 7 years) is reflective of certain real-world scenarios, particularly in fields where data is subject to rigorous quality filterings and models are subject to rigorous evaluations between deployments. Such models are expected to adapt to slow changes in the data distribution (year to year) and tolerate faster but small changes (month to month). Examples of such settings include image classification for autonomous driving, where updates are thoroughly tested in controlled environments to ensure safety and compliance with regulations. Even models, like ChatGPT undergo updates at a relatively smaller frequencies (After its initial release about an year ago, GPT-4 was only recently updated with new knowledge).   \n\nMoreover, **our work is  the first benchmark for continual training at the scale of 12.7 B image-text pairs** (e.g., CLOC [Cai et al. 2021] has only 39M samples, see others in Table 4). This initial exploration into the realm of 4-7 timesteps  multi-year web-scale data can pave the way for future work to explore the more challenging settings with 50-60 time steps. We have prioritized the large-scale aspect of our benchmark and tapped the largest sources of publicly available image-text data. Our benchmarks effectively span 14 years from 2008-2022. As such, creating a benchmark with more than 14 years with data prior to 2008 might be infeasible as of now as the scale and quality of earlier data is much less.\n\nAlternatively, our benchmark itself provides a testbed to experiment with finer granularity of months (Sec 2.3). Our dataset release will include time stamped data at the granularity of months. We believe that it would be an interesting direction for future work to explore continual learning at finer granularities in our TiC-DataComp benchmark where we have 70-80 time steps. In such regimes, we imagine that one can explore techniques that alternate between full model updates (which can be done less frequently) and finetuning a few parameters to adapt to more rapid distribution shifts (which can be done more frequently). \n\n- >**Is memory constrained? For TiC-RedCaps and TiC-YFCC15M, all and exp/equal only differ at last timestep \u2026 This issue is mitigated in TiC-DataComp as at last timestep \u2026  note: Am I correct that buffer conserved here is 2.3x smaller, not the claimed 3.5x smaller? \u2026** \n\nWe apologize for confusion here. Table 1 in the draft depicted the correct sizes and there was a minor typo in the text for Cumulative method description in Sec 3. We have updated the draft and reflected the change. In our experiments, we only replay D size of old data, and thus even for TiC-RedCaps and TiC-YFCC15M, we have differences in the last two steps and for TiC-DataComp, we have differences in the last five steps. This also addresses the confusion between 3.5x reduction in memory buffer (3.5x is the correct reduction).\n\n- > **Replay buffers not utilized effectively, computation implicitly constrains memory. I worry most of the decline in performance is due to random sampling, and hence wanted to see supporting evidence for W2.c).** \n\nWe have added evidence to the W2c weakness above. We also thank the reviewer for their detailed suggestion on methods to try and for their astute feedback.  \n\nThe main goal of our work is to construct the continual learning benchmark and evaluate simple but important baselines. We believe that the method suggested by the reviewer is a very interesting direction for future work on how to improve sampling datasets when selecting the replay buffer for continual learning at scale."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992924617,
                "cdate": 1699992924617,
                "tmdate": 1699993190542,
                "mdate": 1699993190542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xMWy4k1zNO",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fC5h (Part 4)"
                    },
                    "comment": {
                        "value": "> **W4) Little discussion of past work despite similar findings/parallels**\n\nWe thank the reviewer for providing references to these papers. \n\n- > **(Cai et al., 2021) \u2026l aspects of interest here, learning rate modulation, comparing impacts of replay buffer, web-scale training \u2026 but on 600K timesteps. ... CL methods work only on a short scale and degrade beyond 5-20 timesteps.** \n \nYes, we did reference the work of Cai et al. 2021 in our draft and have now expanded (in App A.2) our discussion on it in the revised draft. We agree that this work provides interesting discussion/analysis for continual learning at a large number of steps. However, our study differs from Cai et al. 2021, in several crucial respects: \n\n(i) Training Methodology: We employ noisy supervision using contrastive loss between image-text pairs, as opposed to the cross-entropy loss used by Cai et al. 2021. \n\n(ii) Scale of Experiments: Our experiments on the TiC-DataComp dataset are orders of magnitude larger, scaling up by 200x.\nThese differences introduce unique challenges. The use of contrastive loss (i) necessitates a tailored approach to designing our evaluation studies. The significantly larger scale of our experiments (ii) poses challenges in collecting timestamped data and understanding if and how distribution shifts impact learning at this scale. As mentioned before, we believe that extending the study on our TiC-DataComp benchmark (with 12.7B examples) to a finer granularity as in Cai et al. 2021 is an interesting direction for future work. \n\n- > **Finding (1) on Imagenet IID \u2026  in several works, e.g. \u201cComputationally Budgeted Continual Learning: What Does Matter?\u201d \u2026 Other works like \u201cOne Pass ImageNet\u201d \u2026 have analysis on Sequential/Cumulative variants on Imagenet but  \u2026 not discussed.**\n\nWe are grateful to the reviewer for suggesting these additional references, which we have now incorporated the suggested papers into our discussion (in App D.1). If the reviewer suggests, we are open to moving the Imagenet results fully to the appendix. \n\nOur Imagenet experiments were primarily inspired by the \"loss of plasticity\" phenomenon described in Ash and Adams 2020. Their study demonstrates that models sequentially trained on two splits of CIFAR-10 data (initially on 50%, followed by 100% of data) exhibit poorer generalization compared to models trained from scratch on the entire dataset. Since we do not observe this behavior for continual training of CLIP, we investigated the existence of such behaviors on up to 8 splits of Imagenet. Our findings reveal that the simple cumulative baseline remains competitively close to the Oracle model (that benefits from using the full compute budget on the entire pooled training data from the beginning).\n\nAlthough the referenced works and ours both underscore the effectiveness of continual training on synthetic continual learning setups derived from ImageNet, the specific settings in these studies vary significantly from ours, limiting direct comparisons. A key distinction of our work is the comparison of our final models with an Oracle model, a step not undertaken in the referenced papers as it was not their primary objective.\n\n- >**Other works which have interesting investigations with computational limits (Bornschein et al., 2022) and (Jang et al., 2022) have nice analysis quite relevant to this work. E.g. hyperparamter optimization as a cost in the compute budget C (will be required when training in the unknown)**\n\nYes, we indeed cite these references in our work and now we have expanded the discussion with these papers. Our methodology maintained consistent base hyperparameters across all timesteps, with the exception of specific ablations on learning rate warm-up and maximum learning rate at a medium scale. Consequently, our experimental framework does not account for the costs associated with hyperparameter selection.\n\n> **What is the size of dynamic retrieval and LAIONNet test sets per chunk?** \n\nWe have now added sizes of our evaluation tasks in App C.3. \n\n> **The backward and forward transfer metric utilized \u2026 from (Lin et. al., 2021). (Lopez-Paz & Ranzato, 2017) have a \u2026 different metric for backward and forward transfer, while (D\u00edaz-Rodr\u00edguez et al., 2018) do not \u2026 modification to this metric.** \n\nThanks for highlighting the differences; we have clarified this in the updated draft (in App E). The main commonality in all of our works is the same performance matrix $\\mathcal{E}$. While D\u00edaz-Rodr\u00edguez et al., 2018 used the same forward transfer metric as ours, the backward transfer metric in their work and metrics in Lopez-Paz & Ranzato, 2017 performed centering by subtracting the ID performance. Note that, as discussed above, we have now updated the metrics to include the metrics used in D\u00edaz-Rodr\u00edguez et al., 2018 in our work. \n\n\nWe would be happy to answer any further questions you have. If you do not have any further questions, we hope that you might consider raising your score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993085570,
                "cdate": 1699993085570,
                "tmdate": 1699993202411,
                "mdate": 1699993202411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w6eCN8imz8",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal [Part1]"
                    },
                    "comment": {
                        "value": "W1) Sequential and cumulative models behave quite differently between TiC-YFCC15M and TiC-Datacomp\n\nAuthors main visualization support is:\na) We have now included Figure 6 where we plot the performance evaluation matrix for all of these datasets. \n\nwith conclusions:\n(i) Tic-YFCC suffer from relatively larger drops due to catastrophic forgetting than Tic-Datacomp\n(ii) On re-running the experiments for Tic-YFCC with reduced compute, the gap between ID performances of Sequential and Cumulative-All vanishes.\n\nThe authors additionally provided:\n> (i) We have included a plot highlighting the nature of the shift in TiC-Datacomp in updated Figure 16. We observe similar trends for FID score on feature distribution for TIc-Datacomp. For TV distance, the distance increases initially and then saturates for later timesteps. \n\nOn this point, the updated TV-distance plots did not have increasing drift. To ensure that the drift is not simply between the first timestep and the rest, can the authors extend the Datacomp plot the same graph across some years in the middle and the end timestep additionally?\n\nI am very happy/convinced by the rebuttal on this point and tentatively consider this fully resolved. I would be very happy if I could see the Figure 16 plots across 2-3 timesteps before rebuttal deadline to complete this point."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401047696,
                "cdate": 1700401047696,
                "tmdate": 1700414254968,
                "mdate": 1700414254968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZsSiNuhWKq",
                "forum": "TLADT8Wrhn",
                "replyto": "2V0MwLPWek",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal [Part2]"
                    },
                    "comment": {
                        "value": "> Tasks getting harder over time\n\nFigure 6 (App B) of Oracle comparing the lower diagonal values can indeed precisely tell this apart. Based on this experiment, authors conclude-\n\nFor TiC-YFCC and TiC-Redcaps, the task becomes easier for future time steps. For TiC-Datacomp, the hardness of the task remains almost the same (slight increase). Sounds convincing, surprising and thorough. I can't find any holes here!\n\nW2) Apart from the results for W2 c) I found the rest of the results, they seemed satisfactory. I shall highlight critical issues in the experiment on W2 c) in the next part. \n\nOn LAION-Net results:\n- The gaps in dynamic LAION-Net task seem much smaller than the corresponding ones from the broader retrieval task. \n- Is it a correct to say that a large part of the gap comes from new concepts rather than a distribution shift on existing concepts? (Assumes the retrieval task was the same, the only difference being conditioning on fixing concepts or not. Would appreciate some sort of discussion of this in the work)\n\nMinor note: Would like to have the bestpool results \"For TIC-DataComp (XL), we include results with Bestpool filtering.\" together in this table or alternatively with and without bestpool filtering in Table 5. It was painful to repeatedly scrolling back and forth to compare numbers."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700402618590,
                "cdate": 1700402618590,
                "tmdate": 1700402618590,
                "mdate": 1700402618590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RhlDdOkerJ",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal [Part 3]"
                    },
                    "comment": {
                        "value": "**W3.1) [Too few timesteps]** I am not convinced on this and would push back. Setting this norm would be important for a benchmark paper, as a lot of methods in continual learning fail to mitigate forgetting beyond a few timesteps. Similarly, I am concerned there will be a far larger gap between CL methods and -all, and -all and Oracle. However, with the correction in buffer being D and not 2D this is no longer a critical concern considered for score of the work, just an drawback of this benchmark. If this is specifically acknowledged it would be sufficient for me.\n\n\n**W3.2) [Is memory constrained?]** Resolved fully, thanks!\n\n**[W3.3) Replay buffers not utilized effectively]** This has not been resolved at all, and is a critical drawback. The specific issue pointed out is data-imbalance and using unique samples. The sampling strategy this implies testing with is to balance the number of samples selected between past/current data and prefer deleting the samples which were used for replay over samples which were not when buffer size is constrained. I would strongly recommend testing this correction and presenting results as this is critical for the difference between cumulative-eq and cumulative-all for Datacomp-medium, and in the final version for all datasets.\n\nThe authors instead explored unrelated sampling strategy which had little to do with the question and  requires additional forward passes (similar strategies have been described to not work in past literature precisely due to being too expensive). \n\nW3.2 and W3.3) were the primary concerns I had in this work. W3.2 and W1 were addressed, and I shall raise the score to 6 to reflect this. If W3.3 is sufficiently addressed on the smallest Datacomp dataset, I would have no further concerns and can commit to raising my score to 8."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700404918428,
                "cdate": 1700404918428,
                "tmdate": 1700413617455,
                "mdate": 1700413617455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V38CbZf8GF",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal [Part4]"
                    },
                    "comment": {
                        "value": "All replies except the ImageNet-IID claim seem satisfactory to me (the (D\u00edaz-Rodr\u00edguez et al., 2018) part particularly was quite neat, thanks for this!). The ImageNet-IID claims seems to misrepresent OPIN and CB, as I explain below.\n\n> Our findings reveal that the simple cumulative baseline remains competitively close to the Oracle model (that benefits from using the full compute budget on the entire pooled training data from the beginning)... the specific settings in these studies vary significantly from ours, limiting direct comparisons. A key distinction of our work is the comparison of our final models with an Oracle model, a step not undertaken in the referenced papers as it was not their primary objective.\n\nBoth works compare with an Oracle model! \n\n[OPIN] As per my understanding, the main comparison in OPIN is between Oracle and continually trained models eq to sequential and cumulative-prioritized replay, it seems misleading to claim the contrary. Precisely, Table 1 compares performance at the last timestep between -- Multi-epoch (90 epochs) is Oracle as it does full passes over the data multiple times, One-Pass (Naive) is sequential and One-Pass (Prioritized Replay) is a variant equivalent to Cumulative-prioritized replay (similar variety to equal/exp). Table 2 extends to Oracles with different compute budgets.\n\n[CB] I am less confident about this, but to the best of my knowledge the equivalence should be as follows -- ERM-Naive is Oracle, Naive is the same as cumulative-all. Minor note: This was also the citation for the issues in expensive sampling strategies point I made. \n\nOverall, both works make the same conclusion made here as far as I see on Imagenet-IID, but have far more extensive analysis (the conclusion is not a new finding). I don't see what does the ImageNet experiment say more than preliminarily confirming the conclusion presented in OPIN (and CB for the increased split case). Both works are more recent than (Ash and Adams, 2020) and specifically have ImageNet-IID experiments same as in this work."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406982485,
                "cdate": 1700406982485,
                "tmdate": 1700434278238,
                "mdate": 1700434278238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RjlQtph444",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "Hi!\n\nSorry, maybe that point was too harshly put. The sampling structure that I was thinking about: \n\n**[Memory storage allocation, *not a*)]**: Say in -equal strategy, we would storing D/2 examples from each timestep, and the next iteration D/3 from each past timestep -- here the degree of freedom is how can we choose D/3 out of D/2 samples. My suggestion is prioritize keeping the samples in D/3 which were not/less used for training previously, and break ties by choosing randomly. \n\nTo clarify, samples in the D/3 will be subsampled from D/2, not being disjoint. I presume the standard of one loses access to all  samples once discarded, yes!\n\nNow given the buffer, how to choose samples for training-\n\n**[Sample selection from buffer, use b)]** Given that we have fixed the buffer, we know that we have D samples of current timestep and say D/3 samples of each of the past timesteps, we would over-sample D/3 samples compared to current samples, yes. This is avoiding the data imbalance problem. I am relying that *simple augmentations of the past samples like randomflip* while oversampling would mitigate overfitting to a greater extent. \n\nSo, not (a) but the sampling I mentioned above which is slightly different from random and (b) I wanted to note augmentation as it might help? \n\nOverall, just wanted to give a strategy which I think mitigates this issue cheaply but I expect to be far better than random sampling, trying to maximize performance. If the authors find clever tweaks (I only spent a few minutes atbest thinking about this at the best), please feel free to make them. I would highlight choosing the oversampling factor was important-- I've found that oversampling by a square root of the proportion worked the best (i.e. if ratio is D/6 old samples in a given timestep to D samples of latest timestep, the sampling is \u221a6:1 between them) but maybe authors find something more principled/better working."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431442921,
                "cdate": 1700431442921,
                "tmdate": 1700438249358,
                "mdate": 1700438249358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJi5dD33Wj",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fC5h (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their elaborate response and for clarifying the suggestion made for W3.3 about using the count and over-sampling while replaying old data. We ran additional experiments and discuss our results below: \n \n\n> **W1) Sequential and cumulative models behave quite differently \u2026  very happy/convinced by the rebuttal on this point  \u2026 would be very happy if I could see the Figure 16 plots across 2-3 timesteps before rebuttal deadline to complete this point.**\n\n\nWe thank the reviewer for appreciating our responses on W1. As per your suggestion, we have updated Fig 17 by including heatmaps for TV distances between various reference timesteps, not just 2016. In this revised figure, we can see that as we move away from the reference timestep, the TV distance increases for all reference time steps (represented by different columns). Additionally, we noticed that the TV distance magnitude decreases relatively when later time steps are used as references. We believe that this is due to the inclusion of data from previous years within the 2016 dataset.\n \n\n> **Tasks getting harder over time \u2026 Would like to have the bestpool results \"For TIC-DataComp (XL), we include results with Bestpool filtering.\" together in this table or alternatively with and without bestpool filtering in Table 5.** \n\n\nWe have updated Table 5 with your suggestion. For the final draft, we will consider integrating this result with Table 2. \n\n\n> **[W3.3) Replay buffers not utilized effectively]. The sampling structure that I was thinking about \u2026 [Memory storage allocation, not a)] \u2026 [Sample selection from buffer, use b)] \u2026 Overall, just wanted to give a strategy which I think mitigates this issue cheaply but I expect to be far better than random sampling, trying to maximize performance.**\n\n\nWe have now included results with the suggested method in Table 18 (in Appendix E.2). As the reviewer suggested, we have made the following two modifications: (i) Instead of random sampling, we implemented the count based subsampling that prioritizes not/less used examples; (ii) We oversampled data from old timesteps with ratio inversely proportional to the ratio of examples, i.e., if the old data is of size D/2 and the new data is of size D, then we upsample old data with 2:1 ratio. \n\nHowever, we observe that this method doesn\u2019t improve performance over Cumulative-equal (and in fact hurts the performance). We hypothesize that this can be due to the decreasing marginal utility of labeled data as highlighted in existing work [1]. Their work argues that due to information overlap among data, as the number of samples increases, the marginal benefit a model can extract from the data diminishes. As a result, Cui et al. [1] proposed using of \u201ceffective sample size\u201d instead of actual number of samples to obtain the ratio used to perform re-sampling or re-weighting. We include the expression of effective sample size in more detail in Appendix E.2. \n\nIn our settings (even at small scales), our datasets contain an order of 100k img-text pair even after subsampling data from old timestep. For example, with -equal baseline, when training on the last timestep (i.e., 2022), the smallest dataset (i.e., 2016) is of approximately 400k samples.\nPlugging in the expression for effective sample size from Cui et al. [1], we observe that for all $\\beta \\in (0, 0.99999)$, the ratio of effective sample sizes for different timesteps remains close to 1. This may highlight why our naive over-sampling strategy doesn\u2019t improve over no-oversampling. \n\nDue to a shortage of time and compute, we didn\u2019t perform ablations with $\\beta$'s arbitrarily close to 1, but if the reviewer suggests we would be happy to run those experiments for the final version since we have already implemented the oversampling code. We will explore the benefits of count-based subsampling instead of random sampling separately from oversampling in the final version.\n\n---- \n\n\n[1] Cui et al. Class-Balanced Loss Based on Effective Number of Samples. CVPR 2019."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641324618,
                "cdate": 1700641324618,
                "tmdate": 1700641436458,
                "mdate": 1700641436458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "grAhr85Mnc",
                "forum": "TLADT8Wrhn",
                "replyto": "9QbPMldLYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_fC5h"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "[W3.3] Interesting, oversampling does not help. Performance degrades due to overfitting. This resolves all my main concerns, I raise my score to 8.\n\n[W Part 2] Yep, resolved! Thanks. I agree that many continual learning setups have slightly different assumptions from each other, preventing direct comparisons."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676748709,
                "cdate": 1700676748709,
                "tmdate": 1700677101724,
                "mdate": 1700677101724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nNL935WNCx",
            "forum": "TLADT8Wrhn",
            "replyto": "TLADT8Wrhn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_wUu1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6288/Reviewer_wUu1"
            ],
            "content": {
                "summary": {
                    "value": "The paper creates the first set of webscale Time-Continual (TiC) benchmarks for training vision-language models: TIC-DataComp, TIC-YFCC, and TIC-RedCaps with over 12.7B timestamped imagetext pairs spanning 9 years (2014\u20132022). And they use their benchmarks to curate various dynamic valuations to measure temporal robustness of existing models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper collects a large amount of dynamic data to study how to effectively train CLIP models continuously, ensuring the comprehensiveness of the research.\n\nIn order to ensure fairness in the evaluation, the paper has established a corresponding experimental protocol."
                },
                "weaknesses": {
                    "value": "The dataset being solely focused on training CLIP may be somewhat limited. Can the article consider incorporating more vision-language models?\n\nThe YFCC100M dataset might be somewhat outdated in terms of the years it covers. It may be more representative to explore newer datasets for the research."
                },
                "questions": {
                    "value": "The fundamental issue of continual learning is catastrophic forgetting. If we fine-tune a small number of parameters (e.g., prompt tuning) in the CLIP model, is catastrophic forgetting a major concern? On the other hand, if we fine-tune a large number of parameters, resource limitations may become a factor. Therefore, from this perspective, is it necessary to construct such benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6288/Reviewer_wUu1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699362259695,
            "cdate": 1699362259695,
            "tmdate": 1699636689689,
            "mdate": 1699636689689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qDRA9i1VYY",
                "forum": "TLADT8Wrhn",
                "replyto": "nNL935WNCx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wUu1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for the positive assessment of our work. \n\n> **The dataset being solely focused on training CLIP may be somewhat limited. Can the article consider incorporating more vision-language models?**\n\nWhile our paper primarily focuses on experiments with training CLIP models, the constructed benchmark with timestamped image-text pairs can be used to train other Vision Language Models (VLMs). Moreover, since, CLIP vision encoder and text encoder are the basic blocks for other VLMs, e.g., Stable Diffusion models [1] and LLava models [2], we believe that improving CLIP models have direct implications for other VLMs. We would be happy to run additional experiments if the reviewer has any specific suggestions. \n\n> **The YFCC100M dataset might be somewhat outdated in terms of the years it covers. It may be more representative to explore newer datasets for the research.**\n\nWe agree with the reviewer that YFCC100M contains data from timestamps before 2014. However, we want to **highlight that we also collected Tic-Datacomp which contains 12.7 B data points till timestamp 2022**, overall giving us data from 2004\u20132022. \n\n\n> **The fundamental issue \u2026 catastrophic forgetting \u2026 fine-tune a small number of parameters (e.g., prompt tuning) \u2026  catastrophic forgetting a major concern? \u2026  if we fine-tune a large number of parameters, resource limitations may become a factor \u2026 is it necessary to construct such benchmarks?**\n\nIn the majority of CL benchmarks, catastrophic forgetting can be attributed to significant and abrupt changes between consecutive timesteps, often across the data distribution, the targets, and the task. For example, the data and targets in perm-MNIST or split-MNIST have little relation to each other from one task to another. Our setup of continual pretraining for foundation models differs from these existing CL benchmarks in two crucial ways: (i)  we observe little catastrophic forgetting; and (ii)  by observing new data we not only benefit on tasks from current time step but also improve performance on tasks from old time steps (these observations are supported with Fig 6 and Fig 4 (right)). \n\nAt a high level, by continually training on more data, we hope to improve \u201cgeneral-purpose\u201d abilities of foundation models (e.g., performance on Imagenet). Thus while fine-tuning a limited set of parameters may be an effective strategy for tailoring a model to a confined dataset from a specific domain, this approach is less viable for maintaining and updating general-purpose foundational models  with the dynamically evolving internet data. Given the proportional increase in new data relative to the pre-existing dataset, it necessitates comprehensive model updates. \n\n*(New result)* As per your suggestion, we performed an additional experiment to assess the efficacy of updating selected model parameters versus full-scale fine-tuning with sequential training on the TiC-YFCC dataset. In particular, we compare LiT [1] where we perform full-finetuning on the first time step and then perform LiT for subsequent time steps on Tic-Datacomp (M). We observed that while updating a few parameters does avoid catastrophic forgetting on old timestamps, this happens at the cost of reduced performance on all tasks. For instance, the final Imagenet performance drops substantially from 24.0 (w full finetuning) to 7.3 (w LiT).  This highlights limitations of finetuning only a small number of parameters while trying to keep general purpose foundation models up to date. \n\nWe would be happy to answer any further questions you have. If you do not have any further questions, we hope that you might consider raising your score.\n\n\n[1] Zhai et al. 2021. LiT: Zero-Shot Transfer with Locked-image text Tuning"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992431803,
                "cdate": 1699992431803,
                "tmdate": 1699992447109,
                "mdate": 1699992447109,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PcKxxt6QaB",
                "forum": "TLADT8Wrhn",
                "replyto": "qDRA9i1VYY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_wUu1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6288/Reviewer_wUu1"
                ],
                "content": {
                    "comment": {
                        "value": "Your answer partially solves my questions, but there are still some other weaknesses, such as the lack of sufficient experiments. Therefore, I will keep the score unchanged."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360910039,
                "cdate": 1700360910039,
                "tmdate": 1700360910039,
                "mdate": 1700360910039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]