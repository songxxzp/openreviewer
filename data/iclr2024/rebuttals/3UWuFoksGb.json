[
    {
        "title": "Learning Planning Abstractions from Language"
    },
    {
        "review": {
            "id": "HRFypKT79y",
            "forum": "3UWuFoksGb",
            "replyto": "3UWuFoksGb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a framework that learns state and action abstractions for planning. It does this by leveraging demonstrations with corresponding language annotations. These demonstrations are used to discover actions, which in turn is used to generate state abstractions. Finally, low-level policies are also learned corresponding to the high-level actions. The evaluation is done on two domains and the results show generalization wrt environments and objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written. Except for the algorithm description, other parts like motivation, problem formulation, etc., are explained nicely.\n2. The approach seems to be novel."
                },
                "weaknesses": {
                    "value": "1. Other related approaches: \n* There are related approaches that use language to guide the abstraction process. E.g., Peng et al., the difference is that human input. Here, this paper gets it in the form of language-annotated task descriptions.\n* Approaches like LIV (with PointCLIP instead of CLIP) can learn a latent representation, which can be used for planning. \n\nPeng et al., Learning with Language-Guided State Abstractions.\n\nLIV: Ma et al., LIV: Language-Image Representations and Rewards for Robotic Control.\n\nCLIP: Radford et al., Learning Transferable Visual Models From Natural Language Supervision.\n\nPointCLIP:  Zhang et al., PointCLIP: Point Cloud Understanding by CLIP.\n\n\n2. Reproducibility:\n* I am not sure how reproducible the work is. There are a large number of details that are swept under the rug. And without an algorithm, it gets difficult to follow the paper. The supplementary material is also not submitted. \n* The inputs are not clear. \n\n3. Experimental Evaluation:\n* I would suggest performing experiments for the accuracy of the feasibility function. \n* The experiments from the grid-like BabyAI setup are not convincing of generalization. The paper claims to withhold \"red key\" in training, but they can learn the model agnostic to such properties. So, this is more of a verification that their approach works. But as we can see, the accuracy for novel concept combinations is only 91\\%.\n\nMinor points:\n* Incorrect citation: I do not believe Silver et al. learn (invent) new predicates as stated in the last two lines of page 1."
                },
                "questions": {
                    "value": "1. Who provides the examples for prompting in Fig. 3 left?\n2. The training is performed on how many tasks? Was the environment structure the same for all of them? Or was it changed in between tasks? If it was changed, was it ensured that the test environment configuration was not present in the training set? \n3. What is the reason for not achieving 100\\% accuracy for novel concept combinations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY",
                        "ICLR.cc/2024/Conference/Submission6513/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798802653,
            "cdate": 1698798802653,
            "tmdate": 1700698200753,
            "mdate": 1700698200753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s3C35TkrVB",
                "forum": "3UWuFoksGb",
                "replyto": "HRFypKT79y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "**R4.1:** Related approaches for deriving state abstraction.\n\nThank you for the suggestion! We agree that these methods can potentially help with state abstraction. However, we want to emphasize the key differences between these related approaches and our method, and therefore highlight our novelties.  \n\nThe vision-language representation LIV is trained on the objective such that (1) neighbouring frames in a video are close by in the embedding space, (2) their distances to the language goal smoothly decrease. These properties are useful for language-conditioned manipulation; however, we note that the representation is only trained on and evaluated on short-horizon interactions (e.g., open microwave, put hat on the bottle, and pineapple in the black pot). These interactions are analogous to a single abstract action in our definition. In comparison, our approach learns a state and action abstraction that is amenable to generating plans that involve multiple high-level actions.  \n\nThe key idea of Peng et al. is to identify task-relevant features of a visual scene conditioned on the input language. The abstraction process is done by extracting a list of objects along with their properties (e.g., silver pan, blue pot, and red square) and then using an LLM to determine which objects are relevant. Compared to this method, we do not assume that groundings of the object concepts are known, and show that our model can learn to reason about these concepts from language-annotated demonstrations. More importantly, the key idea of our approach is that the state abstraction should be latent. We illustrate that our model is able to predict that placing an object into the sink is not feasible if the sink is full. We do not explicitly symbolize \"full\" because it depends on the size of the object and the available space in the sink. Discretizing a raw state into explicit symbols, a process named textualization by Peng et al., prevents the method from performing fine-grained geometric reasoning.\n\nFinally, we want to highlight the challenges of applying CLIP and PointCLIP to our domain. CLIP mainly aligns images with textual descriptions of the visual contents. This is different from our abstract state representation which is tightly coupled with actions. In addition, existing research has found that CLIP behaves as a bag of words. This suggests that CLIP and its extension PointCLIP would not be able to infer the sink is currently full such that \"placing a pan into the sink\" is not feasible.\n\nA promising approach is to finetune these vision-language representations. We are currently running an experiment where we replace the point-cloud-based encoder with the LIV image encoder. Because our original data is collected with point clouds, we are currently recollecting the multi-view RGB image data to train the model. We will provide an update soon.\n\n**R4.2:** Details for the model.\n\n**A:** Thanks for the suggestion. We have included a new section (Appendix A) in the supplementary to document details of our models and training details. In addition, we have included new sections in the supplementary to document details of the baselines (Appendix B) and environment settings (Appendix C). We have also included more visualizations of the environment and the generated plans by our algorithm in Appendix D. We will also release the code.\n\nRegarding your particular question on inputs:\n- In BabyAI, the input to all methods is a 2D grid encoded with one-hot per-grid features (colors, object categories, agent's facing directions, etc.). This follows the original BabyAI paper [D1].\n- In Kitchen-World, the input to all methods is segmented 3D point clouds. In particular, the scene is composed of a set of objects; there is a colored point cloud for each object.\n\n[D1] Chevalier-Boisvert et al. BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning. ICLR 2019.\n\n**R4.3:** Accuracy for feasibility function.\n\n**A:** Thank you for the great suggestion! We include a new evaluation on the accuracy of the feasibility function in the Kitchen-World environment in Appendix D (which is also copied down below). We break down the evaluation across different generalization environments and different prediction horizons. In short, our model is capable of generating highly accurate feasibility predictions.\n\n| Task Setting      | Generalization       | Train  | Avg.   | $k$    | $k+1$  | $k+2$  | $k+3$  | $k+4$  | $k+5$  |\n| ----------------- | -------------------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n| All-Objects       | New Env.             | 99.70  | 98.17  | 99.00  | 98.51  | 97.79  | 97.32  | 96.45  | 98.89  |\n| Sink              | New Env.             | 99.84  | 98.01  | 98.13  | 98.55  | 98.10  | 97.38  | 97.63  | 97.01  |\n| All-Objects       | New Concept Comb.    | 91.97  | 90.08  | 88.74  | 90.36  | 87.67  | 90.24  | 88.68  | 99.01  |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590559435,
                "cdate": 1700590559435,
                "tmdate": 1700590559435,
                "mdate": 1700590559435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tTAVj0vBI2",
                "forum": "3UWuFoksGb",
                "replyto": "s3C35TkrVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and updates to the paper. I think I have better clarity on the differences in the proposed approach as compared to the alternative approaches I suggested. \n\n1. I would suggest adding citations for \"existing research has found that CLIP behaves as a bag of words.\"\n\n2. Why do you think the accuracy increases for $k+5$ as compared to $k+1$ for all-objects in the new table you provided? Is it significant?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687988748,
                "cdate": 1700687988748,
                "tmdate": 1700687988748,
                "mdate": 1700687988748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hdDtFMIlkc",
                "forum": "3UWuFoksGb",
                "replyto": "YDVhRad8uK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_NUXY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for prompt response. I will increase the score to 6 to reflect the updated change after author discussion."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698183378,
                "cdate": 1700698183378,
                "tmdate": 1700698183378,
                "mdate": 1700698183378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qBFSdARy55",
            "forum": "3UWuFoksGb",
            "replyto": "3UWuFoksGb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_V9Cw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_V9Cw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a framework for learning state and action abstractions from language-annotated demonstrations. The abstract actions and states are use to train a transition model in the latent space to learn the feasilbility of newer latent actions. This allows agents to generalize actions learned from language to longer, unseen tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originalty:** The paper investigates a problem is not tackled in the literature but can realistically exist. The paper is a novel and creative framework for addressing this problem.\n\n**Clarity:** The paper is well-writtten.\n\n**Significane:**  This work has the potential to be impactful in language-based agent interactions. Furthermore, the framework can be adapted to other sequential planning domains. \n\n**Quality:** The problem described is well-motivated. The approach to addressing the problem is laid out clearly and simply and it reads reasonably. The model framework is creative and intuitive. The experimental design is sound and makes sense to test their claims and results support the claims made by the authors."
                },
                "weaknesses": {
                    "value": "I don't have any major gripes. However, I found the description of the experimental domains lacking. Particularly I am not totally clear on the difference between the key-door and two-corridor environments."
                },
                "questions": {
                    "value": "No questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6513/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6513/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6513/Reviewer_V9Cw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798879650,
            "cdate": 1698798879650,
            "tmdate": 1699636731466,
            "mdate": 1699636731466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3eD7hUg8be",
                "forum": "3UWuFoksGb",
                "replyto": "qBFSdARy55",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**R3.1:** Difference between key-door and two-corridor.\n\n**A:** Thank you for bringing this up. We have added a detailed discussion of the different task settings in Appendix C. In short, two-corridor is one specific kind of key-door environment and two-corridor is designed to be a harder task than randomly sampled key-door environments on average.\n\nIn particular, in key-door, the connectivities between nearby rooms are randomly sampled. Therefore, it is likely that the required number of high-level actions is small or there is only one possible high-level action that is feasible at a state. In two-corridor, we specifically designed an environment where there are two corridors so that the planning length is long (maximum 7 high-level actions if the goal is to reach the room at the very end), and that at each given state, there will be at least two feasible actions. Therefore, in order to achieve the goal within a limited number of steps, the agent must unroll its learned transition model to select which corridor to enter."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590484157,
                "cdate": 1700590484157,
                "tmdate": 1700590484157,
                "mdate": 1700590484157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ws4R1g25gQ",
                "forum": "3UWuFoksGb",
                "replyto": "3eD7hUg8be",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_V9Cw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_V9Cw"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge that I have read the author's comments. My score remains the same."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693160472,
                "cdate": 1700693160472,
                "tmdate": 1700693160472,
                "mdate": 1700693160472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "afYcjwCM4p",
            "forum": "3UWuFoksGb",
            "replyto": "3UWuFoksGb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_B6My"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_B6My"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework for solving problems in sequential-decision making by combining LLM-generated high-level abstract actions, imitation learning and a low-level policy by a framework-agnostic traditional RL agent.\n\nThe pipeline in more detail is that given a prompt in human language which defines \u201ca language goal\u201d, an LLM decomposes to a verb and corresponding nouns and adjectives (e.g. \u2018place\u2019, \u2018bowl\u2019, \u2018green\u2019), with the assumption that these prompts can always be decomposed to this format. After this,\n\n- a state abstraction function is learned that can identify the objects in the environment\n\n- an abstract transition model is learned which predicts the next state given the current abstract state and high-level action. This model also has a feasibility component that predicts whether a future action can accomplish the language goal\n\n- A breadth-first search algorithm selects the shortest sequence of actions that accomplishes the language goal\n\n- Finally, a low-level policy is applied according to the sequence of high-level actions. These policies are learned with traditional RL\n\nThe paper tests the method on two environments: BabyAI (three task setting) and Kitchen-Worlds (two task settings), compare against low-level (regular) and high-level (when the agent has access to the defined ) RL in the former and Goal-Conditioned BC in the latter."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: the paper proposes a novel way to solve sequential decision making problems by combining LLM prompting, imitation learning and traditional reinforcement learning.\n\nQuality: the paper places the work in the literature very well, comparing the differences between previous works and mentions future work. The problem formulation is mostly clearly written up.\n\nClarity: The paper is mostly well-written and apart from a few inconsistencies, easy to understand.\n\nSignificance: its originality could be considered significant."
                },
                "weaknesses": {
                    "value": "I have three main reservations:\n\n- There is no available code, no experiment details (chosen hyperparameters, tuning) about the algorithm or the baselines and as such the results are not reproducible\n\n- The experiments themselves, the results, and the metrics are described in a very high level without details which does not allow the reader to indeed verify how well they support the claims.\n\n- Scalability: as the number of actions, objects and their combinations increase, the necessary training data size increases intractably (combinatorial explosion). I am concerned that this approach might be feasible for simple problems only due to its inherent limitations.\n\nFurthermore, there are a few things that are unclear to me which could be further weaknesses. (I\u2019ve listed the questions in the next section.)\n\nMy initial rating is due to the above reasons. I would be willing to increase the score if the above concerns are addressed adequately.\n\nClarity issues:\nThe notion of \u201ctasks\u201d is not defined in the problem formulation. I understand this is not easy to do, but including it would make the paper stronger. The expression \u201clanguage goal\u201d is also used a few times throughout the paper, without definition (and I believe tasks = language goals)\n\n4.2 third paragraph third sentence. Did you mean to say something along the lines of \u201cmaps the abstract state representation at the current step and an abstract action $a\u2019$ to the next abstract state $s\u2019_{t+1}$\u201d ?\n\nTypos/style issues that did not affect the score:\n\nIntroduction\nFirst paragraph; A \u201cgood\u201d state and action representations -> Remove \u201cA\u201d\n\u201cAs the state abstraction extracts relevant information about the actions to be executed\u201d no need for the first \u201cthe\u201d, and could you back this up with some sort of example in brackets, ideally with a citation (I can sort of guess what you mean, but I find this a bit too vague and unclear) \n\nThird paragraph: \u201can particular object\u201d -> a particular object\n\n4th paragraph: \u201cthat setups\u201d -> that sets up\n\n4.2\n\nSecond paragraph:\n\u201cWhich encodes the raw state from a given point cloud\u201d: from -> to\n\u201cBy applying a pooling operation for the point cloud\u201d for -> to\n\nThird paragraph:\n\n\u201cThe abstract transition function $\\tau\u2019$ takes the following form. It maps [...] -> remove \u201ctakes the following form. It\u201d\n\n5th paragraph\n\n\u201cWe appends\u201d -> we append\n\n\u201cOf the Transformer encoder at this query possible will be used\u201d -> no need for possible?\n\n4.3\n1st paragraph: last sentence is not needed, or its information content should be moved towards the end of the second paragraph. E.g. \u201cwe can generate the final tree, the leaves of which are $a\u2019_K$ which correspond to the underlying abstract action in the language goal\u201d.\n\n2nd paragraph: subsequence -> subsequent\n\nsearchs ->searches\n\n5\nEXPERIMENT -> EXPERIMENTS\n\n5.2\n\n2nd paragraph: \u201cdesigned to evaluate models\u2019 abilities\u201d -> missing the after evaluate\n\nCitation for the Goal-Conditioned BC baseline is missing.\n\n6\n\n\u201cA framework that leverage\u201d -> leverages"
                },
                "questions": {
                    "value": "In the state abstraction function, how do you know when to stop training to have the right number of point clouds? Or else, do you assume that the number of objects in the environment is known beforehand? Is this end-to-end trained? (I think the paper would benefit if these points were made clearer there.)\n\nWhat kind of pooling operation do you use? Max or average? And what is the dimensionality of the per-object latent feature?\n\nHow is the generalization success rate actually calculated? What does it mean that an agent \u201cfails to solve the tasks\u201d? I am assuming given the allowed number of steps?\n\nFor the loss of the feasibility prediction model, do you use the L_2 norm?\n\nThis is just notation, but in the reinforcement learning literature s\u2019 usually denotes the next state, and $\\hat{s}$ is used for an approximate state. Why did you decide to change this convention?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811904191,
            "cdate": 1698811904191,
            "tmdate": 1699636731347,
            "mdate": 1699636731347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L4N2Se71jJ",
                "forum": "3UWuFoksGb",
                "replyto": "afYcjwCM4p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**R2.1:** Implementation and experiment details.\n\n**A:** Thank you for the suggestion. We have included new sections in the supplementary to document details of our models (Appendix A), baselines (Appendix B), and environment settings (Appendix C). We have also included more visualizations of the environment and the generated plans by our algorithm in Appendix D. We will also release the code.\n\n**R2.2:** Scalability as number of concepts increases.\n\n**A:** Thank you for bringing up this point. We agree that scalability would be an issue for both learning and planning when the number of actions and objects increases. However, we should point out that this is _exactly_ the reason we learn a **compositional** space of candidate actions. In particular, we use LLMs to decompose instructions into steps, and steps into verbs and their object arguments. Therefore, our method can generalize to unseen compositions of concepts (see Table 1 and Table 2, the \"novel concept composition\" columns).\n\nPlanning with a compositional space of possible actions would be another challenge, for all methods. In our paper, we have used a simple tree search algorithm with action feasibility pruning to generate plans. For more complex scenarios, more sophisticated algorithms, such as using symbolic planning tools [B1] or learning high-level policies to guide the search as in AlphaGo [B2] would be possible. We have tried a similar version with learned high-level policies; however, in practice, we find this policy barely generalizes to longer plans and hurts the performance.\n\n[B1] Helmert, Malte. \"The fast downward planning system.\" JAIR (2006): 191-246.\n[B2] Silver, David, et al. \"Mastering the game of Go with deep neural networks and tree search.\" nature 529.7587 (2016): 484-489.\n\n**R2.3:** Definition of task and language goal.\n\n**A:** Thanks for the suggestion. We have updated our problem formulation section to include definitions of tasks. Your understanding is correct that in this paper, each task $t$ in the environment is associated with a natural language instruction (the language goal) $L_t$.\n\n**R2.4:** Details for state abstraction function.\n\n**A:** Thank you for pointing these out. We have made these points more clear in the revision. Here are the direct answers to your questions:\n- **Point clouds**: As stated in Sec 4.2 State abstraction function, we assume access to segmented object point clouds. The number of objects in each scene is $N$ and is known.\n- **End-to-End training**: The state abstraction function $\\phi$, the abstract transition model $\\mathcal{T}'$, and the feasibility prediction model $f_{a'}$ are trained together end-to-end.\n- **Pooling operations**: We use max pooling and the per-object latent feature has a dimension of 352. We provide in Appendix A a list of hyperparameters for the model.\n\n**R2.5:** Details for metric.\n\n**A:** Thank you for bringing this up. We have added evaluation details in Appendix C. In summary, for BabyAI, models need to reach a goal within 200 low-level steps; otherwise, it's considered as a failure. For Kitchen-Worlds, models need to reach a goal within 5 steps; otherwise, it's considered as a failure. For all experiments, we evaluate on 100 task instances and report the average success rate.\n\n**R2.6:** Abstract transition model clarification.\n\n**A:** Your understanding is correct! We meant to say that the abstract transition model takes in the current abstract state $s'\\_t$ and abstract action $a'$, and predicts the next abstract state $s'\\_{t+1}$. We have updated the text to clarify this point.\n\n**R2.7:** Notation clarification.\n    \n**A:** Thank you for the suggestion. We have chosen to use $s_t$ and $s'_t$ to represent the raw state and the abstract state at step $t$, respectively. Since we always use the subscript $t$ there are no ambiguities. However, we do agree that this will potentially create confusion for readers. We plan to use different symbols $x$ and $s$ to represent raw states and abstract states in the final version.\n\n**R2.8:** Loss clarification.\n\n**A:** Yes, we use L_2 norm. We have updated the equation to highlight this."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590398499,
                "cdate": 1700590398499,
                "tmdate": 1700590398499,
                "mdate": 1700590398499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fOeps6mfdB",
                "forum": "3UWuFoksGb",
                "replyto": "L4N2Se71jJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_B6My"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_B6My"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nI will also need more time to revisit the revised version of the paper and the details in the response. Thank you for your response and the updates to the paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713589030,
                "cdate": 1700713589030,
                "tmdate": 1700713589030,
                "mdate": 1700713589030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m0xDdsfG0S",
            "forum": "3UWuFoksGb",
            "replyto": "3UWuFoksGb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_Q2SJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6513/Reviewer_Q2SJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an RL agent that solves the problem by utilizing symbolic abstraction and object-centric representation learning. Given a problem and a goal description for the environment, the proposed method uses LLM as a parser to translate the goal description (or instruction) as a collection of action predicates (verb and objects combination), where those action predicates are used as action abstraction (options or skills in hierarchical reinforcement learning).\n\nTraining requires human demonstration annotated with an action predicate. Given the demonstration data, the proposed method trains several functions. State abstraction function takes in the segmented output of a point cloud transformer to disentangle pixel input and it provides a latent space state representation. Action transition model takes in the latent abstract state and symbolic encoding of action predicates. Feasibility function predicts whether an abstract action is applicable in the current state.\n\nAfter training necessary functions with annotated demonstration data, the planning stage uses the state abstraction function to get a latent state and utilize the feasibility function to select applicable actions. Planning with symbolic action predicates is done by brute-force search over all actions in the problem. Last,  given a high-level plan, the low-level policy is trained."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Originality: The novel aspect of the presented method is combining symbolic planning with action predicates extracted from natural language goal descriptions or instructions and latent space representation learning with point cloud transformers. The abstract planning is done at the symbolic level, and the abstract state transitions are tracked with object-centric representation learned from segmented pixel data.\n* Quality: The overall description of the method is easy to understand and the experiment was conducted on two types of the environments.\n* Clarity: Figures help understanding the overall approach\n* Significance: I think this is an interesting work that integrating many things to work."
                },
                "weaknesses": {
                    "value": "* Originality: Individual components are existing approaches and the originality is on bring those components and implement an agent to solve mini-grid and kitchen world problems.\n* Quality: Due to missing details, it is difficult to assess the quality.\n* Clarity: There are many missing details in the paper.\n* Significance: The comparison is made only against a simpler baselines (end to end RL and behavior cloning)."
                },
                "questions": {
                    "value": "### General questions\n1. The title is \u201clearning planning abstractions from language.\u201d In the paper, the role of the LLM is parsing an instruction sentence to extract action predicates and objects. The remaining part of the work is independent of language models or language. The parsing could have been done manually or other methods. I cannot see the rationale of using LLM, other than demonstrating that LLM can do the parsing. What is \u201clearned\u201d from language?\n\n2. What if the goal description or instruction did not reveal enough information to extract required high-level actions? Then, should we collect demonstrations following the derived high-level actions?\n\n3. The instructions in the paper are quite simple sentences to parse. What is the longest abstract plan needed to solve the problem? How many abstract actions were needed?\n\n### Section 4\n4. In section 4.2, how the model was trained given annotated demonstrations? Is it learned per each abstract action? How many trajectories were given to the training process? How did you train Point Cloud Transformer for mini-grid environment and kitchen world environment? Can you present the details on the training of models?\n\n5. In section 4.3, planning is done with BFS search. If the feasibility prediction fails, how did you handle the error? Does a set of actions derived from LLM parser always guarantee to solve a problem? How can you ensure the action space can solve all problems in the test set? Or a human demonstrator should create trajectories that solves problem given the action predicates?\n\n6. In section 4.4, low-level policy is trained using an actor-critic algorithm. Can you present the details?\n\n### BabyAI experiments.\n\n7. The report on BabyAI experiments shows that baseline will not completely fail in all problems. The presented paper and the report also used similar low-level policy algorithm and neural network architectures. It also offers imitation learning experiments. Can you make some comparison with those baselines? What are the high-level actions extracted from LLM and what are the plans found for the problems? Can you present the sample efficiency or training/test performance?\n\n[1] Hui, D. Y. T., Chevalier-Boisvert, M., Bahdanau, D., & Bengio, Y. (2020). BabyAI 1.1.\u00a0arXiv preprint arXiv:2007.12770.\n\n### Kitchen Worlds experiments.\n8. How many high-level actions were annotated and trained to solve this environment?\u00a0 From the description in the experiment section, the length of the plan is mostly one or two. Could you present details on the high-level plan and the low-level policies?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6513/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894480985,
            "cdate": 1698894480985,
            "tmdate": 1699636731232,
            "mdate": 1699636731232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wxfjpN1isg",
                "forum": "3UWuFoksGb",
                "replyto": "m0xDdsfG0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "**R1.1:** Parsing language with LLMs.\n\n**A:** Thanks for the suggestion. Following your suggestion, we would like to clarify how we are \"learning from language\" and \"how we use language models.\"\n\nOur goal in the paper is to recover from language a symbolic abstract action space, composed of *object-level* concepts (such as colors and shapes) and *action* concepts (e.g., pick up). These object and action concepts can be recombined in a compositional manner to form the entire action space. In particular, we learn the grounding of these concepts: we learn recognition models for objects and policies for actions from paired demonstrations and texts, then use them in a planning algorithm to achieve a goal specified in language. In short, our method learns object and action concepts from language, and uses these concepts in language for planning.\n\nIn order to decompose natural language instructions into individual concepts, we leverage large language models to parse sentences. We agree with the reviewer that the parsing can be done by other methods as well; we choose to use LLM to perform parsing due to its simplicity and accuracy. We have added example prompts we use for GPT-4 in Appendix A.1. \n \n**R1.2:** Incomplete language instructions.\n\n**A:** Thank you for bringing up this point. In our paper, the LLM is only used to generate the high-level goal (e.g., reaching a target object); the generation of high-level action sequences is performed by our planning algorithm, which takes environmental states as its input (for example, if reaching the target requires opening particular doors, the planner will handle such situations.)\n\nFor scenarios where a user did not fully specify their goals, additional clarifications would be needed. This could be implemented by, for example, generating questions to the user. We did not consider such scenarios, therefore, we added a discussion of this limitation in Appendix E.\n\n**R1.3:** Details for abstract plans and actions.\n\n**A:** Thanks for bringing this up. We briefly discuss the abstract plans in the Setup subsections for BabyAI (Sec 5.1) and Kitchen-Worlds (Sec 5.2) due to the page limit. We have now added more details in Appendix C. We summarize the details below.\n\nFor the BabyAI experiment, the most challenging case requires 5 abstract actions to solve. This is a nontrivial task because the models are only trained on task instances that require at most 3 abstract actions to complete. In addition, there are strong dependencies between the actions (e.g., the agent must first find the key to unlock an intermediary door before gaining access to the next key). Our empirical results show that baselines obtain significantly worse performance than our model. \n\nFor the Kitchen-Worlds experiment, the most challenging case requires 4 abstract actions to solve (please see the Setup subsection in Sec 5.2). Similar to the BabyAI experiment, the abstract actions also have strong dependencies (e.g., the agent needs to first pick the object from the sink, place the object in empty space, then pick up the target object, and finally place the target object in the sink). In comparison to CALVIN [A1], which is a widely-used language-conditioned benchmark for long-horizon manipulation in 3D environments, our task is more challenging because (1) we significantly vary the environment layouts and (2) at test time, our goal instruction only contains a single goal action, and the agent needs to plan out additional subgoals before the target action; in contrast, CALVIN provides step-by-step instructions for solving tasks with 1-5 steps. \n\n[A1] Mees et al. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. RAL, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590019743,
                "cdate": 1700590019743,
                "tmdate": 1700590019743,
                "mdate": 1700590019743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJPFyzgamd",
                "forum": "3UWuFoksGb",
                "replyto": "m0xDdsfG0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2"
                    },
                    "comment": {
                        "value": "**R1.4:** Details for training (Section 4.2).\n\n**A:** We present training details in Sec 4.2. We have updated the text to further clarify the training procedure. Here we summarize the main points:\n\n- **Data**: Our model is trained on sequences of paired states and abstract actions, where each is in the form of $s\\_0, a'\\_0, s\\_1, \\cdots, a'\\_{\\ell-1}, s\\_\\ell$. At each iteration, we first randomly sample a suffix from a demonstration trajectory to obtain $s\\_k, a'\\_k, s\\_{k+1}, \\cdots, a'\\_{\\ell-1}, s\\_\\ell$. Next, we use $\\phi$ to encode $s\\_{k}$ as $s'\\_{k} = \\phi(s\\_{k})$, and recurrently apply the Transformer-based abstract transition model and feasibility prediction model: $s'\\_{i+1} = \\mathcal{T}'(s'\\_{i}, a'\\_i)$ and $\\overline{\\textit{feas}}\\_i = f(s'\\_i, a'\\_i)$, for all $i=k,\\cdots,\\ell-1$. \n- **Objective**: The training loss is defined as $\\mathcal{L} = \\sum\\_{i=k+1}^{\\ell} \\| s'\\_i - \\phi(s\\_i) \\|\\_2 + \\sum_{i=k}^{\\ell-1} \\text{BCE}(\\overline{\\textit{feas}}\\_i, \\textit{feas}\\_i)$, where $\\text{BCE}$ is the binary cross-entropy loss, and $\\textit{feas}\\_i$ is the groundtruth feasibility label. $\\textit{feas}\\_i$ is true for all $i < \\ell$, and is true for $i=\\ell$ if the trajectory execution is successful (i.e., the last action is successful) or false otherwise.\n- **Model**: A single model is learned for all abstract actions because the abstract feasibility prediction model $f(s', a')$ and the abstract transition model $\\mathcal{T}'(s', a')$ are both conditioned on the given abstract actions. The Point Cloud Transformer is trained as part of the state abstraction function $\\phi$ with the loss described above. \n- **Data**: For BabyAI, we train all models on 100,000 trajectories. For Kitchen-Worlds, we train all models on around 10,000 trajectories. We provide details in Appendix C. \n\n**R1.5:** Details for planning (Section 4.3).\n\n**A:** We always choose the sequence of abstract actions with the highest feasibility scores. This design ensures that the agent will always pick an action at any state. To deal with situations where the action selected by the agent is not helpful for reaching the goal, we perform a close-loop replan after the agent finishes executing each action, therefore allowing the agent to recover.\n\nIn this paper, we assume that the set of actions derived from language annotations of the training trajectories covers the space of actions required to complete the testing tasks. As discussed in Sec 4.1, in contrast to representing each action term as a textual sentence, the decomposed action and object concepts allow us to enumerate all possible actions an agent can take easily. In the experiments, we empirically validate whether our method can generalize to new combinations of action and object concepts in by evaluating our model on \"Novel Concept Combinations\".\n\nWe note that it's non-trivial to generalize a manipulation policy to completely new actions (e.g., from pick-and-place to opening a microwave door). As discussed in a recent paper representative of the state of generalizable manipulation [A2], there are four levels of generalization: (1) different initial object placements, (2) different background and distractor objects, (3) new object-skill combinations, (4) new environment layouts. In this paper, our method shows strong generalization in all four levels. \n\n[A2] Bharadhwaj et al. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. arXiv:2309.01918, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590163086,
                "cdate": 1700590163086,
                "tmdate": 1700590195541,
                "mdate": 1700590195541,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wplqv0gDRw",
                "forum": "3UWuFoksGb",
                "replyto": "ngopk8MVj6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_Q2SJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6513/Reviewer_Q2SJ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nI need more time to revisit the revised version of the paper and the details in the response.\nThanks very much for your response and the updates to the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6513/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680081109,
                "cdate": 1700680081109,
                "tmdate": 1700680081109,
                "mdate": 1700680081109,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]