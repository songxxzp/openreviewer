[
    {
        "title": "Flatness-aware Adversarial Attack"
    },
    {
        "review": {
            "id": "RPCWK2ooy5",
            "forum": "gHwS4DzkYu",
            "replyto": "gHwS4DzkYu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission684/Reviewer_fujR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission684/Reviewer_fujR"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose flatness-aware adversarial attack (FAA) to improve the adversarial transferability. In particular, FAA adopts a regularizer for the gradient to make the generated adversarial example located in a flat local optimum. To avoid the Hessian matrix calculation, they utilize Taylor expansion to approximate the Hessian matrix. Experiments on ImageNet dataset show the effectiveness of FAA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The authors have conducted several experiments to validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The launch experiment is not solid enough. In my opinion, the experiments can only conclude that transferable adversarial examples might be in flatter local optima. It is a necessary but not sufficient condition that adversarial examples in flatter local optima are more transferable.\n\n2. It is not a new method that connects flat regions and input regularization methods [1]. Eq. (4) is similar to the Eq. (3) in [1], making the motivation rather limited. The main difference is how to approximate the Hessian matrix. Tylor expansion is not a novel way for such an approximation.\n\n3. I am curious why did the author adopt the number of iterations $T=20$, since existing works mainly adopt $T=10$. Increasing the number of iterations might result in overfitting, which degrades the baselines' performance.\n\n4. It is expected to see more momentum-based baselines, such as [1], [2].\n\n5. Why does Table 2 miss the baseline RAP? It should be a significant baseline since it is the first paper that locates the adversarial examples in flat local optima for better transferability.\n\n\n[1] Ge et al. Boosting Adversarial Transferability by Achieving Flat Local Maxima. arXiv Preprint arXiv: 2306.05225, 2023.\n\n[2] Zhang et al. Improving the Transferability of Adversarial Samples by Path-Augmented Method. CVPR 2023."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697612219707,
            "cdate": 1697612219707,
            "tmdate": 1699635995538,
            "mdate": 1699635995538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JwyjT3QFIc",
                "forum": "gHwS4DzkYu",
                "replyto": "RPCWK2ooy5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts on our paper and your valuable suggestions. We answer your questions point-by-point as follows:\n\n**Question 1:**\nThe launch experiment is not solid enough.\n\n**Response:**\nWe would like to clarify some misunderstandings concerning our launch experiment.\nOur launch experiment aims to show that adversarial examples residing in flat regions enjoy better transferability (i.e., the necessary condition), instead of demonstrating the equivalence between the transferability of adversarial samples and flat regions.\nAs shown by you, the results support our objective (the necessity).\n\nIn fact, this paper delves into the transferability problem, with the hope of augmenting the transferability for the effectiveness of black-box attacks.\nIn Section 1, drawing inspiration from input regularization methods, we hypothesize that adversarial examples located in flat regions enjoy better transferability.\nThis conjecture is preliminarily supported by visualizations of the loss landscape (Figure 1).\nTo further substantiate this hypothesis, we conduct the launch experiment.\nLeveraging the empirical evidence gathered, we are led to the conviction that adversarial examples in flat regions indeed possess superior transferability.\nConsequently, we propose our method, FAA, to improve the transferability of adversarial examples.\nBy the way, Appendix C also offers a theoretical demonstration to support this hypothesis.\n\n\n**Question 2:**\nThe novelty of this paper.\n\n**Response:**\nWe carefully check [1] and think it is enlightening. However, this paper distinguishes itself from [1] by several key parts, constituting our contributions. The significance of transferability research to our community is two-fold: first, it enhances the understanding of transferability, which has implications for various related fields; second, it improves the effectiveness of black-box attacks, with potential implications for the development of defenses and increasing awareness of vulnerabilities in neural networks. Herein, we expound on how our work provides new insights into these two aspects.\n\n(I) Our motivation varies. Existing literature, including [1,2], analogizes the transferability of adversarial examples to models' generalization ability, inspiring exploration of techniques that enhance generalization to boost transferability, such as SAM-like optimization formulation [2]. Unlike these, to our best knowledge, we are the first to observe the potential associations between input regularization methods and flat regions (paragraph 1 in Intro). By clarifying how input regularization methods enable adversarial example convergence towards flat regions, we illuminate their limitations (paragraph 3 in Intro). This insight motivates the design of FAA (paragraph 3 in Intro). Thus, this paper deepens our understanding of existing input regularization methods by rethinking them from a unique perspective.\n\n(II) On the technical pathway, despite the similarity between Eq. 3 in [1] and Eq. 4 in our paper, their underlying technical trajectories differ fundamentally. They penalize maximum norms while we penalize gradients of samples around $x+\\delta$. This subtle variation can indeed lead to significant differences. For instance, L1 and L2 regularizations appear highly similar in form, but their underlying mechanisms and effects are substantially different (L1 regularization induces sparsity). Specifically, from the perspective of conjugate functions [3], loosely speaking, Eq.3 in [1] would be a relaxation of Eq. 4 in our paper. In other words, our penalty term provides a tighter bound for flat regions, resulting in the superior performance of our method.\n\n(III) Our theoretical contribution is pioneering (Appendix C). To the best of our knowledge, we are the first to provide theoretical proof validating the relationship between transferability and flat regions. While [2] speculates on this relationship based on intuition, [1] follows suit. Their results are empirical, whereas ours possess theoretical underpinnings, positioning our paper in parallel rather than merely building upon (or overlapping) their work.\n\n(IV) The effectiveness of our method is significant. FAA is the first to achieve a success rate of over 90\\% in attacking nearly all mainstream models, spanning CNNs and transformers, with or without defenses. Our attacks on real-world systems further demonstrate its effectiveness in the physical world. As evidenced in responses to questions 4 and 5, our attack success rate surpasses baselines by a clear margin. For practitioners and real-world applications, the primary concern lies in effectiveness rather than other factors. From this standpoint, our attacks hold immense value for industrial and DNN applications.\n\nIn summary, these four significant parts constitute the contributions that set this paper apart from others, paving the way for advancements in this field."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699763041597,
                "cdate": 1699763041597,
                "tmdate": 1699763247579,
                "mdate": 1699763247579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vfYwxQJlFZ",
                "forum": "gHwS4DzkYu",
                "replyto": "JoaRerPTSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission684/Reviewer_fujR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission684/Reviewer_fujR"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "Thank the authors for the reply. I have read the rebuttal and the other reviews. However, I still think my concerns have not been addressed.\n\n+ The motivation and approach are similar to [1]. Also, flat local maxima resulting in better transferability is not a new idea [2].\n\n+ I have read the paper again and am confused if Eq. (7) holds. Eq. (6) is a Taylor expansion, ensuring the equivalence of the function's value within a limited vicinity. Nevertheless, this does not inherently guarantee a corresponding equivalence in their gradients. Consequently, it appears that Equation (7) may not be a direct derivative of Equation (6).\n\nWith the concerns about the novelty and approach, I tend to maintain my score currently.\n\n[1] Boosting Adversarial Transferability by Achieving Flat Local Maxima, arXiv 2023.\n\n[2] Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation, NIPS 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047463345,
                "cdate": 1700047463345,
                "tmdate": 1700047463345,
                "mdate": 1700047463345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xLmA4OMqah",
            "forum": "gHwS4DzkYu",
            "replyto": "gHwS4DzkYu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission684/Reviewer_tmEk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission684/Reviewer_tmEk"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach to improve the transferability of adversarial attacks using the properties of input signals: the more flat region on the loss curve they occupy, the more transferrable they are."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A paper seems to provide a very good method because:\n- it provides almost 100% transferability for both normal (Table 1) and secured (Table 2) models (!) for untargeted attacks with a huge margin over other methods\n- it beats other methods for targeted attacks (Table 3) - again with a significant margin\n- it provides a solid reasoning based on observations, and the theory behind it\n- and even the theory was adopted towards the fast computation cycle (to get rid of Hessian matrix computation) \n- it was tested in real CV applications"
                },
                "weaknesses": {
                    "value": "Although the paper provide a lot of insights, there are still some (I hope minor and improvable) drawbacks:\n- Page 4, the ref. to Mean Value Theorem: it'd be better to refer to some classic mathematical results (I guess they are discovered hundreds of years ago, not just 15)\n- Page 16, Table 5: It's not clear what \"Approximation Error\" exactly means: is it the overall Hessian-based additive term, or Term1, or Term2 (Equation 8)?\n- Page 19, Appendix E.8: \"As shown in Figure 9, FAA produces flatter adversarial examples than FAA\" -> \"As shown in Figure 9, FAA produces flatter adversarial examples than RAP\"?\n- But my main concern is the (based on my assessment) theory-related inference on Pages 15 and 15 (Appendix C). Let me briefly provide it so the errors could be corrected:\n\nFirst of all, I'm not sure that the Eq. (9) is correct. It uses a Taylor Expansion of a complex function (which in turn relies on the derivative of a complex function), so I think the correct way to do it: $f(g(x))=f(g(a))+\\nabla g(a)\\nabla f(g(a))(x-a)$, but in the Eq. (9) the multiplicative term $\\nabla F$ is omitted, but it is not always equal to 1, right?\n\nPage 15: \"we use p(x) \u2264 p(x + \u03b4)\" which is incorrect, and should be $p(x)\\geq p(x+\\delta)$\n\nPage 15: \"Notice that the flatness-aware item punishes the norm of gradients of samples around x + \u03b4. Therefore, this induces ||\u2207logF (x + \u03b4)||2 and ||$\\nabla^{2}$logF (x + \u03b4)|| to be 0.\" The implication is incorrect, the small first derivative doesn't say anything about the amplitude of the second derivate (example: $\\sin x^2$)\n\nPage 15: at the end of Appendix C, when doing all the approximations, nothing has been said about $\\nabla \\log p(x+\\delta)$ which also needs to be close to zero, right?"
                },
                "questions": {
                    "value": "Q1: I would be interested in $l_0$/patch-based optimization and transferability, because it is the most applicable techniques for the real-world adversarial attacks, and the authors' thought on extensibility of their framework to this case (taking into account the differentiability problem).\n\nQ2: Why we need the Appendix B (minmax problem complexity)? It seems like a very obvious thing and doesn't provide any insight at all\n\nQ3: I am very interested whether the theoretic part (see my notes above) can be corrected and improved as now it seems to have a lot of mistakes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission684/Reviewer_tmEk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629840467,
            "cdate": 1698629840467,
            "tmdate": 1699635995472,
            "mdate": 1699635995472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TLKBRYDE2H",
                "forum": "gHwS4DzkYu",
                "replyto": "xLmA4OMqah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the efforts on our paper and your valuable suggestions.\nWe have included the following response in the revised paper.\nWe will upload the revised version before the end of the discussion period.\nWe answer your questions point-by-point as follows.\n\n---\n\n**Question 1:**\nI am very interested whether the theoretic part (see my notes above) can be corrected and improved as now it seems to have a lot of mistakes.\n\n**Response:**\nFor the first one, I'm not sure that the Eq. (9) is correct...\nIn Equation 9, we treat $L(F(\\cdot),\\cdot)$ as a holistic function and differentiate it, so there is no mathematical error in this expression.\nRegarding the effectiveness of linear expansion, we want to provide some discussion to support Equation 9.\nFirstly, for the domain of adversarial examples, the assumption of neural networks as approximately linear is common, as seen in [3].\nThe supporting argument is that modern neural networks still exhibit a high degree of linearity, as the activation functions of neural networks often demonstrate favorable linear properties, such as ReLU.\nFurthermore, albeit empirically, assuming linearity in neural networks when studying adversarial examples often yields good results.\nOur attack results also support this.\nLastly, within a small neighborhood, this assumption is often feasible, as per the Taylor series expansion.\nOur perturbations are typically constrained to a small magnitude to maintain the human-imperceptibility of crafted adversarial examples.\n\n\nFor the second one, 'we use $p(x) \\leq p(x + \\delta)$ which is incorrect, and should be $p(x) \\geq p(x+\\delta)$'.\nWe have revised this typo.\nThanks for your careful check again.\n\nFor the third one, \"Notice that the flatness-aware item punishes the norm of gradients of samples around $x + \\delta$. Therefore, this induces...\"\nWe want to clarify some things.\nIn fact, according to Equation 4, our gradient penalty term is $||\\nabla L(F(x+\\delta+\\Delta),y)||, \\Delta \\sim U(-b,b)$ instead of $||\\nabla L(F(x+\\delta),y)||$.\nThe second derivative can be considered as a metric for the change rate of the first-order derivative.\nIntuitively speaking, when the penalty strength is sufficiently large, there is $||\\nabla L(F(x+\\delta+\\Delta),y)|| \\rightarrow 0$.\nConsequently, the second derivative at $x + \\delta$ naturally becomes zero.\nFormally, the norm of the second derivative can be expressed by $ ||\\nabla^2 L(F(x+\\delta),y)|| = ||\\lim_{\\Delta \\rightarrow 0} \\{ (\\nabla L(F(x+\\delta+\\Delta),y) - \\nabla L(F(x+\\delta),y)) \\Delta^{-1} \\}||$.\nPunishing $||\\nabla L(F(x+\\delta+\\Delta),y)||$ indeed minimizes $||\\nabla^2 L(F(x+\\delta),y)||$.\n\n\nFor the fourth one, at the end of Appendix C, when doing all the approximations, nothing...\nWhen the proxy and target models exhibit similarity, the gradients of the proxy and target model also present high similarity.\nIn fact, as demonstrated in [1] (the conclusion of Section 5.1), The distance between $log p(x)$ and $log F(x)$ can control the magnitude of $||\\nabla log p(x) - \\nabla log F(x)||$.\nIn other words, a smaller distance between $log p(x)$ and $log F(x)$ results in smaller gradient distances.\nEmpirically, a well-trained model $F(x)$ typically yields a smaller $||\\nabla log p(x) - \\nabla log F(x)||$.\nConsequently, we are also able to impose a certain degree of penalty on $\\nabla log p(x+\\delta)$.\n\n---\n\n**Question 2:**\nPage 4, the ref. to Mean Value Theorem: it'd be better to refer to some classic mathematical results (I guess they are discovered hundreds of years ago, not just 15).\n\n**Response:**\nTo the best of our understanding, the origins of the Mean Value Theorem can be traced back to 1823, when its formal proof was established by mathematician Cauchy. We have updated our references accordingly.\n\n---\n\n**Question 3:**\nPage 16, Table 5: It's not clear what \"Approximation Error\" exactly means: is it the overall Hessian-based additive term, or Term1, or Term2 (Equation 8)?\n\n**Response:**\nThe approximation error is the overall Hessian-based additive term.\nWe have revised the manuscript to enhance its clarity.\n\n---\n\n\n**Question 4:**\nPage 19, Appendix E.8: \"As shown in Figure 9, FAA produces flatter adversarial examples than FAA\" -> \"As shown in Figure 9, FAA produces flatter adversarial examples than RAP\"?\n\n**Response:**\nThanks for your careful check.\nThis is a typo error and it is \"As shown in Figure 9, FAA produces flatter adversarial examples than RAP\".\nWe have revised this typo."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959620845,
                "cdate": 1699959620845,
                "tmdate": 1700221964108,
                "mdate": 1700221964108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97ZwtsVvN4",
                "forum": "gHwS4DzkYu",
                "replyto": "xLmA4OMqah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/2)"
                    },
                    "comment": {
                        "value": "**Question 5:**\nI would be interested in $l_0$/patch-based optimization and transferability, because it is the most applicable techniques for the real-world adversarial attacks, and the authors' thought on extensibility of their framework to this case (taking into account the differentiability problem).\n\n**Response:**\nWe believe the extension of FAA to patch-based attacks is easy.\nWe can directly add our gradient penalty term to the optimization targets in [4].\n\nIn contrast, the constraint of L0 norm is quite hard to handle, primarily stemming from the difficulty in identifying the optimal set of pixels to optimize.\nIn response, we advocate for an approximation solution as a remedy.\nSpecifically, we relax L0 norm constraint by introducing an L1 regularization term (or other sparsity-inducing terms) in the optimization target (Eq. 4) to supplant the L0 norm constraint.\nThis ensures the differentiability of the optimization target (Eq. 4).\nFinally\uff0cwe retain perturbation elements that exert the most substantial influence on the optimization target (Eq. 4)\uff0cthereby satisfying L0 constraint.\n\n---\n\n**Question 6:**\nWhy we need the Appendix B (minmax problem complexity)? It seems like a very obvious thing and doesn't provide any insight at all.\n\n**Response:**\nTo our knowledge, some researchers seem to not be very familiar with the difficulty of employing gradient descent algorithms to address the bi-level optimization problem.\nThus, we provide an example to supplement this.\n\n---\n\nReference:\n\n[1] On the Second Mean-Value Theorem of the Integral Calculus, 1909\n\n[2] On the Robustness of Split Learning against Adversarial Attacks, ECAI 2023.\n\n[3] Explaining and harnessing adversarial examples, ICLR 2014.\n\n[4] Adversarial T-shirt! Evading Person Detectors in A Physical World, ECCV 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959687369,
                "cdate": 1699959687369,
                "tmdate": 1699959859130,
                "mdate": 1699959859130,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "832yoGa5HM",
            "forum": "gHwS4DzkYu",
            "replyto": "gHwS4DzkYu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission684/Reviewer_DvXX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission684/Reviewer_DvXX"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an adversarial attack algorithm that leverages flat loss regions to generate transferable adversarial examples. Current methods use input regularization to generate better transferable adversarial examples. In this work, the authors instead derive a flatness-aware regularization term. Further, they propose a Hessian approximation for the gradients. The approach is shown to outperform existing transfer attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The approach is well-motivated and is intuitive. The overall presentation is good, and the paper is well written.\n2. While similar approaches have been applied to defenses, flatness aware attacks are an interesting application.\n3. The paper also shows effective results for real world image classifiers as well as a variety of robust and non-robust models.\n4. The attack is significantly successful even when the proxy is not adversarially trained."
                },
                "weaknesses": {
                    "value": "It might be useful to evaluate the approach for flatness-aware adversarial defenses like TRADES (Zhang et al, ICML 2019), SAM-AT [1], and ATEnt [2]. These methods leverage sharpness aware losses to introduce flat loss landscapes with respect to the input, and could possibly behave differently.\n\n[1] Wei, Zeming, Jingyu Zhu, and Yihao Zhang. \"On the Relation between Sharpness-Aware Minimization and Adversarial Robustness.\", ADvML Workshop at ICML 2023.\n[2] Jagatap, Gauri, et al. \"Adversarially robust learning via entropic regularization.\" Frontiers in artificial intelligence 4 (2022): 780843."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799860988,
            "cdate": 1698799860988,
            "tmdate": 1699635995405,
            "mdate": 1699635995405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]