[
    {
        "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map"
    },
    {
        "review": {
            "id": "1qY73zHh8d",
            "forum": "mYWsyTuiRp",
            "replyto": "mYWsyTuiRp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_GB37"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_GB37"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on analyzing Feed-Forward (FF) blocks in the Transformer model, specifically regarding their impact on input contextualization. The authors utilize a refined attention map by combining norm-based analysis and the integrated gradient method, which offers completeness in understanding the FF block's behavior. In the experiment, Wikipedia excerpts and Stanford Sentiment Treebank v2 datasets are used, and the authors analyzed 11 11 masked LMs and one casual LM. The results show that FF blocks do modify input contextualization by amplifying specific linguistic compositions, such as subword pairs forming a single word. Furthermore, the authors discover that the FF block and its surrounding components tend to cancel out each other's contextualization effects, shedding light on the mechanism and suggesting redundancy in processing within the Transformer layer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Analyzing Feed-Forward (FF) blocks in Transformer models through a refined attention map is novel, which combines norm-based analysis and the integrated gradient method. While previous research has explored the behavior of FF blocks, this study offers a unique perspective by leveraging the aforementioned techniques to gain a comprehensive understanding of their impact on input contextualization. \n\n2. The experiments conducted with masked and causal language models demonstrate the effectiveness of the approach in capturing the modification of input contextualization by FF blocks. The clarity and precision of the analysis enhance the quality of the research, ensuring reliable and valid results.\n\n3. The paper is clear and easy to follow. The description of the refined attention map, norm-based analysis, and integrated gradient method is presented in a clear and understandable manner. The experiments and their results are well-explained, enabling readers to grasp the implications of FF block behavior on input contextualization."
                },
                "weaknesses": {
                    "value": "1. To enhance the clarity of the proposed method, it would be beneficial to provide a running example that illustrates the step-by-step process. By walking readers through a concrete example, they can more easily grasp the methodology and its application.\n\n2. To better ground the paper in the existing literature, it would be valuable to provide a more detailed and comprehensive literature review. By thoroughly reviewing relevant prior research, the paper can establish its position within the broader academic discourse and highlight its unique contributions."
                },
                "questions": {
                    "value": "What could be the challenges of applying the proposed method in larger models, such as OPT, LLaMA, as mentioned in the future work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Reviewer_GB37"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698320149178,
            "cdate": 1698320149178,
            "tmdate": 1699636845058,
            "mdate": 1699636845058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lsTFdoipBH",
                "forum": "mYWsyTuiRp",
                "replyto": "1qY73zHh8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer GB37"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments!\n\n# Providing running examples\n> Weaknesses 1  \n> To enhance the clarity of the proposed method, it would be beneficial to provide a running example that illustrates the step-by-step process. By walking readers through a concrete example, they can more easily grasp the methodology and its application.\n\nWe agree that providing concrete examples would be helpful to readers. We will add them to the camera-ready version.\n\n# More comprehensive review of existing literature\n> Weaknesses 2  \n> To better ground the paper in the existing literature, it would be valuable to provide a more detailed and comprehensive literature review. By thoroughly reviewing relevant prior research, the paper can establish its position within the broader academic discourse and highlight its unique contributions.\n\nAs for the additional literature related to this study, for example, there is research on not only reverse engineering of transformer models (mechanistic interpretability) [Elhage+\u201921,\u201922] but also research on comparing transformer models with human behavior [Oh&Schuler\u201922]. Unfortunately, due to space limitations, it is not immediately possible to add them to the paper; we will try to enhance the Introduction and Related work to be more comprehensive as much as possible in the camera-ready version.  \n\n[Elhage+\u201921] A Mathematical Framework for Transformer Circuits (Anthropic) https://transformer-circuits.pub/2021/framework/index.html  \n[Elhage+\u201922] Softmax Linear Units (Anthropic) https://transformer-circuits.pub/2022/solu/index.html  \n[Oh&Schuler\u201922] Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal (EMNLP 2022) https://aclanthology.org/2022.emnlp-main.632/\n\n# Challenges in applying to larger models\n> Questions  \n> What could be the challenges of applying the proposed method in larger models, such as OPT, LLaMA, as mentioned in the future work?\n\nAlthough we used relatively small transformer models to demonstrate our proposed analysis method, analyzing LLMs is an important future work. Our method can be extended to LLMs without any technical modification, but it generally requires a computational cost typically beyond an academic resource. Formally, given an input of length $N$ and a model containing FF's intermediate dimension $d$, estimating one attention map requires $Nd$ times Integrated Gradients computation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498225727,
                "cdate": 1700498225727,
                "tmdate": 1700498225727,
                "mdate": 1700498225727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lMLHkrRpJT",
            "forum": "mYWsyTuiRp",
            "replyto": "mYWsyTuiRp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_wwew"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_wwew"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use attention map to analyze the effects of feed-forward blocks in transformers. Different from previous works which mostly focus on studying attention weights, the authors leverage norm-based analysis and integrated gradient methods on the the effect of feed-forward networks, residual connection, and layer normalization. Experiments done one different models (including different sizes of BERT and RoBERTa) and on different dataset (Wikipedia and Stanford Sentiment Treebank) suggest that feedforward networks (FF) amplify specific types of linguistic compositions, and its surrounding components tend to cancel out each other's contextualization effects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper studies the contextualization effects of feed-forward blocks by leveraging attention maps, which were mostly ignored before (by only looking at attention weights). This presents new views on analyzing what each block in transformer is functioning.\n2. The findings that FF and surrounding components tend to cancel out each other's effects are interesting. This may provide more perspectives in designing and training transformer models."
                },
                "weaknesses": {
                    "value": "1. The paper suggests that because of the cancelled out effects in surrounding components there is \"potential redundancy in transformer layers\". However, there is not enough evidence to justify this claim (e.g., by training a transformer model removing some of the components). Without more experiments and results, it is not convincing what the conclusion of this paper is. More importantly, there is no systematic evaluation on the linguistic patterns (e.g., linguistic patterns distribution from different datasets on each layer) apart from some sampled amplified pairs. The results presented in Table 1 and Figure 5 are not evident.\n2. The results on different BERT sizes, and different seeds do not seem to be always consistent (e.g., Figure 9, 10 in the appendix)."
                },
                "questions": {
                    "value": "1. Why is b set to a zero vector in equation 12?\n2. Why do you think the patterns of changes between BERT and GPT-2 are quick different in each component (e.g., from Fig. 3)? What does this entail for different architectures?\n3. Are results from Section 5 averaged across positions? Would position representation bias your findings?\n4. How is the micro contextualization change (by subtracting a pre-FF attention map) in Section 5.2 different from measuring correlations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Reviewer_wwew"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699509750884,
            "cdate": 1699509750884,
            "tmdate": 1700631235358,
            "mdate": 1700631235358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fn9RlxBxRb",
                "forum": "mYWsyTuiRp",
                "replyto": "lMLHkrRpJT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer wwew"
                    },
                    "comment": {
                        "value": "We appreciate your insightful review!\n\n# Limitations of some experiments\n> Weaknesses 1  \n> The paper suggests that because of the cancelled out effects in surrounding components there is \"potential redundancy in transformer layers\". However, there is not enough evidence to justify this claim (e.g., by training a transformer model removing some of the components). Without more experiments and results, it is not convincing what the conclusion of this paper is. \n\nThe results in Section 6 revealed that contextualization effects by the Feed-forward network (FF) are canceled by the surrounding components. We agree that this *only suggests the potential redundancy* in the transformer layers. That is why we hedged with the words \u201cpotential\u201d, \u201csuggest,\u201d or \u201cimply\u201d in the paper. It would be worthwhile to directly investigate the redundancy of FF, e.g., by observing the model\u2019s behavior change while removing or pruning them. However, we fear that adding such experiments exceeds the focus of a single conference paper. We have refined the manuscript (Section 6) to state the need for further investigation explicitly.\n\n> More importantly, there is no systematic evaluation on the linguistic patterns (e.g., linguistic patterns distribution from different datasets on each layer) apart from some sampled amplified pairs. The results presented in Table 1 and Figure 5 are not evident.\n\nSection 5.2 shows that FF emphasizes specific linguistic compositions, at least across two different models, BERT-base and GPT2 (Figure 5). We agree that the current validation is not fully systematic and this problem can be alleviated by conducting additional experiments on other datasets. We will address this point in the camera-ready version. Nevertheless, it is worth noting that the essential contribution of this study is in part that it provides the new interpretation method and we hope that the acceptance of our study will facilitate more extensive linguistic analyses as you suggested in this community.\n\n# Consistency of results across different sizes and seeds\n> Weaknesses 2  \n> The results on different BERT sizes, and different seeds do not seem to be always consistent (e.g., Figure 9, 10 in the appendix).\n\nIn Section 5.1, we investigated how much FF, RES2, and LN2 change contextualization in each layer. FF and LN2 tend to change largely in the middle to latter layers in most BERT models with different sizes or seeds. The main claims of this section are: (i) FF block changes contextualization; and (ii) the changes are strong in particular layers rather than even in all layers. At least these points were consistently observed across these models (Figures 9 and 10).\n\n# Reasons for setting a zero vector for the baseline vector in Integrated Gradients\n> Questions 1  \n> Why is b set to a zero vector in equation 12?\n\nThere are mainly two reasons:\n1. Using zero vector is one of the most common treatments employed as the neutral baseline for Integrated Gradients [Bastings&Filippova\u201920; Nayak+\u201921].\n2. Since the activation function $g$ in FF satisfies $g(0) = 0$, we can ignore a particular term (output relative to baseline input) when applying Integrated Gradients with the zero baseline vector. This allows Integrated Gradients to fully additively decompose the output into the inputs (Appendix B.2), which is desirable for combining with the norm-based approach.\n\n[Bastings&Filippova\u201920] The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? (BlackboxNLP 2020) https://aclanthology.org/2020.blackboxnlp-1.14/  \n[Nayak+\u201921] Using Integrated Gradients and Constituency Parse Trees to explain Linguistic Acceptability learnt by BERT (ICON 2021) https://aclanthology.org/2021.icon-main.11/"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497959282,
                "cdate": 1700497959282,
                "tmdate": 1700497959282,
                "mdate": 1700497959282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8YRm4Svov0",
                "forum": "mYWsyTuiRp",
                "replyto": "XJSvwLfe1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Reviewer_wwew"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Reviewer_wwew"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. I have adjusted my evaluation accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631265733,
                "cdate": 1700631265733,
                "tmdate": 1700631265733,
                "mdate": 1700631265733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dFCxnY3xfx",
            "forum": "mYWsyTuiRp",
            "replyto": "mYWsyTuiRp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_Dtpy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_Dtpy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a way of analyzing feedforward blocks (=FFB) [including residual connections and the layer norm layers intervening] extrapolating from the typical attention maps of the mid and late 2010s and the slightly more recent concept of a \"refined\" attention map that also considers the values and output projection (Equation 8). They call this base formulation ATB. The authors further extend this notion to incorporate FFBs. The FFB's nonlinearity which makes it additively non-decomposable [a-la linearity of expectation] is overcome by using the integration gradients paper [Sundararajan et al, 2017]. The three resultant formulations, which incorporate just the linear, thenceforth residual connections and layer norm are christened ATBFF ATBFFRES and ATBFFRESLN. The Pre-Layer-Norm variants are [like the post-LayerNorm one before] named likewise. \n\nThe authors analyze a good variety of encoder only LM checkpoints in addition to a decoder only lM checkpoint [GPT2-117M] , which is also an instance of the pre-LN formulation.\n\nThe authors then interestingly proceed to analyze the contextualization changes caused by the FF block , both in terms of extent of change [based on flattening the pairwise values and taking Spearman Correlation of before-after], linguistic contextualization, as well as the dynamics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Their formalism is extended to both pre and post Layer Norm variants of transformers.\n- Tested on a large variety of encoder LM architectures.\n- Decoder-only architectures are also covered [though just one, i.e. GPT2-117M]."
                },
                "weaknesses": {
                    "value": "- It would have been nice if the authors could have discussed and potentially also experimented with atleast one alternative formulation to IG, or atleast one of its variants [They do mention other formulations in B.1 Appendix but did not see further broaching of this angle beyond this]\n- [Doesn't apply after authors comprehensively addressed this on 20th Nov] A marginal weakness but one nonetheless [and this is alluded to in future work], would have been nice to see this for a new-age LLM, of which some variants are available at lower or comparable parametrizations to GPT2-117M (e.g. OPT-125M)\n\nPost-Script: The authors exhaustively addressed the second point! Thanks for that!"
                },
                "questions": {
                    "value": "- What is the computational [and memory] complexity of generating these maps? This may sound nitpicky but with increasingly large LLMs which barely fit in the accelerator time and memory bounds whether at training or inference time, this can indeed become a factor and consideration in how widely this gets adapted.\n- I know mechanistic explanation is a somewhat orthogonal paradigm of interpreting large transformer architectures, but it would be nice to have some comments on how this can relate or synergize with that paradigm [if at all]\n- What is the effect of banded local attention [alternating banded local sparse and global attention] are a common part of the architecural recipe in many GPT3 or later LLMs so this would be a valuable insight to have."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7136/Reviewer_Dtpy",
                        "ICLR.cc/2024/Conference/Submission7136/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699592660336,
            "cdate": 1699592660336,
            "tmdate": 1700600275563,
            "mdate": 1700600275563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ofJgxMbSN",
                "forum": "mYWsyTuiRp",
                "replyto": "dFCxnY3xfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer Dtpy"
                    },
                    "comment": {
                        "value": "We appreciate your invaluable comments.\n\n# Alternatives to Integrated Gradients\n>Weaknesses 1  \n>It would have been nice if the authors could have discussed and potentially also experimented with atleast one alternative formulation to IG, or atleast one of its variants [They do mention other formulations in B.1 Appendix but did not see further broaching of this angle beyond this]\n\nYes, we can use another formulation, such as Shapley values (SV) (Appendix B.1), instead of Integrated Gradients (IG) in our proposed method. At least in the case of using SV, it is required to calculate an expected output change over **all permutations** of input features; that is, SV increases in computational cost more rapidly than IG as input becomes longer. This could especially be a severe problem in modern LLM use, such as providing long prompts/few-shot instances to models and/or considering verbose outputs generated by chain-of-though style reasoning; thus, we used IG as a practical choice.  \nIn the camera-ready version, we will add this discussion and an experimental comparison of IG and SV for small models in Appendix.\n\n# Application to a new-age LLM\n>Weaknesses 2  \n>A marginal weakness but one nonetheless [and this is alluded to in future work], would have been nice to see this for a new-age LLM, of which some variants are available at lower or comparable parametrizations to GPT2-117M (e.g. OPT-125M)\n\nWe are starting to experiment on OPT-125M. We may be able to share results during the discussion period. In any case, we plan to add it to the camera-ready version.\n\n# Computational cost of the proposed method\n> Questions 1  \n>What is the computational [and memory] complexity of generating these maps? This may sound nitpicky but with increasingly large LLMs which barely fit in the accelerator time and memory bounds whether at training or inference time, this can indeed become a factor and consideration in how widely this gets adapted.\n\nGiven an input of length $N$ and FF's intermediate dimension $d$, estimating one attention map requires the computation of Integrated Gradients $Nd$ times. In other words, the computational cost increases with input length and model size. Fortunately, our preliminary analysis shows that the practical computational cost (speed and memory use) seems to be proportional or better than the linear increase with respect to input length and model size ($N$ and $d$) due to some implementation tricks and parallelism (e.g., provided by captum https://captum.ai/).\n\n# Relation to mechanistic explanation\n> Questions 2  \n> I know mechanistic explanation is a somewhat orthogonal paradigm of interpreting large transformer architectures, but it would be nice to have some comments on how this can relate or synergize with that paradigm [if at all]\n\nWe recognize that mechanistic interpretability/explanation is an attempt to reverse engineer neural networks at the algorithmic level. We believe that our focus, \u201chow the information of a particular token propagates to surrounding tokens in the model (similar to MOV or ADD commands in assembler),\u201d can be an important perspective and provide a contribution there. In particular, our study may facilitate understanding MLPs (feed-forward networks), which has been difficult to interpret mechanistically [Elhage+\u201922]. We will explicate this point in the camera-ready version (currently, the paper has space limitations, and we will modify the paper after the discussions converge).\n\n[Elhage+\u201922] Softmax Linear Units (Anthropic) https://transformer-circuits.pub/2022/solu/index.html\n\n# Local sparse and global dense attention in recent LLMs\n> Questions 3  \n> What is the effect of banded local attention [alternating banded local sparse and global attention] are a common part of the architecural recipe in many GPT3 or later LLMs so this would be a valuable insight to have.\n\nThanks for your insightful comment. We will mention this as a future work in the camera-ready version. We hypothesize that the FF affects local and global attention differently; for example, a similar observation as this study (Figure 5) may be observed for local contextualization but, global attention may be affected differently, such as FF selectively expanding specific contextual information that is related to the target token."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497651438,
                "cdate": 1700497651438,
                "tmdate": 1700497651438,
                "mdate": 1700497651438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rqcWIowKNt",
                "forum": "mYWsyTuiRp",
                "replyto": "2JBy3yYUxg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Reviewer_Dtpy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Reviewer_Dtpy"
                ],
                "content": {
                    "title": {
                        "value": "That's great!"
                    },
                    "comment": {
                        "value": "Thanks for the insightful update and running this impromptu at short notice ... this makes the range of models covered much more solid"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600169209,
                "cdate": 1700600169209,
                "tmdate": 1700600169209,
                "mdate": 1700600169209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BSnyN78Hon",
            "forum": "mYWsyTuiRp",
            "replyto": "mYWsyTuiRp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_Ur6S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7136/Reviewer_Ur6S"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed methods to analyzed the feed-forward blocks with regards to the input contextualization. It leverages the completeness property of existing norm-based analysis and the integrated gradient method.\n\nThe motivation to analyze the feed-forward blocks include the following:\n1. The feed-forward blocks account for 2/3 of the layer parameters.\n2. There is a growing interest in feed-forward blocks (new approaches focusing on the feed-forward blocks)\n3. Previous work reported that feed-forward blocks perform some linguistic operations\n\nTheir experiments using masked-LM and casual-LM have shown that feed-forward blocks modify the input contextualization by amplifying specific types of linguistic compositions. Feed-forward blocks and layer normalization largely control contextualization. \n\nThey also found that feed-forward block and other blocks cancel out each other's contextualization effects, which might indicates redundancy in the Transformer computations. (Feed-forward blocks' effects are weekend by surrounding residual and normalization layers.)"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is the first to analyze the whole feed-forward blocks, including the non-linear activation function. The non-linear activation function has been previously excluded from the norm-based analyses because it cannot be decomposed additively by the distributive law.\n\nCombining the norm-based and the integrated gradients, the paper is able to quantify and visualize the effects of the whole feed-forward block.\n\nThe paper also provides detailed analysis that points to interesting properties of the feed-forward blocks. The discovery of redundancy might leed to new improvement of the architecture."
                },
                "weaknesses": {
                    "value": "The author pointed out that the future work might be working with the latest large language model."
                },
                "questions": {
                    "value": "I think it might help the reader with a description or definition of contextualization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699598358822,
            "cdate": 1699598358822,
            "tmdate": 1699636844691,
            "mdate": 1699636844691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fw5UDebskW",
                "forum": "mYWsyTuiRp",
                "replyto": "BSnyN78Hon",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review by Reviewer Ur6S"
                    },
                    "comment": {
                        "value": "We appreciate your insightful feedback and positive reviews.\n\n# Application to the latest large language models\n>Weaknesses  \n>The author pointed out that the future work might be working with the latest large language model.  \n\nWe agree that this direction is an attractive future work, given that the LLMs\u2019 internal mechanisms have been less understood. Our method can be extended to LLMs, and as a first step, we are starting experiments with OPT-125M (we may be able to share results during the discussion period). We will append these results in the camera-ready version at the latest. \n\n# Description or definition of contextualization\n>Questions  \n>I think it might help the reader with a description or definition of contextualization.\n\nWe have clarified what is referred to by \u201ccontextualization\u201d in the second paragraph of the Introduction (Section 1) and the second paragraph of the Background (Section 2)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497414985,
                "cdate": 1700497414985,
                "tmdate": 1700497414985,
                "mdate": 1700497414985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]