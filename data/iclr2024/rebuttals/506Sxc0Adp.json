[
    {
        "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data"
    },
    {
        "review": {
            "id": "g3Ti9oyqSu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_Z6o3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_Z6o3"
            ],
            "forum": "506Sxc0Adp",
            "replyto": "506Sxc0Adp",
            "content": {
                "summary": {
                    "value": "This paper proposes a \u2018diversity coefficient\u2019 that can measure the diversity in certain dataset and a \u2018cross diversity coefficient\u2019 that measure the difference between two datasets. Experiments show that the coefficient has three ideal characteristics: the coefficient increases as more datasets are concatenated, the number of latent concepts increases, and a richer vocabulary is used. Besides, the proposed efficient also show that the current public pre-training datasets has relatively high diversity, which could explain why we can get good LLM trained on them."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The dataset quality is a very important topic but unfortunately not well-defined and studied. It is great that this paper devotes to this novel and critical topic.\n\n2) Measuring the dataset quality in terms of diversity is a nice and neat idea, authors also propose ideal characteristics of the diversity metric and ways to justify them."
                },
                "weaknesses": {
                    "value": "1) I understand that authors want to show the diversity coefficient can work by comparing real data diversity with theoretical lower and upper bound. However, if we treat lower bound as baseline, it is too weak to say the dataset having better diversity is really diverse?\n\n2) Since this approach requires finetuning LM, it is actually introducing some computational overhead. I understand this overhead is not large, but it should be stated in the limitation section.\n\n3) More recent data-centric related works should be added. Most related works listed are before 2022. Only two citations are after 2023, one is GPT-4, and the other is Palm 2, neither of which are references to Data Centric methods. To my knowledge, there is at least one paper about increasing data diversity from Meta AI in 2023.08. \u201cD4: Improving LLM Pretraining via Document De-Duplication and Diversification\u201d. Probably can add to the Related Work?\n\n4) Since the coding data is increasing important, authors should provides more insights about the diversity of code pre-training dataset.\n\n5) Some conclusions are not well-justified.  In appendix G, experiments conclude that \u2018higher data diversity leads to higher test performance\u2019, but fail to ablate the influence of dataset overlap. In fact, the Pile dataset itself contains both the USPTO and PubMed, so I think model trained on the \u2018USPTO+PubMed\u2019 is better than models trained on either of them cannot purely attribute to the diversity increase. It\u2019s possible that the high diversity is not the key factor, the actual factor is high diversity increase the overlap between train and test set, and therefore increase the test performance. In addition, the table in Appendix G should be further clarified. It\u2019s hard to get which dataset is the training dataset and which one is the eval set.\n\nIn general, I think this paper is good for me. Although there are some unclear statements, it is okay because this topic is very new and not well-defined. I can consider further increasing the score if the authors solve my concerns well."
                },
                "questions": {
                    "value": "1) The introduction of Task2Vec in the \u2018method\u2019 section is brief and should mention the further explanation in the Appendix A(I got stuck in this part for a while). Besides, I am kind of confusing about the dimension of vector\u2019s embedding. According to the formula I think the diagonal dimension should equal to the number of model\u2019s parameters. If that\u2019s true the vector will be extremely large\u2026\n\n2) The setting \u201cRandomly initialized GPT-2 without fine-tuning\u201d in Section 4 makes me confused. Does that mean you only reset the LM head or the whole model? My concern is different random initialization could lead to very different results. So it\u2019s not safe to conclude that \u2018using a random probe network (always)**underestimates** diversity\u2019.\n\n3) What\u2019s the relationship between the diversity efficient and the cross diversity efficient? What insight can we get by comparing them together?\n\n4) small typo, \u2018due\u2019 should be \u2018due to\u2019 in Sec 3.4\n\n5) In table 1, why concatenating C4 and Wikitext-103 decrease the Div Coeff? Any analysis on this exception?\n\n6) In table 1, what does the \u2018Combination of Five Datasets(MIX2)\u2019 mean? The explanation of MIX2 setting is not very clear to me. Why only five datasets? Why 0.77 vs 0.23?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697441185878,
            "cdate": 1697441185878,
            "tmdate": 1699636734437,
            "mdate": 1699636734437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q74C2M6Dbl",
                "forum": "506Sxc0Adp",
                "replyto": "g3Ti9oyqSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to great questions"
                    },
                    "comment": {
                        "value": "We thank Reviewer Z6o3 for their time and feedback! Replying to each comment, suggestion and/or question in turn:\n\nThis is an interesting point! One of the main reasons we used the theoretical lower/upper bounds is because the diversity coefficient is a completely new metric for quantifying diversity formally. Therefore, before this work there was no sense what the minimum/maximum would be for this metric. They would look like random number without no reference even if we used any real data set. Therefore, we verified that 1. the metric indeed behaves well 2. and even though the lower/upper bounds are adversarial, given a real sense of if the diversity numbers the coefficient reports behave well. We do acknowledge that the lower bound might be a simple baseline but believe it\u2019s a crucial baseline due to the novelty of the diversity coeff. However, we did try to provide a real world sense/baseline by providing 12 different real world NLP data data sets \u2013 where 3 (NIH, USPTO, PUBMED) are single topic/area data sets. Using this we can see the ratio of the least diverse to most diverse is d(Pile-cc)/d(NIH)=0.2497/0.15=1.67, which still suggests the web crawled type data are nearly twice as diverse! We are happy to emphasis this later point and include in the discussion section the nuances and importance of the upper/lower bounds in our work.\nThank you for pointing it out! We will include this weakness in the discussion and include in the future work that we plan to study the effectiveness of our method without any fine-tuning and only with a zero shot prompt. Though despite using fine-tuning we only fine-tune the final layer!\nWe are happy to compute diversities for pre-training data sets coding datasets, great idea! \nGreat observation! We are happy to address it clearly in the revision. We tried addressing it via two ways: 1. we used OpenWebText2 to avoid overlaps with the training set and we still observed the described trend (with lower CE losses). 2. In figure 6 in the appendix, we computed the \u201calignment coefficient\u201d of the train and the test data and observed a positive correlation of r=0.8, meaning that the story is more nuanced as you hinted & alignment in train and test are important. 3. we do want to emphasizes that we used the test sets at eval so although there is overlap in terms of concepts/type of data there isn\u2019t \u201ccheating\u201d in that we didn\u2019t train in the test set. In addition, when the data is more diverse the likelihood of training on more relevant data is increased which is one of the reasons diversity might be improving performance. We are happy to add these observations in the discussion and motivate our future work that proposes a combination of both the diversity coefficient (~coverage) and alignment as a data quality metric.\n\nAnswers to questions:\nWe are happy to emphasize the remarks made in appendix A in the revisions ot our paper. In addition, we are happy to also remind the reader that a perturbation to a weight in governed (to 2-order approx) by the Fisher information matrix, thus, suggesting it is a good finger print measuring the importance of weights to a specific task. This is currently true that the vector is of the size of the parameters. We can include this in the discussion section and some ways to remedy it e.g., via dimensionality reduction + if we store the vectors we don\u2019t have to recompute them which makes it easier to work with them.\nI think this is a correct observation that we over stated the interpretations. We are happy to adjust commensurate to what we actually did. We do hypothesize they conclusions are likely valid because the all random initialization are around the origin with some distribution related to a Gaussian, so we hypothesize our observations hold on average. We want to emphasize that the most important thing is to use a consistent probe network throughout so that diversity of coefficients are comparable. \nThe main insight from using the diversity coeff. vs. cross-diversity is that our conclusions are robust to different ways one could reasonable define diversity. Which strengthens the observations of the high diversity of LLM open training data sets. The empirical relationship of diversity & cross-diversity is that the former results higher values because it does not mix sequences between data sets. So different data sets are compared directly. An advantage of cross-diversity is that it can also be used to compute the average difference between two data sets cleanly.\nHappy to fix!\nInteresting question. We first observe that the difference is only 0.002 between the two and the confidence interval for C4+WikiText is 0.001 which makes the two values much closer. An alternative analysis using Cohen\u2019s d to measure effect size reports the difference between the two is 0.1585, while this effect size is usually considered low in classical statistics."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507694471,
                "cdate": 1700507694471,
                "tmdate": 1700507694471,
                "mdate": 1700507694471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7B28CfZLEj",
                "forum": "506Sxc0Adp",
                "replyto": "5DhIKnItrz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_Z6o3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_Z6o3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I have read the rebuttal and decided to maintain my score.\nA kind reminder is, please consider writing the reply in a point-to-point manner, such as:\nQ1: ...\nA1: ...\nor so\n\nI spent quite a few minutes to figure out which question are you replying to."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532921562,
                "cdate": 1700532921562,
                "tmdate": 1700532921562,
                "mdate": 1700532921562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EI0G95DiKz",
                "forum": "506Sxc0Adp",
                "replyto": "g3Ti9oyqSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Your Consideration"
                    },
                    "comment": {
                        "value": "Hello,\n\nThank you for considering our response! We appreciate the time you\u2019ve taken to consider the strengths of our paper, and respect your decision with regards to our score.\n\nWe apologize for the lackluster formatting. Thus, we\u2019ve enhanced and structured our response below as a matter of good form. \n\n### Of course, we completely understand if your mind is made up, and, to be clear, *we have no expectation whatsoever of you reading our below responses.* \n\n(All that being said, we would of course be very grateful if you found a few minutes to give the bolded portions a skim!)"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654280124,
                "cdate": 1700654280124,
                "tmdate": 1700693915914,
                "mdate": 1700693915914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dkbeM5vnVf",
                "forum": "506Sxc0Adp",
                "replyto": "g3Ti9oyqSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Insightful Critique | Weaknesses 1 - Question 1"
                    },
                    "comment": {
                        "value": "Thank you for your review of our work! We\u2019re pleased to know that we have a shared belief in the importance of understanding the role of data quality in the development of LLMs, and that you find our introduction of the diversity coefficient as way of conceptualizing quality a merit of our paper. \n\nWe also believe you raise some thoughtful points of critique, and we address them below. We fully acknowledge that our responses here are relatively long; we endeavored to respond to your insightful critiques thoroughly, and hope our responses help you better evaluate the merits of our paper. \n\n### *Please feel free to skim the bolded sentences to get a quick overview of our main points.*\n\n\n## Weakness 1\n\nThis is an interesting point! One of the main reasons we used the theoretical lower/upper bounds is because the diversity coefficient is a completely new metric for quantifying diversity formally. Therefore, before this work there was no sense what the minimum/maximum would be for this metric.  **E.g. if the lower bound (uniformly 1 token) is 0.15 and the upper bound (random tokens) is 0.2, then a coefficient of 0.16 seems low. Alternatively, if the lower bound is 0.0 and the upper bound is 0.175, a coefficient of 0.16 seems high.**  Therefore, we verified that 1. the  **metric indeed behaves as expected at these extremes**  and 2. real world data sits at a reasonable point in relation to these extremes. \n\nIn addition, we also provide  **real world, natural data reference points by analyzing 12 different real world NLP datasets. Three of these (NIH, USPTO, PUBMED) are single topic/area data sets, which are highly uniform in content and style/format and, accordingly, have relatively low diversity coefficients.**  Hence, we see that the ratio of the least diverse to most diverse datasets tested is d(Pile-cc)/d(NIH)=0.2497/0.15=1.67, which still suggests the web crawled type data are nearly twice as diverse as single-source, narrow-domain data. We are happy to  **emphasize this later point and include in the discussion**  section the nuances and importance of the upper/lower bounds in our work.\n\n## Weakness 2\n\nWe agree that this is an important limitation to mention, and will **include it in our discussion.** Though an important note is that despite using fine-tuning, we only fine-tune the final layer.\n\n## Weakness 3\n\nWe are  **happy to add more recent data-centric works to our Related Work.**  In particular, D4\u2019s support of semantic deduplication methods appears to be of strong support and relatedness to the diversity coefficient. In addition, works like DoReMi (by Xie et. al.) also signal the importance of intelligent choice of data mixtures i.e. sub-datasets and their proportions.\n\n## Weakness 4\n\nWe are happy to  **compute diversities for pre-training data sets coding datasets.** \n\n## Weakness 5\n\nGreat observation! We are happy to address it clearly in the revision. We tried addressing it via two ways: \n1. We used  **OpenWebText2 to avoid overlaps**  with the training set and we  **still observed the described trend (with lower CE losses).** \n2. In Figure 6 in the appendix, we computed the \u201calignment coefficient\u201d of the train and the test data and observed a positive correlation of r=0.8, meaning that the story is more nuanced as you hinted & alignment in train and test are important. \n3. We have gathered new results, evaluating the experimental models  **on Pile-CC, which has no overlap with any training dataset. We again observe lower CE losses with higher training data diversity. We would be happy to update our figure in Appx. G with this result.**\n4. We do want to emphasizes that we used the validation sets at eval, so although there is overlap in terms of concepts/type of data, the test data has never been seen during training. In addition, when the data is more diverse, the likelihood of training on more relevant data is increased, which is one of the reasons greater diversity likely improves performance. We are happy to  **add these observations in the discussion**  and motivate our future work that proposes a combination of both the diversity coefficient (~coverage) and alignment as a data quality metric.\n\n## Question 1\n\nWe are happy to  **emphasize the remarks made in appendix A in the revisions of our paper.**  In addition, we are happy to also remind the reader that a perturbation to a weight in governed (to 2-order approx) by the Fisher information matrix, thus, suggesting it is a  **good fingerprint measuring the importance of weights to a specific task.**  It is true that the **embedding vector is of the size of the parameters.**  We can include this in the discussion section and some ways to remedy it e.g., via dimensionality reduction + if we store the vectors we don\u2019t have to recompute them which makes it easier to work with them."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654436234,
                "cdate": 1700654436234,
                "tmdate": 1700694062383,
                "mdate": 1700694062383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wXEi2csEVh",
                "forum": "506Sxc0Adp",
                "replyto": "g3Ti9oyqSu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 2 - Question 6"
                    },
                    "comment": {
                        "value": "## Question 2\n\nI think this is a  **correct observation that we over stated the interpretations. We are happy to adjust commensurate to what we actually did.**  We do hypothesize the conclusions are likely valid because all random initialization are around the origin with some distribution related to a Gaussian, so we hypothesize our observations hold on average. We want to emphasize that the most important thing is to use a consistent probe network throughout so that diversity of coefficients are comparable. \n\n## Question 3\n\nThe main insight from using the diversity coeff. vs. cross-diversity is that our conclusions are  **robust to different ways one could reasonable define diversity,**  which strengthens the observations of the high diversity of open-source LLM training datasets. The empirical relationship of diversity & cross-diversity is that the latter results in higher values because all text for a batch embedding is drawn from a _single_ sub-dataset, rather than randomly from any dataset. Hence, embeddings of different sub-datasets often push up average distance between embeddings given the clean informational separation between the batches of text. \n\nTherefore,  **cross-diversity allows different sub-datasets to be directly compared directly with one another, which can be important if one is aiming to construct corpora by using diverse _subsets_ of data**  (as is the case with The Pile). (Normal) diversity, on the otherhand, is very useful to assess the variation of a dataset when  **agnostic to or lacking information about specific sub-datasets used.**\n\n## Question 4\n\n**Happy to fix!**\n\n## Question 5\n\nInteresting question. We first observe that the difference is only 0.002 between the two and the confidence interval for C4+WikiText is 0.001, which means the two values are very close. An alternative analysis using Cohen\u2019s d to measure effect size reports the difference between the two is 0.1585, and this effect size is usually considered low in classical statistics. In addition, the cross-diversity coefficient shows the diversity is greater than C4 alone, demonstrating that there is  **significant informational difference _between_ C4 and WikiText.**  Hence, we conclude that  **since C4 is already very diverse, adding in text that\u2019s diverse from C4 (namely WikiText) doesn\u2019t lead to the _overall_ diversity of C4+WikiText changing much,**  as the level of variation amongst batches of text (agnostic of what subset it comes from) is still just about as high after as it was before.\n\n## Question 6\n\nGreat question, we will  **clarify the detail in the appendix in the revision + our code will make it fully transparent where these numbers come from.**  \n\nThe mixture proportions were computed such that they were faithful to the original data mixture used for LLaMAv1. For example, if LLaMAv1 had a ratio of crawl data to wiki data of 2:1 then we choose our mixture proportions to match that as closely as possible. We chose LLaMAv1 in particular because:\n1. We want our diversity values to be reflective of what is done in practice \n2. We chose LLaMAv1 because it\u2019s widely used and open mixture proportions are public/open."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654506818,
                "cdate": 1700654506818,
                "tmdate": 1700654882790,
                "mdate": 1700654882790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6iGYIrRf8j",
            "forum": "506Sxc0Adp",
            "replyto": "506Sxc0Adp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_FQiZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_FQiZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the quality of data used for training Large Language Models. It introduces the Task2Vec diversity coefficient as a metric to evaluate this quality. By analyzing public datasets and conducting interpretability experiments, the authors claim that a higher diversity coefficient indicates better data quality. They suggest that this coefficient could serve as a useful tool for constructing high-quality datasets for training models."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. **Addressing an Important Problem**: The paper tackles the significant issue of data quality metrics for large language models. \n2. **Introduction of Diversity Coefficient in Context**: The paper brings the concept of a diversity coefficient into the discussion of data quality for large language models. \n3. **Reference to Existing Literature**: The paper builds upon existing work, particularly on Task2Vec and Fisher Information Matrix."
                },
                "weaknesses": {
                    "value": "1. **Limited Novelty and Insight**: The paper largely relies on existing methodologies, including Task2Vec diversity coefficient and latent concept analysis, and does not offer new or noteworthy findings. \n2. **Issues with Model Updates and Task2Vec**: It's not clear if the model weight is updated after computing the Task2Vec embedding for each batch. This is crucial as it affects the validity of the reported results, especially those in Section 4.\n3. **Unexplained Necessity for Task2Vec in Measuring Data Quality**: The paper uses token distribution as a metric for dataset diversity but fails to clarify why Task2Vec coefficients are needed instead of direct token distribution metrics. The lack of this clarification adds confusion and questions the relevance of using Task2Vec for data quality measurement.\n4. **Unclear Practical Utility of Data Quality Metric**: The paper suggests the diversity coefficient as a potential data quality metric but does not empirically validate this claim. This is a significant concern, especially considering the emphasis on Task2Vec and model diversity in the paper.\n5. **Methodological Ambiguities and Undefined Concepts**: The paper contains unclear statements and undefined notations, particularly in the methods section."
                },
                "questions": {
                    "value": "### Major Concerns\n\n1. **Potential Issue with Model Updates and Task2Vec**: Is the model weight updated after the Task2Vec embedding is computed for each batch? If so, doesn't this imply that the diversity should inherently be high since the information from the first batch has already been learned and therefore won't be reflected in the Task2Vec embedding for the second batch? How does this affect the importance of your results in Section 4?\n\n2. **Task2Vec and Token Distribution: Confusion about Relevance to Data Quality**:\n    - In Section 2.2.4, token distribution is used as a metric for dataset diversity. Why then is it necessary to calculate Task2Vec diversity coefficients by fine-tuning the model? How is this relevant to data quality?\n    - Following from the above, Section 3.4 revisits the correlation between diversity coefficient and vocabulary size. Could you clarify why this aspect is repeatedly emphasized?\n\n### Additional Questions\n3. **Ambiguities in Section 2.1**:\n    1. You mentioned \"(partially) fine-tuning\" the final layer of a fixed neural network to solve the \"current task (or batch)\". Could you clarify what you mean by \"partially fine-tuning\"? Also, how do you define \"solving the current task (or batch)\" in this context?\n    2. The variable $t$ is used but not explicitly defined. Could you specify what $t$ represents and its range of possible values?\n    3. When discussing the expectation over both $t$ and $\\hat{x}_t$, could you elaborate on why and how this expectation is taken?\n\n4. **Minor Formatting Issues**: The in-text citation style appears to be inconsistent across the paper.\n\n5. **Potential Typo on Page 2**: In the contributions section, item 4 states, \"...the **high** diversity of public datasets for LLM pre-training is **high**...\" This appears to be a typo."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451686115,
            "cdate": 1698451686115,
            "tmdate": 1699636734314,
            "mdate": 1699636734314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UjAxkdcxxJ",
                "forum": "506Sxc0Adp",
                "replyto": "6iGYIrRf8j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Thorough Feedback | Weaknesses 1-4"
                    },
                    "comment": {
                        "value": "Thank you for your review of our work! We\u2019re pleased to know that we have a shared belief in the importance of understanding the role of data quality in the development of LLMs, and that you find our introduction of the diversity coefficient, built off strong foundations from prior work, a merit of our paper. \n\nWe also believe you raise some thoughtful points of critique, and we address them below. We fully acknowledge that our responses here are relatively long; we endeavored to respond to your insightful critiques thoroughly, and hope our responses help you better evaluate the merits of our paper. \n\n### *Please feel free to skim the bolded text to get a quick overview of our main points.*\n\n## Weakness 1\n\nThe novelty of our work is the use of previous work to  **measure important, non-trivial concepts, like the inherent diversity of language data, and make novel, quantitative, non-trivial observations of LLM pre-training datasets.**  In addition, we are the first to show with a quantitative diversity metric the  **relationship between diversity and performance (see Appendix G, in Supplementary Materials).**  Hence, we respectfully disagree that our work lacks novelty, as we believe our work is noteworthy by being the first to  **extensively test and propose a formal and rigorous diversity metric for use on real-world datasets used to train foundation models.**\n\n## Weakness 2 (Cont. in Question 1)\n\nBriefly, we use a  **separate, deep-copied probe network for each text batch, such that the updates from finetuning on batch N are not carried over when finetuning on batch N+1.**  Hence, there is no concern of information from one batch affecting the embedding of another batch.\n\nThat being said, we agree that this detail was not clearly explained in the paper, and  **will update section 2.2.3 (in Methods) to clearly state this detail.**\n\n## Weakness 3\n\nAt a high level, Task2Vec is our method of  **capturing the information of a particular sample of a dataset quantitatively;**  we then find the expected (cosine) distance between these Task2Vec embeddings to see how varied the information in a dataset is. Previous work has shown that Task2Vec embeddings appear to  **capture important characteristics of the embedded data [1].**  As such, Task2Vec provides us a much  **richer (and novel) capture of variation of text information than raw token distribution statistics.**  E.g. raw token distributions may only very coarsely detect the topic of text, and do not (explicitly) detect syntactical features like variation in writing style and document format. As explained in Question 2, we explain why, for certain lines of experimentation, we do in fact use token distribution metrics.\n\nOne may then ask why we use Task2Vec embeddings, rather than the more model conventional activations. We use Task2Vec because  **activations were not successful at measuring diversity in our preliminary experiments.**  One major issue we observed was that the distances between activations were consistently very large in magnitude (we observed this behavior on both on Mini-Imagenet and on GPT-2 with random tokens when we varied the vocabulary size from 1 to whole vocabulary). We are happy to provide data and plots we have to illustrate this point. \n\nIn addition, the expected cosine distance between activations was always within a small range, making the  **activations not very good at measuring variation in diversity among datasets.** \n\nFinally, the most concerning part was the large jumps in expected cosine distance we saw from small to large vocabularies,  **implying instability of an activations-based metric.**  \n\nIn contrast, the FIM measures which parameters are important via a second order approximation (the reason fine-tuning the final layer is important). Therefore,  **Task2Vec attempts to obtain a unique task dependent vector (in contrast to activations), and there is strong evidence that it can do so successfully [1].** \n\n## Weakness 4\n\nFirst, we argue that  **it has generally been a commonly held wisdom in the LLM community that using diverse pretraining data benefits downstream model performance.**  For a few illustrative examples, consider the that:\n\n- The creators of GPT-3 made a deliberate effort to create a dataset composed of (qualitatively) diverse data [2] .\n\n- The creation of The Pile dataset was directly motivated by the desire to create an open-source high quality, diverse dataset [3]. A number of leading open-source models are partially trained on The Pile, such as Meta\u2019s OPT models and LLaMA 1 models, Microsoft\u2019s Megatron-Turing NLG 530B, and EleutherAI\u2019s GPT-Neo models.\n\n- The creators of LLaMA 1 made a deliberate effort to train on diverse data, particularly preprocessed web-crawls, which they noted improved performance [4]. Note that the diversity coefficient recognizes that C4 and Pile-CC, pre-processed CommonCrawl datasets, are among the most diverse datasets tested."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653491118,
                "cdate": 1700653491118,
                "tmdate": 1700653491118,
                "mdate": 1700653491118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5ECMi7fEt",
                "forum": "506Sxc0Adp",
                "replyto": "6iGYIrRf8j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weaknesses 4 - Question 2"
                    },
                    "comment": {
                        "value": "### (Weakness 4 cont.)\n- The creators of the TinyStories dataset make a deliberate effort to make their data sufficiently diverse in order to improve downstream model performance [5].\n\nGiven the above evidence of the importance diversity among researchers and practitioners, consider the following concrete use-cases:\n\n### Use-case 1:  **Rigorously characterizing the effect of data diversity on (general) downstream LLM performance.**\n\nAs argued in the above section, there is already a widespread belief among practitioners and researchers that training models on diverse pretraining data likely leads to better general downstream performance on language-based tasks. That being said,  **this conjecture remains highly qualitative and imprecise**  as to e.g. the optimal degree of diversity for pretraining data (e.g. TinyStories appears to indicate ideal data diversity depends on the size of the model being trained [5]).\n\nBy using the diversity and cross diversity metrics, researchers can  **gain a precise, quantitative, and, in our view, well-supported measurement of the inherent variability of a model\u2019s pretraining data. Hence, this empowers researchers to more rigorously analyze how model abilities develop with respect to this important characteristic of the data.**  For example, the diversity metric allows for a rigorous and precise study of how accuracy on a wide-ranging benchmark like MMLU varies when models are trained on more or less diverse pretraining corpora.\n\n### Use-case 2:  **Enabling a better choice of training corpora for the training and development of an LM.**\n\nThe diversity coefficient & cross diversity coefficient can serve as a  **valuable tool practitioners (and researchers) can use to curate their training corpora.**  For instance, many researchers and practitioners operate under limited compute budgets for training their LMs. When curating training corpora, scaling laws provide guidance on the optimal size of the dataset used one should use during training, but this  **leaves ambiguous what characteristics of the data one should optimize for when selecting the N billion tokens one will use**  for training. By  **comparing against the diversity values of widely used/known datasets we provide in our paper,**  the practitioner could test the diversities of candidate datasets to determine whether each is sufficiently diverse (and not overly diverse) for their purposes and choose to train on the data which satisfies their desired level of diversity.\n\nFor another example of using diversity metrics for dataset curation, consider a researcher aiming to create a large natural language corpora of high quality, diverse training data. One could  **use the cross-diversity coefficient between candidate datasets to determine which datasets to include in one\u2019s overall corpora in order to ensure one\u2019s corpora is composed of maximally (or, rather, optimally) diverse subsets.**  In fact, the researcher or practitioner can collect small \u2018sample datasets\u2019 from certain sources (e.g. transcriptions of the most listened to podcasts on Spotify) and  **test whether this data is diverse enough from one\u2019s existing corpora.**  Depending on if the sample data is sufficiently diverse, one could  **make an informed decision as to whether to continue scraping**  the given source, or search for a more diverse source.\n\n## Weakness 5\n\nWe clarify the ambiguities in Question 3, and will  **happily update our writing in methods to reflect greater clarity of these concepts.**\n\n## Question 1\n\n### Potential Issue with Model Updates and Task2Vec\nGreat question! We use a  **separate, deep-copied probe network for each text batch, such that the updates from finetuning on batch N are not carried over when finetuning on batch N+1.**  Hence, there is  **no concern of information from one batch affecting the embedding of another batch.**  We are happy to update our section 2.2.3 to clarify this detail.\n\n###  If so, doesn't this imply\u2026\nWe believe the above point answers this concern. In addition, if this were an issue, it would have showed up in our sanity check when computing the theoretical lower bound of the diversity coefficient, i.e. when the data comes from a distribution with almost all probability mass on one token.  **Since the diversity coefficient for this lower bound is ~0.05, very close to the minimum of zero, the evidence likewise suggests we don\u2019t have this problem.**\n\n## Question 2\n\n### In Section 2.2.4\u2026\n**Task2Vec uses the diagonal of the FIM to compute a \u201cfingerprint\u201d of a task. The FIM can only indicate which weights are important in the new task if some of the weights change.**  This is because the FIM is a 2nd order approximation to a perturbation in the weights i.e., E_{x ~\\sim \\hat{p}}[KL(p_w' || p_w)] = \\delta_w F \\delta_w^\\top + o(delta(w)^2). Therefore, the theory suggests we need  **a change in the direction of the relevant task (or current batch) to get the relevant vector/embedding for that task/batch.**"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653714788,
                "cdate": 1700653714788,
                "tmdate": 1700694419119,
                "mdate": 1700694419119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QsY5tBeoA0",
                "forum": "506Sxc0Adp",
                "replyto": "6iGYIrRf8j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### (Question 2 cont.)\n\nHence, Task2Vec fine-tunes the final layer on the text batch for the task of next-token prediction on the given text sequences. \n\nIntuitively, diverse data is an important factor in preventing model overfitting (e.g. LLMs trained on only dialog-format data would have great difficulty writing an argumentative essay) and  **promoting generality of model ability,**  as the model has been trained to perform well on a wide variety of tasks (i.e. next-token prediction on a wide variety of text). Furthermore, using diverse training data increases the chance that test-time demands are similar to data seen during training,  **providing the model something akin to \u2018in-domain\u2019 performance for a larger number of domains.**\n\nIn this instance, we create synthetic data using extreme token distributions to examine the practical upper & lower bounds that our metric can attain and thereby  **give context to the coefficients of real world datasets.**\n\n### Section 3.4 revisits\u2026\nTo validate the diversity coefficient is a reliable metric for diversity, it has to behave in ways we\u2019d expect a diversity metric would. By including  **experiments on the size of the vocabulary, which is a controlled & clearly interpretable test setting, we show that the diversity coefficient recognizes when data uses a richer vocabularly, i.e. is clearly more diverse.**  \n\nThus, we argue this is evidence that the diversity coefficient is also  **able to recognize this type of richness & diversity of data in cases of natural language corpora,  where the size of vocabulary (or number of latent concepts) in untenable to precisely define.** Furthermore, by experimenting on variation in the vocabulary size, we show that  **the diversity coefficient captures a broad notion of data diversity,**  i.e. not simply variation in number of latent concepts. We are happy to clarify this motivation in our revisions, if you find it would add to the strength of our paper.\n\n## Question 3\n\n### 3.1 \nBy \u201cpartially fine-tune the final layer,\u201d we mean we  **do a limited number of updates with Adam to the final layer of the probe network using the current batch.**  This is required by the Task2Vec method. \n\nBy \u201csolving the current task,\u201d we mean  **\u201cminimize the cross-entropy loss**  on the next token prediction task on the given batch of text sequences\u201d. \n\n### 3.2 \n**$t$ is the index for the current token and, thus, $t$ ranges from 0 to the length of sequence.**  We use this notation to indicate that we take the average across each token of the sequence when computing log loss. \n\n### 3.3 \nThe FIM is the  **expected covariance of the scores**  (gradients of the log-likelihood) with respect to the model parameters. Therefore, to calculate the FIM, we find the  **average log likelihood of predicted tokens** $\\hat{x}\\_t$ **given context** $x\\_{1:t-1}$ **across indices** $t$ **in sequence** $x$ **(conventional log-likelihood in NLP), find the covariance matrix of this loss wrt the probe network parameters** $w$, **and then find the expectation of these covariance matrices across indices** $t$ **in sequence** $x$ **and sequences** $x$ **from batch** $B$.\n\nWe agree that some of these details may be confusing as currently presented in the paper, and **we would be happy to add clarifications of 3.1, 3.2, 3.3 to the paper.**\n\n## Questions 4 & 5\n\n**We\u2019re happy to fix citation formatting errors and typos in the paper.**\n\n## References\n\n[1] Achille et. al. Task2Vec: Task embedding for meta-learning. 2019. https://arxiv.org/pdf/1902.03545. \nKey Quote: \u201cThe FIM is also related to the (Kolmogorov) complexity of a task, a property that can be used to define a computable metric of the learning distance between tasks.\u201d\n\n[2] Brown et. al. Language models are few-shot learners. CoRR, 2020. https://arxiv.org/abs/2005.14165.  \nKey Quote: \u201cOur basic pre-training approach\u2026[includes] scaling up of dataset\u2026 diversity\u201d \n\n[3] Leo Gao et. al. The pile: An 800gb dataset of diverse text for language modeling. 2020. https://arxiv.org/abs/2101.00027. \nKey Quote: \u201cRecent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for [LLMs]. With this in mind, we present The Pile\u2026\u201d\n\n[4] Touvron et, al. Llama: Open and efficient foundation language models. 2023. https://arxiv.org/abs/2302.13971. \nKey Quotes: \u201cOur training dataset is a mixture of several sources\u2026that cover a diverse set of domains\u201d; \u201cDuring exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance\u201d\n\n[5] Eldan et. al. TinyStories: How small can language models be and still speak coherent english?. 2023. https://arxiv.org/abs/2305.07759.\nKey Quote: \u201cThe main challenge in using [LLMs] for producing training data is generating a dataset that is sufficiently diverse\u2026\u201d"
                    },
                    "title": {
                        "value": "Question 2 - Question 5 | References"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654084596,
                "cdate": 1700654084596,
                "tmdate": 1700695149931,
                "mdate": 1700695149931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yNxnfQ1ubn",
                "forum": "506Sxc0Adp",
                "replyto": "QsY5tBeoA0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_FQiZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_FQiZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for providing this long response, though my concerns and questions cover the same points so you could have answered them once. \n\nMy question about the model updates was clarified by the authors. Thank you.\n\nRegarding the necessity of Task2Vec in measuring data quality, the authors responded that \"_richer (and novel) capture of variation of text information than raw token distribution statistics_,\" but I don't see this claim supported by any experiment or theory in the paper, especially regarding its advantage in measuring data quality. \n\nRegarding my concern about the practical utility of the proposed diversity coefficient, I am aware that diversity is important for data curation. While I can imagine all these scenarios the authors mentioned, I was mainly asking for empirical evidence of the effectiveness of using the proposed metric in any of these scenarios, which is missing from the paper and not provided in the response. \n\nOverall, while I appreciate the authors' efforts in writing this long response, my main concerns were not adequately addressed. Therefore, I would like to keep my original rating."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734047389,
                "cdate": 1700734047389,
                "tmdate": 1700734047389,
                "mdate": 1700734047389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d4gMQQZstI",
            "forum": "506Sxc0Adp",
            "replyto": "506Sxc0Adp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_H6fT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_H6fT"
            ],
            "content": {
                "summary": {
                    "value": "This paper begins by observing that current trends in pre-trained large language models (LLMs) mostly focus on scaling model and data size, while ignoring the quality of pre-training data. The author proposes that the quality of pre-training data is also an important factor for training powerful LLMs. To this end, they propose using the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality. The authors also validate the diversity coefficient by demonstrating its interpretability and correlation with intuitive diversity properties aligned with human intuitions, and provide ways to evaluate the diversity coefficient for a given dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and rigorous.\n2. I find the idea of improving the performance of pre-trained large language models from a data quality perspective meaningful. Considering the increasing pre-training cost, using small and high-quality data and avoiding the scaling of model and data size is a good way.\n3. The authors demonstrate that the public datasets for LLM pre-training have high formal diversity using well-motivated lower and upper bounds. They also provide practitioners with simpler ways to evaluate the diversity coefficient. It is more important."
                },
                "weaknesses": {
                    "value": "1. The definitions in sections 2.1 and 2.2 are essential to understanding the paper\u2019s main argument. However, they are somewhat difficult to comprehend. To make these definitions more accessible, I suggest providing more illustrations in addition to citing related papers.\n\n2. More experiments should be conducted to investigate the correlation between diversity coefficient and LLM performance. For example, on more models and datasets with multiple runs. The end goal of this research should be to improve the performance of LLMs."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692241679,
            "cdate": 1698692241679,
            "tmdate": 1699636734197,
            "mdate": 1699636734197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KWPsPFiP1N",
                "forum": "506Sxc0Adp",
                "replyto": "d4gMQQZstI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Insightful Feedback + Weaknesses 1-2"
                    },
                    "comment": {
                        "value": "Hello,\n\nThank you for your review of our work! We\u2019re pleased to know that you appreciate the writing quality of the paper, and that we have a shared belief in the importance of using a data quality-centric perspective when thinking about training LLMs. In addition, we\u2019re glad that you find our analysis of the diversity of public datasets and practical methods for use of our metric compelling! \n\nWe also believe you raise some thoughtful points of critique, and we address them below. \n\n### *Please feel free to skim the bolded sentences to get a quick overview of our main points.*\n\n\n## Clarity of Definitions in Methods Section\n\nWe agree that this portion of the paper is fairly dense at the moment, and that clearer explanations, visual aids, and citing similar work would help better communicate this important mathematical framework.\n\nIn terms of visual aids,  **we include an illustration of the diversity coefficient computation pipeline in Appx. F, Figure 5.**  Unfortunately, we only reference this visual aid near the end of Section 2.2, and do not do so with sufficient emphasis. Hence, we will  **revise Sections 2.1 and/or 2.2 to include an earlier and/or emphasized reference to our illustration of the diversity computation pipeline**  so that it may serve as an effective visual aid for understanding Sections 2.1 and 2.2.\n\nIn terms of related works, we also agree that adding in references to past related work would help shore up one\u2019s understanding of our approach. As such,  **we will edit Sections 2.1 and 2.2 to include helpful references to related work,**  such as Edwards et. al. (https://arxiv.org/pdf/1606.02185), a precursor of Task2Vec which similarly uses an unsupervised method of training a probe network on a dataset in order to extract meaningful quantitative information about the characteristics of the dataset, and Ngyuyen et. al., (https://arxiv.org/pdf/1908.01091) which also uses Task2Vec embeddings to determine properties of a certain data sequence (in this case, the \u2018hardness\u2019 of the sequence in a continual learning setting).\n\nWe will also  **generally proofread and revise this section for clarity**  so that our methods are more readily understandable. If you have any other ideas for specific improvements, or believe our above proposed changes may be inadquate, please feel free to suggest!\n\n\n## Experiments on the Relationship Between the Diversity Coefficient and LLM Performance\n\nWe agree that this is an important area for investigation! As such, we have conducted preliminary experiments on models of small scale (using 50M parameter models, trained on ~1.3B tokens; see Appendix G, within Supplementatry Material, for details) and  **found that models trained on more diverse data achieve lower loss (i.e. higher performance) on diverse eval datasets.**  This indicates that a high diversity coefficient of pretraining data lead to a meaningful in downstream model performance.\n\nWe also find that  **it is generally a commonly held piece of wisdom in the LLM community that using diverse pretraining data benefits downstream model performance:**\n\n- The creators of GPT-3 made a deliberate effort to create a dataset composed of (qualitatively) diverse data [1] .\n\n- The creation of The Pile dataset was directly motivated by the desire to create an open-source high quality, diverse dataset [2]. A number of leading open-source models are partially trained on The Pile, such as Meta\u2019s OPT models and LLaMA 1 models, Microsoft\u2019s Megatron-Turing NLG 530B, and EleutherAI\u2019s GPT-Neo models.\n\n- The creators of LLaMA 1 made a deliberate effort to train on diverse data, particularly preprocessed web-crawls, which they noted improved performance [3]. Note that the diversity coefficient recognizes that C4 and Pile-CC, pre-processed CommonCrawl datasets, are among the most diverse datasets tested.\n\n- The creators of the TinyStories dataset make a deliberate effort to make their data sufficiently diverse in order to improve downstream model performance [4]. \n\nGiven we believe the diversity coefficient closely tracks an intuitive human sense of how diverse data is, the above points present further evidence that using  **training data with a sufficiently high diversity coefficient is likely to lead to better downstream performance.**"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653123548,
                "cdate": 1700653123548,
                "tmdate": 1700653526382,
                "mdate": 1700653526382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EXcdfQywRO",
                "forum": "506Sxc0Adp",
                "replyto": "d4gMQQZstI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 2 cont. | References"
                    },
                    "comment": {
                        "value": "### (Weakness 2 cont.)\n\nIn addition, we observe  **compelling empirical evidence that diverse pretraining data is an important driver of In-Context Learning (ICL) in LLMs:**  \n\n- A \u2018mixture of concepts\u2019 structure in pretraining data is key for instilling ICL ability, and data with only 1 concept i.e. data with very low semantic and syntactic diversity failed to produce ICL in transformer models [5]. Note that Section 3.4 demonstrates that the diversity coefficient of data strongly correlates with the number of latent concepts in (synthetic) data.\n\n- Training models on qualitatively diverse natural language corpora strengthened models\u2019 ICL ability, such as using training corpora composed of intuitively cross-diverse sub-datasets [6]. \n\n- Training transformers on data with a large number of rarely occurring classes (in this case, character symbols), and with dynamic item meanings and interpretations lead to higher ICL ability [7]. \n\nThough our main goal of our paper is to establish and support the diversity coefficient as a measure of data diversity (and, thus, we leave more significant experimentation on the relationship between the diversity coefficient and performance to future work), we still agree that we did not address this topic sufficiently compellingly in our introduction, results, and/or discussion sections. \n\nHence, if you believe it would contribute to the strength of our paper, we would be happy to:\n\n- **Edit and revise our introduction and/or discussion**  to better reflect our understanding of the connection between data diversity and model performance.\n\n- **Expand our results from appendix G by conducting a similar experiment with ~125M parameter models trained on ~2.5B tokens**  (using ~3-4 pretraining datasets that vary in diversity), and evaluating the downstream performance of these models on diverse text.\n\n\n## References\n\n[1] Brown et. al. Language models are few-shot learners. CoRR, 2020. https://arxiv.org/abs/2005.14165.  \nKey Quote: \u201cOur basic pre-training approach\u2026[includes] scaling up of dataset\u2026 diversity\u201d \n\n[2] Leo Gao et. al. The pile: An 800gb dataset of diverse text for language modeling. 2020. https://arxiv.org/abs/2101.00027. \nKey Quote: \u201cRecent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present The Pile\u2026\u201d\n\n[3] Touvron et, al. Llama: Open and efficient foundation language models. 2023. https://arxiv.org/abs/2302.13971. \nKey Quotes: \u201cOur training dataset is a mixture of several sources\u2026that cover a diverse set of domains\u201d; \u201cDuring exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance\u201d\n\n[4] Eldan et. al. TinyStories: How small can language models be and still speak coherent english?. 2023. https://arxiv.org/abs/2305.07759.\nKey Quote: \u201cThe main challenge in using large language models for producing training data is generating a dataset that is sufficiently diverse\u2026\u201d\n\n[5] Xie et. al. An explanation of in-context learning as implicit bayesian inference. 2021. https://arxiv.org/abs/2111.02080\nKey Quote: \u201cWhen pretrained with only one concept, in-context learning fails.\u201d\n\n[6] Shin et. al. On the effect of pretraining corpora on in-context learning by a large-scale language model. 2022. https://arxiv.org/pdf/2204.13509.\nKey Quote: \u201cOur study shows that diverse pretraining corpora strengthen the ability of in-context learning.\u201d\n\n[7] Chan et. al. Data distributional properties drive emergent in-context learning in transformers. 2022. https://arxiv.org/pdf/2205.05055. \nKey Quote: \u201c[Data with] a large number of rarely occurring classes [promotes in-context learning]\u201d"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653223464,
                "cdate": 1700653223464,
                "tmdate": 1700653223464,
                "mdate": 1700653223464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sims5IGPgP",
                "forum": "506Sxc0Adp",
                "replyto": "EXcdfQywRO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_H6fT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_H6fT"
                ],
                "content": {
                    "title": {
                        "value": "To authors"
                    },
                    "comment": {
                        "value": "Thanks for the reply. My concerns are addressed. I will keep my positive rating."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662639782,
                "cdate": 1700662639782,
                "tmdate": 1700662639782,
                "mdate": 1700662639782,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cmCgzlp66v",
            "forum": "506Sxc0Adp",
            "replyto": "506Sxc0Adp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_kREU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6529/Reviewer_kREU"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the diversity coefficient metric [1] to textual pre-training datasets. To be more specific, the approach first estimates the Task2Vec embeddings [2] for batches of sequences in the dataset, followed by computing the expected distance between two random batches. This diversity metric is then computed on various existing pre-training datasets (and their combination), followed by some analysis of the same.\n\n[1] Brando Miranda, Patrick Yu, Yu-Xiong Wang, and Sanmi Koyejo. The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence.\n\n[2] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation behind building data quality metrics is an important direction for building better language models.\n- The paper is well written and easy to follow along."
                },
                "weaknesses": {
                    "value": "- Unclear practicality of the proposed metric: while the metric is shown to reasonably estimate the inherent \u201cdiversity in a dataset,\u201d I\u2019m quite unsure about its extrapolation to measuring \u201cdataset quality.\u201d \n- Limited novelty: The paper directly builds on two existing lines of work [1, 2], and is also limited by its application to different practical scenarios (more in questions).\n- The proposed diversity metric is model-dependent (GPT-2 is used in the paper), while the downstream effect of the probe model on the diversity metric isn\u2019t studied (more in questions). \n\n[1] Brando Miranda, Patrick Yu, Yu-Xiong Wang, and Sanmi Koyejo. The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence.\n\n[2] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning."
                },
                "questions": {
                    "value": "- What are some practical use-cases for the proposed diversity metric? \n- Keeping aside the vague intuition, why do you think data diversity should be a good indicator of data quality?\n- The considered scenario for estimating the utility of a larger batch-size is also debatable: a larger batch-size in addition to providing diverse data also directly affects the optimization. Nonetheless, in addition to showing a saturating diversity with increasing batch size, how does the model performance change with increasing batch size?\n- Assuming data diversity is a good indicator of data quality, do you think data diversity estimated using GPT-2 as the probe-model would translate to better training of (1) models with different architectures (e.g., encoder-decoder, encoder-only), and (2) different model-sizes (e.g., GPT-3, \u2026)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6529/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6529/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6529/Reviewer_kREU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903648876,
            "cdate": 1698903648876,
            "tmdate": 1699636734084,
            "mdate": 1699636734084,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wy1NTuXmRK",
                "forum": "506Sxc0Adp",
                "replyto": "cmCgzlp66v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for Insightful Critique + Response Overview"
                    },
                    "comment": {
                        "value": "Hello,\n\nThank you for your review of our work! We\u2019re pleased to know that you appreciate the quality of writing in the paper, and that we have a shared belief in the importance of understanding the role of data quality in the performance and development of LLMs. \n\nIn addition, we believe you raise some thoughtful points, and we address them below. We organize our responses by section (denoted by \u201c # \u201d), where each section addresses a specific point or set of points you raise.\n\n1. Our first comment below addresses the connection between data diversity and data quality. \n2. Our second comment below addresses practical use-cases of the metric. \n3. Finally, our third comment addresses the implications of the diversity coefficient for models of varied size and architecture and the novelty of our work. It also contains a basic reference list, which we refer to throughout our response.\n\nWe fully acknowledge that our responses here are relatively long; we endeavored to respond to your insightful critiques thoroughly, and hope our responses help you better evaluate the merits of our paper. \n\nTo assist with ease of reading and understanding, we have bolded our key points, so \n\n### *please feel free to skim bolded sentences to quickly become acquainted with the main points of our response.*"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652317689,
                "cdate": 1700652317689,
                "tmdate": 1700693545137,
                "mdate": 1700693545137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7pzAbxTN6q",
                "forum": "506Sxc0Adp",
                "replyto": "cmCgzlp66v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Connection between Data Diversity and Data Quality"
                    },
                    "comment": {
                        "value": "# Connection between Data Diversity and Data Quality\n\nWe believe that the diversity of a dataset is an important determinant of data quality and a driver of increased downstream performance of models trained on these datasets, and in this section, we will provide justification of this claim.\n\nOne reason for believing this is that  **it has generally been a commonly held wisdom in the LLM community that using diverse pretraining data benefits downstream model performance.**  For a few illustrative examples, consider the that:\n\n- The creators of GPT-3 made a deliberate effort to create a dataset composed of (qualitatively) diverse data [1] .\n\n- The creation of The Pile dataset was directly motivated by the desire to create an open-source high quality, diverse dataset [2]. A number of leading open-source models are partially trained on The Pile, such as Meta\u2019s OPT models and LLaMA 1 models, Microsoft\u2019s Megatron-Turing NLG 530B, and EleutherAI\u2019s GPT-Neo models.\n\n- The creators of LLaMA 1 made a deliberate effort to train on diverse data, particularly preprocessed web-crawls, which they noted improved performance [3]. Note that the diversity coefficient recognizes that C4 and Pile-CC, pre-processed CommonCrawl datasets, are among the most diverse datasets tested.\n\n- The creators of the TinyStories dataset make a deliberate effort to make their data sufficiently diverse in order to improve downstream model performance [4]. \n\nThus, we see that many real-world practitioners and members of the LLM research community find that  **the diversity of pretraining data is an important driver of downstream model performance**  and, thus, that data diversity is likely a key component of data quality.\n\nAnother important reason data diversity is a likely driver of data quality is that there is  **compelling empirical evidence that diverse pretraining data is an important driver of In-Context Learning (ICL) in LLMs.**  For support of this, consider the following examples:\n\n- A \u2018mixture of concepts\u2019 structure in pretraining data is key for instilling ICL ability, and data with only 1 concept i.e. data with very low semantic and syntactic diversity failed to produce ICL in transformer models [5]. Note that Section 3.4 demonstrates that the diversity coefficient of data strongly correlates with the number of latent concepts in (synthetic) data.\n\n- Training models on qualitatively diverse natural language corpora strengthened models\u2019 ICL ability, such as using training corpora composed of intuitively cross-diverse sub-datasets [6]. \n\n- Training transformers on data with a large number of rarely occurring classes (in this case, character symbols), and with dynamic item meanings and interpretations lead to higher ICL ability [7]. \n\nThus,  **dataset diversity appears to be an important driver of ICL ability, and we believe the diversity coefficient quantifies this characteristic of data well, such as its ability to recognize data with a variety of concepts/classes, and data that is intuitively/qualitatively diverse.**  Since ICL is an important factor in the overal performance and capabilities of LLMs, we believe that this is another dimension on which data diversity is an important driver of data quality.\n\nFinally, we have conducted small scale experiments (see Appendix G, within Supplementatry Material) and found that  **models trained on more diverse data achieve lower loss (i.e. higher performance) on diverse eval datasets.**  This indicates that diverse pretraining data can be a meaningful contributor to model performance.\n\nThat being said, we believe  **the relationship between data diversity and data quality is likely non-monotonic.**  There is probably an optimal range for the diversity of training data for a given LLM, where data with too low of a diversity coefficient is too homogenous to \u2018properly challenge\u2019 the model (e.g. prevent overfitting to certain styles of writing) while data with too high of a diversity coefficient is too \u2018chaotic\u2019 and varied for the model to \u2018learn from properly.\u2019\n\nTherefore, we believe the diversity coefficient would allow the research community to **better assess the data quality of their training corpora with regard to whether the data is sufficiently (and not overly) diverse, enabling them to achieve their desired downstream performance.**\n\nHowever, we also agree that we did not explain this rationale sufficiently compellingly in our introduction, results, and/or discussion sections. Hence, if you believe it would contribute to the strength of our paper, we would be happy to:\n\n- **Edit and revise our introduction and/or discussion** to better reflect our understanding of the connection between data diversity and data quality. \n\n- **Expand our results from appendix G by conducting a similar experiment with ~125M parameter models trained on ~2.5B tokens** (using training datasets that vary in diversity), and evaluating the downstream performance of these models on diverse eval datasets."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652472624,
                "cdate": 1700652472624,
                "tmdate": 1700652667838,
                "mdate": 1700652667838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PjTHrllkVP",
                "forum": "506Sxc0Adp",
                "replyto": "cmCgzlp66v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Practical Use-Cases of the Diversity Coefficient"
                    },
                    "comment": {
                        "value": "# Practical Use-Cases of the Diversity Coefficient\n\nBuilding on the reasoning from above, we present concrete practical use cases for the diversity and cross diversity metrics we propose in the paper.\n\n## Use-case 1: **Rigorously characterizing the effect of data diversity on (general) downstream LLM performance.**\n\nAs argued in the above section, there is already a widespread belief among practitioners and researchers that training models on diverse pretraining data likely leads to better general downstream performance on language-based tasks. That being said,  **this conjecture remains highly qualitative and imprecise**  as to e.g. the magitude of the effect of using diverse pretraining data and the optimal degree of diversity for pretraining data (e.g. TinyStories appears to indicate ideal data diversity depends on the size of the model being trained [7]).\n\nBy using the diversity and cross diversity metrics, researchers can  **gain a precise, quantitative, and, in our view, well-supported measurement of the inherent variability of a model\u2019s pretraining data. Hence, this empowers researchers to more rigorously analyze how model abilities develop with respect to this important characteristic of the data,**  without being impeded by the fact that real-world natural language corpora do not have clear and precise labeling of e.g. the number of unique concepts they contain. For example, the diversity metric allows for a rigorous and precise study of how accuracy on a wide-ranging benchmark like MMLU varies when models are trained on more or less diverse pretraining corpora.\n\n## Use-case 2:  **Rigorously studying the effect of data diversity on the development of ICL ability in LLMs in more realistic and meaningful settings.**\n\nAs argued in the first section, we observe experimental evidence that training data with a diversity of concepts and style/format appear to be a strong driver of ICL ability in transformers. But, these experiments were  **conducted in primarily \u2018toy\u2019 settings,**  with models 2-3 orders of magnitude smaller than current leading LLMs in terms of both parameter count and dataset size, and which use data that is drawn from synthetic rather than real-world, natural language sources. Hence, these experiments  **leave open the question of whether their results will generalize to more realistic settings.** \n\nGiven we believe the **diversity coefficient allows for a well-supported, quantitative assessment of data diversity in real-world, natural language data settings,**  we believe this will empower researchers to study the emergence of ICL (with respect to data characteristics) in more realistic settings, leveraging large natural language corpora to train larger and more representative experimental models. Ultimately, this enables results such as  **finding the R^2 value of the relationship between model performance and natural language training data diversity.**  This enables researchers to more rigorously understand the mechanism by which advanced LLMs, such as LLaMA 2 or GPT-3, acquire powerful ICL abilities.\n\n## Use-case 3:  **Enabling a better choice of training corpora for the training and development of a LM.**\n\nThe diversity coefficient and cross diversity coefficient can serve as a  **valuable tool that practitioners (and researchers) can use to curate their training corpora.**  For instance, many researchers and practitioners operate under limited compute budgets for training their LMs. When curating training corpora, scaling laws provide guidance on the optimal size of the dataset used one should use during training, but this  **leaves ambiguous what characteristics of the data one should optimize for when selecting the N billion tokens one will use**  for training. By  **comparing against the diversity values of widely used/known datasets we provide in our paper,**  the practitioner could test the diversities of candidate datasets to determine whether each is sufficiently diverse (and not overly diverse) for their purposes and choose to train on the data which satisfies their desired level of diversity.\n\nFor another example of using diversity metrics for dataset curation, consider a researcher aiming to create a large natural language corpora of high quality, diverse training data (similar to the intent behind The Pile). One could  **use the cross-diversity coefficient between candidate datasets to determine which datasets to include in one\u2019s overall corpora in order to ensure one\u2019s corpora is composed of maximally (or, rather, optimally) diverse subsets.**  In fact, the researcher or practitioner can collect small \u2018sample datasets\u2019 from certain sources (e.g. transcriptions of the most listened to podcasts on Spotify) and  **test whether this data is diverse enough from one\u2019s existing corpora.**  Depending on if the sample data is sufficiently diverse, one could  **make an informed decision as to whether to continue scraping**  the given source, or search for a more diverse source."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652654906,
                "cdate": 1700652654906,
                "tmdate": 1700652654906,
                "mdate": 1700652654906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0mdsYBoDcc",
                "forum": "506Sxc0Adp",
                "replyto": "cmCgzlp66v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Relevance of Div to Various Sizes/Archs | Novelty | References"
                    },
                    "comment": {
                        "value": "# Relevance of Diversity Coefficient to Models of Various Sizes & Architectures.\n\nWe believe that the diversity coefficient of a model\u2019s training data has important implications for models of various architectures and/or sizes, but, as we describe below,  **what _exactly_ these implications are depend on what specific downstream model is used.**\n\nFor instance, examining the TinyStories dataset, we see datasets with a level of diversity that is helpful for large models (e.g. OpenWebText or C4 for models like GPT-3 or LLaMA 1 20B) can inhibit the abilities of small models, which perform qualitatively better when trained on less (but still sufficiently) diverse text [4]. Thus, we see that  **the optimal range of diversity likely increases as the model becomes larger.**\n\nAnother approach is to consider the conceptual foundations of Task2Vec embeddings. The Task2Vec method is based on the Fisher Information Matrix (FIM), which is related to the  **Kolmogorov complexity of (in our case) next token prediction on a batch of text sequences [8].**  Given the complexity of a next token prediction task is independent of what downstream model is being used, the resulting FIM, Task2Vec embedding, and, thus,  **diversity coefficient is likewise meaningful, regardless of what downstream model is being used to train on the data.**  We simply use GPT-2 as our probe network as a  **practical, reasonable, and relatively efficient implementation of the FIM computation**  need to generate a Task2Vec embedding. Given this understanding, we believe that the diversity coefficient, computed using a GPT-2 probe network, still has important implications for model performance, regardless of a model\u2019s specific architecture (e.g. encoder only vs. decoder only).\n\nThus, for the same reason we believe data diversity is an important factor of overall data quality, we believe that the diversity coefficient of training data has important implications for models of varied size and architecture,  **independent of the downstream model\u2019s similarity to the particular probe network we use**  (in this case, GPT-2). \n\n\n# Novelty of our work\n\nThe novelty of our work is the use of previous work to  **measure important, non-trivial concepts, like the inherent diversity of language data, and make novel, quantitative, non-trivial observations of LLM pre-training datasets.**  In addition, we are the first to show with a quantitative diversity metric the  **relationship between diversity and performance (see Appendix G, in Supplementary Materials).**  Hence, we respectfully disagree that our work lacks novelty, as we believe our work is noteworthy by being the first to  **extensively test and propose a formal and rigorous diversity metric for use on real-world datasets used to train foundation models.**\n\n\n# References\n\n[1] Brown et. al. Language models are few-shot learners. CoRR, 2020. https://arxiv.org/abs/2005.14165.  \nKey Quote: \u201cOur basic pre-training approach\u2026[includes] scaling up of dataset\u2026 diversity\u201d \n\n[2] Leo Gao et. al. The pile: An 800gb dataset of diverse text for language modeling. 2020. https://arxiv.org/abs/2101.00027. \nKey Quote: \u201cRecent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present The Pile\u2026\u201d\n\n[3] Touvron et, al. Llama: Open and efficient foundation language models. 2023. https://arxiv.org/abs/2302.13971. \nKey Quotes: \u201cOur training dataset is a mixture of several sources\u2026that cover a diverse set of domains\u201d; \u201cDuring exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance\u201d\n\n[4] Eldan et. al. TinyStories: How small can language models be and still speak coherent english?. 2023. https://arxiv.org/abs/2305.07759.\nKey Quote: \u201cThe main challenge in using large language models for producing training data is generating a dataset that is sufficiently diverse\u2026\u201d\n\n[5] Xie et. al. An explanation of in-context learning as implicit bayesian inference. 2021. https://arxiv.org/abs/2111.02080\nKey Quote: \u201cWhen pretrained with only one concept, in-context learning fails.\u201d\n\n[6] Shin et. al. On the effect of pretraining corpora on in-context learning by a large-scale language model. 2022. https://arxiv.org/pdf/2204.13509.\nKey Quote: \u201cOur study shows that diverse pretraining corpora strengthen the ability of in-context learning.\u201d\n\n[7] Chan et. al. Data distributional properties drive emergent in-context learning in transformers. 2022. https://arxiv.org/pdf/2205.05055. \nKey Quote: \u201c[Data with] a large number of rarely occurring classes [promotes in-context learning]\u201d\n\n[8] Achille et. al. Task2Vec: Task embedding for meta-learning. 2019. https://arxiv.org/pdf/1902.03545. \nKey Quote: \u201cThe FIM is also related to the (Kolmogorov) complexity of a task, a property that can be used to define a computable metric of the learning distance between tasks.\u201d"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652988328,
                "cdate": 1700652988328,
                "tmdate": 1700652988328,
                "mdate": 1700652988328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XeWh6hvYFY",
                "forum": "506Sxc0Adp",
                "replyto": "0mdsYBoDcc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_kREU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6529/Reviewer_kREU"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your elaborate responses, and highlighting the key messages in each paragraph. However, reviewing being a subjective matter, I would like to stick to my original rating."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732171326,
                "cdate": 1700732171326,
                "tmdate": 1700732171326,
                "mdate": 1700732171326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]