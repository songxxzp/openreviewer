[
    {
        "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book"
    },
    {
        "review": {
            "id": "DF1XMMAVHl",
            "forum": "tbVWug9f2h",
            "replyto": "tbVWug9f2h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_tEGh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_tEGh"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how effectively LLMs adapt to a task which is guaranteed to have no overlap with LLM training data. The task is translation between English and Kalamang, an endangered language with little to no online presence. The authors introduce a new dataset, MTOB (Machine Translation from One Book), which contains (1) a linguistic analysis of the Kalamang language, (2) a bilingual dictionary, and (3) a small English-Kalamang parallel corpus. They benchmark several LLMs on the translation task, experimenting with different in-context learning settings. They find that the bilingual dictionary and parallel corpus enables some translation capabilities, and a large context size learning from the grammar book leads to notable performance gains. However, no LLMs outperform a human baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work is a solid scientific investigation into an unexplored topic. The paper is excellently written - clear, well organised, and engaging. \n\n**-- 1. Novel benchmark --**\n\nMTOB is a unique benchmark that enables interesting experiments. It would undoubtedly be a useful resource for future work, as it offers an alternative to the current paradigm of raw text training and highly structured fine-tuning. The idea of using it to mimic second language learning is interesting.\n\n**-- 2. New experimental ideas --**\n\nThe paper introduces a few new ideas in its experimental framework with the aim of testing generalization capabilities beyond the training set.\n1. Using content guaranteed to not be on the web.\n2. Testing model knowledge on the task *before* training to check for potential train-test overlap.\n3. The motivation of testing crystalised intelligence, as opposed to fluid intelligence.\n\nThese are all ideas that could find use in other contexts/domains.\n\n**-- 3. Interesting findings --**\n\nThe experiments reported reveal some insightful findings. For example, the failure of traditional finetuning in this setting is interesting, and so is the difficulty of incorporating grammar knowledge into smaller context LLMs. \n\n**-- 4. Handling of ethical considerations --**\n\nThe authors actively engage with ethical considerations around working with an endangered language. Their approach in working with the Kalamang language community is a model for how other NLP researchers should proceed."
                },
                "weaknesses": {
                    "value": "While the topic of interest and proposed dataset are novel contributions, my main concern with the paper is the lack of innovation in terms of modelling and evaluation. Furthermore, I believe more should be done to prove that this task is truly distinct from standard extremely low-resource translation. This could be shown empirically through experiments comparing Kalamang translation to translation involving other (somewhat online) extremely low-resource languages.\n\n**-- 1. Narrow modelling comparisons --**\n\nThe experiments would be improved by comparing other types of models besides recent LLMs. Sequence-to-sequence PLMs like mT5 have been shown to perform well on low-resource MT (https://aclanthology.org/2022.naacl-main.223.pdf) .\n\nFurthermore, the nature of MTOB could be leveraged by more specialised neural architectures, such as neural MT models that incorporate bilingual dictionaries (https://aclanthology.org/2021.acl-long.382/, https://aclanthology.org/2020.acl-main.143.pdf). An analysis spanning different models would reveal more about the true difficulty of the task. There is a growing literature evaluating LLMs for low-resource MT (e.g. https://arxiv.org/pdf/2309.07423v1.pdf) and so far it seems that they fall short of massively multilingual NMT models, which could just be because they are trained/tuned for very different types of tasks.\n\n**-- 2. Insufficient interpretable evaluation--**\n\nWhile some qualitative examples are provided in the appendix, the paper would be strengthened by more such analysis. Could error types across models / in-context settings be quantified to some extent? More generally, since MTOB is framed as a unique benchmark, I would expect some interesting findings from a more nuanced evaluation framework (along the lines of page 7, paragraph 3 \u201cIn contrast, the grammar book\u2026\u201d). Such analysis could help motivate why MTOB is unique (e.g. if it is found that LLMs make certain types of errors on MTOB that they do not make on other tasks).\n\n**-- 3. Lack of comparison to extremely low-resource MT--**\n\nWhile any knowledge of the Kalamang language is new to the LLMs, the task itself (translation) is well known to LLMs given the wide availability of parallel corpora online and the popularity of machine translation as a task. The authors discuss this distinction themselves (fluid vs crystalised intelligence). However, there is some doubt as to how different English to Kalamang translation is in terms of task difficulty, compared to translation involving other extremely low-resource languages that have a limited online presence. This would call into question the value of MTOB as an NLP resource, as it is currently being claimed in the paper.\n\nFor example, it seems that for some extremely low-resource languages LLMs have basically no translation capabilities (see some of the zero-shot experiments here https://arxiv.org/pdf/2309.07423v1.pdf), even thought these languages are included in publicly available test sets. The paper would be improved through some comparison of the MTOB task with extremely low-resource MT e.g. qualitative differences in model performance or proof of data contamination even for extremely low-resource languages."
                },
                "questions": {
                    "value": "1. Was any part of the translation train/test set previously released along with the Kalamang grammar book?\n2. Did you test any other type of baselines (e.g. sequence-to-sequence models) on the translation task?\n3. Have you considered using chrF++ as your primary metric since it (arguably) enhances automatic evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7870/Reviewer_tEGh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672633939,
            "cdate": 1698672633939,
            "tmdate": 1700639008312,
            "mdate": 1700639008312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kfiOuslxfB",
                "forum": "tbVWug9f2h",
                "replyto": "DF1XMMAVHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tEGh"
                    },
                    "comment": {
                        "value": "Thank you for reading our paper and for your constructive feedback! We have improved our paper based on your feedback.\n\nWith regard to the strengths:\n* Re Interesting findings: For us, the most interesting result was that Claude 2 with long context did so well when given 100k tokens of the book. This is not something that we expected given recent work that is somewhat negative about long context LLMs (e.g. Lost in the Middle; Liu et al. 2023 -- https://arxiv.org/abs/2307.03172).\n\nWith regard to the weaknesses:\n* We agree that we should have included a traditional machine translation method as a baseline. We have run baselines finetuned on the ~1100 available parallel sentences and included full details along with a table of results in the general response to all reviewers. We also agree that future work should explore other kinds of architectures/LLMs finetuned specifically for machine translation; we think an interesting research question is how to most effectively combine traditional massively multilingual machine translation data with instruction tuning/in-context learning capabilities. As you point out with the Robinson et al. paper, even though LLMs are trained on both kinds of data, the MT capabilities don\u2019t always make it through.\n* We agree that more interpretable evaluation is always better. Unfortunately analyzing the results in this way, especially across different kinds of retrieved context, is very time-consuming and error-prone since there is a lot of context retrieved for each example (as in Figures 7 and 8, let alone the 50K and 100K chunks). It also seems safe to say that we can\u2019t automate this analysis using current LLMs, since they are the ones generating the outputs with errors. We think that the qualitative examples and analysis in the paper give a good sampling of the kinds of errors that the models and retrieval methods make, and the chrF scores match our subjective impressions of output quality well. The fact that the errors are qualitatively similar to hallucinations in LLMs more generally and that there are fewer errors with better LLMs\u2014despite the exclusion from pretraining data\u2014is evidence that in-context learning in LLMs is really improving and underscores the value of MTOB.\n*  We have conducted a comparison to low-resource MT and included the results in the response to all reviewers. But we will also clarify: we do not think that we claim in the paper that English <--> Kalamang translation is more difficult than translation with other low-resource languages. (Certainly, given equal amounts of data, we have no reason to think this would be the case.) Rather, we were trying to say the following:\na) We can be confident there isn\u2019t a mass of in-domain training data hidden in the pretraining set, unlike languages with more speakers, so we can test data efficiency in a principled way. The fact that there are only ~1600 parallel sentences means that yes, it is probably harder from a traditional NMT perspective, but\nb) We have the model learn from explanations in a grammar book to make the task easier, and\nc) We show that the task is actually possible to a great extent using the human baseline.\nSo in some sense we are saying our task framing is actually easier than traditional low-resource MT from parallel sentences, because we have reason to believe that the task is possible.\n\nWith regard to the questions you asked:\n1. Yes, we mention in the expanded Limitations section in the Appendix (page 22 under \u201cPotential Contamination\u201d) that the train/test parallel sentences are in a CSV file on GitHub as of February 2021, as part of the source code for Dictionaria. (And we mentioned in the introduction the steps we are taking to prevent additional contamination due to the release of this paper.) It is possible that LLMs were trained on it, or maybe it was filtered out because the CSV file is very messy. Regardless, we saw no signs that the model baseline outputs were memorized; they changed very reasonably with different kinds of provided context.\n2. See the new baseline mentioned above that we ran in response to your feedback.\n3. Thank you for the suggestion. We have re-evaluated all results using chrF++ and we will add the results to the paper; see the included numbers in the table in the general response to all reviewers. We find that the (relative) chrF results are very similar to chrF, with a slightly bigger gap between the model-based methods and the human baseline."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596337652,
                "cdate": 1700596337652,
                "tmdate": 1700596337652,
                "mdate": 1700596337652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jLMdjBWTPl",
                "forum": "tbVWug9f2h",
                "replyto": "DF1XMMAVHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_tEGh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_tEGh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my queries and for reporting your new results, which will definitely add value to the paper. I have increased my overall rating and contribution score.\n\nI still feel that, given the unique experimental opportunity presented by the new dataset, the work would have been strengthened by exploring innovation in modelling and/or evaluation.\n\nNevertheless, many researchers in computational linguists would be interested in the resources and findings of this paper. Furthermore, the benchmarking study is carried out excellently and the paper reads easily thanks to the great writing."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639796689,
                "cdate": 1700639796689,
                "tmdate": 1700725047894,
                "mdate": 1700725047894,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CS2jiwbZws",
            "forum": "tbVWug9f2h",
            "replyto": "tbVWug9f2h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_fSip"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_fSip"
            ],
            "content": {
                "summary": {
                    "value": "The work targets the OOD problem and the difficulty to assess it during training, especially when pre-trained models have seen very large swathes of open internet. Picking a domain where we can find an example scenarios that is rare on the web helps and they selected a low resource language with limited web presence. They create a benchmark which validate a model which has been trained by very few samples - in this case grammar content which is very high quality to demonstrate its value."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well presented and the core hypothesis is both clear and verified. Additionally, the paper does a good job of calling out the limitations of the work and future enhancements. The work itself feels motivated by a sincere desire to help communities who are disadvantaged due to their language being marginalized."
                },
                "weaknesses": {
                    "value": "As the authors themselves list in limitations, though a specific dataset was chosen to enhance the model, at least part of the information might have leaked into the pre-training dataset, especially since there are other languages close to the source language which may have a larger presence.\n\nThe work would benefit from going beyond one sample to another language, especially since, as the authors state, there is a very high number of low resource languages. This would help the hypothesis and solution more convincing. The work otherwise comes across as a focused effort to provide more access to a disadvantaged group and then an attempt at generalization."
                },
                "questions": {
                    "value": "How confident are you that the solution would generalize to other low resource scenarios?\n\nThere has been recent work (e.g. from Microsoft Research - phi models)  on using high quality but smaller sample set to train a high quality model. How does your work compare to that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699152525718,
            "cdate": 1699152525718,
            "tmdate": 1699636965170,
            "mdate": 1699636965170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iymZcUrpom",
                "forum": "tbVWug9f2h",
                "replyto": "CS2jiwbZws",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fSip"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and helpful feedback on our paper!\n\nWith regard to the weaknesses you mentioned: \n* Two clarifications on leakage:\n    * First, it is only the test set where avoiding contamination is critical. If the training set is included in pretraining data, we agree this is unideal, but we can at least say that it was possible to learn to perform the task from this data. The main point is that by picking a language with so few speakers, we can be confident that there is not a mass of undetected in-domain training data scraped into pretraining datasets.\n    * Second, in terms of related languages, we note in the Background section that Kalamang is not known to be closely related to other languages. In addition, the other languages in its family (which is just a geographical grouping, not genealogical) have similarly low speaker counts (100s-1000s).\n* We agree that it will be important to explore more languages in future work. That was out of scope for this paper due to the lengthy process of obtaining consent from the respective communities and performing the human baseline.\n\nWith regard to your questions:\n* Other than the fact that Kalamang can be tokenized conveniently, we see no reason why it should be easier or harder than other low-resource languages. We expect there to be more variation across the quality/comprehensiveness/presentation/length of different grammar books, which is an interesting topic for future work to explore. (What is the best way to organize grammar books for this application?)\n* We see Phi mostly as a distillation method; Phi 1.5 is mostly trained on synthetic data from a larger LLM (for 150B tokens), and ablations show that the synthetic data is key to the results. We expect that you could use an approach like Phi to produce a small Kalamang translation model: one could generate synthetic translation data using a large model with long resources provided in context, and distill that synthetic data into a much smaller task-specific model. But it is not a solution to the extreme data efficiency requirements of this task (which we expect will need in-context learning)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596190343,
                "cdate": 1700596190343,
                "tmdate": 1700596190343,
                "mdate": 1700596190343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vucZwPEJqC",
                "forum": "tbVWug9f2h",
                "replyto": "iymZcUrpom",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_fSip"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_fSip"
                ],
                "content": {
                    "title": {
                        "value": "Thank you authors for your response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I retain my positive rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716938637,
                "cdate": 1700716938637,
                "tmdate": 1700716938637,
                "mdate": 1700716938637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JnVXs8fXrs",
            "forum": "tbVWug9f2h",
            "replyto": "tbVWug9f2h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_M62r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7870/Reviewer_M62r"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an interesting approach to translation: having an LLM directly learn a language (Kalamang) from raw linguistics reference materials. They feed these documents to various large language models, in a variety of different ways (via additional pretraining, by providing extra context in the prompt, etc.) and evaluate the model\u2019s ability to translate to/from Kalamang. An additional baseline is also provided in the form of a human who learnt the language from the same reference materials.\n\nThe approach is interesting in that it constitutes a modern take on rule-based MT, where instead of providing a structured grammar to a heavily feature-engineered model, the hope is that the language model will learn itself to interpret the human-readable descriptions. It is very valuable as a way of evaluating LLM performance on a clearly measurable, task-oriented benchmark which has direct real-world applications (translation of under-resourced languages.)"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The authors propose a novel, clearly measurable and well-defined evaluation benchmark of LLM translation capabilities.\n* The experimental setup is solid, and there is an extensive selection of baselines. The addition of a human baseline is particularly appreciated as it helps put numbers in context.\n* The authors recognise the risk of the reference materials leaking into LLM training datasets, and take active steps to prevent it.\n* The work is conducted with the involvement and consent of the language community it concerns."
                },
                "weaknesses": {
                    "value": "* A minor wish would have been to see some additional baselines involving more standard MT approaches. If we're saying that this work has the potential of helping with the translation of under-resourced languages, it might be worth trying, amongst others: standard neural MT, trained on the little parallel data that\u2019s available + parallel data from related languages; traditional rule-base MT; neural MT trained on synthetic data generated from templates. I recognise that these approaches would likely be more involved than simply feeding data to an LLM, but they are all reasonable approaches that a researcher interested in building MT for Kalamang might try."
                },
                "questions": {
                    "value": "* How do you expect this approach to stack up against traditional MT (e.g. neural sequence-to-sequence MT with e.g. a transformer encoder/decoder architecture), when using techniques such as cross-lingual transfer from related languages, backtranslation, data mining, synthetic data augmentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699293388116,
            "cdate": 1699293388116,
            "tmdate": 1699636965070,
            "mdate": 1699636965070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SB6kAN3ArG",
                "forum": "tbVWug9f2h",
                "replyto": "JnVXs8fXrs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer M62r"
                    },
                    "comment": {
                        "value": "Thank you for your close reading of our paper and for your helpful feedback! \n\nWith regard to the weakness you mentioned, we agree that we should have included a traditional machine translation method as a baseline. We have run this baseline and included full details along with a table of results in the general response to all reviewers. \n\nWe also agree that it would also be interesting to evaluate more classical methods such as rule-based machine translation. We did not have time to design such a system in this short response period, but we will add a discussion of these methods to the paper. We think it would be especially interesting in future work to try to design a system combining rule-based translation and in-context learning from grammar books; we hope that our dataset can provide a robust means of evaluation in such cases.\n\nWith regard to your question, we expect that the benefit of data augmentation techniques scales with the quality of the model that generates the synthetic data. So if the number of available parallel sentences is small, the quality of synthetic data generated with a traditional finetuned NMT model will not be great, even with multilingual transfer etc. By contrast, if a long context LLM can learn to translate in context using a grammar book (and we know that humans can perform this task pretty well from the same data), then this could be used to generate much higher quality synthetic data to either self-improve or distill into a smaller model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596117438,
                "cdate": 1700596117438,
                "tmdate": 1700596117438,
                "mdate": 1700596117438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nqQlmyjYGf",
                "forum": "tbVWug9f2h",
                "replyto": "SB6kAN3ArG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_M62r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7870/Reviewer_M62r"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for following up on my remarks and for conducting the additional experiments with several NMT baselines. Having read the other reviews, I do not see any of the listed weaknesses as deal-breakers. I am happy to confirm my positive rating for this paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705407991,
                "cdate": 1700705407991,
                "tmdate": 1700705407991,
                "mdate": 1700705407991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]