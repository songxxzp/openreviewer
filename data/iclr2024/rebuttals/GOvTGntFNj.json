[
    {
        "title": "Query-Efficient Offline Preference-Based Reinforcement Learning via In-Dataset Exploration"
    },
    {
        "review": {
            "id": "nJSmTXaXBX",
            "forum": "GOvTGntFNj",
            "replyto": "GOvTGntFNj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission863/Reviewer_HCQm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission863/Reviewer_HCQm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies improving preference-based RL in offline settings via in-dataset exploration. They proposed an algorithm, called OPRIDE. First, they show that this algorithm achieves statistical efficiency by establishing a bounded suboptimality. Then, they developed a practical version of this algorithm, and experiments show this approach outperforms other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is easy to follow. The description of the algorithm is clear, and the theoretical claims are also clearly stated.\n\nThe theoretical part looks sound to me, although I haven't checked the proof in detail."
                },
                "weaknesses": {
                    "value": "This paper has 10 pages. I am not sure if it violates the paper length limit. Regarding its content, I have the following concerns.\n\nI don't generally understand what \"query-efficient\" means in this paper, especially in the theoretical part (section 3.1). If I understand it correctly, the authors proposed an algorithm and showed that it achieves good suboptimality upper bound. Then how is small optimality related to \"query efficiency\"? I would expect \"query efficiency\" to have something to do with active learning. Otherwise it is simply \"statistical efficiency\".\n\nAnother concern is that there seems to be a big gap between the theoretical algorithm and the practical one (i.e. algorithm 1 vs algorithm 2). All theoretical results are developed for alg 2 while the experiments are conducted for alg 1. I think this is acceptable only when the gap between the two algorithms is small. However, I feel that the gap is large in my current understanding. For instance, alg 2 assumes a query oracle but alg 2 doesn't. Alg 1 uses an optimistic bonus, but no correspondence is found for alg 1. In addition, Alg 1 is heuristic and lacks some explanation. For example, why the loss functions in (14) and (15) are introduced is not explained (eg, why using expectile loss). The intuitive explanation of adding clipped Gaussian noise is not provided, which may be confusing."
                },
                "questions": {
                    "value": "I didn't understand Tables 1 and 2. I am not sure what these numbers in the tables are. I would suppose they are the output of the learned reward model. However, this confused me even more since comparing the absolute reward value seems meaningless under the Bradley-Terry model (because the distribution will not change under reward shift).\n\nThe authors claim to use 10 queries in the experiments. I am not sure how complicated the experiment is, but 10 sounds really small. Could the authors explain more about it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704868495,
            "cdate": 1698704868495,
            "tmdate": 1699636012900,
            "mdate": 1699636012900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wOBbZSKYME",
                "forum": "GOvTGntFNj",
                "replyto": "nJSmTXaXBX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCQm (Part 1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nWe thank the Reviewer for the constructive comments and clarify your concerns as follows. \nWe would appreciate it if you have any further\nfeedback.\n\n**W1: Does this paper violate the paper length limit?**\n\n**A for W1:**\nThis paper does not violate the paper length limit.\nAccording to the ICLR guidelines, the reproducibility statement does not count toward the page limit.  \n\nPlease refer to the author guideline (https://iclr.cc/Conferences/2024/AuthorGuide) for more details.\n\n**W2: How is a small suboptimality related to \"query efficiency\"?**\n\n**A for W2:**\nA smaller suboptimality bound indicates that we need fewer queries to achieve the same performance bound and, hence, better statistical efficiency. We use \"query efficiency\" to refer to the fact that our method aims to reduce the number of required preference queries. At each iteration, our method uses an online exploration objective to select a query to ask for preference labels, which can be seen as a special form of active learning. \n\n\n**W3.1: Gap between Algorithm 1 and Algorithm 2.**\n\n**A for W3.1:**\nWe respectfully disagree with this assessment. Algorithm 1 is an extension of Algorithm 2 in the deep learning setting without significant gaps. Specifically,\n\n1. **Query Oracle.** Both Algorithms 1 and 2 require querying the preference between two given trajectories, as standard in preference-based RL. There is no difference in the requirement for query oracles between Algorithms 1 and 2.\n\n2. **Optimistic Bonus.** The optimistic bonus is used for selecting policy candidates that are possibly optimal, which aligns with the theoretical formulation for the near-optimal policy set as in Equation (8). In fact, Equation (8) can be rewritten as the policy set where every policy is an optimal policy under some optimistic Q-functions. Then, Algorithm 2 can be rewritten using an optimistic bonus, as in some prior works [2]. The variance-based bonus in Equation (12) reflects the uncertainty over the reward functions, which aligns well with the confidence set formulation as in Equation (7). \n\n3. **Expectile Loss.** The expectile loss in Equations (14) and (15) is used as a conservative estimation for the value function, as proposed in IQL [1]. This aligns well with the use of PEVI in Algorithm 2, which uses a pessimistic value estimation to exploit the offline dataset."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631487768,
                "cdate": 1700631487768,
                "tmdate": 1700638524859,
                "mdate": 1700638524859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cD7Qp715jM",
                "forum": "GOvTGntFNj",
                "replyto": "nJSmTXaXBX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HCQm (Part 2/2)"
                    },
                    "comment": {
                        "value": "**W3.2 The intuitive explanation of adding clipped Gaussian noise.**\n\n**A for W3.2:**\nThe motivation to add weights is to learn better credit assignments with trajectory-wise information.\nSince we are only allowed to query the preference between trajectories, it is possible that\nwe can not learn proper temporal credit assignment with only fixed data. \n\nFor example, suppose we have two trajectories $\\tau\\_1$ and $\\tau\\_2$ that finish a task with two steps, knowing the preference between $\\tau\\_1$ and $\\tau\\_2$ give us no knowledge about how well they do at each step.\nSince we are not allowed to generate new trajectories, a natural idea is to give weights to different parts of the trajectory and ask for the preference again. \n\nSpecifically, we can give high weights for step 1 and low weights for step 2 to distinguish which trajectory performs better at step 1. \nTherefore, augmenting trajectories can help for better temporal credit assignment and efficient preference learning.\n\nWe have added experiments on Meta-world tasks to further demonstrate the advantage of using weighted trajectories for queries, as shown in Appendix G. We have made this point more clear in the revised version.\n\nAs to the choice of using clipped Gaussian noise, we clip the range of the weight to avoid large weights that can lead to high variance in the learning process, as commonly used in practice, like in TD3. \nWe also demonstrate the importance of weight clipping in the chain MDP experiment shown in Figure 10 (a) in Appendix G. We have made this motivation more clear in the revised version.\n\n**Q1: What these numbers in the table 1 and 2 are?**\n\n**A for Q1:**\nThe numbers in Tables 1 and 2 represent the (normalized) performance of the offline RL algorithm using the reward labels learned via the given preference-based reward learning algorithm. A higher performance indicates a better learned reward function.\n\n**Q2: Why 10 queries work well?**\n\n**A for Q2:**\nTheoretically, preference queries contain rich information such that learning from preferences is not harder than standard RL [3]. \nOur theoretical result shows that offline PbRL can be made more efficient in the number of queries by leveraging the knowledge from the offline dataset. A similar phenomenon is also observed in prior and concurrent works [4,5].\n\nAnother possible reason is that offline algorithms are known to be robust to the reward functions [6,7,8]. Therefore, we can have a good performance as long as the reward function learned from preference queries does not deviate too much from the true reward functions.\n\n\nThanks again for the valuable comments.\nWe hope our additional experimental results and explanation have cleared your concerns, and we sincerely hope the reviewer can re-evaluate our paper based on our response.\nMore comments on further improving the presentation are also very much welcomed.\n\n[1] Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline reinforcement learning with implicit q-learning.\" arXiv preprint arXiv:2110.06169 (2021).\n\n[2] Jin, Chi, et al. \"Provably efficient reinforcement learning with linear function approximation.\" Conference on Learning Theory. PMLR, 2020.\n\n[3] Wang, Yuanhao, Qinghua Liu, and Chi Jin. \"Is RLHF More Difficult than Standard RL? A Theoretical Perspective.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[4] Shin, Daniel, Anca D. Dragan, and Daniel S. Brown. \"Benchmarks and algorithms for offline preference-based reward learning.\" arXiv preprint arXiv:2301.01392 (2023).\n\n[5] Anonymous. \"Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation.\" https://openreview.net/forum?id=EG68RSznLT\n\n[6] Shin, Daniel, Anca D. Dragan, and Daniel S. Brown. \"Benchmarks and algorithms for offline preference-based reward learning.\" arXiv preprint arXiv:2301.01392 (2023). \n\n[7] Li, Anqi, et al. \"Survival Instinct in Offline Reinforcement Learning.\" arXiv preprint arXiv:2306.03286 (2023). \n\n[8] Hu, Hao et al. \u201cUnsupervised Behavior Extraction via Random Intent Priors.\u201d arXiv preprint arXiv:2310.18687 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631520744,
                "cdate": 1700631520744,
                "tmdate": 1700638505795,
                "mdate": 1700638505795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jZ0MDEWZeR",
            "forum": "GOvTGntFNj",
            "replyto": "GOvTGntFNj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents OPRIDE, an offline preference-based reinforcement learning (PbRL) algorithm that selects trajectories to query based on a pessimistic-value, optimistic-reward approach. Additionally, OPRIDE uses random weights to augment the offline dataset.\n\nMoreover, this paper presents a mathematical justification for using its pessimistic-value, optimistic-reward approach, setting a likely bound on the suboptimality of the policy value estimation.\n\nExperiments in AntMaze and MetaWorld show OPRIDE beating recent baselines such as Preference Transformer. For instance, across 30 MetaWorld tasks and 5 runs, OPRIDE increases the average reward by 27%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* As far as I could follow, mathematically robust motivation, with a bound on the suboptimality of the value estimation.\n* The evaluation results are impressive, beating Preference Transformer in the MetaWorld tasks."
                },
                "weaknesses": {
                    "value": "* _W1_: Though the individual sections are really well written, I had trouble following the paper. To mitigate this, I would suggest better delineating the article in the introduction, highlighting key take-aways in each section, adding a summary of Pessimistic Value Iteration (which OPRIDE builds on) to the main paper, and providing more examples. The mathematical analysis is very general (this is a strength of the paper), but providing examples of how it is instantiated for linear MDP with Bradley-Terry preferences would aid in comprehension.\n* _W2_: There is still a question whether the same gains observed in MetaWorld and AntMaze apply to other environments such as mujoco-gym and perhaps from human preferences too. (Preference Transformer tested against all these environments)\n\n**[[Post-rebuttal update]]**\n\nThe authors revised the manuscript, adding more \"sign-posting\" and repeating the motivations for each section. As a result the paper is easier to read, thus addressing _W1_. The manuscript remains rather mathematical, but that is the nature of the research.\n\nAs for _W2_, the authors conducted additional experiments with mujoco-gym, but OPRIDE surpassing or equalling Preference Transformer (albeit by a lower margin than in Metaworld)."
                },
                "questions": {
                    "value": "* _Q1_: In definition 2, the instantiation of $\\phi$ for linear MDP with the Bradley-Terry preference model depends on another $\\phi$. Is this a mistake? Or is the definition truly recursive? \n* _Q2_: What is $T$ on the theoretical guarantees section? Do you mean $K$ the number of queries?\n* _Q3_: In figure 4, for `push-v2`, both baselines beat OPRIDE at K>15. Why is that? Is there a downside to OPRIDE as the number of queries goes up?\n\n**[[Post-rebuttal update]]**\n\nThe authors addressed all questions and updated the manuscript accordingly. See discussion for more details.\n\n**Nitpicks and suggestions**\n\n* When introducing equation 5, explain that SubOpt is the sub-optimality measure, it will help readers later on.\n* In equation 6, mention that $o$ is the ground-truth human preference.\n* In section 3.2 $N$ is meant to use the number of reward functions, but $N$ was earlier used to indicate the size of the offline dataset.\n* In \"Answer to Question 1\", _tables 3 and 2_, should be _tables 1 and 2_ instead."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission863/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp",
                        "ICLR.cc/2024/Conference/Submission863/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771333210,
            "cdate": 1698771333210,
            "tmdate": 1700684320104,
            "mdate": 1700684320104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A0G1IEg8ts",
                "forum": "GOvTGntFNj",
                "replyto": "jZ0MDEWZeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8oYp"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks for your positive and insightful comments. We provide clarification to your concerns below.\nWe would appreciate it if you have any further feedback.\n\n**W1: More clear presentation.**\n\n**A for W1:**\nWe thank the Reviewer for pointing this out. As suggested, we highlighted key takeaways in each section, added a summary of Pessimistic Value Iteration to the main paper, and provided more examples in the revised version.\n\n**W2: Whether the same gains observed in MetaWorld and Antmaze apply to other environments such as mujoco-gym and perhaps from human preferences too.**\n\n**A for W2:**\n\nTo investigate whether our method is generally applicable to other environments, we conduct experiments on three mujoco tasks, including hopper, walker2d, and halfcheetah. As shown in Table 1, our method outperforms the baseline method PT, especially on the hopper tasks.\n\n\n| | OPRIDE | PT |\n| :---: | :---: | :---: |\n| hopper-medium-v2 | **47.5$\\pm$2.3** | 36.9$\\pm$2.1 |\n| hopper-medium-expert-v2 | **72.1$\\pm$1.5** | 68.0$\\pm$2.6 |\n| walker2d-medium-v2 | 71.8$\\pm$2.4 | 71.7$\\pm$2.6 |\n| walker2d-medium-expert-v2 | 108.5$\\pm$0.2 | 109.4$\\pm$0.3 |\n| halfcheetah-medium-v2 | 42.4$\\pm$0.2 | 42.1$\\pm$0.1 |\n| halfcheetah-medium-expert-v2 | **88.8$\\pm$0.8** | 81.9$\\pm$0.1 |\n| Average | **71.8** | 68.3 |\n\nTable 1. Additional experimental results on Mujoco tasks.\n\n\n**Q1: In definition 2, does the instantiation of linear MDP with the Bradley-Terry preference model depend on another $\\phi$? Is the definition truly recursive?**\n\n**A for Q1:**\nA linear MDP with the Bradley-Terry preference model does not depend on another $\\phi$. Definition 2 is only used to show that a linear MDP with the Bradley-Terry preference model naturally satisfies the definition of a generalized linear preference model. In linear MDP, the reward function is linear with respect to the feature map $\\phi(s,a)$, i.e., $r(s,a)=\\theta^T \\phi(s,a)$, and the preference over trajectories $\\tau\\_1$ and $\\tau\\_2$ can be written as \n\n$$ P(\\tau\\_1<\\tau\\_2) = \\sigma ( \\sum\\_{s,a \\in \\tau\\_1} \\theta^T\\phi(s,a) -  \\sum\\_{s,a \\in \\tau\\_2} \\theta^T\\phi(s,a)) $$\n$$ = \\sigma ( \\theta^T(\\sum\\_{s,a \\in \\tau\\_1} \\phi(s,a) -  \\sum\\_{s,a \\in \\tau\\_2} \\phi(s,a))) $$\n\nTherefore, by overloading the notation $\\phi$ and let $\\phi(\\tau\\_1,\\tau\\_2) = \\sum\\_{s,a \\in \\tau\\_1} \\phi(s,a) -  \\sum\\_{s,a \\in \\tau\\_2} \\phi(s,a)$, a linear MDP with the Bradley-Terry preference model naturally satisfy the definition of generalized linear preference model.\n\n**Q2: What is $T$ on the theoretical guarantees section? and Does $K$ mean the number of queries?**\n\n**A for Q2:**\nThanks for pointing this out. This is a typo, and it should be $K$ rather than $T$ in the theoretical guarantee section. Yes, $K$ means the number of queries. We have corrected this in the revised version.\n\n**Q3: Why do both baselines beat OPRIDE at $K>15$ for push-v2 in Figure 4? And is there a downside to OPRIDE as the number of queries goes up?**\n\n**A for Q3:**\nThis is because push-v2 is a relatively simple task, and the performance saturates with a few queries. To verify this,\nWe evaluate the performance of the algorithms with 30 queries on push-v2, and the result is shown in Figure 4. We can see that OPRIDE performs well compared with other methods when the number of queries goes up. \n\n**S: Nitpicks and suggestions.**\n\n**A for S:**\nWe appreciate the valuable suggestions of the reviewer, and we have corrected them in the revised version.\n\nThanks again for the valuable comments. \nWe sincerely hope our response has cleared your remaining concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631391762,
                "cdate": 1700631391762,
                "tmdate": 1700631391762,
                "mdate": 1700631391762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v4iy3dZi5p",
                "forum": "GOvTGntFNj",
                "replyto": "A0G1IEg8ts",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for detailed rebuttal. In what follows I will answer to each of the points raised.\n\n--------\n\n**W1** Thank you for the revisions, I have revised the additions to the manuscript  and they indeed aid in comprehension.\n\n\n**W2** Thank you for the additional experiments. It is reassuring to see that OPRIDE outperform Preference Transfomer in these tasks.  However, for walker-walk (and to an extent for halfcheetah) the gains are relatively minor. Do authors have an hypothesis as to why? Is it that the tasks are relatively simple? Or is there any other insight we can extract for OPRIDE?\n\n**Q1** Thank you for the clarification. This makes sense to me. I realise this is a minor point of the paper, but I suggest authors add the intermediate step $\\phi(\\tau_1, \\tau_2) = \\sum_{s,a \\in \\tau_1} \\phi(s,a) - \\sum_{s,a \\in \\tau_2} \\phi(s,a)$ to the text. \n\n**Q2** Thank you for the clarification.\n\n\n**Q3** Thanks for updating the figure, the explanation makes sense. I also appreciated the addition of standard deviations to the figure.\n\n\n-----\nI am aware that we are very close to the deadline (and that depending on your locale you may preparing for a holiday), but if at all possible I would be very thankful for an answer to the follow-up question in **W2**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677332308,
                "cdate": 1700677332308,
                "tmdate": 1700677332308,
                "mdate": 1700677332308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZmRbgaDgF",
                "forum": "GOvTGntFNj",
                "replyto": "jZ0MDEWZeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks again for your valuable comments. We provide a response below for the points you raised.\n\n**W2 The improvement on some Mujoco tasks is relatively minor.**\n\n**A for W2**\nThis is because Mujoco locomotion tasks have a simple reward function such that a simple preference-based reward learning method can lead to a saturated performance. To verify this, we calculate the correlation coefficient between each dimension of the observation and the reward. As shown in Table 1, the observation can have a very high correlation (>0.9) with the reward in some dimension for every locomotion task, making the reward learning easy on these tasks. This phenomenon is also observed in concurrent works like [1]. Please refer to https://anonymous.4open.science/r/ICLR2024-7244-4A6F/README.md for a visualization of the correlation. \nTherefore, simple methods can also lead to reasonable performance on these tasks, and we believe that tasks like Meta-world, where learning the reward function is more challenging, are a better test bed for query efficiency.\n\n| Tasks |  |  |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Hopper | 0.1 | 0.44 | 0.11 | 0.11 | 0.66 | **0.99** | 0.15 | 0.47 | 0.10 | 0.30 | 0.02|\n| Walker2d | 0.18 | 0.40 | 0.20 | 0.32 | 0.00 | 0.27 | 0.18 | 0.04 | **0.97** | 0.46 | 0.63 |\n| | 0.37 | 0.34 | 0.06 | 0.37 | 0.30 | 0.02|  |  |  |  |  |\n| Halfcheetah | 0.07 | 0.10 | 0.15 | 0.05 | 0.08 | 0.06 | 0.07 | 0.01 | **0.91** | 0.11 | 0.10 |\n| | 0.12 | 0.03 | 0.13 | 0.01 | 0.03 | 0.14 |  |  |  |  |  |\n\nTable 1. The correlation coefficient between each dimension of observation and the reward function.\n\n **Q1 Add the intermediate step to text**\n\n **A for Q1**\n We thank the Reviewer for the suggestion, and we have added it to the manuscript.\n \n\nWe sincerely hope that our response can address your remaining concerns. More suggestions and discussions are welcomed.\n\n[1] Anonymous. Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation. https://openreview.net/forum?id=EG68RSznLT"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682329132,
                "cdate": 1700682329132,
                "tmdate": 1700682873330,
                "mdate": 1700682873330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usDk4Z9tvI",
                "forum": "GOvTGntFNj",
                "replyto": "VZmRbgaDgF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Reviewer_8oYp"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your swift response, I appreciate it. The explanation for the performance difference makes sense to me, and I recommend adding it as context to the appendix with the Mujoco results.\n\nBased on the fact that all the weaknesses and questions I have raised have been addressed, I have increased my review rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684421567,
                "cdate": 1700684421567,
                "tmdate": 1700684421567,
                "mdate": 1700684421567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3juJghyhPP",
            "forum": "GOvTGntFNj",
            "replyto": "GOvTGntFNj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission863/Reviewer_i96V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission863/Reviewer_i96V"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an algorithm for query-efficient offline preference-based RL. In this setting, the dataset of trajectories is fixed and the agent must choose pairs of to submit for a preference. \n\nFirst, they assume a linear transition and reward model. Building on top of the PEVI method, they construct a confidence set of reward functions using a projected MLE estimator. Given that confidence set, they construct a set of near-optimal policies. In order to explore maximally, they choose the pair of policies for which there are two reward functions in the confidence set under which these policies achieve maximally different values. They show that the suboptimality of the method is bounded by the sum of two terms: one that is O(1/sqrt(N)) in the dataset size and one that is O(1/sqrt(K)) in the number of comparisons.\n\nNext, they give an implementable algorithm that works with a static offline dataset. It seems that they augment each trajectory L times with L sampled clipped Gaussians. They train N reward functions by a preference transformer method and compute an optimistic reward function using ensemble disagreement. At each round, the agent chooses the trajectories for which the weighted sum of reward predictions has the largest difference between 2 of the ensemble members. Finally, using IQL, they train optimistic V and Q functions and extract a policy. \n\nThey evaluate the implementation on a set of Meta-World manipulation tasks as well as Antmaze. In each they make 10 pairwise queries prior to evaluation the policy performance. They compare against  a handful of recent methods for offline preference-based reward learning. In the experiments, it is clear that this method performs best on the set of manipulation + Antmaze benchmarks. Afterwards, the authors proform studies of the number of queries required to achieve good performance and an ablation where they remove the optimism and the selection method and see how that affects performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results. You can incorporate Markdown and Latex into your review. See https://openreview.net/faq.\n\nThe problem setting is reasonable and seems like a good way of supervising given large offline dynamics datasets and a hard-to-compute reward function. The theoretical setup seems reasonable and in line with prior work, and it seems like the bounds are interpretable. The theoretical algorithm seems broadly of the right flavor to solve a problem like this. \n\nBroadly speaking, the experimental section shows good results and some ablations and further empirical study is conducted. I don\u2019t think there are yet many methods that address this problem and there is therefore not a wide list of methods to compare against."
                },
                "weaknesses": {
                    "value": "I think the exposition and presentation of this paper is poor. I walked away from the first read without a new insight into the nature of preference based learning. I\u2019m sure there is one to be had in this paper but the lack of writing to inform rather than to describe procedures makes it far more difficult to take one away. In particular:\n- The motivation for the augmentation with a truncated Gaussian missed me entirely.\n- The chain experiment is not described well in the text or the appendix.\n- The justification for the use of projected MLE or even the definition of the projected MLE is missing from the method section. \n- There are a substantial number of typos and errors in the writing. \n- It's not clear to me what the counterfactual trajectories are. \nSee questions for things I concretely would like to know in order to increase my score."
                },
                "questions": {
                    "value": "At the end of 3.1 you mention that \u201cquerying with an offline dataset can be much more sample efficient when N >> T\u201d. What is T here?\nWhat is \u201cTrue Reward\u201d in table 1 / 2?\nCan you explain why you augment the trajectories with a truncated Gaussian?\nWhy a truncated Gaussian rather than some other random variable?\nAre we sure that an ensemble of reward functions is going to give reasonable uncertainty estimates of the preference model?  What if preferences have high aleatoric uncertainty?\nHow is the optimization problem in (13) solved?\nWhat do counterfactual trajectories have to do with weighting?\nHow does this policy error bound compare to e.g. a G-optimal design and then PEVI? The Pacchiano et al result?\nWhat are the steps in the x axis of Figure 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission863/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781355161,
            "cdate": 1698781355161,
            "tmdate": 1699636012747,
            "mdate": 1699636012747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Uywel8izsg",
                "forum": "GOvTGntFNj",
                "replyto": "3juJghyhPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i96V (Part 1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThanks for your valuable comments. We have provided additional experimental results and explanations to address your concerns, and we hope the following clarifications shed light on the points you raised.\n\n**W1: The motivation for the augmentation.**\n\n**A for W1** \nThe motivation to add weights is to learn a better credit assignment with trajectory-wise information.\nSince we are only allowed to query the preference between trajectories, it is possible that\nwe can not learn proper temporal credit assignment with only fixed data. \n\nFor example, suppose we have two trajectories $\\tau\\_1$ and $\\tau\\_2$ that finish a task with two steps, knowing the preference between $\\tau\\_1$ and $\\tau\\_2$ give us no knowledge about how well they do at each step.\nSince we cannot generate new trajectories, a natural idea is to give weights to different parts of the trajectory and ask for the preference again. \n\nSpecifically, we can give high weights for step 1 and low weights for step 2 to distinguish which trajectory performs better at step 1. \nTherefore, augmenting trajectories can help for better temporal credit assignment and efficient preference learning.\nWe have added experiments on Meta-world tasks to demonstrate further the advantage of using weighted trajectories for queries, as shown in Appendix G. We have made this point more clear in the revised version.\n\n**W2: Details of the chain MDP experiment.**\n\n**A for W2:**\nWe thank the reviewer for pointing this out, and we gave a more detailed description of the chain MDP experiment in Appendix E in the revised version.\n\n**W3: Justification for the use of projected MLE and  the definition of the projected MLE.**\n\n**A for W3:**\nBy assumption, the parameter $\\theta$ for the reward function is bounded, but an MLE estimator can generally be unbounded. Therefore, we project the MLE estimator to the space with bounded norms, ensuring the estimation is in the confidence set [1,2]. We have made this motivation more clear in the revised version.\n\n**W4: Typo errors.**\n\n**A for W4:**\nWe thank the reviewer for pointing it out, and we have substantially revised the manuscript.\n\n**W5 & Q8: What the counterfactual trajectories are? What do counterfactual trajectories have to do with weighting?**\n\n**A for W5  Q8:**\nThe counterfactual trajectories in Figure 2(c) refer to the two colored trajectories that are out of the dataset. We can not infer the preference between these two out-of-dataset trajectories if weighted queries are not allowed. \n\nAs explained in W1, by adding weights to trajectories, we can learn better credit assignments. \nWe can use the former 2-stage task in W1 as an example. \nWithout weighted queries, we can not infer the preference between the counterfactual trajectories $\\tau'\\_1$ and $\\tau'\\_2$, where $\\tau'\\_1$ acts as $\\tau\\_1$ at step 1 and acts $\\tau\\_2$ at step 2, and $\\tau'\\_2$ acts as $\\tau\\_2$ at step 1 and acts $\\tau\\_1$ at step 2. \nWith weighted queries, we can identify the preference at each step.\nTherefore, weighted queries help reason about preferences between \n the counterfactual trajectories $\\tau'\\_1$ and $\\tau'\\_2$ without generating new trajectories.\n\n**Q1: What is $T$ in \u201cquerying with an offline dataset can be much more sample efficient when N >> T\u201d at the end of Section 3.1?**\n\n**A for Q1:**\nThanks for pointing it out. This is a typo, and it should be $K$, the number of comparisons. Intuitively, online preference-based RL requires online samples to learn both dynamics and the preference function. In contrast, in offline settings, the dynamics information is already contained in the dataset, and we only need to infer the preference function, which requires fewer samples than the online one.\n\n**Q2: What is ``True Reward'' in the table 1/2?**\n\n**A for Q2:**\nIn Table 1/2, ``True Reward'' denotes the performance of offline RL algorithms under the original reward function of the dataset, which can be seen as the oracle performance for preference-based reward learning methods.\nWe have added a detailed explanation in the revised version to enhance clarity.\n\n**Q3 & Q4: Can you explain why you augment the trajectories with a truncated Gaussian? Why a truncated Gaussian rather than some other random variable?**\n\n**A for Q3 & Q4:**\nThe motivation for using weighted queries is explained in W1.\n\nWe clip the range of the weight to avoid large weights that can lead to high variance in the learning process, as commonly used in practice [3].\nWe conduct additional ablation studies to demonstrate the importance of weight clipping in the chain MDP experiment shown in Figure 10 (a) in Appendix G. \n\nUsing other random variables is also possible, but we found that Gaussian weights perform better in practice. Please refer to Appendix G's Figure 10 (b) for an ablation study."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631219390,
                "cdate": 1700631219390,
                "tmdate": 1700631327695,
                "mdate": 1700631327695,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "La6HOsiKXl",
                "forum": "GOvTGntFNj",
                "replyto": "3juJghyhPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission863/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i96V (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q5 & Q6: Is an ensemble of reward functions going to give reasonable uncertainty estimates of the preference model? What if preferences have high aleatoric uncertainty?**\n\n**A for Q5 & Q6:**\nIt is a common practice to use an ensemble of reward functions to estimate the uncertainty of the preference model, and they are known to lead to good empirical performance [4,5]. Under the Bradley-Terry preference model, the aleatoric uncertainty is accounted for as a soft probability rather than a one-hot probability. In general cases, advanced methods like bootstrapping have the potential to improve uncertainty estimation further and can separate aleatoric and epistemic uncertainties [6], which is an interesting direction for future work.\n\n\n**Q7: How is the optimization problem in (13) solved?**\n\n**A for Q7:**\nEquation (13) takes the pair of trajectories that maximize the value difference. Note that there is no optimization procedure (for continuous variables) needed, and we only need to search for the best candidate, which requires $O(M^2N^2)$ time, where $M$ is the number of sampled trajectories, and $N$ is the number of Q-values.\n\n\n**Q9: How does this policy error bound compare to, e.g., a G-optimal design and then PEVI?**\n\n**A for Q9:**\nWithout weighted queries, pure offline methods are known to suffer from an exponential policy error bound [7]. When using weighted queries, a G-optimal design and then PEVI would yield a similar performance bound, but the second term consisting of the number of queries $K$ in Equation (10) would depend on an additional coverage coefficient [8]. This can lead to suboptimal performance when the dataset has poor coverage, which motivates us to introduce exploration into offline PbRL.\n\n**Q10: How does this policy error bound compare to the Pacchiano et al result?**\n\n**A for Q10:**\nPacchiano et al. focus on the pure online setting, considering two cases: a known model and an unknown one to be learned online. Our result considers the offline case, where the model is estimated via an offline dataset, which requires a careful treatment balancing offline exploitation (for dynamics estimation) and online exploration (for preference learning).\n\n\n**Q11: What are the steps in the $x$ axis of Figure 3?**\n\n**A for Q11:**\nFigure 3 depicts the performance of the offline RL algorithm as a variable of the offline training procedure.\nThe $x$-axis represents the training steps of the offline RL algorithm. We have made this clear in the revised version.\n\n\nThanks again for the valuable comments. \nWe sincerely hope our additional\nexplanations have cleared the concern. \nMore comments on further improving the presentation are also very much welcomed.\n\n[1] Faury, Louis, et al. \"Improved optimistic algorithms for logistic bandits.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Filippi, Sarah, et al. \"Parametric bandits: The generalized linear case.\" Advances in neural information processing systems 23 (2010).\n\n[3] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[4] Christiano, Paul F., et al. \"Deep reinforcement learning from human preferences.\" Advances in neural information processing systems 30 (2017).\n\n[5] Shin, Daniel, Anca Dragan, and Daniel S. Brown. \"Benchmarks and Algorithms for Offline Preference-Based Reward Learning.\" Transactions on Machine Learning Research (2022).\n\n[6] Chua, Kurtland, et al. \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models.\" Advances in neural information processing systems 31 (2018).\n\n[7] Zhan, Wenhao, et al. \"Provable Offline Reinforcement Learning with Human Feedback.\" arXiv preprint arXiv:2305.14816 (2023).\n\n[8] Agarwal, Alekh, et al. \"Reinforcement learning: Theory and algorithms.\" CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32 (2019)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission863/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631281748,
                "cdate": 1700631281748,
                "tmdate": 1700631315930,
                "mdate": 1700631315930,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]