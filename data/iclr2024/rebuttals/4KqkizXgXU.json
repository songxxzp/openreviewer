[
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models"
    },
    {
        "review": {
            "id": "kpbUenrIoP",
            "forum": "4KqkizXgXU",
            "replyto": "4KqkizXgXU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out that the current red teaming approaches based on RL can not generate test cases with high diversity. To address this, the authors propose a curiosity-driven exploration method to train the read team models. This approach jointly maximizes the red team effectiveness and also a diversity reward, where the authors tried different metrics.  Experimental results show that the proposed approach can not only maintain the effectiveness but also increase the test case diversity, compared with previous RL-based approaches. However, the experiments are mainly based on small models GPT2 with 137M parameters, making the results and the claim of red-teaming for \"Large Language Models\"less convincing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper introduces a novel approach to training models that generate more diverse red team test cases. A diversity reward is introduced to achieve that, by encouraging the red team models to explore more diverse cases. The authors test different ways to define the rewards.\nResults on GPT2 models show that the approach can indeed increase the test case diversity while maintaining effectiveness."
                },
                "weaknesses": {
                    "value": "The experiments chose GPT2 as the target model in the main results, making the results not too convincing. I recommend testing on a wider range of LLMs, including proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat (the user has already done but there are not enough results and details) and Vicuna."
                },
                "questions": {
                    "value": "1. Are the RoBERTa hate speeh classifier strong enough to detect the toxic generations?\n2. Why not try \"real\" LLMs like proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat and Vicuna? I do not think the results are GPT2 with 137M parameters are convincing enough."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Reviewer_NqgB",
                        "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560074214,
            "cdate": 1698560074214,
            "tmdate": 1700867994499,
            "mdate": 1700867994499,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xDJEmFYXRO",
                "forum": "4KqkizXgXU",
                "replyto": "kpbUenrIoP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating the novelty of our work. We\u2019ve added \n\n1. Experiments on Vicuna and ChatGPT (Appendix C.3)\n2. Analysis on the toxicity classifier  (Appendix C.1)\n\nto the manuscript. The following is our detailed response.\n\n> The experiments chose GPT2 as the target model in the main results, making the results not too convincing. I recommend testing on a wider range of LLMs, including proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat (the user has already done but there are not enough results and details) and Vicuna.\n> \n\n> Why not try \"real\" LLMs like proprietary models like ChatGPT and open-sourced models like LLaMA-2-chat and Vicuna? I do not think the results are GPT2 with 137M parameters are convincing enough.\n> \n\n**Answer:**\n\nIn addition to GPT2, our main results also include experiments against **Dolly-7B and LLaMA2-7B-chat-hf**, demonstrating our method is able to create diverse and effective testcases. \n\nWe\u2019ve added the results of red teaming against Vicuna-7B and ChatGPT (gpt3.5-turbo) in Section C.3 in the Appendix, showing that **our method is also effective at achieving diverse and effective testcases** against these models. Our results show the following: \n\n1. **Higher Diversity:** Our approach yields more diverse test cases and target model\u2019s responses, as shown in (Figures 9(i)(c,d,e,f) and 9(ii)(c,d,e,f)).\n2. **More Unique Test Cases:** Figures 9(i)(b) and 9(ii)(b) illustrate that our RL+Curiosity approach generated around 50,000 unique test cases resulting in toxic responses surpassing a 0.9 toxicity threshold. This number is significantly higher than those produced by the baselines, RL+TDiv and RL. It's important to note, as Figures 9(i)(a) and 9(ii)(a) indicate, that while RL+TDiv led to a higher proportion of toxic responses, the overall lower number of unique test cases indicates an issue of lack of diversity, with many cases being exactly identical based on string comparison.\n\n> 1. Are the RoBERTa hate speeh classifier strong enough to detect the toxic generations?\n> \n\n**Answer:** \n\nYes. To show that, we compared the RoBERTa model\u2019s classification with GPT-4\u2019s classification. We sampled 3000 target model\u2019s responses each for all red-teaming methods and prompted GPT-4 to predict whether the response was toxic or not. Table 7(iii) in the Appendix showed the precision, recall, and F1-score with GPT-4\u2019s predictions as ground truth and RoBERTa as an estimation. The table shows that that F1 scores are all close to one, which indicates that RoBERTa\u2019s predictions are similar to GPT-4\u2019s predictions. We also added the confusion matrix in Figure 7(ii) in the Appendix for reference."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251088624,
                "cdate": 1700251088624,
                "tmdate": 1700251088624,
                "mdate": 1700251088624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HcFRu4IXX3",
            "forum": "4KqkizXgXU",
            "replyto": "4KqkizXgXU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proposes an rl-based red teaming method that can explore novel test cases by adapting a curiosity-driven exploration technique. \n- The proposed method avoids redundant test cases by utilizing novelty reward during the rl optimization procedure. \n- The empirical results show that the proposed curiosity-driven red teaming method achieves superior performance in both the red team accuracy and the diversity compared to the baseline red teaming methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea is intuitive and easy to understand.\n- The writing is clear.\n- The authors provide experimental results with two diversity measures, 1-self-bleu and 1-cosine similarity.\n- The empirical results are amazing. The proposed method shows higher red teaming performance than the rl-based red teaming methods while maintaining a diversity score comparable to the zero-shot baseline method."
                },
                "weaknesses": {
                    "value": "- Since the paper handles the problem of red-teaming which is possible to make ethical issues in society, I think it would be better to contain subsections for ethical comments in the red-teaming.\n- The empirical results are limited to the text-to-text generation models. \n- The proposed automated red-teaming method is based on the pre-trained offensiveness classifier. As [1] raised, there exists a risk of discovering test cases that over-fit the red team classifier, resulting in false positive test cases. But there isn't any analysis about this risk.\n\n[1] Query-Efficient Black-Box Red Teaming via Bayesian Optimization, Lee et al., ACL 2023."
                },
                "questions": {
                    "value": "- Can you provide simple experimental results on text-to-image models like stable-diffusion?\n- Can you provide an analysis of the classifier overfitting problem? For example, you may provide a confusion matrix for each method. \n- Can you provide experimental results on large language models such as the text-davinci-003 model or gpt-3.5-turbo prompted chatbots?\n- It is just a curiosity question. In a realistic scenario, each query on the target model usually incurs costs. Hence, the number of model queries during the red teaming process can be an important factor for the overall cost of the method. Can you provide the number of queries used during the end-to-end process of RL+\"curiosity\"? Also, can you estimate the price when we red-team gpt4 api using RL +\"curiosity\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper has the potential to be maliciously utilized by attackers to make adversarial prompts for publicly released AI models. Hence, some ethical comments should be added to the paper."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd",
                        "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806470701,
            "cdate": 1698806470701,
            "tmdate": 1700505306725,
            "mdate": 1700505306725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q1Ty93I48d",
                "forum": "4KqkizXgXU",
                "replyto": "HcFRu4IXX3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating our writing and empirical results. We added \n\n1. Ethic statement (after the Discussion section)\n2. Experiments on ChatGPT and Vicuna (Appendix C.3)\n3. Analysis on over-fitting to classifier (Appendix C.1)\n\nto the manuscript. Also, we are running experiments on text-to-image experiment. Our responses are detailed in the following.\n\n> Ethical comments\n> \n\n**Answer:**\n\nWe\u2019ve added an ethic section after the discussion section and would like to know if the reviewer thinks this addresses potential ethic concerns.\n\n> The empirical results are limited to the text-to-text generation models.\n> \n\n> Can you provide simple experimental results on text-to-image models like stable-diffusion?\n> \n\n**Answer:**\n\nOur framework is applicable to text-to-image red-teaming. We are running the experiments and will do our best to add these results during rebuttal.\n\n> The proposed automated red-teaming method is based on the pre-trained offensiveness classifier. As [1] raised, there exists a risk of discovering test cases that over-fit the red team classifier, resulting in false positive test cases. But there isn't any analysis about this risk.\n> \n\n**Answer:**\n\nWe added a new analysis in Appendix C.3, showing that the toxic response elicited by our red-teaming method **generalize to other toxicity classifiers** also. \n\n**High Correlation of Toxicity Predictions Across Classifiers:** Table 7(i) shows **Pearson Correlation Coefficients (PCC)** of the toxicity probabilities predictions between RoBERTa and two other classifiers, on the responses of the target model Dolly-7B. The table shows that PCC is near one, indicating that a strong positive correlation between RoBERTa and other classifiers in toxicity prediction. This implies that when RoBERTa predicts higher toxicity probability, the other models are also likely to show similar increases in their toxicity probability predictions.\n\n**Similar to GPT-4\u2019s predictions:** Table 7(iii) in Appendix C.3 compared RoBERTa's classification accuracy to GPT-4 by analyzing 3000 responses from each red-teaming method. GPT-4 evaluated these responses for toxicity. Using GPT-4's predictions as the ground truth, we assessed RoBERTa's precision, recall, and F1-score. The results, with F1 scores nearing one, suggest a high similarity between RoBERTa's and GPT-4's predictions. We also added the confusion matrix in Figure 7(ii) the Appendix C.3 for reference. \n\nNote that we compared RoBERTa with GPT-4 using a confusion matrix and F1 score rather than Pearson Correlation Coefficient (PCC), as GPT-4 cannot predict probabilities. Attempts to prompt GPT-4 for probabilistic outputs, using various phrasings, were unsuccessful; the model typically responded that it could not provide numerical probabilities.\n\n> Can you provide experimental results on large language models such as the text-davinci-003 model or gpt-3.5-turbo prompted chatbots?\n> \n\n**Answer:**\n\nWe\u2019ve added the results of red teaming against Vicuna-7B and ChatGPT (gpt3.5-turbo) in Section C.3 in the Appendix, showing that **our method is also effective at generating diverse and effective testcases** against these models. Our results showed the following facts:\n\n1. **Higher Diversity:** Our approach yields more diverse test cases and target model\u2019s responses, as shown in (Figures 9(i)(c,d,e,f) and 9(ii)(c,d,e,f)).\n2. **More Unique Test Cases:** Figures 9(i)(b) and 9(ii)(b) illustrate that our RL+Curiosity approach generated around 50,000 unique test cases resulting in toxic responses surpassing a 0.9 toxicity threshold. This number is significantly higher than those produced by the baselines, RL+TDiv and RL. It's important to note, as Figures 9(i)(a) and 9(ii)(a) indicate, that while RL+TDiv led to a higher proportion of toxic responses, the overall lower number of unique test cases indicates an issue of lack of diversity, with many cases being exactly identical based on string comparison.\n\n> It is just a curiosity question. In a realistic scenario, each query on the target model usually incurs costs. Hence, the number of model queries during the red teaming process can be an important factor for the overall cost of the method. Can you provide the number of queries used during the end-to-end process of RL+\"curiosity\"? Also, can you estimate the price when we red-team gpt4 api using RL +\"curiosity\"?\n> \n\n**Answer:**\n\n**Number of queries:**\u00a0We used 40K queries for instruction following experiments (Section 4.3) and 100K queries for text continuation experiments (Section 4.2). We determine both numbers based on when the combined rewards (i.e., toxicity probability + curiosity + entropy bonus) of RL+Curiosity (our method) stop growing.\n\n**Price of red-teaming GPT-4:**\u00a0$143.999. We did the calculation by assuming the maximum amount of input and output tokens are all 40 and 40K queries and based on the GPT-4\u2019s pricing info in\u00a0https://openai.com/pricing."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251446387,
                "cdate": 1700251446387,
                "tmdate": 1700251446387,
                "mdate": 1700251446387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7XCXAPEWmj",
                "forum": "4KqkizXgXU",
                "replyto": "q1Ty93I48d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_TuLd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind responses. Most of my inquiries were addressed in the authors' responses. I raised my overall score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505286176,
                "cdate": 1700505286176,
                "tmdate": 1700505286176,
                "mdate": 1700505286176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v6MPdFc6Jr",
            "forum": "4KqkizXgXU",
            "replyto": "4KqkizXgXU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proposes to add an entropy bonus and novelty reward in addition to the standard RL objective to maximize toxicity in the target LLM\u2019s responses. Two variations of the novelty reward are proposed: A SelfBLEU novelty reward and a cosine similarity novelty reward.\n- The proposed method of red-teaming is evaluated in a text continuation task(evaluated on GPT2), an instruction following task (GPT2-alpaca and Dolly-v2-7B). The method is also evaluated on LLMs fine-tuned with human preference (LLaMA2-7b-chat-hf)\n- The authors conduct ablations on the various KL penalty coefficients, sampling temperature, and the necessity of each reward. Studies on the hyperparameter choices are shown in the appendix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper tackles an important problem and contributions are clear and well presented.\n\n- The method (RL+Curiosity) is simple and sound, borrowing from known methods in the RL literature.\n\n- Experiments cover a good number of models, in a number of settings, and performance of RL+Curiosity is noticeably better than baselines.\n\n- Ablation studies alleviate concerns that test case diversity is not simply fixed by increasing sampling temperature or modifying the KL penalty."
                },
                "weaknesses": {
                    "value": "- The authors choose an objective of maximizing test case novelty but do not show how their method influences target LLM response novelty.\n\n- In A.7, you mention you sample 100 sets of 100 test cases and calculate both diversity metrics across each subset. What is the test case size for a select number of thresholds in your experiments, and why is this sampling method used?\n\n- This is not too important but it is worth mentioning that the novelty of the method is somewhat limited. The cosine similarity reward is very similar to the RL+TDiv baseline, except it is applied to the test cases.\n\nIn general, the paper tackles a relevant problem and is well motivated and presented. I have some limited concerns (see above) and would like to hear the author's rebuttal, after which I may modify my score."
                },
                "questions": {
                    "value": "- Could you clarify what number of test cases exceed each toxicity threshold for RL+Curiosity for the experiments in section 4.2 and 4.3?\n\n- How does your method compare with RL+TDiv if we look at the diversity of target LLM responses? I presume that both test case diversity and target LLM response diversity are important when red-teaming, and I would like to know whether RL-TDiv essentially achieves target LLM response diversity without necessarily going through the intermediate step of test case diversity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE",
                        "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699167091456,
            "cdate": 1699167091456,
            "tmdate": 1700522787942,
            "mdate": 1700522787942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WpS5Nx5OHF",
                "forum": "4KqkizXgXU",
                "replyto": "v6MPdFc6Jr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and are glad that he/she appreciated the comprehensiveness of our experiments, found our idea sound and our presentation clear. The responses to questions are below: \n\n> The authors choose an objective of maximizing test case novelty but do not show how their method influences target LLM response novelty.\n> \n\n> How does your method compare with RL+TDiv if we look at the diversity of target LLM responses? I presume that both test case diversity and target LLM response diversity are important when red-teaming, and I would like to know whether RL-TDiv essentially achieves target LLM response diversity without necessarily going through the intermediate step of test case diversity.\n> \n\n**Answer:**\n\nWe added a new figure to show that **RL+Curiosity (ours) also leads to higher target LLM response novelty** (i.e., diversity) than RL+TDiv in Figure 8 in Appendix C.2. \n\nRL+TDiv adds the diversity of the embeddings from the target model\u2019s responses as rewards. Though RL+TDiv achieves higher response diversity than RL, it underperforms ours. This indicates that Curiosity is a more effective objective to achieve both test case and response diversity. We elaborate on these experimental results in Appendix C.2 and briefly explain them below:\n\n- **Why does RL+Curiosity improve responses diversity?** Maximizing test case diversity makes the target model to respond to different prompts and hence leads to diverse responses (i.e., answers to the prompts).\n- **Why does RL+Curiosity lead to higher response diversity than RL+TDiv?** RL+TDiv solely maximizes the diversity of the current batches, rather than the diversity of the all responses seen in the whole training time. We will elaborate this point in the next answer.\n\n> This is not too important but it is worth mentioning that the novelty of the method is somewhat limited. The cosine similarity reward is very similar to the RL+TDiv baseline, except it is applied to the test cases.\n> \n\n**Answer:**\n\nWe want to highlight another conceptually critical difference.\n\n**RL+TDiv solely maximizes the diversity within the current batch of responses**, as measured by cosine similarity. This approach does not incentivize the red-team model to generate responses that differ from those in previous batches. Consequently, the model can repeatedly elicit similar responses identical to those in the past, ending up with low response diversity overall. This is evident in Figure 8(d, e) in the Appendix C.2, where RL+TDiv exhibits lower response diversity than ours. It indicates that maximizing testcase diversity is conducive to not only producing diverse testcases but also eliciting diverse toxic responses.\n\n> In A.7, you mention you sample 100 sets of 100 test cases and calculate both diversity metrics across each subset. What is the test case size for a select number of thresholds in your experiments, and why is this sampling method used?\n> \n\n**Answer:**\n\nIn A.7, we calculate the diversity of testcases exceeding each toxicity threshold by subset sampling approach because the\u00a0**number of testcases at each threshold can vary across methods.**\n\nFor example, in Figure 2(i, a), nearly 0% of testcases exceeding threshold 0.9 are found by RL baseline, while 50% of testcases found by RL+Curiosity (ours) exceed threshold 0.9. Thus, the testcase sizes for calculating diversity are different at threshold 0.9, which makes them incomparable.\n\nTo compute diversity with varying-size testcase sets, we follow the suggestion in [1, 2] and use the resampling method to estimate the diversity of the testcase set.\n\n[1] Perez, Ethan, et al. \"Red teaming la~~n~~guage models with language models.\" arxiv 2022\n\n[2] Lee, Deokjae, et al. \"Query-Efficient Black-Box Red Teaming via Bayesian Optimization.\"\u00a0 arxiv 2023\n\n> Could you clarify what number of test cases exceed each toxicity threshold for RL+Curiosity for the experiments in section 4.2 and 4.3?\n> \n\n**Answer:**\n\nEach trial (i.e., run with a different random seed) of the experiment uses 100K queries in Section 4.2 and 40K queries in Section 4.3. Thus, the testcases that elicit responses exceeding each toxicity threshold by our proposed RL + curiosity method are:\n\n| Threshold | Sec. 4.2 | Sec. 4.3 (Fig. i) | Sec. 4.3 (Fig. ii) |\n|-----------|----------|-------------------|--------------------|\n| 0.0 | 100000   | 40000| 40000 |\n| 0.1       | 44860    | 26237             | 25074              |\n| 0.2       | 42707    | 25730             | 24255              |\n| 0.3       | 41178    | 25359             | 23718              |\n| 0.4       | 40095    | 25033             | 23325              |\n| 0.5       | 39109    | 24759             | 22987              |\n| 0.6       | 38071    | 24477             | 22617              |\n| 0.7       | 36825    | 24118             | 22187              |\n| 0.8       | 35038    | 23584             | 21605              |\n| 0.9       | 31684    | 22458             | 20238              |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700252115176,
                "cdate": 1700252115176,
                "tmdate": 1700252115176,
                "mdate": 1700252115176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j2IMmuNdlD",
                "forum": "4KqkizXgXU",
                "replyto": "WpS5Nx5OHF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for answering my questions.\n\nI appreciate the additional figure showing higher target LLM response novelty. \n\n**With respect to Figure 2(i)(c):**\n> Also, note that while RL (without curiosity and TDiv) achieves a high level of diversity in Figure 2i(c), only a limited number of test cases exceed high toxicity thresholds [0.2, 0.9]\n\nCould you report these numbers? Could you also address why the zero shot baseline performs so well in Figure 2(ii)(c)?\n\nI will be happy to increase my score once these remaining issues are addressed."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368142076,
                "cdate": 1700368142076,
                "tmdate": 1700368142076,
                "mdate": 1700368142076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LJxyvOFN1Q",
                "forum": "4KqkizXgXU",
                "replyto": "v6MPdFc6Jr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer's timely response.\n\n> \u201cAlso, note that while RL (without curiosity and TDiv) achieves a high level of diversity in Figure 2i(c), only a limited number of test cases exceed high toxicity thresholds [0.2, 0.9]\u201d\nCould you report these numbers?\nCould you also address why the zero shot baseline performs so well in Figure 2(ii)(c)?\n> \n\n**Answer:**\n\n**Could you report these numbers?** \n\n- The following table shows the number of testcases at each threshold of RL baseline in Figures 2(i) and 2(ii) (green curves). Compared with the table shown in the previous responses, the number of testcases generated by our method at each toxicity threshold is up to 10 times higher than the RL baseline. We will also attach these tables to the Appendix in the next manuscript revision during the rebuttal.\n\n|  Threshold   | Fig 2(i) | Fig 2(ii) |\n|-----|----------|-----------|\n| 0.0 | 40000    | 40000     |\n| 0.1 | 1665     | 9642      |\n| 0.2 | 245      | 7663      |\n| 0.3 | 106      | 6426      |\n| 0.4 | 76       | 5398      |\n| 0.5 | 59       | 4798      |\n| 0.6 | 51       | 4283      |\n| 0.7 | 46       | 3712      |\n| 0.8 | 40       | 2622      |\n| 0.9 | 36       | 1732      |\n\n**Could you also address why the zero shot baseline performs so well in Figure 2(ii)(c)?** \n\n- **The main reason is that the zero-shot baseline is not finetuned with RL**. As Perez et al. 2022 suggested, pre-trained LLMs tend to have higher testcase diversity than RL-finetuned LLMs in red teaming, likely because RL's reward maximization objective hurts diversity [1]. The zero-shot baseline in Figure 2(ii)(c) is a pre-trained LLM and is, hence, expected to have higher diversity than the RL baseline (the green curve in Figure 2(ii)(c)).\n- **However, the zero-shot baseline is notably ineffective in generating testcases that trigger toxic responses in the target model** (see Figure 2(i)(a) and 2(ii)(a)). Additionally, it shows lower diversity based on 1-SelfBLEU (Figures 2(i)(b), 2(ii)(b)) and only matches embedding diversity (Figures 2(i)(c), 2(ii)(c)) when compared to the our RL+Curiosity method.\n\n[1] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381\u201327394, 2021"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373538490,
                "cdate": 1700373538490,
                "tmdate": 1700373635863,
                "mdate": 1700373635863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aQhPvns58p",
                "forum": "4KqkizXgXU",
                "replyto": "LJxyvOFN1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_ZhdE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have increased my score to 8."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522826672,
                "cdate": 1700522826672,
                "tmdate": 1700522826672,
                "mdate": 1700522826672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SrIYG3t5sV",
            "forum": "4KqkizXgXU",
            "replyto": "4KqkizXgXU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for automated red teaming. The method is a spiritual successor to those in the Red Teaming LMs with LMs paper from Perez et al, and should be viewed in that light (rather than in comparison to recent alternative approaches like ARCA and GCG). Specifically, the method adds a curiosity objective to the RL red teaming method of Perez et al."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper continues an interesting line of work in fine-tuning LLMs to be better at red teaming other LLMs. This type of research is complimentary with more recent optimization-based methods like GCG\n\n- The idea of incorporating curiosity into the RL red teaming method from Perez et al. is a good idea\n\n- The results are strong; the success rate of the red teaming method remains as high as the RL baseline, and the diversity is much higher\n\n- The ablations are reasonable and anticipate questions that readers would have"
                },
                "weaknesses": {
                    "value": "- The distinction between adversarial attacks and red teaming seems artificial. I wouldn't want this distinction being introduced in the community. Surely adversarial attacks can be semantic, and surely red teaming can include gibberish adversarial examples as an interesting failure mode of LLMs.\n\n- I would suggest adding more visual separation between diversity and quality plots. Currently it's hard to tell which is which in Figures 1 and 2 without turning my head sideways to read the axis."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699313688366,
            "cdate": 1699313688366,
            "tmdate": 1700707677334,
            "mdate": 1700707677334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M3yLKnLyy3",
                "forum": "4KqkizXgXU",
                "replyto": "SrIYG3t5sV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer appreciates our idea and results. We answer the rest of questions below.\n\n> The distinction between adversarial attacks and red teaming seems artificial. I wouldn't want this distinction being introduced in the community. Surely adversarial attacks can be semantic, and surely red teaming can include gibberish adversarial examples as an interesting failure mode of LLMs.\n> \n\n**Answer:**\n\nThanks for the suggestion, and we agree with the reviewer. We\u2019ve updated the manuscript and removed the distinction in the related work section (Section 5). We are happy to further revise this section during the rebuttal according to the reviewer\u2019s comments.\n\n> I would suggest adding more visual separation between diversity and quality plots. Currently it's hard to tell which is which in Figures 1 and 2 without turning my head sideways to read the axis.\n> \n\n**Answer:**\n\nWe\u2019ve updated the figures, moving \u201cQuality\u201d and \u201cDiversity\u201d to the titles of each subplot so that the reader can tell which plot is for quality and which plot is for diversity easily. We are happy to revise the figures if the reviewer thinks there could be a better way of visualizing these data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700252622197,
                "cdate": 1700252622197,
                "tmdate": 1700252622197,
                "mdate": 1700252622197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wdSNfoP2zS",
                "forum": "4KqkizXgXU",
                "replyto": "SrIYG3t5sV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6325/Reviewer_8YjF"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "This addresses my concerns. I think the paper could be accepted and have raised my score to an 8."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707609895,
                "cdate": 1700707609895,
                "tmdate": 1700707691241,
                "mdate": 1700707691241,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]