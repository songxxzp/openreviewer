[
    {
        "title": "Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphically Encryption: A Dynamic Programming Approach"
    },
    {
        "review": {
            "id": "ZnNAaB92Zi",
            "forum": "Mhu9iNGKqP",
            "replyto": "Mhu9iNGKqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_ov7g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_ov7g"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a layer-wise approximation of activations to reduce the FHE-based secure inference time. By considering the characteristics of each layer, the proposed layer-wise approximation does not require re-training while maintaining the model performance (e.g., accuracy). In terms of polynomials approximations, this paper employs the weighted least squares approximation method with the input distribution to approximate the activation function and utilizes a dynamic programming algorithm to reduce the approximation polynomials\u2019 degrees. In terms of the FHE ciphertext evaluation, the authors propose modulating the ciphertext moduli-chain layer-wise to reduce the inference time. Compared to prior works, this paper reduces the secure inference time by over 3 times on ResNet-20/ResNet-32 on dataset CIFAR-10 with negligible accuracy loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper studies the input distribution of the activation, uses the weighted least squares approximation method, and presents a dynamic programming algorithm designed to determine the optimal degree set for each layer. This algorithm efficiently obtains the optimal polynomial degrees, minimizing the overall negative impact on classification accuracy. The authors give a detailed mathematical analysis of the polynomial approximation method and dynamic programming design.\n2. The authors propose a moduli-chain management method to achieve additional reductions in inference runtime. This method focuses on removing unused moduli. Instead of using a single moduli chain, the authors propose to use multiple moduli chains for each depth of the activation approximation polynomial function. In this way, this paper can reduce the number of bootstrapping operations."
                },
                "weaknesses": {
                    "value": "1. The authors only evaluate the proposed method on ReLU. However, other activation functions, e.g., Swish, Sigmoid, GeLU, are not evaluated.\n\n2. Some visualization of (1) the ReLU function, (2) the proposed approximation, and (3) other baselines will be helpful to understand the effect of the proposed approximation.\n\n3. The authors need to show the improvement introduced by each technique, including the weighted MSE over minimax, layerwise over uniform, dynamic programming over simple greedy, etc. Right now, when compared with Lee et al., 2022a, it seems most of the benefits come from minimizing the weighted MSE, which drastically reduces the polynomial order.\n\n4. Only a simple dataset, i.e., Cifar-10, is demonstrated. Larger datasets are important as they may impact the distribution of the activation functions as well as the required approximation accuracy."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443198166,
            "cdate": 1698443198166,
            "tmdate": 1699636754988,
            "mdate": 1699636754988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6bCKWHndno",
                "forum": "Mhu9iNGKqP",
                "replyto": "ZnNAaB92Zi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer ov7g"
                    },
                    "comment": {
                        "value": "Thank you for providing detailed insights into the aspects that need to be supplemented in our research. Your reviews are greatly appreciated.\n\n**Simulations for other activations**: The optimized approximation method we propose is adaptable to other activation functions, as outlined in Theorem 2.2. Given that this theorem establishes the minimization of mean squared error (MSE) by the derived polynomial, we can theoretically regard it as the optimal polynomial for minimizing inference errors. Furthermore, the ReLU function, in contrast to other activation functions, is non-differentiable, which can result in substantial errors. \n\nAlso, our findings show that when approximating ReLU using the weighted least squares method, the MSE is higher than other differentiable activation functions (e.g., Swish, Sigmod, GELU). See the table below which presents the minimized for each activation where $\\mu=0$ and $\\sigma^2=1$. The minimized MSE can be evaluated by the second equation in Theorem 2.2.\n\n| Degree | ReLU | GELU | Swish | Sigmoid |\n|:---:|:---:|:---:|:---:|:---:|\n| 0 | 0.34085 | 0.34564 | 0.31308 | 0.04338 |\n| 1 | 0.09085 | 0.09564 | 0.06308 | 0.00069 |\n| 2 | 0.00612 | 0.00113 | 0.00154 | 0.00069 |\n| 3 | 0.00612 | 0.00113 | 0.00154 | 0.00004 |\n\nThe efficacy of our method in approximating the challenging ReLU function supports that it should perform effectively in approximating other activation functions as well.\n\n**Visualization for the proposed approximate polynomial**: In the final version of the paper, we will include a graph comparing the characteristics of the existing minimax polynomial with those of the proposed polynomial. This will aid in better understanding the differences between the two.\n\n**Contributions for each proposed technique**: We provide the simulation results that show the improvement by each technique.\n\nFirst, we compare the results of the minimax polynomial and weighted LS with uniform degree for each layer. We additionally simulate on ResNet pre-trained with CIFAR-10. The minimax polynomial results are presented in Table 2. Applying the minimax polynomial with a degree of 2,835 (depth=13) leads to a degradation in the performance of the original model by -1.92%. However, the weighted LS method does not require such a large degree (or depth) to maintain the performance of the original model. See the table below providing the results of polynomial approximation with weighted LS.\n\n| Backbone | Original | Degree=15(Depth=4) | 31(5) | 63(6) | 127(7) | 255(8) |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| ResNet-20 | 91.52% | 88.50% | 91.13% | 91.70% | 91.60% | 91.59% |\n| ResNet-32 | 92.49% | 86.02% | 91.24% | 92.39% | 92.66% | 92.62% |\n\nOur simulation indicates that approximate polynomials generated by the weighted LS method with degrees 31 and 63 are sufficient to maintain the original classification accuracy for ResNet-20 and 32, respectively.\n\nNext, we compare the method of using different polynomial degrees per layer with the approach of using a uniform degree. The table below compares the expected time latency ($\\sum_{i=1}^L T_{i}^{\\text{Rel},\\nu}(d_i)$, see equation 4 in the paper) required by each degree-setting method at the point where there is a 1% degradation in the performance of the original model .\n\n| Backbone | Uniform | Greedy | Proposed |\n|:---:|:---:|:---:|:---:|\n| ResNet-20 | 347.50 | 340.00 | 339.50 |\n| ResNet-32 | 652.50 | 600.75 | 600.25 |\n\nAs evident from the table above, when applying a uniform degree, the time latency is highest, while utilizing dynamic programming to employ an optimal depth results in the lowest latency. Hence, our approach can be considered the most efficient in terms of minimizing time latency.\n\n**Simulation for various dataset**: We agree that the various simulations for many datasets are important. However, the challenges for large dataset in FHE-based PI still need further exploration. Pre-trained models that classify such datasets tend to have more intricate functions requiring optimization. For instance, non-linear functions such as max-pooling involve complex multivariate functions, and there hasn't been a suggested method to effectively approximate these functions in encrypted data for use in DNN inference. Moreover, due to current time latency issues, attempts to classify extensive datasets like TinyImageNet without retraining are restricted to operating on the final layers with encrypted data using FHE [1].\n\n[1] D. Kim et al, \"Optimized Privacy-Preserving CNN Inference With Fully Homomorphic Encryption.\" IEEE Transactions on Information Forensics and Security 18 (2023): 2175-2187."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538505961,
                "cdate": 1700538505961,
                "tmdate": 1700538505961,
                "mdate": 1700538505961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YPtGQqTBLH",
            "forum": "Mhu9iNGKqP",
            "replyto": "Mhu9iNGKqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_8y16"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_8y16"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper proposes a layerwise degree optimization method for activation functions in fully homomorphic encryption (FHE) to reduce inference time while maintaining classification accuracy. \n2. The previous approaches approximated activation functions uniformly, but this work takes into account the unique characteristics of each layer. \n3. The simulations were performed using the Lattigo library on a high-performance computing system."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author target the polynomial approximation problem for private Inference acceleration, which is a necessary and essential part for FHE-based private inference.\n2. The authors employ the weighted least squares approximation method and optimize the degrees of activation functions using a dynamic programming algorithm. \n3. They also propose modulating the ciphertext moduli-chain layerwise to further reduce inference time. \n4.Experimental results on the CIFAR-10 dataset using the ResNet model show that the proposed method significantly reduces inference times compared to previous approaches."
                },
                "weaknesses": {
                    "value": "1. I just have one major concern. There exist some work use 2-degree polynomial approximation [1,2] with ignorable accuracy loss. How does the proposed method compared to other advanced low-degree polynomial approximation techniques? As a low-degree polynomial approximation would outperforms the results in this work.\n\nReference:\n\n[1] Park, Jaiyoung, et al. \"AESPA: Accuracy preserving low-degree polynomial activation for fast private inference.\" arXiv preprint arXiv:2201.06699 (2022).\n[2] Peng, Hongwu, et al. \"PASNet: Polynomial Architecture Search Framework for Two-party Computation-based Secure Neural Network Deployment.\" 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 2023."
                },
                "questions": {
                    "value": "See Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Reviewer_8y16"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608978911,
            "cdate": 1698608978911,
            "tmdate": 1699636754876,
            "mdate": 1699636754876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t2CBJdpfVg",
                "forum": "Mhu9iNGKqP",
                "replyto": "YPtGQqTBLH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer 8y16"
                    },
                    "comment": {
                        "value": "Thank you for recognizing that the polynomial approximation problem is a crucial aspect in reducing latency for private inference (PI).\n\nMany studies employ low-degree polynomials as activation functions in PI using multi-party computation (MPC) protocols, allowing data sharing among multiple parties. Both of the studies you presented are MPC-based works which achieved high performance using low-degree polynomials. In contrast, our work focuses solely on encrypted data inferences using fully homomorphic encryption (FHE). FHE-based PI enables a server to perform homomorphic computations by itself without requiring additional communication costs.\n\nWithin the realm of PI, MPC-based methodologies, similar to our study, tackle similar challenges and hold significance owing to unique advantages, like precise computation of non-arithmetic functions such as ReLU. Yet, directly contrasting these fields presents hurdles. Each prioritizes different optimization facets: FHE-based PI focuses on minimizing computation time for overall inference, whereas MPC-based PI grapples with substantial impediments in communication costs during computations. Despite recent efforts in MPC-based approaches employing low-degree polynomials for activation functions, their drawback lies in the substantial consumption of several gigabytes of communication costs [1]. Previous studies lack comprehensive quantitative assessments between FHE-based and MPC-based PI. Depending on the context, MPC-based methodologies might prove beneficial, while FHE-based approaches could offer distinctive advantages. Acknowledging the significance of both technologies is pivotal, given their representation as critical domains in continual progression.\n\n[1] P. Mishra et al, \"Delphi: A Cryptographic Inference System for Neural Networks,\" in Proc. Workshop Privacy-Preserving Mach. Learn. Pract., Nov. 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536803915,
                "cdate": 1700536803915,
                "tmdate": 1700536803915,
                "mdate": 1700536803915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kFAQla9s2l",
            "forum": "Mhu9iNGKqP",
            "replyto": "Mhu9iNGKqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_ghjk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_ghjk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach to enable private inference by using only Homomorphic Encryption (HE), without the need for retraining or model redesign.  The authors achieve this by replacing ReLUs with polynomial functions. They employ dynamic programming techniques and leverage layer-specific characteristics to adaptively select polynomial degrees for different layers in a pre-trained model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  Authors exploited the layer-specific characteristics to significantly reduce the degree of polynomials for HE-only PI. \n\n2. The proposed approach does not require any redesigning or fine-tuning of the model, which often helps recover the accuracy. \n\n\n3. Methods are very well presented in the paper."
                },
                "weaknesses": {
                    "value": "$\\bullet$ **Complexity and scalability of the proposed approach:** An essential concern regarding the presented solution lies in its complexity, an aspect left unexplored in the paper. Given that the method determines the appropriate polynomial degree for each layer, its computational complexity varies depending on the network's depth. This adaptability makes it increasingly impractical for deeper networks, such as ResNet101 and ResNet152. In contrast, the complexity of PI-specific manual ReLU pruning depends solely on the number of stages within the networks and remains independent of the total number of layers.\n\nAdditionally, the authors must have included a comparative analysis of their approach's complexity with previous methods that involve retraining, with a focus on absolute time, to demonstrate the advantages of their method, which eliminates the need for retraining.\n\n$\\bullet$ **Lack of comprehensive empirical evaluation:**  The experimental evaluation in the paper is limited to CIFAR-10 using ResNet20 and ResNet32 networks. As per the standard practice in PI [1, 2, 3], an evaluation on CIFAR-100 and TinyImageNet datasets with networks such as ResNet18 should have been included to validate the effectiveness of the proposed solution on complex datasets. \n\n\n$\\bullet$ **Comparison with prior work:** To demonstrate the superiority of the HE-only solution over a hybrid approach (HE + MPC) [1, 2, 3],  a comparative analysis of the end-to-end PI runtime is required. Also, a discussion on low-degree polynomial substitution should have been included, which has been shown to be effective even on complex datasets such as [4]. \n\n\n**In conclusion,** I question the practicality of using higher-degree polynomials, especially considering their need for extensive bootstrapping when compared to the hybrid approach (HE/VOLE for linear layers and GC/OT for ReLUs). Additionally, this paper doesn't offer any new insights to enhance the feasibility of PI. \n\n1. Kundu et al., \u201cLearning to Linearize Deep Neural Networks for Secure and Efficient Private Inference,\u201d ICLR 2023.\n\n2. Cho et al., \u201cSelective network linearization for efficient private inference,\u201d ICML 2022.\n\n 3. Jha et al., \u201cDeepReduce: ReLU reduction for fast private inference,\u201d ICML 2021.\n\n4.  Chrysos et al., \"Regularization of polynomial networks for image recognition,\" CVPR 2023"
                },
                "questions": {
                    "value": "Given that Table 1 indicates the need for a high-degree polynomial in the initial layer, is it possible to solely utilize the identity connection in these layers? Previous research on PI-specific ReLU optimization has shown that the initial ReLUs are not critical and can be eliminated without significant performance degradation. Is this applicable here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Reviewer_ghjk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793269615,
            "cdate": 1698793269615,
            "tmdate": 1699636754730,
            "mdate": 1699636754730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PjxANUHkO8",
                "forum": "Mhu9iNGKqP",
                "replyto": "kFAQla9s2l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer ghjk -1-"
                    },
                    "comment": {
                        "value": "Thank you for valuable comments and understanding that our method can be applied in situations where redesigning and fine-tuning are not allowed.\n\n**Complexity and scalability of the proposed approach**: We agree that the computational complexity of our method depends on the network\u2019s depth. In Section 3.2, page 7, we provide the complexity of the proposed dynamic programming algorithm with big O notation.\nHowever, once we obtain the set of the polynomial degrees from the proposed algorithm, we can perform the inference on all test datasets without additional computations for determining polynomials. Furthermore, the operations in the proposed algorithm involve operations between plaintexts, not ciphertexts. Therefore, we can efficiently obtain the optimal set of the polynomial degrees within a short timeframe. Consequently, obtaining the optimal polynomials for a given backbone network is expected to pose no significant burden when performing inference on the dataset.\n\n**Comparative analysis with works involving retraining**: We firstly want to note that the suggested works [1-3] are based on MPC protocol, and [4] requires retraining for achieving low degree polynomial activations.\n\nWhen considering the computational complexity of two methods, focusing solely on inference time is unfair. This is because one must also account for the time required to retrain the model. When retraining is involved, the methods to retrain can vary widely based on the size of the target dataset or the structure of the backbone model, making it challenging to precisely estimate the retraining time. As the target dataset or backbone model grows in complexity, the time needed for retraining is likely to increase. However, if effective retraining allows for achieving high performance using lower-degree polynomials, the resulting inference time may decrease.\n\nIn scenarios like our research, where retraining is not allowed and the given backbone model must be used as-is, the time complexity will encompass finding the optimal polynomial approximation and the inference time. For our proposed algorithm, as stated in Section 3.2, the time complexity is denoted by the number of layers $N_L$ that need to be approximated, remaining independent of the model's structure or dataset size. Thus, the time spent finding the optimal polynomial is considerably shorter compared to the time required for retraining. Of course, in contrast to cases where retraining is allowed, the degree of the approximating polynomial demanded is higher, resulting in relatively longer inference times.\n\nIn summary, the consideration of time complexity in PI involves two different scenarios:\n- Involving retraining: Retraining time + Inference time\n- Not allowing retraining: Polynomial optimizing time + Inference time\n\nBoth approaches are crucial in PI research and can be applicable depending on the circumstances. If the model owner providing PI services has sufficient resources to train the model and enough time for retraining, applying the polynomial approximation technique allowing retraining might be preferable. However, situations might arise where resources or time for retraining are insufficient, and there's no guarantee that retraining always yields optimal results. In such cases, performing rapid polynomial approximation tailored to the already trained model, as proposed in our research, could be more favorable. Therefore, directly comparing the time consumption between PI with and without retraining is challenging, and previous studies have not extensively explored this direct comparison.\nOur research aimed to reduce the bottleneck of the polynomial degree, which becomes the time complexity issue in scenarios where retraining is not allowed. By minimizing this aspect more than in previous studies, we believe our work contributes to reducing the overall time spent in PI.\n\nSuch distinctions in research aren't exclusive to PI. For instance, the domain of quantizing DNNs, where researchers also differentiate between areas that allow retraining and those that don't. Some studies [5] aim for high model performance by permitting retraining in the pursuit of efficiently executing inference through quantization. Conversely, other studies [6] focus on finding appropriate precision bits without retraining to minimize performance degradation. Similar to the earlier discussion, in both these domains, the objectives and optimization targets vary, making it challenging to definitively label one as superior to the other. Considering that research in PI can progress independently based on the allowance of retraining, it would be valuable to acknowledge these aspects.\n\n[1-4] The papers that mentioned in the review.\n\n[5] S. Anwar et al, \"Fixed Point Optimization of Deep Convolutional Neural Networks for Object Recognition,\" ICASSP, IEEE, 2015.\n\n[6] D. Lin et al, \"Fixed Point Quantization of Deep Convolutional Networks,\" ICML, 2016."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535018015,
                "cdate": 1700535018015,
                "tmdate": 1700535018015,
                "mdate": 1700535018015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "inZ7Vi9uox",
                "forum": "Mhu9iNGKqP",
                "replyto": "kFAQla9s2l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer ghjk -2-"
                    },
                    "comment": {
                        "value": "**Comparison between previous works with hybrid approach and our work**: Both approaches, the hybrid approach (allowing MPC) and the FHE-only approach, share the common goal of performing privacy-preserving inference. However, directly comparing these domains is impracticable due to differences in their focus areas: these approaches deal with *distinct domains* and optimize *different bottlenecks*.\n\n*Distinct domain.* We kindly ask for consideration regarding the fact that research in these two domains: hybrid and FHE-only, is conducted for distinct environmental aspects. If the client can participate in computations alongside the server during PI service provision, employing a hybrid approach is desirable. However, situations may arise, due to communication constraints (such as the client being intermittently offline) or for security reasons concerning the server's computational process, where client involvement during inference is not permitted. In such cases, the hybrid model becomes unfeasible, necessitating the exclusive use of an FHE-only model. Therefore, the enhancement of FHE-only models a crucial research focus on some scenarios. This is why our current line of work has been shaped: to improve upon FHE-only models by comparing and contrasting them with their counterparts.\n\n*Different bottleneck.* Each approach prioritizes different optimization aspects: FHE-only PI focuses on minimizing computation time for overall inference, while hybrid PI addresses significant challenges related to communication costs during computations. Despite recent MPC-based attempts to use low-degree polynomials for activation functions, their drawback lies in their consumption of several gigabytes of communication cost [7]. Due to these inherent and critical distinctions, quantitatively comparing these challenges is challenging. To our knowledge, there lacks proper papers that fairly compare FHE-based PI and MPC-based PI. \n\nMoreover, hardware accelerators have effectively addressed the inference time, previously a bottleneck in FHE-only approaches. Several studies [8-10] demonstrate the practicality of executing homomorphic operations and inference on DNNs using the RNS-CKKS scheme with FHE\u2019ed data. Our research compares the effects of reducing the degree of the approximating polynomial in situations where the backbone network mandates the direct utilization of given pre-trained parameters without retraining, contrasting this with conventional minimax approximations [11,12]. Directly comparing the overall end-to-end PI runtime with other hybrid works seems difficult. However, with the ongoing advancements in hardware accelerators and related technologies, integrating them with our layerwise optimization gives ample reason to anticipate a further reduction in the overall runtime.\n\nSumming up, the PI researches on hybrid approach and FHE only approach exhibit distinct differences, each serving specific environments and application domains. Consequently, independent research is imperative in both areas to accommodate their unique applicability.\n\n[7] P. Mishra et al, \"Delphi: A Cryptographic Inference System for Neural Networks,\" in Proc. Workshop Privacy-Preserving Mach. Learn. Pract., Nov. 2020.\n\n[8] S. Kim et al, \"BTS: An Accelerator for Bootstrappable Fully Homomorphic Encryption,\" Proceedings of the 49th Annual International Symposium on Computer Architecture. 2022.\n\n[9] SU. Yang, et al, \"FPGA-based Hardware Accelerator for Leveled Ring-LWE Fully Homomorphic Encryption,\" IEEE Access, 2020.\n\n[10] H. Lee et al, \"A 2.7~13.3 uJ/boot/slot Flexible RNS-CKKS Processor in 28nm CMOS Technology for FHE-based Privacy-Preserving \nComputing,\" IEEE ISSCC, San Francisco, CA, USA, Feb. 2024, accepted for presentation. \n\n[11] E. Lee et al, \"Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed \nConvolutions,\" ICML, 2022.\n\n[12] D. Kim et al, \"Optimized Privacy-Preserving CNN Inference With Fully Homomorphic Encryption.\" IEEE Transactions on Information Forensics and Security 18 (2023): 2175-2187, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565969594,
                "cdate": 1700565969594,
                "tmdate": 1700565969594,
                "mdate": 1700565969594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cdwck1IG8m",
                "forum": "Mhu9iNGKqP",
                "replyto": "kFAQla9s2l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer ghjk -3-"
                    },
                    "comment": {
                        "value": "**Simulation for complex dataset**: Experiments could be conducted on the CIFAR-100 dataset to further demonstrate the effectiveness of our algorithm. We check the performance of our optimizing method by classifying plaintext CIFAR-100 on ResNet-32.\n\n| Method | Max. Degree | Max. Depth | Original Acc. | Approx. Acc. |\n|:---:|:---:|:---:|:---:|:---:|\n| Previous [11] | 6,075 | 14 | 69.50% | 69.43% |\n| Proposed | 511 | 9 | 69.49% | 68.94%* |\n\n(*The accuracy of the model using the proposed approximated polynomials is obtained in plaintext.)\n\nOur technique significantly reduced depth consumption while maintaining the performance of the pre-trained model. Since the maximum depth consumption is reduced from 14 to 9, we can anticipate that the computation time for bootstrapping and ReLU evaluation will also be significantly reduced in ciphertext CIFAR-100 classification using the proposed method. Given the limited discussion timeframe, demonstrating other outcomes such as ciphertext experiments might be challenging at present. However, efforts will be made to sufficiently supplement the experimental content before the final revision deadline.\n\nFHE-based PI still presents several unresolved issues necessitating further exploration, prompting us to propose future work for classifying larger datasets. Pre-trained models capable of classifying large datasets often contain more complex functions requiring optimization. For instance, non-arithmetic functions like max-pooling involve multivariate functions, and there hasn't been a proposed approach to effectively approximate these in encrypted data for inference in DNNs. Additionally, due to the current time latency issues, instances of classifying large datasets like TinyImageNet without retraining are limited to performing on the last few layers with FHE'ed data [12].\n\n**Reply for the last question**: We note that the optimized polynomial degrees would depend on the datasets, the trained models, and the retraining process. While not directly within the scope of PI research, analogous studies in the field of quantization\u2013specifically, those focusing on quantizing weights and activations without retraining the model\u2013have indicated the need for higher bit-precision in the initial layers during quantization [6]. This observation is consistent with our research, where we use higher-degree polynomials to approximate the initial ReLU functions. Additionally, if our method were to be employed in scenarios permitting retraining as in prior MPC-based work, the initial ReLU functions might be eliminated.\n\n[11] E. Lee et al, \"Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Convolutions,\" ICML, 2022.\n\n[12] D. Kim et al, \"Optimized Privacy-Preserving CNN Inference With Fully Homomorphic Encryption.\" IEEE Transactions on Information Forensics and Security 18 (2023): 2175-2187, 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566030980,
                "cdate": 1700566030980,
                "tmdate": 1700566030980,
                "mdate": 1700566030980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QFbmPHztwu",
                "forum": "Mhu9iNGKqP",
                "replyto": "inZ7Vi9uox",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Reviewer_ghjk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Reviewer_ghjk"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for a detailed rebuttal. \n\n\nHere are a few notes\n\n$\\bullet$ Auhtors' argument for the re-training time in the previous work, which used the HE and MPC, is not justified. Firstly, the re-training process in the these previous work, similar to the method proposed in this study, occurs in plaintext rather than ciphertext. Additionally, not all previous studies necessitated re-training. While fine-grained ReLU optimization [1,2] require re-training, while [3] does not, instead utilizing KD (applying KD is different than re-training, you need to just modify the loss function). Finally, in the context of private inference, the time investment made by a service provider in training a model is less critical. Once the model is deployed, the focus shifts to the efficiency and cost-effectiveness of the private inference. \n\n$\\bullet$ On the last question\n\nPrior work [1-3] consistently demonstrated that, ReLUs in the initial layers (precisely,  Stage 1) can be dropped without impacting accuracy. In fact, [2] has the theoretical proof for the same. This observation is consistent across different network and datasets. Leverging this insights, authors could have reduced the requirement of higher degree polynomials in the initial layer of the ResNets. \n\nI still have do not see the practicality of using such a higher degree of polynomials, especially, when [5] demonstrated that HE-only solution can be achieved with **only degree 4 polynomials**, also scalable to ImageNet datasets. \n\n[5]  Baruch et al., Training Large Scale Polynomial CNNs for E2E Inference over Homomorphic Encryption. \n\n\n I will keep my score, given the current version of the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685149185,
                "cdate": 1700685149185,
                "tmdate": 1700685149185,
                "mdate": 1700685149185,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FVCoECa5DB",
            "forum": "Mhu9iNGKqP",
            "replyto": "Mhu9iNGKqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_J1kw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6616/Reviewer_J1kw"
            ],
            "content": {
                "summary": {
                    "value": "The paper makes two main contributions to reduce the inference latency of deep convolutional neural networks (CNNs such as ResNet-20/32) when the input data is encrypted using a fully homomorphic encryption (FHE) scheme (RNS-CKKS).\n\n1) Assuming that the input distribution to the ReLU activation function is a normal distribution, it has been claimed that optimizing the mean squared error (MSE) of the polynomial approximation is better than the conventional minimax approach (which assumes uniform distribution). This has been achieved by tying it to minimizing the variance of the loss function.\n\n2) A dynamic-programming based method has been proposed to determine \"optimal\" polynomial degree in each layer of a neural network. Based on this optimization approach, it have been shown that the inference latency can be reduced 3-4x compared to the baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper address an important problem in the field of private inference and the proposed solution appears to be based on solid principles, provided that the stated assumptions are true."
                },
                "weaknesses": {
                    "value": "1) Two key claims in the paper have been stated without any strong validation.\n\na) Firstly, what is the guarantee that the input to the ReLU activation layer will always follow a normal distribution? Is this true for every layer of the neural network? What happens if this assumption does not hold?\n\nb) More importantly, why is the \"variance of the loss\" a good surrogate for the classification accuracy? The challenge in encrypted domain inference is not sensitivity to small approximation errors. Contrarily, even a single large (unbounded) approximation error can screw up the entire inference process (as observed in the bit-flip attack on machine learning models). This is reason we need some bound on the approximation error (leading to the minimax formulation).\n\n2) It is well-established in the literature on MPC-based private inference that not all ReLUs are equally important in the inference process. For example, see Peng et al., \"AutoReP: Automatic ReLU Replacement for Fast Private Network Inference\", ICCV 2023 and the references therin. In fact, the literature on MPC-based private inference does not stop layer-wise and tries to find exactly which particular neuron requires more accurate approximation. \n\na) It is important to acknowledge the progress made on ReLU reduction in the field of MPC-based private inference because it is directly relevant to the problem considered in this work.\n \nb) Can the proposed dynamic programming approach be scaled one step further to find the optimal polynomial degree for each specific neuron?\n\n3) The results indicate that there is orders of magnitude decrease in the max. polynomial degree (see Table 3), which is surprising not reflected in the max. depth as well as the overall inference time. There should be a more in-depth analysis of how the max. polynomial degree impacts the inference time because that forms the core motivation for this work."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6616/Reviewer_J1kw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699161127557,
            "cdate": 1699161127557,
            "tmdate": 1699636754616,
            "mdate": 1699636754616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "loGD3x2V1x",
                "forum": "Mhu9iNGKqP",
                "replyto": "FVCoECa5DB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer J1kw -1-"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and feedback that we need to take into account.\n\n**Assumption of normality**: In our simulations, we conducted empirical verification to confirm that the input values of each activation function follow a normal distribution by drawing a histogram of input data. We will include our empirical results supporting the normality of input distribution. It is also noteworthy that similar normality assumptions have been employed in prior studies [1, 2].    \nFurthermore, our proposed method can be applied to arbitrary input distributions. Theorem 2.1 shows that we can obtain polynomials minimizing the mean squared error for arbitrary input distributions. We are grateful for your important comments. We will ensure our revised paper includes the initial assumption of normality and the general applicability of our work. \n\n[1] I, Takumi et al, \"Highly accurate CNN Inference Using Approximate Activation Functions Over Homomorphic Encryption,\" 2020 IEEE International Conference on Big Data (Big Data). IEEE, 2020.\n\n[2] P, Hongwu et al. \"AutoReP: Automatic ReLU Replacement for Fast Private Network Inference,\" ICCV, 2023.\n\n**Discussion of the proposed variance of the loss**: We provide an intuition to explain why the variance of the loss increment $Var(\\Delta L)$ is a proper surrogate in our optimization. Because of $\\Delta \\mathcal{L}= \\frac{\\partial \\mathcal{L}}{\\partial a_{i,j}} \\Delta a_{i,j}$, where $a_{i,j}$ is a random variable representing a polynomial approximation error (see page 5), we can set $E[\\Delta \\mathcal{L}]=\\frac{\\partial \\mathcal{L}}{\\partial a_{i,j}} E[\\Delta a_{i,j}]$. Our empirical findings indicate that $E[\\Delta a_{i,j}]\\approx 0$, which leads to $E[\\Delta \\mathcal{L}]\\approx 0$. Hence, we can claim that $Var(\\Delta L) \\approx E[(\\Delta L)^2]$. \nThe practical implication of this approximation is important. By focusing on minimizing $Var(\\Delta L)$, we can effectively reduce the noise power on the loss function (i.e., maximizing the signal to noise ratio (SNR) of loss). \n\nAs you mentioned, we fully agree that a single large approximation error can severely compromise the total inference accuracy. This holds true even in the context of minimax approximation. Recognizing the impact of a single value on the overall inference, the configuration of the approximation range becomes crucial. Consequently, both prior research employing minimax approximation and our own study have considered the task of defining the approximation range [3,4].\n\nIn the study employing minimax approximation, the approximation range should be set such that all input values of each activation function fall within the approximation range. In our approach, employing the weighted least squares method, we aimed to reduce the size of regions where polynomial values diverge significantly. To achieve this, we scaled the obtained standard deviation $\\sigma$ of the input distribution as an approximation weight, as detailed in the last part of Section 2.2, page 4.\n\nOnce an appropriate approximation range is established, the inference error can be considered as determined by the small errors in each layer. These small errors in each layer are expected to have varying impacts on the overall inference result. Through the discussion in Section 3.1, we argue that the diverse contributions of each layer directly correlate with the variance of the loss increment $Var(\\Delta L)$. This discussion gains significance from the assumption that input values exist within appropriate approximation ranges, ensuring the absence of single large approximation errors in each layer.\n\nOur simulation results further support the validity of our approximation range settings. The minimal difference in classification accuracy before and after approximating the ReLU function with a polynomial supports our approach (see the results of the proposed method in Table 2).\n\nAnother reason that the variance of the loss increment can be considered as a stable measure that indicates the classification accuracy is that the value $\\Delta L$ is always positive. We can assume that the activation nodes are on the global minimum of the loss function. Then, the small errors of the activation function always result in positive change on the loss function. Therefore, the reduction of the variance $\\Delta L$ leads the activation function to the global minimum, which gives better results in classification accuracy.\n\n[3] J.-W. Lee et al, \"Privacy-Preserving Machine Learning with Fully Homomorphic Encryption for Deep Neural Network,\" IEEE Access, 10:30039\u201330054, 2022b.\n\n[4] E. Lee et al, \"Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Convolutions,\" ICML, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533837268,
                "cdate": 1700533837268,
                "tmdate": 1700533837268,
                "mdate": 1700533837268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "31XEg1x6pI",
                "forum": "Mhu9iNGKqP",
                "replyto": "FVCoECa5DB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6616/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer J1kw -2-"
                    },
                    "comment": {
                        "value": "**Comments about AutoReP**: Thank you for bringing the recent AutoReP study to our attention. We will acknowledge AutoReP and other pertinent research on MPC-based private inference in our paper. It's noteworthy that AutoReP introduces a method for selecting specific ReLU functions to replace with identical degree polynomials, which is technically distinct from our approach. In our work, due to the limitations of fully homomorphic encryption (FHE) in evaluating ReLU operations, we substitute all ReLU functions in nodes with fine-grained polynomials, which are optimized by dynamic programming approach. \nMoreover, AutoReP employs retraining to identify ReLU activation nodes for polynomial replacement. In contrast, our method does not require retraining, making it more suited for scenarios where retraining is not feasible.\nAdditionally, we would like to note that both AutoReP and our work are considered contemporaneous according to the ICLR reviewer guidelines (https://iclr.cc/Conferences/2024/ReviewerGuide) because AutoReP was posted on arXiv in August 2023, a month prior to our submission in September 2023.\n\n**Optimizing for each specific neuron**: In AutoReP, a more optimized approximation approach is adopted by considering the characteristics of each neuron individually, rather than uniformly applying the same polynomial across the entire layer. While the RNS-CKKS scheme also allows applying different polynomials to each neuron within a single ciphertext, the RNS-CKKS scheme used in our FHE-based technology follows a single instruction multiple data (SIMD) approach, where multiple data are encoded into a single ciphertext for homomorphic operations. Consequently, even if different polynomials are applied to the values within a single ciphertext, the depth consumption of the ciphertext depends on the maximum degree among those polynomials. In FHE-based operations, the most crucial factor influencing computation time is the depth consumption of ciphertexts. Therefore, applying different polynomials to each neuron may not result in significant differences in overall computation time.\n\nHowever, our proposed dynamic programming technique can work independently of the SIMD structure, making it applicable to MPC-based schemes as well. By assessing the input distribution for each neuron and formulating an optimization problem with optimization variables for each degree (i.e., $d_{i,j}$ for $j$th node in the $i$th layer), we can optimize the polynomials tailored to each individual neuron. \n\n**The impact of degrees and depths on the inference time**: In FHE-based schemes, the total inference time is closely associated with the depth consumption of a polynomial rather than the degree. The depth consumption is almost proportional to the logarithm of the polynomial degree (depth consumption = $\\lceil \\log_2 (d+1) \\rceil$, where $d$ is the degree of the polynomial).\n\nThe analysis of the depth consumption and its relation to the computation time for the polynomials proposed in each layer is presented in Section 4. As illustrated in Figure 1, page 7, as the depth $\\delta$ consumed by each polynomial in a layer decreases, both bootstrapping and polynomial evaluation times decrease (blue graph). Furthermore, experimental results using the moduli chain management method proposed in Section 4 confirm that as the depth of the approximated polynomials decreases, the computation time decreases even more (orange graph).\nBy reducing depth consumption, we can effectively reduce the overall inference time. We provide detailed analysis on this aspect in Section 4."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6616/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534095846,
                "cdate": 1700534095846,
                "tmdate": 1700534095846,
                "mdate": 1700534095846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]