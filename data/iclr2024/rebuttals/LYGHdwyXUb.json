[
    {
        "title": "Efficient Multi-task Reinforcement Learning via Selective Behavior Sharing"
    },
    {
        "review": {
            "id": "DN6C9PcyW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_H9dR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_H9dR"
            ],
            "forum": "LYGHdwyXUb",
            "replyto": "LYGHdwyXUb",
            "content": {
                "summary": {
                    "value": "This paper advocates for the selective adoption of shareable behaviors across tasks while concurrently mitigating the impact of unshareable behaviors, a proposition that is well-motivated and promising. However, certain sections, notably the abstract and introduction, require further elucidation. More comprehensive conceptual and empirical comparisons with existing literature in this domain are required. Considering the extensive body of relevant work in this field, the evidence presented in the paper falls short of substantiating the acceptance of this work, leading me to recommend a weak rejection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(a) The intuition behind the algorithm design is novel and interesting: using Q-value to identify potentially shareable behaviors and encourage exploration.\n\n(b) The algorithm part (Section 4) is well-presented and easy to follow.\n\n(c) The empirical analysis is detailed and informative about the properties of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "(a) Section 1 requires further elucidation concerning comparisons with prior works and descriptions of the proposed algorithm.\n\n(b) It would be advantageous to include studies on \"skills\" within related works, given their conceptual similarity to the \"shared behaviors\" discussed in this paper.\n\n(c) The algorithm's design, which learns a distinct policy for each task, could potentially diminish sample efficiency.\n\n(d) The shareable behaviors can be adopted in a more efficient manner (e.g., forming a hierarchical policy), rather than only used for gathering training data.\n\n(e) The selected baselines for comparisons are kind of weak and can be further strengthened.\n\n(f) The comparisons with baselines depicted in Figure 4 fail to demonstrate notable improvements conferred by the proposed algorithm."
                },
                "questions": {
                    "value": "(a) The definition of \" conflicting behaviors\" should be elaborated in Section 1.\n\n(b) In Section 2, the authors mention \"However, unlike our work, they share behavior uniformly between policies and assume that optimal behaviors are shared across tasks in most states.\" More explanations are required for \"share behavior uniformly\" and \"assume that optimal behaviors are shared across tasks\", where the latter one seems not to be true.\n\n(c) \" Yu et al. (2021) uses Q-functions to filter which data should be shared between tasks in a multi-task setting.\" It would be good to provide more detailed comparisons with this related work.\n\n(d) It should be \"argmax\" for the equation in Section 3.\n\n(e) Theoretically, the policy network is trained to give actions with the maximized Q-value. That is, $i = \\arg\\max_jQ_{i}(s, a_j)$ (Line 9 of Algorithm 1) should hold in most cases, which may make this key algorithm design trivial.\n\n(f) The baseline \"Fully-Shared-Behaviors\" cannot be viewed as a fair comparison, since the agent cannot identify which task it is dealing with. The task identifiers should also be part of the input. There are many research works in this area, such as [1-3] and the ones listed by the authors in Section 2. It would be beneficial to provide comparisons with these works as well.\n\n[1] Sodhani, Shagun, Amy Zhang, and Joelle Pineau. \"Multi-task reinforcement learning with context-based representations.\" In International Conference on Machine Learning, pp. 9767-9779. PMLR, 2021.\n\n[2] Yang, Ruihan, Huazhe Xu, Yi Wu, and Xiaolong Wang. \"Multi-task reinforcement learning with soft modularization.\" Advances in Neural Information Processing Systems 33 (2020): 4767-4777.\n\n[3] Hessel, Matteo, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado Van Hasselt. \"Multi-task deep reinforcement learning with popart.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, pp. 3796-3803. 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697145845273,
            "cdate": 1697145845273,
            "tmdate": 1699637081976,
            "mdate": 1699637081976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jv09yrd8mD",
                "forum": "LYGHdwyXUb",
                "replyto": "DN6C9PcyW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback especially regarding clarifications in the writing and additional related works to add.  We have incorporated these into the paper.  Additionally, we would like to emphasize that the goal of this paper is to advocate for selective behavior sharing as a promising avenue in multi-task RL which can be complementary to (and not in competition with) many of the existing parameter sharing and data sharing methods that are more well known.  We address your remaining concerns and questions below.\n\n### Weakness C,D: Distinct policy for each task: \nWe clarify that our algorithm\u2019s design is not limited to distinct policies. In fact, the set of experiments in Section 6.2 show that QMP can be implemented with a single multi-head multi-task policy and that the behavior sharing advantages are complementary with parameter sharing advantages.\n\nFurthermore, the choice of distinct v/s shared task policies depends on the task family. Evidently, parameter-sharing is not always optimal and could even hurt training over distinct policies (See Multistage Reacher results in Figure 5 and [3]). Since we consider task sets where different tasks may require conflicting behavior, we base our primary comparisons on distinct policy networks, but also provide results on parameter-sharing + QMP.\n\n### Weakness E: Weak Baselines\nWe believe the reviewer is referring to the comparisons we make in Section 6.1, which are comparisons with other behavior sharing methods and not multi-task RL methods in general.  This supports our argument that QMP is an effective way to share **behaviors** in multi-task RL problems. Importantly, we are not claiming that QMP, or even behavior sharing, is the best MTRL method on its own, and therefore do not compare with complementary non-behavior sharing methods. In fact, we show QMP can be combined with methods like parameter sharing (Section 6.2) to gain the advantages of both methods.\n\n### Weakness F: Lack of notable improvements\nWe would like to note that reviewers QRge, Zv3o, and neqy all noted that the empirical results were comprehensive and demonstrated the effectiveness of QMP.  In our experiments, we chose a wide range of environments and tasks with shared and conflicting behaviors to demonstrate that our simple method of QMP improves sample efficiency and convergence across general multi-task sets, and did not design tasks specifically to favor our method which may have shown more dramatic differences. This is in line with the theoretical proofs in `[Appendix G]` showing that QMP guarantees a weak monotonic improvement over the baseline of no behavior sharing.\n\n### Question C: Comparisons to data-sharing methods\n- `[Fig 9 (c)]` Yu et al. (2021) [1] requires a ground truth reward function to re-label shared data between methods so it is not applicable to our problem setting. In Fig 9 (c), we show that QMP even outperforms CDS which makes an extra assumption of ground truth reward-relabeling.\n- `[Fig 4]` We do compare against CDS\u2019s follow-up work UDS [2], in Section 6.1, which does not require the true reward labels. QMP consistently outperforms UDS.\n\n### Question E: Should QMP always choose its own task policy?\n`[Theorem G.2]` While the task policy $\\pi_i$ is trained to maximize $Q_i$ over continuous action spaces, it always lags behind $Q_i$, which is itself constantly being updated with new rollouts over training. So the Q-filter can select a different policy if $pi_i$ is not fully optimized for the current state, and a different task policy suggests a better action. When this happens, Theorem G.2 guarantees performance improvement.\n\n### Question F: \u201cFully-Shared-Behaviors\u201d baseline\nWe use this baseline in section 6.1 specifically to answer \u201cHow does QMP compare to other forms of behavior sharing?\u201d, by comparing against a policy that has the same behavior everywhere for all tasks, and therefore mask out the task identifiers to enable policies to learn shared behaviors across tasks.  This is intended as a behavior sharing comparison, not an analysis on parameter sharing in single policies which we do compare against in Section 6.2.\n\n### [References]\n[1] Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C. Conservative data sharing for multi-task offline reinforcement learning. NeurIPS 2021.\\\n[2] Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Finn, C., and Levine, S. How to leverage unlabeled data in offline reinforcement learning. ICML 2022.\\\n[3] Yu, Tianhe, et al. Gradient surgery for multi-task learning. NeurIPS 2020.\\"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729249529,
                "cdate": 1700729249529,
                "tmdate": 1700729249529,
                "mdate": 1700729249529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yKck3XYFly",
            "forum": "LYGHdwyXUb",
            "replyto": "LYGHdwyXUb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_neqy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_neqy"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Q-Switch Mixture of Policies (QMP) to facilitate the selective sharing of behaviors across tasks, enhancing exploration and information gathering in Multi-task Reinforcement Learning (MTRL). Assuming that different tasks demand distinct optimal behaviors from the same state, QMP employs the Q network of the current task to determine the exploration policy from a pool of all tasks and generate rollout data for efficient training. The method showcases performance improvements across various benchmark scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and is easy to follow.\n2. The concept of sharing \"behavior\" instead of data or parameters is intriguing.\n3. Empirical results demonstrate that QMP effectively handles multiple tasks with conflicts, and the experiments are detailed and comprehensive."
                },
                "weaknesses": {
                    "value": "1. The central issue with this paper lies in its use of $Q_i$ to evaluate shareable behaviors. In reality, $Q_i(s, a)$ estimates the \"expected discounted return of policy $\\pi_i$ after executing action $a$ in state $s$,\" emphasizing that the trajectory to the left is generated by $\\pi_i$. However, the authors employ it to evaluate \"behavior,\" which could be interpreted as an action sequence following state $s$.\" Although the authors acknowledge that \"the Q-function could be biased when queried with out-of-distribution actions from other policies,\" even if we assume that the Q-function fits well, it still struggles to accurately evaluate another policy using only $Q_i(s, \\pi_j(s))$.\n2. The method, while simple, appears more heuristic in nature and lacks guarantees.\n3. The discussion of related works is insufficient and comes across as disjointed and poorly structured."
                },
                "questions": {
                    "value": "Regarding the Weaknesses mentioned, do you think there are ways to address these concerns or clarify the usage of $Q_i$ for evaluating behaviors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697950758219,
            "cdate": 1697950758219,
            "tmdate": 1699637081782,
            "mdate": 1699637081782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gGrMmeEAbM",
                "forum": "LYGHdwyXUb",
                "replyto": "yKck3XYFly",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback on where we can provide additional intuition for our method and on the structure of the related works.  We address your concerns and questions below.\n\n### Theoretical guarantees\n`[Appendix G]` Please refer to the combined response, where we add convergence and policy improvement proofs for the QMP Algorithm\u2019s modification due to its mixture policy.\n\n### Using Q-function to evaluate shareable behaviors\n`[Appendix H]` Please refer to the combined response, where we justify (based on prior work, mathematical intuition, and empirical results) why temporally-extended behaviors can effectively obtain high-rewarding trajectories, even when the Q-switch is used to evaluate the mixture policy on the current action.\n\nWe note that, for a single action proposal from another task policy, the Q-function can indeed evaluate $Q_i(s, \\pi_j(s))$ because $\\pi_j(s)$ is just another action to evaluate for $Q_i$. We prove the utility of other tasks in Task $i$\u2019s policy improvement step in Theorem G.2 for H=1, i.e., behaviors of length 1. Further justification for rolling out $\\pi_j$ for a sequence of future states is provided in Appendix H.\n\n### Related Work Discussion\n`[Section 2]` We added several relevant works on MTRL through shared representations and skill learning as a single task behavior sharing example.\n\nWe hope the added theoretical results and justifications addresses all your concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729177309,
                "cdate": 1700729177309,
                "tmdate": 1700729177309,
                "mdate": 1700729177309,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vwcZ2zLYRV",
            "forum": "LYGHdwyXUb",
            "replyto": "LYGHdwyXUb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers sharing learned behaviors across tasks in multi-task reinforcement learning (MTRL). To preserve optimality, authors propose a method called Q-switch Mixture of Policies (QMP). When training the multi-task policies, QMP estimates the shareability between task policies and incorporates them as temporally extended behaviors to collect training data. Experiments on a wide range of manipulation, locomotion and navigation MTRL task families demonstrate the effectiveness of QMP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-written and easy to follow\n2.\tSharing learned behaviors among tasks is an interesting and important topic in RL.\n3.\tAuthors conducted extensive experiments and analysis to empirically illustrate the effectiveness of QMP."
                },
                "weaknesses": {
                    "value": "1. Lack of theoretical analysis on the convergence (rate) of QMP.\n2. Lack of intuition and detailed analysis on why the proposed method works (i.e., why could QMP make sharing desired behaviors among tasks possible? Please see Question 2 for my concern).\n\nPlease refer to my questions below for my concerns."
                },
                "questions": {
                    "value": "1. Why do you choose to roll out H steps instead of just one step after choosing one policy to collect data? QMP just uses the Q function under a particular state to choose the behavioral policy, which cannot guarantee that the selected behavior policy is helpful for the current task after stepping out of the considered state.\n2. Will QMP cause undesired behaviors sharing? As I mentioned in Question 1, the selected behavioral policy will roll out for H(>1) steps, which may incurs sub-optimal behaviors.\n3. Why choosing the behavioral policy based on the learned Q-function to collect data will essentially share desired behaviors among tasks? I understand that using Q-function as a proxy may help select more better actions. However, the Q-function may be biased and not learned well during training, which could even hurt the learning process.\n4. Will QMP even slow down the training process? Say, the learned policy for the current task proposes an action, which is optimal but has an under-estimated Q function. Due to the biased Q function, QMP selects another policy to collect data, which chooses a sub-optimal action. Although the TD update will fix the estimated Q value of the sub-optimal action, it may be more efficient if we directly update the Q value of the optimal action, the thing that we really care about.\n\nI am willing to raise my scores if you could solve my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8641/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8641/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8641/Reviewer_Zv3o"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697966360630,
            "cdate": 1697966360630,
            "tmdate": 1699637081650,
            "mdate": 1699637081650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q1hFA8wrG4",
                "forum": "LYGHdwyXUb",
                "replyto": "vwcZ2zLYRV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your thorough review and your positive comments on the direction of our paper and the extensive empirical results and analysis.  We address your questions and concerns below.\n\n## Theoretical Analysis on the convergence (rate) of QMP\n`[Appendix G]` Please refer to the combined response, where we add convergence and policy improvement proofs for the QMP Algorithm\u2019s modification due to its mixture policy. We prove that QMP\u2019s mixture policy induces a Bellman operator that is a contraction, and QMP further guarantees a weak monotonic policy improvement over its base algorithm, SAC.\n\n## Intuition and analysis on why QMP works for behaviors\n\n### Q1: Using Q-function to evaluate H-step behaviors\n`[Appendix H]` Please refer to the combined response, where we justify (based on prior work, mathematical intuition, and empirical results) why temporally-extended behaviors can effectively obtain high-rewarding trajectories, even when the Q-switch is used to evaluate the mixture policy on the current action.\n\n### Q2: Undesired Behavior Sharing?\nWe address this in detail in the combined response, but in summary, it is possible for QMP to cause undesired behavior sharing through the Q-filter but is able to recover quickly and self-correct due to our method design which only uses other task policies to *gather training data*. We see this in effect in Task 4 of Multistage Reacher which completely conflicts with the other tasks but QMP still performs as well as other baselines that do not share behaviors.\n\n### Q3: Concerns about bias in learned Q-function\nThe reviewer correctly points out that the learned Q-function may be biased, leading to QMP selecting sub-optimal policies. However, this is true for any type of Q-learning where a policy tries to pick actions to maximize a learned Q-function (methods based on SAC [1] and Deterministic Policy Gradient [2]). This generally does not hinder performance in any RL algorithm due to self-correction from online data collection. In fact, many performant Q-learning-based RL algorithms, learn policies directly to maximize a learned Q-function [2, 6, 7].  As explained in Section 4.2 in the paper, the learned Q-function is a critical component of many online RL algorithms [1,2] and has even shown to be a useful filter for high-quality training data [3,4,5]. Our results in conflicting tasks (see QMP vs. No-shared in Figure 8, Task 4) demonstrate that QMP does not suffer from Q-function bias any more than the RL algorithm it is built on top of.\n\n### Q4: Could QMP slow down the training progress?\nAs addressed in Q3, QMP is hindered by the same Q-function bias as most Q-learning algorithms but does not impact sample efficiency in any of the task sets we tried even when there are directly conflicting behaviors (see example in Q2).  \n\nWe hope the combination of a simple method with strong empirical results and analysis, and the added theoretical results and justifications addresses all your concerns.\n\n## [References]\n[1] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. ICML 2018. \\\n[2] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv 2015. \\\n[3] Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C. Conservative data sharing for multi-task offline reinforcement learning. NeurIPS 2021. \\\n[4] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Overcoming exploration in reinforcement learning with demonstrations. ICRA 2018. \\\n[5] Sasaki, F. and Yamashina, R. Behavioral cloning from noisy demonstrations. ICLR 2020.\\\n[6] Silver, David, et al. \"Deterministic policy gradient algorithms.\" ICML 2014.\\\n[7] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" ICML 2018."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729069583,
                "cdate": 1700729069583,
                "tmdate": 1700729069583,
                "mdate": 1700729069583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WcTVR9uL2k",
            "forum": "LYGHdwyXUb",
            "replyto": "LYGHdwyXUb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_QRge"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8641/Reviewer_QRge"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes QMP, a behavior-sharing method in multitask reinforcement learning. QMP uses a mixture of policies to determine which policy is better to collect data for each task. Experiments are conducted on various robotics domains, showing the superior performance of QMP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of this paper is clear and easy to follow.\n\nExperiments are extensive and results are promising, significantly outperforming baselines."
                },
                "weaknesses": {
                    "value": "Some technical details need to be clarified:\n\n1) Since the Q-switch evaluates each $Q_1(s,a_j)$, it inherently assumes all tasks share the same state-action space, or at least assumes the same size of state-action dimension. \n\n2) The reviewer noticed that the paper mentioned 'out-of-distribution action proposal ' in Section 4.3, how does the agent know the action is out-of-distribution? Do you mean the action may be [-10,10] while the action space is [-1,1]?  \n\n3) The reviewer is not convinced by the criterion used for collecting data. If the Q-value of some task $j$'s action $a_j$ at state $s$ is the highest, then the QMP will rollout $\\pi_j$ for H steps. What if the next state's $Q(s', a_{j'})$ is the worst among all tasks? How does this work, though the results are very promising? When sampling from the dataset, do you filter the samples with lower rewards?\n\n4) How to sample the data from $\\pi_i$ and QMP? Is there a specific fraction or equal sampling?\n\n5) \n\nSome questions about experiments:\n\n1) The reviewer feels the comparison is unfair regarding the shared-parameters baseline. The key problem in MTRL is to address the interference when multiple tasks use one network to train the policy. Therefore, a lot of papers come up with different ideas, such as conflict gradient resolution (PcGrad, CAGrad)  and pathway finding (soft modularization, T3S). However, this paper only compared to the basic Multi-head-SAC. While QMP has a designed criterion to select what behaviors to share and how to share. That's why the results in Figure 5 show that 'Parameters + Behaviors' performs worse than 'Behaviors Only'.\n\nThe literature review lacks related works such as [1-4].\n\n[1] Conflict-averse gradient descent for multi-task learning\n\n[2] Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning\n\n[3] T3S: Improving Multi-Task Reinforcement Learning with Task-Specific Feature Selector and Scheduler\n\n[4] Provable benefit of multitask representation learning in reinforcement learning"
                },
                "questions": {
                    "value": "Please refer to the pros and cons part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699076499868,
            "cdate": 1699076499868,
            "tmdate": 1699637081547,
            "mdate": 1699637081547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j32Yo2Tpp1",
                "forum": "LYGHdwyXUb",
                "replyto": "WcTVR9uL2k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your helpful feedback and pointing us to relevant works to include, that we have incorporated a discussion in the revision.  We address your concern about the H-step rollouts in the combined response and the remaining questions below.\n\n### 1. Shared state-action dimension\nThis is a common assumption in many multi-task learning works [1-7]. Experiments on MT10 show that the state-action space does not need to be necessarily shared and only the dimensions are shared. Even specialized architectures like Perceiver that can take multimodal and diverse inputs are complementary to our work, if the Q-function is modeled in that way. So, the requirement of same state-action dimensionality is a consequence of the choice of Q-network, not a limitation of behavior-sharing.\n\n### 2. Out-of-distribution action proposal\nOut-of-distribution means that the Q-switch, i.e., $Q_i$ may not yet be well-trained on (s, $a_j$), where the $a_j$ is another task\u2019s proposal. In this case, the Q(s, $a_j$) value might be inaccurate. Even if the Q-switch erroneously selects this action $a_j$ to collect data, it would self-correct because by collecting data with $a_j$, the Q-switch would be more accurate on (s, $a_j$) now.\n\n### 3a. Theoretical grounds for criterion used to collect data\n`[Appendix G]` Please refer to the combined response, where we add convergence and policy improvement proofs for the QMP Algorithm\u2019s modification due to its mixture policy.\n\n### 3b. Using Q-function to do temporally extended data collection\n`[Appendix H]` Please refer to the combined response, where we justify (based on prior work, mathematical intuition, and empirical results) why temporally-extended behaviors can effectively obtain high-rewarding trajectories, even when the Q-switch is used to evaluate the mixture policy on the current action.\n\n### 4. Data Sampling Details\nTo train the policy and critic for task $i$, we sample uniformly from the data collected in the replay buffer by QMP\u2019s mixture policy $\\pi_i^{\\text{mix}}$ without needing to do any specialized data balancing or priority sampling based on rewards.  This means that the data collected by QMP is helpful for training $\\pi_i$ without additional implementation tricks.\n\n### Baseline Parameter Sharing Method\nIn section 6.2, we use a widely used baseline parameter sharing method, instead of more sophisticated methods like the reviewer points out. This is because this experiment was not meant to compare parameter sharing v/s behavior sharing, but to show that they are **complementary** \u2014 which we will clarify in the text. Our goal is *not* to compare parameters-only v/s behaviors-only, because they are orthogonal and complementary ways to share knowledge in MTRL. Our goal is to show that in simultaneous MTRL, behavior-sharing can augment other ways of sharing.\n- Particularly, in Maze Navigation, we see that Parameters-Only already improves significantly over Neither, which shows that methods like PCGrad are not needed in this environment. Our goal is to show that even in this case, Parameters + Behavior sharing further improves significantly over Parameters-Only.\n- As the reviewer points out, in Multistage Reacher, because of the potential conflicts in tasks, parameters-only performs poorly than Neither. However, still Parameters + Behaviors improves over parameters-only. In this case, while combinations of QMP and more sophisticated parameter sharing may yield even better results. However, we believe that the multi-head policy we used is sufficient evidence to demonstrate that QMP is **complementary** with parameter sharing.\n\nWe hope this clarifies and addresses the concerns raised.\n\n### [References]\n[1] Teh et al. Distral. Robust multitask reinforcement learning. NeurIPS, 2017.  \n[2] Ghosh et al. Divide-and-conquer reinforcement learning. ICLR, 2018. \\\n[3] Yu et al. Conservative data sharing for multi-task offline reinforcement learning. NeurIPS, 2021. \\\n[4] Yu et al. How to leverage unlabeled data in offline reinforcement learning. ICML, 2022.  \n[5] Kalashnikov et al., Scaling up multi-task robotic reinforcement learning. CoRL 2021b.  \n[6] Reed, Scott, et al. \"A Generalist Agent.\" TMLR 2022. \\\n[7] Jaegle, Andrew, et al. Perceiver IO: A General Architecture for Structured Inputs & Outputs. ICLR 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728946750,
                "cdate": 1700728946750,
                "tmdate": 1700728946750,
                "mdate": 1700728946750,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]