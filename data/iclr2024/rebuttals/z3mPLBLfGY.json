[
    {
        "title": "Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning"
    },
    {
        "review": {
            "id": "6h3tZI7hrp",
            "forum": "z3mPLBLfGY",
            "replyto": "z3mPLBLfGY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_KrxA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_KrxA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Generalist Equivariant Transformer (GET) to develop a universal representation of 3D molecular complexes and the GET model for capturing domain-specific hierarchies and domain-agnostic interaction physics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is well-motivated and well-written.\n* The proposed geometric graph of sets can be applied to various types of molecules, unifying the encoding process across different molecular structures."
                },
                "weaknesses": {
                    "value": "Related concerns are discussed in the questions section."
                },
                "questions": {
                    "value": "* The paper includes comparisons with several models, such as SchNet, DimeNet++, EGNN, and ET. However, there are other models that exhibit superior performance, including GemNet, NeurIPS, Allegro, MACE, etc. The paper would benefit from a more comprehensive comparison with existing methods. While the experimental results demonstrate the effectiveness of the proposed method, a more detailed comparison with other state-of-the-art models would strengthen the paper's claims. Additionally, it is important to cite these related works, as they are highly relevant to the topic of equivariant GNNs.\n* Can the authors discuss the implications in practical applications?\n* Typo: The model compared with GET is TorchMDNet (ET), not TorchMD."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698331195492,
            "cdate": 1698331195492,
            "tmdate": 1699636945963,
            "mdate": 1699636945963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EbBNzgLsN8",
                "forum": "z3mPLBLfGY",
                "replyto": "6h3tZI7hrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KrxA"
                    },
                    "comment": {
                        "value": "We sincerely thank your constructive comments. We address your concerns below and have revised our paper accordingly.\n\n> Q1: The paper includes comparisons with several models, such as SchNet, DimeNet++, EGNN, and ET. However, there are other models that exhibit superior performance, including GemNet, NeurIPS, Allegro, MACE, etc. The paper would benefit from a more comprehensive comparison with existing methods. While the experimental results demonstrate the effectiveness of the proposed method, a more detailed comparison with other state-of-the-art models would strengthen the paper's claims. Additionally, it is important to cite these related works, as they are highly relevant to the topic of equivariant GNNs.\n\nThanks for the valuable suggestion! We appreciate your recommendation to conduct a more comprehensive comparison with existing state-of-the-art models. In response to your feedback, we have augmented our experimental comparisons by including four strong baselines, namely GemNet, Equiformer, MACE, and LEFTNet. MACE was selected due to its superior performance among the suggested baselines according to its paper [A]. Our GET consistently outperforms these additional strong baselines, reaffirming its effectiveness and superiority. We have also expanded the discussion in the related works section to provide readers with a comprehensive understanding of the landscape of existing methods and emphasize the unique contributions and strengths of our proposed approach.\n\n\n[A] Batatia, I., Kovacs, D. P., Simm, G., Ortner, C., & Cs\u00e1nyi, G. (2022). MACE: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural Information Processing Systems, 35, 11423-11436.\n\n> Q2: Can the authors discuss the implications in practical applications?\n\nThank you for raising the question regarding the practical implications of our work.\n\nFirstly, we believe that the introduction of a unified molecular representation marks a significant stride in the field of geometric molecular representation learning. The challenge of data scarcity, primarily stemming from the high costs associated with wetlab experiments, has long hindered progress in this domain. Our approach posits that, despite the limited availability of data in specific molecular domains, the underlying interaction mechanisms are shared across diverse domains. Consequently, we propose a unified model capable of accommodating data from different molecular domains, presenting a promising solution to the challenge of data scarcity. The keypoint of this strategy lies in the invention of unified molecular representations and corresponding models that exhibit robust generalization across different molecular domains. Our work serves as a first step towards this vision, demonstrating that our GET benefits from training on mixed data across different domains and exhibits exceptional zero-shot ability even on entirely unseen domains.\n\nSecondly, in practical applications such as affinity prediction, our model offers a valuable tool for leveraging abundant data from other domains to enhance predictive performance in specific domains that suffer from data scarcity. We illustrate this feasibility through our zero-shot experiments on RNA-ligand affinity prediction in Section 4.3. By demonstrating the adaptability of our model to a new domain without the need for specific traning data, we showcase the practical utility of our approach in scenarios where data is limited.\n\nWe have revised the manuscript (Appendix L) to further emphasize these points, and we hope this clarifies the practical significance of our proposed method.\n\n> Q3: Typo: The model compared with GET is TorchMDNet (ET), not TorchMD.\n\nWe are sorry for our choice of ambiguous abbreviation. We have corrected it in the revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319294286,
                "cdate": 1700319294286,
                "tmdate": 1700319294286,
                "mdate": 1700319294286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G7PyCt1NOc",
                "forum": "z3mPLBLfGY",
                "replyto": "6h3tZI7hrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments! To address your concerns, we have expanded our experiments with the results for four state-of-the-art models and discussed the practical implications. Please feel free to check if there is still any confusion. As the end of the discussion phase is approaching, we really hope we can hear from your feedback."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530571633,
                "cdate": 1700530571633,
                "tmdate": 1700530571633,
                "mdate": 1700530571633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qdEZ6s00lE",
                "forum": "z3mPLBLfGY",
                "replyto": "6h3tZI7hrp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer KrxA:\n\nAs the discussion period draws to a close today, we are reaching out to kindly inquire whether our responses have effectively addressed any concerns you may have had. If there are any lingering concerns or if you have additional questions, we would be grateful for the opportunity to address them. Your thoughtful consideration is crucial to the success of our work and we look forward to hearing from you."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714077678,
                "cdate": 1700714077678,
                "tmdate": 1700714077678,
                "mdate": 1700714077678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g3GtMbmgkz",
            "forum": "z3mPLBLfGY",
            "replyto": "z3mPLBLfGY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a unified representation of molecules as geometric graphs of sets, by performing all-atom analysis via hierarchical processing.\nThe model is based on the Transformer architecture where atom-level cross-attention is performed between the atoms of the same block (a subset of atoms) and where another attention is then applied at the block level. Equivariant layer norm and feed-forward layers are applied to maintain the equivariance of the coordinates. \nSeveral experiments on molecular interactions show the superiority of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is original in its hierarchical approach and equivariant design.\n2. The overall performances are significant"
                },
                "weaknesses": {
                    "value": "1. The paper is not clear: what makes the contributions' novelty hard to understand. Please see Question 1.\n2. The assessment is not informative. Please see Question 2."
                },
                "questions": {
                    "value": "The paper needs refinement in order to be clearer by providing motivations and explanations and most importantly better emphasize the technical contributions of the work.\n\n1. Clarity:\n\nEquation (3) is not well defined (the MLP/RBF used) it seems closely related to Shnet or Physnet [3]. Also, learning/refining the self-attention has been investigated (e.g. [1]).\n\nUnclear how (5) is different from (Jin and al.)'s pooling.\n\nThere is some lack of motivation as to why one needs to keep track of the coordinates (which requires all the equivariant design of the non-self-attention layers) rather than working with equivariant measures such as the interatomic pairwise distances as most works do (and using other metrics too, e.g. C-RMSD [3,4]).\n\nIt is unclear (at least at first sight without looking at the appendix) how equation (10) is E(3)-equivariant (especially rotation).\n\nMost importantly (and that's related to question 2), it's unclear (given the well-known properties of Transformers with long-range dependencies processing) why hierarchical processing is better than atom-level analysis.\n\n2. Assessment: \n\nIn order to have a good comparison with the baselines, one should add the models' complexity and capacity comparison.\nThus, while comparing to old baselines such as Shnet or Dimenet++ (Tables 2 and 3), one should compare the difference in capacity and complexity of the models which still perform well and should be much more efficient.\n\n[1]  Geometric transformer for end-to-end molecule properties prediction.\n\n[2]  PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges\n\n[3] Molecular geometry prediction using a deep generative graph neural network\n[4] Diffusion-based molecule generation with informative prior bridges"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570351543,
            "cdate": 1698570351543,
            "tmdate": 1699636945793,
            "mdate": 1699636945793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MUfdsfOOOQ",
                "forum": "z3mPLBLfGY",
                "replyto": "g3GtMbmgkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cg1F (1/3)"
                    },
                    "comment": {
                        "value": "We sincerely thank your constructive comments. We address your concerns below and have revised our paper accordingly.\n\n> Q1: Equation (3) is not well defined (the MLP/RBF used) it seems closely related to Shnet or Physnet [3].\n\nWe apologize for the unclear presentation. The MLP is a 2-layer multi-layer perceptron with SiLU [A] activation. The RBF function embeds distances into $d_{\\text{rbf}}$-dimensional vectors with the following definition:\n\n$$\\text{RBF}(d)[k]\u00a0=\u00a0u(\\frac{d}{c})* \\exp(-\\frac{|d-\\mu_k|}{2 \\gamma}),\u00a01\\leq\u00a0k \\leq\u00a0d_{\\text{rbf}}$$\n\nwhere $\\mu_k$ is uniformly distributed in $[0, c]$($c=7.0$ in our paper), $\\gamma\u00a0=\u00a0\\mu_k\u00a0-\u00a0\\mu_{k-1}\u00a0=\u00a0\\frac{c}{d_{\\text{rbf}}}$, and $u(x)$ is the polynomial envelope function:\n\n$$u(x) =\u00a01\u00a0-\u00a0\\frac{(p+1)(p+2)}{2}\u00a0x^p\u00a0+\u00a0p(p+2)x^{p+1}\u00a0-\u00a0\\frac{p(p+1)}{2}x^{p+2}$$\n\nWe set $p=5$ in our paper. The reason why we apply the RBF function is inspired by [A][B] for its promising performance. We have added the definition to the revision.\n\n[A] Sch\u00fctt, K., Kindermans, P. J., Sauceda Felix, H. E., Chmiela, S., Tkatchenko, A., & M\u00fcller, K. R. (2017). Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 30.\n\n[B] Gasteiger, J., Gro\u00df, J., & G\u00fcnnemann, S. (2020). Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123.\n\n> Q2: Also, learning/refining the self-attention has been investigated (e.g. [1]).\n> \n> [1] Geometric transformer for end-to-end molecule properties prediction.\n\nThanks for the comment. We would like to point out that our GET is directly implemented on 3D coordinates with equivariance, while [1] actually uses pairwise distance matrix to represent the geometry to achieve invariance. Moreover, we inject the inductive bias of custom hierarchy into message passing, while [1] is implemented on atomic level only. We have updated the related work with more discussion on the difference between our work and previous literature on geometric transformers/GNNs.\n\n> Q3: Unclear how (5) is different from (Jin and al.)'s pooling.\n\nWe are sorry we did not express the difference clear enough. Equation (5) is the pooling on atomic pairwise relations to obtain the relation between two blocks, while (Jin and al.)'s pooling is implemented on atoms within each block to obtain the block-level representations. In particular, (Jin and al.)'s method first pools atom-level features into residue-level features, and then conducts message passing on the residue-level graphs. In constrast, our bilevel method retains the information of both atom-level and residue-level features simultaneously in each layer, namely, allowing both attention-based message passing between the atoms winthin the same residue and the neighbor residues."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318924117,
                "cdate": 1700318924117,
                "tmdate": 1700318924117,
                "mdate": 1700318924117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cyEwJ5LRdH",
                "forum": "z3mPLBLfGY",
                "replyto": "g3GtMbmgkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cg1F (2/3)"
                    },
                    "comment": {
                        "value": "> Q4: There is some lack of motivation as to why one needs to keep track of the coordinates (which requires all the equivariant design of the non-self-attention layers) rather than working with equivariant measures such as the interatomic pairwise distances as most works do (and using other metrics too, e.g. C-RMSD [3,4]).\n> \n> [3] Molecular geometry prediction using a deep generative graph neural network\n> \n> [4] Diffusion-based molecule generation with informative prior bridges\n\nThank you for raising the insightful question regarding equivariant designs. We would like to answer this question from two aspects: theoretical expressivity and practical efficiency.\n\n**Theoretical Expressivity:**\n\nExisting theoretical work [A] (Theorem 8) has established that equivariant methods, which involve keeping track of coordinates, offer greater expressivity compared to invariant methods relying on pairwise distances (please see Theorem 8 in [A]). Specifically, the study reveals that equivariant Graph Neural Networks (GNNs) share expressivity with the Geometric Weisfeiler-Leman Test (GWL), while invariant counterparts align with the Invariant GWL (IGWL). The crucial distinction emerges in scenarios commonly encountered in molecular representation learning, where detecting isomorphic geometric graphs pose a challenge for invariant methods due to the lack of directional geometry in pairwise distances. In contrast, equivariant counterparts, such as our proposed GET, effectively handle these cases, enhancing the model's capacity to discern intricate geometric relationships within molecular structures.\n\n**Practical Efficiency:**\n\nFurthermore, the practical efficiency of our approach is necessary for addressing the challenges associated with scalability in molecular interaction graphs. Fully connected graphs, often implemented in invariant methods, exhibit $O(n^2)$ complexity ($n$ being the number of nodes), making them less viable for large graphs. Most existing methods relying on pairwise distances use $n \\times n$ distance matrices, limiting their application to small molecular graphs with fewer than 50 nodes [B][C]. In our experiments, molecular interaction graphs are considerably larger, ranging from 230 to 872 atom nodes on average, as they incorporate extensive biomolecular structures. The equivariant nature of our GET, with linear complexity relative to the number of nodes, ensures its scalability to these larger graphs in practical settings.\n\nWe hope this clarification provides a thorough understanding of our motivation for choosing equivariant designs in GET.\n\n[A] Joshi, C.K., Bodnar, C., Mathis, S.V., Cohen, T. &amp; Lio, P.. (2023). On the Expressive Power of Geometric Graph Neural Networks. Proceedings of the 40th International Conference on Machine Learning, in Proceedings of Machine Learning Research 202:15330-15355 Available from https://proceedings.mlr.press/v202/joshi23a.html.\n\n[B] Molecular geometry prediction using a deep generative graph neural network\n\n[C] Diffusion-based molecule generation with informative prior bridges\n\n> Q5: It is unclear (at least at first sight without looking at the appendix) how equation (10) is E(3)-equivariant (especially rotation).\n\nWe apologize for the mistake in the presentation of equation (10). In equation (10), $\\sigma_m$ should be a one-dimension scalar used to scale the geometric message vector $\\vec{\\mathbf{m}}_{ij, p}\\in\u00a0\\mathbb{R}^{3}$. Intuitively, this operation only changes the length of the vector but maintains the direction, thus it is equivariant towards $O(3)$ transformations. We have corrected the equation in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319059130,
                "cdate": 1700319059130,
                "tmdate": 1700319059130,
                "mdate": 1700319059130,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RIEWI1xfdZ",
                "forum": "z3mPLBLfGY",
                "replyto": "g3GtMbmgkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cg1F (3/3)"
                    },
                    "comment": {
                        "value": "> Q6: Most importantly (and that's related to question 2), it's unclear (given the well-known properties of Transformers with long-range dependencies processing) why hierarchical processing is better than atom-level analysis.\n\nThank you for your thoughtful consideration and raising this important question.\n\nFirst, high complexity ($n^2$) is needed to achieve long-range dependencies processing for normal Transformers, which might not be applicable here. As mentioned in the previous answer to Q4, molecular interaction graphs can be extremely large, incorporating a multitude of atoms and interactions. In the context of fully connected attention matrices in Transformers, the computational complexity becomes a significant concern with such large atom-level graph sizes.\n\nFurthermore, the choice of hierarchical processing is informed by the need to capture and preserve block-level geometry, which plays a crucial role in certain molecular interaction tasks (e.g. PPA). Atom-level analysis, while valuable in capturing local interactions, may overlook the broader spatial arrangements and relationships between molecular components at higher levels of abstraction. Our bilevel representation allows us to retain and leverage the block-level and atom-level geometry simultaneously, enhancing the model's capacity to discern complex spatial patterns and dependencies within and between the molecular structures.\n\nWe appreciate your concern and hope this explanation sheds light on our rationale for choosing hierarchical processing.\n\n> Q7: In order to have a good comparison with the baselines, one should add the models' complexity and capacity comparison. Thus, while comparing to old baselines such as Shnet or Dimenet++ (Tables 2 and 3), one should compare the difference in capacity and complexity of the models which still perform well and should be much more efficient.\n\nThank you for the valuable suggestion!\n\nIn our revised manuscript, we have thoughtfully addressed the comparison of model complexity and capacity. Detailed information on the number of parameters and training times for all baselines, both old and new, is available in Table 9 in Appendix G.\n\nWhen comparing GET to simpler yet weaker baselines (e.g., SchNet and EGNN), it is evident that GET may have more parameters and a slower training speed. However, it's crucial to note that GET exhibits competitive parameter and computation efficiency when compared with more complex yet stronger baselines, such as Equiformer, MACE, and LEFTNet. It's worth mentioning that a significant portion of parameters in GET is attributed to the Feedforward Neural Network (FFN) that projects latent features to higher dimensions in intermediate layers, aligning with the structure of vanilla Transformers. Without this part, GET has the least parameters among all the models.\n\nMoreover, the throughput of GET is comparable to both atom-level and hierarchical counterparts. This aligns with the model's design, which considers both block-level and atom-level geometry. To further elucidate the efficiency of our approach, we provide a complexity analysis of GET in Appendix D, showing its linear complexity concerning the number of nodes in the graph. This characteristic indicates favorable scalability to large graphs in practical settings.\n\nIn conclusion, our model maintains a moderate number of parameters and efficient training speed while consistently outperforming baselines across multiple evaluation experiments. We believe these detailed analyses contribute to a comprehensive understanding of the efficiency and effectiveness of our proposed model."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319179412,
                "cdate": 1700319179412,
                "tmdate": 1700319179412,
                "mdate": 1700319179412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HEpqBg99m5",
                "forum": "z3mPLBLfGY",
                "replyto": "g3GtMbmgkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable suggestions! To alleviate your concerns, we have clarified the motivation for our equivariant and bilevel design, as well as analyzed the capacity and complexity of different models. Please feel free to check if there is still any problem. As the end of the discussion phase is drawing near, we really hope we can hear from your feedback."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530533799,
                "cdate": 1700530533799,
                "tmdate": 1700530533799,
                "mdate": 1700530533799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u8u5YObSVw",
                "forum": "z3mPLBLfGY",
                "replyto": "HEpqBg99m5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers and rebuttal. \n\nRegarding Q6:\n\nMy concern was the following: given the huge context (now reaching millions) modern LLMs can handle, why atom-level analysis is worse than the hierarchical one? It can be practically performed (obviously with more or less strong resources) so complexity is not an issue, and should theoretically encode all levels of representation for optimal prediction.\n\nRegarding Q7:\n\nOther close baselines (Equiformer) seem to have close performance with much fewer parameters. How does it support the gain in accuracy claim?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566732624,
                "cdate": 1700566732624,
                "tmdate": 1700566732624,
                "mdate": 1700566732624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PHXzvPvN1O",
                "forum": "z3mPLBLfGY",
                "replyto": "WKmNs9Z0Wk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_cg1F"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers.\n\nUnfortunately, my concerns remain unmoved. The fact existing molecular datasets are of very moderate size is a good motivation for atom-level analysis in regard to the processing and analysis capabilities of modern Transformers. I was not referring to the number of data samples but to the context/molecule size. Given that modern Transformers can *efficiently process* extremely large contexts without requiring hierarchical analysis why would it not be the case for molecular tasks.\nAlso, the fact the ablation study did not support this was the reason for this question in the first place."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727301038,
                "cdate": 1700727301038,
                "tmdate": 1700727301038,
                "mdate": 1700727301038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aYo1iNGgRM",
            "forum": "z3mPLBLfGY",
            "replyto": "z3mPLBLfGY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_cTJZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_cTJZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a bilevel representation that can represent small molecules and large ones (e.g. proteins) in a unified manner. The core is to cluster the complex into a set of blocks, where intra-block connections are dense while inter-block connections are sparse. Based on this representation, the authors propose a Generalist Equivariant Transformer (GET) that captures both domain-specific hierarchies and domain-agnostic interaction physics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed representation and GET are reasonable and novel.\n2. The paper is structured well and clearly written.\n3. The authors conduct extensive experiments to support their claim, and the results are good."
                },
                "weaknesses": {
                    "value": "I do not see obvious weaknesses in the proposed method and the experiments. Below are some small flaws:\n1. How to construct the bilevel representation should be elaborated (e.g., what is the K of the KNN graph?). \n2. Typo: (In Sec. 1) \"In this paper, we approache ...\""
                },
                "questions": {
                    "value": "1. Compared to KNN, are there better ways to construct a block?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767370017,
            "cdate": 1698767370017,
            "tmdate": 1699636945486,
            "mdate": 1699636945486,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kTCX0x9Bj3",
                "forum": "z3mPLBLfGY",
                "replyto": "aYo1iNGgRM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cTJZ"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your positive comments. We address your concerns below and have revised our paper accordingly.\n\n> Q1: How to construct the bilevel representation should be elaborated (e.g., what is the K of the KNN graph?).\n\nWe apologize for the missing details. Each node in the graph consists of a set of atoms in the block. Suppose there are $n_i$ atoms in block $i$, then it is assigned with a feature matrix $\\mathbf{H}_i \\in \\mathbb{R}^{n_i \\times d}$ and a coordinate matrix $\\vec{\\mathbf{X}}_i \\in \\mathbb{R}^{n_i \\times 3}$, where each row corresponds to one atom. And edges are deduced from the k-nearest neighbors of each node, where distance is defined as the minimum pairwise distances between atoms in two blocks. In this paper we use $K=9$ which is enough for supporting the competitive performance of our model. Overall, the topology of the graph is defined on the block-level geometry while atom-level instances are maintained via extensions of node features from single vectors to dynamic matrices. We have updated the manuscript for a clearer presentation in Section 3.1.\n\n> Q2: typo: (In Sec. 1) \"In this paper, we approache ...\"\n\nThanks for pointing this out. We have fixed the typo in the revision.\n\n> Q3: Compared to KNN, are there better ways to construct a block?\n\nThanks for the comment. The most common ways to construct the graph are KNN graph, radius graph (adding edges by distance cutoff), and complete graph (each node connects to all other nodes). Practically we choose KNN graph because its complexity (i.e. Number of edges) is linear to the number of nodes, which shows desired scalability to larger graphs used in our paper. It is worth mentioning that achieving good balance between complexity and performance through better construction of the graph is also a valuable research question. Existing literature has been exploring combination of different constructions [A]. This might also be related to sparse attention [B] considering that the topology of graphs can be represented as adjacent matrices.\n\n[A] Zhang, Z., Xu, M., Jamasb, A., Chenthamarakshan, V., Lozano, A., Das, P., & Tang, J. (2022). Protein representation learning by geometric structure pretraining. arXiv preprint arXiv:2203.06125.\n\n[B] Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. Efficient transformers: A survey. arXiv 2020. arXiv preprint arXiv:2009.06732."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318320643,
                "cdate": 1700318320643,
                "tmdate": 1700318320643,
                "mdate": 1700318320643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cfXZGisejB",
                "forum": "z3mPLBLfGY",
                "replyto": "aYo1iNGgRM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive comments! We have elaborated on the construction of the blocks and the bilevel graph to ease your concerns. As the deadline of the discussion phase is approaching, we really hope we can hear from your feedback."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530484462,
                "cdate": 1700530484462,
                "tmdate": 1700530484462,
                "mdate": 1700530484462,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LzZzfxycC6",
            "forum": "z3mPLBLfGY",
            "replyto": "z3mPLBLfGY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_g12k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7748/Reviewer_g12k"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a generalist equivariant Transformer architecture for learning 3D molecular interactions. The proposed GET model is composed of bilevel attention networks, feed-forward networks, and layer normalization modules, all of which are all equivariant to E(3)-equivariant transformations. The GET model is able to simultaneously learn both the atom- and block-level information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is clearly written. The problem is well motivated.\n+ The paper proposes a universal representation for molecular complexes. This approach has the potential to streamline and unify various interaction studies across different molecular domains.\n+ The GET model ensures that all modules are E(3)-equivariant and also allows capturing interactions at block and atom scales, potentially leading to richer and more informative interaction modeling.\n+ The experiments across small molecules, proteins, and nucleic acids suggest that GET has strong generalization capabilities."
                },
                "weaknesses": {
                    "value": "- The reliance on domain-specific knowledge for the construction of building blocks may limit the universality of the proposed GET model (e.g., each residue in the proteins is one node). This may render the comparison with hierarchical models (e.g., GVP) a bit unfair.\n- The paper would be strengthened by a more thorough comparison with state-of-the-art methods, e.g., GemNet, Equiformer, EquiformerV2, and LEFTNet.\n- The experiments conducted on PPA and PBA datasets may not fully demonstrate the effectiveness of the proposed model, given their relatively small scale.\n- I do not see a particular design of modeling molecular interactions in the GET architecture. It appears that it is also possible to evaluate the GET model on a broader range of molecular settings, especially bare molecules.\n- The equivariant transformers and attention mechanisms raise potential concerns for practical application due to their high computational complexity, as noted in Table 9."
                },
                "questions": {
                    "value": "- Including the graphical illustration (Figure 4) within the main text could provide readers with a more immediate and clear understanding of the architecture and operational flow of GET.\n- Adding statistics for the PDBBind benchmark within the paper would offer a more concrete assessment for readers unfamiliar with the benchmark."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7748/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000440310,
            "cdate": 1699000440310,
            "tmdate": 1699636945322,
            "mdate": 1699636945322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HZvn5KPgCm",
                "forum": "z3mPLBLfGY",
                "replyto": "LzZzfxycC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g12k (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank your constructive comments. We address your concerns below and have revised our paper accordingly.\n\n> Q1: The reliance on domain-specific knowledge for the construction of building blocks may limit the universality of the proposed GET model (e.g., each residue in the proteins is one node). This may render the comparison with hierarchical models (e.g., GVP) a bit unfair.\n\nThank you for the comment. There could be some misunderstandings here. We provide the explanations below.\n\nFirst, we would like to emphasize that our GET model can accommodate arbitrary construction of building blocks, using wheter domain-specific kowledge or purely data-driven approaches. To show this universality, we did implement GET-PS in section 4.2, Table 3, which uses a data-driven method (i.e. principal subgraphs in the PS-VAE paper [A]) to build blocks in small molecules, and it indeed achieves brilliant performance. \n\nSecond, we want to clarify that one significant strength of our GET is the capability of capturing both coarse-grained (blocks) and fine-grained (atoms) geometry. In section 4.2, for fair comparison, all the block-level baselines and the hierarchical baselines use exactly the same building blocks as our GET. However, their inability to retain bilevel geometry simultaneously makes them suboptimal compared to our proposed GET.\n\n[A] Kong, X., Huang, W., Tan, Z., & Liu, Y. (2022). Molecule generation by principal subgraph mining and assembling. Advances in Neural Information Processing Systems, 35, 2550-2563.\n\n> Q2: The paper would be strengthened by a more thorough comparison with state-of-the-art methods, e.g., GemNet, Equiformer, EquiformerV2, and LEFTNet.\n\nThank you for your insightful recommendation to conduct a more thorough comparison with state-of-the-art methods. We have expanded our experimental comparisons by incorporating four suggested baselines: GemNet, Equiformer, MACE (suggested by Reviewer KrxA), and LEFTNet in Table 2. Besides, owing to their commendable performance and high efficiency, MACE and LEFTNet are further selected for testing on the mix-domain data augmentation in Figure 3 and zero-shot generalization ability in Table 3. Delightfully, our GET consistently outperforms all these strong baselines in all cases, demonstrating its general effectiveness and superiority. We have updated the manuscript with the results of these additional experiments.\n\n> Q3: The experiments conducted on PPA and PBA datasets may not fully demonstrate the effectiveness of the proposed model, given their relatively small scale.\n\nThanks for the comment.\n\nWe admit that PPA and LBA are of small scale, but these two datasets are our suitable choice to verify the effectiveness of GET in learning unified representations between different types of molecular interactions, namely, protein and protein in PPA, and protein and small molecule in LBA.  Actually, in the domain of molecular interaction, how to address the scarcity of data is a long-term problem.  While it is arduous to collect more data due to the high cost of wetlab experiments, we have tried to enhance the robustness and reliability of the empirical evaluations by running parallel experiments with different random seeds and data splits.\n\nBesides the experiments on  PPA and LBA, we additionally evaluate the zero-shot ability of our GET on DNA/RNA-ligand binding affinity prediction in section 4.3, as this is strong evidence for the exceptional generalizability and high data efficiency of the proposed method. It indicates our model's proficiency in adapting to completely unseen data, which is particularly valuable in addressing the challenge of data scarcity.\n\nWe also want to emphasize the promising strategy outlined in our paper, which involves better utilizing all available data from different domains through the proposed unified molecular representation. This strategy aligns with the broader goal of overcoming data scarcity challenges by maximizing the information gleaned from diverse datasets."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318025109,
                "cdate": 1700318025109,
                "tmdate": 1700318072104,
                "mdate": 1700318072104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0YMwIXCx63",
                "forum": "z3mPLBLfGY",
                "replyto": "LzZzfxycC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g12k (2/2)"
                    },
                    "comment": {
                        "value": "> Q4: I do not see a particular design of modeling molecular interactions in the GET architecture. It appears that it is also possible to evaluate the GET model on a broader range of molecular settings, especially bare molecules.\n\nThanks for the suggestion! In this paper, we mainly focus on proposing a unified molecular representation for different types of molecular complexes, as well as a corresponding model for effective representation learning. The motivation behind is that a unified molecular representation is critical for modeling interaction geometry between multiple molecules, and endows the model with inspiring generalizability. While it is possible to evaluate GET on bare molecules, it is out of the main scope of this paper and be better left for further exploration. We have discussed this point in the limitation section (now in Appendix L due to the space limit) in the revision.\n\n> Q5: The equivariant transformers and attention mechanisms raise potential concerns for practical application due to their high computational complexity, as noted in Table 9.\n\nThanks for the comment. We appreciate your attention to this matter, and we have carefully considered your comments.\n\nWe acknowledge that equivariant transformers and attention mechanisms can introduce computational complexity. However, we would like to emphasize that our approach remains highly competitive in terms of efficiency when compared to new strong baselines (GemNet, Equiformer, MACE, and LEFTNet). In practical terms, our model offers a compelling balance between performance and computational speed, as demonstrated by the results in Table 9. Also, we have provided complexity analysis in Appendix D, which shows good scalability (linear instead of quadratic) with respect to the number of atoms in the complex graph.\n\nMoreover, despite any inherent complexity, our approach remains significantly faster than traditional computational tools (such as rosetta) in real-world applications, supporting its practicality. \n\nWe have updated the manuscript to reflect these clarifications and additions to Table 9. We believe that these revisions enhance the overall quality and understanding of our work.\n\n> Q6: Including the graphical illustration (Figure 4) within the main text could provide readers with a more immediate and clear understanding of the architecture and operational flow of GET.\n\nThanks for the advice! While it might be hard to directly include figure 4 in the main text due to the space limit, we have added a reference to it at the beginning of section 3 (Method).\n\n> Q7: Adding statistics for the PDBBind benchmark within the paper would offer a more concrete assessment for readers unfamiliar with the benchmark.\n\nThanks for the advice! We have added necessary statistics for the PDBBind benchmark in the revision (Section 4.1 and Appendix F)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318162970,
                "cdate": 1700318162970,
                "tmdate": 1700318162970,
                "mdate": 1700318162970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zzyQoT8rGK",
                "forum": "z3mPLBLfGY",
                "replyto": "LzZzfxycC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments! To address your concerns, we have clarified our claims, provided additional results for four state-of-the-art models, and analyzed the complexity between different models. Please feel free to check if there is still any confusion. As the deadline of the discussion phase is drawing near, we really hope we can hear from your feedback."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530432579,
                "cdate": 1700530432579,
                "tmdate": 1700530432579,
                "mdate": 1700530432579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aPclCd6Dh9",
                "forum": "z3mPLBLfGY",
                "replyto": "LzZzfxycC6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer g12k:\n\nAs the discussion phase is approaching its end today, we would appreciate it a lot if you could confirm whether our response has alleviated your concerns. Your insights are invaluable, and we are eager to address any remaining concerns or questions you may have. We really hope we can hear from your feedback, which plays a crucial role in enhancing the quality of the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713286199,
                "cdate": 1700713286199,
                "tmdate": 1700713286199,
                "mdate": 1700713286199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lHlkY5AUy6",
                "forum": "z3mPLBLfGY",
                "replyto": "aPclCd6Dh9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_g12k"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7748/Reviewer_g12k"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your hard work. I believe most of my concerns have been cleared except for Q4. Considering that a unified molecular representation is important for modeling interaction geometry between multiple molecules, and that the GET model does not particularly model interactions of molecular interactions, it would not be unreasonable to evaluate the performance of GET on molecular tasks at first."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7748/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730125226,
                "cdate": 1700730125226,
                "tmdate": 1700730125226,
                "mdate": 1700730125226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]