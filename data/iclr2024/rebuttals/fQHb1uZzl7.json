[
    {
        "title": "Unifying Feature and Cost Aggregation with Transformers for Dense Correspondence"
    },
    {
        "review": {
            "id": "nclKPlabOl",
            "forum": "fQHb1uZzl7",
            "replyto": "fQHb1uZzl7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission806/Reviewer_HY22"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission806/Reviewer_HY22"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a dense matching estimation method by unifying both feature and cost volume aggregation with transformer. The authors first analyze the merits and faults of feature and cost aggregation and then claim that using them interleavely could improve the feature representation. Experiment results show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. To the best of my knowledge, this is the first work that discusses the relationship between feature and cost aggregation (learning). The authors carefully discuss their merits and faults in Sec.1 and Sec.3.\n2. Interleavely aggregating features between feature and cost volumes is interesting. The visualization results of Fig.2, and Fig.3 also verify the effectiveness.\n3. The authors provide complete experimental details in the supplementary."
                },
                "weaknesses": {
                    "value": "1. The usage of \"aggregation\" is a little confusing. In my opinion, \"aggregation\" means combining multiple features into a single one, which should usually be used to describe the process of attention aggregation of ($QK^T$ and $V$). However, I am not sure whether \"aggregation\" is suitable to be used to indicate the whole learning process of cost volume learning. Because many cost volume learning is not related to attention learning.\n2. Although the authors analyze the merits of feature/cost aggregation, some claims have not been clarified. For example, feature matching is \"challenged by repetitive patterns and background clutters\", while cost volume learning enjoys \"robustness to repetitive patterns and background clutter\". No evidence is shown in this paper to support this claim.\n3. The authors did not formulate the method presentation well in Sec.4 and Fig.4, which makes the proposed method suffer from too complicated designs and difficult to follow. I strongly recommend the authors introduce the shape and reshape of the most important tensors to make the whole pipeline clearer. The concatenation in Eq(3) is operated along which dimension? Why $C'$ appears again in Eq.5 as $QK^T$, while $C'$ should be already defined as the output of the cost volume feature?\n4. The experiments are not solid enough. The proposed method needs to be compared with more recent methods. In the geometric matching results from Tab2, most competitors are from 2020 and 2021, which are far from \"state-of-the-art\". Only one flow estimation method is considered (GMFlow). However, as discussed in the supplementary, the comparison is not fair, because GMFlow is trained on Sintel rather than DPED-CityScape-ADE+MegaDepth fine-tuning. Besides, all these competitors are trained on DPED-CityScape-ADE **or** MegaDepth (the proposed method is trained with DPED-CityScape-ADE **and** MegaDepth finetuning) as said in supplementary B.1. The authors should clarify this."
                },
                "questions": {
                    "value": "As discussed in the related works, many stereo-matching and optical flow works use Transformer-based cost aggregation networks. \nThe idea proposed in this paper should be a general way to improve all feature matching-based tasks, and I think there are no enormous model differences among stereo, flow, and dense matching. So the authors should compare these SOTA stereo and flow methods in a more fair way. For example, re-training the model with the same data setting for dense matching or verifying the effectiveness of the proposed method in stereo and flow estimation benchmarks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486722506,
            "cdate": 1698486722506,
            "tmdate": 1699636007877,
            "mdate": 1699636007877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rC4JiLrvLW",
                "forum": "fQHb1uZzl7",
                "replyto": "nclKPlabOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer HY22 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments! Our responses can be found below: \n\n> Unsure whether the term \"aggregation\" is suitable for operations performed over cost volumes.\n\nIn the stereo matching literature, \"cost aggregation\" plays a pivotal role. This process involves combining primary matching costs, typically through a filtering process, to generate a comprehensive disparity map, as noted in source [A]. The scope of \"aggregation\" in this context is broad and versatile. It can encompass various techniques ranging from the application of simple Gaussian kernels to more complex methods like 3D convolutions, as highlighted in source [B]. Additionally, the term also covers techniques such as semi-global or local attentions, as well as other forms of attention mechanisms, indicated in source [C].\n\nThese diverse aggregation techniques serve to illustrate the multifaceted nature of cost volume learning in stereo matching. The term \"aggregation\" is not merely confined to the domain of attention-based feature combination but extends to a variety of methods, each contributing to the accurate and efficient processing of stereo imagery. This wide-ranging application of the term confirms its appropriateness and relevance in the field, demonstrating that cost volume learning indeed benefits from and incorporates a spectrum of aggregation strategies. However, we can change the term to \"refinement\" if it better fits to describe our approach.\nWe highly appreciate the reviewer's comment in this regard.\n\n[A] Cross-Scale Cost Aggregation for Stereo Matching\n[B] Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation\n[C] GA-Net: Guided Aggregation Net for End-to-end Stereo Matching\n\n> Evidences to claim that feature matching is challenged by repetitive patterns and background clutters and cost aggregation enjoys robustness to repetitive patterns and background clutter. \n\n\nThere is a well-established understanding in computer vision that feature matching can be particularly challenging in scenes with repetitive patterns and cluttered backgrounds. This difficulty arises from the reliance of feature matching techniques on identifying and aligning distinct keypoints or features between images. In environments where textures are repetitive or backgrounds are complex, these features often become ambiguous, leading to mismatches and inaccuracies. This limitation is widely recognized in the literature and has led to the development of enhanced feature detection algorithms, such as SIFT, and more recent methods like LOFTR, which aim to refine local features to extract high-quality matches in indistinctive or low-texture regions.\n\nTo support our claim, we will reference these matching methods in our paper. They exemplify the ongoing efforts in the field to address the challenges posed by such complex environments. However, we acknowledge that the assertion regarding the robustness of cost aggregation to these challenges might be contentious. Nevertheless, while our paper does not provide specific empirical evidence to support this claim (although we provide visualizations that can explain such robustness in Fig. 4 to some extent), it is nonetheless informed by a substantial body of research in the semantic matching literature. Numerous studies in this area have demonstrated the effectiveness of cost aggregation in various contexts.\n\nNevertheless, in light of the comment, we agree that it is prudent to moderate our claim regarding the robustness of cost aggregation to environments with repetitive patterns and cluttered backgrounds. We recognize that this aspect could be seen as a hypothesis rather than a conclusively proven fact. We appreciate the constructive comment and will accordingly adjust the tone of our claim to reflect a more cautious stance on this matter."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142899484,
                "cdate": 1700142899484,
                "tmdate": 1700283015078,
                "mdate": 1700283015078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2vAbicvWwI",
                "forum": "fQHb1uZzl7",
                "replyto": "nclKPlabOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer HY22 (2/2)"
                    },
                    "comment": {
                        "value": "> The dimension of concatenation in Eq.3.\n\n\nWe apologize for any confusion caused by the presentation of our methodology and appreciate the opportunity to clarify. To enhance understanding, we will include the dimensions of all tensors in our method section.\n\nAddressing the reviewer's query regarding the concatenation process, it is performed as follows: We start with a cost volume of shape h\u00d7w\u00d7h\u00d7w. To this, we concatenate a feature map of shape h\u00d7w\u00d7c, treating one of the h\u00d7wh\u00d7w dimensions as the spatial dimension. This results in a combined shape of h\u00d7w\u00d7(hw+c), effectively merging the cost volume and feature map information.\n\nRegarding Equation 5, the use of C\u2032 is an intentional workaround to avoid the conventional computation of the attention map, typically represented as QKT in standard attention mechanisms. Instead of computing the attention map traditionally, we utilize the cost volume to transform it into a matching distribution. This approach effectively functions as a cross-attention map. The effectiveness of this method is validated in Table 4, where we demonstrate its practical utility and performance. We hope this explanation clears up any ambiguities.  If there are further questions or additional clarifications needed, we are more than willing to provide further details.\n\n> Include more recent works in Table 2, and GMFlow is trained on different dataset. Verify effectiveness of the proposed method in optical flow.  \n\nWe apologize for any oversight in our initial presentation of recent methods in our comparative analysis. While Table 1 comprehensively includes recent methods, we acknowledge that Table 2 appeared somewhat outdated. To address this, we have now included an evaluation of DKM, a method that has been tested on HPatches, with its results available in its public GitHub repository. We have compared these results with those of our method:\n\n| Methods           |   I  |   II |   III | IV    | V     | Avg   |\n|-------------------|:----:|-----:|------:|-------|-------|-------|\n| PDC-Net (CVPR'21) | 1.15 | 7.43 | 11.64 | 25.00 | 30.49 | 15.14 |\n| DKM (CVPR'23)     | 2.35 | 7.24 | 16.48 | 29.86 | 39.35 | 19.06 |\n| Ours              | 1.91 | 6.13 | 5.62  | 6.36  | 19.44 | 7.88  |\n\nAs illustrated, our method demonstrates highly competitive performance, even when juxtaposed with newer methods like DKM.\n\nRegarding the comparison with GMFlow, we recognize and apologize for the previously unfair comparison. Unlike other methods in our comparison that were trained on MegaDepth, GMFlow was not, which might have skewed the results.  To address this and to further evaluate the generalizability of our method as suggested by the reviewer, we conducted additional evaluations in the context of optical flow. For these evaluations, we utilized the KITTI and SINTEL datasets. Aligning with the practices of GLU-Net, PDC-Net, and GoCOR, our method was not trained on the Chair and Things datasets but rather on MegaDepth. The results of these additional evaluations are presented below:\n\n| Methods  | KITTI-2015 | SINTEL (Final) |\n|----------|:----------:|-------:|\n| RAFT     |    5.32    |   2.69 |\n| PDC-Net  | 5.40       | 4.54   |\n| PDC-Net+ |    4.53    |      - |\n| GMFlow   | 7.77       | 3.44   |\n| Ours     | 4.39       | 4.16   |\n\nFrom the results, while ours outperforms others in KITTI-2015, it is observed that ours achieves lower AEPE than existing flow methods. However, it should be noted that when compared to PDC-Net and PDC-Net+, ours consistently performs better, which validates the effectiveness of our approach in optical flow task as well. We acknowledge that newer, more sophisticated technical designs may emerge in the future, but we wish to emphasize that while numerous architectural designs, e.g., convex upsampling, uncertainty module or GOCor-like correlation optimization scheme, could potentially be included and enhance the synergy between feature aggregation and cost aggregation, our framework works as a stepping stone that stands out for its simplicity and effectiveness."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142940209,
                "cdate": 1700142940209,
                "tmdate": 1700143011335,
                "mdate": 1700143011335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gNEXqy8Odm",
                "forum": "fQHb1uZzl7",
                "replyto": "rC4JiLrvLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Reviewer_HY22"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Reviewer_HY22"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response from the authors and additional experiments"
                    },
                    "comment": {
                        "value": "Thanks for the response from the authors and additional experiments. \n\n1. The concern about \"aggregation\" is addressed by the additional illustration from the authors. But I still recommend to provide some discussion about this term in the paper.\n\n2. For the claim about solving \"challenged by repetitive patterns and background clutters\", the authors explain the importance of this in feature matching. I am already aware that tackling this issue is an intricate and widely recognized problem within the domain of feature matching. Thus the way to handle it attaches much attention to this field. Regrettably, the authors commit to tuning down the claim, diminishing the overall contribution of this paper.\n\n3. For the experiment results compared to DKM, I think the authors should further provide the results of AUC 3/5/10px as shown in the DKM paper to make the comparison more solid.\n\n4. For the results of flow estimation, the improvement is not very significant compared to PDC-Net+ (KITTI) and GMFlow (SINTEL)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580440030,
                "cdate": 1700580440030,
                "tmdate": 1700580440030,
                "mdate": 1700580440030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YOQ7Gtf22t",
                "forum": "fQHb1uZzl7",
                "replyto": "nclKPlabOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply!"
                    },
                    "comment": {
                        "value": "Thanks for the reply!\n\n1. We wish to emphasize that although we said that we will tone down the sentence, this does not mean that the substantial body of study in the semantic matching literature has been adopting cost aggregation for no benefits. It has been recognized by several works that cost aggregation enjoys strong generalization power [A, B], many works in semantic matching adopts cost aggregation to account for the background clutters, repetitive patterns and intra-class variations and finally, we also clearly showed adopting cost aggregation boosts the performance. Moreover, our contribution should not be diminished from the toned down claim, but rather be enhanced since we managed to show  the improved performance in semantic matching. This is an evidence that our method better considered such challenges.\n\n2. We agree that for solid comparisons, we should add DKM if we include pose estimation and visual localization. However, we wish to highlight that these sparse matching tasks are beyond our paper's scope and  we **already evaluated our method on 5 dense correspondence benchmarks, and also yielded competitive performance on additional benchmarks we used for this rebuttal.** We also discussed a limitation that the typical dense matching method faces for sparse matching tasks (while DKM is also a dense method and it attains SOTA, it's their main contribution that they propose a method that overcomes this drawback. Our contribution lies more on thorough and comprehensive investigation and focus on dense tasks rather than achieving SOTA on every matching related tasks). We also outperforms DKM in HPatches, which we believe this indicates some trade-offs. Similar to any other works, we believe that it would not be fair to penalize our work for not including as many as 10 datasets (if we include sparse matching tasks) from different tasks. \n\nFor the small improvements in Optical flow datasets, we wish to stress that not only it should not diminish our contribution because we don't outperform them by significantly large margin, but also it should be noted that it is quite common for most (even the very recent papers) optical flow methods to also report similar extent of improvements every year (We respectfully refer the reviewer to paperswithcode. The changes in EPEs are mostly decimal points). The focus should be on how we obtain better results than PDCNet+, as we adopt similar training strategy and exceeding their performance should sufficiently demonstrate the effectiveness of the unified aggregation approach we introduce. \n\nWe hope that the merits of our approach is recognized, as we have shown that our approach achieved highly strong performance in 5 different benchmarks (with large margin.). We sincerely hope that the contributions of  our work is not ignored for not being able to achieve SOTA in tasks beyond the paper's scope, since MegaDepth, ScanNet and YFCC100M are used for evaluating sparse matching tasks in outdoor and indoor pose estimation, respectively, while our focus is on dense matching.\n\n\n\n\n[A] A simple and efficient approach for adaptive stereo matching. \n[B] Graftnet: Towards domain generalized stereo matching with a broad-spectrum and task-oriented feature."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583511145,
                "cdate": 1700583511145,
                "tmdate": 1700645682670,
                "mdate": 1700645682670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pjApO7ILGu",
            "forum": "fQHb1uZzl7",
            "replyto": "fQHb1uZzl7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission806/Reviewer_gxEA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission806/Reviewer_gxEA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to combine feature and cost aggregation to address the dense feature matching task.\n\nThe main idea is to use cost score matrix for both self- and cross-attention feature updating, to learn more discriminative features and compute better cost matrix.\n\nExperiments on some semantic and geometric matching datasets show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper is generally presented well;\n\n2) The idea of use cost score matrix for both self- and cross-attention feature updating is simple and effective;\n\n3) Experimental results are good."
                },
                "weaknesses": {
                    "value": "1)  Section 5.1. It is quite blurry for me whether previous state-of-the-art methods in Table 1 and 2 are trained on the same datasets; For example, the proposed method is trained on the DPED-CityScape-ADE and MegaDepth datasets;\n\n2) The performance of the proposed method on the optical flow (KITTI, Sintel) task is blurry for me;\n\n3) It's good to see the improved matching performance on the HPatches dataset. However, I want to see whether the improved matching would lead to better Rotation and translation estimations. \n\n4) Using cost score matrix for both self- and cross-attention feature updating is good. However, this contribution may be constrained to large overlapping ratio between images. If pairwise images have small overlapping ratio, the cost score matrix is noisy, and may provide wrong guidance for feature updating. Would you please check whether the proposed method works on some challenging image pairs from the MegaDepth dataset.\n\n5) Please show some failure cases."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636963108,
            "cdate": 1698636963108,
            "tmdate": 1699636007792,
            "mdate": 1699636007792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kp4drwfOv0",
                "forum": "fQHb1uZzl7",
                "replyto": "pjApO7ILGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer gxEA (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough reviews. Our response can be found below : \n\n> Whether Table 1 and 2 competitors are trained on the same datasets: \n\nIn Table 1, to ensure a fair and consistent comparison, all methods were trained and evaluated on the same datasets. Specifically, for evaluations on SPair-71k, all methods were trained on SPair-71k, and for evaluations on PF-PASCAL and PF-WILLOW, they were trained on PF-PASCAL. This uniform approach guarantees that the results in Table 1 are directly comparable, as each method was subjected to the same training and testing conditions.\n\nFor Table 2, we acknowledge that there was some variation in the training strategies employed. All methods, with the exception of GMFlow, were trained either on MegaDepth or a combination of DPED, ADE20K, and Cityscapes (DPED-ADE-Cityscape). Notably, except for DMP, which is trained on DPED-ADE-Cityscape, all other methods were trained on MegaDepth. While this introduces some variation, the majority of methods being trained on MegaDepth still allows for a reasonably fair comparison. However, we recognize and apologize for the potential unfairness in the comparison with GMFlow, which was not trained on MegaDepth.\n\nTo address this and to further evaluate the generalizability of our method as suggested by the reviewer, we conducted additional evaluations in the context of optical flow. For these evaluations, we utilized the KITTI and SINTEL datasets. Aligning with the practices of GLU-Net, PDC-Net, and GoCOR, our method was not trained on the Chair and Things datasets but rather on MegaDepth. The results of these additional evaluations are presented below:\n\n| Methods  | KITTI-2015 | SINTEL (Final) |\n|----------|:----------:|-------:|\n| RAFT     |    5.32    |   2.69 |\n| PDC-Net  | 5.40       | 4.54   |\n| PDC-Net+ |    4.53    |      - |\n| GMFlow   | 7.77       | 3.44   |\n| Ours     | 4.39       | 4.16   |\n\nFrom the results, while ours outperforms others in KITTI-2015, it is observed that ours achieves lower AEPE than existing flow methods. However, it should be noted that when compared to PDC-Net and PDC-Net+, ours consistently performs better, which validates the effectiveness of our approach in optical flow task as well. We acknowledge that newer, more sophisticated technical designs may emerge in the future, but we wish to emphasize that while numerous architectural designs, e.g., convex upsampling, uncertainty module or GOCor-like correlation optimization scheme, could potentially be included and enhance the synergy between feature aggregation and cost aggregation, our framework works as a stepping stone that stands out for its simplicity and effectiveness. \n\nWe hope this additional information and the results of our extended evaluations address the concerns raised and provide a more complete understanding of the capabilities of our method.\n\n> Optical flow transferability.\n\nWe provide quantitative results above."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142756362,
                "cdate": 1700142756362,
                "tmdate": 1700149734956,
                "mdate": 1700149734956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Xt9WHU1PA",
                "forum": "fQHb1uZzl7",
                "replyto": "pjApO7ILGu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer gxEA (2/2)"
                    },
                    "comment": {
                        "value": "> Pose estimation evaluation\n\nFollowing reviewer's suggestion, we perform additional evaluation on pose estimation. Specifically, we use YFCC100M and ScanNet. The results are shown below:\n\n| Methods            | 5$\\degree$ | 10$\\degree$ | 15$\\degree$ |\n|--------------------|:----------:|------------:|------------:|\n| SP+SG (CVPR'19)    |    38.7    |   59.13 | 75.8        |\n| LOFTR (CVPR'21)    |    42.4    |        62.5 | 77.3        |\n| PDCNet+ (TPAMI'23) |    35.51   |       58.08 | 74.50       |\n| Ours               | 37.62      | 59.83       | 73.22       |\n\n| Methods            | 5$\\degree$ | 10$\\degree$ | 15$\\degree$ |\n|--------------------|:----------:|------------:|------------:|\n| SP+SG (CVPR'19)    |    16.16   |       33.81 | 51.84       |\n| LOFTR (CVPR'21)    |    16.88   |   62.533.62 | 77.350.62   |\n| 3DG-STFM (ECCV'22) | 23.6       | 43.6        | 61.2        |\n| PDCNet+ (TPAMI'23) |    20.25   |       39.37 | 57.13       |\n| Ours               | 20.23      | 39.21       | 57.77       |\n\nThe upper table is YFCC and the below is ScanNet.As shown, although it surpasses PDCNet and others, it is apparent that our method is not  state-of-the-art. Nevertheless, we wish to highlight that our formulation, which  aims at  dense matching, should be separated from sparse matching (feature matching) works. A pervasive challenge in dense matching methods is their requirement to extract all matches between views. This challenge often results in many dense matching networks facing difficulties in sparse or semi-sparse methods for geometry estimation,  which is shared by many other works. Our framework is also not immune to this limitation. A specific area where our current architecture falls short is in its approach to handling confidence scores. While our cost volumes do encode a confidence score for each tentative match, a recent study like Lightglue has shown the benefits of explicitly estimating these confidence scores directly from the network, particularly for sparse matching tasks. This explicit estimation technique enhances performance in challenging scenarios, such as those involving occlusions or uncertain regions. Through empirical experiments, we've found that our model struggles more in these situations, largely attributed to the absence of a dedicated confidence estimation module.   To provide a complete understanding of our framework's capabilities and limitations, we will introduce a new section in our paper dedicated to discussing this limitation. \n\nMoreover, we wish to stress that the true essence and value of our work lie in the comprehensive and thorough investigations we have conducted into the relationships and synergies between feature aggregation and cost aggregation, as appreciated by the reviewer HY22, especially in the context of dense matching tasks. Our research goes beyond merely presenting a new technical design for state-of-the-art performance. Instead, it offers a deeper understanding of the underlying principles and interactions within these aggregation methodologies. This in-depth exploration, we believe, provides a more substantial and lasting contribution to the field.\n\n> Whether the proposed method works on minimal overlapping image pairs (MegaDepth Evaluation)\n\nWe agree that if the cost score matrix is noisy, it may provide wrong guidance for feature updating. However, as shown in qualitative results on semantic matching datasets in the supplementary material, our method performs quite well given extreme scale differences, and as there are numerous image pairs with extreme geometric scenarios as well as additional challenges like background clutters and intra-class variations, we may assume that our method is quite competitive. Nevertheless, following the reviewer's suggestion, we also try evaluating our method on the MegaDepth dataset and the results are summarized below:  \n\n\n| Methods                    | 1px   | 3px   | 5px   |  \n|----------------------------|-------|-------|-------|\n| PDC-Net (D) (CVPR 2021)    | 68.95 | 84.07 | 85.92 |  \n| PDC-Net (MS) (CVPR 2021)   | 71.81 | 89.36 | 91.18 |  \n| PDC-Net+ (H) (TPAMI 2023)  | 73.9  | 89.21 | 90.48 |  \n| Ours                       | 73.54 | 91.71 | 92.99 |  \n\n\nWhile ours achieves competitive performance, we agree that ours may struggle when image pairs with large geometric deformations are given. We will be sure to add the limitations and future directions of our method in the manuscript.\n\n> Failure cases\n\nWe will visualize failure cases for both geometric and semantic matching and include the limitation discussed above as well. We appreciate the reviewer's comment."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142823660,
                "cdate": 1700142823660,
                "tmdate": 1700146548169,
                "mdate": 1700146548169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VZu8hjoEUk",
            "forum": "fQHb1uZzl7",
            "replyto": "fQHb1uZzl7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission806/Reviewer_4EyK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission806/Reviewer_4EyK"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new vision transformer architecture to conduct feature aggregation and cost aggregation for dense matching tasks. The authors show distinct characteristics of feature aggregation and cost aggregation. and use self- and cross-attention mechanisms to unify the feature and cost aggregation. They validate the effectiveness of the proposed method with semantic matching and geometry matching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea to unify feature aggregation and cost aggregation is interesting. It compensates for the lack of semantic information in cost representation and helps to drive the features in each image to become more compatible with others.\n2. They conduct extensive experiments on semantic matching and geometry matching to validate the effectiveness of the proposed method UFC. UFC can improve the matching performance. And they provide step-by-step ablations of each component.\n3. The paper is well-organized and easy to understand. The authors visualize the changes in feature maps and cost volumes, which helps understand how their method works. I see that feature aggregation can preserve semantic information and geometry structure and the cost aggregation reduces the noise in cost volumes."
                },
                "weaknesses": {
                    "value": "1. In visualization results Figure 2, features with integrative aggregation methodology preserve the semantic information. However, it seems the proposed method damages the local discriminative ability of features."
                },
                "questions": {
                    "value": "1. The local discriminative ability of features is also important for dense matching tasks. I would like to see an analysis of whether the proposed method causes damage in this perspective or whether this issue can be avoided in some design of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission806/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission806/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission806/Reviewer_4EyK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732767371,
            "cdate": 1698732767371,
            "tmdate": 1699636007716,
            "mdate": 1699636007716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lJUkJno5JK",
                "forum": "fQHb1uZzl7",
                "replyto": "VZu8hjoEUk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 4EyK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the reviews. Our response is shown below : \n\n> An analysis of the local discriminative ability of features.\n\nOne of the goals of our proposed aggregation is also to synthesize and incorporate broader contextual information. This often involves balancing local feature details with more global, semantic information. While the process enhances the overall semantic understanding of the image, it can sometimes appear to diminish the acuity of local features. This is not necessarily a flaw but rather a trade-off, aiming for a more holistic understanding of the image data. As demonstrated in Fig. 4 of the supplementary material, top row, where image pairs have extreme geometric deformation and repetitive patterns, our framework can warp all pixels to the corresponding locations unlike others. This shows our framework does not necessarily experience such damage."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142566951,
                "cdate": 1700142566951,
                "tmdate": 1700142610673,
                "mdate": 1700142610673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iPnM5osvMB",
            "forum": "fQHb1uZzl7",
            "replyto": "fQHb1uZzl7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission806/Reviewer_96VU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission806/Reviewer_96VU"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an integrative feature and cost aggregation modules in a CNN architecture for a semantic correspondence task. The introduced module cleans up noisy matches in the cost volume and thus improves the matching accuracy. The paper demonstrates better accuracy on semantic matching and geometric matching tasks by using their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good results\n\n  The paper achieves good accuracy on both semantic and geometric matching tasks (Table 1 and 2). It demonstrates the effectiveness of the proposed aggregation modules.\n\n- Detail analysis\n\n  The paper provides a sufficient amount of analysis. Fig. 3 visualizes the qualitative comparison of the proposed modules (from (f) to (h)). Further, the ablation study (Table 3 and 4) validates the proposed ideas."
                },
                "weaknesses": {
                    "value": "Despite the good accuracy on both tasks, there are concerns about novelty/contributions.\n\n- Existing ideas in other literature\n\n  Similar ideas on feature and cost volume aggregation have been demonstrated in other literature such as stereo matching [a,b] and optical flow estimation [c]. Actually the related work section (Sec. 2) summarizes those relevant papers very well. Compared to the existing solutions, what would be the new technical design (in self-/cross-attention) of the proposed module, except for applying it to semantic & geometric matching problems? Can the newer technical design from the proposed module also benefit other tasks that use cost volume, eg., stereo matching, optical flow, scene flow, etc. ?\n\n   [a] Attention-Aware Feature Aggregation for Real-time Stereo Matching on Edge Devices, ACCV 2020\n   \n   [b] Attention Concatenation Volume for Accurate and Efficient Stereo Matching, CVPR 2022\n   \n   [c] GMFlow: Learning Optical Flow via Global Matching, CVPR 2022\n\n- Limitation\n\n  Discussion on the limitation is missing. What would be the limitation of the method or unsolved problems?\n\n\n- Can the paper provide more qualitative examples and discuss where the gain mainly originates?\n\n  Table 4 shows the accuracy improvement by adding more components. I am wondering if the paper can also include some qualitative examples and discuss where the gain mainly originates, such as resolving some particular matching ambiguity. It would be great if the paper can provide more insights related to its improvement."
                },
                "questions": {
                    "value": "- Increase of learnable parameters\n\n  How many number of learnable parameters account for the new module (i.e., integrative feature and cost aggregation module)? How significant are they compared to the number of parameters of the entire network (15.5M)? Probably it's also good to include an extra column in Table 3 and 4 for the number of network parameters.\n\n- Resolution of the cost volume\n\n  What's the resolution of the cost volume (saying the input image resolution is HxW)? At each level the features are upsampled ($D^{l}_s$), but how can the resolution of the cost volume ($C^{l})$ remain the same over different pyramid levels? Is there any reason to fix the resolution of the cost volume?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812465428,
            "cdate": 1698812465428,
            "tmdate": 1699636007642,
            "mdate": 1699636007642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sMLynCutQG",
                "forum": "fQHb1uZzl7",
                "replyto": "iPnM5osvMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 96VU (1/3)"
                    },
                    "comment": {
                        "value": "We highly appreciate for the reviewer's constructive comments. Our response is shown below : \n\n> What would be the key component to benefit the tasks? And would it also benefit optical flow?\n\nThe key technical design to benefit the task is the capturing of synergy between features, cost volumes, and their aggregations. While our approach to achieve this is indeed by incorporating self-attention and cross-attention mechanisms for effective aggregation, it's important to note that these are part of a suite of carefully considered technical designs. These designs are meticulously crafted to meet three key objectives: minimizing computational resources (achieved through Conv4D operations and maintaining a fixed resolution of the cost volume), maximizing performance (enhanced by residual connections among aggregated costs for training stability), and fostering synergy between feature aggregation and cost aggregation (via our innovative attention designs).\n\nHowever, we contend that these technical aspects, while significant, constitute a smaller part of our overall contribution. The true essence and value of our work lie in the comprehensive and thorough investigations we have conducted into the relationships and synergies between feature aggregation and cost aggregation, especially in the context of dense matching tasks. Our research goes beyond merely presenting a new technical design for state-of-the-art performance. Instead, it offers a deeper understanding of the underlying principles and interactions within these aggregation methodologies, as appreciated by the reviewer HY22. This in-depth exploration, we believe, provides a more substantial and lasting contribution to the field.\n\nThis general approach, as the reviewer kindly suggested to show, is not constrained to geometric and semantic and it can also be evaluated  on other tasks, *e.g.,* optical flow, to further demonstrate the generalizability of our framework.  The results are shown below:\n\n| Methods  | KITTI-2015 | SINTEL (Final) |\n|----------|:----------:|-------:|\n| RAFT     |    5.32    |   2.69 |\n| PDC-Net  | 5.40       | 4.54   |\n| PDC-Net+ |    4.53    |      - |\n| GMFlow   | 7.77       | 3.44   |\n| Ours     | 4.39       | 4.16   |\n\nFrom the results, while ours outperforms others in KITTI-2015, it is observed that ours achieves lower AEPE than existing flow methods. However, it should be noted that when compared to PDC-Net and PDC-Net+, ours consistently performs better, which validates the effectiveness of our approach in optical flow task as well. We acknowledge that newer, more sophisticated technical designs may emerge in the future, but we wish to emphasize that while numerous architectural designs, e.g., convex upsampling, uncertainty module or GOCor-like correlation optimization scheme, could potentially be included and enhance the synergy between feature aggregation and cost aggregation, our framework works as a stepping stone that stands out for its simplicity and effectiveness. \n\n\nWe highly appreciate the reviewer's constructive comment, and we will integrate the results into the final manuscript.\n\n> Limitations\n\nA pervasive challenge in dense matching methods is their requirement to extract all matches between views. This challenge often results in many dense matching networks facing difficulties in sparse or semi-sparse methods for geometry estimation. Our framework is not immune to this limitation. A specific area where our current architecture falls short is in its approach to handling confidence scores. While our cost volumes do encode a confidence score for each tentative match, a recent study like Lightglue has shown the benefits of explicitly estimating these confidence scores directly from the network, particularly for sparse matching tasks. This explicit estimation technique enhances performance in challenging scenarios, such as those involving occlusions or uncertain regions. Through empirical experiments, we've found that our model can struggle in these situations, largely due to the absence of a dedicated confidence estimation module.  To provide a complete understanding of our framework's capabilities and limitations, we will introduce a new section into our paper dedicated to discussing this limitation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142405885,
                "cdate": 1700142405885,
                "tmdate": 1700146156699,
                "mdate": 1700146156699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qw2eWrujNs",
                "forum": "fQHb1uZzl7",
                "replyto": "iPnM5osvMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 96VU (2/3)"
                    },
                    "comment": {
                        "value": "> Qualitative comparisons of Table 4.\n\n\nIn response to the suggestion of including qualitative comparisons, we recognize the importance of such visualizations in complementing our quantitative results, as presented in Table 4. However, the qualitative results as the performance improves would primarily manifest as minor adjustments in the accuracy of warped keypoints or pixels. These changes, while significant in a quantitative sense, may not be visibly distinguishable. However, as an alternative, we present PCA visualization as in Fig.9 and Fig.10 in the supplementary material, where we provide more examples of how our method works. These visualizations are designed to provide an intuitive understanding of how our model achieves its performance gains. They offer a clear illustration of the internal workings and improvements brought about by our framework. For example, if we look at (I-IV), they semantically match with the change from raw features or cost volume to those that are aggregated, which are visualized in Fig.9 and Fig.10 in the supplementary material, while the rest (V-VI) do not provide much intuition but rather are similar to the already presented visualizations with being higher resolution (meaning more fine details). \n\nNevertheless, to provide additional information for better understanding, we revised the supplementary material and included Fig.11, where we show PCA visualization comparisons of the each variant introduced in the component ablation table. The caption is colored red to draw attention to the change. As the comparison shows, while there exist apparent differences between (I) and (IV), we acknowledge that the differences are not visually apparent as we add the components from (II) to (IV). This is because the resolutions of features are defined in the coarsest level, which makes it little bit hard for us to interpret the visualization. Nevertheless, we believe that the differences present in the visualizations contributed to the improvements and that we provide more intuitive visualizations in Fig. 9 and Fig. 10. \n\n> It would be good to include the number of learnable parameters for Table 3 and Table 4.\n\nWe appreciate the reviewer's suggestion. A small issue is that we intentionally adjusted all variants' number of learnable parameters to be similar for the experiments in Table 3 and 4 as mentioned in section 5.5, but we agree that it would be more informative to include the number of learnable parameters for each module, i.e., integrative self-attention and cross-attention, in our architecture.   The table below shows the comparison. \n\n|                                                    | Number of params |\n|----------------------------------------------------|:----------------:|\n| l = 1, integrative self-attention                  |      1612896     |\n| l = 1, cross-attention with matching distribution  | 308576           |\n| l = 2, integrative self-attention                  |      2511968     |\n| l = 2, cross-attention with matching distribution  | 683104           |\n| l = 3, integrative self-attention                  | 6513248          |\n| l = 3, cross-attention with matching distribution  | 2692960          |\n| l=1,2,3 cross-attention with matching distribution | 3684640          |\n| l=1,2,3 integrative self-attention                 | 10638112         |\n| Total attention params                             | 14.3 M           |\n| Total params                                       | 14.5 M           |\n\nNote that we also include similar ablation study in Table 2 of the supplementary material, which shows the comparison by varying depth of the transformer layers. We will include this in the supplementary material, as a separate table."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142479549,
                "cdate": 1700142479549,
                "tmdate": 1700145468404,
                "mdate": 1700145468404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uHWPYgeNII",
                "forum": "fQHb1uZzl7",
                "replyto": "iPnM5osvMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 96VU (3/3)"
                    },
                    "comment": {
                        "value": "> Resolution of cost volumes at each level\n\nWe apologize for any confusion caused by our initial explanations and appreciate the opportunity to clarify. In our architecture, as depicted in the upper branch of Figure 5, the cost volumes at each level are indeed fixed to the coarsest resolution. This specification applies specifically to the region labeled as \"Cost Volume,\" where Conv4D operations are performed over the output cost volumes resulting from both self- and cross-attention mechanisms.\n\nAdditionally, within the aggregation modules illustrated in Figure 4, the dimensions of the cost volumes are maintained to be consistent with those of the feature maps. This design choice ensures that the cost volumes and feature maps are compatible for effective integration within these modules.\n\nThe rationale behind fixing the cost volume dimensions to the coarsest resolution in certain parts of the network, particularly for the residual connections, is driven by memory efficiency considerations. Since cost volumes inherently require more memory to store compared to feature maps, reducing their dimensions in specific regions of the network helps in managing the overall memory usage more effectively. This approach strikes a balance between maintaining the integrity and effectiveness of the cost volume information while optimizing the memory demands of the network.\n\nWe hope this clarification addresses the concerns and provides a clearer understanding of our architectural choices and their underlying motivations. If there are further questions or aspects that require additional explanation, we are more than willing to provide the necessary information."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142512836,
                "cdate": 1700142512836,
                "tmdate": 1700142512836,
                "mdate": 1700142512836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aWjF0xzbkH",
                "forum": "fQHb1uZzl7",
                "replyto": "uHWPYgeNII",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Reviewer_96VU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Reviewer_96VU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed responses! They resolved most of my concerns.\n\nOne quick question: why do the visualized PCA features in Fig 10 and 11 in the supplemental seem to show distinctive sparse blobs, rather than dense smoothed visualization? If the method provides distinctive (or discriminative) features only for certain areas of the objects, can this method be suitable for the dense correspondence task? It would be also interesting to see a visualization of a dense matching field (or optical flow field) of the examples in Fig 10 and 11 in the supplemental."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723444513,
                "cdate": 1700723444513,
                "tmdate": 1700723444513,
                "mdate": 1700723444513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ILTzSNpSVn",
                "forum": "fQHb1uZzl7",
                "replyto": "iPnM5osvMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission806/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Quick Reply"
                    },
                    "comment": {
                        "value": "Thanks for the reply and we are glad to find that most of the concerns are resolved!\n\nBefore answering your question, we first would like to ask if the reviewer wants to see PCA visualizations of feature maps (H x W x 3) for geometric matching or  the flow map (H x W x 2) itself. \n\nFor the question, we can easily answer this, since it's because of the way visualize them by PCA, and the distinctive sparse blobs are the 3 principal components that represent the most distinctive ones. Other components represent the other regions well, but one thing is that they just are not very intuitive. We have a few PCA visualizations for geometric matching datasets ready, and if this is what the reviewer wants to see, we can quickly revise the supplementary material. Note that these were also not very intuitive, visualizing some horizontal or vertical lines (we hypothesize that they indicate flows). \n\nPlease let us know and we will get back to you as soon as possible!"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725081283,
                "cdate": 1700725081283,
                "tmdate": 1700725599531,
                "mdate": 1700725599531,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]