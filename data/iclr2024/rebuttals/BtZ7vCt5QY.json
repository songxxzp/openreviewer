[
    {
        "title": "Causal-StoNet: Causal Inference for High-Dimensional Complex Data"
    },
    {
        "review": {
            "id": "KcNygeAI5k",
            "forum": "BtZ7vCt5QY",
            "replyto": "BtZ7vCt5QY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_vAHH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_vAHH"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an algorithm, CausalStoNet, which aims to give accurate causal inferences for systems of up to 100 variables (after variable reduction) under fairly relaxed nonlinearity conditions. The method is based on deep learning ideas."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This was a good paper, I thought. The applications were fruitful, and the explanation of the theory was clear. It was well-situated in the literature, and useful and correct extensions of the method were proposed."
                },
                "weaknesses": {
                    "value": "Several methods are compared, but I don't see a discussion of looking for the best available methods. Are these methods in Table 2 the best available methods? See also my questions below."
                },
                "questions": {
                    "value": "1.\tI guess, in my mind, it does very little good to say a method can deal with high dimensions without saying how high. There are two examples, one for 43 variables and another for 100 (genome reduced to this). Some may not consider this to be high-dimensional, so it would be better to say up front what dimension one hopes to achieve with the method (after variable reduction is done).\n2.\tAlso, it\u2019s important to say what density such models can attain. It\u2019s possible with some very high-dimensional data that the models may, in fact, be very dense, a situation that can be addressed currently in the linear Gaussian or non-Gaussian case. However, this hasn\u2019t been addressed to my knowledge for models with more general connection functions, a possible advantage of the method in this paper. \n3.\tThere are recent papers that address a dense searches for the linear, Gaussian case and linear, non-Gaussian cases. The secret of some of these papers is to relax the Faithfulness condition, something which does allow some nonlinear functions to be addressed in a linear framework. The secret of others (like DirectLiNGAM) is to move to the linear, non-Gaussian regime. It would be really wonderful if an approach like the one in this paper could be shown to improve these algorithms (which look to be state-of-the-art) for some choices of nonlinear functions.\n4.\tAlong these lines, I think it\u2019s important when saying you\u2019re outperforming methods to include in this the relaxation of assumptions that you\u2019ve done and to compare only to other methods that relax assumptions in similar ways or that use stronger assumptions, explicitly noting this and show that with the stronger assumptions worse results are achieved. (The latter is not always the case.)\n5.\tThe restriction to binary data for some of the theory is somewhat severe since very few real datasets consist entirely of binary variables, though perhaps I misunderstand.\n6.\tI\u2019m not sure how \u201cMNR\u201d abbreviates \u201cMissing At Random\u201d; I think it must be \u201cMAR.\u201d\n\n\nNOTE: I read the response; I thought it was convincing, so I'm raising my rating. Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Reviewer_vAHH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681683850,
            "cdate": 1698681683850,
            "tmdate": 1700480418427,
            "mdate": 1700480418427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hPBGBFoiRZ",
                "forum": "BtZ7vCt5QY",
                "replyto": "KcNygeAI5k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W1: Several methods are compared, but I don't see a discussion of looking for the best available methods. Are these methods in Table 2 the best available methods? \n\nFor this paper, we mainly consider the estimation of Average Treatment Effect (ATE). For estimation of ATE, there are numerous well-adopted methods developed based on different modeling techniques. In our paper, we consider the neural-network based methods (i.e. DragonNet, X-Learner) and methods in semiparametric literature (e.g., Double Selection Estimator (DSE), Approximate Balancing Residual Estimator (ARBE), and TMLE) as baselines. \n\nSince the proposed method is based on neural networks, comparing with other neural-network based methods is essential. For semiparametric methods, DSE and ARBE are state-of-the-art methods for high-dimensional causal inference, and TMLE can be easily extended to machine learning methods in terms of nuisance parameters estimations. We summarize the performance of these methods in simulation studies and ACIC dataset in Table 1. The methods listed in Table 2 are state-of-the art methods for estimation of Conditional Average Treatment Effect (CATE), most of which are also neural network based. Although CATE estimation is not our main focus, we still listed the comparison of CATE estimation just to highlight the existing neural network-based methods on CATE estimation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338873400,
                "cdate": 1700338873400,
                "tmdate": 1700346161105,
                "mdate": 1700346161105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OxWbnkgBIt",
                "forum": "BtZ7vCt5QY",
                "replyto": "KcNygeAI5k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Q1: It would be better to say up front what dimension one hopes to achieve with the method (after variable reduction is done)\n\nIn the rebuttal, we have tried a high-dimensional example with $p=1000$ and $n=800, 1500, 3000$. Please see the results in the global response. \nThe proposed method worked for all cases and outperformed the baselines.\n\n## Q2: Also, it\u2019s important to say what density such models can attain. \n\nWe added a simulated experiment for a function with nonlinear outcome function and nonlinear treatment effect function, for which the neural network can be dense as the true functions are not exact neural network functions. Please see the results in the global response. The proposed method worked for all cases and outperformed the baselines.\n\n## Q3: Connection with causal graph discovery \n\nThis is a thoughtful question. Indeed, the sparse DNN has been used in the literature for learning causal graphs via a double regression method, see Liang and Liang (2022)[1]. The method proposed in this paper can be equally applied there.  \n\n[1] Liang, S. and Liang, F. (2022) A Double Regression Method for Graphical Modeling of\nHigh-dimensional Nonlinear and Non-Gaussian Data. Statistics and Its Interface, in press (see also arXiv:2212.04585\n\n## Q4: \n\nWe will follow this suggestion to state the performance of the algorithm in revising the paper.\n\n## Q5 & Q6: \n\nWe are sorry for the misunderstanding. In this paper, we just assume the treatment variable is binary, while leaving the outcome and explanatory variables unrestricted. Furthermore, we have discussed the extensions of the binary treatment variable to multi-level or continuous treatment variables."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339104966,
                "cdate": 1700339104966,
                "tmdate": 1700346875244,
                "mdate": 1700346875244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PgCFuXqhjX",
                "forum": "BtZ7vCt5QY",
                "replyto": "OxWbnkgBIt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_vAHH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_vAHH"
                ],
                "content": {
                    "title": {
                        "value": "Convincing response."
                    },
                    "comment": {
                        "value": "I thought this was a convincing response, thanks."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480324351,
                "cdate": 1700480324351,
                "tmdate": 1700480324351,
                "mdate": 1700480324351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qd5B2EP4fp",
            "forum": "BtZ7vCt5QY",
            "replyto": "BtZ7vCt5QY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_GhoU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_GhoU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a deep-learning-based causal inference method, that assumes the nuisance parameters are nonlinear high-dimensional models, fit by a stochastic neural network with sparsity-pursuing properties. This paper also gives theoretical guarantees, building upon a series of papers by Liang and colleagues on Bayesian neural nets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tries to address an important problem in causal inference, by taking the nuisance parameters to be high-dimensional sparse nonlinear models. State-of-the-art deep learning methods are used, e.g. stochastic/Bayesian sparsity-pursuing neural nets, to address this challenge.\n\nThe theoretical results look sound, building upon a series of earlier works on statistical guarantees of Bayesian neural nets by Liang and colleagues."
                },
                "weaknesses": {
                    "value": "1. Even though I selected ``good'' in Presentation, I believe the exposition can nonetheless be significantly improved. For example, some of the key assumptions and elements are delayed to Appendix so the flow seems a bit broken, in particular in the first several sections. I strongly recommend that the authors consider revising their manuscript to make the flow smoother.\n\n2. In the simulation, this paper considered scenarios with p=100/200 and n=10000, which do not seem to be a very high-dimensional regime. What would be the performance if we further increase the covariate dimension p or decrease the sample size n?\n\n3. Missing references: doubly-robustness should be traced back to Robins et al. 1994 JASA. Also, Farrell et al. was cited twice in the paper (one arxiv version, one joe published version). A recent paper led by Xiaohong Chen and colleagues (Chen, Liu, Ma, Zhang, to appear in JoE) also addresses a similar problem, though their nuisance models are slightly different (Barron space). This paper should also be cited.\n\n4. Theory-practice gap: As we know, almost all theoretical works on deep learning do not reflect practice. For instance, sparse neural nets are generally difficult to fit, as persuasively argued in Farrell, Liang, and Misra ECTA 2021. The authors are recommended to comment on this, for readers to better understand what are the key elements that allow Causal-Stonet to learn the sparse neural nets.\n\n5. In causal inference, often times $x$ has clear scientific meaning. For sparse models, I would imagine that the input layer sparsity seems to be more important. If putting sparsity in output layer, this is essentially saying that there is some sparse nonlinear representation of the input. By assuming sparse neural nets, however, this is saying that the nonlinear representation itself is also in some sense sparse. Is it really aligned with our view of the real world or is it more like a contrived modeling assumption? I hope that the authors could further comment on the modeling philosophy adopted in this paper."
                },
                "questions": {
                    "value": "1. Do the theoretical results rely on equation (6)? If so, then the theoretical results seem to go against the conventional wisdom in the deep learning literature, that is, the neural parameters themselves are not scientifically meaningful so it is not that important to learn the neural net parameters.\n\n2. In theory, the propensity model and outcome model are both sparse models with sparsity levels lower than n^{3/16}. In linear models, the sparsity allowed is n^{1 / 2} up to log factors. Could the authors provide a heuristic explanation on the rate n^{3 / 16}?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Reviewer_GhoU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714244857,
            "cdate": 1698714244857,
            "tmdate": 1699636366263,
            "mdate": 1699636366263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NEtj7gu311",
                "forum": "BtZ7vCt5QY",
                "replyto": "Qd5B2EP4fp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W1: I believe the exposition can nonetheless be significantly improved. \n\n\nWe will follow your suggestion to move the key assumptions and elements into the main text, ensuring a smooth flow. \n\n\n## W2: What would be the performance if we further increase the covariate dimension p or decrease the sample size n?\n\nPlease refer to the experiment results in the global response, where we considered the cases with $p=1000$ and $n=800$, 1500 and 3000. The proposed method worked for all cases and outperformed the baselines.  \n\n## W3: Missing references\n\nThe references will be updated as suggested: Cite Robins et al. 1994 [1] for doubly-robustness, remove the arXiv paper Farrell et al. 2018, and add Chen et al. 2024 [2]. \n\n[1] Robins, J.M., Rotnitzky, A., and Zhao, L.P. (1994) Estimation of regression coefficients when some regressors are not always observed. J. Am. Stat. Assoc., 89, 846-866. \n\n[2] Chen, X., Liu, Y., Ma, S., and Zhang, Z. (2024) Causal inference of general treatment effects using neural networks with a diverging number of confounders, Journal of Econometrics, 238, 105555\n\n## W4: Theory-practice gap\n\nTricks for efficiently training sparse DNNs have been discussed and developed in Sun et al. [1,2]. Specifically, Sun et al. [1] recommended starting with an overparameterized DNN, while Sun et al. [2] further proposed a prior-annealing method. \nThis method involves gradually incorporating the prior onto an overparameterized DNN in an annealing manner and is potentially immune to local traps. In this paper, we adopt the approach suggested by Sun et al. [1], training the sparse StoNet by initially employing an overparameterized model.\n\n[1] Y. Sun, Q. Song, and F. Liang. Consistent sparse deep learning: Theory and computation. Journal of the American Statistical Association, 117(540):1981\u20131995, 2022.\n\n[2] Y. Sun, W. Xiong, and F. Liang. Sparse deep learning: A new framework immune to local traps and miscalibration. NeurIPS 2021, 2021.\n\n## W5: Modeling philosophy \n\nThank you for your thoughtful question. We address this query from two perspectives. In biological neural networks, as discussed in [1], a wealth of empirical evidence suggests that neurons represent information using sparse distributed patterns of activity. Concerning artificial DNNs, as shown in [2] based on the random matrix theory, the stochastic gradient-based training process results in an implicit heavy-tailed self-regularization in their weights, i.e., achieving an essentially sparse representation even without a sparse penalty used during the training process. \nThe proposed work makes this sparse pattern of connection weights more explicit and validates the downstream inference.\n\n[1]  S. Ahmad and J. Hawkins (2016) How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites, ArXiv:1601.00720.\n\n[2] C.H. Martin and M.W. Mahoney (2021). Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning, J. Mach. Learn. Res., 22, 165:1-73."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338542159,
                "cdate": 1700338542159,
                "tmdate": 1700345796630,
                "mdate": 1700345796630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wXrc3p0YUR",
                "forum": "BtZ7vCt5QY",
                "replyto": "Qd5B2EP4fp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Q1: Identifiability of neural network parameters.\n\nIndeed, it is known that the DNN model is generally nonidentifiable due to the symmetry of the network structure. For example, the approximation of the neural networks can be invariant if one permutes the orders of certain hidden nodes, simultaneously changes the signs of certain weights and biases if tanh is used as the activation function, or re-scales certain weights and bias if Relu is used as the activation function. In this paper, following Sun et al. (2022) [1], we consider a set of networks, denoted by $\\boldsymbol{\\Omega}$, for which each element can be viewed as a class of equivalent DNN models. Then we define an operator $\\nu(\\boldsymbol{\\gamma},\\boldsymbol{\\theta}) \\in \\boldsymbol{\\Omega}$ that maps any neural network to $\\boldsymbol{\\Omega}$ via appropriate transformations such as nodes permutation, sign changes, weight rescaling. To define $\\boldsymbol{\\Omega}$ usually comes with constraints on the neural network, and Liang et al. (2018) [2] gives an example of such constraints. However, even if we relax this identifiability assumption on network parameters, the validity of StoNet as an approximator to DNN can still be justified, since StoNet and DNN have asymptoticlaly equivalent loss functions (see equation (5)).\n\n[1] Y. Sun, Q. Song, and F. Liang. Consistent sparse deep learning: Theory and computation.\nJournal of the American Statistical Association, 117(540):1981\u20131995, 2022.\n\n[2] Liang, F., Li, Q., and Zhou, L. (2018), \u201cBayesian neural networks for selection of drug sensitive genes,\u201d Journal of the American Statistical Association, 113, 955\u2013972.\n\n## Q2: Could the authors provide a heuristic explanation on the rate $O(n^{3/16})$?\n\nAgain, this is a thoughtful question. A heuristic explanation is that the size of the true DNN model is allowed to increase with the sample size $n$ at the rate $O(n^{3/16})$, while ensuring the validity of inference for the treatment effect.  \n\nWe note that this rate is derived based on the Laplace approximation error \nfor high-dimensional Bayesian sparse nonlinear models, as presented in Theorem 2.3 by Sun et al. 2022 [1]. \nSpecifically, Sun et al. (2023) showed that the Laplace approximation error is of $O(r_n^4/n)$, \na general result for the MAP-based posterior mean approximation in Bayesian sparse nonlinear models. \n\nWe believe that this rate can be improved \nunder the frequentist framework, but the proof appears to be challenging. Refer to our discussions about Farrell et al. (2021) [2] in the  ``Related Works'' paragraph for further insights.  \n\n[1] Sun, Q. Song, and F. Liang. Consistent sparse deep learning: Theory and computation.\nJournal of the American Statistical Association, 117(540):1981\u20131995, 2022.\n\n[2] M. Farrell, Tengyuan Liang, and S. Misra. Deep neural networks for estimation and inference.\nEconometrica, 89:181\u2013213, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338705856,
                "cdate": 1700338705856,
                "tmdate": 1700346117308,
                "mdate": 1700346117308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qjIShJbwkF",
                "forum": "BtZ7vCt5QY",
                "replyto": "Qd5B2EP4fp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_GhoU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_GhoU"
                ],
                "content": {
                    "title": {
                        "value": "thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for the authors' thoughtful response. I am satisfied with the response. If there were a score 7, I would have been willing to improve the current score (6) to 7."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441714201,
                "cdate": 1700441714201,
                "tmdate": 1700441847712,
                "mdate": 1700441847712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8KfgSfX583",
            "forum": "BtZ7vCt5QY",
            "replyto": "BtZ7vCt5QY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_7o25"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4033/Reviewer_7o25"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an approach to causal inference tailored for high-dimensional complex data. This approach draws upon recent developments in deep learning techniques, including sparse deep learning and stochastic neural networks. By leveraging these techniques, the proposed approach effectively tackles both the high dimensionality and the complexity of the underlying data generation process. Moreover, it can handle scenarios with missing values in the datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method Causal-StoNet is proven to have the universal approximation ability, making it a versatile tool for modeling outcome and propensity score functions.\n\n- The paper provides a strong theoretical foundation for its approach, including proofs and mathematical support, enhancing its reliability."
                },
                "weaknesses": {
                    "value": "1/ It seems to me that the proposed method is a direct application of stochastic neural networks. Please clearly highlight technical innovation of the proposed method.\n\n2/ Can the authors explain which component of their method make it works well for high-dimensional confounder X? Why the existing methods such as: CEVAE, CFR Net are not possible to deal with high dimensions of X?\n\n3/ It seems to me that many concepts and notations in the paper are unexplained. For example, why do we choose $\\sigma_{0,n}^n$ to be a very small number while $\\sigma_{1,n}^2$ is relatively large? Is there any rationale for this? What is $\\pi(\\theta)$ in Eq.~9? Is it the prior?\n\n4/ Since $Y_{mis}^i$ is unobserved, how do you minimise Eq.~9?"
                },
                "questions": {
                    "value": "Please see section weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4033/Reviewer_7o25",
                        "ICLR.cc/2024/Conference/Submission4033/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807013692,
            "cdate": 1698807013692,
            "tmdate": 1700495233843,
            "mdate": 1700495233843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ArgLfBDCCg",
                "forum": "BtZ7vCt5QY",
                "replyto": "8KfgSfX583",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W1: It seems to me that the proposed method is a direct application of stochastic neural networks. Please clearly highlight technical innovation of the proposed method.\n\nThe technical innovations of this paper lie in two  aspects. First, we propose using the stochastic neural network as a general tool for modeling natural systems by incorporating middle-level observations as visible units in some hidden layers. While we acknowledge that the stochastic neural network has been proposed in the literature, our utilization of it is innovative. This approach enables the modeling of many complex systems by deep neural networks (DNNs) in a natural way, facilitating downstream inference.\n\nSecond, to ensure that Causal-StoNet can handle high-dimensional data and that the resulting statistical inference is valid, we impose a mixture Gaussian prior on each connection of the Causal-StoNet (see Equation (8)). Under some regularity conditions, we establish consistency for structure selection, outcome function estimation, and propensity score function estimation. Consequently, we establish the validity of statistical inference for the treatment effect. Without the sparsity prior, statistical inference with Causal-StoNet cannot be valid.\n\nHere, we would like to emphasize that for many neural network-based methods, justifying the validity of downstream inference can be challenging unless the dataset size is sufficiently large and the network size is relatively small. On the other hand, we are aware that deep neural networks trained with stochastic gradient-based methods possess the property of implicit heavy-tailed self-regularization (see Ref. [1]). However, since the self-regularization phenomenon cannot be rigorously measured, at least for now, establishing the validity of downstream inference can still be challenging. Our method provides a theoretical guarantee for the validity of the neural network-based causal inference.\n\n[1] C.H. Martin and M.W. Mahoney (2021). Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning,\nJ. Mach. Learn. Res., 22, 165:1-73.\n\n## W2: Can the authors explain which component of their method make it works well for high-dimensional confounder X? Why the existing methods such as: CEVAE, CFR Net are not possible to deal with high dimensions of X?\n\nAs mentioned above, the sparsity prior enables Causal-StoNet to perform effectively with high-dimensional confounders. Moreover, the resulting sparse structure ensures consistency of the estimation and the validity of the subsequent causal inference. \n\nIn contrast, both CEVAE and CFRNet utilize fully-connected neural networks, where the consistency of estimation cannot be ensured. Consequently, validating the subsequent causal inference becomes challenging. For comparison, we have applied CEVAE and CFRNet to the ACIC example, see Table 2. The comparison indicates that both methods are inferior to the proposed one for the example.\n\nReferences: Shalit, Uri, Fredrik D. Johansson, and David Sontag. \u201cEstimating individual treatment effect: generalization bounds and algorithms.\" International Conference on Machine Learning. PMLR, 2017. \n\nJohansson, Fredrik, Uri Shalit, and David Sontag. \u201cLearning representations for counterfactual inference.\u201d International conference on machine learning. PMLR, 2016.\n\n## W3: It seems to me that many concepts and notations in the paper are unexplained. For example, why do we choose $\\sigma_{0, n}^n$ to be a very small number while $\\sigma_{1, n}^n$ is relatively large? Is there any rationale for this? What is $\\pi(\\theta)$ in Eq.~9? Is it the prior?\n\nWe will double-check and clarify the notations in the revision. The restriction on the noise is needed to ensure that the Causal StoNet can have asymptotically equivalent loss function as DNN (see equation (5)), which is crucial in justifying that the Causal-SoNet is a good approximator of the underlying DNN and hence a valid universal learner for the treatment  model and the outcome model. The $\\pi(\\theta)$ in equation (9) refers to the mixture Gaussian prior.\n\n## W4: Since is $\\boldsymbol{Y_{\\text{miss}}}^i$ unobserved, how do you minimise Eq.~9?\n\nEquation (9) can be minimized by solving a mean-field equation, as given in (10), using an adaptive SGMCMC algorithm. \nThe Adaptive SGMCMC algorithm operates by iterating over the following two steps: \n\n(1) Sampling $\\boldsymbol{Y_{\\text{miss}}}^i$ from $\\pi(\\boldsymbol{Y_{\\text{miss}}}^i | \\boldsymbol{X}, A, \\boldsymbol{Y}, \\boldsymbol{\\theta})$; \n\n(2) update $\\boldsymbol{\\theta}$ based on the sampled latent variables. \n\nFor more details of the algorithm, please refer to section 3.2. Additionally, Appendix A1 provides the pseudo-code for the training algorithm of Causal-StoNet, and Appendix A6 includes a brief discussion of the convergence analysis of the algorithm."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700336039627,
                "cdate": 1700336039627,
                "tmdate": 1700345558993,
                "mdate": 1700345558993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f6mAw0Qpiq",
                "forum": "BtZ7vCt5QY",
                "replyto": "ArgLfBDCCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_7o25"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4033/Reviewer_7o25"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Many thanks to the authors for their clarifications. Integrating these insights into the paper would significantly enhance its clarity. Additionally, I appreciate the authors' inclusion of additional experimental results. With all these improvements taken into account, I would like to elevate the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495207376,
                "cdate": 1700495207376,
                "tmdate": 1700495207376,
                "mdate": 1700495207376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]