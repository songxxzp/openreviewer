[
    {
        "title": "Language Model Cascades: Token-Level Uncertainty And Beyond"
    },
    {
        "review": {
            "id": "x4Ee7XnhLg",
            "forum": "KgaBScZ4VI",
            "replyto": "KgaBScZ4VI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_Qrz5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_Qrz5"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to apply simple model cascades to structured output problems of varying length: try to first use a small model, then fall back to a larger model if it appears the small model is insufficiently confident. This task has been used to good effect on problems with simpler output spaces, such as multi-class prediction. In those settings, the log prob of the prediction can be used as a proxy for confidence; low probabilities from a small model trigger inference from a large model. However, in language generation tasks, the length can vary broadly, hence the log probabilities can vary broadly as well. The direct analog of log probabilities would be to sum the log probs of each prediction, but this has undesirable scale issues based on sequence length.\n\nThe authors propose both simple confidence estimate techniques (average, percentiles) and complex methods (learned functions of percentiles, embeddings) that lead to substantial improvements. In some settings the cascade performs better than either model alone, suggesting that the model is attaining some kind of ensemble effect."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present an accessible introduction to cascades as well as the challenges of application to language generation tasks. The methods they propose are straightforward and easy to implement, and seem to work well.\n\nThe authors evaluate several different tasks, using both simple and complex models, and present reasonable gains.\n\nThe post-hoc methods provide some interesting insights."
                },
                "weaknesses": {
                    "value": "The authors only work with a single base model: FLAN-T5. It's not clear how well these results generalize.\n\nThere are other methods of confidence estimation beyond logprobs (see, e.g. https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737/Calibrated-Interpretation-Confidence-Estimation-in) -- would like to see more analysis here."
                },
                "questions": {
                    "value": "In the \"Intermediate embeddings\" approach, only decoder representations are used. However, wouldn't it potentially be useful to characterize aspects of the input? I could see that some inputs might be more reasonable for a smaller model; others might have complexities that are more suited to a larger model. Even input length could potentially be useful. Do you have empirical experimentation here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826590123,
            "cdate": 1698826590123,
            "tmdate": 1699636670601,
            "mdate": 1699636670601,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MCLNaWaqXJ",
                "forum": "KgaBScZ4VI",
                "replyto": "x4Ee7XnhLg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Qrz5"
                    },
                    "comment": {
                        "value": "Thanks for the positive feedback and thoughtful comments.\n\n> The authors only work with a single base model: FLAN-T5. It's not clear how well these results generalize.\n\nThanks for the suggestion. Our focus on FLAN-T5 models was motivated by their versatile performance on a range of downstream tasks (owing to instruction tuning), as well as them offering models of different sizes.\n\n> There are other methods of confidence estimation beyond logprobs (see, e.g. https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737/Calibrated-Interpretation-Confidence-Estimation-in) -- would like to see more analysis here.\n\nThanks for this reference. From our reading, the paper considers *min aggregation* versus *mean aggregation* for confidence measure (Section 4.4). The paper finds that *min aggregation* leads to better calibration which we have already included in this work. \n\nWe note also that Appendix D covers some other approaches to estimating uncertainty. While some of these may be computationally prohibitive for a cascade setting, it would be of interest to see if others offer value over the information in softmax probabilities. This could be a good direction for future work (as noted in Section 5).\n\n> In the \"Intermediate embeddings\" approach, only decoder representations are used. However, wouldn't it potentially be useful to characterize aspects of the input? I could see that some inputs might be more reasonable for a smaller model; others might have complexities that are more suited to a larger model. Even input length could potentially be useful. Do you have empirical experimentation here?\n\nYes, we had experimented with both output only, as well as input and output combined representations. But, adding input representations did not seem to help. We have added results for 3 datasets with input representations also in Appendix F.3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547403723,
                "cdate": 1700547403723,
                "tmdate": 1700549945693,
                "mdate": 1700549945693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y7dNfchxxZ",
                "forum": "KgaBScZ4VI",
                "replyto": "x4Ee7XnhLg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your time and effort in giving detailed comments and feedback. We have updated our paper based on this and have answered your questions above. Please let us know if you have any further questions or comments."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639490866,
                "cdate": 1700639490866,
                "tmdate": 1700639490866,
                "mdate": 1700639490866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q1m9Bbj8Cs",
            "forum": "KgaBScZ4VI",
            "replyto": "KgaBScZ4VI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_MA2R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_MA2R"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the model cascade for generation tasks. It notices a crucial difference between cascading for classification tasks and cascading for generation task and points out that the natural extension of predicted class uncertainty to generative tasks, predicted sequence uncertainty, is biased by sequence length, leading to sub-optimal deferral decisions.\nIt then designs a deferral rule to obtain the score/confidence of the small LM and decides when to defer an input to the larger model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tIt demonstrates that simple sequence-level LM confidence measures for deferral can lead to sub-optimal cost-quality tradeoffs due to length bias.\n2.\tThe paper proposes a simple yet effective method employing the quantile of the log-likelihood to design a deferral rule. \n3.\tThe Proposal of a post-hoc deferral rule trained on quantile features and the input embeddings of both the small LM and the large LM. The extensive experiments on FLAN-T5 verify the efficacy of the method."
                },
                "weaknesses": {
                    "value": "1.\tCompared with simple averaging the log probability, the major advantage of quantile is that it reflects more about the overall log probability distribution of the sequence and is more robust to outliers. To highlight the motivation of the proposal, more evidence for the existence of the outlier is expected and the showcase in Figure 1 is not sufficient.\n2.\tIn Figure 2 and Figure 3, it seems that the best generation performance is obtained in the middle of the curve, other than the endpoint where all examples are deferred. Does this mean that sometimes smaller LM outperform larger ones?\n3.\tThe author claims that Chow-Sum is overly biased towards deferring longer predictions. However, from Figure3(a) we can obverse that when the output length (y-axis) is between 150 words to 250 words, the score of the oracle deferring strategy is smaller than the Chow-Sum, which says the opposite: the chow-sum isn\u2019t biased towards deferring longer predictions when compared with the oracle.\n4.\tAs another line of work, speculative decoding also aims at the trade-off balance between efficiency and performance and I think the authors should discuss or compare the difference between these two lines of work."
                },
                "questions": {
                    "value": "1.\tWhat is the performance of Post-Hoc-Embed-1?\n2.\tHow is the $\\Phi(x)$ computed in detail?\n3.\tWhat is the performance of the post-hoc-quantile when predicting the golden deferring label?\n4.\tIn figure2, figure3 and figure5, all figures use the deferring rate as the x-axis. I am curious about whether we could use the inference time cost as the x-axis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699026134765,
            "cdate": 1699026134765,
            "tmdate": 1699636670482,
            "mdate": 1699636670482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rIyroWmHqh",
                "forum": "KgaBScZ4VI",
                "replyto": "q1m9Bbj8Cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MA2R"
                    },
                    "comment": {
                        "value": "Thanks for the detailed feedback and insightful comments.\n\n> Compared with simple averaging the log probability, the major advantage of quantile is that it reflects more about the overall log probability distribution of the sequence and is more robust to outliers. To highlight the motivation of the proposal, more evidence for the existence of the outlier is expected and the showcase in Figure 1 is not sufficient.\n\nWe wish to emphasize that our goal in leveraging probability quantiles is not only to achieve robustness to outliers: it is to obtain a richer view of the distribution than a single summary statistic (like the sum or mean). For example, while the sum of log-probabilities may be low, if even a couple of tokens have high uncertainty, that could indicate a prediction is uncertain (per Figure 1).\n\n**Our experimental results confirm this goal is worthwhile**: by learning a post-hoc deferral rule on top of quantiles, one can significantly improve the cost-quality tradeoff compared to simply using a single summary statistic (see Table 1).\n\n> In Figure 2 and Figure 3, it seems that the best generation performance is obtained in the middle of the curve, other than the endpoint where all examples are deferred. Does this mean that sometimes smaller LM outperform larger ones?\n\nThanks for raising this: indeed, this indicates that **the small model may be superior to the larger one on certain examples**. For example, on TriviaQA, we find that on 1.1% of test examples, the FLAN-T5 Base model is correct while the FLAN-T5-Large model is incorrect. This \u201cnon-monotonicity\u201d of the performance of models of different sizes has been observed previously, e.g.,\n\nNarayan et al., Predicting on the Edge: Identifying Where a Larger Model Does Better. 2022.\nKim et al., Speculative Decoding with Big Little Decoder, 2023,\n\n> The author claims that Chow-Sum is overly biased towards deferring longer predictions. However, from Figure3(a) we can observe that when the output length (y-axis) is between 150 words to 250 words, the score of the oracle deferring strategy is smaller than the Chow-Sum, which says the opposite: the chow-sum isn\u2019t biased towards deferring longer predictions when compared with the oracle.\n\n**We believe there is a misunderstanding in the interpretation of Figure 3(a).** The x-axis denotes the score quantile for different methods. At a given score quantile (say, x=0.2), we consider the behavior of each method when the corresponding fraction of lowest scoring samples are deferred (e.g., samples with lowest 20% scores are deferred). The y-axis represents the average output length of the deferred samples. \n\nAt x=0.2, the Chow-Sum method has a y value of 250, whereas the Oracle method has a y value of around 125. So, this means that the first 20% of samples which Chow-Sum defers have an average length of 250, whereas the Oracle method\u2019s deferred samples have an average length of 125. Thus, Chow-Sum prefers to defer longer predictions.\n\nTo further illustrate this, we have also added more plots in Appendix F.1 which shows the histogram and CDFs of lengths of outputs deferred by each method at different deferral rates.  Each plot considers the samples which are deferred by each method at the mentioned deferral rate. Figure 11 and 12 shows that the Oracle method has a higher fraction of points deferred with lower prediction lengths as compared to the Chow-Sum method at different deferral rates. **These plots conclusively provide evidence that Chow-Sum is overly biased towards deferring longer predictions as compared to the Oracle method.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547325556,
                "cdate": 1700547325556,
                "tmdate": 1700549881164,
                "mdate": 1700549881164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4JYItisce",
                "forum": "KgaBScZ4VI",
                "replyto": "q1m9Bbj8Cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MA2R"
                    },
                    "comment": {
                        "value": "> As another line of work, speculative decoding also aims at the trade-off balance between efficiency and performance and I think the authors should discuss or compare the difference between these two lines of work.\n\nThanks for raising this interesting point. Indeed, the recent line of work on speculative decoding is conceptually related to cascading: both approaches involve orchestrating between a small and large model to improve inference efficiency. \n\nHowever, they have slightly different use-cases: speculative decoding critically assumes that it is feasible to use the large \u201cverifier\u201d model to score predictions from the small \u201cdrafter\u201d model. Note that such scoring needs to be done *for every test example*. However, this may not always be feasible: e.g., consider a setting where the large model has >100B parameters, which could result in a high latency even when scoring sequences. By contrast, cascading can still be applicable in such cases, since the large model needs only be invoked on samples where the deferral mechanism predicts to forward.\n\nOn the flip side, an appealing characteristic of speculative decoding is that it is provably quality-neutral. On the other hand, cascading offers a *tradeoff* between quality and cost. (Typically, it is empirically observed that the cascade can also be quality neutral at moderate deferral rate to the large model.) It is of considerable interest to study mechanisms that combine the strengths of cascading and speculative decoding; however, this would be worthy of a separate paper in itself.\n\n>  What is the performance of Post-Hoc-Embed-1?\n\nThanks for pointing this out. We did not include this in the main paper for the sake of clarity and readability. Nonetheless, here is the performance comparison, summarized via the AUC values for each dataset, and percentage improvement over random.\n| Dataset | Post-Hoc-Embed-1 AUC-DF (% improvement over random) |\n|----------|-------------------|\n|anli-r1 |0.546 (+ 8.80)|\n|anli-r2 | 0.450 (+ 2.21) |\n|anli-r3 |0.426 (+ 6.07)|\n| boolq | 0.840 (+ 2.96)|\n| imdb |0.964 (+ 1.23) |\n|lambada |0.693 (+ 2.07) |\n|mnli |0.719 (+ 11.82) |\n|piqa |0.718 (+ 2.17)|\n|squad|0.408 (+ 1.65)|\n|triviaqa |0.099 (+ 17.65)|\n|tydiqa-fi|0.333 (+ 4.47)|\n|tydiqa-id|0.242 (+ 4.62)|\n|tydiqa-sw|0.162 (+ 5.47)|\n|winogrande|0.644 (+ 1.02)|\n|wmt-de-fr| 0.404 (+ 4.23)|\n|wmt-en-fr| 0.444 (+ 3.40)|\n|wmt-fr-en| 0.618 (+ 2.99)|\n\nWe see that Post-Hoc-Embed-1 is able to improve upon Post-Hoc-Quantile and Chow-Quantile methods slightly, but is slightly inferior compared to the Post-Hoc-Embed-1+2 method. This intuitively makes sense as this has more information compared to the Post-Hoc-Quantile method but still does not include any information about model 2. We have also added this text in Section 4.3 in the draft."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547968568,
                "cdate": 1700547968568,
                "tmdate": 1700547968568,
                "mdate": 1700547968568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FVC129EFYj",
                "forum": "KgaBScZ4VI",
                "replyto": "q1m9Bbj8Cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your time and effort in giving detailed comments and feedback. We have updated our paper based on this and have answered your questions above. Please let us know if you have any further questions or comments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639578780,
                "cdate": 1700639578780,
                "tmdate": 1700639578780,
                "mdate": 1700639578780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WkGKctOlae",
            "forum": "KgaBScZ4VI",
            "replyto": "KgaBScZ4VI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel method for uncertainty estimation in large language models (LLM). The method is based on LLM cascades, where a large model predicts difficult instances and a second small model predicts easy instances. In addition, the model uses the Chow-sum and Chow-average as a rule for assigning confidence (uncertainty) to token-level outputs from the LLM cascade.  The main contributions are: i) method for token-level uncertainty estimates, and ii) application of different natural language processing (NLP) tasks. The method shows that the confidence estimates based on the output probabilities from LLMs are biased."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A principled method for uncertainty estimation in LLM (i.e. FLAN-T5).\n- Clear description of background knowledge and related work needed to understand the proposed method.  \n- The authors perform a  comprehensive comparison of the proposed method with different NLP tasks."
                },
                "weaknesses": {
                    "value": "- Motivation for the lack of comparison with other uncertainty estimation methods.\n- A possible extra contribution can be the use or discussion of the method for NLP tasks under out-of-distribution (OOD) or domain adaptation."
                },
                "questions": {
                    "value": "Please address the following questions during the rebuttal:\n\n- Please elaborate on the relation/difference of the Chow estimates with proper scoring rules (e.g. NLL, Brier score).\n- Could the proposed estimates be directly compared/evaluated to estimates from deep ensembles instead of a cascade or even jointly? (Lakshminarayanan, Balaji et al. \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.\u201d Neural Information Processing Systems (2016).)\n- For the machine translation evaluation: \n\n Is the output generated by beam search? Please speculate on the effect of the hyperparameters used on the generation, do they have an effect on the output length?\n\n Please speculate for the use of the proposed method for robustness to OOD in MT. different domains can be used to evaluate a change in distribution, is the uncertainty estimate robust to such change?\n\n- Please elaborate on the use of output probabilities from the LLMs as uncertainty estimates compared to other methods (e.g. deep enembles, MC dropout)? (e.g. Baan, Joris et al. \u201cUncertainty in Natural Language Generation: From Theory to Applications.\u201d ArXiv abs/2307.15703 (2023): n. pag.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6173/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6173/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699473187170,
            "cdate": 1699473187170,
            "tmdate": 1700560004118,
            "mdate": 1700560004118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2lTgoL0bKt",
                "forum": "KgaBScZ4VI",
                "replyto": "WkGKctOlae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WdbA"
                    },
                    "comment": {
                        "value": "Thanks for the detailed feedback and encouraging comments!\n\n> Please elaborate on the relation/difference of the Chow estimates with proper scoring rules (e.g. NLL, Brier score).\n\nThis is a good question. A proper loss (or negative proper scoring rule) is typically defined for a predicted probability *and* a ground truth label; e.g., we may compute the log-loss $-log \\hat{p}(y \\mid x)$ for a predicted distribution $\\hat{p}( \\cdot \\mid x )$ and label $y$.\n\nIn our setting, we only have the predicted distribution at inference time; the ground truth label $y$ is unknown, and so cannot be used to compute a confidence measure for deferral. With Chow\u2019s rule, we compute the maximum value of $\\hat{p}( y\u2019 \\mid x )$ over *all possible* labels $y\u2019$.\n\n> Could the proposed estimates be directly compared/evaluated to estimates from deep ensembles instead of a cascade or even jointly? (Lakshminarayanan, Balaji et al. \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.\u201d Neural Information Processing Systems (2016).)... Please elaborate on the use of output probabilities from the LLMs as uncertainty estimates compared to other methods (e.g. deep enembles, MC dropout)? (e.g. Baan, Joris et al. \u201cUncertainty in Natural Language Generation: From Theory to Applications.\u201d ArXiv abs/2307.15703 (2023): n. pag.)\n\nThanks for the interesting suggestion. First, we note that cascades and ensembles solve slightly different problems. Cascading is often used as a way of improving the prediction *efficiency*, by selectively leveraging *one* model from a given family. Ensembling is often used as a way of improving the prediction *quality*, by suitably leveraging *multiple* models from a given family.\n\nDeep ensembles are generally understood to provide more reliable uncertainty estimates than a single model. However, this comes at the expense of a higher compute cost than invoking a single model. For cascading, efficiency is paramount: such a higher cost would defeat any gains obtained by the improved uncertainty estimates. A similar concern would arise in approaches based on averaging multiple models using dropout.\n\nOne can in fact combine cascading and ensembling, per Wang et al., \u201cWisdom of Committees: An Overlooked Approach To Faster and More Accurate Models\u201d. However, such approaches have not (to our knowledge) been explored in the language model domain. We agree this could be extremely interesting for future work!\n\n> Motivation for the lack of comparison with other uncertainty estimation methods.\n\nAs discussed above, many uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network. Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.\nWe have also updated the text in Appendix D to include this discussion.\n\n> For the machine translation evaluation: Is the output generated by beam search?\n\nThe outputs were generated by greedy decoding. To verify that the results are not an artifact of this decoding strategy, we have also added results for beam search in Appendix F.4 in Figures 15, 16 and 17. The output lengths are on average lower in beam search as compared to greedy decoding. We observe that the same findings still hold in this case.\n\n> Please speculate on the effect of the hyperparameters used on the generation, do they have an effect on the output length?\n\nThanks for raising this important point. We added a plot (Figure 18) in Appendix F.4 which compares the distribution of output lengths using greedy decoding versus beam search (beam size 10). As our intuition suggests (and also pointed out by Reviewer 7G2G), we see that outputs generated by beam search have lower lengths on average as compared to the outputs generated by greedy decoding because they are less prone to get stuck in repetitions. But, as the figure suggests, there is still a considerable probability mass on longer lengths and hence, they are still likely to generate longer outputs with repetitive text. Thus, as the rest of the figures in Appendix F.4 suggest, our insights and methods still generalize to outputs generated using beam search."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547128055,
                "cdate": 1700547128055,
                "tmdate": 1700547128055,
                "mdate": 1700547128055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NuKT8khWjw",
                "forum": "KgaBScZ4VI",
                "replyto": "WkGKctOlae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WdbA"
                    },
                    "comment": {
                        "value": "> Please speculate for the use of the proposed method for robustness to OOD in MT. different domains can be used to evaluate a change in distribution, is the uncertainty estimate robust to such change?\n\nThanks for the interesting suggestion. We note that the distinction between OOD and ID data is not immediately clear in our setting of instruction tuned LMs. Specifically, we are operating in a setting where there is a single LM that is first pre-trained, and then fine-tuned on a given instruction-tuning set (the FLAN mixture for FLAN-T5). The latter implicitly contains multiple tasks, and aims to help the model generalize to new tasks.\nOne narrow definition of \u201cOOD\u201d in this setting could be based on whether or not a task is included in the instruction-tuning mixture. For example, the TyDiQA dataset is not included in the FLAN mixture. One might thus consider TyDiQA as, in a narrow sense, \u201cOOD\u201d for FLAN-T5 models. We have included results for TyDiQA in our paper. On one of the three languages for TyDiQA, our method outperforms other methods. \nFor OOD in machine translation, if the reviewer has a specific task in mind, we would be happy to consider experimenting with it."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547179935,
                "cdate": 1700547179935,
                "tmdate": 1700547179935,
                "mdate": 1700547179935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L7v499kFuO",
                "forum": "KgaBScZ4VI",
                "replyto": "2lTgoL0bKt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. I have no further comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559928919,
                "cdate": 1700559928919,
                "tmdate": 1700559928919,
                "mdate": 1700559928919,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FQLUZeQU8f",
            "forum": "KgaBScZ4VI",
            "replyto": "KgaBScZ4VI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
            ],
            "content": {
                "summary": {
                    "value": "This paper is about learning deferral rules for LM cascades -- essentially, how can you predict when to rely on a small model's output and when should you fall back to a large, more expensive model? They propose a connection to the classical problem of classification with rejection, in which a model can choose to reject classifying an instance. The optimal strategy in this case is to reject whenever the model's confidence fails to pass a threshold derived from the cost of rejection. It is easy to see how LM cascades fit into this framework: deferring to the larger model is equivalent to rejecting the output of the smaller model.\n\nHowever, it is not trivial to generalize rejection from classification to generation. In classification the model predicts only one label along with an easy-to-interpret probability score (e.g. from softmax), whereas generation entails producing variable-length *sequences* of tokens. Although each of these tokens has an associated per-token probability, aggregating them is a challenge: simply summing the logprobs causes short sequences to be rejected (this is mathematically necessary, as it is equivalent to multiplying numbers less than one), while averaging them causes *long* sequences to be rejected (this observation is interesting, and surprising to me).\n\nNoting the weaknesses of these two baselines, the authors' main contribution is to introduce alternative techniques to score the model output. The main technique, which they call Chow-Quantile, is based on sorting the per-token logprobs for an instance and then picking the value at some alpha-quantile of this, where alpha is a hyperparameter. For example, picking the 0-quantile is equivalent to scoring sequences based on the *lowest* per-token prob. They also propose various post-hoc techniques that allow classifiers to be trained on top of these quantiles.\n\nThey exhibit experiments applying their various deferral techniques to a variety of tasks, including MT and QA (and also surprisingly MNLI, which seems inapplicable because it classification, not generation). These results seem to show an advantage for using their techniques over the baselines across many deferral levels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents a simple approach that seems to be very effective. The connection to rejection in classifiers is intuitive but had not occurred to. The paper is easy to follow. The clarity of presentation convinces me that it would be easy for me to try the approach myself, either for its own utility or as a replication study. I admire the accessibility of this work."
                },
                "weaknesses": {
                    "value": "Although the paper as a whole is very clear, there are places where the experiments lack specifics (see the questions section). Additionally, there are places where the experimental set-up seems to be suboptimal: greedy decoding was used, but this is more prone to hallucination than beam search. Some of the conclusions of the experiments might be artifacts of these hallucinations. I can think specifically of these two: \n\n1) there was a negative correlation between translation quality and sequence length; was this because many of the long sequences were hallucinations? \n\n2) the Chow-average model favored longer sequences, which there is no intuitive reason for. Could it be because hallucinations often get trapped in loops where the same short phrases get repeated with high probability? This would drive up the average."
                },
                "questions": {
                    "value": "--Which WMT set was used? It needs to be identified and cited. \n\n--I don't understand how the MNLI experiments work. It is noted in Section 3.4 that MNLI is a multi-class classification problem, but the techniques proposed in this paper are not for classification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6173/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G",
                        "ICLR.cc/2024/Conference/Submission6173/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699478855157,
            "cdate": 1699478855157,
            "tmdate": 1700581910455,
            "mdate": 1700581910455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m8uCltnCUf",
                "forum": "KgaBScZ4VI",
                "replyto": "FQLUZeQU8f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7G2G"
                    },
                    "comment": {
                        "value": "Thanks for the detailed feedback and encouraging comments!\n\n> Additionally, there are places where the experimental set-up seems to be suboptimal: greedy decoding was used, but this is more prone to hallucination than beam search.\n\n**We have added cascade plots with beam search** for three datasets (TriviaQA, WMT DE->FR and WMT FR->EN; see Appendix F.4, Figures 15, 16), and a length analysis plot for WMT FR->EN (Appendix F.4, Figure 17). We used a beam size of 10 for this experiment. The output lengths are on average lower in beam search as compared to greedy decoding (as shown in Figure 18 in Appendix F.4 as suggested by reviewer WdbA) which gives more support to the reviewer\u2019s intuition that beam search is less likely to get caught in repetitive hallucinations as compared to greedy decoding but still gets affected by this problem. Thus, we observed similar conclusions to our main results. Thus, our results also extend to decoding with beam search, and are not purely an artifact of greedy decoding.\n\n> the Chow-average model favored longer sequences, which there is no intuitive reason for. Could it be because hallucinations often get trapped in loops where the same short phrases get repeated with high probability? This would drive up the average.\n\nWe believe the reviewer has the correct intuition. As noted, Chow-average preferred keeping **longer** sequences and deferring **shorter** sequences (see (0, 0.2) quantile score range of Figure 3a). Longer sequences got a higher Chow-average score on average for two reasons: (a) the later tokens tend to have higher probability as per Figure 3c, and (b) as the reviewer suggests, there are loops where the same short phrase gets repeated. That is why, after normalization by the sequence length, Chow-average prefers these longer sequences. \n\n> there was a negative correlation between translation quality and sequence length; was this because many of the long sequences were hallucinations?\n\nPer discussion in Section 3.5 (see also Figure 4), longer predictions tend to have repetitions, which results in lower translation quality.\n\n> --Which WMT set was used? It needs to be identified and cited.\n\nThanks for pointing this out. We have used WMT 14 FR-> EN, WMT 14 EN-> FR and WMT 19 DE-> FR. We also updated the draft to include these citations in Section 3.4.\n\n> --I don't understand how the MNLI experiments work. It is noted in Section 3.4 that MNLI is a multi-class classification problem, but the techniques proposed in this paper are not for classification.\n\nSorry for the confusion. Following the T5 and FLAN-T5 papers, we treat *all* problems as finding a text to text mapping. So for MNLI, we encode the classes as strings, namely, \u201centailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d. We take the model\u2019s output text and perform a string comparison to the label. We have updated the draft to clarify this."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546993323,
                "cdate": 1700546993323,
                "tmdate": 1700546993323,
                "mdate": 1700546993323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iU3Jumxv5H",
                "forum": "KgaBScZ4VI",
                "replyto": "m8uCltnCUf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the informative response. I have adjusted my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6173/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582133426,
                "cdate": 1700582133426,
                "tmdate": 1700582133426,
                "mdate": 1700582133426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]