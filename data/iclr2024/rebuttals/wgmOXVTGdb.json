[
    {
        "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer"
    },
    {
        "review": {
            "id": "Uo6NuFd7Lb",
            "forum": "wgmOXVTGdb",
            "replyto": "wgmOXVTGdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_t8xx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_t8xx"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, VAE and GAN are combined with DETR to realize multimodal layout generation. A large-scale ad banner data set with 7,196 samples containing English characters is presented. According to the experimental results of three data sets on ad banner, CGL, and CLAY, the method achieves SOTA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is easy to follow. \n\n* A large-scale ad banner dataset is collected for the layout design task.\n\n* The results show that the model achieves excellent performance."
                },
                "weaknesses": {
                    "value": "* Many technical details are not well-motivated and validated, e.g., VAE and DETR structures. \n\n* It seems the method combines multiple popular techniques and the novelty in the technical part is unclear. \n\n* Simply considering the box layout and ignoring font information and box aspect ratios makes the task less extensible. \n\n* The method requires dozens of loss functions for supervision. I am not sure how to tune weighting factors and make sure each term properly works."
                },
                "questions": {
                    "value": "The importance and necessity of VAE design is not validated. As the method takes a generative pipeline, I am interested in the variations and the latent spaces. Moreover, a proper validation of this key design is also important."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Reviewer_t8xx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8305/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698068896482,
            "cdate": 1698068896482,
            "tmdate": 1701053026905,
            "mdate": 1701053026905,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LHUdBdFhrX",
                "forum": "wgmOXVTGdb",
                "replyto": "Uo6NuFd7Lb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t8xx"
                    },
                    "comment": {
                        "value": "We thank all the reviewers for their constructive suggestions, which help improve the completeness of our submission. We are encouraged that the reviews are positive in the following five levels:\n\n* The paper is \"**easy to follow**\" (Reviewer t8xx).\n* The problem we are researching is \u201c**important**\u201d (Reviewer Gw2u) and our idea is \u201c**interesting**\u201d (Review j8Mq), \u201c**effective**\u201d (Review j8Mq), \u201c**practically useful**\u201d (Reviewer Gw2u), and is **one of our strengths** (Reviewer EPYk).\n* Our dataset contribution is \u201c**useful**\u201d (Reviewer j8Mq), \u201c**valuable**\u201d (Reviewer Gw2u),  and is **one of our strengths** (Reviewer EPYk, Reviewer t8xx).\n* Our evaluation is \u201c**extensive**\u201d (Reviewer Gw2u), and our results are \u201c**SOTA**\u201d (EPYk), \u201c**effective**\u201d (Reviewer j8Mq), \u201c**excellent**\u201d (Reviewer t8xx), and \u201c**good**\u201d (Reviewer Gw2u).\n* Our graphical system and user study are **one of the strengths** (Reviewer EPYk).\n\nWe now address individual questions of **Reviewer t8xx** below.\n\n1. **[Presentation: 2 fair?]**\n    - This **contradicts your first strength point** that \u201cthe paper is easy to follow\u201d, and there is no specific weakness point or question about the presentation. We therefore appeal the reviewer\u2019s re-evaluation about the \u201cpresentation\u201d rating and final rating.\n\n2. **[The VAE and DETR structures are not well-motivated and validated]**\n    - This is a **factual misunderstanding**. We have already discussed our motivations. Due to the space limit, we had to mention them in **the original submission Appendix Sec. A**. For VAE, our motivation has been articulated in **Paragraph 4**. VAE has been pre-validated by Larsen et al 2016 for image generation, and has been validated by us in **Table 3 LayoutDETR-VAE rows and LayoutDETR-VAEGAN rows**. With the help of VAE, the best results (in bold) are achieved in several metrics on several datasets. For DETR, our motivation has been articulated in **Paragraph 6 in the original submission Appendix Sec. A**. DETR has been pre-validated by Carion et al 2020 for object detection, and has been validated by CGL-GAN and ICVT for layout generation (see **Table 1**). We will manage to move this part to the main paper. We therefore appeal the reviewer\u2019s re-evaluation about the \u201csoundness\u201d rating and final rating.\n\n3. **[Technical novelty]**\n    - We understand this is a subjective judgement, and the debate would never end. We would rather argue using Prof. Michael Black\u2019s Guide to Reviewers: Novelty in Science (https://medium.com/@black_51980/novelty-in-science-8f1fd1a0a143). We quote some:\n        - About the novelty of our connections between two established fields, multimodal conditioned layout generation and visual detection, Prof. Michael Black says \u201cThe novelty arose from the fact that nobody had put these ideas together before.\u201d \u201cFortunately, these connections also turned out to be valuable, resulting in practical algorithms that were state of the art.\u201d \u201cTo see the connections for the first time, before others saw them, was like breathing for the first time.\u201d \u201cThe resulting paper embodies the translation of the idea into code, experiments, and text. In this translation, the beauty of the spark may be only dimly glimpsed. My request of reviewers is to try to imagine the darkness before the spark.\u201d\n        - About the novelty of our simple and reasonable reuse of LayoutGAN++ and DETR architectures, Prof. Michael Black says \u201cI value simplicity over unnecessary complexity; the simpler the better. Taking an existing network and replacing one thing is better science than concocting a whole new network just to make it look more complex.\u201d \u201cIf a paper has a simple idea that works better than the state of the art, then it is most likely not trivial. The authors are onto something and the field will be interested.\u201d \u201cThe inventive novelty was to have the idea in the first place. If it is easy to explain and obvious in hindsight, this in no way diminishes the creativity (and novelty) of the idea.\u201d\n        - About the novelty of our newly-collected dataset, Prof. Michael Black says \u201cNovelty (and value) come in many forms in papers. A new dataset can be novel if it does something no other dataset has done, even if all the methods used to generate the dataset are well known.\u201d\n    - We therefore appeal the reviewer\u2019s re-evaluation about the \u201ccontribution\u201d rating and final rating."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8305/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122374684,
                "cdate": 1700122374684,
                "tmdate": 1700122374684,
                "mdate": 1700122374684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IWN3wBIsIu",
            "forum": "wgmOXVTGdb",
            "replyto": "wgmOXVTGdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_EPYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_EPYk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed LayoutDETR which can inherit high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem. It learns to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- study layout generation and visual detection with a unified framework\n- proposed a new banner ads dataset\n- achieve state of art in layout generation in terms of metrics of realism, accuracy, and regularity\n- built graphical system and conduct user study"
                },
                "weaknesses": {
                    "value": "- the empty space detection on a background image and the layout generation of foreground can be decoupled as two separate steps. It is better to compare with such a baseline, and justify the superiority of doing it with a joint model.\n- the proposed dataset (images) is collected in prior work. The new contribution here is the detected text objects, background inpainting and the text class annotation, which is not as significant as a new dataset.\n- There are some concerns about the quality of data set. According to the way the data set was constructed, there are only texts as foreground objects, without other elements such as vector shape, image. This is very limited. Also, the inpainted background may contain artifacts which the generator can leverage for text location prediction. How is the text image patch obtained? If it's cropped from the original image, it has the same background patten, which may contain shortcut information for layout prediction."
                },
                "questions": {
                    "value": "- please clarify whether there are only text as foreground object in the dataset and all the experiments.\n- why Crello dataset is not multi modal? What is the unique part of the proposed data set?\n- explain whether it's possible to apply this paper to the problem: \"Towards Flexible Multi-modal Document Models\"\n- Eq 6, should it be p_1^i ?\n- the paper does not evaluated diversity of the generated results. It would be good to show some visual examples of different design variations for one background image."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The banner ads images may contain copyright logos, faces, or other protected images."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8305/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559306786,
            "cdate": 1698559306786,
            "tmdate": 1699637032738,
            "mdate": 1699637032738,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U55NqDsLgC",
                "forum": "wgmOXVTGdb",
                "replyto": "IWN3wBIsIu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EPYk"
                    },
                    "comment": {
                        "value": "We thank all the reviewers for their constructive suggestions, which help improve the completeness of our submission. We are encouraged that the reviews are positive in the following five levels:\n\n* The paper is \"**easy to follow**\" (Reviewer t8xx).\n* The problem we are researching is \u201c**important**\u201d (Reviewer Gw2u) and our idea is \u201c**interesting**\u201d (Review j8Mq), \u201c**effective**\u201d (Review j8Mq), \u201c**practically useful**\u201d (Reviewer Gw2u), and is **one of our strengths** (Reviewer EPYk).\n* Our dataset contribution is \u201c**useful**\u201d (Reviewer j8Mq), \u201c**valuable**\u201d (Reviewer Gw2u),  and is **one of our strengths** (Reviewer EPYk, Reviewer t8xx).\n* Our evaluation is \u201c**extensive**\u201d (Reviewer Gw2u), and our results are \u201c**SOTA**\u201d (EPYk), \u201c**effective**\u201d (Reviewer j8Mq), \u201c**excellent**\u201d (Reviewer t8xx), and \u201c**good**\u201d (Reviewer Gw2u).\n* Our graphical system and user study are **one of the strengths** (Reviewer EPYk).\n\nWe now address individual questions of **Reviewer EPYk** below.\n\n1. **[Compare to the baseline: empty space detection on a background image and the layout generation of foreground can be decoupled as two separate steps]**\n    - In fact, we **have already compared to such a baseline and outperformed it (Table 1 in the original submission): CGL-GAN** does use saliency detection to extract the empty background first, and then learns the layout generation using GANs. We will highlight this in the next iteration. We therefore appeal the reviewer\u2019s re-evaluation about the \u201csoundness\u201d rating and final rating.\n\n2. **[The proposed dataset (images) is collected in prior work]**\n    - This is a **factual misunderstanding**. We did collect our own images. Due to the space limit, we had to mention it in **the original submission Appendix Sec. C Paragraph 1**: In the image level, we spent non-trivial manual efforts to go through all the images in Pitt Image Ads Dataset. We then thoughtfully filtered out those with single modality, low quality, or old-fashioned designs. We finally selected qualified 3,536 images out of noisy 64,832 images in the original dataset. Moreover, we additionally searched on Google Image Search Engine with the keywords \u201dXXX ad banner\u201d where \u201dXXX\u201d goes through a list of 2,765 retailer brand names including the Fortune 500 brands. For each keyword search, we crawled the top 20 results and manually filtered out non-ads, single-modality, low-quality, or offensive-content images. We then selected 4,321 valid ad banner images. Combining the two sources, we in total obtained 7,857 valid ad banner images with arbitrary sizes. We will manage to move this part to the main paper. We therefore appeal the reviewer\u2019s re-evaluation about the \u201ccontribution\u201d rating and final rating.\n\n3. **[The inpainted background may contain artifacts which the generator can leverage for text location prediction]**\n    - This is a reasonable concern. In fact, we **have already considered and resolved it**. Due to the space limit, we had to discuss it in **the original submission Appendix C last paragraph**: \u201cIt is worth noting that inpainting clues may leak the layout bounding box ground truth information and shortcut training. Therefore, during training, we intentionally inpaint background images at additional random subregions that are irrelevant to their layouts.\u201d As a result, such random training augmentations introduce additional possible inpainting artifacts that do not respond to layout locations. This avoids training from being overfitting to inpainting artifacts, and makes the inference not to focus on any such artifacts. We will move this part to the main paper. We therefore appeal the reviewer\u2019s re-evaluation about the \u201csoundness\u201d rating and final rating.\n\n4. **[The text patch has the same background patten, which may contain shortcut information for layout prediction]**\n    - This is a **factual misunderstanding**. As mentioned in **the original submission Sec. 4 last paragraph**, the textual strings are extracted by OCR. And we condition on the textual strings rather than text patches for layout generation. There is no background pattern information for the text input and consequently no such shortcut. We therefore appeal the reviewer\u2019s re-evaluation about the \u201csoundness\u201d rating and final rating."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8305/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121101586,
                "cdate": 1700121101586,
                "tmdate": 1700121101586,
                "mdate": 1700121101586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gxanZlwP8v",
            "forum": "wgmOXVTGdb",
            "replyto": "wgmOXVTGdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_Gw2u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_Gw2u"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies graphic layout generation conditioned multimodal inputs, including background image, foreground image and text.\n\nThe main contribution is to adapt an exciting Transformer-based detector architecture as a content-conditioned layout generator and explore its training under different generative frameworks including GAN, VAE and VAE-GAN.\n\nA new ad banner dataset with rich semantic annotations is created and will be released for the training and evaluation of generative models for graphic layouts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is studying an important problem. Conditioning layout generation models on rich contents will certainly make the models more practically useful.\n\n2. The newly constructed banner dataset with detailed and rich annotations can be of value to the layout generation community.\n\n3. The evaluation is extensive and the results look good."
                },
                "weaknesses": {
                    "value": "1. The amount of technical contribution is small. While I appreciate the great effort that has been input into the work on building the system, testing different design choices and building the banner dataset, I think technical novelty and insight brought by the paper is limited. The whole work is more like constructing a working system by borrowing techniques from another domain directly (e.g., DETR) and combining components from other existing layout methods, e.g., (Kikuchi et al., 2021) and (Li et al., 2020), without any significant modification. Thus, the paper may not be of great interest to the ICLR audience, and perhaps fits better with more system-oriented conferences or journals.\n\n2. The evaluation is insufficient. The paper is aimed at conditional layout generation. However, all the quantitative metrics as well as the user study only evaluate layout quality, and another important aspect of results is ignored \u2014 how well generated layouts match the input contents. Thus, an experiment on layout-content consistency is needed but is missing in the current paper."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8305/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570998029,
            "cdate": 1698570998029,
            "tmdate": 1699637032600,
            "mdate": 1699637032600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6FHuejWXgZ",
                "forum": "wgmOXVTGdb",
                "replyto": "gxanZlwP8v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gw2u"
                    },
                    "comment": {
                        "value": "We thank all the reviewers for their constructive suggestions, which help improve the completeness of our submission. We are encouraged that the reviews are positive in the following five levels:\n\n* The paper is \"**easy to follow**\" (Reviewer t8xx).\n* The problem we are researching is \u201c**important**\u201d (Reviewer Gw2u) and our idea is \u201c**interesting**\u201d (Review j8Mq), \u201c**effective**\u201d (Review j8Mq), \u201c**practically useful**\u201d (Reviewer Gw2u), and is **one of our strengths** (Reviewer EPYk).\n* Our dataset contribution is \u201c**useful**\u201d (Reviewer j8Mq), \u201c**valuable**\u201d (Reviewer Gw2u),  and is **one of our strengths** (Reviewer EPYk, Reviewer t8xx).\n* Our evaluation is \u201c**extensive**\u201d (Reviewer Gw2u), and our results are \u201c**SOTA**\u201d (EPYk), \u201c**effective**\u201d (Reviewer j8Mq), \u201c**excellent**\u201d (Reviewer t8xx), and \u201c**good**\u201d (Reviewer Gw2u).\n* Our graphical system and user study are **one of the strengths** (Reviewer EPYk).\n\nWe now address individual questions of **Reviewer Gw2u** below.\n\n1. **[Technical novelty]**\n    - We understand this is a subjective judgement, and the debate would never end. We would rather argue using Prof. Michael Black\u2019s Guide to Reviewers: Novelty in Science (https://medium.com/@black_51980/novelty-in-science-8f1fd1a0a143). We quote some:\n       - About the novelty of our connections between two established fields, multimodal conditioned layout generation and visual detection, Prof. Michael Black says \u201cThe novelty arose from the fact that nobody had put these ideas together before.\u201d \u201cFortunately, these connections also turned out to be valuable, resulting in practical algorithms that were state of the art.\u201d \u201cTo see the connections for the first time, before others saw them, was like breathing for the first time.\u201d \u201cThe resulting paper embodies the translation of the idea into code, experiments, and text. In this translation, the beauty of the spark may be only dimly glimpsed. My request of reviewers is to try to imagine the darkness before the spark.\u201d\n       - About the novelty of our simple and reasonable reuse of LayoutGAN++ and DETR architectures, Prof. Michael Black says \u201cI value simplicity over unnecessary complexity; the simpler the better. Taking an existing network and replacing one thing is better science than concocting a whole new network just to make it look more complex.\u201d \u201cIf a paper has a simple idea that works better than the state of the art, then it is most likely not trivial. The authors are onto something and the field will be interested.\u201d \u201cThe inventive novelty was to have the idea in the first place. If it is easy to explain and obvious in hindsight, this in no way diminishes the creativity (and novelty) of the idea.\u201d\n       - About the novelty of our newly-collected dataset, Prof. Michael Black says \u201cNovelty (and value) come in many forms in papers. A new dataset can be novel if it does something no other dataset has done, even if all the methods used to generate the dataset are well known.\u201d\n    - We therefore appeal the reviewer\u2019s re-evaluation about the \u201ccontribution\u201d rating and final rating.\n\n2. **[All the quantitative metrics as well as the user study only evaluate layout quality]**\n    - This **contradicts your strength point** that \u201cthe evaluation is extensive\u201d. Moreover, this is a **factual misunderstanding**. We do render foreground elements on top of background images according to the real/generated layouts. See **Fig. 1,3,7 in the original submission** for the rendered samples, and **Fig. 3 caption** for the rendering procedure. As a result, all of the Image FID, Image KID, and user study are based on the photorealistic real/rendered images, so as to consider the layout-content consistency. We will highlight this in the next iteration. We therefore appeal the reviewer\u2019s re-evaluation about the \u201csoundness\u201d rating and final rating."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8305/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120183656,
                "cdate": 1700120183656,
                "tmdate": 1700120183656,
                "mdate": 1700120183656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "scnHAL9TW9",
                "forum": "wgmOXVTGdb",
                "replyto": "6FHuejWXgZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8305/Reviewer_Gw2u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8305/Reviewer_Gw2u"
                ],
                "content": {
                    "comment": {
                        "value": "For the second point in the authors' response, layout-content consistency in fact refers to how well generated layouts (or rendered images) match the input contents, which can also be referred to as input matching. The Image FID and KID only measure how well rendered images match real images, thus focusing on perceptual quality rather than layout-content consistency. Take the user study for example. In order to test layout-content consistency, the participants should be shown a pair of generated designs as well as their input condition, and asked to tell which better matches (or presents) the input condition, rather than just being asked to judge which looks better."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8305/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739293201,
                "cdate": 1700739293201,
                "tmdate": 1700739293201,
                "mdate": 1700739293201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZFxziQX8bw",
            "forum": "wgmOXVTGdb",
            "replyto": "wgmOXVTGdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_j8Mq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8305/Reviewer_j8Mq"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the layout generation task by reformulating it as a detection problem. A transformer-based architecture, i.e., LayoutDETR, is proposed to detect reasonable locations, scales and spatial relations for elements in a layout. A new banner dataset is established with rich semantic annotation. The proposed solution is further integrated into a graphical system to scale up the layout generation process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of applying the visual detection framework for the layout generation task is interesting and effective. The collected could be useful for future research in the community. The experimental results show the effectiveness of the proposed method under six evaluation metrics."
                },
                "weaknesses": {
                    "value": "1. The first contribution of this paper is that no existing methods can handle all those modalities at once. However, as shown in Table 1, Vinci can also use these modalities as conditions.\n2. The computation cost analysis of the proposed solution is missing. Since the model contains a variety of input modalities, I was wondering about the computational cost and runtime analysis of the proposed method and existing works.\n3. It would be better to show the diversity of the generated layouts and discuss the limitations of the proposed method."
                },
                "questions": {
                    "value": "1. How to distinguish the foreground image and the background image if the background images are defined with arbitrary sizes?\n2. Why Image FID that uses image features pre-trained on ImageNet could be used to evaluate the quality of the rendered graphic designs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8305/Reviewer_j8Mq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8305/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676649816,
            "cdate": 1698676649816,
            "tmdate": 1699637032479,
            "mdate": 1699637032479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fxCjmHkzCW",
                "forum": "wgmOXVTGdb",
                "replyto": "ZFxziQX8bw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8305/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j8Mq"
                    },
                    "comment": {
                        "value": "We thank all the reviewers for their constructive suggestions, which help improve the completeness of our submission. We are encouraged that the reviews are positive in the following five levels:\n\n* The paper is \"**easy to follow**\" (Reviewer t8xx).\n* The problem we are researching is \u201c**important**\u201d (Reviewer Gw2u) and our idea is \u201c**interesting**\u201d (Review j8Mq), \u201c**effective**\u201d (Review j8Mq), \u201c**practically useful**\u201d (Reviewer Gw2u), and is **one of our strengths** (Reviewer EPYk).\n* Our dataset contribution is \u201c**useful**\u201d (Reviewer j8Mq), \u201c**valuable**\u201d (Reviewer Gw2u),  and is **one of our strengths** (Reviewer EPYk, Reviewer t8xx).\n* Our evaluation is \u201c**extensive**\u201d (Reviewer Gw2u), and our results are \u201c**SOTA**\u201d (EPYk), \u201c**effective**\u201d (Reviewer j8Mq), \u201c**excellent**\u201d (Reviewer t8xx), and \u201c**good**\u201d (Reviewer Gw2u).\n* Our graphical system and user study are **one of the strengths** (Reviewer EPYk).\n\nWe now address individual questions of **Reviewer j8Mq** below.\n\n1. **[Vinci also handles all these modalities]**\n    - We have already discussed **the obvious limitations of Vinci in the original submission Sec. 2 second last paragraph**: \u201cVinci relies on a finite set of predefined layout candidates to choose background images from a pool of food and beverage domains. Their method is unable to design layouts conditioned on arbitrary backgrounds in open domains, natural or handcrafted, plain or cluttered, like ours.\u201d We agree with the reviewer to lower our key in the next iteration.\n\n2. **[Computation cost and runtime analysis]**\n    - In fact, we have already measured training and runtime computation costs. Due to the space limit, we had to report them in **the original submission Appendix Sec. B 2nd last paragraph**: \u201cWe train on 8 NVIDIA A100 GPUs for 110k iterations in 4 days. Inference is much more efficient: we load only G into a single NVIDIA A100 GPU and it consumes only 2.82GB of memory. It takes only 0.38 sec to generate a layout given foreground and background conditions.\u201d We will manage to move this part to the main paper.\n    - Following the reviewer's suggestion, we also measure and summarize the space complexity and training/runtime time complexity of our model variants and all the baselines below. Although our training is relatively costly, it is just one-time. Our runway cost is one of the most efficient.\n      | Method            | #parameters (M) | Training on 8x A100 GPUs (hours) | Runtime on 1x A100 GPU (seconds per layout) |\n|-------------------|:---------------:|:-----------------------:|:-----------------------------------:|\n| LayoutGAN++       |       252       |            52           |                 0.38                |\n| READ              |       310       |            89           |                 0.38                |\n| Vinci             |       251       |            51           |                 0.36                |\n| LayoutTransformer |       190       |            36           |                 0.39                |\n| CGL-GAN           |        88       |            77           |                 0.40                |\n| ICVT              |        61       |            34           |                 0.44                |\n| LayoutDETR-GAN    |       270       |            96           |                 0.38                |\n| LayoutDETR-VAE    |       189       |            35           |                 0.38                |\n| LayoutDETR-VAEGAN |       351       |            99           |                 0.38                |\n\n3. **[Show diversity of the generation]**\n    - In fact, we have already discussed the diversity of generation. Due to the space limit, we had to mention it in **the original submission Appendix Sec. E Paragraph 2 and Fig. 8**.\n\n4. **[The limitation of the proposed method]**\n    - In fact, we have already discussed the limitation. Due to the space limit, we had to mention it in **the original submission Appendix Sec. F and Fig. 9**.\n\n5. **[How to distinguish foreground and background images?]**\n    - For our ad banner dataset and CGL dataset, we intend not to distinguish foreground and background images as there is no foreground bounding box ground truth collected. All the visual contents except texts are regarded as the background image condition. On the other hand, because CLAY dataset has the ground truth annotation for all the foreground visual elements, all the training and testing on CLAY are conditioned on textual plus visual foreground inputs (e.g. icons).\n\n6. **[Why is Image FID pretrained on ImageNet a useful metric?]**\n    - Image FID and Image KID are standard metrics to evaluate the photorealism between real and generated images. In our case, the real/generated images are rendered by overlaying foreground elements on top of background according to the real/generated layouts. Considering foreground and background images contain natural images, Image FID and Image KID are meaningful to measure the layout-content consistency."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8305/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119387801,
                "cdate": 1700119387801,
                "tmdate": 1700119387801,
                "mdate": 1700119387801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]