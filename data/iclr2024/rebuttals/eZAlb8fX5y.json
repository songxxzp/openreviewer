[
    {
        "title": "KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization"
    },
    {
        "review": {
            "id": "jVUhDEL6wU",
            "forum": "eZAlb8fX5y",
            "replyto": "eZAlb8fX5y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_nvar"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_nvar"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dynamic quantization technique to reduce the computational cost and required memory for storing K/V cache on GPU. The authors propose a quantization method that symmetrically quantizes the KV cache into ternary digits (-1, 0, 1), thereby obviating the need for a dequantization stage, which is required in conventional methods.  The experimental results demonstrate the efficacy of their quantization technique, showcasing lower perplexity compared to alternative methods and a reduction in computational workload."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present pioneering research by demonstrating that KV caches can be quantized into ternary digits with minimal impact on perplexity. This innovative approach not only eliminates the need for a dequantization stage but also offers the benefits of reduced GPU memory usage and computational complexity."
                },
                "weaknesses": {
                    "value": "- The authors present statistics regarding the maximum and minimum values of the KV cache in each layer, but do not delve into the distribution of data within each KV cache. While data distribution may be less relevant when quantizing data into n-bit integers, this paper opts for ternary digit quantization. Consider a key embedding to follow a normal distribution with the distance between the average and maximum value being 3 times the standard deviation (Max = $\\mu + 3\\sigma$). According to the authors' equation ($X_q = \\lceil \\frac{X}{\\Delta} \\rfloor, \\Delta = \\text{max}(|X|)$, approximately 86.6% of the numbers will be quantized to zero ($P(-1.5<\\frac{x-\\mu}{\\sigma}<1.5)$). Such a substantial portion of key embeddings being quantized to zero might suggest sparsity in attention scores. Hence, it would be more persuasive if the authors compared their work against models employing sparse attention, as opposed to traditional attention mechanisms.\n\n- The claim that quantizing to ternary digits reduces the number of multiplications and additions should be validated through runtime measurements, particularly when considering that LLM inference is primarily constrained by memory bandwidth and latency. It would be also advisable for the authors to measure runtime performance compared to other works, such as TRT-LLM. Additionally, as the authors propose a dynamic quantization approach, it is implied that the KV cache should still be stored in its original precision (e.g., FP32) and quantized before calculating attention scores. In contrast, TRT-LLM, one of the baseline models, employs static quantization, eliminating the need for repetitive quantization, and allows for lower precision storage in CPU/GPU memory. Consequently, the authors should address the potential overhead in terms of memory usage and quantization latency.\n\n- There are also some minor suggestions:\n  - Many sentences begin with conjunctions such as \"And\" and \"So.\" To maintain formality, it is recommended to avoid starting sentences with conjunctions.\n\n  - A minor typographical error is present in Section 1, line 2 (\"konw\" should be corrected to \"known\"). Moreover, the terms \"Experiment setup/result\" should be revised to \"Experimental setup/result\" for clarity and consistency."
                },
                "questions": {
                    "value": "- Would the utilization of a sparse attention method yield superior results compared to KVTQ in terms of perplexity, runtime, and memory usage?\n- Could you provide insights into the performance enhancements achieved with KVTQ when compared to other baseline methods, particularly in terms of runtime and memory usage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4580/Reviewer_nvar"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698473040196,
            "cdate": 1698473040196,
            "tmdate": 1699636435775,
            "mdate": 1699636435775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4nzFMnDBrs",
                "forum": "eZAlb8fX5y",
                "replyto": "jVUhDEL6wU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments!\n\n---\n\n**KVTQ vs naive sparsity(item 1)**\n\nAs show in table 3, we evaluate the sparse ratio of the KV cache of difference series of \nmodels under our KVTQ quantization setting.\nThe KV cache of different models has different sparse ratio after quantization.\nThe sparse ratio of the KV cache with the largest quantization step is high as your \nanalyze.\nBut if we only use one channel of ternary digit to express the KV cache, the accuracy is \nunacceptable.\nSo we use 4 channels of ternary digits with different quantization step to express the K \ncache and 3 channels for the V cache.\nThe sparse ratio of the KV cache with the smallest quantization step is about 50%.\n\n\nScissorhands [1] compresses the KV cache by only saving the key-value embeddings with \nhigh attention scores, which is one kind of sparsity.\nThey give the perplexity result as a curve in a figure. \nIt can be seen from the figure that when the sparse ratio is greater than 50%, ppl \nincreases significantly.\n\n**Performance(item 2)**\n\n\nTernary digits are more suitable to be stored in ternary memory device.\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nFor example, the KVTQ method can be combined with ternary memory device and computing in \nmemory technology to avoid the cost of moving the KV cache.\n\n\nFor LLM inference on devices such as GPUs, the K4/V4 method proposed in our paper is a \nbetter choice.\nAs we know, we also the first use fine-grained dynamic quantization to quantizing the KV \ncache to 4 bits/3 bits and systematically evaluate the performance.\nWe wanted to highlight the KVTQ method so we did not list the performance data of the \nK4/V4 method.\n\nWe will revise the paper to emphasize that the advantages of KVTQ requires using ASIC to\nrelease.\n\n\n**Type and presentation(item 3)**\n\nThank you, we will fix the typo error and refine our presentation follow your suggestions.\n\n\n---\n\nIf any questions remain, we are happy to engage in further discussion!\n\n\n\n**Reference**\n\n- [1] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios\nKyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance\nhypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114747618,
                "cdate": 1700114747618,
                "tmdate": 1700114747618,
                "mdate": 1700114747618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DEmTYyevOC",
            "forum": "eZAlb8fX5y",
            "replyto": "eZAlb8fX5y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_iUPn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_iUPn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel fine-grained dynamic quantization method, named KVTQ, for compressing the Key-Value cache of LLMs into hardware-efficient ternary digits. The authors highlight that, unlike traditional weights-only quantization, which requires dequantization for each use, their method eliminates the need for dequantization and allows matrix multiplication to be efficiently conducted using simple addition and subtraction. They claim to be the first to demonstrate that compressing the KV cache to ternary digits can be achieved with negligible impact on perplexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation behind employing KVTQ (Key-Value Token Quantization) is clearly articulated and easy to grasp. The paper is well-written, with a logical flow that makes it easy to follow the core concept. However, I must admit that my expertise may not fully equip me to assess the technical novelty in this particular field.\n\nFrom what I understand, a key aspect of KVTQ is its ability to avoid additional dequantization steps. Instead, it can directly replace dequantization and subsequent matrix multiplication with a summation operation. This approach seems to have practical implications, particularly in reducing computational complexity.\n\nThe empirical results demonstrate a significant reduction in perplexity across various sizes of OPT/LLama models, which is noteworthy given that many expensive operations are bypassed. This aspect of KVTQ seems to be a valuable contribution, potentially leading to more efficient processing in relevant applications. However, a deeper technical analysis might be necessary to fully appreciate the novelty and implications of this approach."
                },
                "weaknesses": {
                    "value": "I confess that my understanding of quantization isn't particularly deep, which somewhat hinders my ability to fully grasp the implications of the results shown in Tables 3 and 4. \nHowever, I think the paper falls short in providing comprehensive information on the practical aspects of implementing KVTQ. Details like the physical size of the Key-Value (KV) cache when KVTQ is in use, as well as the memory overhead and latency during forward passes, are missing. These details are crucial for understanding not just the theoretical benefits of KVTQ, but also its real-world applicability and efficiency."
                },
                "questions": {
                    "value": "see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727652430,
            "cdate": 1698727652430,
            "tmdate": 1699636435700,
            "mdate": 1699636435700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jnUGkZzfR3",
                "forum": "eZAlb8fX5y",
                "replyto": "DEmTYyevOC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments!\n\n---\n\nTable 3 shows the sparse ratio of the KV cache which have been quantized using the KVTQ \nmethod.\nThe sparsity feature of the quantized KV cache means that we use multiple channels of \nternary digits to represent the KV cache does not result in too many additional additive \ncalculation while eliminating multiplication calculations.\n\nTable 4 is a supplementary experiment to illustrate that K cache is more sensitive to \nquantization than V cache. \nThis conclusion supports that we give K cache more bits when quantizing KV cache.\n\nTernary digits are more suitable to be stored in ternary memory device.\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nFor example, the KVTQ method can be combined with ternary memory device and computing in \nmemory technology to avoid the cost of moving the KV cache.\n\nFor LLM inference on devices such as GPUs, the K4/V4 method proposed in our paper is a \nbetter choice.\nAs we know, we also the first use fine-grained dynamic quantization to quantizing the KV \ncache to 4 bits/3 bits and systematically evaluate the performance.\nWe wanted to highlight the KVTQ method so we did not list the performance data of the \nK4/V4 method.\n\nWe will revise the paper to emphasize that the advantages of KVTQ requires using ASIC to\nrelease.\n\n\n---\n\nIf any questions remain, we are happy to engage in further discussion!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115150533,
                "cdate": 1700115150533,
                "tmdate": 1700115150533,
                "mdate": 1700115150533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0TnLYdb0w",
                "forum": "eZAlb8fX5y",
                "replyto": "jnUGkZzfR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Reviewer_iUPn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Reviewer_iUPn"
                ],
                "content": {
                    "title": {
                        "value": "responding to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response. I will maintain my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677350792,
                "cdate": 1700677350792,
                "tmdate": 1700677350792,
                "mdate": 1700677350792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iQ28jiWXZX",
            "forum": "eZAlb8fX5y",
            "replyto": "eZAlb8fX5y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_cw8R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_cw8R"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines KV cache compression algorithms, specifically focusing on ternary representation. The ternary format streamlines computations by eliminating the need for a dequantization step and primarily utilizing addition and subtraction operations. The authors explore distinct quantization sensitivities for 'K' and 'V', each quantized with varying bit numbers. The study evaluates the LLaMA and OPT models and delves into the sparsity arising from the ternary representation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors provide detailed results concerning the quantization of K and V bits. By monitoring the range of K and V values across various LLaMA and OPT models, the authors validate the rationale for allocating distinct quantization bits to K and V.\n\n- The paper highlights the unique computational advantages of ternary representation. Contrary to recent weight-only quantizations, ternary representation simplifies computations to mainly additions and subtractions.\n\n- The study showcases sparsity across different channels, illustrating the potential computational savings from '0' weights in ternary quantization.\n\n- The presented quantization techniques and computational approaches are clear and uncomplicated."
                },
                "weaknesses": {
                    "value": "- While ternary computations can simplify attention-related calculations, the authors haven't quantified the reduction in latency or the number of FLOPs saved by their method.\n\n- Given that ternary representation uses 2 bits to represent -1, 0, or +1, its memory footprint might surpass binary-based quantization. A comparison of memory usage between previous quantization methods and the proposed approach is essential.\n\n- The correlation between sparsity and computational reduction doesn't directly equate to reduced latency or improved throughput. Instead of merely highlighting channel sparsity, tangible hardware benefits should be assessed.\n\n- What is the net effect on inference? The paper would benefit from a thorough estimation or actual measurement results."
                },
                "questions": {
                    "value": "Please refer to the list of weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773928483,
            "cdate": 1698773928483,
            "tmdate": 1699636435621,
            "mdate": 1699636435621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NP9vx8DiH5",
                "forum": "eZAlb8fX5y",
                "replyto": "iQ28jiWXZX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments!\n\n---\n\n**Flops(item 1)**\n\nIn section 3.5 we describe the amount of multiplication operations in the attention \nblock, but due to limited space we did not give a complete example.\nWe will refine this part and add more results in appendix.\n\nHere we use LlaMa-7B as an example:\n\nFor LlaMa-7B the head number is 32 and head size is 128.\nIf we assume the input sequence is 1024 and the maximum sequence length is 4096 and the \naverage sequence will be $(1024 + 4096) / 2$.\nAs mentioned in section3.5, the KVTQ method reduce \n$4 \\times B \\times NH \\times S \\times HS$ times of multiplication operations.\n\nIf we assume the batch size is 16, in the generation process the reduced multiplication \noperations of one layer of the LlaMa-7B  model are:\n$$ 4 * 16 * 32 * (1024 + 4096) / 2 * 128 * (4096 - 1024) = 1.875T $$\nFor the LlaMa-7B model with 32 transformer blocks, the total reduced multiplication \noperations are:\n$$ 1.875T * 32 = 60T $$\n\n\n\n**Memory usage(item 2)**\n\nIn KVTQ, we use 4 groups of ternary digits to express the K cache and 3 groups of ternary\ndigits to express the V cache.\nAs you said, if we want to storage the ternary K cache in binary memory device directly, \nwe have to use 8 bits and for V cache, we have to use 6 bits.\nThe memory usage is less than the 8-bit static quantization used in TRT-LLM but the \nmemory usage is higher than the K4/V4 fine-grained dynamic quantization we proposed in \nour paper as comparison method.\n\nTernary digits are more suitable to be stored in ternary memory device.\nAnd saving memory is not the only target considered by the KVTQ method. \nThe K4/V4 method we proposed in our paper saves more memory compared to the KVTQ method, \nbut the K4/V4 method does not have the computing advantages which we mentioned in section\n3.5.\n\n\n**Sparsity and performance(item 3)**\n\nAs you said, sparsity and computational reduction doesn't directly equate to reduced \nlatency or improved throughput.\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nFor hardware such as GPU, the K4/V4 method we proposed in our paper is more suitable.\n\n\n**Over all performance(item 4)**\n\nTernary digits are more suitable to be stored in ternary memory device.\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nFor example, the KVTQ method can be combined with ternary memory device and computing in \nmemory technology to avoid the cost of moving the KV cache.\n\nWe will revise the paper to emphasize that the advantages of KVTQ requires using ASIC to\nrelease.\n\n---\n\nIf any questions remain, we are happy to engage in further discussion!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115357271,
                "cdate": 1700115357271,
                "tmdate": 1700115357271,
                "mdate": 1700115357271,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2fc5lvzpsQ",
                "forum": "eZAlb8fX5y",
                "replyto": "NP9vx8DiH5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Reviewer_cw8R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Reviewer_cw8R"
                ],
                "content": {
                    "title": {
                        "value": "Response from Reviewer cw8R"
                    },
                    "comment": {
                        "value": "Thank you for your detailed responses. However, I still have some critical concerns:\n\n1. In highly parallelizable computing systems like GPUs, the direct relationship between reduced computational workload and improved latency or throughput is not always clear, especially when certain computations become less parallelizable. Therefore, the stated 60T reduction in multiplication operations doesn't clearly translate to more efficient inference.\n2. The concept of a ternary memory device is intriguing, but I'm uncertain about its practicality. As far as I'm aware, there aren't any commercial memory systems available that can distinctly recognize three values (such as -1, 0, +1) in one bit-cell memory structure. Could you clarify what you mean by this?\n3. The necessity for a customized ASIC warrants a more detailed explanation. What specific features of such ASIC are essential for your approach? If the focus is heavily on sophisticated hardware, it might be more appropriate to consider publishing this research in a hardware-oriented community.\n\nGiven these unresolved issues, I have decided to maintain my original score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563337391,
                "cdate": 1700563337391,
                "tmdate": 1700563337391,
                "mdate": 1700563337391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nHMrAYJw6k",
            "forum": "eZAlb8fX5y",
            "replyto": "eZAlb8fX5y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_ZD7v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_ZD7v"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method to compress the KV-cache into ternary, effectively reducing both storage and computational costs. Specifically, regarding computational costs, the ternary KV-cache eliminates the need for reweighting, converting multiplications into addition and subtraction operations. The work uses experimental statistics to guide the ternarization process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tTernarization indeed results in a reduction in storage and computational costs.\n\n2.\tExperimental findings play an instructive role in quantization. The paper discovers that it's preferable to allocate more bits to K due to its larger numerical rang."
                },
                "weaknesses": {
                    "value": "1.\tThe paper's description of the actual quantization method might lead to misleading. The term \"ternary\" suggests that K and V are genuinely ternary with values {-1, 0, +1}. However, the paper assigns \"multiple channels\" to each value with varying steps. This essentially equates to a higher quantization bit count. As the paper states, \u201cwe use 4 channels of ternary digits for the key embeddings and 3 channels of ternary digits for the value embeddings.\u201d This means K is 4-bit quantized, and V is 3-bit quantized? This crucial point lacks adequate discussion and might mislead readers.\n2.\tThe quantization method utilized is dynamic, meaning the quantization step must be dynamically determined. This approach may not be hardware-friendly. To determine a single max and min value requires scanning the entire tensor. For larger tensors, this method could introduce significant latency.\n3.\tThe ternary compensation algorithm employed originates from ABC-Net and is not original.\n4.\tThe paper lacks experimental details. While there were experiments on PPL, the motivation of \u201creducing storage and computational costs\u201d is not reflected in the experiments. It remains unproven whether ternary quantization is GPU-friendly. Likewise, there's no evidence provided to demonstrate if dynamic quantization will introduce substantial latency."
                },
                "questions": {
                    "value": "1.\tCan you provide clarity on the choice of using \"ternary\" in your terminology when the actual quantization might suggest higher bit counts?\n2.\tHow do you address the potential hardware inefficiencies of the dynamic quantization method, especially with larger tensors?\n3.\tSince the ternary compensation algorithm is taken from ABC-Net, how is your quantization method different from it?\n4.\tYou should provide more experiments that can showcase the effectiveness of your method in terms of computational and storage savings. For example, how much decoding latency can be reduced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812128023,
            "cdate": 1698812128023,
            "tmdate": 1699636435522,
            "mdate": 1699636435522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "388zFXlHag",
                "forum": "eZAlb8fX5y",
                "replyto": "nHMrAYJw6k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments!\n\n---\n\n**Why we use \"ternary\"(Q1)**\n\nThe ternary digits have two nice properties.\nFirst for $A \\times B $, when either A or B contains only ternary digits, the matrix \nmultiplication can be completed by addition and subtraction operations.\n4 bit/3 bit integers do not have this feature.\nWe give a tiny example in figure 3.\n\nSecond the ternary representation will result in sparsity.\nWe can take advantage of this feature to reduce the amount of calculations.\n4 bit/3 bit integers do not have this feature.\nFigure 3c is an example.\n\nFor these two reasons, we use \u201cternary\u201d in our paper.\n\n\n**Overhead of dynamic quantization(Q2)**\n\n\nFor text generation applications, the KV cache is quantized once and used multiple times. \nDuring the generation process, for each new query, we only have to quantize the key-value\nembeddings corresponding the current query since the key-value embeddings of the past \nquerys have been cached.\nFor each new query, the size of key-value embedding is $1 * hiddensize$.\n\nCompared to static quantization of the KV cache, the only difference is that we have to\ncompute the quantization step during the inference process for dynamic quantization.\nThe amount of calculation introduced by dynamic quantization is relatively small and can\nbe ignored.\n\n\n**Relationship with ABC-Net(Q3)**\n\nTernary quantization have been used in computer vision tasks formerly and we are the \nfirst which use this technique in quantizing the KV cache of LLMs.\nPreviously, binary/ternary quantization was used within the scope of quantization-aware \ntraining(QAT). \nOur KVTQ applies ternary quantization under the scope of post-training quantization(PTQ).\n\nWe mentioned ABC-Net for it gives us the idea to use the **multiple groups** idea rather \nthan it use ternary quantization. \nABC-Net try to quantize AI models of computer vision tasks to binary value.\nABC-Net focuses on binary quantization and the method is under the QAT scope. \nWe will refine the description of the relation between our work and ABC-Net.\n\n\n**Memory and performance(Q4)**\n\nTernary digits are more suitable to be stored in ternary memory device.\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nFor example, the KVTQ method can be combined with ternary memory device and computing in \nmemory technology to avoid the cost of moving the KV cache.\n\n\nFor LLM inference on devices such as GPUs, the K4/V4 method proposed in our paper is a \nbetter choice.\nWe will revise the paper to emphasize that the advantages of KVTQ requires using ASIC to\nrelease.\n\n---\n\nIf any questions remain, we are happy to engage in further discussion!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116010144,
                "cdate": 1700116010144,
                "tmdate": 1700116010144,
                "mdate": 1700116010144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LKqOhq8Poq",
            "forum": "eZAlb8fX5y",
            "replyto": "eZAlb8fX5y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_gbCq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4580/Reviewer_gbCq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to use ternary quantization, KVTQ, on the KV cache in large language model inference to compress the memory space and improve the attention computation efficiency. KVTQ uses a group of ternary digits of different quantization steps to express the KV cache to help alleviate the accuracy degradation of multiple channels. The experiments show that the proposed KVTQ can outperform 4-bit KV cache quantization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper systematically studies the KV cache quantization settings, including dynamic/static quantization, symmetric/asymmetric quantization, and quantization precision difference for K and V cache.\n+ The evaluation results of the proposed KVTQ are promising, especially on the newer large language models LLaMa."
                },
                "weaknesses": {
                    "value": "- The novelty of the proposed ternary quantization is limited since it was first proposed by ABC-Net.\n- This paper lacks measured memory usage and memory footprint of the KV cache for the proposed KVTQ method.\n- This paper also lacks measured latency/throughput using the proposed KVTQ method. The actual improvement of replacing the multiplication in attention with addition using ternary digits is unclear."
                },
                "questions": {
                    "value": "Please provide the experiment results mentioned in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699322110568,
            "cdate": 1699322110568,
            "tmdate": 1699636435437,
            "mdate": 1699636435437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O76cG9hYio",
                "forum": "eZAlb8fX5y",
                "replyto": "LKqOhq8Poq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4580/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments!\n\n---\n\n**Relationship with ABC-Net(item 1)**\n\nTernary quantization have been used in computer vision tasks formerly and we are the \nfirst which use this technique in quantizing the KV cache of LLMs.\nPreviously, binary/ternary quantization was used within the scope of quantization-aware \ntraining(QAT). \nOur KVTQ applies ternary quantization under the scope of post-training quantization(PTQ).\n\nWe mentioned ABC-Net for it gives us the idea to use the **multiple groups** idea rather \nthan it use ternary quantization. \nABC-Net try to quantize AI models of computer vision tasks to binary value.\nABC-Net focuses on binary quantization and the method is under the QAT scope.\nWe will refine the description of the relation between our work and ABC-Net.\n\n**Memory usage(item 2)**\n\nIn KVTQ, we use 4 groups of ternary digits to express the K cache and 3 groups of ternary\ndigits to express the V cache.\nIf we want to store the ternary K cache in binary memory device directly, we have to use \n8 bits and for V cache, we have to use 6 bits.\nTernary digits are more suitable to be stored in ternary memory device.\n\nSaving memory is not the only target considered by the KVTQ method. \nThe K4/V4 method we proposed in our paper saves more memory compared to the KVTQ method, \nbut the K4/V4 method does not have the computing advantages which we mentioned in section\n3.5.\n\n\n**Performance(item 3)**\n\nOn existing LLM inference devices, such as GPUs, the KVTQ method can be run.\nBut the KVTQ method requires a customized ASIC to release its advantages.\nDuring the generation process, for each new query we need to load all the cached KV \nembeddings into the computing unit.\nFor example, the KVTQ method can be combined with ternary memory device and computing in \nmemory technology to avoid the cost of moving the KV cache.\n\nFor LLM inference on devices such as GPUs, the K4/V4 method proposed in our paper is a \nbetter choice.\nAs we know, we also the first use fine-grained dynamic quantization to quantizing the KV \ncache to 4 bits/3 bits and systematically evaluate the performance.\n\nWe will revise the paper to emphasize that the advantages of KVTQ requires using ASIC to\nrelease.\n\n---\n\nIf any questions remain, we are happy to engage in further discussion!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116102346,
                "cdate": 1700116102346,
                "tmdate": 1700116102346,
                "mdate": 1700116102346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]