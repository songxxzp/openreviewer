[
    {
        "title": "Treatment Effects Estimation By Uniform Transformer"
    },
    {
        "review": {
            "id": "YmDRgBNY09",
            "forum": "oOGqJ6Z1sA",
            "replyto": "oOGqJ6Z1sA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_Ybv2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_Ybv2"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the average treatment on the treated (ATT) in the Holder smooth paradigm of Robins et al. (2008), where minimax rates are known. Previous work characterizes the lower bound and shows that a higher order influence function estimator attains it. This paper proposes an alternative estimator based on kernel smoothing to also achieve the minimax lower bound. In the \u201chard cases\u201d where the nuisances are non-smooth, relatively few estimators have been shown to have good properties; for the \u201ceasy cases\u201d where the nuisances are collectively smooth enough, this is a very mature literature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: To my knowledge, this proposed estimator is new.\n\nQuality: The results are cohesive. I recommend the authors mention Appendix B in the main text; I did not notice it at first.\n\nClarity: The writing is clear.\n\nSignificance: Proposing alternative estimators that achieve the minimax lower bound in hard cases is theoretically interesting, though perhaps not practically significant."
                },
                "weaknesses": {
                    "value": "The introduction is too general for a paper that is ultimately about ATT only. \n\nThe kernel balancing weight literature is rich with many recent installments that should be at least mentioned in the introduction: Kallus (2020), Hirshberg et al. (2019), Singh (2021), and Bruns-Smith et al. (2023).\n\nIn Corollary 1, the convergence in distribution result for \u201ceasy\u201d cases ends up being similar to several works that are unreferenced. \n\nI will raise my score if these are addressed."
                },
                "questions": {
                    "value": "What are the advantages of this approach versus the higher order influence function approach, which seems to have the same kinds of guarantees?\n\nHow do we choose the bandwidth in practice?\n\nIs Theorem 2 a straightforward extension of the Robins et al. (2008) lower bound result? What are the aspects of it that are new? Close comparisons would help here.\n\nI will raise my score if these are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Reviewer_Ybv2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764810334,
            "cdate": 1698764810334,
            "tmdate": 1700777067950,
            "mdate": 1700777067950,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iSiQ5yZyyY",
                "forum": "oOGqJ6Z1sA",
                "replyto": "YmDRgBNY09",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ybv2"
                    },
                    "comment": {
                        "value": "We are very grateful for your careful reading of our work and your comments/suggestions,\nwhich helped improve the paper significantly. Here is our response for your comments.\n\n\u201cI recommend the authors mention Appendix B in the main text; I did not notice it at first.\u201d\n\nResponse: Thanks for the suggestion. We now mention Appendix B at the end of Section 4.\n\n\u201cThe introduction is too general for a paper that is ultimately about ATT only.\u201d\n\nResponse: Thanks for the comment. Due to the page limit, we only present the estimator for average treatment on the treated (ATT). As illustrated in Appendix C.1, the method can also be used to estimate the average treatment effect (ATE). The theory can also be accordingly generalized straightforwardly. \n\n\u201cThe kernel balancing weight literature is rich with many recent installments that should be at least mentioned in the introduction: Kallus (2020), Hirshberg et al. (2019), Singh (2021), and Bruns-Smith et al. (2023).\u201d\n\nResponse: Thanks for the great suggestions. These references are added in the revised version of the paper. \n\n\u201cIn Corollary 1, the convergence in distribution result for \u201ceasy\u201d cases ends up being similar to several works that are unreferenced.\u201d\n\nResponse: This is a great point. We have added some reference on $\\sqrt{n}$-consistent estimator in the smooth case.\n\n\u201cWhat are the advantages of this approach versus the higher order influence function approach, which seems to have the same kinds of guarantees?\u201d\n\nResponse: Thanks for the question. The bias reduction technique in the higher-order influence estimator is efficient but often involves elaborate expressions. In contrast, WUNT is a naive plug-in functional estimator after applying a uniform transformer. Because of the simple form, WUNT does not need an extra bias reduction process but only a carefully selected tuning parameter to reduce the bias. Such a naive plug-in functional estimator with a selected tuning parameter is also used to construct a simple estimator for the integrated squared density (Gine & Nickl, 2008).\n\n\u201cHow do we choose the bandwidth in practice?\u201d\n\nResponse: Thanks for the question. If we know the smoothness, we can use the bandwidth suggested in Theorem 1 and 3. If we do not know the smoothness, we can adopt Lepski\u2019s method (Lepski, 1991; 1992) to select the tuning parameter in a data-driven way. We add the details to select bandwidth in Section C.3 in the appendix.\n\n\u201cIs Theorem 2 a straightforward extension of the Robins et al. (2008) lower bound result? What are the aspects of it that are new? Close comparisons would help here.\u201d\n\nResponse: Thanks for the question. The setting in Robins et al. (2008, 2017) is a bit different from ours, although they are roughly equivalent. In Robins et al. (2017), the illustrative example is from missing data. If we translate it to a causal inference setting, $\\alpha$ and $\\beta$ are the smoothness levels of propensity score and response functions. However, our paper uses the smoothness levels of the probability density function and response functions. Because of different settings, the construction of the least favorable hypothesis is slightly different, so we write out the proof details for completeness. We also want to point out that the proof in our paper highly relies on the results of Robins et al. (2009)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699847652310,
                "cdate": 1699847652310,
                "tmdate": 1699847652310,
                "mdate": 1699847652310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a03LaQARiC",
            "forum": "oOGqJ6Z1sA",
            "replyto": "oOGqJ6Z1sA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_424z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_424z"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method for estimating treatment effects using balancing weights, involving two key steps:\n- a uniform transformation ensuring a uniform distribution density on the weight's denominator\n- a density estimation of the transformed numerator\n\nUnder some regularity conditions and given that the density on the denominator is known, the authors establish an upper bound on the estimator's error rate and show that this rate of convergence is minimax. Additionally, they propose an adaptive uniform transformer that, when combined with the estimator, yields consistent treatment effect estimation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors introduce a novel effect estimation strategy utilizing balancing weights through uniform transformation, together with a new adaptive uniform transformer.\n\n- The paper provides a sharp characterization (in terms of minimax optimality) of effect estimation through uniform transformers, which is new to the literature. \n\n- The paper is clear and well-written."
                },
                "weaknesses": {
                    "value": "- The use of a uniform transformer is not well-motivated. It is not clear to me, from the paper, why a uniform transformation is preferred over an estimator with densities $f_T(X)$ and $f_C(X)$ estimated separately without any transformation. Is it because it is harder to study such an estimator and thus harder to obtain the optimal bandwidth?\n\n- The upper bound in Theorem 1 still relies largely on having accurate knowledge of $f_C$ (and also its smoothness level due to the $\\alpha,\\beta<\\gamma$ constraint), and it doesn\u2019t seem to me that the proposed method with the adaptive uniform transformer is able to achieve the minimax rate in general.\n\n- The minimax optimality results are only for a very restricted class of problems.\n\nAlso see questions."
                },
                "questions": {
                    "value": "- What is the difference between estimating both the densities of $f_T(X)$ and $f_C(X)$, as compared to the proposed strategy? In particular, under the conditions on $f_C(X)$ estimation posed in Corollary 1, is it possible for an estimator without the transformation to achieve the same rate?\n\n- In general, what is the rate that can be achieved using the adaptive transformer? When adding strong enough regularity conditions, can it in general achieve the rate specified in Corollary 1? And what would those conditions be like?\n\n- What is the advantage of using the proposed method, as compared to a conventional doubly robust estimator?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Reviewer_424z",
                        "ICLR.cc/2024/Conference/Submission6688/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6688/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785630474,
            "cdate": 1698785630474,
            "tmdate": 1700658939819,
            "mdate": 1700658939819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "snJKEapsra",
                "forum": "oOGqJ6Z1sA",
                "replyto": "a03LaQARiC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 424z (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading of our paper and constructive suggestions. Here is our response for your comments. \n\n\u201cThe use of a uniform transformer is not well-motivated. It is not clear to me, from the paper, why a uniform transformation is preferred over an estimator with densities $f_T(X)$ and $f_C(X)$ estimated separately without any transformation. Is it because it is harder to study such an estimator and thus harder to obtain the optimal bandwidth?\u201d\n\nResponse: This is a great question. Because of the uniform transformer, the estimator for $\\mu_{CT}$ is just a simple U-statistics, so we can choose a suitable tuning parameter (which also relies on the smoothness level of the response function) to reduce bias. Whether this idea can extend if we estimate $f_T(X)$ and $f_C(X)$ separately is unclear. If we have an estimator for $f_C(X)$, a potential estimator is\n$$\n\\hat{\\mu}_{CT}={\\sum_{i_1,i_2=1}^nY_{i_1}(1-Z_{i_1})\\hat{f}_C^{-1}(X_{i_1})K_H(X_{i_1}- X_{i_2})Z_{i_2} \\over \\sum_{i_1,i_2=1}^n (1-Z_{i_1})\\hat{f}_C^{-1}(X_{i_1})K_H(X_{i_1}- X_{i_2})Z_{i_2}}\n$$\nThe theoretical analysis can be different. This direction deserves further study in the future.\n\nAnother benefit of using a transformer is that Rosenblatt\u2019s transformation relies only on the cumulative distribution function. In some applications, estimating the (conditional) cumulative distribution function is easier than the probability density function. For example, when $d=1$, the uniform transformer can be estimated by an empirical cumulative distribution function, and we do not need to estimate the probability density function.\n\n\u201cThe upper bound in Theorem 1 still relies largely on having accurate knowledge of $f_C(X)$ (and also its smoothness level due to the $\\alpha, \\beta<\\gamma$ constraint), and it doesn\u2019t seem to me that the proposed method with the adaptive uniform transformer is able to achieve the minimax rate in general.\u201d\n\nResponse: Thanks for raising this good point. The proof of Theorem 2 suggests that whether $f_C(X)$ is known in advance or not, the minimax rate of estimating $\\mu_{CT}$ is the same. In addition, the existing minimax optimal estimators also need extra assumptions to achieve the minimax rate. For example, in Robins et al. (2017), Section 3.1 suggests that the higher order influence estimator also needs the function g to be known or easy to estimate (e.g., smooth enough). It is still an open question if the minimax optimal rate can be achieved without these extra assumptions. This is an interesting future direction.\n\n\u201cThe minimax optimality results are only for a very restricted class of problems.\u201d\n\nResponse: This is a valid point. In this paper, we only focus on the estimation of the average treatment effect and show the feasibility of a simple weighting estimator, demonstrating the potential of this idea. We will continue working in this direction and see whether we can extend the idea in simple U-statistics to more general problems."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846500966,
                "cdate": 1699846500966,
                "tmdate": 1699847127781,
                "mdate": 1699847127781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tmuWipeBlG",
                "forum": "oOGqJ6Z1sA",
                "replyto": "a03LaQARiC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 424z (Part 2)"
                    },
                    "comment": {
                        "value": "\u201cWhat is the difference between estimating both the densities of $f_T(X)$ and $f_C(X)$, as compared to the proposed strategy? In particular, under the conditions on $f_C(X)$ estimation posed in Corollary 1, is it possible for an estimator without the transformation to achieve the same rate?\u201d\n\nResponse: Thanks for the question. If we only focus on designing the best estimator for $f_T(X)$ and $f_C(X)$, then plugin them. This strategy could lead to a dominating bias, as argued in Robins et al. (2017). Therefore, when we design the estimator for $f_T(X)$ and $f_C(X)$, our target should be estimating the average treatment effect. It is not immediately clear whether the idea in the paper can be extended if we estimate $f_T(X)$ and $f_C(X)$ separately. If we have an estimator for $f_C(X)$, a potential estimator is\n$$\n\\hat{\\mu}_{CT}={\\sum_{i_1,i_2=1}^nY_{i_1}(1-Z_{i_1})\\hat{f}_C^{-1}(X_{i_1})K_H(X_{i_1}- X_{i_2})Z_{i_2}\\over \\sum_{i_1,i_2=1}^n (1-Z_{i_1})\\hat{f}_C^{-1}(X_{i_1})K_H(X_{i_1}- X_{i_2})Z_{i_2}}\n$$\nThe theoretical analysis can be different. This direction definitely deserves further study in the future.\n\n\n\u201cIn general, what is the rate that can be achieved using the adaptive transformer? When adding strong enough regularity conditions, can it in general achieve the rate specified in Corollary 1? And what would those conditions be like?\u201d\n\nResponse: This is a great question. The exact rate can be found at the end of step 1 in Proof of Theorem 3 (Section D.5). If the adaptive transformer is estimated from a data set with sample size in control $n_{0}$, the condition we need is\n$$\nn_0^{-2\\alpha\\over d}\\le n_1^{-4(\\alpha+\\beta)\\over d+2((\\alpha+\\beta)}.\n$$\nIt is easier to satisfy when the dimension $d$ is smaller.\n\n\u201cWhat is the advantage of using the proposed method, as compared to a conventional doubly robust estimator?\u201d\n\nResponse: Thanks for the great question. Section 5 of Robins et al. (2017) pointed out that conventional doubly robust estimators are essentially first-order estimators. When the underlying functions are non-smooth, Robins et al. (2017) demonstrates that the bias could dominate the estimation error and thus introduce a higher-order influence estimator to reduce bias better. Robins et al. (2017) shows that a higher-order influence estimator can effectively reduce bias to achieve minimax optimality when the underlying functions are non-smooth. Our estimator is also introduced to estimate the average treatment effect and achieve minimax optimality when the underlying functions are non-smooth. Different from a higher-order influence estimator, our proposed method is a weighting method. In a word, our estimator is a more efficient than the conventional doubly robust estimator in the non-smooth setting."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699847108353,
                "cdate": 1699847108353,
                "tmdate": 1699847108353,
                "mdate": 1699847108353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a4yxOohEVs",
                "forum": "oOGqJ6Z1sA",
                "replyto": "tmuWipeBlG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Reviewer_424z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Reviewer_424z"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658910375,
                "cdate": 1700658910375,
                "tmdate": 1700658910375,
                "mdate": 1700658910375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vrWKg6KL6L",
            "forum": "oOGqJ6Z1sA",
            "replyto": "oOGqJ6Z1sA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_oSbf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6688/Reviewer_oSbf"
            ],
            "content": {
                "summary": {
                    "value": "This study proposes a novel framework for average treatment effects (ATEs) estimation by using uniform transformation. The authors develop an estimator that does not employ the exact estimation of the propensity score. Then, they show the finite-sample nonparametric minimax optimality for the estimator (but I think the estimator is not nonparametric in a usual sense...?) and asymptotic distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Firstly, I note that I could not grasp the contributions of this study, especially the novelty of the proposal of the uniform transformer. Therefore, I could not assess this study well. If possible, I want to deepen my understanding and clarify the contributions of this study through an interaction with the authors.\n\nThe reasons why I could not understand the contributions are based on the following three points.\n\n1. Necessity of the exact estimation of the propensity score.\nIn the 2000s, there were enormous arguments about what kind of weighting functions are effective in ATE estimation. As a result of long arguments, researchers found that using the **true** propensity score does not achieve the asymptotic lower bound proposed by Hahn (1998). That is, some finite-sample bias improves the asymptotic efficiency of estimators, as shown by Hirano et al. (2003). Does this study lie in those literature or completely discuss different topics?\n\n2. Significance of the use of a kerne-based U-statistics.\nTo the best of my knowledge, existing studies such as Hirano et al. (2003) employ kernel-based U-statistics, at least in theoretical analysis. What is the difference from the existing approaches?\n\n3. Minimax rate.\nI could not understand why the authors use the minimax optimality for nonparametric estimators. The estimator used by the authors is typically referred to as a semiparametric estimator, which is characterized by nonparametric and parametric estimators. Then, the authors are interested in the estimator error of the parametric part of the semiparametric estimator. Therefore, I believe that the nonparametric minimax optimal rate is a bit meaningless in this context (in other words, I think that although the authors say that the proposed estimator is nonparametric, the estimator is (semi)parametric). Furthermore, the asymptotic variance in Corollary 1 is not efficient from the viewpoint of the semiparametric efficiency bound proposed by Hahn (1998). Is the estimator really efficient or minimax optimal?\n\nAdditionally, I could not understand connections to existing estimators whose asymptotic variance aligns with the semiparametric efficiency bound; that is, the authors' estimator cannot be more efficient than the existing efficient estimators. Overall, I could not understand the author's intent in this study.\n\nMy assessment may be biased my knowledge of existing literature of this topic, and I am not confident on my assessment. I hope that I can deepen the understanding via communication with the authors."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6688/Reviewer_oSbf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6688/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699695434012,
            "cdate": 1699695434012,
            "tmdate": 1699695601488,
            "mdate": 1699695601488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a9dN8NTyxt",
                "forum": "oOGqJ6Z1sA",
                "replyto": "vrWKg6KL6L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oSbf (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed and constructive comments. Here is our response for your comments. \n\n\u201cNecessity of the exact estimation of the propensity score. In the 2000s, there were enormous arguments about what kind of weighting functions are effective in ATE estimation. As a result of long arguments, researchers found that using the true propensity score does not achieve the asymptotic lower bound proposed by Hahn (1998). That is, some finite-sample bias improves the asymptotic efficiency of estimators, as shown by Hirano et al. (2003). Does this study lie in those literature or completely discuss different topics?\u201d\n\nResponse: Thanks for the great question. Similar to Robins et al. (2008, 2017), the discussion in this paper assumes we don\u2019t know the true propensity score. Our main point is that the best way to estimate weights or propensity scores does not necessarily lead to the best estimator for the average treatment effect when the probability density functions and response functions are not smooth. In the non-smooth setting, we need to carefully balance the bias and variance in the final estimator for the average treatment effect. \n\n\u201cSignificance of the use of a kerne-based U-statistics. To the best of my knowledge, existing studies such as Hirano et al. (2003) employ kernel-based U-statistics, at least in theoretical analysis. What is the difference from the existing approaches?\u201d\n\nResponse: Thanks for the question. In Hirano et al. (2003), the propensity score is estimated by a Series Logit Estimator (SLE), which is then used in the weighting method. Most previous literature focuses on a setting where probability density functions/propensity score and response functions are smooth enough. For example, Hirano et al. (2003) assumes the propensity score is continuously differentiable of order $s\\ge 7d$ (Assumption 4). In Hahn (1998), propensity score and response functions are assumed to be continuously differentiable of all orders (Assumption (iv) in Theorem 6). Chan et al. (2016) assumes that the propensity score is $s$-times continuously differentiable, where $s > 13d$, and the response function is $t$-times continuously differentiable, where $t> 3d/s$. Our paper's discussion mainly focuses on a setting where the sum of the smoothness levels of probability density and response functions is smaller than $d/2$. Whether these methods can still work effectively in the non-smooth setting is unclear. Besides our proposed method, another estimator designed for a non-smooth setting is the higher-order influence estimator introduced by Robins et al. (2017). The bias reduction technique in the higher-order influence estimator involves elaborate expressions. In contrast, our proposed method is a simple functional estimator after applying a uniform transformer. Because of the simple form, the new method does not need an extra bias reduction process but only a carefully selected tuning parameter to reduce the bias."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846310099,
                "cdate": 1699846310099,
                "tmdate": 1699846310099,
                "mdate": 1699846310099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "929raBwSiH",
                "forum": "oOGqJ6Z1sA",
                "replyto": "vrWKg6KL6L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6688/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oSbf (Part 2)"
                    },
                    "comment": {
                        "value": "\u201cMinimax rate. I could not understand why the authors use the minimax optimality for nonparametric estimators. The estimator used by the authors is typically referred to as a semiparametric estimator, which is characterized by nonparametric and parametric estimators. Then, the authors are interested in the estimator error of the parametric part of the semiparametric estimator. Therefore, I believe that the nonparametric minimax optimal rate is a bit meaningless in this context (in other words, I think that although the authors say that the proposed estimator is nonparametric, the estimator is (semi)parametric). Furthermore, the asymptotic variance in Corollary 1 is not efficient from the viewpoint of the semiparametric efficiency bound proposed by Hahn (1998). Is the estimator really efficient or minimax optimal?\u201d\n\nResponse: Thanks for the question. As we mentioned before, there are two regimes in estimating the average treatment effect: $\\alpha+\\beta>d/2$ (smooth) and $\\alpha+\\beta<d/2$ (non-smooth). In the smooth regime, $\\sqrt{n}$-consistent estimators exist (i.e., the convergence rate is $O(1/\\sqrt{n})$), and there is a very mature literature. For example, Hirano et al. (2003) and Hahn (1998) lie in this regime (the estimators in these two papers need stronger conditions than $\\alpha+\\beta>d/2$). To compare different $\\sqrt{n}$-consistent estimators, we can compare their variance from the viewpoint of the semiparametric efficiency. The semiparametric efficiency bound is an asymptotic lower bound on variances of estimators, so a method that achieves such a lower bound can be called an optimal method. When it comes to the non-smooth regime, $\\sqrt{n}$-consistent estimators do not exist anymore, and the convergence rate of estimators is usually slower than $O(1/\\sqrt{n})$. Minimax optimality is usually used to characterize the optimality of a method in the non-smooth regime (see Robins et al. (2017)). Compared with the smooth regime, relatively few estimators demonstrate good properties in the non-smooth regime. In this paper, we focus on the non-smooth regime, introduce an estimator designed for the non-smooth regime, and show it can achieve a minimax optimal rate in the non-smooth regime.\n\n\u201cAdditionally, I could not understand connections to existing estimators whose asymptotic variance aligns with the semiparametric efficiency bound; that is, the authors' estimator cannot be more efficient than the existing efficient estimators. Overall, I could not understand the author's intent in this study.\u201d\n\nResponse: Thanks for the comments. Again, this paper focuses on a non-smooth setting, while most existing methods are designed for a smooth setting. It is unclear if these methods designed for smooth settings are still efficient in non-smooth settings. This paper aims to develop a method that can work well in a non-smooth setting."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6688/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846360960,
                "cdate": 1699846360960,
                "tmdate": 1699846360960,
                "mdate": 1699846360960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]