[
    {
        "title": "Reflective Policy Optimization"
    },
    {
        "review": {
            "id": "1TJuXDaykM",
            "forum": "OlwW4ZG3Ta",
            "replyto": "OlwW4ZG3Ta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_X1ce"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_X1ce"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new policy optimization method named reflective policy optimization based on expanding surrogate function via condition state-action visitation distributions. This paper also provided a clipped surrogate function(following PPO) for efficient calculating.\nCompared with current policy optimization methods, this reflective policy optimization performs better on many benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a novel method with a tighter bound than TRPO. The clipped optimization objective provided by this method is simple and general enough and can be directly applied to many existing methods to replace the PPO objective. It shows improved or comparable performance in the experiments, however again considering the simplicity and generality of the method, this method is valuable.\n\nOverall, the paper is well-written and easy to follow. Adding a detailed expression of the loss function for k=2 before Theorem 4.2 would have made it easier for me to understand."
                },
                "weaknesses": {
                    "value": "It is better to add some experiments on the environments with discrete action space, e.g. MinAtar[1].\n\nThe gap between the original reflective policy optimization and the clipped version cannot be ignored.\n\nThe method is sensitive to some important hyperparameters, as shown in Figure 4, in Swimmer and Walker2d.\n\n\n[1] Young, K. Tian, T. (2019). MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments. arXiv preprint arXiv:1903.03176."
                },
                "questions": {
                    "value": "In the ablation experiment for k, when k increases the variance of the objective function will increase significantly, which puts forward a high requirement for fine adjustment of $\\epsilon$ and $\\beta$. So I want to know what is the setting of these hyperparameters in the ablation experiment. How to set such hyperparameters to get a reliable result, choosing the right k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6789/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6789/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6789/Reviewer_X1ce"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698426914199,
            "cdate": 1698426914199,
            "tmdate": 1699636784419,
            "mdate": 1699636784419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pBhspB035b",
                "forum": "OlwW4ZG3Ta",
                "replyto": "1TJuXDaykM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer X1ce"
                    },
                    "comment": {
                        "value": "We are profoundly grateful for the time you have taken from your busy schedule to provide a detailed evaluation of our work. We sincerely appreciate your professional feedback. We are committed to addressing the issues raised and are confident that our revisions will significantly improve the contribution of our work to the RL community. \n\n**Question 1**: In the ablation experiment for k, when k increases the variance of the objective function will increase significantly, which puts forward a high requirement for fine adjustment of $\\epsilon$ and $\\beta$. So I want to know what is the setting of these hyperparameters in the ablation experiment. How to set such hyperparameters to get a reliable result, choosing the right k?\n\n**Answer 1**: Your point is valid. In the ablation study, as shown in Fig. 5, when k=3, the objective function contains five hyperparameters, which take the values of ($\\epsilon=0.2, \\epsilon_1=0.1, \\beta_1=0.3, \\epsilon_2=0.1,\\beta_2=0.1$). We fixed $\\epsilon, \\epsilon_1, \\beta_1$ and $\\epsilon_2$, and did not extensively fine-tune the hyperparameters, only $\\beta_2\\in${0.1, 0.3}. Considering the variance factor, we adjust the hyperparameters according to the following rules: $ \\epsilon\\geq\\epsilon_1\\geq\\epsilon_2\\geq \\cdots$ , and $ \\beta_1\\geq\\beta_2\\geq\\cdots$ . In this way, a more reliable result can be obtained. To verify the effectiveness of the  RPO and how to choose k, we conducted experiments on the CliffWalking (see Fig. 2). It was found experimentally that when k = 3, the results obtained were a little better than when k = 2, but the improvement was not significant. Again considering the factor of the number of hyperparameters, we suggest a choice of k of 2.\n\n**Question 2**: (The first point in Weaknesses) It is better to add some experiments on the environments with discrete action space, e.g. MinAtar[1].\n\n**Answer 2**: We sincerely accept your expert guidance. Firstly, we have thoroughly read your reference: Young, K. & Tian, T. (2019). MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments. arXiv preprint arXiv:1903.03176. We have conducted additional experiments in the discrete action environment. We are immensely grateful for the expert guidance you provided on the MinAtar environment, as well as the relevant literature you recommended. The content of the literature has offered us a substantial opportunity for in-depth learning. Following your suggestions, we have extended our experimental scope to the more challenging Atari environment beyond MinAtar, aiming to demonstrate the comprehensive performance of our RPO algorithm. We look forward to your evaluation of these additional experimental results. We randomly selected twelve Atari environments for these experiments. The results of the MinAtar and Atari experiments have been incorporated into the new version of our paper for your review.\n\n**Question 3**: (The second point in Weaknesses) The gap between the original reflective policy optimization and the clipped version cannot be ignored.\n\n**Answer 3**: We also recognize the phenomenon you pointed out, which is indeed critical and important. However, our experiments show that our clipping operation did not significantly alter the optimal policy but rather made the training more stable during policy updates in most environments.\n\n**Question 4**: (The third point in Weaknesses) The method is sensitive to some important hyperparameters, as shown in Figure 4, in Swimmer and Walker2d.\n\n**Answer 4**: We are deeply appreciative of your thorough and professional insights. Indeed, as you have pointed out, we have observed sensitivity to parameter variations in specific scenarios. Through ablation studies, it gives some insight into fine-tuning the parameters of the algorithm. For some environments, we should reduce the parameter $\\beta$ value, reducing the sensitivity of the parameters.\n\n \n\nOnce again, thank you for your thorough review and valuable comments on our work. We look forward to your further guidance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380542822,
                "cdate": 1700380542822,
                "tmdate": 1700383102657,
                "mdate": 1700383102657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YxO5BowynE",
                "forum": "OlwW4ZG3Ta",
                "replyto": "pBhspB035b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_X1ce"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_X1ce"
                ],
                "content": {
                    "title": {
                        "value": "appreciate the reply"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to answer my questions. \nThe additional experiments conducted by the author have assuaged some of my concerns. From the point of view of the simplicity of the method and empirical performance, I am inclined to maintain a positive perspective in my commentary."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632965780,
                "cdate": 1700632965780,
                "tmdate": 1700632965780,
                "mdate": 1700632965780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bO69IrF0Q7",
            "forum": "OlwW4ZG3Ta",
            "replyto": "OlwW4ZG3Ta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_WKkT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_WKkT"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors propose a Reflective Policy Optimization(RPO) that considers state-action pairs for n-steps in the policy optimization algorithm. The paper theoretically shows that the relationship between the performance of two policies depends on the next state-action pair and performance\u2019s lower bound can be similar to Trust Region Policy Optimization(TRPO). Authors prove that as n increases, the solution space shrinks and can be contained in the solution space of the (n-1) steps. The practical implementation is based on Proximal Policy Optimization(PPO), and experimental results show that it has better convergence speed and average return performance than the baseline algorithm in cliff-walking and mujoco environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) For two policies, authors propose to maximize a generalized lower bound that directly takes into account that policy performance is related to the next state-action pair. In particular, they interestingly prove that the optimized policy is reflective through some theory and show that TRPO is a special case of this method, which allows the policy to be monotonically improved.\n2) In this paper, authors consider multi-step RL directly from the policy optimization perspective, which is different from the previously proposed multi-step value estimation. In addition, for the practical implementation of the proposed algorithm, the objective function based on PPO was proposed and the correlation between hyperparameters was explained.\n3) In the cliff walking environment, the proposed algorithm was experimentally shown to fall off the cliff less and reach the goal faster than the existing algorithms, which is consistent with the performance expected by the authors at the beginning of the paper. It also improved the average return and convergence speed compared to the existing algorithms in the mujoco environment, which is commonly tested in policy optimization."
                },
                "weaknesses": {
                    "value": "1) The theorems and proofs proposed in the paper are interesting, but the performance shown in the experiments does not seem to be much different from the performance of the baselines. In particular, there are many mentions of convergence speed, but the results shown do not show a significant improvement over existing methods. It is necessary to show the performance improvement in an environment where the subsequent state-action can be better considered.\n2) The clipping working environment that the authors consider seems a little less relevant to the general field of policy optimization. The proposed environment is more appropriate to be considered in a constrained reinforcement learning or safe reinforcement learning environment where dangerous situations should be avoided. Therefore, the comparison target and baseline algorithm should be the constrained reinforcement learning or safety reinforcement learning algorithm.\n3) In the introduction section of the paper, it is argued that existing policy optimization algorithms do not directly consider the impact of subsequent state-actions in the trajectories. Also, in the example, if the agent repeatedly visits the cliff state, it is dangerous because agent is likely to act to fall off the cliff. However, in policy optimization, since there is a discounted sum of reward term, it is possible to provide feedback on the cliff state and the falling action. Therefore, the motivation of the proposed method is difficult to understand, as it allows us to consider the impact of subsequent state action under the influence of rewards.\n4) The proposed algorithm increases the number of hyperparameters as the value of k increases. The authors claim that k=2,3 is suitable because of the stability of learning, but even with k=3, it requires 5 hyperparameters to adjust the clipping and learning rate. In addition, the experiments show the case of k=2, and the best performing hyperparameters are not consistent across experimental environments. Therefore, it can be said that the algorithm is sensitive to hyperparameters."
                },
                "questions": {
                    "value": "1) Please answer the questions posed in the weaknesses\n2) In Section 4, a new generalized lower bound is defined via Theorem 4.1. In this part, the authors introduce a new surrogate objective function, which has a slightly different semantics than the surrogate objective function in Theorem 3.1, just minus one. Also, they say that they directly optimize the information of current and future state-action pairs, can you explain what the difference is, or can you compare the difference experimentally?\n3) The paper states that it applied multi-step RL directly from a policy optimization point of view, but it seems that Generalized Adversarial Estimation (GAE) was also used when training. could you tell me if there was any correlation between the GAE parameters and multi-step k in experiments?\n4) The paper shows that the convergence speed increases with the value of K, but the performance is only shown in three environments. Can you explain whether the performance varies significantly depending on the difficulty of the environment or the dimensionality of the state and action?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649849527,
            "cdate": 1698649849527,
            "tmdate": 1699636784311,
            "mdate": 1699636784311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EVhAHvNgDN",
                "forum": "OlwW4ZG3Ta",
                "replyto": "bO69IrF0Q7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer WKkT (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful feedback. Receiving such professional guidance from you is indeed a privilege for us. We are committed to addressing the issues raised and are confident that our revisions will significantly improve the contribution of our work to the RL community. We sincerely request that you reconsider your evaluation of our manuscript and consider revising your score accordingly. Your continued support and feedback are invaluable to us.\n\n**Question 1**: (Weaknesses 1) The performance shown in the experiments does not seem to be much different from the performance of the baselines.\n\n**Answer 1**: We are grateful for your query about the performance improvement and apologize for not articulating this clearly in our paper. In our experiments, we found that our method has a significant advantage over the original PPO. However, to showcase the robustness and broad applicability of our approach, we did not perform specific parameter tuning. The results post-tuning will be detailed in the paper's appendix to transparently exhibit RPO\u2019s performance. In Fig. 3, our method is compared not only with the PPO algorithm but also with some algorithms that have modified PPO from an on-policy to an off-policy approach (GePPO and OTRPO) and it is found that our method is also advantageous. Figure 1 shows that the direct use of subsequent data plays a positive role and our approach can be seamlessly integrated into existing algorithm frameworks.\n\nWe once again apologize for any confusion caused by our lack of clarity in conveying that specific parameter tuning was not conducted to demonstrate the robustness and general adaptability of our method. Please forgive any inconvenience this may have caused during your review. Your guidance is invaluable, and we will take greater care in this regard in our subsequent paper submissions.\n\n**Question 2**: (Weaknesses 2) The proposed environment is more appropriate to be considered in a constrained RL or safe RL. \n\n**Answer 2**: Thank you for your approval of our proposed algorithm. Our proposed method serves as a supplement to existing reinforcement learning approaches. It is not only applicable to classic reinforcement learning scenarios but may also play a more significant role in constrained reinforcement learning or safe reinforcement learning.  It also requires further research on this algorithm in order to better apply it in different environments. It will be a point that can be studied in the future.\n\n**Question 3**: (Weaknesses 3) About the motivation of the proposed method.\n\n**Answer 3**: I am very sorry that we did not make this example more straightforward. We will modify it in the introduction. With this example, we want to consider the impact of the subsequent data directly. The value function potentially contains information about the subsequent data. We need to consider the question: Is it the best way to optimize a policy using only value functions? The answer is definitely not. There is another way to optimize the current policy by directly using the subsequent data, unlike optimizing the policy using only the value function. Intuitively, the direct use of the subsequent data may speed up the algorithm's convergence  and improve sample efficiency. In the subsequent sections of this paper, we verify this intuition theoretically and experimentally, respectively.\n\n**Question 4**: (Weaknesses 4) Sensitivity of hyperparameters.\n\n**Answer 4**: Fine-tuning the hyperparameters of our algorithm indeed leads to improved performance. In the main experiment (as shown in Figure 3), we fixed the hyperparameters without fine-tuning  for a fair comparison. From Fig. 3, even with fixed hyperparameters, our approach achieves good results compared with PPO and the off-policy version of PPO. \n\n**Question 5**: (Original question 2) About theorem 3.1 and 4.1, can you explain what the difference is, or can you compare the difference experimentally?\n\n**Answer 5**: Consider $L_1(\\pi, \\hat{\\pi})$ of the theorem 3.1 as an example. If the environment is unknown, it can only be optimized by sampling. If we sample a trajectory $\\tau$, and $(s_0, a_0, s_1, a_1)\\in\\tau$, if $ A^{\\hat{\\pi}}(s_1, a_1)<0 $ and $ r_0-1 <0$, we know that $ (r_0-1)r_1 A^{\\hat{\\pi}}(s_1, a_1)>0 $. Optimizing the $ L_1(\\pi, \\hat{\\pi}) $ (considering the extreme case, using one sample), the probability of $ a_1 $ is increased. However, $ A^{\\hat{\\pi}}(s_1, a_1)<0 $, we should decrease the probability of $ a_1 $. It's a contradiction. Thus this term \"1\" of $ r_0-1 $ may adversely affect policy optimization though the theory is sound. This situation exists when the environment is unknown. But for $\\hat{L}_1(\\pi, \\hat{\\pi})$ in the theorem 4.1, there is no such problem. We also experimentally compared these two methods, *i.e.*, RPO vs.* TayPO (with the term \u201d1\"). The possibility of what we said earlier is also verified from the experiment."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379771534,
                "cdate": 1700379771534,
                "tmdate": 1700379771534,
                "mdate": 1700379771534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tu6L33ZMr7",
                "forum": "OlwW4ZG3Ta",
                "replyto": "bO69IrF0Q7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer WKkT (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Question 6**: (Original question 3) Could you tell me if there was any correlation between the GAE parameters and multi-step k in experiments?\n\n**Answer 6**: In this paper, GAE is evaluated in the same way as PPO, *i.e.*, $\\hat{A}^{GAE}_t=\\delta^t+(\\gamma \\lambda)\\delta^{t+1}+\\cdots+(\\gamma \\lambda)^{T-t+1} \\delta^{T-1}  $,\n\n where $ \\delta^t=r_t+\\gamma V(s_{t+1})-V(s_t)$.\n\nAssume that the sampled trajectory is $\\tau = (s_0,a_0,s_1,a_1, \\cdots, s_t, a_t, \\cdots)$, we can calculate the advantage functions ($\\hat{A}^{GAE}_0, \\hat{A}^{GAE}_1, \\cdots, \\hat{A}^{GAE}_t, \\cdots$). \n\nWhen k=2, if we sample ($s_t, a_t, s_{t+1}, a_{t+1}$), the current policy is optimized by ($\\hat{A}^{GAE}_t$, \n\n$\\hat{A}^{GAE}_{t+1}$). It can be seen that the relationship between them is that when k is fixed, the number of advantage functions is also fixed.\n\n**Question 7**: (Original question 4)   The performance is only shown in three environments about with the value of k.\n\n**Answer 7**: Previously, due to limited resources and time, we did not perform parameter tuning for k=3. We only ran some experiments in three environments and included these results in our paper. Now, we have supplemented this experiment in the Appendix."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380449729,
                "cdate": 1700380449729,
                "tmdate": 1700382810236,
                "mdate": 1700382810236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m1rQ8sbLVM",
                "forum": "OlwW4ZG3Ta",
                "replyto": "Tu6L33ZMr7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_WKkT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_WKkT"
                ],
                "content": {
                    "comment": {
                        "value": "To the author,\n\nThank you for taking the time to address the shortcomings and questions raised. This helped me to understand the paper.\n\nSincerely."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568404622,
                "cdate": 1700568404622,
                "tmdate": 1700568404622,
                "mdate": 1700568404622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HoXnA3sb6q",
                "forum": "OlwW4ZG3Ta",
                "replyto": "bO69IrF0Q7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Some additional supplementary experiments."
                    },
                    "comment": {
                        "value": "Thanks again for your time and effort in reviewing our paper! To further allay your concerns, we add some Atari experiments. We remain open to any further feedback and are committed to making additional improvements if needed. \n\nTable 1 Comparison of RPO and PPO performance at 60% timesteps in MuJoCo.\n| Environments |  Timesteps   |    RPO     |   PPO    |\n| :----------: | :---------: | :--------: | :------: |\n| HalfCheetah  | 0.6 million | **3100.0** |  2190.4  |\n|   Reacher    | 0.6 million |  **-5.7**  | **-5.7** |\n|   Swimmer    | 0.6 million | **155.1**  |  133.9   |\n|   Walker2D   | 0.6 million | **3855.8** |  3402.6  |\n|    Hopper    | 0.6 million | **3206.0** |  2720.3  |\n|   Humanoid   | 3.0 million | **5349.3** |  4835.4  |\n\nTable 2 Comparison of RPO and PPO performance at 60% timesteps in Atari.\n| Environments |  Timesteps  |     RPO     |    PPO    |\n| :----------: | :--------: | :---------: | :-------: |\n|   Asterix    | 30 million | **9879.4**  |  8346.6   |\n|  BattleZone  | 30 million | **29640.7** |  21862.5  |\n|    Boxing    | 30 million |  **98.3**   |   87.5    |\n|   Breakout   | 30 million |  **300.8**  |   200.4   |\n|  Centipede   | 30 million | **5752.2**  |  3246.5   |\n|  DoubleDunk  | 30 million |  **-5.3**   |   -7.7    |\n|    Enduro    | 30 million |    311.5    | **371.3** |\n| FishingDerby | 30 million |  **30.2**   |   24.6    |\n|   Kangaroo   | 30 million | **9974.6**  |  8907.4   |\n|   Phoenix    | 30 million | **28538.1** |  20409.7  |\n|    Pong    | 30 million |  **21.0**   |   **21.0**    |\n|    Qbert     | 30 million | **14308.4** |  13933.0  |\n\nIn order to better present the advantages of this method RPO over PPO, in addition to our experiments on MuJoCo, we randomly selected 12 Atari environments for our experiments (runs 50 million, 3 seeds).\nAs shown in Tables 1 and 2, our proposed algorithm RPO achieves better results compared to PPO at the 60% timesteps in MuJoCo and Atari.  For more information, please see the appendix."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576229814,
                "cdate": 1700576229814,
                "tmdate": 1700581031517,
                "mdate": 1700581031517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GSp4rPiYgj",
            "forum": "OlwW4ZG3Ta",
            "replyto": "OlwW4ZG3Ta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers policy optimization from the trajectory perspective and proposes to optimize polices with the information from both the current state-action and the subsequent state-action pairs. To achieve this, the paper unfolds the performance improvement lemma over time steps to get the unrolled surrogate objective, i.e., the generalized surrogate, and a \u201cresidual\u201d term. The paper then shows the \u201cresidual term\u201d can be well bounded, yielding a policy improvement strategy: by iteratively pushing up the lower bound. The paper then use the same clipping scheme from the PPO and forms the Reflective Policy Optimization (RPO). This method has been evaluated on Mujoco control benchmarks and showed good performance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "### Originality\nThe idea of consider the subsequent state-action pairs in the policy optimization sounds interesting and novel. The theory (if correct) can add new insights into the policy gradient methods when considering unrolling it for a long horizon."
                },
                "weaknesses": {
                    "value": "### Quality & significance\n**There can be some technical misstatement & errors in the main contributions**.\n\nFirst, the following statement can be erroneous: \u201cIf you recombine the above equation $(r_0-1)r_1 A^{\\hat{\\pi}}(s_1, a_1)]\\cdot r_1 >0$, optimizing it will be found to increase the probability of $a_1$ . However, $A_{\\hat{\\pi}}(s_1 , a_1) < 0$, we should decrease the probability of $a_1$ . This would present a contradiction.\u201d **Optimizing this objective can result in a bit more complicated consequence than what\u2019s been stated here**: since the policy in both $r_0$ and $r_1$ is parameterized with a same set of parameters, optimizing the objective may increase the probability of $a_0$ and thus tip over the sign of $r_0-1$.  The problem is $a_0$ and $a_1$ are from the same parametrized policy. \n\n\nSecond, **the monotonic improvement statement of Theorem 4.1 can be erroneous**. Consider a special case of $k=1$, which yields the TRPO bound as suggested by the paper (assume this statement is correct), then the lower bound will be $\\alpha_0 \\hat{L}_0(\\pi, \\hat{\\pi}) - \\hat{C}_1(\\pi, \\hat{\\pi})$, \n\nwhich equals to $E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [r_0 A^{\\hat{\\pi}}(s_0, a_0)] - \\frac{\\gamma R_{max}}{(1-\\gamma)^3}||\\pi - \\hat{\\pi}||_1^2$. This is the surrogate objective implied by Theorem 4.1 and should be improved upon. However, **this surrogate objective is strictly negative for $\\gamma=0.995$ used by this paper**: consider the first term\n\n$E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [r_0 A^{\\hat{\\pi}}(s_0, a_0)] = E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [ (r_0-1) A^{\\hat{\\pi}}(s_0, a_0)] $ (this is because the advantage is defined on $\\hat{\\pi}$ and the integral over $\\hat{\\pi}$ will be zero. Hence, \n\n$E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [ (r_0-1) A^{\\hat{\\pi}}(s_0, a_0)] $ \n\n$\\leq \\frac{  ||\\pi - \\hat{\\pi}||_1^2 Rmax }{1-\\gamma} $ (this is a direct result from the proof of Corollary A.2 in appendix, i.e., $G_1$). \n\nThus, \n\n$E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [r_0 A^{\\hat{\\pi}}(s_0, a_0)] - \\frac{\\gamma R_{max}}{(1-\\gamma)^3}||\\pi - \\hat{\\pi}||_1^2$\n\n$\\leq \\frac{  ||\\pi - \\hat{\\pi}||_1^2 Rmax }{1-\\gamma} - \\frac{\\gamma Rmax}{(1-\\gamma)^3}||\\pi - \\hat{\\pi}||_1^2$\n\n$\\leq \\frac{  ||\\pi - \\hat{\\pi}||_1^2 Rmax }{1-\\gamma} [ 1- \\frac{\\gamma}{(1-\\gamma)^2}] < 0$ (for $\\gamma =0.995)$\n\nSo, the maximum value of the lower bound is strictly negative. **It turns out to be impossible to get \"a monotonically improving sequence of policies $\\{\\pi_i\\}_{i=1}^{\\infty}$ satisfying $\\eta(\\pi_0) \\leq \\eta(\\pi_1) \\leq ...$\" for this case $k=1$**.\n\nPlease correct me if my understanding is wrong. \n\n### Clarity\nThere are some confusing points and unclear definitions. See in my questions."
                },
                "questions": {
                    "value": "1. I don\u2019t see the point in the examples of \u201ccliff\u201d and \u201ctreasure\u201d states in the second paragraph of Introduction Section. Would the value function just suffice whether a state is favourable?\n\n2. Most of the citations are using a wrong format. It seems that the author misused the citep & citet. \n\n3. there is a notation issue in the second line of $\\eta(\\pi) - \\eta(\\hat{\\pi})$. The first term on RHS should be with (s_0, a_0).\n\n4. The $||\\pi - \\hat{\\pi}||_1$ has not been defined in the paper. \n\n5. The definition of the conditional occupancy measure is a bit confusing. From the proofs in the appendix, this conditional occupancy measure is a distribution with the conditions on the initial state-action distribution, i.e., the stationary state distribution induced from a predefined initial state distribution. Then I\u2019m not sure if $(s_0, a_0)\\sim\\rho$ and $(s_i, a_i)\\sim\\rho(\\cdot|s_{i-1}, a_{i-1})$ can be defined coherently. \n\nSome language issues (those do not affect my assessment)\n* \u201cbetween the performance of \\pi and \\hat{\\pi} from a trajectory-based.\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761722788,
            "cdate": 1698761722788,
            "tmdate": 1699636784151,
            "mdate": 1699636784151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ub5D46AjtX",
                "forum": "OlwW4ZG3Ta",
                "replyto": "GSp4rPiYgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer KkoJ  (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful feedback. We greatly appreciate your constructive suggestions, which have provided valuable guidance for enhancing the quality of our manuscript. We are committed to addressing the issues raised and are confident that our revisions will significantly improve the contribution of our work to the RL community. We sincerely request that you reconsider your evaluation of our manuscript and consider revising your score accordingly. Your continued support and feedback are invaluable to us.\n\n**Question 1**: About the examples of \u201ccliff\u201d and \u201ctreasure\u201d stated in the second paragraph of the Introduction Section. Would the value function suffice whether a state is favorable?\n\n**Answer 1**: I am very sorry for not making this example clear. We will modify it in the introduction. Let's briefly restate the example. Consider an environment with a \"cliff.\" what would an agent do if it performed an action under a state and fell into a \"cliff\"? This action is dangerous. Meanwhile, this state might also be dangerous due to the fact that the next time the agent reaches this state again, it is likely to perform the same action. Hence, the agent also needs to avoid getting to this state again and keep out of this state as much as possible. The previous action, when reaching this state, the previous action also needs to directly avoid being performed because it is possible to fall into that state again. The same result is found for the \"treasure\" environment.  Hence, it is necessary to optimize the previous action directly with the subsequent state-action pairs information, not only through the value function, thereby improving sample efficiency. \n\nYes, this way is feasible if the policy is optimized using the value function of the current state due to the fact that the value function potentially contains information about the subsequent states. We need to consider the question: Is it the best way to optimize a policy using only value functions? The answer is definitely not. What we are trying to convey with this example is that there is another, better way to optimize the current policy by directly using the subsequent data, unlike optimizing the policy using only the value function. Intuitively, the direct use of the subsequent data may speed up the convergence of the algorithm and improve sample efficiency. In the subsequent sections of this paper, we verify this intuition theoretically and experimentally, respectively.\n\n**Question 2** : About the citep \\& citet.\n\n**Answer 2**: Thank you very much for pointing this out, we revise it in the next version.\n\n**Question 3** : In the second line of $\\eta(\\pi)-\\eta(\\hat{\\pi})$, the first term on RHS should be with ($s_0, a_0$).\n\n**Answer 3**: Thank you very much, we revise it in the next version.\n\n**Question 4** : The $\\|\\pi-\\hat{\\pi}\\|_1 $ has not been defined in the paper.\n\n**Answer 4**: Thank you very much, we revise it. $\\|\\pi-\\hat{\\pi}\\|_1$  is defined as $\\max_s\\sum_a|\\pi(a|s)-\\hat{\\pi}(a|s)|$.\n\n**Question 5** : About $ (s_0, a_0)\\sim \\rho $, and $ (s_i, a_i)\\sim \\rho(\\cdot|s_{i-1},a_{i-1}) $.\n\n**Answer 5**: I'm very sorry, I didn't make that clear. Let us explain in detail below. From the definition of $ \\rho(s) $ in Preliminaries Section, we see that $ \\mathbb{P}(s_t=s|\\rho_0, \\pi) $ has Markov properties. So, $ \\rho^{\\pi}(s) $ has the same properties. From the proof of Theorem A.1 (see appendix), the link between the performance difference of $ \\pi $ and $ \\hat{\\pi} $ and state visit distribution and conditional state visit distribution has been established. We know that $ \\rho(s_i|s_{i-1}, a_{i-1})= \\rho(\\cdot|s_{i-1}, a_{i-1}, s_{i-2}, a_{i-2}, \\cdots, s_0, a_0)$, which is the conditional distribution under $ s_{i-1} $ and $ a_{i-1} $, and $ s_{i-1}\\sim\\rho(\\cdot|s_{i-2}, a_{i-2}) $ and $ a_{i-1}\\sim \\pi(\\cdot|s_{i-1}) $. For example, given the distributions of $ \\rho(s_0) $ and $ \\pi(a_0|s_0) $, $ \\rho(s_1|s_{0}, a_{0}) $ must exist according to Bayesian theory, but the exact form is difficult to give directly. Their relationship is $ \\rho(\\cdot) = \\int \\rho(\\cdot|s_0, a_0) \\rho(s_0) \\pi(a_0|s_0) ds_0da_0 $. For the general case, we have $\\rho(\\cdot) = \\int \\rho(\\cdot|s_{i-1}, a_{i-1})\\rho(s_{i-1}|s_{i-2}, a_{i-2})\\pi(a_{i-1}|s_{i-1})\\cdots \\rho(s_0) \\pi(a_0|s_0) ds_{i-1}da_{i-1}\\cdots ds_0da_0 $."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379185552,
                "cdate": 1700379185552,
                "tmdate": 1700379185552,
                "mdate": 1700379185552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VMk4lDpNVF",
                "forum": "OlwW4ZG3Ta",
                "replyto": "GSp4rPiYgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer KkoJ  (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Question 6** : (Weaknesses) about $ (r_0-1)r_1 A^{\\hat{\\pi}}(s_1, a_1) = [(r_0-1) A^{\\hat{\\pi}}(s_1, a_1)]\\cdot r_1>0 $ ... present a contradiction. Optimizing this objective can result in a bit more complicated consequence than what\u2019s been stated here. \n\n**Answer 6**: Your point is valid. From the perspective of parameter optimization, optimizing this objective be considered a more complex situation. In this paper, we consider this function intuitively without focusing on the specific form of the parameters. When the model is unknown, we optimize this objective function by sampling and give intuitively the possible problems which there is no way to avoid. \n\n**Question 7** : (Weaknesses) about the monotonic improvement statement of Theorem 4.1.\n\n**Answer 7**: Thank you for providing a proof, the approach of proof is correct. We have identified a flaw in one of the proof steps, *i.e.*, it is not true that $ E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}}[(r_0-1) A^{\\hat{\\pi}}(s_0, a_0)]\\frac{\\|\\pi-\\hat{\\pi}\\|_1^2R\\max}{1-\\gamma}$, \n\nbut rather that $ E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}}[(r_0-1) A^{\\hat{\\pi}}(s_0, a_0)] \\leq \\frac{\\|\\pi-\\hat{\\pi}\\|_1 R\\max}{1-\\gamma}$.\n\nWe can see that $ G_1(\\pi, \\hat{\\pi}) = E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}(\\cdot)}(r_0-1) E_{s_{1}, a_{1} \\sim \\rho^{\\pi}(\\cdot|s_{0}, a_{0})}A^{\\hat{\\pi}}(s_1, a_1) $ is different from $ E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}}[(r_0-1) A^{\\hat{\\pi}}(s_0, a_0)]$. \n\nTherefore, the results of $ G_1 $ cannot be used directly. We give the complete proof below. \n$ E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}}[(r_0-1) A^{\\hat{\\pi}}(s_0, a_0)] - \\frac{ \\gamma R\\max}{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2\\leq \\frac{ R\\max}{1-\\gamma}\\|\\pi-\\hat{\\pi}\\|_1 -\\frac{ \\gamma R\\max}{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2 $. \n\nWhen $ 0<\\|\\pi-\\hat{\\pi}\\|_1<\\frac{1-\\gamma}{\\gamma} $, the upper bound is greater than zero. So the lower bound of performance has solutions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379672041,
                "cdate": 1700379672041,
                "tmdate": 1700379672041,
                "mdate": 1700379672041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SffbKlImvd",
                "forum": "OlwW4ZG3Ta",
                "replyto": "VMk4lDpNVF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Re the answer to Q6, I don't think the authors address my concern. This is actually a crucial motivating example to introduce the \"reflective\" idea. A rigorous and flawless descriptions should be given. \n\nRe the answer to Q7, I think my confusion point was the definition of $\\rho(\\cdot|s_{i-1}, a_{i-1})$ (thank you for clarifying this but I'm still not confident in this definition). Anyway, the authors' new derivation appears to be wrong to me. Specifically, given the authors' bound on the first term, I update the bound as follows \n\n$E_{s_0, a_0\\sim\\rho^{\\hat{\\pi}}} [r_0 A^{\\hat{\\pi}}(s_0, a_0)] - \\frac{\\gamma R_{max}}{(1-\\gamma)^3}|\\pi - \\hat{\\pi}|_1^2$\n\n$\\leq \\frac{  |\\pi - \\hat{\\pi}|_1 Rmax }{1-\\gamma} - \\frac{\\gamma Rmax}{(1-\\gamma)^3}|\\pi - \\hat{\\pi}|_1^2$\n\n$= \\frac{Rmax}{1-\\gamma}( |\\pi - \\hat{\\pi}|_1 - \\frac{\\gamma}{(1-\\gamma)^2}|\\pi - \\hat{\\pi}|_1^2)$ \n\nThis quadratic form takes the maximal when $|\\pi - \\hat{\\pi}|_1 = \\frac{(1-\\gamma)^2}{2\\gamma}\\approx 1e^{-5}$ for $\\gamma=0.995$. Further note that $|\\pi - \\hat{\\pi}|_1 = \\max_s \\sum_a|\\pi(a|s) - \\hat{\\pi}(a|s)|$. Not sure how the policy can be updated from $\\hat{\\pi}$ to $\\pi$ given that the maximum update over all state space is bound by an infinitesimal value $1e^{-5}$."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605320161,
                "cdate": 1700605320161,
                "tmdate": 1700605320161,
                "mdate": 1700605320161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CbxRNbEHrB",
                "forum": "OlwW4ZG3Ta",
                "replyto": "GSp4rPiYgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We greatly appreciate your constructive suggestions, which have provided valuable guidance for enhancing the quality of our manuscript. We will answer your questions as best we can. Lastly, we thank you for taking the time to engage with our work.\n\n**Re-Question 6**:  Rigorous and flawless descriptions should be given as an example.\n\n**Re-Answer 6**:  I am very sorry for not making this example clear. I've added a more rigorous description in our manuscript, and we've explained it to you to make it easier for you to understand. \n\nConsider $L_1(\\pi, \\hat{\\pi})$ in Eqn. (2) as an example.  We consider this function without focusing on the specific form of the parameters. When the environment is unknown, it can only be optimized by sampling. Considering the extreme case, the function  $L_1(\\pi, \\hat{\\pi})$ is optimized by using a sample $(s_0, a_0, s_1, a_1)$, *i.e.*, $L_1(\\pi, \\hat{\\pi})\\approx(r_0-1)r_1 A^{\\hat{\\pi}}(s_1, a_1) $.\n\nIf $ A^{\\hat{\\pi}}(s_1, a_1)<0 $ and $ r_0-1 <0$, we know that $ (r_0-1)r_1 A^{\\hat{\\pi}}(s_1, a_1)=[(r_0-1)A^{\\hat{\\pi}}(s_1, a_1)]r_1>0 $. The probability of $ a_1 $ is increased. However, when $ A^{\\hat{\\pi}}(s_1, a_1)<0 $, we should  decrease the probability of $ a_1 $. It's a contradiction. Thus, this term \"1\" of $ r_0-1 $ may adversely affect policy optimization, though the theory is sound. This situation exists when the environment is unknown. \n\n**Re-Question 7** : (Weaknesses) About the monotonic improvement statement of Theorem 4.1.\n\n**Re-Answer 7**:  Firstly, we define $\\rho(\\cdot|s, a)$ from another perspective.\n\nInspired by discount state visitation distribution $\\rho(\\cdot)$, *i.e.*, \n$$\\rho^{\\pi}(s) = (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^t\\mathbb{P}(s_t=s|\\rho_0, \\pi),$$\nwe can similarly  give a definition of $\\rho^{\\pi}(s,a,s')$. It is defined as $$\\rho^{\\pi}(s,a,s') = (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^t\\mathbb{P}(s_t=s,a_t=a,s_{t+1}=s'|\\rho_0, \\pi),$$\nwhere $\\mathbb{P}(s_t=s,a_t=a,s_{t+1}=s'|\\rho_0, \\pi)=\\mathbb{P}(s_t=s|\\rho_0, \\pi)\\pi(a_t=a|s_t=s)P(s_{t+1}=s'|s_t=s,a_t=a),$ Further, we can give the definition of $\\rho^{\\pi}(\\cdot|s, a)$, *i.e.*, $\\rho^{\\pi}(s'|s, a)=\\frac{\\rho^{\\pi}(s, a,s')}{\\rho^{\\pi}(s, a)}$.\n\nBy this way, $\\rho^{\\pi}(s_2|s_1, a_1)=\\frac{\\rho^{\\pi}(s_0,a_0,s_1,a_1,s_2)}{\\rho^{\\pi}(s_1,a_1|s_0,a_0)\\rho^{\\pi}(s_0,a_0)}$. Similarly, we can define $\\rho^{\\pi}(s_i|s_{i-1}, a_{i-1})$.\n\nSecondly, thank you for re-providing proof. The approach of proof is correct. Moreover, we have identified a flaw in one of the proof steps. The penalty term in the lower bound is $\\frac{ \\gamma R\\max }{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2$ instead of $\\frac{ \\gamma R\\max }{(1-\\gamma)^3}\\|\\pi-\\hat{\\pi}\\|_1^2$ (see theorem 4.1 of my paper or theorem 1 of the TRPO paper [1]). So, the quadratic form takes the maximal when $\\|\\pi-\\hat{\\pi}\\|_1=\\frac{1-\\gamma}{2\\gamma}$.\n\nLastly, we are well aware of your concern that $\\|\\pi-\\hat{\\pi}\\|_1$ may be too small. By the analysis of the theorem, we are more interested in the existence of solutions. In practice, as the TRPO paper says, \"the KL divergence is bounded at every point in the state space, ...,  is impractical to solve due to the large number of constraint\".  This issue is familiar to our approach but rather a challenge universally encountered by all lower bound algorithms, *e.g.*, TRPO [1], CPO [2], GePPO [3], and so on. \n\n[1] Schulman J, Levine S, Moritz P,et al. Trust Region Policy Optimization. ICML, 2015.\n\n[2] Achiam J , Held D, Tamar A,et al. Constrained Policy Optimization. ICML, 2017.  \n\n[3] Queeney J , Paschalidis I C,et al. Generalized Proximal Policy Optimization with Sample Reuse. NeurIPS, 2021."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633819394,
                "cdate": 1700633819394,
                "tmdate": 1700634892330,
                "mdate": 1700634892330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BLedR1wIs5",
                "forum": "OlwW4ZG3Ta",
                "replyto": "CbxRNbEHrB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_KkoJ"
                ],
                "content": {
                    "title": {
                        "value": "penalty term"
                    },
                    "comment": {
                        "value": "Why the penalty term is $\\frac{\\gamma Rmax}{(1-\\gamma)^2}|\\pi - \\hat{\\pi}|_1^2$? Based on the Theorem 4.1, if we consider the case $k=1$, the penalty term should be $\\frac{\\gamma Rmax}{(1-\\gamma)^3}|\\pi - \\hat{\\pi}|_1^2$. No?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639113423,
                "cdate": 1700639113423,
                "tmdate": 1700639113423,
                "mdate": 1700639113423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C5Ba11GtGv",
                "forum": "OlwW4ZG3Ta",
                "replyto": "GSp4rPiYgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to penalty term"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We would be honored to respond to each of your queries in detail. Lastly, we thank you for taking the time to engage with our work.\n\nFrom theorem 4.1, when $k=1$, we can obtain the following inequality\n\n$$\\eta(\\pi)-\\eta(\\hat{\\pi})\\geq \\alpha_0 E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}}[r_0 A^{\\hat{\\pi}}(s_0, a_0)]- \\hat{C}_1(\\pi, \\hat{\\pi}),$$\n\nwhere $\\alpha_0=\\frac{1}{1-\\gamma}$ and $\\hat{C}_1(\\pi, \\hat{\\pi})=\\frac{\\gamma R\\max}{(1-\\gamma)^3}\\|\\pi-\\hat{\\pi}\\|_1^2$.\n\nIf the right-hand side of the above inequality is optimized as a whole, the penalty term is $\\frac{\\gamma R\\max}{(1-\\gamma)^3}\\|\\pi-\\hat{\\pi}\\|_1^2$.\n\nIf we omit $\\alpha_0$, the objective function we optimize is given by \n\n$$E_{s_0, a_0 \\sim \\rho^{\\hat{\\pi}}}[r_0 A^{\\hat{\\pi}}(s_0, a_0)]- \\frac{\\gamma R\\max}{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2,$$\n\nso, the penalty term is $\\frac{\\gamma R\\max}{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2$."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641486809,
                "cdate": 1700641486809,
                "tmdate": 1700641540954,
                "mdate": 1700641540954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i1Qky1ED9f",
            "forum": "OlwW4ZG3Ta",
            "replyto": "OlwW4ZG3Ta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called Reflexive Policy Optimization, which the authors claim uses information \u201cfrom trajectory data\u201d more efficiently to optimize a policy. The authors claim that agents have \u201cintrospection\u201d and \u201creflect on prior experience\u201d. The method is claimed to have guaranteed monotonic progress improvement on the original policy performance objective and that it \u201ccontracts the solution space of the optimized policy\u201d, thus \u201cexpediting\u201d the training procedure. Moreover, the authors claim the method is superior to baselines on Mujoco tasks.\n\nI didn\u2019t really understand the motivation for the paper, and I found it very confusing and difficult to follow. I expand in the following sections."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I believe the authors want to say that their method performs a kind of hindsight credit assignment \\citep{harutyunyan2019}, but this is just a hunch. I didn't really understand from the text.\nThe paper appears to be backed by theory and support their superior performance claims with empirical results. Unfortunately, I did not really understand the motivation and method to assess these  properly. Please see below my points of confusion. I am happy to revise my score if the authors the motivation and goal of the paper, and expand on the points below.\n\n\n@article{harutyunyan2019,\n  author       = {Anna Harutyunyan and\n                  Will Dabney and\n                  Thomas Mesnard and\n                  Mohammad Gheshlaghi Azar and\n                  Bilal Piot and\n                  Nicolas Heess and\n                  Hado van Hasselt and\n                  Greg Wayne and\n                  Satinder Singh and\n                  Doina Precup and\n                  R{\\'{e}}mi Munos},\n  title        = {Hindsight Credit Assignment},\n  journal      = {CoRR},\n  volume       = {abs/1912.02503},\n  year         = {2019},\n  url          = {http://arxiv.org/abs/1912.02503},\n  eprinttype    = {arXiv},\n  eprint       = {1912.02503},\n  timestamp    = {Wed, 20 Apr 2022 07:47:18 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-02503.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}"
                },
                "weaknesses": {
                    "value": "First, I do not understand why the authors believe proximal methods do not already account for the true gradient of the policy which also considers the contribution through the stationary distribution.  Indeed, in the original CPI paper \\citep{kakade}, the authors first derive the form of the bound that contains the stationary distribution with the current policy, and not a prior policy, then use a mixture policy to ensure that the new policy is \u201cclose\u201d to the prior one and justify replacing the current stationary distribution with the previous one. However, a lot of work has been done since, and it is known that proximal PG methods \\citep{bhandari21, shani2019, vaswani2021} (and algorithmic implementation \\citep{li2022analytical}, PPO, TRPO, MDPO \\citep{tomar2020}, MPO \\citet{abdolmaleki2018} etc.) are functional-gradient methods \\citep{vaswani2021}, in the sense that the lower bound the algorithms optimize are linearizations of the policy performance objective in the direct policy representation. The functional gradient is $d_{\\pi_t}^\\top Q_{\\pi_t}$. In that sense the stationary distribution that the algorithms use is w.r.t. (with respect to) the previous policy (see \\citet{bandhari2019, vaswani2021, sutton2000, agarwal2019}). As they follow the true policy gradient, these methods are convergent \\citep{bhandari21, vaswani2021, xiao2022, johnson2023}, at least their theoretical versions, under adaptive step sizes. The true policy gradient does take into account the gradient through the stationary distribution.\nI did not understand at all the examples they give, which seem to say that \u201c0\u201d has some special meaning when it comes to the reward. \n\nThe text is very unclear, vague, and grammatically incorrect, which makes it very difficult to follow the authors\u2019 arguments.\nIn the motivating introduction, the authors discuss multi-step methods, but this is completely out of context, and use words which have not been defined and are not scientifically rigorous, like \u201cwe give a nice theory.\u201d, \u201cthe policy can be reflective\u201d, \u201c this empowers the agent to engage in introspection and introduce modifications to its actions within the current state to a certain degree\u201d, \u201csome admirable algorithms\u201d, \u201cdirect optimization of this generalized surrogate objective function may have to be done very carefully\u201d, \u201cRPO efficiently utilizes \u2018good\u2019 experiences, makes adjustments based on \u2018bad\u2019 experiences\u2019\u201d\n\nWith respect to scientific correctness, at some point the authors completely remove a term from the bound saying that \u201cthis term \u201c1\u201d may adversely affect policy optimization. I did not understand that at all. The authors say \u201cfor $k = 1$, the $l_1$ norm constraints are replaced by KL constraints\u201d, but this to me makes no sense at all. \n\nThe notation is also confusing, as it seems the authors reuse \u201cr\u201d to represent the importance sampling ratio between the current policy and a prior one. It is unclear what \u201ck\u201d represents as it is not defined, just implied, and seems very important throughout the paper, with multiple reference and impacting the algorithms\u2019 empirical performance.\n\nThen, I also did not really understand the emphasis on the exploration-exploitation coupling on on-policy methods since the whole point of the paper is to improve off-policy methods like PPO and TRPO. These proximal methods are off-policy algorithms, which is the entire point of the efficiency of the methods, the fact that it can reuse prior experience (from the previous policy) multiple times to minimize the lower bound. Otherwise, on-policy policy gradient methods like \\citet{sutton2000} (or algorithmic implementations with parallel streams of experiences like A3C \\citep{mnih16}, Impala \\citep{espeholt18}) directly use the PGT \\citep{sutton2000} w.r.t the parameter vector and rebuild the linear lower bound at each time-step so they don\u2019t need a lower bound surrogate objective that is quasi-concave and can be optimized for multiple updates. \n\n\nThe authors claim the method maintains mototonic improvement, but it is known that proximal-gradient methods, with adaptive step size have this property \\citep{alfano2023, chen2022, xiao2022, johnson2023}, since they are functional-gradient methods, at least for tabular direct or softmax parametrizations.\n\n@misc{li2022analytical,\n      title={An Analytical Update Rule for General Policy Optimization}, \n      author={Hepeng Li and Nicholas Clavette and Haibo He},\n      year={2022},\n      eprint={2112.02045},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n\n@InProceedings{bhandari21,\n  title = \t { On the Linear Convergence of Policy Gradient Methods for Finite MDPs },\n  author =       {Bhandari, Jalaj and Russo, Daniel},\n  booktitle = \t {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},\n  pages = \t {2386--2394},\n  year = \t {2021},\n  editor = \t {Banerjee, Arindam and Fukumizu, Kenji},\n  volume = \t {130},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {13--15 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v130/bhandari21a/bhandari21a.pdf},\n  url = \t {https://proceedings.mlr.press/v130/bhandari21a.html},\n  abstract = \t { We revisit the finite time analysis of policy gradient methods in the one of the simplest settings: finite state and action MDPs with a policy class consisting of all stochastic policies and with exact gradient evaluations. There has been some recent work viewing this setting as an instance of smooth non-linear optimization problems, to show sub-linear convergence rates with small step-sizes. Here, we take a completely different perspective based on illuminating connections with policy iteration, to show how many variants of policy gradient algorithms succeed with large step-sizes and attain a linear rate of convergence. }\n}\n@article{mnih16,\n  author       = {Volodymyr Mnih and\n                  Adri{\\`{a}} Puigdom{\\`{e}}nech Badia and\n                  Mehdi Mirza and\n                  Alex Graves and\n                  Timothy P. Lillicrap and\n                  Tim Harley and\n                  David Silver and\n                  Koray Kavukcuoglu},\n  title        = {Asynchronous Methods for Deep Reinforcement Learning},\n  journal      = {CoRR},\n  volume       = {abs/1602.01783},\n  year         = {2016},\n  url          = {http://arxiv.org/abs/1602.01783},\n  eprinttype    = {arXiv},\n  eprint       = {1602.01783},\n  timestamp    = {Mon, 13 Aug 2018 16:47:40 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/MnihBMGLHSK16.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@InProceedings{espeholt18,\n  title = \t {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},\n  author =       {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},\n  booktitle = \t {Proceedings of the 35th International Conference on Machine Learning},\n  pages = \t {1407--1416},\n  year = \t {2018},\n  editor = \t {Dy, Jennifer and Krause, Andreas},\n  volume = \t {80},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {10--15 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf},\n  url = \t {https://proceedings.mlr.press/v80/espeholt18a.html},\n  abstract = \t {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.}\n}\n\n@article{vaswani2021,\n  author       = {Sharan Vaswani and\n                  Olivier Bachem and\n                  Simone Totaro and\n                  Robert Mueller and\n                  Matthieu Geist and\n                  Marlos C. Machado and\n                  Pablo Samuel Castro and\n                  Nicolas Le Roux},\n  title        = {A functional mirror ascent view of policy gradient methods with function\n                  approximation},\n  journal      = {CoRR},\n  volume       = {abs/2108.05828},\n  year         = {2021},\n  url          = {https://arxiv.org/abs/2108.05828},\n  eprinttype    = {arXiv},\n  eprint       = {2108.05828},\n  timestamp    = {Wed, 18 Aug 2021 19:45:42 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-05828.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@article{agarwal2019,\n  author       = {Alekh Agarwal and\n                  Sham M. Kakade and\n                  Jason D. Lee and\n                  Gaurav Mahajan},\n  title        = {Optimality and Approximation with Policy Gradient Methods in Markov\n                  Decision Processes},\n  journal      = {CoRR},\n  volume       = {abs/1908.00261},\n  year         = {2019},\n  url          = {http://arxiv.org/abs/1908.00261},\n  eprinttype    = {arXiv},\n  eprint       = {1908.00261},\n  timestamp    = {Fri, 09 Aug 2019 12:15:56 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-1908-00261.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@article{shani2019,\n  author       = {Lior Shani and\n                  Yonathan Efroni and\n                  Shie Mannor},\n  title        = {Adaptive Trust Region Policy Optimization: Global Convergence and\n                  Faster Rates for Regularized MDPs},\n  journal      = {CoRR},\n  volume       = {abs/1909.02769},\n  year         = {2019},\n  url          = {http://arxiv.org/abs/1909.02769},\n  eprinttype    = {arXiv},\n  eprint       = {1909.02769},\n  timestamp    = {Mon, 16 Sep 2019 17:27:14 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-02769.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@article{abdolmaleki2018,\n  author       = {Abbas Abdolmaleki and\n                  Jost Tobias Springenberg and\n                  Yuval Tassa and\n                  R{\\'{e}}mi Munos and\n                  Nicolas Heess and\n                  Martin A. Riedmiller},\n  title        = {Maximum a Posteriori Policy Optimisation},\n  journal      = {CoRR},\n  volume       = {abs/1806.06920},\n  year         = {2018},\n  url          = {http://arxiv.org/abs/1806.06920},\n  eprinttype    = {arXiv},\n  eprint       = {1806.06920},\n  timestamp    = {Mon, 13 Aug 2018 16:48:15 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-1806-06920.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@inproceedings{sutton2000,\n author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},\n volume = {12},\n year = {1999}\n}\n\n@misc{johnson2023,\n      title={Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes}, \n      author={Emmeran Johnson and Ciara Pike-Burke and Patrick Rebeschini},\n      year={2023},\n      eprint={2302.11381},\n      archivePrefix={arXiv},\n      primaryClass={math.OC}\n}\n@misc{xiao2022,\n      title={On the Convergence Rates of Policy Gradient Methods}, \n      author={Lin Xiao},\n      year={2022},\n      eprint={2201.07443},\n      archivePrefix={arXiv},\n      primaryClass={math.OC}\n}\n@article{tomar2020,\n  author       = {Manan Tomar and\n                  Lior Shani and\n                  Yonathan Efroni and\n                  Mohammad Ghavamzadeh},\n  title        = {Mirror Descent Policy Optimization},\n  journal      = {CoRR},\n  volume       = {abs/2005.09814},\n  year         = {2020},\n  url          = {https://arxiv.org/abs/2005.09814},\n  eprinttype    = {arXiv},\n  eprint       = {2005.09814},\n  timestamp    = {Fri, 22 May 2020 16:21:28 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-09814.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@misc{alfano2023,\n\ttitle        = {Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization},\n\tauthor       = {Carlo Alfano and Patrick Rebeschini},\n\tyear         = 2023,\n\teprint       = {2209.15382},\n\tarchiveprefix = {arXiv},\n\tprimaryclass = {cs.LG}\n}\n@inproceedings{chen2022,\n\ttitle        = {Sample Complexity of Policy-Based Methods under Off-Policy Sampling and Linear Function Approximation},\n\tauthor       = {Chen, Zaiwei and Theja Maguluri, Siva},\n\tyear         = 2022,\n\tmonth        = {28--30 Mar},\n\tbooktitle    = {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},\n\tpublisher    = {PMLR},\n\tseries       = {Proceedings of Machine Learning Research},\n\tvolume       = 151,\n\tpages        = {11195--11214},\n\turl          = {https://proceedings.mlr.press/v151/chen22i.html},\n\teditor       = {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},\n\tpdf          = {https://proceedings.mlr.press/v151/chen22i/chen22i.pdf},\n\tabstract     = {In this work, we study policy-based methods for solving the reinforcement learning problem, where off-policy sampling and linear function approximation are employed for policy evaluation, and various policy update rules (including natural policy gradient) are considered for policy improvement. To solve the policy evaluation sub-problem in the presence of the deadly triad, we propose a generic algorithm framework of multi-step TD-learning with generalized importance sampling ratios, which includes two specific algorithms: the $\\lambda$-averaged $Q$-trace and the two-sided $Q$-trace. The generic algorithm is single time-scale, has provable finite-sample guarantees, and overcomes the high variance issue in off-policy learning. As for the policy improvement, we provide a universal analysis that establishes geometric convergence of various policy update rules, which leads to an overall $\\Tilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexity.}\n}"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790624335,
            "cdate": 1698790624335,
            "tmdate": 1699636784018,
            "mdate": 1699636784018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gz0Jrfdxx1",
                "forum": "OlwW4ZG3Ta",
                "replyto": "i1Qky1ED9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer kAky"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. We must clarify, however, that our paper does not address the issues of \"0\" and \"true gradient\" mentioned in your review. Moreover, we have not received specific questions regarding our manuscript. Should you have any particular concerns about our work, we earnestly request that you point out the key aspects of the Weaknesses and any questions. We would be honored to respond to each of your queries in detail. Lastly, we thank you for taking the time to engage with our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378420829,
                "cdate": 1700378420829,
                "tmdate": 1700378420829,
                "mdate": 1700378420829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ODi46R2g9",
                "forum": "OlwW4ZG3Ta",
                "replyto": "Gz0Jrfdxx1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "The other reviews and responses helped me understand the paper somewhat better. \n\nHowever I'm still not completely sure I understand the motivation. Is the point to transform the d_{pi_t} stationary distribution w.r.t. to the previous policy pi_t back into an on-policy distribution, or re-weight the trajectory data in each iteration?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577261946,
                "cdate": 1700577261946,
                "tmdate": 1700577261946,
                "mdate": 1700577261946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E81M4IhGtD",
                "forum": "OlwW4ZG3Ta",
                "replyto": "wVt5BLz6tV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Reviewer_kAky"
                ],
                "content": {
                    "title": {
                        "value": "Re"
                    },
                    "comment": {
                        "value": "Q1: not sure this definition is very specific, mixing Monte-Carlo with TD would be using subsequent experience, but I don't think that is what you are doing, is it? Whether the value function is insufficient should depend on its accuracy, no?\nOnce the objective is linearized and a lower bound surrogate objective function defined, optimizing that to optimality guarantees improvement, and at optimal rate under a specific coefficient/step-size/proximal regularization in front of the KL.\nIf your value function is inaccurate by some epsilon, the sample efficiency quantifies how many calls you need to obtain an epsilon'-accurate policy.  \n\nQ2 & Q3: 'r' is generally used for the reward in RL, I think got confused a few times because I need to reload this conflicting notation again in memory. Why not use any other letter/symbol in the alphabet(s)?\n\nQ2 A linear model of the policy performance is not a lower bound no? Nor is there anywhere any claim of such right? Only a quasi-Newton model can be a lower bound under a specific regularization. Not sure I understand this example?\n\nQ2 & Q3: not sure I understand why you are considering L1 norms and you move between Bregman divergences and L1 norms without careful explanation of these steps\n\nQ4 The adaptive step-size proximal method converges at the gamma rate of policy iteration, which is unimprovable. For sample efficiency, there should be a difference in the number of calls to the environment between a method that uses an eps-inaccurate value function and your version, which I understand improves by using the same data. I assume the difference comes from using the same data between in absence of a model. I would like to understand this point better. Is that where your main contribution is?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664006851,
                "cdate": 1700664006851,
                "tmdate": 1700664006851,
                "mdate": 1700664006851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zrKdjHkZcR",
                "forum": "OlwW4ZG3Ta",
                "replyto": "i1Qky1ED9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6789/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Re"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. I'm sorry I don't particularly understand what you're saying. We will do our best to answer your questions. Lastly, we thank you for taking the time to engage with our work.\n\n**Re-Question 1**: Did you do that mixing Monte-Carlo with TD would be using subsequent experience? Accuracy of the value function?\n\n**Re-Answer 1**: No, my proposed method doesn't use it. Our approach is in line with TRPO [1] and CPO [2]. The key of those method is to not update the policy too much, thus making the algorithm more stable, when the dynamic model is unknown. \n\nBelow, we we illustrate the differences between our proposed approach RPO and TRPO [1].\n\nThe objective function of TRPO [1] (derived rather than adding regular terms):\n\n$$E_{s,a}[\\frac{\\pi(a|s)}{\\hat{\\pi}(a|s)} A^{\\hat{\\pi}}(s, a)]- \\hat{C}_{1}(\\pi, \\hat{\\pi}),$$\n\nwhere  $\\hat{C}_{1}(\\pi, \\hat{\\pi})=\\frac{\\gamma R\\max}{(1-\\gamma)^2}\\|\\pi-\\hat{\\pi}\\|_1^2$.\n\nWhen $k=2$, the objective function of my method RPO (derived rather than adding regular terms):\n\n$$E_{s,a}[\\frac{\\pi(a|s)}{\\hat{\\pi}(a|s)} A^{\\hat{\\pi}}(s, a)]+\\alpha E_{s,a, s',a'}[\\frac{\\pi(a|s)}{\\hat{\\pi}(a|s)}\\frac{\\pi(a'|s')}{\\hat{\\pi}(a'|s')} A^{\\hat{\\pi}}(s', a')] - \\hat{C}_{2}(\\pi, \\hat{\\pi}),$$\n\nwhere $\\alpha$ is a constant.\n\nOur approach adds an additional  term compared to TRPO. It can be seen that our method is very different from previous methods [1,2,3]. The methodology of previous studies [3,4,5] of sample efficiency does not directly apply in my case. \n\nNote that the objective function of TRPO can be obtained in terms of adding the regular term (Bregman divergences, *e.g.*, KL), but the original objective functions of TRPO is derived from policy performance (see paper TRPO [1] and my paper).\n\n**Re-Question 2**: About 'r' and example.\n\n**Re-Answer 2**: Thank you very much for your suggestion. We will use a different symbol to represent it. \n\nWe approach it from an optimization perspective, considering the action probabilities of the policy, and the penalty term prevents the policy updates from being too large. In the TayPO paper [6], they use $L_0(\\pi, \\hat{\\pi})+L_1(\\pi, \\hat{\\pi})$ as an objective function and omit the penalty term. Optimizing this objective function may pose issues. We consider this function without focusing on the specific form of the parameters. When the environment is unknown, it can only be optimized by sampling. Considering the extreme case, the function  $L_1(\\pi, \\hat{\\pi})$ is optimized by using a sample $(s_0, a_0, s_1, a_1)$, *i.e.*, $L_1(\\pi, \\hat{\\pi})\\approx(I_0-1)I_1 A^{\\hat{\\pi}}(s_1, a_1) $.\n\nIf $ A^{\\hat{\\pi}}(s_1, a_1)<0 $ and $ I_0-1 <0$, we know that $ (I_0-1)I_1 A^{\\hat{\\pi}}(s_1, a_1)=[(I_0-1)A^{\\hat{\\pi}}(s_1, a_1)]I_1>0 $. The probability of $ a_1 $ is increased. However, when $ A^{\\hat{\\pi}}(s_1, a_1)<0 $, we should  decrease the probability of $ a_1 $. It's a contradiction. Thus, this term \"1\" of $ I_0-1 $ may adversely affect policy optimization, though the theory is sound. This situation exists when the environment is unknown. \n\n**Re-Question 4**: About \"sample efficiency\" and \"contribution \".\n\n**Re-Answer 4**: I'm guessing you're asking both questions. \n\nSample efficiency refers to how many samples are needed to achieve the same level of performance. In other words, how much algorithm performance can be achieved using the same samples.\n\nFrom theorem 4.2, we shows that the solution space is contracting when $k=2$. Reducing the solution space is beneficial in accelerating the convergence of the algorithm in some sense. Better performance can be achieved in the same number of steps of the algorithm iteration. In other words, with the same number of samples, we achieve better policy performance.\n\nWe generalized the TRPO algorithm, and proposed a new lower bound, combining the current data and subsequent data to optimize the current policy. Theoretical analyses show that our proposed method, in addition to satisfying the monotonic improvement of policy performance, can effectively reduce the solution space of the optimized policy, resulting in speeding up the training procedure of the algorithm. \n\n\n\n\n\n[1] Schulman J, Levine S, Moritz P,et al. Trust Region Policy Optimization. ICML, 2015.\n\n[2] Achiam J , Held D, Tamar A,et al. Constrained Policy Optimization. ICML, 2017.  \n\n[3] Hepeng L and Nicholas C ,et al. An Analytical Update Rule for General Policy Optimization. ICML, 2022.\n\n[4] Jalaj B and Daniel R. On the Linear Convergence of Policy Gradient Methods for Finite MDPs. AISTATS, 2021.\n\n[5] Alekh A and Sham M,et al. Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes. 2020.\n\n[6] Yunhao T,and Michal V,et al. Taylor expansion policy optimization. ICML, 2020."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740152000,
                "cdate": 1700740152000,
                "tmdate": 1700740901995,
                "mdate": 1700740901995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]