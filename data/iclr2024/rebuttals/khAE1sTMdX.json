[
    {
        "title": "Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm"
    },
    {
        "review": {
            "id": "T0mRFZiGtL",
            "forum": "khAE1sTMdX",
            "replyto": "khAE1sTMdX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_4rAY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_4rAY"
            ],
            "content": {
                "summary": {
                    "value": "This study proposes UniMP, a method for enhancing individual user\nexperiences by integrating multi-modal user information\neffectively. It introduces a flexible data format for combining\ndiverse user inputs, fine-grained user modeling, and a multi-task\noptimization approach. UniMP can outperform specialized methods in\nvarious personalization tasks and is particularly effective in\ntransfer learning scenarios with new users and domains. It also\ndemonstrates the ability to generate personalized content and handle\nnoisy multi-modal input. The study highlights the importance of\ncontext reconstruction and token-level re-weighting mechanisms in\nimproving training effectiveness. Overall, UniMP offers a versatile\napproach to multi-modal personalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. UniMP seamlessly integrates multi-modal user data, accommodating\nvarious input types, enhancing personalized recommendations.\n\n2. The model's fine-grained user modeling ensures accurate user\npreference predictions, improving overall performance.\n\n3. UniMP excels in transfer learning, adapting well to new users and\ndomains, providing robust personalized recommendations."
                },
                "weaknesses": {
                    "value": "The study's weakness lies in its perceived lack of novelty in\ncombining image and language learning, which may not be considered\nhighly innovative.\n\nYou claim it's \"multi-modal,\" but it only deals with language and\nimages. Since there's no validation with video or audio, the term\n\"universal\" might be overstated."
                },
                "questions": {
                    "value": "You claim it's \"multi-modal,\" but it only deals with language and\nimages. Since there's no validation with video or audio, the term\n\"universal\" might be overstated. Would it be advisable to revise the\ntitle or abstract to specify \"language and images\" to better reflect\nthe scope?\n\nThe dataset's limitation to Amazon data is acknowledged. To enhance\nthe study's scope, have you considered validating it with datasets\nlike Yelp, where there's a combination of image uploads, reviews, and\nratings? Additionally, could you extend the multi-modal validation to\ninclude video and audio data, such as YouTube viewing history?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Reviewer_4rAY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698541597946,
            "cdate": 1698541597946,
            "tmdate": 1699636618013,
            "mdate": 1699636618013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hNHT9UOhC7",
                "forum": "khAE1sTMdX",
                "replyto": "T0mRFZiGtL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4rAY (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions. We deeply appreciate your thoughtful feedback and are grateful for the time and effort you've invested.\n\n> **Q1:** The dataset's limitation to Amazon data is acknowledged.\n\nThank you for your valuable suggestion. We primarily selected the Amazon dataset as our experimental platform due to its extensive and varied content, including reviews, ratings, images, interactions, and product attributes. This comprehensiveness makes it particularly suited for multi-modal, multi-task evaluations. Recognizing the importance of validating our findings across different datasets, we have also applied our UniMP method to the Netflix [1] and HM [2] datasets. For Netflix, the images are obtained by crawling movie posters from the website. The results, as detailed in our tables, affirm the effectiveness and adaptability of UniMP across these varied datasets.\n\n|              |               |          HM        |                 |          |        Netflix          |                 |\n|:------------:|:---------------:|:----------------:|:---------------:|:---------------:|:----------------:|:---------------:|\n|              |       HR@5      |      NDCG@5      |      MRR@5      |       HR@5      |      NDCG@5      |      MRR@5      |\n|      P5      |      0.0101     |      0.0063      |      0.0046     |      0.0742     |      0.0413      |      0.0316     |\n|     VIP5     |      0.0122     |      0.0118      |      0.0093     |      0.0936     |      0.0589      |      0.0395     |\n|    S3-Rec    |      0.0185     |      0.0121      |      0.0102     |      0.1155     |      0.0632      |      0.0511     |\n|    UniSRec   |      0.0196     |      0.0139      |      0.0107     |      0.1324     |      0.0856      |      0.0644     |\n| UniMP (Ours) | **0.0313** | **0.0206** | **0.0172** | **0.1723** | **0.1196** | **0.1024** |\n\nWhile we are keen to broaden our multi-modal validation to encompass video and audio data, we currently face the limitation of not having access to publicly available datasets containing raw video or audio content. Despite this, our method holds the capability for extension to various other data modalities. For instance, we could process image frames from a video through our vision encoder and apply pooling techniques to derive representations for further analysis.\n\n> **Q2:** The term \"universal\" might be overstated. Would it be advisable to revise the title or abstract to specify \"language and images\" to better reflect the scope?\n\nWe appreciate the constructive feedback and valuable suggestions. We have updated the abstract to explicitly mention 'language and images,' ensuring it more accurately represents the scope of our study. Additionally, we've substituted the term 'universal' with 'unified' throughout the paper, as it more appropriately aligns with our work's focus and scope. We will also update the title to reflect these changes as soon as OpenReview enables this functionality."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348801471,
                "cdate": 1700348801471,
                "tmdate": 1700348801471,
                "mdate": 1700348801471,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uihkLrMOSc",
            "forum": "khAE1sTMdX",
            "replyto": "khAE1sTMdX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_s3tt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_s3tt"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on multi-modal personalization systems where the input consists not only text and ID's also images. The output is also multi-modal output generation. The authors take advantage of LLMS as multi-modal prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has following strengths.\n  - Good experimental study and results\n  - Consider multi-modal multi-tasks"
                },
                "weaknesses": {
                    "value": "- This paper has a lot of things, but lacks of novelty.\n- All the components are already in the literature, bridging pre-trained vision and language models is not new, cross attention idea etc."
                },
                "questions": {
                    "value": "- What is x (the visual input of the item), how do you provide it in eq 1? Apart from the special token [IMG]?\n- If you are already providing the visual input in user's interaction history, you are using the same image in visual encoder again into cross attention module? (As far as l understand, no visual information is added to the user's history, the features come from visual encoder, if this is correct, l suggest that seperate image from the user's history in Fig 1.)\n- What is s in page 3 Section 2.1 line 5?\n\nTypos\n\nPage 2, traditional methods suffers -> suffer\npage 4, line 2,  infomration -> information\npage 4, Section 2.2, the visual input -> The visual input"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Reviewer_s3tt",
                        "ICLR.cc/2024/Conference/Submission5845/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750329146,
            "cdate": 1698750329146,
            "tmdate": 1700731912166,
            "mdate": 1700731912166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZjCVIpgvqT",
                "forum": "khAE1sTMdX",
                "replyto": "uihkLrMOSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s3tt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions. We appreciate the opportunity to make the following clarifications.\n\n> **Q1:** Good experimental study and results. Consider multi-modal multi-tasks. But lack of novelty. The components are shown in the literature.\n\nWe greatly appreciate your recognition of our work, particularly our comprehensive experimental study, the robust results obtained, and our proposal of a multi-modal, multi-task approach. In terms of novelty, we are the first to propose the problem of unified multi-modal personalization for various tasks. Our method addresses the inherent challenges through a comprehensive approach that encompasses three complementary aspects: the formulation of input data, the fusion of modalities in network modeling, and the design of optimization objectives. The novelty of our work lies in the unique solution to each perspective and the seamless integration of these components to formulate an overall effective strategy. Next we will elaborate on how each component has been designed and adapted to create a distinctive approach.\n\nFor user input modeling, we are the first to emphasize and verify the feasibility of utilizing raw multi-modal content, as opposed to relying solely on single-modal data or product IDs to represent user sequences. This choice of content acts as a bridge, facilitating transferability and generalization across new domains and users. The comparative analysis of generalization capabilities, presented in Table 4 of our paper, highlights the strengths of our method. \n\nFor modal fusion within the network, while bridging pre-trained vision and language models and the concept of cross-attention is not new, our implementation and the specific challenges we address in our paper are unique. In previous works, CLIP [1] processes visual and textual data separately through modality-specific encoders and only combines them at the final layer to compute similarity scores. VIP5 [2], on the other hand, merges visual embeddings, obtained from a frozen visual encoder, with textual inputs right at the beginning for language modeling. We refer to these as \"late fusion\" and \"early fusion\" strategies, respectively. These methods focus on processing a single image-text pair. In contrast, UniMP is designed to utilize a user's entire history, involving multiple products, for the generation. To handle this, we proposed UniMP to align and integrate the visual information into the language model through the specific cross-attention design (instead of concatenation). The visual information of each product is conditioned exclusively on its corresponding textual data during fusion. We found that removing exclusive attention leads to a substantial performance decrease (0.0337$\\rightarrow$0.0244). This fusion is also performed at every layer, allowing for a more nuanced and detailed understanding of user preferences. To demonstrate the effectiveness of this approach, we've included an experiment titled 'w/o Fine-grained' in Table 3 of our submitted paper, showing our method's performance without the cross-attention design. Additionally, we have conducted comparisons with Early Fusion, Late Fusion, and without Exclusive Attention strategies respectively in the below table, which further validate the effectiveness and rationale behind our proposed fusion method.\n\n|                         |     HR@5     |   NDCG@5   |    MRR@5   |\n|:-----------------------:|:------------:|:----------:|:----------:|\n|           VIP5          |    0.0262    |   0.0163   |   0.0127   |\n|  UniMP w/o Exclusive Attention |   0.0244    |   0.0165   |   0.0132  |\n|  UniMP w/o Cross-attention |   0.0257     |   0.0192   |   0.0161   |\n| UniMP with Early Fusion |    0.0271    |   0.0185   |   0.0135   |\n|  UniMP with Late Fusion |   0.0274     |   0.0193   |   0.0156   |\n|          UniMP          | **0.0337** | **0.0231** | **0.0196** |\n\n\nFurthermore, our approach to enhance multi-task optimization, which focuses on improving alignment and prioritization, coupled with the introduction of a new multi-modal personalization benchmark, provides critical insights into this emerging domain. \n\nIn summary, the integration of all these components presents a holistic solution for addressing real-world challenges in achieving unified multi-modal personalization. We believe our paper contributes valuable insights and advancements to the field, and we hope that the reviewer will consider these aspects of novelty and innovation in our work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348532576,
                "cdate": 1700348532576,
                "tmdate": 1700348532576,
                "mdate": 1700348532576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y4Y8iiRRCK",
                "forum": "khAE1sTMdX",
                "replyto": "uihkLrMOSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_s3tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_s3tt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. You have clarified my concerns and I raised my score accordingly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731859817,
                "cdate": 1700731859817,
                "tmdate": 1700731859817,
                "mdate": 1700731859817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pgnHWhIBzv",
            "forum": "khAE1sTMdX",
            "replyto": "khAE1sTMdX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_Y26x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_Y26x"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a unified personalization generative framework from multi-modal sources with the help of various LMs. Specifically, this work devises a generative personalization framework, UniMP, that can suit many downstream applications, including item recommendation, product search, preference prediction, explanation generation, etc. To achieve this, UniMP devises a universal data format, a user modeling architecture by combining a vision model and a language model, and a token generation framework by integrating multiple personalization learning tasks. Experiments on e-commerce datasets validate the effectiveness of the proposed framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of proposing a unified personalization framework is intriguing and promising in light of the rapid development of various LMs. \n\n2. This work addresses several challenges encountered in multi-task learning when the different backbone pre-trained LMs are in different modalities."
                },
                "weaknesses": {
                    "value": "1. Regarding the technical novelty in data fusion and user modeling, the contributions of this work are not impressive. \nIn particular, the strategies of data fusion and user modeling are kind of straightforward. The effectiveness is not validated. Please clarify or verify your choice. \n\n2. The presentation of this work should improved. Typos can be found without too much effort. \nFor instance,  \n\n- the abbreviation UniMP is not introduced when it first appears in the Introduction (see the 3rd paragraph on page 2); \n- in the 1st paragraph of Section 2.1, \"Based on s, ...\". \n\nBesides, the notations should be reorganized to make them more readable; e.g., $i_i$ (in Eq. (1)) is confusing."
                },
                "questions": {
                    "value": "1. How do you incorporate the behavioral data, like click, browse, and scroll?\n\n2. How do you define and characterize the multi-modal information? The category, brand, description, and price are all textual inputs; why are they defined as multi-modalities in the paper?\n\n3. What are the choices of backbone LMs?\n\n4. How does a user handle the token limits of the LLMs when s/he aims to apply this framework?\n\n5. How to apply the proposed framework to other domains other than e-commerce?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828216852,
            "cdate": 1698828216852,
            "tmdate": 1699636617801,
            "mdate": 1699636617801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kqLndHUIz3",
                "forum": "khAE1sTMdX",
                "replyto": "pgnHWhIBzv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y26x (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for providing your valuable feedback, and we appreciate the opportunity to make the following clarifications:\n\n> **Q1:** The strategies of data fusion and user modeling are kind of straightforward. The effectiveness is not validated.\n\nThank you for raising the question. Regarding data fusion, for previous works, CLIP [1] processes visual and textual data separately through modality-specific encoders and only combines them at the final layer to compute similarity scores. VIP5 [2], on the other hand, merges visual embeddings, obtained from a frozen visual encoder, with textual inputs right at the beginning for language modeling. We refer to these as \"late fusion\" and \"early fusion\" strategies, respectively. These methods focus on processing a single image-text pair. In contrast, UniMP is designed to utilize a user's entire history, involving multiple products, for the generation. To handle this, we proposed UniMP to align and integrate the visual information into the language model through the specific cross-attention design (instead of concatenation). The visual information of each product is conditioned exclusively on its corresponding textual data during fusion. We found that removing exclusive attention leads to a substantial performance decrease (0.0337$\\rightarrow$0.0244). This fusion is also performed at every layer, allowing for a more nuanced and detailed understanding of user preferences. To demonstrate the effectiveness of this approach, we've included an experiment titled \"w/o Fine-grained\" in Table 3 of our submitted paper, showing our method's performance without the cross-attention design. Additionally, we have conducted comparisons with Early Fusion, Late Fusion, and without Exclusive Attention strategies respectively in the table below, which further validate the effectiveness and rationale behind our proposed fusion method. We have also incorporated the results in Appendix A.6 of the revision.\n\n\n|                         |     HR@5     |   NDCG@5   |    MRR@5   |\n|:-----------------------:|:------------:|:----------:|:----------:|\n|           VIP5          |    0.0262    |   0.0163   |   0.0127   |\n|  UniMP w/o Exclusive Attention |   0.0244    |   0.0165   |   0.0132  |\n|  UniMP w/o Cross-attention |   0.0257     |   0.0192   |   0.0161   |\n| UniMP with Early Fusion |    0.0271    |   0.0185   |   0.0135   |\n|  UniMP with Late Fusion |   0.0274     |   0.0193   |   0.0156   |\n|          UniMP          | **0.0337** | **0.0231** | **0.0196** |\n\n\nIn our approach to user modeling, we emphasize the use of raw multi-modal content, as opposed to relying solely on single-modal data or product IDs to represent user sequences. This choice of content acts as a bridge, facilitating transferability and generalization across new domains and users. The comparative analysis of generalization capabilities, presented in Table 4 of our paper, highlights the strengths of our method. Notably, UniMP demonstrates a substantial edge in transfer learning scenarios.\n\n> **Q2:** The presentation of this work should improved.\n\nThanks for pointing out the typos. We've addressed these corrections and polished the paper. We'll continue to improve the overall presentation and organization of the paper, aiming to enhance its readability.\n\n> **Q3:** How do you incorporate the behavioral data, like click, browse, and scroll?\n\nThanks for the question. For the Amazon dataset, there are purchase and review behaviors available. In the case of Netflix, it provides data on viewing behavior. To accurately model these behaviors, we incorporate the items that have been purchased or viewed, along with their corresponding attributes, into the user sequence. Moreover, review texts are utilized to represent review behaviors. If additional behavioral signals like clicks, browsing, and scrolling are available, we could assign specific behavior-type tokens to distinguish these various activities and describe the duration of each behavior in a textual format.\n\n> **Q4:** How do you define and characterize the multi-modal information? The category, brand, description, and price are all textual inputs; why are they defined as multi-modalities in the paper?\n\nApologies for any confusion caused. In our paper, we refer to \"multi-modal information\" as the combination of textual and visual content. Meanwhile, elements like category, brand, description, and price are classified as \"heterogeneous information\", which can be represented in a textual format."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348227846,
                "cdate": 1700348227846,
                "tmdate": 1700348227846,
                "mdate": 1700348227846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uaWXlavLH0",
                "forum": "khAE1sTMdX",
                "replyto": "0j53mRRg0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_Y26x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_Y26x"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I prefer to keep my previous scores."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730258044,
                "cdate": 1700730258044,
                "tmdate": 1700730258044,
                "mdate": 1700730258044,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZfeGeRZNfH",
            "forum": "khAE1sTMdX",
            "replyto": "khAE1sTMdX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_Epwj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5845/Reviewer_Epwj"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces UniMP, a unified framework for multi-modal personalization that seeks to simplify the integration of various data modalities and tasks. It constructs a universal data format that facilitates the incorporation of diverse user historical data. It also presents a cross-attention mechanism that enables multi-modal user modeling. Furthermore, it combines several personalization tasks into a cohesive token generation framework and introduces context reconstruction and token-level reweighting for alignment. The experimental results show that UniMP can outperform competitive baselines on various benchmark tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation to establish a unified paradigm for multi-modal recommendation is good.\n- The structure is clear and it is well-written.\n- The evaluation is extensive and the experimental results look promising."
                },
                "weaknesses": {
                    "value": "- The rational behind the design choice of its approach is not well-explained.\n- The experimental setting needs clarification.\n- Whether this method can perform well on other datasets except for Amazon datasets is unknown."
                },
                "questions": {
                    "value": "The paper introduces a unified framework for multi-modal recommendation, which is commendably motivated. Nonetheless, I have concerns regarding the design of its approaches and the evaluation setups, as outlined below:\n- The authors introduce a cross-attention mechanism to merge visual and textual data. The rationale for this choice remains ambiguous. It is important to discuss how this approach compares to other vision-language fusion techniques such as CLIP[1] and other multi-modal recommendation systems like VIP5.\n- The necessity of the context reconstruction loss is still unclear to me. I wonder the detailed design of this loss item and why it can benefit alignment. And about the token-level reweighting, I think it is important to give more definitions/explanations of easy/hard tokens and how to distinguish.\n- In the evaluation part, it is crucial to offer clarity on the experimental settings. Some baseline models like MF and LightGCN operate differently from sequential models like S3Rec and BERT4Rec. Ensuring a clear distinction between these setups is essential to maintain fairness in comparisons.\n- The experiments utilize sub-datasets from the Amazon dataset, which might possess similar data distributions. This might limit the generalizability of the results. It would be informative to observe the recommendation model's performance, especially in scenarios of zero/few-shot recommendations, on diverse datasets with visual content.\n\n[1] Radford et al. Learning Transferable Visual Models From Natural Language Supervision. ICML 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5845/Reviewer_Epwj",
                        "ICLR.cc/2024/Conference/Submission5845/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699046578961,
            "cdate": 1699046578961,
            "tmdate": 1700709847590,
            "mdate": 1700709847590,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9VEbGBKLSg",
                "forum": "khAE1sTMdX",
                "replyto": "ZfeGeRZNfH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Epwj (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions. We hope the following answers can address your concerns.\n\n> **Q1:** The rational behind the cross-attention design of its approach is not well-explained. It is important to discuss how this approach compares to other vision-language fusion techniques such as CLIP[1] and other multi-modal recommendation systems like VIP5.\n\nThank you for bringing out the question. Our fusion method's main innovation, compared to CLIP [1] and VIP5 [2], lies in our unique approach to fusion strategy and fusion position. CLIP processes visual and textual data separately through modality-specific encoders and only combines them at the final layer to compute similarity scores. VIP5, on the other hand, merges visual embeddings, obtained from a frozen visual encoder, with textual inputs right at the beginning for language modeling. We refer to these as \"late fusion\" and \"early fusion\" strategies, respectively. These methods focus on processing a single image-text pair.\n\nIn contrast, UniMP is designed to utilize a user's entire history, involving multiple products, for the generation. To handle this, we proposed UniMP to align and integrate the visual information into the language model through the specific cross-attention design (instead of concatenation). The visual information of each product is conditioned exclusively on its corresponding textual data during fusion. We found that removing exclusive attention leads to a substantial performance decrease (0.0337$\\rightarrow$0.0244). This fusion is also performed at every layer, allowing for a more nuanced and detailed understanding of user preferences. To demonstrate the effectiveness of this approach, we've included an experiment titled \"w/o Fine-grained\" in Table 3 of our submitted paper, showing our method's performance without the cross-attention design. Additionally, we have conducted comparisons with Early Fusion, Late Fusion, and without Exclusive Attention strategies respectively in the table below, which further validate the effectiveness and rationale behind our proposed fusion method. We have also incorporated the results in Appendix A.6 of the revision.\n\n|                         |     HR@5     |   NDCG@5   |    MRR@5   |\n|:-----------------------:|:------------:|:----------:|:----------:|\n|           VIP5          |    0.0262    |   0.0163   |   0.0127   |\n|  UniMP w/o Exclusive Attention |   0.0244    |   0.0165   |   0.0132  |\n|  UniMP w/o Cross-attention |   0.0257     |   0.0192   |   0.0161   |\n| UniMP with Early Fusion |    0.0271    |   0.0185   |   0.0135   |\n|  UniMP with Late Fusion |   0.0274     |   0.0193   |   0.0156   |\n|          UniMP          | **0.0337** | **0.0231** | **0.0196** |\n\n\n> **Q2:** The necessity of the context reconstruction loss is still unclear to me. I wonder the detailed design of this loss item and why it can benefit alignment. And about the token-level reweighting, I think it is important to give more definitions/explanations of easy/hard tokens and how to distinguish.\n\nWe apologize for the missing details. For the context reconstruction loss, this term involves predicting product attributes for each historical product image in a user's sequence. Unlike our primary task in the paper (e.g., recommendation), which utilizes the entire user history encompassing multiple products, context reconstruction loss focuses on individual products. This approach aligns with the pre-training objective of the model, such as generating detailed descriptions for each specific image. The integration of context reconstruction loss serves as a bridge, harmonizing the optimization of our model. Importantly, the inclusion of context reconstruction loss doesn't incur extra computational costs, yet it substantially enhances training effectiveness by enriching the information learned.\n\nAs for token-level reweighting, we recognize that reconstructing context is generally simpler than accurately predicting user preferences, which is a key motivation for our approach. To address this, we propose a reweighting term designed to automatically differentiate between tokens that are easier or harder to predict. We've also included the details and discussion about the loss in Section A.7 of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348021825,
                "cdate": 1700348021825,
                "tmdate": 1700348021825,
                "mdate": 1700348021825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7KyAjgmK6u",
                "forum": "khAE1sTMdX",
                "replyto": "5TIikGfhaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_Epwj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5845/Reviewer_Epwj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the update."
                    },
                    "comment": {
                        "value": "Thank you for the detailed response and additional experiments. It helped clarify many of my concerns. I raised my score accordingly. About my concern with the baseline settings, I suggest adding the categories explicitly in the Table 1, which can be more clear."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709966382,
                "cdate": 1700709966382,
                "tmdate": 1700709966382,
                "mdate": 1700709966382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]