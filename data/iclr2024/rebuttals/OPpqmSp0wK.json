[
    {
        "title": "Multi-label Cluster Discrimination for Visual Representation Learning"
    },
    {
        "review": {
            "id": "Pnbwuqot2P",
            "forum": "OPpqmSp0wK",
            "replyto": "OPpqmSp0wK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on addressing the limitations of instance discrimination commonly used in image-text contrastive learning, such as CLIP. The authors propose a multi-label cluster discrimination method aimed at improving the encoding ability. They employ offline clustering to assign multiple labels to each image and subsequently conduct multi-label classification to learn the semantic structure within a single image. The authors support their methods with extensive experiments and perform ablation studies to analyze the function of each component."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work considers the multi-label properties of a single image and emphasizes the learning of better semantic structure in data.\n2. The designed loss function elegantly separates the loss from positive and negative classes, which enhances the parallelism and scalability during training.\n3. The experiments in this work are extensive and convincing, with thorough ablation studies."
                },
                "weaknesses": {
                    "value": "1. Clarity: \n   - This manuscript requires further refinement in terms of writing to facilitate reader comprehension, particularly by providing detailed explanations for the mathematical symbols used in the text, thus reducing reading barriers.\n2. Experiments: \n   - In section $3.2, the authors claim efficient parallel computation and scalability of the model training process. However, is there quantitative data to support this point?\n   - Does the incorporation of clustering significantly improve the training time? \n3. Reproducibility:\n   - In section $3.2, you employ some distribution training techniques but details are not provided, which hinders the reproducibility of the work."
                },
                "questions": {
                    "value": "1. The performance reported in the CLIP paper differs from your reproduced version. In Tab 1 and Tab 2, which may influence the validation of improvement of your model. Have you checked the implementation and settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Reviewer_KbPe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698132545075,
            "cdate": 1698132545075,
            "tmdate": 1700678199440,
            "mdate": 1700678199440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ot5ySlrZ9W",
                "forum": "OPpqmSp0wK",
                "replyto": "Pnbwuqot2P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KbPe"
                    },
                    "comment": {
                        "value": "Thanks for your valuable suggestions to improve our paper. We were glad to see your positive ranking at the beginning. Sorry for our late reply due to the time cost of additional experiments. We hope the revised version has solved all of your concerns on this paper.\n\n**Weakness1: detailed explanations for the mathematical symbols used in the text .**\n\n**A1:**  In the revised version, we add detailed explanations for all mathematical symbols used in the equations and text.\n\n**Weakness2: In section 3.2, the authors claim efficient parallel computation and scalability of the model training process. However, is there quantitative data to support this point? Does the incorporation of clustering significantly improve the training time?**\n\n**A2:**  In the revised Appendix A.1, we add a detailed explanation regarding the gradient calculation and we also include time cost comparison on the classification layer.\n\nAs we can see, the proposed method enables an elegant separation of positive and negative gradient calculation, which can decrease the communication frequency of calling the allreduce operation. \nTo compare the time cost on the classification layer, we train UNICOM, MLC, and MLCD on the LAION-400M dataset with one million classes.\nWe use ViT-B/32 as the backbone, and the final embedding dimension is 512.\nThe batch size is set as $32,800$ and the A100 GPU number is 80 in total distributed across 10 computation nodes.\nThe time cost of the forward and backward steps regarding the backbone is $409$ms.\nFor MLC and MLCD, the positive class number is set as 8. \nFor the single-label classification method, UNICOM, the time cost on the classification layer is $75$ms. \nFor MLC and MLCD, the time cost on the classification layer is $138$ms and $82$ms. The proposed multi-label cluster discrimination method slightly increases the time cost by 9.3% on the classification layer compared to the single-label cluster discrimination method, UNICOM. Compared to the widely used multi-label classification method, the proposed method obviously decreases the time cost by 45.6% on the classification layer. Therefore, the proposed decomposition of contrastive loss ensures efficient parallel computation across multiple GPUs from different computation nodes with minimal communication overhead. \n\nWith the aid of efficient feature quantization (Johnson et al., 2019), it only takes around 10 minutes to complete the offline clustering step on the large-scale LAION-400M dataset. For multi-label training, there is a slight increase of 9.3% in the time cost of the classification layer compared to the single-label cluster discrimination method.\n\n**Weakness3:  In section 3.2, you employ some distribution training techniques but details are not provided, which hinders the reproducibility of the work.**\n\n**A3:**  In the revised Appendix A.1, we add a detailed explanation regarding the gradient calculation and the distribution training. In the supplementary material, we put our training code, which is visible to the public now and can be used for reproducing our method.\n\n**Q1: The performance reported in the CLIP paper differs from your reproduced version. In Tab 1 and Tab 2, which may influence the validation of improvement of your model. Have you checked the implementation and settings?**\n\n**A1:** Even though CLIP has generously open-sourced the models, they have not provided the evaluation tools. \nTo this end, we contacted the UNICOM [1] authors to get their evaluation toolkit and all test datasets for the task of linear probe. In UNICOM, the authors have developed a GPU-accelerated logistic regression algorithm, facilitating batch-wise assessment of linear probe performance. In this paper, the performance of CLIP is also reported by using this evaluation toolkit.\n\nFor the task of zero-shot classification, we adopted the same experimental settings as FLIP [2] and referred to the FLIP paper to report the zero-shot results.  \n\n[1] An, Xiang, et al. Unicom: Universal and Compact Representation Learning for Image Retrieval. ICLR 2023.\n\n[2] Li, Yanghao, et al. Scaling language-image pre-training via masking. CVPR 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3251/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699133673,
                "cdate": 1700699133673,
                "tmdate": 1700699133673,
                "mdate": 1700699133673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r8lsQcTiaM",
            "forum": "OPpqmSp0wK",
            "replyto": "OPpqmSp0wK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new clustering-based unsupervised algorithm for vision foundation models pre-training. The key idea is to assign images into multiple clusters as pseudo labels for unsupervised representation learning. The motivation is that existing clustering-based pre-training methods assign each image into a single cluster, which enforces the models to focus on the most salient part of images and overlook the other regions that may also be meaningful. Besides, the authors also optimise the conventional margin loss formulation by decoupling the optimisations of positive and negative pairwise similarity. The proposed algorithm has been shown effective in severe classification-oriented downstream tasks including linear probe, zero-shot classification and retrieval."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The algorithm proposed in this paper is intuitive and effective in learning discriminative imagery feature representations. The analysis and decomposition of triplet loss make sense to me and are potentially beneficial to a wide range of applications as a generic improvement to a widely adopted metric learning design."
                },
                "weaknesses": {
                    "value": "+ my key concern on the high-level idea is whether the top-k closest clusters to an image can really reveal what objects/attributes (will use \u201cconcepts\u201d for clarity hereafter) are involved in it. At the cluster level, samples of the same clusters are likely to share more nearest clusters (in a global picture) but the concepts involved in each independent image are almost random. Is it possible that the multiple labels assigned to the same images provide models with additional knowledge about the co-occurrence/relevance of different concepts (cluster-to-cluster relationships) rather than actually telling models what is involved in images (sample-to-cluster relationships)? It will be interesting to see more exploration and analysis of why the multi-label clustering idea is beneficial. One simple verification can be pre-training on a dataset with known non-overlapping class structure, eg ImageNet, and see if the multi-label clustering still benefits. \n\n+ The modifications made to triplet loss make sense to me but their effects are unclear. How will the proposed model perform if all its designs are kept unchanged except for replacing L\u2019_MLCD (Eq.6) with L_MLC\n\n\n+ What are the blue and green cells standing for in the grids pointing to the text \u201ccontrastive loss\u201d?\n\n+ Whilst Fig.2 is the first figure being referred to, Fig.1 is simply mentioned as the illustration of visual representation learning but it lacks further explanation/discussion.\n\n+ In Eq.1, I assume the pairwise similarity is cosine similarity if following CLIP, but without normalisation of features, it is just an inner product. So I\u2019m wondering if it is a mistake or my misunderstanding.\n\n+ In Eq.1, the index in the cumulative sum starts from 0 to k while that in Eq.2 is from 1 to k, is this deliberate and why?\n\n+ The exponential function is denoted as exp and e at the same time in Eq.3, which makes the equation really confusing when the feature representations are also denoted as e_i.\n\n+ The ablation studies are a bit unclear to me. For example, when investigating the effects of sample ratio, the best linear probe performance is obtained when the sample ratio is set to 0.1, and the best result is 75.2. However, the linear probe performance of the proposed model shown in Table 5 is 84.6."
                },
                "questions": {
                    "value": "Although the proposed method yielded impressive performance, it is also crucial for me to figure out the underlying reasons for the effectiveness. So further evidence and discussions about this will be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640326345,
            "cdate": 1698640326345,
            "tmdate": 1699636273444,
            "mdate": 1699636273444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0GR2hHvuQW",
                "forum": "OPpqmSp0wK",
                "replyto": "r8lsQcTiaM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g3TE"
                    },
                    "comment": {
                        "value": "Thanks for your insightful suggestions to improve our paper.\n\n**Q1: Is it possible that the multiple labels assigned to the same images provide models with additional knowledge about the co-occurrence/relevance of different concepts (cluster-to-cluster relationships) rather than actually telling models what is involved in images (sample-to-cluster relationships)? pre-training on a dataset with known non-overlapping class structure, eg ImageNet, and see if the multi-label clustering still benefits.** \n\n**A1:** We added corresponding experiments and visualization in the revised version.\n\n |   CASE   | DATA | 0.5K | 1K   | 2K   | 4K   | 8K   | 20K |\n | ------   | ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n |   UNICOM | IN1K | 42.1 | 58.4 | 61.5 | 62.8 | 62.4 | 61.5|\n |   MLCD   | IN1K | 63.2 | 67.2 | 68.2 | **69.9** | 69.7 | 69.0|\n\nIn Tab. 7 (a), we compare the proposed multi-label cluster discrimination and the single-label cluster discrimination on ImageNet with the clustered class number ranging from 0.5K to 20K. The clustering step is conducted by using the features predicted by the CLIP model. In the discrimination step, both MLCD and UNICOM employ the negative class center sampling with a ratio of $0.1$, and the positive number for MLCD is set as $8$.\nPre-training is executed on the ImageNet training dataset by 100 epochs. The model backbone is ResNet-50. The evaluation is undertaken using a linear probe on the ImageNet validation set.\nAs we can see, the proposed multi-label learning significantly surpasses UNICOM and achieves the best performance of 69.9% when the class number is 4K, which is four times of the true class number of ImageNet. In Fig. 5 of the Appendix, we visualize the top three labels for our training samples. When training with multiple labels, our method can not only capture the label correlations (e.g., paintbrush and oil painting, shield and armour, ladle and frying pan) but also learn complementary visual signals (e.g., different breeds of dogs, different locations of figs) to improve visual representation learning. \n\n**Q2: The modifications made to triplet loss make sense to me but their effects are unclear. How will the proposed model perform if all its designs are kept unchanged except for replacing L\u2019_MLCD (Eq.6) with L_MLC.**\n\n**A2:** In the revised version, we add the experiments to verify the effectiveness of the proposed MLCD.\n\n|CASE | DATA | Finetune | Linear Probe | Zero Shot|\n| ------   | ------ | ------ | ------ | ------ | \n|MLC  | LAION-400M| 80.9 | 76.9 | 63.9 |\n|MLCD | LAION-400M| 81.2 | 78.1 | 64.5 | \n\nTab. 7 (a), we compare the performance of the vanilla MLC and the proposed MLCD on the ImageNet validation dataset. Pre-training is executed on the LAION-400M dataset by 32 epochs. The model backbone is ViT-B/32. \nBoth MLC and MLCD employ the negative class center sampling with a ratio of $0.1$. MLCD outperforms MLC in all three settings: fine-tuning, linear classification, and zero-shot, confirming the effectiveness of the two additional optimization targets. In Appendix. A.1, we compare their gradient calculation and time cost on the classification layer. The proposed contrastive loss decomposition can significantly decrease the communication cost, facilitating distributed training on large-scale training data.\n\n**Q3: What are the blue and green cells standing for?**\n\n**A3:**  For Fig. 1(a), blue cells stand for the similarity scores of positive image-text pairs. The similarities in the blue cells need to be increased during training.\nFor Fig. 1(b), there are two training samples in the minibatch.\nThe blue and green cells stand for the positive scores in their corresponding one-hot target logits.\nFor Fig. 1(c), there are also two training samples in the minibatch.\nHowever, each sample has three positive labels. Thus, the target logits contain three positive similarity scores that need to be increased during training.\n\n**Q4: Whilst Fig.2 is the first figure being referred to, Fig.1 is simply mentioned as the illustration of visual representation learning but it lacks further explanation/discussion.**\n\n**A4:** In the revised introduction section, we refer to Fig. 1 first when we introduce instance discrimination methods and cluster discrimination methods. In the revised preliminary section, we add detailed explanations of Fig. 1 (a) and Fig. 1 (b). In Section 3.2, we also refer to Fig. 1 (c) when we introduce our method. \n\n**Q5: In Eq.1, I assume the pairwise similarity is cosine similarity if following CLIP, but without normalization of features, it is just an inner product. So I\u2019m wondering if it is a mistake or my misunderstanding.**\n\n**A5:** We employed the normalization to all embedding features, and the pair-wise similarity is the cosine similarity. In the revised version, we added the keyword of normalization to features around Eq. (1)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3251/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698376157,
                "cdate": 1700698376157,
                "tmdate": 1700699249484,
                "mdate": 1700699249484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XaambtbLlo",
                "forum": "OPpqmSp0wK",
                "replyto": "mBoksnYBSV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3251/Reviewer_g3TE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and the additional materials provided. Most of my concerns have been addressed but I still found it hard to understand the underlying reason why the proposed multi-label clustering idea is effective. The Figure 2 and the new Figure 5 tells a different story. To be concrete, the examples shown in Figure 2 imply that the model works because web images usually involve more than one object while multiple labels assigned to the same images can help avoid false negative supervision signals. On the other hand, Figure 5 demonstrates that the multi-labels assigned to the same images are mostly depending on the relevance between classes, rather than what is shown in the images (e.g. the class of armour is not involved in the second example in Figure 5 but it is still one of the k-nearest clusters to the target image, and the k-nearest clusters of the third example don't include a container/box even though this is explicitly shown in the target image). The experiments on ImageNet as well as the authors' discussions seem to support the latter reason but this is not consistent with the current story in the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3251/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721210914,
                "cdate": 1700721210914,
                "tmdate": 1700721210914,
                "mdate": 1700721210914,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "axnazSOF5k",
            "forum": "OPpqmSp0wK",
            "replyto": "OPpqmSp0wK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a simple but effective method to facilitate the representation learning of the vision-language model. The method consists of two steps. In the clustering step, the authors cluster the dataset into enormous centers and utilize several closest centers as the class labels for every single image, enhancing the learning of semantic structure of training data. The discrimination step incorporates a multi-label classification loss to separate losses and promote distributed training. The experimental results are solid."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: This work extends the discrimination power of CLIP model by introducing a multi-label loss to boost the semantic learning ability of the vision-language model.\nQuality: The improvement achieved by the proposed method is remarkable on certain datasets, and the ablative study provides comprehensive and detailed insights into its functioning.\nClarity: This paper is reader-friendly and smooth. The experimental setting is quite reasonable.\nSignificance: This paper shows the benefit of using multi-label loss for clustering-based discriminative constrastive learning. This setting should be considered when developing powerful pre-trained vision-language model for downstream tasks."
                },
                "weaknesses": {
                    "value": "(1) The novelty and originality of this work are limited. It seems like the method proposed in this paper incorporates several techniques introduced in the literature. It does not offer sufficient technical inspirations for the readers to follow. \n(2) With respect to the limited technical novelty of this work and overall moderate improvement (I see in Table 1 and Table 2), it may not seem to be worthwhile using such huge computing resources (80 NVIDIA A100 GPUs), especially considering that visual-language pre-training field has already achieved remarkable performance.\n(3) According to my understanding, as proposed method is developed upon the feature embedding from the pre-trained CLIP model and it does not involve any textual information in the proposed multiple label loss. If this is correct, this paper should make this more clear.\n(4) In Equation 5, two new items are further introduced into the multi-label loss. This is regarded as one of the key contributions by this paper. However, its efficacy does not seem to be clearly verified in the ablation study. This needs to be addressed."
                },
                "questions": {
                    "value": "(1) As one of the main contributions of this paper, the authors claim that the modification of optimization loss can elegantly separate the positive class labels and negative class labels, resulting in promotion of the distributed training on large-scale training data. Please explain and experimentally demonstrate how this modification can facilitate the distributed learning more clearly. For example, in Subsection Distributed Multi-label Classification of Section 3.2 MULTI-LABEL CLUSTER DISCRIMINATION, The first sentence \u201cEq. 6 is able to distribute the weights associated with one million class centers across all GPUs with minimal communication overhead.\u201d Why? \n(2) Some technical details are missing, e.g., in Section 4.1, the authors should explicitly point out the number of classes (k) and number of positive centers (l) they use when pre-training the model on LAION-400M dataset.\n(3) In Table 3, the best results consist of both the proposed method in this paper and the FLIP (i.e., 89.1%). Notably, only the results of the proposed methodology have been highlighted.\n(4) In Section 4.6, the meaning of the y-axis of the charts should be provided to improve the clarity of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns. The authors have discussed the limitation at the end of the paper."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3251/Reviewer_S2Uc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3251/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699511337881,
            "cdate": 1699511337881,
            "tmdate": 1699636273358,
            "mdate": 1699636273358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkyzkXxhCU",
                "forum": "OPpqmSp0wK",
                "replyto": "axnazSOF5k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the weaknesses mentioned by Reviewer S2Uc"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments and suggestions to improve this paper.\n\n**Weakness1: novelty and originality.** \n\n**A1:**  In the instance discrimination methods (e.g., CLIP), each image-text pair represents one unique class. CLIP can provide richer forms of labels for a single image, e.g., objects, scenes, actions, and relations, at multiple levels of granularity. However, negative pairs that share similar semantics will be undesirably pushed apart in the embedding space. \n\nIn the cluster discrimination methods (e.g., UNICOM), visually similar instances are pulled together by the classification step. However, UNICOM only defines a single pseudo-label for each image.\n\nTo the best of our knowledge, we are the first to propose multi-label learning on the large-scale dataset (LAION-400M).\nThe proposed multi-label cluster discrimination approach can not only capture the semantic structures in the data but also support the learning of multiple granularity of labels for a single image. In addition, we introduce a novel decomposition of contrastive loss, to \navoid ambiguity during the optimization of $(s_j-s_i)$ as well as decrease the communication cost during distributed training.\n\n**Weakness2: moderate improvement.** \n\n**A2:** In this paper, we mainly compared our method with the latest state-of-the-art methods (e.g., UNICOM [1] and FLIP [2]). In the task of linear probe, our method outperforms UNICOM by 1.3% (Tab. 1). In the task of zero-shot classification, our method surpasses FLIP by 1.5% (Tab. 2). Vision representation learning is one of the most competitive research topics, and our improvement compared to this year's publications is indeed significant. \n\n[1] An, Xiang, et al. Unicom: Universal and Compact Representation Learning for Image Retrieval. ICLR 2023.\n\n[2] Li, Yanghao, et al. Scaling language-image pre-training via masking. CVPR 2023.\n\n**Weakness3:  The proposed method is developed upon the feature embedding from the pre-trained CLIP model and it does not involve any textual information in the proposed multiple label loss.**\n\n**A3:** In this paper, we focus on improving visual representation by introducing multi-label cluster discrimination. Our method does not involve text learning. In Fig. 1, we compare our method with CLIP and UNICOM. CLIP consists of the image encoder and text encoder, while UNICOM and the proposed method only contain the image encoder. In Section 4.1, we mention how to train an additional text encoder following Locked-image Tuning (LiT)[3], which teaches a text model to read out good representations from a locked image model for zero-shot classification and image-text retrieval tasks.\n\n\"To assess the performance of zero-shot classification and zero-shot image-text retrieval tasks, we employ contrastive learning to train a text encoder from scratch for 32 epochs with a frozen visual encoder following LiT (Zhai et al., 2022b). The structure of the text encoder is also identical to CLIP. \"\n\n[3]Zhai, Xiaohua, et al. Lit: Zero-shot transfer with locked-image text tuning. CVPR 2022.\n\n**Weakness4: In Equation 5, two new items are further introduced into the multi-label loss. This is regarded as one of the key contributions by this paper. However, its efficacy does not seem to be clearly verified in the ablation study.**\n\n**A4:** \nIn the revised version, we add the experiments to verify the effectiveness of the proposed MLCD.\n\n|CASE | DATA | Finetune | Linear Probe | Zero Shot|\n| ------   | ------  | ------        | ------               | ------        | \n|MLC    | LAION-400M     | 80.9 | 76.9 | 63.9 |\n|MLCD | LAION-400M     | 81.2 | 78.1 | 64.5 | \n\nTab. 7 (a), we compare the performance of the vanilla MLC and the proposed MLCD on the ImageNet validation dataset. Pre-training is executed on the LAION-400M dataset by 32 epochs. The model backbone is ViT-B/32. \nBoth MLC and MLCD employ the negative class center sampling with a ratio of $0.1$. MLCD outperforms MLC in all three settings: fine-tuning, linear classification, and zero-shot, confirming the effectiveness of the two additional optimization targets. In Appendix. A.1, we compare their gradient calculation and time cost on the classification layer. The proposed contrastive loss decomposition can significantly decrease the communication cost, facilitating distributed training on large-scale training data."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3251/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697355079,
                "cdate": 1700697355079,
                "tmdate": 1700697355079,
                "mdate": 1700697355079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dA9po3jX4S",
                "forum": "OPpqmSp0wK",
                "replyto": "axnazSOF5k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3251/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the questions arised by Reviewer S2Uc"
                    },
                    "comment": {
                        "value": "**Q1: Please explain and experimentally demonstrate how this modification can facilitate distributed learning more clearly. \u201cEq. 6 is able to distribute the weights associated with one million class centers across all GPUs with minimal communication overhead.\u201d Why?** \n\n**A1:** \nIn the revised version (Appendix A.1), we add a detailed explanation regarding the gradient calculation and we also include a time cost comparison on the classification layer.\n\nAs we can see, the proposed method enables an elegant separation of positive and negative gradient calculation, which can decrease the communication frequency of calling the allreduce operation. \nTo compare the time cost on the classification layer, we train UNICOM, MLC, and MLCD on the LAION-400M dataset with one million classes.\nWe use ViT-B/32 as the backbone, and the final embedding dimension is 512.\nThe batch size is set as $32,800$ and the A100 GPU number is 80 in total distributed across 10 computation nodes.\nThe time cost of the forward and backward steps regarding the backbone is $409$ms.\nFor MLC and MLCD, the positive class number is set as 8. \nFor the single-label classification method, UNICOM, the time cost on the classification layer is $75$ms. \nFor MLC and MLCD, the time cost on the classification layer is $138$ms and $82$ms. The proposed multi-label cluster discrimination method slightly increases the time cost by 9.3% on the classification layer compared to the single-label cluster discrimination method, UNICOM. Compared to the widely used multi-label classification method, the proposed method obviously decreases the time cost by 45.6% on the classification layer. Therefore, the proposed decomposition of contrastive loss ensures efficient parallel computation across multiple GPUs from different computation nodes with minimal communication overhead.\n\n**Q2: Some technical details are missing, e.g., in Section 4.1. the number of classes (k) and number of positive centers (l).**\n\n**A2:** In the revised version, we add these technical details in Section 4.1.\n\"In the following experiments, unless otherwise specified, the model used is ViT-L/14, the number of classes ($k$) is one million, and the number of positive labels ($l$) assigned to each image is $8$.\"\n\n**Q3: Table 3, all the best results need to be highlighted.**\n\n**A3:**   We fixed this in the revision, and we also checked other tables.\n\n**Q4: the meaning of the y-axis of the charts should be provided to improve the clarity**\n\n**A4:** In Fig. 3, we visualize per-dataset differences in the performance. The Y-axis shows the performance difference. Most of the evaluation metrics are accuracy as shown in Tab. 10 (Appendix). Green bars indicate our model outperforms the baselines, while the orange bars depict our model is surpassed by the baselines. Our model outperforms UNICOM on 23 datasets on the task of linear probe and surpasses FLIP on 15 datasets on the task of zero-shot classification. In the revised version, we added a detailed explanation in the caption of Fig. 3."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3251/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697775047,
                "cdate": 1700697775047,
                "tmdate": 1700697775047,
                "mdate": 1700697775047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]