[
    {
        "title": "Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning"
    },
    {
        "review": {
            "id": "UzX03N0Wg7",
            "forum": "B5kAfAC7hO",
            "replyto": "B5kAfAC7hO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to leverage the L-step decodable POMDPs to tackle the linear structure of POMDPs. Specifically, by conditioning on the recent L-step history (x_h), Q value does not need to rely on belief states or full history. Further, they show that Q value can be expressed in a linear form wrt P(z_h | x_h,a_h) where z_h is a latent variable, learned by some ELBO. The algorithm uses linear structure and learns the representation and Q value together with some sampling method. On both continuous and visual benchmarks, the proposed approach outperforms baselines in the terms of sample efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work is original in POMDP literature with some theoretical guarantees (but I don\u2019t have the expertise to check) and good quality. The technical writing is mostly clear, but some clarification is still needed. \n\nThe empirical results are persuasive that the proposed approach outperforms the other baselines in most domains in the chosen continuous and visual control benchmarks."
                },
                "weaknesses": {
                    "value": "This work has a main issue in its story writing:\n\n1. The title and abstract is quite vague and overly broad \u2013 basically it just said it is about a theoretical framework on RL or planning (I am confused which one) in POMDPs. \n\n2. Moreover, the introduction on the theoretical framework is rather limited, unclear, unstructured, and seems overly strong. The 4 bullet points are most useful, but structured. \n\n3. The claim \u201capplied to a real-world problem\u201d is especially strong, as the work obviously requires some assumptions on POMDPs, and no real world (like real robots) evaluation is performed. \n\n4. The claim \u201cstate-of-the-art\u201d is also too strong, as obviously Dreamerv2 and DrQ-v2, published in 2021, are no longer SOTA. \n\n5. The claim also touches \"offline POMDPs\" but I did not see any results.\n\nI don\u2019t think this work has much technical significance since it is heavily relied on recent work (Ren et al, 2023a). Also, it would be better to point out how important linear structure is in solving POMDPs more explicitly."
                },
                "questions": {
                    "value": "1. Is L-step decodability same as (or subsumed by) L-order MDPs?\n2. Lack some definition on the latent variables z. From Eq 17, the objective of representation learning is exactly the same as belief-based approach. Let k=1, it is the standard ELBO of observation reconstruction/prediction, plus a KL divergence regularization between posterior and prior. In this sense, the optimal z seems to be belief state b. Is this correct? For k > 1, z might be different from b as it involves policy. \n3. The paper talks about \u201clow-rank POMDPs\u201d, but no definition is provided. How is it connected to L-step decodability? \n4. How partially observable is in the visual control benchmarks? As they were also tackled by Markovian methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6845/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE",
                        "ICLR.cc/2024/Conference/Submission6845/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698284741194,
            "cdate": 1698284741194,
            "tmdate": 1700696782259,
            "mdate": 1700696782259,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EfxLipl3av",
                "forum": "B5kAfAC7hO",
                "replyto": "UzX03N0Wg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rUpE"
                    },
                    "comment": {
                        "value": "We provided some necessary introduction and common knowledge in RL and addressed the questions below. \n\n* RL vs. Planning \n\n  In RL,  the model learning refers to estimating the transition and reward,  and planning is referring to finding the optimal policy with the given or learned transition and reward (as explained in the Introduction section). We rigorously follow these definitions, where in our paper the algorithm **learns** a proper representation during the learning phase that can be used to perform planning (finding the optimal policy) easily.\n\n* Real-world problems \n\n  We tested our algorithm on MuJoCo environments with partial observations, and image-based RL problems, which are considered as real-world benchmarks in the RL community [1, 2]. We will rephrase this claim. \n  Again, we would emphasize the proposed  \u00b5LV-Rep is the first practical representation learning algorithm for POMDPs with tractable planning and exploration, and justified by rigorously  theoretical guarantees and strongly empirical performances, as far as we know. \n  It is unrealistic to ask one single conference paper to complete every aspect from theory, to simulation, to real robot deployment. \n\n* Offline POMDPs\n\n  The pessimism mechanism in offline setting is similar to the optimism in the online setting, and the major difference between the optimism in online setting and pessimism in offline setting is that, in the offline setting we **minus** the same bonus from the reward while in the online setting we **add** the bonus into the reward. We already added the corresponding discussion in Appendix D in our updated version.\n\n* Novelty and Significance comparing to Ren et al. (2023a)\n\n  We strongly disagree with this claim about the novelty and significance of our work.  Based on our personal communication with the authors of Ren et al (2023a), how to extend LV-Rep to POMDP is \u201cextremely difficult, which cost them one year already\u201d, as we also discussed in Section 4.  \n  The extension is highly nontrivial and significant as we discussed in Section 3, especially considering the research on planning in POMDPs has stuck for decades. We emphasize the simplicity of the algorithm should be considered as an advantage, which enables the LV-Rep to more practical settings to handle partial observations, rather than a drawback.\n\n* L-step decodable POMDPs vs  L-order MDPs\n\n  We are not sure what is the concrete definition of L-order MDPs. We assume the reviewer is asking about the megastate MDPs defined in [3]. In fact, the connection between L-step decodable POMDPs and L-step megastate MDPs have been discussed extensively in [3]. \n  One can convert L-step decodable POMDPs to a L-step megastate MDPs. This reduction is useful in justifying the difficulty of the problem by characterizing the lower bound dependency on the problem size. However, such a simple reduction will induce extra dependency in the megastate, as the successive megastates have overlap of the state trajectories. This extra dependency does not reduce the difficulty in planning, and it does not provide justification for low-rank linear representation in MDPs. \n  We understand the reviewer would like to use this reduction to justify the triviality of applying the existing representation learning for MDPs to POMDPs. However, as our personal communication and discussion with the authors of [1, 3, 4, 5], this argument does not hold.\n\n* Latent Variable z vs. Belief of state\n\n  No. Such understanding is incorrect. \n  Recall that by definition that belief is the posterior of the true state, while our latent variable z does not have to be the state but an arbitrary variable, as long as it satisfies the factorization in Eq. 16. Meanwhile, belief is a distribution, while z is a variable.  \n  These properties of latent variable representation unleash us from identifiability restriction, which is one of our benefits to overcome the difficulty we discussed in Section 3 about recovering belief. We also discussed this in our main text in page 5 the remark about identifiability. \n\n* Low-rank POMDPs\n\n  The low-rank structure is indicating the latent variable factorization can be finite, or even if it is infinite, the eigendecay speed is fast, as we discussed in Appendix. The L-step decodability property ensures the rank is smaller than the exponential of the horizon, as discussed in [3].\n\n* Non-Markovian in Visual Control \n  \n  In the image-based visual control benchmarks, although we can have the images, the control states, e.g., velocity, can not be inferred from one single image. There is no doubt that the Markovian methods can be applied for these tasks, but the performances will be significantly degenerated, due to the missing of important information. \n  To further demonstrate the non-markovian property, we tested the algorithms with markovian assumption (L=1) vs non-markovian (L>1) (Figure 6 of Appendix H.1). As we can see, the non-Markvoian algorithm performs significantly better."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381771289,
                "cdate": 1700381771289,
                "tmdate": 1700381771289,
                "mdate": 1700381771289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3qi8FkP6OZ",
                "forum": "B5kAfAC7hO",
                "replyto": "UzX03N0Wg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, I would like to address the tone of your response, which appears somewhat condescending. The phrase \"We provided some necessary introduction and common knowledge in RL\" suggests an assumption about the reviewer's level of expertise, akin to that of a student recently introduced to RL. Additionally, I noticed the absence of a customary 'thank you' in your response to my review, a courtesy extended to other reviewers. This inconsistency in approach is somewhat dismissive.\n\nHowever, as a professional reviewer, I still respond to your disconcerting rebuttal: \n\n> RL vs. Planning\n\nI definitely know what they are. So what is the planning part in your algorithm, like Dreamer's planning? The paper is too dense, as the other reviewer pointed out, and it's hard to grasp the core component of the algorithm. \n\nThe problem on this point is writing. The title and abstract are too broad. The introduction focuses on the motivation and prior work too much. \n\n> By \u201cefficient\" we mean the statistical and computational complexity avoids an exponential dependence on history length, while the computational components of learning, planning and exploration are computationally feasible; while by \u201cpractical\" we mean that every component of an algorithm can be easily implemented and applied to a real-world problem. In this paper, we provide a affirmative answer to these questions. \n\nDon't you feel these arguments are too strong? If so, RL is solved. I don't know anyone will call dm-control a real-world problem. To clarify, I don't ask you to work on real robots, but ask you to soften your overall writing. Unfortunately, you did not make any changes. \n\n> Offline POMDPs\n\nOk, during rebuttal you have added it and the proof required the readers for some external references.\n\n> The claim \u201cstate-of-the-art\u201d \n\nUnfortunately again, you did not respond to this.\n\n> Significance\n\nI'm not familiar with Ren et al.'s work. I also think the anecdotal evidence like personal communication is not a professional way to show some work is generally hard. Furthermore, the fact that the work is hard does not imply the work is significant. \n\nBut at least, section 4.1 is very straightforward once you have L-step decodability assumption. Let the other reviewers and AC decide the main algorithm's significance. \n\n> L-order MDPs\n\nThis concept stems from L-order HMM. You can view it as the recent L observations (and actions) in a POMDP compose of the state in the corresponding MDP. \n\n> Latent Variable z vs. Belief of state\n\nYou did not directly answer my question. Can you compare Eq 17 with Dreamer's model learning objective? A belief state is a posterior, but it can be also viewed as a vector, expressed with the parameters of the posterior. In Dreamer or other variational belief approaches, they make it stochastic, i.e., we get a belief state z sampled from a distribution. \n\n> Low-rank POMDPs\n\nSo L-step decodability belongs to low-rank POMDPs?\n\n> Non-Markovian in Visual Control\n\nSounds good!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462525938,
                "cdate": 1700462525938,
                "tmdate": 1700502412872,
                "mdate": 1700502412872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hYLzC3uNxq",
                "forum": "B5kAfAC7hO",
                "replyto": "UzX03N0Wg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer rUpE"
                    },
                    "comment": {
                        "value": "Thanks for being professional. The following are our replies.\n\n\n* Significance \n  The claim that \u201cSec 4.1 is straightforward\u201d after seeing the results lacks the basic respect not only for us, but also for the previous researchers, who are working on this problem. \n  The L-step decodability assumption has been proposed by Efroni et al, in 2020, and a recent followup paper trying to figure out computation tractable planning (Guo et al, 2023) is just published, while still leaving the planning problem partially open. If the planning problem is indeed \u201cstraightforward\u201d as the reviewer claimed, the problem should be solved far earlier than our paper.  \n  Again, we emphasize that simplicity does not mean straightforward. The simplicity of an algorithm should be considered as an advantage, rather than a drawback. \n\n* RL vs. Planning  \n\n  * Re: Planning. \n    \n    With the proposed representation, the Q-function can be represented linearly. And the planning is executing Bellman backup in the linear space as we described. While in Dreamer, it is not clear in which space the Bellman backup should be conducted to obtain a theoretical rigorous algorithm. \n\n   * Re: RL.\n    \n     We never claim we solved RL problem, as we emphasize in the main text, the paper is trying to answer the question: \u201cHow can we design an efficient and practical RL algorithm for structured partial observations\u201d, instead of solving RL, which we never claimed. \n\n     This question is right before the paragraph you cited. The paragraph you cited is written to explain the problem in detail. In fact, the proposed algorithm indeed avoids exponential dependence on history length, while with tractable planning and exploration. Meanwhile, if you read the paragraph you cited carefully, it claims that our algorithm **can be** applied to real-world problems. If you think there is any step of the proposed algorithm that cannot \u201cbe easily implemented and applied to a real-world problem\u201c, please point it out and we are open to discuss. \n\n* L-order MDPs\n\n  In the RL community, this is known as L-step mega-state MDPs. We already explained the connection between L-decodable and L-step mega-state MDP in our last response, and more importantly, the difficulties in planning still exist even with the form of so-called \u201cL-order MDPs\u201d, due to the observation overlaps in mega-state. \n\n* Latent Variable z vs. Belief of state\n\n  We would like to clarify that belief refers to the posterior of **true state**, which requires extra identifiability assumption, while as we discussed in the remark in page 5. \n\n\n* Terminology confusion \n\n   * \u201cReal-world problems\u201d\n\n      We are sorry that the term \u201creal-world\u201d makes the reviewer uncomfortable. In fact, we already modified the claim in our experiment setting. While there are only three places in maintext, where the words show up:\n\n       * \u201cIn real-world reinforcement learning, state information is often only partially observable,\u201d in abstract;\n       * \u201cSuch algorithms have been applied to many real-world applications with image- or text-based observations (Berner et al., 2019; Jiang et al., 2021), sometimes even surpassing human-level performance (Mnih et al., 2013; Kaufmann et al., 2023).\u201d in page 1 introduction;\n       * \u201cwhile by \u201cpractical\" we mean that every component of an algorithm can be easily implemented and applied to a real-world problem.\u201d in page 2.\n\n       If read carefully, you may see the first one is describing the problem setting, the second is describing dreamer and chatgpt, the third one is describing the target for which we are pursuing.\n\n    * \"State-of-the-art\u201d\n\n      We have softened our claim and removed \u201cstate-of-the-art\u201d, which is used for our competitors, not our method. \n\nWe sincerely believe the paper should be reviewed by evaluating the contribution and significance, instead of criticizing the usage of terminology."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531171804,
                "cdate": 1700531171804,
                "tmdate": 1700531724209,
                "mdate": 1700531724209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "icafhZC46Q",
                "forum": "B5kAfAC7hO",
                "replyto": "UzX03N0Wg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. However, I must express that my primary concerns remain unaddressed. I find the tone of your response to be still condescending, as highlighted in the following points:\n\n>  \u201cSec 4.1 is straightforward\u201d\n\nThe phrase \u201cSec 4.1 is straightforward\u201d was not meant to imply triviality. Simplicity in scientific approaches is often commendable and can be non-trivial. My intention was not to undermine the significance of your work, which I am uncertain about. \n\nYour criticism on me that \"lacks the basic respect not only for us, but also for the previous researchers, who are working on this problem.\" is unfounded and quite troubling. Such an accusation is not conducive to a constructive scientific discourse.\n\n> We never claim we solved RL problem, as we emphasize in the main text, the paper is trying to answer the question.\n\nWhen I referred to your strong claims, it was my interpretation of the tone and implications in your paper, not an accusation that you claimed to have \"solved the RL problem\".\n\n> cannot \u201cbe easily implemented and applied to a real-world problem\"\n\nTo be specific, you can check the assumptions made in your approach. For example, the definitions 1,2 are on the structures of POMDPs. How does your algorithm scale to very large L, which is common in real-world problem? \n\n> L-order MDPs\n\nThanks and I got it now. However, your previous claim hat \n\n> We understand the reviewer would like to use this reduction to justify the triviality of applying the existing representation learning for MDPs to POMDPs. However, as our personal communication and discussion with the authors of [1, 3, 4, 5], this argument does not hold.\n\nYour assumption that my critique is intended to trivialize is again groundless. My goal was to seek clarity.\n\n> Belief of state\n\nTo clarify, I was asking about approximate belief state, like what these variational approaches learn. \n\n> \"State-of-the-art\u201d\n\nI did not see that you have updated your PDF. \n\n> We sincerely believe the paper should be reviewed by evaluating the contribution and significance, instead of criticizing the usage of terminology.\n\nIt is essential to support claims of being SOTA with solid empirical evidence. The usage of such terminology is not just a semantic issue; it reflects the scope and impact of your claims.\n\nFinally, my objective is not to criticize but to ensure the clarity, accuracy, and significance of your contributions to the field."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540024415,
                "cdate": 1700540024415,
                "tmdate": 1700540094316,
                "mdate": 1700540094316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hVQJCywHd",
                "forum": "B5kAfAC7hO",
                "replyto": "UzX03N0Wg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
                ],
                "content": {
                    "title": {
                        "value": "My main concerns are not resolved and I maintain my score"
                    },
                    "comment": {
                        "value": "Thanks for your replies. As the discussion is close to end, I do not expect further responses to my following comment and summary. I believe with additional efforts, your work has the potential for publication in a top-tier conference.\n\n> We believe \u201cstraightforward\u201d and no \u201ctechnical significance\u201d might not be appropriate to be used here.\n\nI'd like to clarify my previous statement: I mentioned the work has \"not much significance\", not \"no significance\". I trust this misquotation was accidental.\n\nRegarding the use of \"straightforward\", I maintain its appropriateness. The key observations, as the paper stated, are straightforward in replacing belief state with history, and history with $L$-step memory, based on your assumptions.\n\n> the claim of real-world problem, large $L$\n\nConcerning your assumption on $L$-step and its real-world applicability, testing with $L$ values between 2 to 5 is insufficient to claim easy implementation in real-world problems without empirical evidence. The validity of the linear structure assumption in real-world problems also remains unclear.\n\nI recommend softening the claim about real-world applicability to \"has the potential to be applied\", unless you provide empirical support.\n\n> \u201cBelief of state\u201d\n\nLet me be straightforward. Here I rewrite Eq. 17 with $l=1$:\n$$\n\\max_q E_{q(z_h \\mid x_h,a_h,o_{h+1})}[\\log P(o_{h+1} \\mid z_h)] - D_{KL}(q(z_h \\mid x_h,a_h,o_{h+1}) \\mid\\mid p(z_h\\mid x_h))\n$$\nThe first term is to reconstruct observations, and the second term is KL divergence between the posterior of $z$ and prior of $z$.  The DreamerV2 model learning objective is very similar to this, except that they have a reward prediction term (which I am unsure about your method) and the prior also conditions on $a_h$ (which I believe should be added). \n\n> state-of-the-art \n\nI found you changed it in the main PDF, but not supplementary material. Please also update the supp accordingly. \n\nFinally, I want to conclude my review. I maintain my rating due to the seemly limited significance of the work, dense writing style, unclear contributions and connections with prior work, some overreaching claims, and the tone and unprofessionalism of responses to my feedback (e.g., several anecdotal evidences and assumed negative intent on the reviewer) which I flagged for future ethics review."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695071894,
                "cdate": 1700695071894,
                "tmdate": 1700699108558,
                "mdate": 1700699108558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MP7uaZ7CPl",
            "forum": "B5kAfAC7hO",
            "replyto": "B5kAfAC7hO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
            ],
            "content": {
                "summary": {
                    "value": "This paper contributes a new algorithm for RL in structured POMDPs.\nIt proposes to use a latent variable model to learn a linear representation of the value function in L-step decodable POMDPs.\nThe proposed approach shows good performance in a large set of tasks when compared with baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The work is highly relevant for the RL community, as it explicitly tackles problems with partial observability, a fundamental challenge for applying RL in real-world tasks.\n\n- The method proposed is relatively novel, as it combines efficient linear representations with L-step decodable POMDPs.\n\n- The empirical evaluation considers many tasks and shows the proposed method has strong performance compared with multiple baselines."
                },
                "weaknesses": {
                    "value": "- The presentation could be improved. Some technical details are inconsistent or lack an appropriate definition (see detailed comments below). This makes important parts of the paper challenging to comprehend, such as the discussion of Eq 9.\n\n- The paper is also very dense, which makes some parts too condensed. For instance, the theoretical analysis only states the assumptions and an informal version of the sample complexity of the algorithm without including an analysis of this result.\n\n- The empirical evaluation is limited to a comparison with other algorithms. It would be interesting to provide an ablation study to show how the different components of the algorithm contribute to its performance. For example, how the algorithm performs without optimistic exploration.\nFurthermore, it would be interesting to make a hyper-parameter sensitivity analysis, for example, evaluating how the algorithm performs with different values of L.\n\n\n[Detailed comments]\n- wrong typesetting of the observation function in the first paragraph of the preliminaries\n- in the preliminaries, should the agent receive a reward r(s_h, a_h)?\n- In the belief definition, it is unclear what is P(s1\\mid o1). It is also unclear what is \\tau.\n- Wrong index in the actions of Eq 2\n- Unclear what is \\theta on Eq 5?\n- \\mu is used for initial state distribution and as a feature map\n- are Eq 6 and Eq 9 missing some <> delimiters?\n- after Eq 7: an practical -> a practical\n- Eq 8: R(s,a) -> r(s,a)\n- Eq 2 and 3 are defined for problems with a finite horizon, then Eq 6 and 8 use discount factor $\\gamma$.\n- Eq 12 uses the latent variable z without a proper introduction"
                },
                "questions": {
                    "value": "1. After Eq 16, could you provide some intuition about what is the parameter l?\n\n2. Could you provide a formal definition of the policy \\mu_pi?\n\n3. The experimental evaluation mentions that the algorithms were tested after running 200K environment steps. Could you comment on this choice of training time? In particular, is this training budget sufficient for the convergence of all algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6845/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6845/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749582209,
            "cdate": 1698749582209,
            "tmdate": 1699636792967,
            "mdate": 1699636792967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Eeb9KR7wuM",
                "forum": "B5kAfAC7hO",
                "replyto": "MP7uaZ7CPl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wQNF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the suggestion on the presentation and other feedback. We address the concerns in the following:\n\n* Presentation \n\n  Despite the complicatedness of the related techniques and the subtleness in planning for POMDPs, which makes the paper technically dense, we try our best to make the paper self-contained, and design a simple algorithm. \n  We have revised our manuscript accordingly and try our best to make the paper more convenient to be understood. We would like to clarify some key typos in the previous manuscript\n\n* * Reward $r(s_h, a_h)$\n\n  In fact the agent can only observe a reward $r(o_h, a_h)$, which can be defined as $$r(o_h, a_h) = \\int r(s_h, a_h) P(o_h|s_h) ds_h$$ and can be viewed as a sample from $r(s_h, a_h)$, which is commonly assumed in the existing POMDP literature [1, 2]\n\n* * $P(s_1|o_1)$ and $\\tau$\n\n    $P(s_1|o_1)$ can be defined as the posterior distribution induced by the initial distribution and emission. $\\tau$ is the history defined in the second paragraph of the preliminaries.\n\n* * $\\theta$ in Equation 5\n\n    $\\theta$ is just a vector that can induce the reward.\n\n* Parameter l in Eq. 16\n\n  For any $l \\in N_+$, $p({o_{h+1:h+l}}|x_h, a_h) \\int_{\\mathcal{Z}} p(z_h|x_h, a_h)  p^\\pi ({o_{h+1:h+l}}|z_h) d z_h$ Is factorizable and share the same $p(z_h|x_h, a_h)$, but with different  $p^\\pi ({o_{h+1:h+l}}|z_h) $. \n  As we discussed that  $p(z_h|x_h, a_h)$ forms the function space for Q-function, and thus, is the latent variable representation we are seeking, we can perform MLE on arbitrary l-step future observations conditional distribution. \n\n* Definition for the moment matching policy $\\nu_\\pi$\n\n  The formal definition of \u03bd\u03c0 is complicated and we provide a formal definition at the beginning of the proof for Lemma 8. We also add more concrete discussion in Appendix C about the moment matching policy. \n\n* Ablation Study \n\n  We have added more ablation studies (Appendix H.1) to demonstrate the effects of the major components, including representation dimension and window size.   \n\n* Experimental setting\n\n  We set up the experiment setting exactly following the benchmark [1], which has been widely adopted in [2,3,4] for fairness, especially for the comparison to LV-Rep with full observation in Table 1 (denoted as Best-FO), to demonstrate the performance gap in MDP and POMDP. \n\n[1] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.\n\n[2] Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In ICML, 2022\n\n[3] Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, and Bo Dai. Latent variable representation for reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023b.\n\n[4] Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Spectral decomposition representation for reinforcement learning. ICLR, 2023c."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381540143,
                "cdate": 1700381540143,
                "tmdate": 1700381540143,
                "mdate": 1700381540143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rHBY5jU0Ck",
            "forum": "B5kAfAC7hO",
            "replyto": "B5kAfAC7hO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_cvkF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6845/Reviewer_cvkF"
            ],
            "content": {
                "summary": {
                    "value": "While Partially Observable Markov Decision Processes (POMDPs) were introduced to address partial information in RL algorithms where full\nobservability is unavailable, such formulation brings computational challenges in learning, exploration, and planning, due to the non-Markovian dependence between observations. This paper aims to address the computational and statistical challenges in Partially Observable Markov Decision Processes (POMDPs). In particular, the authors develop a representation-based perspective that leads to a coherent framework and tractable algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies the problem of designing an efficient and practical RL algorithm for structured partial observations in RL frameworks. Authors introduce a structured POMDP with a low-rank property that allows for a linear representation, called Multi-step Latent Variable Representation (\u00b5LV-Rep), which is a counterpart of linear MDP in the POMDP context. As such, this representation overcomes computational barriers and enables a tractable representation of the value function. The extension of linear MDP to POMDP can be beneficial.\n\nThe paper also proposes a planning algorithm that can implement both the principles of optimism and pessimism in the face of uncertainty for online and offline POMDPs. \n\nTheoretical analysis in sample complexity and PAC guarantee are provided to justify the performance guarantee. \n\nEmpirical comparisons are performed to demonstrate the performance on a set of benchmark environments compared to existing SOTA RL algorithms for POMDPs."
                },
                "weaknesses": {
                    "value": "1. The theoretical analysis relies on quite a few assumptions (including (Finite Candidate Class with Realizability, Normalization Conditions, Regularity Conditions and Eigendecay Conditions), which may not always fulfilled in reality. Can authors comment on the performance of the algorithms when these assumptions break, e.g., how worse the performance is going to be, and which of the assumptions are essential to retain the performance? \n\n2. The theoretical analysis is mainly based on Ren et al., 2023a. It is unclear what are the technical novelties in the analysis compared to Ren et al., 2023a. Authors are expected to explain the difference and highlight the key insights in the proofs.  In particular, the proof of Theorem 12 is unclear by just claiming \"This is a direct extension of the proof of Theorem 9 in Ren et al. (2023a)\". The technical contribution in theory remains questionable.\n\n3. Algorithmically, the proposed main algorithms borrow lots of the elements from Ren et al., 2023a, the novelty appears to be limited.\n\n4. In section 4, it is unclear how the planning algorithm implements pessimism for offline RL.\n\n5. Can authors comment on the tightness of the sample complexity bounds in Lemma 11?"
                },
                "questions": {
                    "value": "See above. In addition, there are some minor grammatical errors in the draft. It is suggested that authors carefully proofread the draft for improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815240125,
            "cdate": 1698815240125,
            "tmdate": 1699636792814,
            "mdate": 1699636792814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4ADo8TRH5H",
                "forum": "B5kAfAC7hO",
                "replyto": "rHBY5jU0Ck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6845/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cvkF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. Please see our response below:\n\n* Assumptions\n\nWe first emphasize that the assumptions are merely for the proofs, which are commonly used in the literature (e.g. [1, 2, 3, 4]) and are thought to be mild. While the proposed \u00b5LV-Rep is still  widely applicable even without the assumptions in practice. In fact, in our empirical experiments, the assumptions are not guaranteed to be held. But the proposed algorithm still outperforms or is comparable to the existing strong competitors, including DreamerV2, which demonstrates the robustness of the proposed algorithm in practice. \n\n* Novelty and Significance comparing to Ren et al. (2023a)\n\nAs we mentioned, our core contribution is not theoretical analysis. Instead, our contribution lies on extending LV-Rep for POMDP that can perform provable representation learning and planning with function approximations (as illustrated in Section 4), which is highly nontrivial and significant as we discussed in Section 3, especially considering the research on planning in POMDPs has stuck for decades.\nWe emphasize the simplicity of the algorithm should be considered as an advantage, which enables the LV-Rep to more practical settings to handle partial observations, rather than a drawback.\n\n* Pessimism in Offline\n\nThe pessimism mechanism in offline setting is similar to the optimism in the online setting, and the major difference between the optimism in online setting and pessimism in offline setting is that, in the offline setting we **minus** the same bonus from the reward while in the online setting we **add** the bonus into the reward. We already added the corresponding discussion in Appendix D our updated version.\n\n* Tightness of Bounds\n\nFirst, we emphasize that our sample complexity bound is for POMDPs, which matches the lower bound dependency on the horizon of actions [1]. Regarding the tightness of the dependency on the other quantities in the sample complexity, it is still an open problem even for  the existing representation learning methods including [2, 3, 4].\nThe proposed  \u00b5LV-Rep is the first practical representation learning algorithm for POMDPs with tractable planning and exploration, with strong theoretical guarantees, as far as we know. How to provide better representation learning algorithms with better sample complexities is still an open problem and beyond the scope of this manuscript.\n\n[1] Efroni, Yonathan, et al. Provable reinforcement learning with a short-term memory. In International Conference on Machine Learning, 2022.\n\n[2] Agarwal, Alekh, et al. \u201dFlambe: Structural complexity and representation learning of low rank mdps.\u201d Advances in neural information processing systems 33 (2020): 20095-20107. \n\n[3] Uehara, Masatoshi, et al. \u201dRepresentation Learning for Online and Offline RL in Low-rank MDPs.\u201d International Conference on Learning Representations. 2022. \n\n[4] Ren, Tongzheng, et al. \u201dLatent Variable Representation for Reinforcement Learning.\u201d The Eleventh International Conference on Learning Representations. 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6845/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381225722,
                "cdate": 1700381225722,
                "tmdate": 1700381225722,
                "mdate": 1700381225722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]