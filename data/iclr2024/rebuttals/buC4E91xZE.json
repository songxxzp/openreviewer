[
    {
        "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection"
    },
    {
        "review": {
            "id": "rnuWP8LMjM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
            ],
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "content": {
                "summary": {
                    "value": "This paper proposes a zero-shot anomaly detection framework using CLIP and object-agnostic text prompts with learnable prompt tokens and a modified attention mechanism."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow overall\n\nExperiments are comprehensive, with many datasets and evaluation metrics, as well as the ablation study\n\nVLMs for anomaly detection is relatively under-explored and prompt learning is especially new amongst AD literature."
                },
                "weaknesses": {
                    "value": "Little/no discussion about the additional cost of training\n\nNo comparison of different ViT backbones of CLIP\n\nFigure 6 feels confusing and unnecessary\n\nIt would be better to provide more intuition why the Focal and Dice loss are necessary\n\nFigure 2 caption should be more explanatory"
                },
                "questions": {
                    "value": "Besides all of the other modifications such as DPAM, why are object-agnostic prompts better than object-aware prompts for a specific object class?\n\nIs it true that hidden layer visual embeddings are only used for the segmentation map, while the final output embedding is only used for the image-level score?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9222/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9222/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9222/Reviewer_5wSC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697376022035,
            "cdate": 1697376022035,
            "tmdate": 1699637160685,
            "mdate": 1699637160685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j5JmOgMWRV",
                "forum": "buC4E91xZE",
                "replyto": "rnuWP8LMjM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5wSC (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive review and encouraging comments.\n\n**Q1: Little/no discussion about the additional cost of training**\n\nWe have supplemented the analysis about the additional cost of training by measuring the time taken during training (training time per epoch) in Table 8 of Appendix D. For a fair comparison, all experiments are conducted in a single NVIDIA RTX 3090 24GB GPU, and the GPU is kept free before evaluation. In Table 8, AnomalyCLIP takes 13.71 min per epoch on MVTec AD (The total number of samples is 1725) and only requires a total of 15 epochs for the whole fine-tuning. Once AnomalyCLIP finishes fine-tuning, AnomalyCLIP can be applied to different datasets and domains without additional training. We also compare AnomalyCLIP with other baselines that need auxiliary data (i.e., CoOp and VAND). For example, the minimum training time per epoch is 12.25 min of CoOp, and hence, AnomalyCLIP has a similar training computation cost as CoOp.\n\n**Q2: No comparison of different ViT backbones of CLIP**\n\nWe have added the backbone ablation study in Table 13 in Appendix D. We originally used ViT-L-14 as the backbones of CLIP, which is a representative backbone in CLIP. To fully explore the effect of different sizes of backbones, we use ViT-B-16-plus and ViT-H-14, which is a smaller and larger model than ViT-L-14, respectively. Compared to pixel-level performance 90.5% AUROC and 90.8% AUROC from ViT-B-16-plus and ViT-H-14, ViT-L-14 achieves the best performance 91.1% AUROC. The results indicate that AnomalyCLIP maintains promising performance across various ViT backbones. Meanwhile, it also reveals that increasing the model size does not necessarily result in improved performance. The larger model may introduce a strong bias towards object semantics, hindering AnomalyCLIP's ability to learn object-agnostic text prompts capturing generic normality and abnormality in an image. However, a smaller backbone like ViT-B-16-plus could cause AnomalyCLIP to struggle with generalization to unseen anomalies.\n\n**Q3: Figure 6 feels confusing and unnecessary**\n\nSince CLIP is pre-trained to align the textual semantics and the corresponding object, it harms the representation of local visual semantics which is important to anomaly localization. Therefore, AnomalyCLIP proposes DPAM to improve the local visual semantics. The DPAM strategy has three variants, i.e., $Q$-$Q$, $K$-$K$, and $V$-$V$ attention. To investigate the effect of different variants, we present their respective ZSAD performance in Figure 6. We agree that these results are not as important as the other results in justifying the effectiveness of AnomalyCLIP; they are presented to provide an insight into the effectiveness of the attention mechanism in CLIP on ZSAD since CLIP has emerged to be one key tool to facilitate this task.\n\n**Q4: It would be better to provide more intuition why the Focal and Dice loss are necessary**\n\nWe thank the reviewer for pointing this out. Although local features are refined by DPAM, there still exists misalignment among textual semantic and local visual semantics, especially for unseen objects. Focal and Dice loss play a crucial role in optimizing local context. They are introduced to empower object-agnostic text prompts to focus on fine-grained, local abnormal regions from intermediate layers of the visual encoder. As mentioned in Section 3.2, Focal loss addresses the imbalance between anomaly and normal pixels, typically caused by the smaller size of anomalous regions. Meanwhile, Dice loss aims to precisely constrain the anomaly boundary by measuring the overlap between the predicted segmentation ($S_n$/$S_a$) and the ground truth mask. To provide a more comprehensive analysis, we have included an ablation study on Focal and Dice loss in Table 14 of Appendix D. Compared to scenarios without local context optimization, Dice loss improves the pixel-level and image-level performance from 80.3%AUROC to 87.2%AUROC and 86.6%AUROC to 90.1%AUROC in pixel level on MVTec AD and VisA. Focal loss also brings the performance gain of 10.3%AUROC and 8.3%AUROC. Combining Focal and Dice loss, AnomalyCLIP achieves the best results (i.e., 91.1%AUROC and 95.5%AUROC). Note that the global context optimization is always used during the ablation, since we need at least one loss function to drive the optimization.\n\n**Q5: Figure 2 caption should be more explanatory**\n\nThank you for your effort on helping improve the paper. We have added more illustrations in the revised version: \"To adapt CLIP to ZSAD, AnomalyCLIP introduces object-agnostic text prompt templates to capture generic normality and abnormality regardless of the object semantics. Then, we introduce glocal context optimization to incorporate global and fine-grained anomaly semantics into object-agnostic textual embedding learning. Finally, textual prompt tuning and DPAM are used to support the learning in the textual and local visual spaces of CLIP.\\\""
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619254376,
                "cdate": 1700619254376,
                "tmdate": 1700619398839,
                "mdate": 1700619398839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YXXoHHc3su",
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_vcXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_vcXy"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets ZSAD leveraging visual language pretrained models. To learn object-agnostic information, this paper proposes to use the well-known text prompt technique, allowing the model to focus on the abnormal image regions rather than the object semantics. Experiments are conducted on several anomaly detection benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe contribution is clear, with well-organized paper architecture.\n2.\tThe motivation sounds reasonable.\n3.\tExperiments show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The motivation behind DPAM appears to be relatively weak. It is not entirely clear why employing V-V self-attention would preserve more localized information compared to the original Q-K attention. Relying solely on a single instance shown in Figure 3 may not furnish sufficient evidence to support this assertion. Furthermore, the rationale for treating these features separately remains unclear. What might be the outcome if we were to combine Q-Q, K-K, V-V, and Q-K features into an ensemble?\n\n2. The ablation studies lack clarity regarding the specific details of each component, making it challenging to assess the actual contributions of the proposed method:\n(i) The experiments in Table 4 involve the incremental addition of modules (T1-T4), but a more informative approach would be to examine the performance when each of them is removed. For instance, it is unclear how DPAM (T1) performs when used in conjunction with both T2, T3, and T4. So an importment setting should be removing T1 but keeping using T2-T4. Additionally, the increase in AUC from 47.9 to 54.8 by introducing T1 does not signify a significant improvement (both values are close to random guessing) for a two-class classification problem.\n(ii) In Table 5, it remains unclear how the models are optimized when both the global and local losses are absent."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697703426216,
            "cdate": 1697703426216,
            "tmdate": 1699637160560,
            "mdate": 1699637160560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sJtinl2lDr",
                "forum": "buC4E91xZE",
                "replyto": "YXXoHHc3su",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vcXy (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and insightful comments.\n\n**Q1: The motivation behind DPAM appears to be relatively weak. It is not entirely clear why employing V-V self-attention would preserve more localized information compared to the original Q-K attention.**\n\nThank you for raising this concern. Please refer to our **General Response** above for an intuitive explanation of DPAM strategy from the attention mechanism.\n\n**Q2: Relying solely on a single instance shown in Figure 3 may not furnish sufficient evidence to support this assertion.**\n\nWe have added experiments to study the effect of DPAM in Table 7 of Appendix D. Without our proposed DPAM module, AnomalyCLIP shows a decrease from 91.1% AUROC to 87.9% AUROC in pixel-level performance and from 91.5% AUROC to 80.7% AUROC in image-level performance on MVTec AD. Additionally, there is a decrease from 95.5% AUROC to 91.9% AUROC in pixel-level performance and from 82.1% AUROC to 73.0% AUROC in image-level performance on VisA. The results demonstrate the superiority of $V$-$V$ attention over $Q$-$K$ attention. However, the performance decreases at the image level is more pronounced than that at the pixel level. This discrepancy is attributed to the fact that the total loss places greater emphasis on local context optimization, driven by a larger local loss compared to the case with DPAM. As a result, AnomalyCLIP lacks sufficient optimization for global alignment, leading to a significant decline in image-level performance. Note that the original CLIP (no DPAM) exploits $Q$-$K$ features and suffers from weak segmentation performance.\n\n**Q3: What might be the outcome if we were to combine Q-Q, K-K, V-V, and Q-K features into an ensemble?**\n\nThank you for your helpful comments. Ensembling different features is an excellent idea. We have added two experiments to explore two ensemble methods, namely AnomalyCLIP$\\_{ensemble1}$ and AnomalyCLIP$\\_{ensemble2}$, involving the ensemble of **Q-Q, K-K, and V-V** and the ensemble of **Q-K, Q-Q, K-K, and V-V**, respectively in Table 12 in Appendix D. We average the logit output of different features for the ensemble. In Table 12, AnomalyCLIP$\\_{ensemble1}$ shows performance improvement by leveraging the advantages of three DPAM features. However, while AnomalyCLIP$\\_{ensemble2}$ outperforms the $Q$-$K$ feature version, it experiences a performance decrease compared to $V$-$V$ from 91.1%AUROC to 90.7%AUROC in pixel level and 91.5%AUROC to 90.8%AUROC in image level on MVTec AD. There is also a decline from 95.5%AUROC to 94.9%AUROC in pixel level and 82.1%AUROC to 80.7%AUROC in image level on VisA. The decline in performance upon adding $Q$-$K$ features to AnomalyCLIP$_{ensemble1}$ suggests that the $Q$-$K$ feature fails to provide valid local visual semantics to facilitate ZSAD. Note that the original CLIP exploits $Q$-$K$ features and gets weak segmentation performance. The good pixel-level performance of $Q$-$K$ in AnomalyCLIP is attributed to local optimization, where the object-agnostic prompt helps alleviate the disrupted local visual semantics of $Q$-$K$."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619176113,
                "cdate": 1700619176113,
                "tmdate": 1700620355674,
                "mdate": 1700620355674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ln4gSCZ16b",
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_GzuZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_GzuZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a zero-shot anomaly detection/segmentation method utilizing CLIP and prompt learning. The authors observe that CLIP-like VLMs learn class semantic features of the object and pay less attention to normality/abnormality, e.g., associated with fine-grained local features. They assume that the normality/abnormality features can be object-agnostic. For adapting the pretrained CLIP to focus on normality and abnormality features, they proposed to learn two text prompts for each respectively. To learn these generic prompts, they utilize both global optimization, i.e., maximizing similarity between image embeddings and text embeddings, and local optimization, i.e., maximizing similarity between image-patch embeddings and text embeddings. They also show that adding more learnable tokens to the text encoder and image encoder of CLIP can improve results, and using V-V attention can help in refining the local features. The paper provides a comprehensive empirical evaluation of 17 real-world datasets showcasing the effectiveness and advantages of the proposed method over existing zero-shot anomaly detection baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper brings the principle of prompt learning for CLIP-like VLMs in zero-shot anomaly detection. \n2. The proposed method achieves remarkable zero-shot anomaly detection performance on both industrial anomaly detection datasets and medical datasets.\n3. The paper provides a large body of ablation studies for each involved design component individually."
                },
                "weaknesses": {
                    "value": "1.  My largest worry is the paper relies on an optimistic assumption of abnormality, namely, the abnormality is object-agnostic.  The assumption might hold in some cases. The learned model is able to detect the abnormality that has similar abnormal patterns to the features in the auxiliary training set, e.g., detecting the defects of a manufactured part. However, the assumption can also fail in some cases. For example, the model may misclassify a product with texture patterns similar to scratches as an abnormal one, even though the scratch-like textures are normal texture patterns of the product.  It would be better to explicitly discuss the successful cases and the potential failures of the proposed method.\n\n2. Minor comment: $S_n$ and $S_a$ are first-time used in $L_{local}$ on page 5, but are clearly defined on page 6. Having the definitions and equations of $S_n$ and $S_a$ before $L_{local}$ would be good.\n\n3. The discussion of some related work about zero-shot anomaly detection, e.g., [1,2,3], is missing.\n\n[1] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus Robert Muller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. Transactions on Machine Learning Research, 2022.\n\n[2] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pretrained model clip. In Proceedings of the AAAI conference on artificial intelligence, 2022\n\n[3] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt. Zero-Shot Batch-Level Anomaly Detection. Thirty-seventh Conference on Neural Information Processing Systems, 2023."
                },
                "questions": {
                    "value": "1. Why is learning two text prompts necessary? Do you have comparisons to $g_n = [V_1][V_2]...[V_E][object]$ and $g_a = [V_1][V_2]...[V_E][damaged][object]$?\n1. How is the performance of the variant with all components except DPAM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698440348290,
            "cdate": 1698440348290,
            "tmdate": 1699637160434,
            "mdate": 1699637160434,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WljEWDQqet",
                "forum": "buC4E91xZE",
                "replyto": "Ln4gSCZ16b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GzuZ (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and the insightful comments.\n\n**Q1: The model may misclassify a product with texture patterns similar to scratches as an abnormal one, even though the scratch-like textures are normal texture patterns of the product. It would be better to explicitly discuss the successful cases and the potential failures of the proposed method.**\n\nThank you for raising this concern. We agree that in a minority of cases, AnomalyCLIP exhibits false detection due to the object-agnostic assumption. However, ZSAD requires detection models to detect anomalies without any training sample in a target dataset, and capturing the object-agnostic anomaly patterns is a promising solution because anomalies from different application scenarios typically have substantial variations in their visual appearance, foreground objects, and background features. As suggested by the reviewer, we have supplemented the experiment to explicitly discuss the successful cases and the potential failures in Figure 7 in Appendix D. In Figure 7(a), AnomalyCLIP accurately detects scratch-like patterns on the product, even when they typically appear in the texture. However, false detection occurs when scratch-like patterns are situated in the background, as depicted in Figure 7(b). Meanwhile, we also show the color stain patterns. As shown in Figure 7(c), AnomalyCLIP successfully detects the color stain, which exhibits subtle visual differences from the detected entities. On the other hand, AnomalyCLIP may face challenges when the normal region displays patterns that are almost indistinguishable to the naked eye from anomalies. For instance, in skin cancer detection, the normal regions are falsely detected as anomalies are visually similar to the disease region in Figure 7(d). Also, the stain interference in the background is a problem. These failure cases illustrate that the importance of mitigating background interference and achieving fine-grained discrimination, especially in cases of visually similar abnormalities. Exploring these challenges for enhancing ZSAD is a valuable direction for future research.\n\n**Q2: Minor comment: $S_n$ and $S_a$ are first-time used in $L_{local}$ on page 5, but are clearly defined on page 6. Having the definitions and equations of $S_n$ and $S_a$ before $L_{local}$ would be good.**\n\nThank you for your effort on helping improve our paper. We have updated the definitions of $S_n$ and $S_a$ before $L_{local}$.\n\n**Q3: The discussion of some related work about zero-shot anomaly detection, e.g., \\[1,2,3\\], is missing.**\n\nWe thank the reviewer for pointing us towards these related works. We have updated our related work section and added the works suggested by the reviewer. CLIP-AD \\[1\\] and ZOC \\[2\\] are early works in the utilization of CLIP for anomaly/out-of-distribution detection task, and they primarily focus on anomaly classification tasks, whereas AnomalyCLIP jointly optimizes anomaly detection and segmentation tasks, enabling both tasks within a single forward pass during inference. Despite ACR \\[3\\] addressing detection and segmentation tasks, it requires distinct auxiliary data for specific zero-shot detection tasks. AnomalyCLIP can adapt to various datasets and domains for zero-shot detection by learning object-agnostic prompts only once.\n\n>**References:**\n>1. Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus Robert Muller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. Transactions on Machine Learning Research, 2022.\n>2. Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pretrained model clip. In Proceedings of the AAAI conference on artificial intelligence, 2022\n>3. Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt. Zero-Shot Batch-Level Anomaly Detection. Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\n**Q4: Why is learning two text prompts necessary?**\n\nWe have supplemented experiments to investigate the effect of shared prompts in Table 11 in Appendix D. When sharing the learnable word embeddings of $g_n$ and $g_a$, AnomalyCLIP achieves 90.5% AUROC in pixel level and 90.9% in image-level on MVTec AD and 95.0% AUROC in pixel level and 81.5% AUROC in image level on VisA. The results show that AnomalyCLIP without sharing also works well for ZSAD and the efficiency of our object-agnostic prompt learning. However, the shared prompt performs slightly worse than the unshared prompts (used in the original paper). The performance decrease is 0.6%AUROC and 0.6%AUROC in image level on MVTec AD and Visa, and 0.5%AUROC and 0.6%AUROC in pixel level. We believe that the separate learning for these two prompts helps discriminate the generic normality and abnormality because when we share the parameters of $V_i$ and $W_i$, the learned semantics of normal and anomaly may be confused."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618612979,
                "cdate": 1700618612979,
                "tmdate": 1700618758207,
                "mdate": 1700618758207,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ra8HO7eYIk",
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_w1Ea"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_w1Ea"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method named AnomalyCLIP which deal with the Zero-shot anomaly detection problem. It leverages the CLIP prompt learning techniques to capture the normal/abnormal information within an image regardless of its foreground objects. The experiment results show the effectiveness of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tLeveraging pre-trained model like CLIP to address anomaly detection is a good direction and an interesting topic.\n2.\tThe paper shows comprehensive and detailed experiments and results which outperforms other baselines."
                },
                "weaknesses": {
                    "value": "1.\tThe idea and some techniques, like glocal context optimization, are similar to a concurrent work[1]. The author may compare with highly related works.\n[1] Gu, Zhaopeng, et al. \"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models.\" arXiv preprint arXiv:2308.15366 (2023).\n2.\tThe DPAM strategy is confusing. The author claims that the Q-Q, K-K, V-V self-attention suffers from different issues, and V-V self-attention derives the best result. However, these three variants are very similar in the DPAM mechanism. It is better to explain it in the paper and show the results of the original Q-K attention.\n3.\tI doubt the contribution of some modules. The module ablation results indicate that the T2 object-agnostic text prompts module contributes the most, similar to CoOp method. However, the author employed a different prompt for the CoOp experiment. For instance, the text prompt template for normality is defined as [V1][V2]...[VN][normal][cls], while that for abnormality is [V1][V2]...[VN][anomalous][cls]. This variation in prompt settings doesn't allow for a fair comparison."
                },
                "questions": {
                    "value": "See Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643021148,
            "cdate": 1698643021148,
            "tmdate": 1699637160325,
            "mdate": 1699637160325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MfizVk3rNJ",
                "forum": "buC4E91xZE",
                "replyto": "ra8HO7eYIk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer w1Ea (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your review and the insightful comments.\n\n**Q1: The idea and some techniques, like glocal context optimization, are similar to a concurrent work \\[1\\].**\n\nThank you for pointing us toward this concurrent work. AnomalyGPT \\[1\\] is a great work for anomaly detection. Below we summarize the difference between AnomalyGPT and AnomalyCLIP as follows:\n\n1. The scope is different. AnomalyGPT **eliminates the need for manual threshold adjustments** via LLM when detecting the seen objects during training. However, AnomalyCLIP aims to **improve the model generalization to unseen objects across diverse datasets and domains.**\n2. We focus on different settings. AnomalyGPT focuses on **unsupervised anomaly detection** and **few-shot anomaly detection**. However, our work aims at **zero-shot anomaly detection**.\n3. Although both AnomalyGPT and AnomalyCLIP propose to combine local and global loss, the purpose is different. AnomalyGPT uses global loss to **quantify the disparity between the text sequence generated by the model and the target text sequence**, ensuring LLM produces corresponding textual descriptions. However, our proposed global context optimization serves to **align the global visual semantics for anomaly classification.**\n4. The supervision information is different. Besides the images and text prompts, AnomalyGPT needs **human-crafted text descriptions for different objects to query the LLM during fine-tuning**. AnomalyCLIP just **uses the images and text prompts.**\n5. We introduce object-agnostic prompt learning to capture the generic normality and abnormality. AnomalyGPT uses human-crafted text prompts to produce textual embeddings.\n6. We propose DPAM to persevere local visual semantics of the visual encoder without additional training. AnomalyGPT introduces a lightweight image decoder to learn the adaption of local features.\n\nIn summary, since these two works have different scopes and focus on different aspects of anomaly detection, and the required supervision information is also different. We can not provide a comparison of the quantitative results between AnomalyGPT and AnomalyCLIP. Instead, we include and discuss AnomalyGPT in Related Work in the revised manuscript, which is marked in orange.\n\n>**Reference:**\n>1. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anomalygpt: Detecting industrial anomalies using large vision-language models, 2023.\n\n**Q2: The DPAM strategy is confusing. These three variants are very similar in the DPAM mechanism. It is better to explain it in the paper and show the results of the original Q-K attention.**\n\nThank you for raising this concern. Please refer to **General Response** for a detailed explanation of the DPAM strategy from the attention mechanism. We have also explained the reason why the $V$-$V$ achieves the best performance among DPAM variants there. We have added the response in **Refinement of the local visual space** in Section 3.3 in the updated paper, as well as a more detailed analysis in Appendix C."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618455248,
                "cdate": 1700618455248,
                "tmdate": 1700620259409,
                "mdate": 1700620259409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B2vrJZslkM",
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_xREe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_xREe"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors address Zero-shot anomaly detection (ZSAD) where training data is unavailable. They introduce AnomalyCLIP, which improves CLIP's generalization for ZSAD by using object-agnostic prompt learning. This allows for generic recognition of normality and abnormality across diverse images. AnomalyCLIP also incorporates both global and local anomaly contexts for optimization. Tested on 17 datasets, AnomalyCLIP demonstrates superior ZSAD performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introducing AnomalyCLIP for Zero-shot anomaly detection (ZSAD) stands out for its originality, exemplified by its novel approach of using object-agnostic text prompts for anomaly detection, a creative departure from traditional methods. The quality of the work is underscored by its robust methodology and extensive validation across 17 diverse datasets. The authors effectively communicate their ideas with clarity, making the complex concepts accessible. Significantly, AnomalyCLIP's ability to generalize across various domains without needing prompt redesign, particularly in industrial and medical settings, marks it as a notable advancement in the field of anomaly detection."
                },
                "weaknesses": {
                    "value": "A weakness in the paper is the unexplained initial use of the term \"glocal.\" Clarifying this key term when first mentioned would improve understanding and clarity."
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808189305,
            "cdate": 1698808189305,
            "tmdate": 1699637160203,
            "mdate": 1699637160203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IaQvHuhU6G",
                "forum": "buC4E91xZE",
                "replyto": "B2vrJZslkM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xREe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and encouraging review. When initially using the term 'glocal' in Section Introduction, we have added the explanation in the revised manuscript: \"We then introduce a novel ZSAD approach, called AnomalyCLIP, in which we utilize an object-agnostic prompt template and a glocal abnormality loss function (**i.e., a combination of global and local loss**) to learn the generic abnormality and normality prompts using auxiliary data.\\\""
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618215540,
                "cdate": 1700618215540,
                "tmdate": 1700618252634,
                "mdate": 1700618252634,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wQfLtPFbAh",
            "forum": "buC4E91xZE",
            "replyto": "buC4E91xZE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_NCKk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9222/Reviewer_NCKk"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a zero-shot anomaly detection (ZSAD) approach using CLIP, named AnomalyCLIP. Different from the previous ZSAD methods, AnomalyCLIP attempts to learn object-agnostic text prompts of normality and abnormality to segment abnormal part. To reach this, the authors leverages textual prompt tunning and DPAM technologies. Extensive evaluations conducted on 17 publicly datasets demonstrates the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The empirical evaluations are extensive covering 17 diverse benchmarks.\n\n2. The proposed approach attempts to learn class-agnostic prompts which seems to contradict the pretrained CLIP that is mostly used to classify semantic objects. The authors successfully mitigate this issue by several technical modules, e.g. object-agnostic text prompt design and DPAM layer."
                },
                "weaknesses": {
                    "value": "1. The prompt template might be too restrictive. \"damaged [cls]\" may not well represent all types of anomalies. For example, if a component is missing or applying the method to other domains than defect identification the proposed prompt template may not work well.\n\n2. According to Figure 2, the pipeline needs to feed the same images into two visual encoders, this would introduce additional computation overhead.\n\n3. The proposed DPAM layer lacks a theoretical basis. It is not clear why replacing Q-K attention map with V-V attention map without updating any visual encoder's parameters into CLIP would improve the performance.\n\n4. The paper has not discussed why the proposed approach is able to learning class-agnostic feature from the pretrained semantic CLIP as the CLIP is trained to be sensitive to semantic classes."
                },
                "questions": {
                    "value": "It is important to discuss why the \"damaged [cls]\" is suitable for more diverse anomaly detection scenarios.\n\nExplanations on the effectiveness of DPAM and computation overhead are necessary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9222/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839936654,
            "cdate": 1698839936654,
            "tmdate": 1699637160099,
            "mdate": 1699637160099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1IACrpmTxb",
                "forum": "buC4E91xZE",
                "replyto": "wQfLtPFbAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9222/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NCKk (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your review and the insightful comments.\n\n**Q1: The prompt template might be too restrictive. \"damaged \\[cls\\]\\\" may not well represent all types of anomalies.**\n\nWe are not sure about whether the reviewer refers to the object-agnostic text prompt \"damaged **\\[object\\]**\\\" (where `object` is a fixed token like `damaged`) or the object-aware text prompt \"damaged **\\[cls\\]**\\\" (where `cls` is a variable token that will be replaced with a specific class name in the algorithm implementation).In our submission, AnomalyCLIP uses the former one. Nevertheless, we posit that the reviewer's main concern is on the limited scope of the token \"damaged\\\" may have for anomaly detection.\n\nSince the pattern of anomaly is typically unknown and diverse, it is practically difficult to list all possible anomaly types. Thus, we agree that the prompt \"`damaged [object]`\\\" can be restrictive. To remedy this problem, during the fine-tuning process, we provide pixel-level anomaly ground truth with diverse anomaly types for training our ZDAD. This way helps substantially extend the scope of the prompt template \"`damaged [object]`\\\". It indicates in the linguistic semantic of the token \"`damaged`\\\", incorporating fine-grained anomaly semantics of diverse anomaly types. This intuition is verified by our ablation study for the prompt template in Table 16 and Table 17 of Appendix D, in which AnomalyCLIP shows stable performance across different datasets when replacing \"`damaged`\\\" with other similar tokens: \"`anomalous`\\\", \"`flawed`\\\", \"`defective`\\\", and \"`blemished`\\\".\n\nAs for the reason why such a prompt template can be applied to other domains, we believe that the learnable part of the prompt template is optimized to focus on the anomaly regions regardless of its foreground object, which allows our model to capture generic object-agnostic, or even domain-agnostic normality and abnormality. Further, compared to object-aware text prompt templates (i.e., \"`damaged [cls]`\\\"), excluding the object semantics from text prompt templates allows learnable text prompt templates to focus on capturing the characteristics of anomalies themselves, rather than the objects. Additionally, the strong generalization of the visual encoder is also important because it can project images from different domains into the same embedding space. Therefore, AnomalyCLIP can detect medical datasets even when fine-tuned on the industrial defect dataset, as shown in Table 2.\n\n**Q2: Explanations on the effectiveness of DPAM and computation overhead are necessary.**\n\nWe provide a detailed explanation of DPAM and attempt to analyze the effectiveness of the attention mechanism in **General Response**. **We would like to clarify that AnomalyCLIP does not need to feed the images into two visual encoders**. In Figure 2, we plot two paths to intuitively provide the computation difference between global and local visual embedding. **Without introducing additional parameters, we just create two paths when computing the attention map (DPAM), and the rest process shares the same visual encoder.** As for computation overhead, we have supplemented the assessment of the time taken during training (training time per epoch) and the inference speed (frames per second, FPS). For a fair comparison, all experiments are conducted in a single 3090 NVIDIA RTX 3090 24GB GPU, and the GPU is kept free before evaluation. In Table 8, AnomalyCLIP takes 13.71 min per epoch on MVTec AD (The total number of samples is 1725) and only requires a total of 15 epochs for the whole fine-tuning. Once AnomalyCLIP finishes fine-tuning, AnomalyCLIP can be applied to different datasets and domains without additional training. We also compare AnomalyCLIP with other baselines that need auxiliary data (i.e., CoOp and VAND). The minimum training time per epoch is 12.25 min of CoOp, and hence the training time taken is similar for fine-tuning methods. As for inference speed, CLIP achieves the 13.23 FPS. However, it suffers from weak detection performance. Although WinCLIP achieves better performance, WinCLIP has only 1.2 FPS because it needs multiple forward image patches to derive the segmentation. AnomalyCLIP outperforms WinCLIP and obtains an FPS of 8.92.\n\nWe also evaluated the computation overhead of DPAM separately. In Table 8, without DPAM, AnomalyCLIP takes 12.98 min to train per epoch. Compared to the 13.71 min for AnomalyCLIP with DPAM, we observe that introducing DPAM does not significantly increase the time complexity. This is attributed to the fact that DPAM only creates the two paths during the computation of the attention map and is frozen during fine-tuning, thereby avoiding the computationally expensive process of gradient computation. Meanwhile, DPAM also does not result in large computation overhead during inference: AnomalyCLIP w/ DPAM gets 8.92 FPS vs. 10.21 FPS for w/o using DPAM."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9222/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617962607,
                "cdate": 1700617962607,
                "tmdate": 1700617962607,
                "mdate": 1700617962607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]