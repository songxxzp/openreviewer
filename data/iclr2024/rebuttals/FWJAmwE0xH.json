[
    {
        "title": "Neural-Symbolic Recursive Machine for Systematic Generalization"
    },
    {
        "review": {
            "id": "uD8wqBXx6p",
            "forum": "FWJAmwE0xH",
            "replyto": "FWJAmwE0xH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_WJqb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_WJqb"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a Neuro-symbolic approach capable of strong systematic generalization: NeuralSymbolic Recursive Machines (NSR).\nThe method is made of 3 modules:\n1. a neural net perception module mapping raw input to grounded symbols. This can be a pre-trained CNN or transformer for Images and Text.\n2. a dependency parser to infer dependencies between grounded symbols in a structured syntax tree, termed Grounded Symbol System (GSS)\n3. a program synthesizer that deduces semantic meanings to a given symbol based on its neighborhood in the GSS tree.\n\nTo train this system with simple input-output (x, y) pairs and without any external expert knowledge or supervision for the GSS, the authors introduced a probabilistic learning method based on deduction-abduction: start with a greedy decoded and incorrect GSS tree, then refine step by step by looking at the potential neighbouring trees, until accurate results are obtained.\nMonte-Carlo sampling is done to sample potential trees.\n\nThe method is tested on three synthetic tasks (SCAN, PCFG, Hint) and a compositional machine translation task. In SCAN, PCFG, and compositional machine translation, NSR obtains 100% accuracy. On Hint, NSR beats all baselines including vanilla Transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a method that unifies the connectionism and the symbolism views of AI. While the attempt has been made multiple times in the past, the proposed method seems original and novel, although additional references could be cited (see the minor suggestions in the Weaknesses section)\n\nThe proposed method is well presented and the paper is easy to read. Figures 3 and 4 in particular made the content of the paper easier to understand and helped gain an intuition on what the method learns.\nExperiments clearly present the strength of the proposed approach over previous baselines.\n\nEventually, this paper addresses an important challenge of current neural architectures: systematic generalization, making this work significant."
                },
                "weaknesses": {
                    "value": "1. All results are comparing the proposed method NSR with various neural architectures and only one neuro-symbolic method: NeSS. The fact that NeSS performs 0% in 2 tasks and 100% in the other two makes it a weak comparing point (and also suggests that NeSS behaves more like a symbolic model than a neuro-symbolic one: it's all or nothing in terms of performance). I would suggest the authors provide at least 1 other neuro-symbolic method to compare against to make the results more significant. It is very clear that the proposed method outperforms vanilla neural methods such as Transformers, which is not surprising given the nature of the tasks being used, but it is less clear if the proposed method is significantly better than previous neuro-symbolic methods that also do not require additional training signal. The work from Minervini et. al. on Greedy Theorem Provers, or other variants could potentially be used as a baseline for some of these tasks.\n\n2. Another weakness of this paper is the ambiguous explanation of how the search for a GSS tree is terminated. Section 1 states that the search for a tree runs \"_until the accurate result is obtained_\", and Section 3.2 doesn't detail this point (or at least not very well). The authors should better define this stop criterion in order to better understand its limitation: what does it mean for the resulting tree to be \"accurate\"? Could the method settle on an \"almost correct\" syntactic tree to save time? and what would the effect of that be on performance?\n\n3. Eventually, at the end of Section 3, the authors state that the three modules of NSR exhibit equivariance and recursiveness. It would be beneficial to explain why this claim is true and provide additional evidence about it.\n\nThe following are minor suggestions:\n\n4.  In Table 3, for the task of compositional translation, it would be interesting to also evaluate the performance of a vanilla transformer like in the previous tables.\n\n5. the work could benefit from a discussion about previous neuro-symbolic works such as Neural Theorem Provers (NTPs) and Greedy NTPs: \"_Differentiable Reasoning on Large Knowledge Bases and Natural Language_\" by Minervini et. al, and previous work trying to add inductive biases to Transformers such as \"_Does Entity Abstraction Help Generative Transformers Reason?_\" by Gontier et. al."
                },
                "questions": {
                    "value": "- see \"weakness (2)\": Could the method settle on an \"almost correct\" syntactic tree to save time? and what would the effect of that be on performance?\n\n- What is the vocabulary size of the primitives considered? Did you try more complex sets of logical primitives? What do you think the effect would be on time and performance?\n\n- Do you have any hints of how to start thinking about representing probabilistic semantics like mentioned in the Limitation section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697668469330,
            "cdate": 1697668469330,
            "tmdate": 1699637084685,
            "mdate": 1699637084685,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zd1LYdH3ce",
                "forum": "FWJAmwE0xH",
                "replyto": "uD8wqBXx6p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for the constructive comments and the acknowledgment of the significance of our work. Below, we provide detailed replies to your comments and questions.\n\n> I would suggest the authors provide at least 1 other neuro-symbolic method to compare against to make the results more significant. The work from Minervini et. al. on Greedy Theorem Provers, or other variants could potentially be used as a baseline for some of these tasks.\n\nThank you for the suggestion! We appreciate your concern and will check Minervini et. al.'s work on Greedy Theorem Provers and other neural-symbolic methods to add more neural-symbolic baselines.\n\n> The ambiguous explanation of how the search for a GSS tree is terminated.\n\nThe search for a GSS tree stops when one of the following conditions is satisfied:\n1. the found GSS can generate the given ground truth `y`, i.e., the root node's value is equal to `y`;\n2. the search steps exceed the pre-defined maximum searching steps.\n\n> It would be beneficial to explain why the three modules of NSR exhibit equivariance and recursiveness.\n\nEquations 1, 2, and 3 demonstrate that in all three modules of the NSR system, the joint distribution is factorized into a product of several independent terms. This factorization process makes the modules naturally adhere to the principles of equivariance and recursiveness, as outlined in Definitions 3.1 and 3.2.\n\n> Suggestions: In Table 3, for the task of compositional translation, it would be interesting to also evaluate the performance of a vanilla transformer like in the previous tables. Discuss previous neuro-symbolic works such as Neural Theorem Provers (NTPs) and Greedy NTPs.\n\nThank you for the suggestions! We will incorporate them in the revision.\n\n> What is the vocabulary size of the primitives considered? Did you try more complex sets of logical primitives? What do you think the effect would be on time and performance?\n\nThe total number of primitives is 15. Whether adding more primitives is beneficial or detrimental depends on their relevance to the task at hand. Including primitives that aren't relevant can lead to longer search time, or could even cause the search to fail if it exceeds the maximum number of search steps allowed. On the other hand, if the new primitives are relevant and useful, they can shorten the length of the program and accelerate the search process.\n\n> Do you have any hints of how to start thinking about representing probabilistic semantics like mentioned in the Limitation section?\n\nProbabilistic programming languages (PPL) or frameworks might be useful for representing probabilistic semantics. PPLs are designed to handle uncertainty in a structured and systematic manner. They integrate probabilistic models with traditional programming concepts, enabling users to represent and manipulate uncertainty explicitly in their models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412236287,
                "cdate": 1700412236287,
                "tmdate": 1700412236287,
                "mdate": 1700412236287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a62Ih8lDYk",
                "forum": "FWJAmwE0xH",
                "replyto": "zd1LYdH3ce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Reviewer_WJqb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Reviewer_WJqb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarifications"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to address my questions and clarify some aspects. It is more clear now. Looking forward to the comparison with other neuro-symbolic baselines (and vanilla transformers for table 3)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682899140,
                "cdate": 1700682899140,
                "tmdate": 1700682899140,
                "mdate": 1700682899140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8oBPozDLec",
            "forum": "FWJAmwE0xH",
            "replyto": "FWJAmwE0xH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_Gi7u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_Gi7u"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a neuro-symbolic architecture called the NSR which consists of 3 steps:\n1. perception module to convert raw input into symbols\n2. a parser to compute a syntax tree over symbols\n3. a program induction module to convert this syntax over induced symbols into a program which can then convert an input into an output deterministically.\n\nEach of these components are separate probabilistic modules (though details about what these models are exactly is unclear from the paper). From results on 3 tasks, we see improvements on generalization compared to neural models."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The subject matter of the paper is to make progress towards improving compositional generalization in learnt models, which is a very important area."
                },
                "weaknesses": {
                    "value": "*Presentation is unclear*: There are very few details about the actual approach in the Section-3 (and Figure-1) to fully understand what exactly the model is (See questions). Unfortunately, because there is a lack of details around the approach, it is hard to do a thorough assessment of this work, and I request the authors to revise their draft.\n\nMoreover, the paper spends too much time (and math notation) on simple definitions such as \u201cequivariance\u201d and \u201crecursiveness\u201d and on flagposting \u201chypothesis\u201d statements. Not necessarily cause for rejection, but I highly suggest that these be moved into an appendix, so more time is spent on explaining the approach.\n\n\n\n*How general is this approach*: Most of the experiments here are on datasets where symbolic approaches are likely to help, but it is unclear how well this approach would do for natural language semantic parsing tasks such as GeoQuery. I'm not fully opposed to having experiments that are only on these programmatic datasets, but it would be good to have an extended discussion on what the symbols and programs look like for more natural data distributions."
                },
                "questions": {
                    "value": "Here are some details I could not get from Section-3:\n\n- What exactly are the symbols in T for each of the datasets? \n- Is every raw input mapped to a single symbol or is there a consolidation step where multiple raw inputs can be associated with the same symbol? \n- What models are used to parameterize all of the distributions in Eq~4? Are these neural networks?\n- What is the overall parameter count?\n- How does inference work for this model?\n- How does this compare to other neuro-symbolic systems, for example \"Neuro-Symbolic Concept Learner\" from Mao et al. 2019?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618921003,
            "cdate": 1698618921003,
            "tmdate": 1699637084577,
            "mdate": 1699637084577,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uoxYMB3t3S",
                "forum": "FWJAmwE0xH",
                "replyto": "8oBPozDLec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[1/2] Clarifications for the questions on the proposed approach"
                    },
                    "comment": {
                        "value": "We sincerely thank you for reviewing our paper and providing constructive feedback. Below, we provide detailed replies to your comments and hope these replies can resolve your major concerns.\n\n> *Presentation is unclear*: few details about the actual approach in Section-3 (and Figure-1), highly suggest moving simple definitions and statements to the Appendix and spending more space on explaining the approach.\n\nThank you for your feedback! We appreciate your suggestions and will incorporate them into the revision. Below, we provide detailed clarifications to your questions regarding our proposed approach.\n\n- > What exactly are the symbols in T for each of the datasets?\n\nFigure-1 shows the examples of `T  = <(x,s,v), e>` for each dataset. \n\nFor SCAN, `x` is the input word, e.g., `left`; `s` is the symbol id, e.g., `5`; `v` is the output actions, e.g., `[LTURN,JUMP]`. `(x,s,v)` forms a node, e.g., `left 5 [LTURN,JUMP]` in the first example of Figure-1. The edge `e` is the arrow, e.g., `left 5 [LTURN,JUMP] -> jump 4 [JUMP]`, which means the value of the node `jump 4 [JUMP]`, i.e., `[JUMP]`, is an input to the node `left 5 [LTURN,JUMP]`.\n\nFor PCFG, `x` is the input word, e.g., `swap`; `s` is the symbol id, e.g., `51`; `v` is the output letters, e.g., `[C,B,A]`. `(x,s,v)` forms a node `swap 51 [C,B,A]` in the second example of Figure-1. The edge `e` is the arrow, e.g., `swap 51 [C,B,A] -> A 0 [A]`, which means the value of the node `A 0 [A]`, i.e., `[A]`, is an input to the node `swap 51 [C,B,A]`.\n\nFor HINT, `x` is the input image, e.g., the image of `*`; `s` is the symbol id, e.g., `12`; `v` is the output number, e.g., `27`. `(x,s,v)` forms a node `* 12 27` in the third example of Figure 1. The edge `e` is the arrow, e.g., `* 12 27 -> 3 3 3`, which means the value of the node `3 3 3`, i.e., `3`, is an input to the node `* 12 27`.\n\n- > Is every raw input mapped to a single symbol or is there a consolidation step where multiple raw inputs can be associated with the same symbol?\n\nCurrently, every raw input is mapped to a single symbol.\n\n- > What models are used to parameterize all of the distributions in Eq~4? Are these neural networks?\n\n\\theta_p and \\theta_s are parameterized by neural networks. \\theta_l are functional programs, as illustrated by Figure 4(b).\n\n- > What is the overall parameter count?\n\nFor SCAN and PCFG, the parameter count is ~0.2M, mainly from the dependency parser. For HINT, the parameter count is ~11M, mainly from the image encoder ResNet-18.\n\n- > How does inference work for this model?\n\nAs illustrated in Figure 2, the neural perception module first maps the input x, e.g., a handwritten expression in Figure 1 (3) HINT, to a symbol sequence, `2 + 3 * 9`. The dependency parsing module then parses the symbol sequence into a tree in the form of dependencies, e.g., `+ -> 2 *`, `* -> 3 9`. Finally, the program induction module uses the learned programs for each symbol to calculate the values of the nodes in the tree in a bottom-up manner, e.g., `3 x 9 => 27, 2 + 27 => 29`.\n\n- > How does this compare to other neuro-symbolic systems, for example, \"Neuro-Symbolic Concept Learner\" from Mao et al. 2019?\n\nMao et al. 2019's NS-CL model relies on a pre-established domain-specific language (DSL) tailored for CLEVR, which limits its flexibility, as the meanings of operations like Filter, Relate, and Query are predefined and unchangeable. Moreover, NS-CL uses GRU for question parsing, a model that prior research [1,2] has identified as less effective in generalizing to longer sequences. In contrast, our model operates with minimal domain-specific knowledge, learns the semantics of symbols directly from data, and demonstrates significantly improved generalization capabilities.\n\n[1] Lake, et al. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. ICML (2018).\n\n[2] Li, et al. A minimalist dataset for systematic generalization of perception, syntax, and semantics. ICLR (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392445204,
                "cdate": 1700392445204,
                "tmdate": 1700412523602,
                "mdate": 1700412523602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lcoSWmFyW7",
                "forum": "FWJAmwE0xH",
                "replyto": "8oBPozDLec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[2/2] An example of how our model might work for natural language semantic parsing tasks such as GeoQuery"
                    },
                    "comment": {
                        "value": "> *How general is this approach*: it would be good to have an extended discussion on what the symbols and programs look like for more natural data distributions, like GeoQuery.\n\nTo discuss how our approach can generalize to natural language semantic parsing tasks, we construct the following example of how our model might work for GeoQuery.\n\nQuestion: `How many rivers are in California?`\n\nSQL query: `SELECT count(*) FROM rivers WHERE state = 'California'`\n\nHere's how our framework might model this example:\n1. The Perception module simplifies the question by removing unnecessary grammatical details:\n```\nhow-many rivers be in California\n```\n\n2. The Parsing module breaks down the simplified sentence into a parse tree.\n3. The Semantic module then generates the query step-by-step in a bottom-up manner. Here's the parse tree with intermediate results:\n```\n(be => [SELECT count(*) FROM rivers WHERE state = 'California']\n    (rivers => [SELECT count(*) FROM rivers]\n        how-many => [SELECT count(*)]\n    )\n    (in => [state = 'California']\n        California => ['California']\n    )\n)\n```\n\nAs discussed in Limitations of Section 5, training the entire framework from scratch on natural language datasets can be challenging. However, we have observed that the parse tree is quite similar to standard dependency structures. As a result, we can leverage existing parsers to initialize the modules in our framework, which could accelerate the training process.\n\nWe hope the above clarifications address your concerns and thank you again for your valuable suggestions. Please let us know if there is any further question."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392590495,
                "cdate": 1700392590495,
                "tmdate": 1700412542678,
                "mdate": 1700412542678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9BBUjNynqr",
            "forum": "FWJAmwE0xH",
            "replyto": "FWJAmwE0xH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_brEA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_brEA"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes a new neurosymbolic model, NSR, which consists of (1) a task-dependent model mapping from inputs to strings; (2) the Chen-Manning dependency parser; (3) a program induction module that is somehow based on DreamCoder. This pipeline is trained by gradient-based optimization (SGD?) using Metropolis-Hastings sampling to estimate the gradient. The proposed method performs well across four tasks, SCAN, PCFG, and HINT, and an artificial machine translation task."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a very interesting approach that achieves very good results on the four tasks tested. In every setting, their model either does the best, or is tied with NeSS because both models achieve 100% accuracy."
                },
                "weaknesses": {
                    "value": "Many statements are made like, \"This stark discrepancy underscores the pivotal role and efficacy of symbolic components\u2014specifically, the symbolic stack machine in NeSS and the GSS in NSR\u2014in fostering systematic generalization.\" But, for an outsider, no explanation is given for why the symbolic components actually lead to better generalization. I would like to see some more explanation or analysis to back this up.\n\nThe program induction module is not described in detail; in equation (3), what is the p in the right-hand side? When you say that you \"leverage\" DreamCoder, do you mean that this module simply is DreamCoder?\n\nFigure 3: image is wrong?"
                },
                "questions": {
                    "value": "Section 3.3: How do you use the gradients? Is this SGD?\n\nWhy is abduction called that? It seems different from abductive reasoning?\n\nDef 3.2: Isn't \"compositionality\" a more usual word for this? \"Recursive\" has a totally different meaning in theory of computation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776125193,
            "cdate": 1698776125193,
            "tmdate": 1699637084458,
            "mdate": 1699637084458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q0Ov33jYZc",
                "forum": "FWJAmwE0xH",
                "replyto": "9BBUjNynqr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your review of our paper and your constructive feedback. Your questions are answered in detail below:\n\n>More explanation or analysis on why the symbolic components actually lead to better generalization.\n\nWe think the effectiveness of symbolic components in fostering systematic generalization in neural networks, such as the symbolic stack machine in NeSS and the Grounded Symbolic System (GSS) in NSR, stems from their ability to introduce *structured, rule-based processing* into otherwise fluid neural computations. These symbolic systems act as a framework within which neural networks can operate more predictably and consistently, thereby enhancing their generalization capabilities. Besides, symbolic components can mitigate overfitting by imposing rule-based constraints on the learning process. These constraints ensure that the model does not merely memorize the training data but learns the underlying rules and structures, thereby improving its ability to generalize to new, unseen data.\n\n> In equation (3), what is the p in the right-hand side? \n\n`p` denotes the probability.\n\n> When you say that you \"leverage\" DreamCoder, do you mean that this module simply is DreamCoder?\n\nThe original DreamCoder does not support noisy examples, so we modified its search process to tolerate a ratio of noisy examples.\n\nWe kindly refer the reviewer to Appendix Section A for more details on the program induction module.\n\n> Figure 3: image is wrong?\n\nYes, thank you for pointing it out! We will correct it in the revision.\n\n> Section 3.3: How do you use the gradients? Is this SGD?\n\nYes, we use the Adam optimizer in practice.\n\n> Why is abduction called that?\n\nNaturally, this is a process of abductive reasoning. Consider the input `x` and output `y` as observations. The goal is to identify the `T` that most likely explains these observed values of `x` and `y`. This suggests that the discovered `T` may not necessarily match the actual ground-truth value of `T`.\n\n> Def 3.2: Isn't \"compositionality\" a more usual word for this?\n\nThank you for pointing it out! The word \"compositionality\" is indeed a more appropriate choice for Def 3.2 and we will update it in the revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412446864,
                "cdate": 1700412446864,
                "tmdate": 1700412446864,
                "mdate": 1700412446864,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uSE9iS2ZdH",
            "forum": "FWJAmwE0xH",
            "replyto": "FWJAmwE0xH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_ih16"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8657/Reviewer_ih16"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Neural-Symbolic Recursive Machine (NSR), a model for systematic generalization in sequence-to-sequence tasks. The key innovation is representing the problem as a Grounded Symbol System (GSS) with combinatorial syntax and semantics that emerge from training data. The NSR incorporates neural modules for perception, parsing, and reasoning that are jointly trained via a deduction-abduction algorithm. Through architectural biases like recursiveness and equivariance, the NSR achieves strong systematic generalization on tasks including semantic parsing, string manipulation, arithmetic reasoning, and compositional machine translation.\n\nOverall, the paper presents a novel neural-symbolic architecture that combines beneficial inductive biases from both neural networks and symbolic systems to achieve human-like generalization and transfer learning abilities. The experiments demonstrate strengths on challenging benchmarks designed to test systematic generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Compositional generalization is an interesting and important direction to explore, which should be one of the most important capabilities of human. Therefore, the problem and the research direction is important.\n- The Neural-Symbolic Recursive Machine (NSR) model is a novel model architecture centered around representing problems as grounded symbol systems. The deduction-abduction training procedure for coordinating the modules is an original contribution for jointly learning representations and programs.\n- The paper clearly explains the limitations of existing methods, the need for systematic generalization, and how the NSR model aims to address this. The model description and learning algorithm are well-explained. The experiments and analyses effectively demonstrate the claims.\n- The paper is technically strong, with rigorous definitions and detailed exposition of the model components and learning algorithm.\n- The experiments systematically test generalization across four datasets with carefully designed splits. The analyses provide insights into when and why the NSR architecture generalizes better than baselines.\n\nOverall, this is a technically strong and well-written paper that makes both conceptual and practical contributions towards an important research direction."
                },
                "weaknesses": {
                    "value": "Although I understand that compositional generalization is currently driven primarily by synthetic datasets like SCAN, I would still like to see the application of this method in real-world scenarios. For example, could it achieve significantly better generalization performance compared to conventional seq2seq models on real machine translation tasks?"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8657/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8657/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8657/Reviewer_ih16"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8657/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837325606,
            "cdate": 1698837325606,
            "tmdate": 1699637084356,
            "mdate": 1699637084356,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5NqKc7zuCW",
                "forum": "FWJAmwE0xH",
                "replyto": "uSE9iS2ZdH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for reviewing our paper!"
                    },
                    "comment": {
                        "value": "We sincerely thank you for reviewing our paper and acknowledging the novelty and effectiveness of the proposed method. Below, we provide a detailed response to your question.\n\n> Could it achieve better generalization performance than conventional seq2seq models on real machine translation tasks?\n\nOur proposed model achieves much better performance on a compositional machine translation task, as described in Section 4.4. Table 3 shows the comparison: our model can achieve perfect generalization accuracy (100\\%), while the conventional seq2seq model performs badly (12\\%)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393686052,
                "cdate": 1700393686052,
                "tmdate": 1700393686052,
                "mdate": 1700393686052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aMMkFactcA",
                "forum": "FWJAmwE0xH",
                "replyto": "5NqKc7zuCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8657/Reviewer_ih16"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8657/Reviewer_ih16"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment from Reviewer ih16"
                    },
                    "comment": {
                        "value": "Thanks the author for response! Yes I understand it is a more challenging task than SCAN, but it is still a synthetic benchmark. For a reference, maybe some datasets achieving the complexity of CFQ [1] in machine translation would be a great fulfillment. But I understand this may be beyond the scope of this article. Overall, the quality of this paper is good and should be accepted.\n\n[1]. Measuring Compositional Generalization: A Comprehensive Method on Realistic Data, ICLR 2022"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8657/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644236607,
                "cdate": 1700644236607,
                "tmdate": 1700644236607,
                "mdate": 1700644236607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]