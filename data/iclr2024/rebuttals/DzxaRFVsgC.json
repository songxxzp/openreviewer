[
    {
        "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"
    },
    {
        "review": {
            "id": "05ySuH7DrX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
            ],
            "forum": "DzxaRFVsgC",
            "replyto": "DzxaRFVsgC",
            "content": {
                "summary": {
                    "value": "This submission proposes a finer-grained visual instruction tuning to endow a pretrained large language model with the capabilities to \u201csee.\u201d Compared to the literature, this work is different in that it has region-level image understanding and flexibility in combining multiple ROI\u2019s and reasoning between them. It initializes the model from pretrained on vision and language tasks, respectively, and then uses a two-step instruction tuning pipeline to build multiple visual reasoning capabilities into it."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, the motivation, technique, presentation, and results look all solid to me. The anonymously released codebase looks usable, which, if true, would serve as a well-compiled dataset and starting point for later multi-modality research."
                },
                "weaknesses": {
                    "value": "I have to say I haven\u2019t followed multimodality research for some time, so my judgment on the novelty of this work (compared to the literature) could be rusty. This might be an important aspect to consider for this work, but my score is assuming this work is novel. \n\nOn presentation, a flowchart of the two-stage training process might be useful to readers. Indicating which datasets are used in which fashion at which stage, with the purpose of bringing about which kind of capabilities."
                },
                "questions": {
                    "value": "Some of my random thoughts, potentially useful to the authors:\n(1)\tHow about visual question-answering datasets? \n(2)\tIs it possible to design some synthetic tasks to train the model, perhaps inserted between the two stages you have right now, to smooth up the training process?\n(3)\tIs it possible to let the LM output coordinates somehow so to interact with users by referring to certain regions in the picture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697318523063,
            "cdate": 1697318523063,
            "tmdate": 1699636096237,
            "mdate": 1699636096237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7Oe9Y7R2OV",
                "forum": "DzxaRFVsgC",
                "replyto": "05ySuH7DrX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yj7F"
                    },
                    "comment": {
                        "value": "Dear Reviewer yj7F,\n\nThank you for your positive comments on our work. Our open-source code is complete and reproducible, and it has been widely adopted by the community. As the first work to utilize LLM to achieve general-purpose region understanding, we believe that we can serve as a solid baseline in this field.\n\nBelow are our responses to  your suggestions:\n\n(1). A flowchart of the two-stage training process\n\nIn the current presentation format, it does appear that the two stages with multiple datasets are somewhat confusing. Introducing a flowchart could indeed be a viable improvement, and we will strive to enhance the presentation format in future versions accordingly. Thank you for your valuable suggestion.\n\n(2). How about visual question-answering datasets?\n\nWhen aiming to enhance performance in image-level VQA tasks, a practical approach for GPT4RoI could involve integrating an off-shelter detector and combining region of interest features with image features to represent an image. Previous research [1] has demonstrated the effectiveness of this technique in image caption task. Given the specific focus of our work on region-understanding tasks, we will further investigate this approach in our new research.\n\n(3). Design some synthetic tasks\n\nUtilizing region-text pair data and ChatGPT, building region-level multi-round understanding and reasoning tasks is a practical way to improve the dialogue ability of GPT4RoI. A related study [2] has conducted similar work following our research.\n\n(4). LM output coordinates\n\nThis suggestion is highly valuable. Currently, GPT4RoI can only mention the input RoI and requires users or detectors to provide all the region of interest. Two remarkable follow-up works [3][4]  of GPT4RoI utilize a significant amount of visual grounding data, allowing LLM to output new regions. This enables the model to have an enhanced interactive experience. We will continuously improve the GPT4RoI to consolidate these two functions in the future.\n\n[1] Faster and Better Image Captioning Transformer Using Dual Visual Features\n\n[2] Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models\n\n[3] Ferret: Refer and Ground Anything Anywhere at Any Granularity\n\n[4] GLaMM : Pixel Grounding Large Multimodal Model"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699781646381,
                "cdate": 1699781646381,
                "tmdate": 1699781646381,
                "mdate": 1699781646381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hBgCOBYpnE",
                "forum": "DzxaRFVsgC",
                "replyto": "7Oe9Y7R2OV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reply"
                    },
                    "comment": {
                        "value": "Cool! Thank you for answering my questions. Good luck!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699803370035,
                "cdate": 1699803370035,
                "tmdate": 1699803370035,
                "mdate": 1699803370035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BatQfuQ48w",
            "forum": "DzxaRFVsgC",
            "replyto": "DzxaRFVsgC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_fWBC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_fWBC"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced the region-of-interest instruction with vision language models. By doing this, it proposed spatial instruction, combining language and the reference to region-of-interest I to an interleave sequence, enabling accurate region referring and enhancing user interaction. By spatial instruction tuning LLM with massive region-text datasets, the model can follow user instructions to solve diverse region understanding tasks, such as region caption and reasoning. The results show that the model outperforms the previous state-of-the-art approach on a wide range of region understanding benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposed a novel method with vision language tasks. It provides a detailed methodology for this.\n- A comprehensive discussion of experiments and results, where the figures are in good quality and readability.\n- The benchmark methods are of good quality."
                },
                "weaknesses": {
                    "value": "- For region caption, the paper only compares the proposal to one model GRiT, which is limited."
                },
                "questions": {
                    "value": "Can you interpret the performance of different tasks (ie Q->A, QA-> R, Q->AR) in Table 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704025132,
            "cdate": 1698704025132,
            "tmdate": 1699636096140,
            "mdate": 1699636096140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KOLbSiQqG6",
                "forum": "DzxaRFVsgC",
                "replyto": "BatQfuQ48w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fWBC"
                    },
                    "comment": {
                        "value": "Dear Reviewer fWBC,\n\nThank you for your appreciation of our work, particularly for recognizing the novelty and experimental results. Below are my responses to some of your concerns.\n\n(1). Why only compare to GRiT\n\nTo assess the region's comprehension capabilities, we rely on ground truth bounding boxes or detection model outputs as input. Consequently, access to the method source code is necessary for conducting the evaluation. **GRiT is the top-1 performance model among all open-sourced projects in the region-caption field**, and serves as a representative baseline in recent studies when specifically evaluating region-caption abilities. We will continuously track the latest advancements in the region caption field and update our results accordingly. We appreciate your valuable feedback and suggestions.\n\n(2). Q->A, QA-> R, Q->AR performance on VCR\n\nThe VCR dataset is a particularly difficult dataset for comprehension and reasoning. It has gained attention from several well-known companies, who have submitted their solutions on the leaderboard. The dataset's distinctive challenge is that a model not only needs to answer complex visual questions but also provide a rationale for why its answer is correct.\n\nThe VCR task consists of two subtasks: Question Answering (Q->A) and Answer Justification (QA->R). In the Q->A setup, a model is given a question and must select the correct answer from four choices. In the QA->R setup, a model is provided with a question and the correct answer, and it needs to justify the answer by selecting the most appropriate rationale from four choices. The performance of models is evaluated using the Q->AR metric, where accuracy is measured as the percentage of correctly answered questions along with the correct rationale."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699777273461,
                "cdate": 1699777273461,
                "tmdate": 1699777273461,
                "mdate": 1699777273461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ress3jgN9J",
            "forum": "DzxaRFVsgC",
            "replyto": "DzxaRFVsgC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a new approach called spatial instruction tuning, which aims to enhance the fine-grained multimodal understanding of vision-language models. The proposed model, GPT4RoI, incorporates references to regions-of-interest (RoI) in instructions by replacing them with RoI features and interleaving them with language embeddings. By training on region-text pair datasets, GPT4RoI enables interactive and conversational experiences, allowing users to interact with the model through both language and drawing bounding boxes. GPT4RoI achieves remarkable results on the Visual Commonsense Reasoning (VCR) dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tFine-grained multimodal understanding: GPT4RoI enables region-level alignment and understanding by incorporating references to RoIs in instructions, allowing for more detailed analysis and reasoning.\n-\tInteractive user experience: Users can interact with GPT4RoI through both language input and drawing bounding boxes.\n-\tGPT4RoI achieves remarkable accuracy on the VCR dataset, surpassing existing models by a significant margin."
                },
                "weaknesses": {
                    "value": "-\tExpanding from image-level to region-level instruction tuning seems like a natural progression, and the approach is straightforward without providing a fresh perspective. Some other papers also explore the region-level large language models [1] but lack the performance comparison. \n-\tIt appears that while this paper utilized more datasets for training, the improvement in results is relatively marginal, as shown in Table 5. \n-\tThis work lacks a comparison of parameters. The current models seem to be quite large, especially large language models. A comparison should be conducted at the same parameter level, e.g., Table 6. \n\n[1] ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning."
                },
                "questions": {
                    "value": "See Weaknesses\n\n-\tComparisons to other works on the same parameter level."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732044597,
            "cdate": 1698732044597,
            "tmdate": 1699636096069,
            "mdate": 1699636096069,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mrZefl0ZNC",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MYqZ: [1/2]"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude to Reviewer MYqZ for their valuable time and effort in reviewing our work. We are grateful for your recognition of the interactive user experience and the impact of our approach on VCR tasks. Below, we address each identified weakness.\n\n(1). Regarding question 1, about the novelty and comparison with ChatSpot:\n\n- **Expanding from image-level to region-level instruction tuning is a fundamental progression in multimodal large language models**\n    \n    The emergence of ChatGPT(LLM) is well-recognized as a new era of deep learning, just like AlexNet in 2012. At that time, how to expand AlexNet used for image classification task to object-level recognition tasks is a big challenge for the whole community, until the proposal of R-CNN in 2014. Although R-CNN is a quite simple design, I believe no one would say that it is just a natural progression from AlexNet to R-CNN. Similarly,  our proposed GPT4RoI is motivated to expand the LLM\u2019s visual instruction tuning from image-level to region-level and serves a fundamental role in the related area.\n    \n    A large portion of academic\u00a0researchers pursue sophisticated designs and even agree that complexity is equal to innovation, this is definitely wrong! No matter Occam\u2019s razor or practical applications is telling ***a basic rule: the simpler, the better***. GPT4RoI incorporates region references in instruction and developing the pioneering model for general-purpose region understanding, we definitely believe it is novel and provides a fresh perspective for the region understanding field.\n    \n- **Concurrent Independent Work: impossible to compare when submitting the manuscript**\n    \n    It is important to note that GPT4RoI **predates** ChatSpot by **about two weeks** and serves as a concurrent research project. Furthermore, the unavailability of ChatSpot's code poses challenges in conducting a comprehensive comparative analysis. Based on their report, ChatSpot addresses the reference problem by incorporating position tokens, which may require more region data to realign the position token with the image position compared to GPT4RoI. Additionally, it is worth mentioning that ChatSpot concatenates region position at the end of the instruction, while GPT4RoI formulates interleaved instructions. This difference makes it challenging for ChatSpot to handle multiple-region understanding tasks like VCR. As a verification, ChatSpot has not presented any demo for multi-region understanding."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699776241483,
                "cdate": 1699776241483,
                "tmdate": 1699801585176,
                "mdate": 1699801585176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UaRVuSjP9K",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MYqZ: [2/2]"
                    },
                    "comment": {
                        "value": "(2). Regarding question 2, utilizing more datasets for training, the improvement in results is relatively marginal, as shown in Table 5.\n\n- The baseline **12-in-1[1]** in Table.5 utilizes 12 pre-training datasets, clearly indicating the use of more data compared to our method.\n- Table.5 (visual 7W) is just one of the benchmarks for reasoning ability. On the other hand, the VCR dataset (Table.6), which has gained considerable attention in recent years and attracted submissions from many companies, is more popular and challenging. We have demonstrated our strong performance on this dataset with significantly less training data, achieving an improvement of **6 points** compared to the previous state-of-the-art. Therefore, we believe that labeling our method's improvement in results as relatively marginal is one-sided.\n\n(3). Lacks a comparison of parameters in Table.6.\n\nThe table below presents the rank, and accuracy of Q-AR, along with the parameters and datasets of various solutions, from the most recent **[VCR leaderboard](https://visualcommonsense.com/leaderboard/)** update on November 11, 2023. We retained the top 6 methods and 2 methods that have reports.\n\n| model | rank | Q-AR(%) | Parameters | Datasets | Open Source |\n| --- | --- | --- | --- | --- | --- |\n| GPT4RoI | 1 | 81.6 | 13B+ | 7 open-source region data | Y |\n| HunYuan | 2 | 75.6 | confidential | confidential | N |\n| Shopee | 3 | 74.4 | confidential(ensemble of 4 models) | confidential | N |\n| KDDI Research and SNAP | 4 | 74.3 | confidential | confidential | N |\n| Kuaishou | 5 | 74.0 | confidential | confidential | N |\n| VQA-GNN + MerlotReserve-Large | 6 | 74.0 | 1B+ (ensemble of 2 models) | YT-Temporal-1B | N |\n| MerlotReserve-Large | 11 | 71.5 | 644M | YT-Temporal-1B | Y |\n| UNIMO+ERNIE | 12 | 71.4 | confidential\uff08ensemble of 7 models\uff09\uff0ceach maybe 355 M | BookWiki, OpenWebText, COCO, Visual Genome, Conceptual Captions, SBU Captions | - |\n\nFinding fair comparisons among solutions on the VCR leaderboard can be challenging due to the confidentiality of most submissions. Even in open-source projects, fair comparisons are often overlooked due to the use of different technical approaches, data, training methods, and even model ensembles. In this diverse landscape, our method, GPT4RoI, stands out by utilizing fully open-source region-level data and surpassing all previous approaches by a significant margin (over 6 points) using just a single model. We believe that our method, as the pioneering usage of LLM for the VCR task with a completely open-source pipeline, establishes a robust new baseline.\n\n[1] *12*-in-*1*:\u00a0*Multi*-*Task Vision and Language Representation Learning*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699776588218,
                "cdate": 1699776588218,
                "tmdate": 1700534081853,
                "mdate": 1700534081853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "plJeQzsuAm",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Does our rebuttal address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer MYqZ,\n\nThank you for reviewing our work and engaging in the rebuttal process to enhance the quality of the paper.\n\nHave our rebuttal adequately addressed your concerns? If you still have any issues with our rebuttal or if there are any new concerns, we are more than willing to continue the discussion with you."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014865293,
                "cdate": 1700014865293,
                "tmdate": 1700014865293,
                "mdate": 1700014865293,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C7J8YNeJya",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer MYqZ,\n\nI hope this message finds you well. The author-reviewer discussion period is drawing to a close, and we look forward to receiving your response at this last moment. \n\nAs it is in response, there may be some misunderstanding for you of the results in Table 5. We have also taken your suggestions and created a more comprehensive Table 6 in the main manuscript which contains more details about previous methods. However, due to the confidential nature of the previously state-of-the-art methods, we were unable to obtain detailed information. Additionally, the open-source solutions are far behind our approach. Therefore, we truly believe that our approach - being ***fully open source*** and achieving a ***landmark performance improvement of 6 points*** and ***ranking 1st on the leaderboard*** - can become a solid baseline.  We have also addressed each of your other questions in detail and sincerely hope that these responses have alleviated any concerns you may have had.\n\nGiven the limited time remaining, I would be grateful if you could take some time to review our responses. If you feel that our responses have addressed your concerns, we would be most appreciative if you could consider changing your initial rating. If you still have any remaining concerns, we are happy to continue discussing them with you during this final window.\n\nThank you for your time and consideration."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639116755,
                "cdate": 1700639116755,
                "tmdate": 1700639116755,
                "mdate": 1700639116755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "siTdX8u0YA",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thanks for the authors\u2019 responses. I have the following three questions (No need to experiment):   \n\n- 12-in-1 utilizes 12 pre-training datasets but what is the amount of data samples compared to this work? Simply providing the magnitude comparison of the data samples is OK.\n- As stated in my initial question, is it possible to perform comparisons with other works on the same parameter level? As shown in Table 6, VQA-GNN-L uses 1B+ parameters but this work uses 7B+ parameters. Can this 7B parameter be reduced to a smaller size?\n- Also in Table 6, can the results of GPT4RoI-7B on the test set be listed for comparison with VQA-GNN-L?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667999196,
                "cdate": 1700667999196,
                "tmdate": 1700667999196,
                "mdate": 1700667999196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nPyxwjwpyg",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the MYqZ"
                    },
                    "comment": {
                        "value": "Dear reviewer MYqZ,\n\nThank you for your response to our rebuttal.\n\nQ1: Magnitude comparison of the data samples\n\n|Method | # data samples |\n|---------| ------------|\n| 12-in-1| 4.4 million|\n|GPT4RoI (ours) | 0.43 million |\n\nWe use only a subset of the data that is used by 12-in-1, which is ten times smaller (0.43 million versus 4.4 million).\n\nQ2: Smaller GPT4RoI\n\nOur approach is pioneering in ***utilizing a Large Language Model (LLM)*** for creating a versatile region understanding model.  The term 'Large' in LLM reflects its substantial parameter size.  We've employed the 7B version, the smallest in the community. So we regretfully inform you that it's currently beyond our capabilities for your suggested experiment. However, we are confident that our work has already made a significant impact by ***demonstrating the potential of LLMs to enhance visual reasoning ability***.\n\nQ3: Test Results of GPT4RoI-7B\n\nFirstly, the VCR test set results need to be manually obtained by emailing the author, but the author has joined OpenAI and seems to be very busy. Even after we sent multiple emails, it still took about 10 days to receive a response with the 13B test results. We are sending an email of GPT4RoI-7B now.\n\nHowever, another fact may already resolve your concerns. You will notice from Table 6 and all previous studies that the ***results on the test set are about 0.4+ higher than those on the validation set (I haven't seen an exception)***. Therefore, the result of 7B should be 5 points higher than VQA-GNN + MerlotReserve-Large. We believe this is a ***landmark improvement*** because ***previous methods have been blocked at around 75\\% for almost a year***. We believe that our method is meaningful enough in this field.\n\nThank you very much for your prompt response. If you have any other questions, please reply to us promptly because time seems to be very urgent. We hope to resolve your issues as soon as possible."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672119455,
                "cdate": 1700672119455,
                "tmdate": 1700706717638,
                "mdate": 1700706717638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yNtpCzM6BQ",
                "forum": "DzxaRFVsgC",
                "replyto": "Ress3jgN9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "There are less than 5 hours remaining for our discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer MYqZ,\n\nThere are less than 5 hours remaining for our discussion, and we sincerely hope that we have adequately addressed your concerns. \n\nWe kindly request your thorough consideration of the notable contributions made by our work. We have developed the ***first LLM-based region understanding model that can follow user instructions***, and our quantitative results have demonstrated its outstanding superiority. Notably, we achieved a ***groundbreaking 6-point improvement***, ***surpassing the performance bottleneck that has persisted in VCR for over a year***.\n\nSince ChatGPT showcased the effectiveness of large models, there has been a shift in focus towards ***designing the entire system***. Various datasets and pre-trained models are being utilized to address problems, ***rather than making minor enhancements to a mature pipeline through meticulous ablations***. GPT4RoI represents a novel paradigm utilizing LLM, distinct from previous methods. We understand your desire for fair comparisons of parameters with traditional methods. However, we must acknowledge that it is indeed challenging for the first method based on LLM. We genuinely appreciate your understanding and consideration of these factors and give your final rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724106177,
                "cdate": 1700724106177,
                "tmdate": 1700724204916,
                "mdate": 1700724204916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o4UvaUSYrD",
            "forum": "DzxaRFVsgC",
            "replyto": "DzxaRFVsgC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to enhance current large multimodal models by injecting regional awareness. \nAuthors leverage the public available regional data as the instruction data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Both qualitative and quantitative results demonstrate that now the model can have a sense of location. \n 2. The presentation is clear. \n3. The figures are easy to read."
                },
                "weaknesses": {
                    "value": "1. Which part of the model design leads to positional awareness is unclear. Authors have \" five lightweight scale shuffle modules\", \"ROI Align\", \"add feature coordinates (Liu et al., 2018) for each level (positional embedding)\", \"extract region-level features with the output size of 14\u00d714\", which part really makes the model work? There is no ablation study. \n2. Finetuning on a specific dataset can lead to the case that the model forgets all other knowledge. For example, fine-tuning on the multichoice dataset will lead to the case that model can not speak out natural languages whatever you ask. \n3. From the qualitative examples, seems like the model can only produce short descriptions, which may not be suitable when answering with long context. I think the author's model may overfit to such datasets with short captions.  \n4. When converting LLaVa instruct dataset into the format with bounding box, I cast doubt on how accurate it is. If many instances appear in the same image, is it ok to attach so many detection results beforehand? Besides, the LLaVa instruct dataset is known to include hallucination\n5. How do authors claim on other methods which use textual coordinate as the grounding token? In this way, they can  not only use bbox as input, but also use them in output."
                },
                "questions": {
                    "value": "1. What is the specific vision model you used? Official CLIP model does not have ViT-H at all.\n2.  For each region, it is compressed into one token? It is enough?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1680/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1680/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825621416,
            "cdate": 1698825621416,
            "tmdate": 1699636095990,
            "mdate": 1699636095990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AKUCkKf3Ol",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kN3t: [1/3]"
                    },
                    "comment": {
                        "value": "Dear Reviewer kN3t,\n\nThank you for the appreciation of our work, especially the recognition of our model's ability in spatial referencing and the positive feedback on our paper presentation.\n\n(1). For Question 1, the core design makes GPT4RoI  positional awareness\n\nThe core design of GPT4RoI  incorporates a region feature extractor and replaces the ROI reference in the instruction with the extracted region feature. The implementation of the region feature extractor can be flexible and varied, as mentioned in our introduction, such as using deformable attention or RoI Align operation. We opted for RoI Align in our study due to its simplicity and widespread familiarity among researchers.\nThe region feature extractor is a fundamental component in object detection and instance segmentation. It can be considered that every detail and parameter of this component has been thoroughly ablated. \n\n We provide a detailed explanation of the origin and significance of each sub-module for better understanding.\n  1. The Scale Shuffle module is an algorithm that we have chosen from the multi-level feature fusing modules [1, 2, 3]. These modules are typically utilized to provide robust feature representation for regions of varying scales. They play a crucial role as an essential component in enhancing the performance of all instance recognition tasks.\n  2. RoI Align is an operation that is employed to extract region features by utilizing the coordinates of a bounding box. It serves as the fundamental module in R-CNN detectors preceding DETR-style ones. \n  3. To address the problem of translation invariance in CNNs when absolute position information is required in your task, a commonly employed technique is to augment feature maps with feature coordinates[4, 5, 6]. \n  4. When aiming to extract more detailed information within a region, we usually increase the RoI Align resolution. such as  from (7x7) in object detection[7] to (14x14) in instance segmentation[8].\n\n\n(2). For Question 2, fine-tuning on a specific benchmark to do the evaluation\n\nAs pioneers in the field of region-understanding general-purpose models, there is currently no specific benchmark available for evaluating region-level dialogue in this context. Creating such an evaluation benchmark falls outside the scope of our research. Therefore, we have chosen to conduct our testing on traditional region understanding tasks, specifically multiple-choice questions. It is common practice in this field to fine-tune models on multiple-choice question tasks to evaluate performance. For instance, LLaVA [9] is fine-tuned for image-level understanding tasks, such as science QA [10].\n\n[1] Scale-Equalizing Pyramid Convolution for Object Detection\n\n[2] Unifying Object Detection Heads with Attentions\n\n[3] Dense Distinct Query for End-to-End Object Detection\n\n[4] An intriguing failing of convolutional neural networks and the coordconv solution\n\n[5] SOLO: Segmenting Objects by Locations\n\n[6] SOLOv2: Dynamic and Fast Instance Segmentation\n\n[7] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n\n[8] Mask R-CNN for Object Detection and Segmentation\n\n[9] Visual Instruction Tuning\n\n[10] Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882438472,
                "cdate": 1699882438472,
                "tmdate": 1699884399637,
                "mdate": 1699884399637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PRS8AVIOGo",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kN3t: [3/3]"
                    },
                    "comment": {
                        "value": "(5). For Question 5, About using textual coordinates as the grounding token.\n\nThis is an extremely valuable question that we have dedicated a lot of thought to. We are delighted to share these perspectives with you.\n\nThe key distinction lies in whether to incorporate the detection function into the LLM. For the method that uses textual coordinates as the grounding token, They have to solve the following challenge\n\n- Aligning a large number of position tokens with their corresponding positions in the image by training on a large set of datasets. But this is actually a simple rule that can be naturally implemented with the operation in detection architectures.\n\n- Modeling geometric properties can be challenging. For example,  if the ground truth box is  $<x_1=0, y_1=0, x_2=5, y_2=5>$, a predicted box of $<x_1=1, y_1=1, x_2=4, y_2=4>$ would be considered a better result than $<x_1=1, y_1=1, x_2=8, y_2=8>$. because it has a higher overlap with the ground truth. However, incorporating this geometric property into the next token prediction task using cross-entropy loss can be challenging. On the other hand, utilizing traditional loss functions such as L1 or IoU loss can naturally handle this geometric constraint.\n- Dense to Sparse [1][4] is a crucial design for detection performance, but embedding such an idea into the sequential form of LLM is challenging.\n\nWe provide two pieces of evidence to support our argument\n\n- The performance of pix2seq[2][3], which utilizes object365 pretrain, falls significantly behind the corresponding specialist [4][5].\n- Even with scaled-up data and parameters, GPT4V still faces challenges in object counting[6]. However, this is a trivial task for detection methods.\n\nAnother approach is to use an external detector to find the potential region of interest, whereas LLM only focuses on analyzing the corresponding region of interest. This is the motivation of GPT4RoI. It requires much less data and allows for quick adaptation to specific domain problems with the corresponding detector. However, the drawback is that the framework may appear less elegant and it assumes input contains all region of interest that need to be analyzed.\n\nBoth approaches have their advantages and disadvantages, and academic research in both directions is thriving (including concurrent works or follow-ups on GPT4RoI). For the first approach, relevant references include [7, 8], while for the second approach, there are [9, 10] besides GPT4RoI. Additionally, there has been research that explores a fusion of the two approaches, as shown in references [11, 12, 13]. \n\n***It is still too early to make a definitive judgment on which approach has absolute superiority at this time.***\n\n\n(6). For Question 6, about vision model architecture\n\nWe apologize for the mistake. We used the ViT-L/14 transformer architecture and will fix this in the next version. Thank you for pointing it out.\n\n(7). For Question 7, about number of tokens for each region\n\nOur experiments found that increasing the number of tokens does not make a difference in these open-source datasets and corresponding benchmarks. \n\n\n[1] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n\n[2] Pix2seq: A Language Modeling Framework for Object Detection\n\n[3] A Unified Sequence Interface for Vision Tasks\n\n[4] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection\n\n[5] Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation\n\n[6] The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)\n\n[7] ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning\n\n[8] Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\n\n[9] Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction\n\n[10] Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models\n\n[11] Ferret: Refer and Ground Anything Anywhere at Any Granularity\n\n[12] GLaMM : Pixel Grounding Large Multimodal Model\n\n[13] NExT-Chat: An LMM for Chat, Detection and Segmentation"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885001914,
                "cdate": 1699885001914,
                "tmdate": 1699885001914,
                "mdate": 1699885001914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uTIDZ0G98t",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear reviewer kN3t,\n\nWe sincerely appreciate your review of our work and your valuable contribution to improving the quality of the paper.\n\nHave we effectively addressed your concerns in our rebuttal? If you still have any issues or new concerns, please feel free to let us know so we can continue the discussion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102915596,
                "cdate": 1700102915596,
                "tmdate": 1700102915596,
                "mdate": 1700102915596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SoLRoB7iCC",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer kN3t,\n\nThe author-reviewer discussion period is drawing to a close. We have provided responses to each of your questions above and made some improvements to the main manuscript following your suggestion. We have added an in-depth discussion regarding the methods that utilize textual coordinates as grounding tokens and enhanced the clarity of the RoI extractor section in the main manuscript.\n\nGiven the limited time remaining, I would be grateful if you could take some time to review our responses. If you feel that our responses have addressed your concerns, we would be most appreciative if you could consider changing your initial rating. If you still have any remaining concerns, we are happy to continue discussing them with you during this final window.\n\nThank you for your time and consideration."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637247306,
                "cdate": 1700637247306,
                "tmdate": 1700724297287,
                "mdate": 1700724297287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mfqzpvUb9X",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Discussion request before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer kN3t,\n\nThere are less than 5 hours remaining for our discussion, and we sincerely hope that we have adequately addressed your concerns.\n\nWe have made every effort to revise the article and address your concerns. At this final moment, we also hope that you can take note of the highlights of our work, specifically the ***first LLM-based region understanding model that can follow user instructions***. Additionally, we would like to draw your attention to its significant progress on VCR. Our model achieved a remarkable performance ***improvement of 6 points***, ***ranking 1st on the leaderboard***. It overcame the long-standing performance bottleneck in VCR, which ***persisted for over a year***. We kindly ask you to consider our rebuttal and make your final rating based on a comprehensive evaluation.\n\nThank you for your time and consideration."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725068369,
                "cdate": 1700725068369,
                "tmdate": 1700725068369,
                "mdate": 1700725068369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qt52wqhilN",
                "forum": "DzxaRFVsgC",
                "replyto": "mfqzpvUb9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate authors' response to my questions. \nI am still curious Is CLIP ROI feature really the key to localizing objects? As early works like RegionCLIP indicate, we actually need to train a region-aware model so that ROI feature can correspond to the feature of a certain region. That being said, directly cropping the CLIP feature will not lead to precise correspondence. \n\nI am just wondering in this case, will \"ROI\" actually makes the proposed methods work. That being said, if we remove the positional embedding, remove the \"shuffle\", remove the learned FPN parameters, will the pure ROI feature get good performance on tasks such as Visual7W and RefCOCOg?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733165749,
                "cdate": 1700733165749,
                "tmdate": 1700733165749,
                "mdate": 1700733165749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0gybzwopsH",
                "forum": "DzxaRFVsgC",
                "replyto": "o4UvaUSYrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1680/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kN3t"
                    },
                    "comment": {
                        "value": "This is an insightful question. However, due to the limited time of ***only two hours remaining***, we will add these results in future versions. \n\nBut We believe that the ***absence of these results will not undermine any of our claimed contributions in the article***.\n\nWe want to emphasize again that our ***main contribution*** is the ***first general-purpose region understanding model based on LLM***.  The modules you mentioned are constructed based on our experience in the detection field. As a pioneering work, ***our model's performance is already satisfactory***. We didn't provide detailed ablation of these sub-components in the initial version because we believe it is not the contribution of our work. As we mentioned before, these reference object components can be implemented in various forms.\n\n\nAs it is the final moment, we kindly request the reviewer to consider ***the overall system and its remarkable performance***. If there are additional specific questions, please feel free to ask, and we will address them in future versions of the paper."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1680/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737225217,
                "cdate": 1700737225217,
                "tmdate": 1700737766144,
                "mdate": 1700737766144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]