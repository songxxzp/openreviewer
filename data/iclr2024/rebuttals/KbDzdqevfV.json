[
    {
        "title": "Correct-by-design Safety Critics using Non-contractive Binary Bellman Operators"
    },
    {
        "review": {
            "id": "fT1ys2uTlx",
            "forum": "KbDzdqevfV",
            "replyto": "KbDzdqevfV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_XLgV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_XLgV"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a framework for obtaining correct-by-design safety critics in RL to a region of the unsafe, \"failure\" state space. This framework exploits the logical safe/unsafe nature of the problem and yields binary Bellman equations with multiple fixed points. These fixed points are meaningful, by characterizing their structure in terms of guaranteeing safety and maximality. The authors empirically evaluate the safety performance of their proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Good theoretical results. However, this theoretical results are based on a specific problem settings, so I have several concerns as written in Weakness\n- This paper is easy to follow."
                },
                "weaknesses": {
                    "value": "### Problem setting\nI am not fully convinced whether the problem setting of this paper is really useful. In this paper, there is no notion of reward and an agent receives a feedback of safety as a binary value. I do feel that typical constrained MDP formulations that are characterized by both reward and safety are much more useful. I personally would like the authors to discuss why this problem settings are considered and its advantages while comparing to:\n    - CMDP with reward and safety feedback\n    - MDP with binary reward feedback\n    - Methods in control theory (e.g., CBF, CLF)\nI know the authors partially discuss the relevance, but I do not think that the advantages of the authors' problem settings and method are well presented.\n\nAlso, the authors consider a deterministic state transition $F$ and discrete action space $\\mathcal{A}$, but this assumption also limits the applicability of this paper.\n\nIn order to make this paper more useful, I would recommend the authors to reconsider the problem settings.\n\n### Strongly related work\n\nAs a strongly related work, Turchetta et al. (2016) exists. This paper considers considers similar MDP as this paper (except for safety feedback is numeric) and defines reachability and returnability sets. I think the authors should discuss what differences and advantages exist. Also, the authors paper may need to consider returnability as well as reachability in order to guarantee the long-term safety.\n\n- Turchetta, Matteo, Felix Berkenkamp, and Andreas Krause. \"Safe exploration in finite markov decision processes with gaussian processes.\" Advances in neural information processing systems 29 (2016).\n\n### Empirical experiments\n\nI am not fully satisfied with the experiments in terms of difficulties of environments, baseline methods, and results. Regarding Inverted Pendulum tasks, if I understand correctly, the task is to stabilize the system. I do not consider that the performance of the authors' proposed method is not well presented while comparing it with SBE in this easy task. In such a well-known control task, the baseline methods should include CBF methods in control theory literature. Also, in autonomous driving tasks, I think that PPO is not a good baseline method to be compared. PPO is a safety-agnostic method and it is unfair to compare the safety performance of the PPO and the authors' proposed method. \n\nAlso, the theoretical and empirical results seem inconsistent (I do not say that they are wrong). If I understand correctly, the authors' theoretical claims regarding fixed points are core contributions, but this point is totally ignored in the empirical evaluations. I am confused what claims the authors want to support by the experiments.\n\nFinally, the experiment section and its appendix are quite confusing.\nIn Appendix A.5, both Tables 1 and 2 have captions of \"Hyperparameters for inverted pendulum experiment\". In Table 2 (I guess this is a table for autonomous driving), SBE (not PPO) is in the algorithm list.\n\n\n### Minor Comments\n- In the first paragraph in Introduction, citations look weird.\n    - Yu et al. (2021),Brunke et al. (2022) --> (Yu et al., 2021; Brunke et al., 2022)\n\n- Please define $\\mathbb{P}^\\pi$. I know what it is, but please do not assume every reader knows that. This is also true with $E_\\pi$."
                },
                "questions": {
                    "value": "[Q1] Could you tell me that reason why the problem settings are considered in this paper based on my commentws?\n\n[Q2] Why returnability is not considered? Is it possible to guarantee a long-term safety?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698371511318,
            "cdate": 1698371511318,
            "tmdate": 1699636654482,
            "mdate": 1699636654482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "YprHHkJKqL",
            "forum": "KbDzdqevfV",
            "replyto": "KbDzdqevfV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_A9xS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_A9xS"
            ],
            "content": {
                "summary": {
                    "value": "Existing approaches to learning safety critics treat them as exactly analogous\nto standard Q functions. However, this view loses sight of the high assurance\nrequired by safety critics, especially in cases where deterministic safety\nguarantees are desirable. This paper proposes a new approach to safety critic\nlearning based on binary Bellman equations which avoids discount factors in\norder to give stronger guarantees about the resulting function. Theoretical\nresults show that while there is not a unique fixpoint for safety critics\ndefined this way, (nearly) any fixpoint provides the desired safety guarantees.\nMoreover, fixpoints are \"maximal\" in a certain sense which allows for freer\nexploration. The experiments provide evidence that for similar levels of safety,\nthe proposed approach is able to maintain higher policy entropy compared to\nprior work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The theoretical contribution is interesting and significant. Analyzing safety\ncritics using tools developed for analyzing reward signals does indeed lose some\nignore some facets of the problem. The maximality result in Theorem 1, part (ii)\nis quite important.\n\nThe results for the pendulum benchmark are promising. In particular, the right\nside of Figure 4, showing that policies can achieve both safe behavior and high\nentropy, is impressive. This has traditionally been a challenging trade-off for\nsafe RL research where safety guarantees usually come with restrictions that\nlimit exploration."
                },
                "weaknesses": {
                    "value": "The restriction to deterministic environments with finite action space is quite\nlimiting. Most other work in this space does not have such restrictions, though\nthey may be necessary for the kind of strong safety guarantees this paper is\npursuing.\n\nThe paper explores only two benchmark environments making it very difficult to\njudge its empirical effectiveness. Of those, one is an extremely low-dimensional\npendulum system. The results in the more complex environment show minimal\nimprovement over an unsafe RL baseline (PPO)."
                },
                "questions": {
                    "value": "I was not quite sure where the one-sided operator appeared in the\nimplementation. Specifically, Algorithm 1 requires that $b^\\theta$ can correctly\nclassify all data points as safe or unsafe, but the one-sided operator only\nrequires that $b^\\theta$ never categorizes a safe point as unsafe (that is, it\nmay overestimate the danger at any point but never underestimate). Is that\nbehavior assumed somewhere within the accuracy check?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Reviewer_A9xS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679801720,
            "cdate": 1698679801720,
            "tmdate": 1699636654373,
            "mdate": 1699636654373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NiJINYSGvu",
            "forum": "KbDzdqevfV",
            "replyto": "KbDzdqevfV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_2vzB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_2vzB"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of enforcing safety in reinforcement learning (RL) algorithms. They define a (binary) Bellman equation to characterise safety for a finite MDP with stochastic policies and deterministic transitions between states and study its properties. They then proceed by proposing an algorithm to learn a neural network to compute safety. An empirical validation on two benchmark is provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clear and addresses a problem of interest for the ICLR community\n\n- The approach is sound. Furthermore, the study of the fixed points of the proposed Bellman operator and the resulting Algorithm 1 are interesting and show promising results"
                },
                "weaknesses": {
                    "value": "- Bellman equations to characterise safety and equivalently reachability properties have been already developed in the literature, and also in the stochastic setting [1,2]. The one proposed by the authors is an extension of those ones, with the added constraint that only actions that have probability 1 of being safe are admitted. This should be clarified and discussed in the paper.\n\n- The approach proposed by the authors is also related to barrier functions [3,4] and in particular neural barrier functions [5], where these functions are employed to compute regions of the state space that are safe (for a finite or infinite horizon). Pros and Cons of the approach of the authors wrt barrier functions should be discussed in the related works\n\n- The authors claim that the fact that the operator in Eqn 4 has multiple fixed points is surprising. This is actually to be expected. In fact, it is well known that fixed points for safety properties will in general depend on initial condition and topology of the graph of the MDP [6].\n\n- I find Figure 5 and its analysis a bit unclear. In fact, the authors stop the simulations at 700 k iterations at a point where the proposed approach was slightly outperforming  PPO. However, in the previous iterations, PPO was almost always giving better performances. So, can you please run the experiment for more iterations? Say at least 1 milion.\n\n[1] Abate, Alessandro, et al. \"Probabilistic reachability and safety for controlled discrete time stochastic hybrid systems.\" Automatica 44.11 (2008): 2724-2734.\n\n[2] Laurenti, Luca, et al. \"Unifying Safety Approaches for Stochastic Systems: From Barrier Functions to Uncertain Abstractions via Dynamic Programming\" arXiv preprint arXiv:2310.01802, 2023\n\n[3] Zeng, Jun, Bike Zhang, and Koushil Sreenath. \"Safety-critical model predictive control with discrete-time control barrier function.\" 2021 American Control Conference (ACC). IEEE, 2021.\n\n[4] Prajna, Stephen, Ali Jadbabaie, and George J. Pappas. \"A framework for worst-case and stochastic safety verification using barrier certificates.\" IEEE Transactions on Automatic Control 52.8 (2007): 1415-1428.\n\n[5] Mathiesen, Frederik Baymler, et al. \"Safety certification for stochastic systems via neural barrier functions.\" IEEE Control Systems Letters 7 (2022): 973-978.\n\n[6] Baier, Christel, and Joost-Pieter Katoen. Principles of model checking. MIT press, 2008."
                },
                "questions": {
                    "value": "Please, see Weaknesses Section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754557393,
            "cdate": 1698754557393,
            "tmdate": 1699636654257,
            "mdate": 1699636654257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "1VF0O4qQOt",
            "forum": "KbDzdqevfV",
            "replyto": "KbDzdqevfV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_WKp6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6073/Reviewer_WKp6"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to use Bellman-like operators to learn safe policies and critics to avoid reaching unsafe states. Unlike the standard RL setting, where the performance of policies can be quantified with rewards, the safety problem that this paper is concerned with only evaluates the safety of a policy in a binary manner. This paper formulates the Bellman operators with real value operations to achieve the binary operations on the binary outcomes. Furthermore, to overcome the non-contraction problem of the proposed Bellman operator, the paper proposes to learn a one-sided Bellman operation of which the fixed points are more conservative in staying in the safe region. The paper developed an algorithm by learning the policy that corresponds to the fixed-point of this one-sided problem iteratively.\nThe paper demonstrates the effectiveness of the proposed method on two benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* `originality`: This paper proposes a Bellman operators for binary outcomes that is unlike the conventional Bellman operations for RL.\n* `quality`: The paper is well written. \n* `clarity`: The problem definitions are clear. The Bellman operator proposed in this paper is sound.\n* `significance`: This paper provides a different choice than the conventional ones for safe RL."
                },
                "weaknesses": {
                    "value": "* `Weakness 1`: It is questionable whether the proposed approach is still valid in a stochastic environment, where there is always a chance of entering an unsafe region regardless of the state that the agent is in. While in a simulated environment, the dynamics can be deterministic, it is essential to acknowledge that stochasticity is prevalent in the real world.\n* `Weakness 2`: It is also questionable whether the proposed approach is valid when the goal does not perfectly align with safety. In the autonomous driving benchmark, the goal is to avoid collision. But in reality, not every task is about set avoidance or reachability.  \n* `Weakness 3`: in the experimental section, the author uses only two benchmarks to validate the proposed approach. It would be better if the author demonstrated how the proposed approach can be adopted to solve different tasks. Also, the baseline RL approach hinges on the reward function. Different reward functions can lead to different performances. It would be better if the author just tried to use a goal-driven reward (+1 reward for reaching the goal -1 for collision, and 0 everywhere else) to train the RL policy for comparison."
                },
                "questions": {
                    "value": "Please address my questions in `Weakness 1` and `2` in the `Weakness` field."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6073/Reviewer_WKp6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793326385,
            "cdate": 1698793326385,
            "tmdate": 1699636654150,
            "mdate": 1699636654150,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]