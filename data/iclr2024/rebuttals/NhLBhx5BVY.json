[
    {
        "title": "Instance Segmentation with Supervoxel Based Topological Loss Function"
    },
    {
        "review": {
            "id": "Ko047ytxw9",
            "forum": "NhLBhx5BVY",
            "replyto": "NhLBhx5BVY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_cPYQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_cPYQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors work on binary as well as instance segmentation of curvilinear structures. This segmentation task is prone to errors such as split and merge mistakes. The authors resolve such errors by extending the topological concept of simple points to superpixels (or supervoxels). While most topological-based methods are computationally expensive, the authors propose an algorithm to reduce the complexity of their method. The authors validate their method on 2 datasets and compare against several topology-aware baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The authors extend the concept of simple points to supervoxels. In the false negative and false positive maps, they check if keeping or removing the connected component (CC) changes the topology or not. If it changes the topology, then they deem it critical and apply high weight to it in the loss function while training. The critical CCs correspond to the split/merge errors that one would like to resolve.\n2) The authors develop an $O(n)$ solution where $n$ is the number of pixels/voxels using standard graph algorithms like BFS. This is much cheaper compared to existing topology-aware methods whose algorithms are atleast $O(n \\log n)$ or $O(n^2)$.\n3) The authors provide adequate proofs of their runtime in the supplementary."
                },
                "weaknesses": {
                    "value": "1) In principle, the novelty of the contribution seems limited as an existing concept of simple points has been extended to superpixels (collection of pixels) instead. \n2) The authors should consider comparing against clDice [1] as a baseline since clDice has shown better performances among the topology-aware methods.\n3) The authors do not provide any ablation study. Considering they have hyperparameters $\\alpha$ and $\\beta$ in their loss function, the authors would benefit from providing an ablation study of these loss weights and provide a discussion on how each hyperparameter affects the results.\n\n**References**\n\n[1] Shit, Suprosanna, et al. \"clDice-a novel topology-preserving loss function for tubular structure segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
                },
                "questions": {
                    "value": "1) Please also see the weakness above.\n2) As the authors claim that they are proposing a topology-aware neural network, they should also evaluate the result on topology-aware metrics like clDice [1], Betti Matching [2], and Betti Number [3].\n3) Please mention if the numbers in bold are just numerically better, or, if t-test [4] has been conducted to check if the performance improvement is statistically significant or not.\n\n**References**\n\n[1] Shit, Suprosanna, et al. \"clDice-a novel topology-preserving loss function for tubular structure segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[2] Stucki, Nico, et al. \"Topologically faithful image segmentation via induced matching of persistence barcodes.\" International Conference on Machine Learning. PMLR, 2023\n\n[3] Hu, Xiaoling, et al. \"Topology-preserving deep image segmentation.\" Advances in neural information processing systems 32 (2019).\n\n[4] Student, 1908. The probable error of a mean. Biometrika, pp.1\u201325."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8037/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8037/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8037/Reviewer_cPYQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8037/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801243064,
            "cdate": 1698801243064,
            "tmdate": 1699636992510,
            "mdate": 1699636992510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jX8xPKmAk8",
                "forum": "NhLBhx5BVY",
                "replyto": "Ko047ytxw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "W1: In principle, the novelty of the contribution seems limited as an existing concept of simple points has been extended to superpixels (collection of pixels) instead.\n\nA1: While the proposed extension is indeed simple to state (i.e., extension of the concept of simplicity from voxels to supervoxels), it took us a while to conceptualize this. We would like to note that the concepts on which our work is built (e.g., simple pixel, supervoxels) has been around for decades and there is an ever-growing need for topology-preserving methods. Therefore, we believe the proposed extension appears simple (and potentially limited) only after the fact. Finally, this extension would not be valuable unless it is supported by an accompanying mathematical and algorithmic framework. We are not aware of shorter, simpler ways of developing a rigorous theory without going through the multiple formal statements in our manuscript. Please also refer to our general response on this.\n\nW2: The authors should consider comparing against clDice [1] as a baseline since clDice has shown better performances among the topology-aware methods.\n\nA2: We are actively working on training a model with the clDice. However, the original code base has some issues, as documented on the official github repository, which are delaying us from obtaining results. If we can resolve these, we aim to report the results of training this model to perform instance segmentation on the EXASPIM23 dataset.\n\nW3: The authors do not provide any ablation study. Considering they have hyperparameters and in their loss function, the authors would benefit from providing an ablation study of these loss weights and provide a discussion on how each hyperparameter affects the results.\n\nA3: Thanks for this comment. Indeed, this important point was inadvertently left out in the initial submission. We have performed hyperparameter optimization beyond what's reported in the original submission to elucidate the sensitivity of performance on hyperparameter values. We found that the performance changes less than 10\\% across changes of a few orders of magnitude around the optimal values of the hyperparameters $\\alpha$ and $\\beta$. Thus, these experiments suggest that careful hyperparameter tuning is not necessary in practice. We will add both the figure describing these findings and a surrounding discussion to the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338533659,
                "cdate": 1700338533659,
                "tmdate": 1700338533659,
                "mdate": 1700338533659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HEvEKIbLMc",
                "forum": "NhLBhx5BVY",
                "replyto": "Ko047ytxw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Questions"
                    },
                    "comment": {
                        "value": "Q1: As the authors claim that they are proposing a topology-aware neural network, they should also evaluate the result on topology-aware metrics like clDice [1], Betti Matching [2], and Betti Number [3].\n\nA1: We have now added an additional metric, the Betti error, to better quantify the results reported for the DRIVE dataset, as suggested. Briefly, the results support our previous findings where the proposed method is a close second-best for this metric and the top performer in other previously reported metrics, despite focusing on a narrower set of topological changes and the ensuing computational simplicity. \n\nQ2: Please mention if the numbers in bold are just numerically better, or, if t-test [4] has been conducted to check if the performance improvement is statistically significant or not.\n\nA2: The bold numbers highlight the best value across all methods. The paper has been updated to indicate this."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338639049,
                "cdate": 1700338639049,
                "tmdate": 1700338639049,
                "mdate": 1700338639049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Sq6YpI11m",
                "forum": "NhLBhx5BVY",
                "replyto": "Ko047ytxw9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Reviewer_cPYQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Reviewer_cPYQ"
                ],
                "content": {
                    "comment": {
                        "value": "Could the authors provide the quantitative results of the answers mentioned in the above response (weakness A3 and questions A1)? Either as comments here or in the updated PDF. \n\nConsidering the proposed method is a close-second on the Betti error metric, which method was the best? Considering this is a topology-based approach, we should see significant improvement in topology-aware metrics."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545523200,
                "cdate": 1700545523200,
                "tmdate": 1700545545158,
                "mdate": 1700545545158,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BISqKgHONg",
            "forum": "NhLBhx5BVY",
            "replyto": "NhLBhx5BVY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_ZGQy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_ZGQy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a \"supervoxel based topological loss function\" to solve the connectivity-related problems in segmentation tasks. The paper considered the key components of false positives and false negatives as key factors affecting the topology and uses loss functions to optimize them. Through algorithm design, the time complexity is reduced. The theoretical proof is rich, and the effectiveness of the model is verified on EXASPIM2 and DRIVE. The visualization effect shows that the loss function proposed in this paper can effectively improve the visualization effect."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposed novel ideas and methods that are simple and effective after being optimized by the proposed algorithm in this article.\n2. The paper verified the effectiveness of the method in both 3D and 2D dimensions."
                },
                "weaknesses": {
                    "value": "1. Lack of visual comparison with baseline in Figure 5.\n2. Performance metrics should be described using formulas.\n3. Lack of comparison with more loss functions that can supervise topology changes."
                },
                "questions": {
                    "value": "Critical components should not include areas that do not affect the topology structure. In the visualization effect of Figure 4, why is the structure more slender compared to the baseline, and can the proposed loss function optimize the segmentation edge?\n\nWhat are the values of \u03b1 and \u03b2 in the experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8037/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698935906404,
            "cdate": 1698935906404,
            "tmdate": 1699636992401,
            "mdate": 1699636992401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e7kMxDfET5",
                "forum": "NhLBhx5BVY",
                "replyto": "BISqKgHONg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Weaknesses"
                    },
                    "comment": {
                        "value": "W1: Lack of visual comparison with baseline in Figure 5.\n\nA1: We have now added the visual comparison to Figure 5, as suggested by this referee.\n\nW2: Performance metrics should be described using formulas.\n\nA2: We have now added equations describing the performance metrics to the Supplementary material and referred to them in the main text when the metrics are introduced.\n\nW3: Lack of comparison with more loss functions that can supervise topology changes.\n\nA3: We have now added an additional metric, the Betti error, to better quantify the results reported for the DRIVE dataset, as suggested. Briefly, the results support our previous findings where the proposed method is a close second-best for this metric and the top performer in other previously reported metrics, despite focusing on a narrower set of topological changes and the ensuing computational simplicity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338778340,
                "cdate": 1700338778340,
                "tmdate": 1700338778340,
                "mdate": 1700338778340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HxUusOfWxa",
                "forum": "NhLBhx5BVY",
                "replyto": "BISqKgHONg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Questions"
                    },
                    "comment": {
                        "value": "Q1: Critical components should not include areas that do not affect the topology structure. In the visualization effect of Figure 4, why is the structure more slender compared to the baseline, and can the proposed loss function optimize the segmentation edge?\n\nA1:We attribute this qualitative difference to the fact that a less slender structure is more likely to result in merge mistakes. The depiction of positively critical components in Figure 3 is a didactic cartoon and might not accurately portray a \"typical\" critical component. In actuality, positively critical components encompass incorrect voxel predictions that act as a bridge between two distinct components. (Plus, any voxels in the false positive mask connected to this bridge which may extend into a large region connected to the boundary of the objects.) It's plausible that the neural network learns to avoid less slender predictions due to these considerations.\n\nWe don't make any claims that this loss function leads to more slender predictions. For neuron segmentation, this is a less important qualitative feature and not easy to quantify."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339282693,
                "cdate": 1700339282693,
                "tmdate": 1700339282693,
                "mdate": 1700339282693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IRd7K9wwde",
            "forum": "NhLBhx5BVY",
            "replyto": "NhLBhx5BVY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_Jbm6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8037/Reviewer_Jbm6"
            ],
            "content": {
                "summary": {
                    "value": "Traditional segmentation methods focus on total voxel accuracy. While some critical voxel errors might change the topology, most would not. This paper proposes a method to find those critical voxels and add additional penalty terms to the loss function whenever these voxels are inaccurately predicted.\n\nThe contributions of this paper are as follows:\n1. An algorithm is proposed to detect split and merge errors which disrupt the number of components relative to the ground truth.\n2. The computational complexity of this algorithm scales linearly with the number of voxels under the assumption of a tree graph.\n3. This approach can be easily integrated into available segmentation frameworks as it is built on top of common voxel-based loss functions.\n4. The experiments conducted show the effectiveness of the proposal relative to competing methods on two datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In addition to the contributions listed above I would highlight the following:\n\n1. The authors provide a good mathematical notation to communicate their methodology.\n2. The didactic images facilitate a better understanding of the approach.\n3. Details of training are explicitly mentioned such as the continuation scheme."
                },
                "weaknesses": {
                    "value": "1. The paper could improve in terms of clarity in several cases. Most importantly, Algorithm 2--a critical part of the paper--is very vague and the only explanation provided about it under Corollary 3 does little to make it clearer.\n2. More emphasis needs to be placed that the O(n) gurantee is only valid if critical components affect both local and global topology, i.e. having a tree graph.\n3. The method is only sensitive to the number of components, while topology is much broader e.g. bifurcations, loops. This limitation needs to be communicated.\n4. The approach comes with hyper-parameters which might be time consuming to set."
                },
                "questions": {
                    "value": "1. The loss function is introduced in Definition 1, and then expanded again in Section 3.3. A better sense of direction would have been conveyed if the two were mixed and mentioned early on in Section 3 and stated that the rest of the Section focuses on finding N(y^) and P(y^) in Definition 4. Removing G(.) and H(.) and having double sums might improve readablility.\n\n2. The definition originally provided for S(.) does not have a subscript and is clear, but the explanation provided for the case with a subscript is hard to grasp.\n\n3. In Method Section before Definition 1, it is stated that y_i \\in {0,1,...,m} and y^_i \\in {0,1,...,l}. Shouldn't they both sets be the same (no need for l)? Also m, l(?), and n are given without definitions.\n\n4. Positively critical components rely on computing S(y^\u2296y^+), while Algorithm 2 only requries knowing S(y\u2296y^-). Why?\n\n5. Condition 1 and Condition 2 are not really defined as such.\n\n6. Potential typos: Section 3.1.2 line 2: false negative mask -> false positive mask; Two lines above Corollary 1: lemma seems extra."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8037/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8037/Reviewer_Jbm6",
                        "ICLR.cc/2024/Conference/Submission8037/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8037/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699462797289,
            "cdate": 1699462797289,
            "tmdate": 1700649633284,
            "mdate": 1700649633284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qbE1wIs3Ge",
                "forum": "NhLBhx5BVY",
                "replyto": "IRd7K9wwde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "W1: The paper could improve in terms of clarity in several cases. Most importantly, Algorithm 2--a critical part of the paper--is very vague and the only explanation provided about it under Corollary 3 does little to make it clearer.\n\nA1: Thank you for this very helpful and explicit feedback. In retrospect, we agree that the initial presentation of this section may be difficult to parse. We have completely rewritten this section to improve clarity and focus the discussion on the high level ideas. The pseudo code has as been significantly revised so that it is more intuitive and better aligned to the descriptions in the text.\n\nW2+W3: The method is only sensitive to the number of components, while topology is much broader e.g. bifurcations, loops. This limitation needs to be communicated. More emphasis needs to be placed that the O(n) guarantee is only valid if critical components affect both local and global topology, i.e. having a tree graph.\n\nA2+A3: While the main definitions and initial formal statements (e.g., Thm 1) capture topological changes in structures broader than tree-like objects, as this referee pointed out, the statements and algorithms surrounding the fast implementation are restricted to tree-structured objects. Indeed, a similar algorithm based on the main definitions and deductions can be implemented in a straightforward way, except that this algorithm will be super-linear in complexity. To address the referee's concern, we have now added explanation to the main text (see discussion following Theorem 3) emphasizing that the proposed fast algorithm will not be sensitive to broader topological changes in more general graph structures, such as creation of a cavity or a loop.\n\nW4: The approach comes with hyper-parameters which might be time consuming to set.\n\nA4: Thanks for this comment. Indeed, this important point was inadvertently left out in the initial submission. We have performed hyperparameter optimization beyond what's reported in the original submission to elucidate the sensitivity of performance on hyperparameter values. We found that the performance changes less than 10\\% across changes of a few orders of magnitude around the optimal values of the hyperparameters $\\alpha$ and $\\beta$. Thus, these experiments suggest that careful hyperparameter tuning is not necessary in practice. We will add both the figure describing these findings and a surrounding discussion to the revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338119204,
                "cdate": 1700338119204,
                "tmdate": 1700338119204,
                "mdate": 1700338119204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bGMX1Huig2",
                "forum": "NhLBhx5BVY",
                "replyto": "IRd7K9wwde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "Q1: The loss function is introduced in Definition 1, and then expanded again in Section 3.3. A better sense of direction would have been conveyed if the two were mixed and mentioned early on in Section 3 and stated that the rest of the Section focuses on finding N(y) and P(y) in Definition 4. Removing G(.) and H(.) and having double sums might improve readablility.\n\nA1: This is a great suggestion and we have updated the paper accordingly. It not only improved readability, but also shortened Section 3.3 so that we can present more visual results in Section 4.\n\n\nQ2: The definition originally provided for S(.) does not have a subscript and is clear, but the explanation provided for the case with a subscript is hard to grasp.\n\nA2: This extra notation is necessary to ensure that each connected component $C\\in\\mathcal S(\\hat y_-)$ intersects with exactly one connected component in the set $\\mathcal S(y)$. To address the referee's concern, we have now added an explanatory footnote after the definition with the subscript, relating to the definition without the subscript.\n\n\nQ3: In the Method Section before Definition 1, it is stated that $y_i \\in\\{0,1,...,m\\}$ and $\\hat y_i \\in\\{0,1,...,l\\}$. Shouldn't they both sets be the same (no need for l)? Also m, l(?), and n are given without definitions.\n\nA3: Thank you for pointing this out, there is a typo in the manuscript. Our original submission contains the lines ``A ground truth segmentation $y = (y_1, . . . , y_n)$ is a labeling of the vertices such that $y_i\\in\\{0, 1,..., m\\}$ denotes the label of node $i \\in V$. Each segment has a label in 1,..., k and the background is marked with 0.'' Instead the last sentence should be \"...Each segment has a label in 1,..., *m and the background is marked with 0\", which defines m to be the number of objects in the image. \n\nThe value l is the number of objects in the predicted segmentation. We assume that the true number of objects m is unknown at the time of inference. It is possible and usually the case that $l \\neq m$, especially in neuron segmentation. We added a footnote explaining why m is not necessarily equal to l.\n\nWe also updated the first sentence in the method section to include the definition of n, it know reads as ``Let $G=(V,E)$ be an undirected graph with the vertex set $V=\\{1,\\ldots, n\\}$. We assume that $G$ is a graphical representation of an image where the vertices represent voxels and edges are defined with respect to a $k$-connectivity''.\n\n\nQ4: Positively critical components rely on computing $S(\\hat y\\ominus \\hat y_+)$, while Algorithm 2 only requires knowing $S(y\\ominus y_-)$. Why?\n\nA4: Thanks for pointing it out. This is actually a typo in Algorithm 2, it should read as $S(y\\ominus \\hat y_x)$ instead of $S(y\\ominus \\hat y_-)$, where $\\hat y_x$ is a placeholder for $\\hat y_-$ and $ \\hat y_+$. We have now updated the pseudocode of the algorithm.\n\n\nQ5: Condition 1 and Condition 2 are not really defined as such.\n\nA5: We have now updated the text following Corollary 3 to clarify this point.\n\n\nQ6: Potential typos: Section 3.1.2 line 2: false negative mask -> false positive mask; Two lines above Corollary 1: lemma seems extra.\n\nA6: Thanks for pointing the typo of Section 3.1.2, we've now fixed this mistake. For the two lines above Corollary, we deleted the first line above the corollary and agree that it's a bit repetitive. We did leave the second line above this corollary because this is where we define y ominus hat y-."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338278962,
                "cdate": 1700338278962,
                "tmdate": 1700338896551,
                "mdate": 1700338896551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0tNPIAD3dP",
                "forum": "NhLBhx5BVY",
                "replyto": "bGMX1Huig2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Reviewer_Jbm6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Reviewer_Jbm6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the comments to improve the quality of your work.\n\nI believe the paper now reads better with having a top-down approach. Several notations have been clarified or corrected which help greatly in understanding the details.\n\nWhile the revisions in Algorithm 2 has made it more precise, it seems like the explanations explicitly about it from the text have been removed (correct me if I am wrong). I think one could easily get lost in all the if statements and a high-level explanation in text is beneficial.\n\nI like the fact that the computational complexity has been emphasized in the form of a theorem. However, the tree-shape assumption needs to be stated as part of the theorem for clarity---and not in the following paragraph. I believe one cannot really do anything without making assumptions, so there is no shame in making and stating them.\n\nIn summary, I find the paper to have improved to a level that is ready for publication."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649588652,
                "cdate": 1700649588652,
                "tmdate": 1700649588652,
                "mdate": 1700649588652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IfseYs5OLc",
                "forum": "NhLBhx5BVY",
                "replyto": "IRd7K9wwde",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8037/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Computation Section Update"
                    },
                    "comment": {
                        "value": "We thank this reviewer for improving their score. We appreciate your additional feedback on Section 3.2. A revised version has been uploaded, incorporating your latest comments. We significantly revised this section to provide a higher level description of the mechanics of the algorithm. In the process, some text was indeed removed.\n \nWe have further updated the paper to improve both the readability and preciseness of our algorithm description. In the most up-to-date version of the paper, we have made slight adjustments to the organization of the algorithm's description. We have also improved the clarity by explicitly indicating the lines in the algorithm to which the text description corresponds.\n \nThank you for pointing out that the statement of Theorem 3 does not refer to a \"tree-structured\" condition. We have updated this theorem so that it now includes the same condition stated in Corollaries 1-3. In addition, we also refer to this condition in the discussion following this Theorem."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8037/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695859999,
                "cdate": 1700695859999,
                "tmdate": 1700695974643,
                "mdate": 1700695974643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]