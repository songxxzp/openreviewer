[
    {
        "title": "Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection"
    },
    {
        "review": {
            "id": "IoiXb8apQm",
            "forum": "NQUXBoGiDU",
            "replyto": "NQUXBoGiDU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_WJNq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_WJNq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Spiking CenterNet, an SNN for object detection. It employs knowledge distillation from a non-spiking SNN to improve the accuracy. The implementation of the proposed method for the Prophesee\u2019s GEN1 Automotive Detection Dataset shows better results than related works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The tackled problem is relevant to the community.\n\n2. The technical descriptions are clear and comprehensive.\n\n3. The results show better results than prior SNNs."
                },
                "weaknesses": {
                    "value": "Some aspects need to be clarified. Please refer to the questions below."
                },
                "questions": {
                    "value": "1. Please highlight more clearly the differences between Cordone et al. (2022) and the proposed method (other than applying knowledge distillation).\n\n2. Please discuss what are the challenges of applying knowledge distillation for SNN object detection, compared to existing knowledge distillation methods between other types of networks.\n\n3. Please provide more details (setup and tool flow) for the implementation of the proposed method on the neuromorphic hardware.\n\n4. If possible, please provide the source code for reviewers\u2019 inspection during the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698149132304,
            "cdate": 1698149132304,
            "tmdate": 1699636294263,
            "mdate": 1699636294263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FtkkkwEHoL",
                "forum": "NQUXBoGiDU",
                "replyto": "IoiXb8apQm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. Our main difference to the work of Cordone et al. besides the use of KD is our CenterNet-based, anchor-less object detection via a heatmap which is particularly suited to spikes (and also enables an easy application of KD). It is also composed of very simple building blocks (see more below). We will try to highlight this stronger in our work.\n\nThank you for pointing out that the challenges of KD in SNN-based object detection are not clear enough in our work; there are a variety of them, and we will try to showcase them better. We should point out, however, that all previous works of KD with SNNs focused on classification as the topic of SNN-based object detection even without KD is not yet very well researched.\n\nWe would like to integrate our model into suitable spiking neuromorphic hardware. That is our main motivation behind choosing relatively simple building blocks of convolution layers immediately followed by spiking neurons to ensure we do not propagate float values over long distances such as the works of Su et al. (2023). However, the only commercially available SNN chip, the Akida Brainchip, is not only aimed at a different neural network framework, but also focuses on running converted, rather than trained SNNs and does not offer many details about the underlying spiking dynamics. Nevertheless, we are in close contact with an institution that is developing their own SNN chip and future works of ours aim at running our models on their hardware.\""
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229664341,
                "cdate": 1700229664341,
                "tmdate": 1700229664341,
                "mdate": 1700229664341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "azPLEeKIOX",
                "forum": "NQUXBoGiDU",
                "replyto": "FtkkkwEHoL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Reviewer_WJNq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Reviewer_WJNq"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your responses. Considering together the other reviewers' comments and responses, my score is confirmed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721341147,
                "cdate": 1700721341147,
                "tmdate": 1700721341147,
                "mdate": 1700721341147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C86ArpEnGv",
            "forum": "NQUXBoGiDU",
            "replyto": "NQUXBoGiDU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_DzYu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_DzYu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Spiking CenterNet for object detection on event streams. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. This work is the first approach that takes advantage of knowledge distillation for object detection using SNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The topic of distillation-boosted SNN for object detection is very interesting and attractive.\n\n2) This paper replaces CenterNet\u2019s upsampling by the more efficient modules from M2U-Net (Laibacher et al., 2019) and add binary skip connections between encoder and decoder, which improves gradient flow despite the spiking communication.\n\n3) This work utilizes Knowledge Distillation (KD) for SNNs in the context of object detection."
                },
                "weaknesses": {
                    "value": "1) The innovative KD in this paper has little effect on the accuracy, and only improves by 0.006. Related knowledge is not explained clearly, such as event-based object detection methods. The advantages of using CenterNet, such as the need for an NMS, are not explained. \n\n2) The authors should explore deeper backbone for the proposed framework, such as ResNet50, ResNet101. Whether the effect is better than shallow network?\n\n3) The details of the proposed method are not clear. For example, how is the identity mapping between encoder and decoder implemented, and how is it different from the implementation in ANN?\n\n4) The relevant work is not fully introduced, and KD has little effect on performance improvement, which brings large energy consumption."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3426/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3426/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3426/Reviewer_DzYu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376124747,
            "cdate": 1698376124747,
            "tmdate": 1699636294182,
            "mdate": 1699636294182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xz2SoEqE99",
                "forum": "NQUXBoGiDU",
                "replyto": "C86ArpEnGv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We have seen that our Knowledge Distillation(KD) approach does indeed improve performance significantly by a mean difference of 1.8 mAP. A single outlier of our 5  training runs without KD with a performance close to the KD-based trainings may mask that effect, but highlights why the maximum value is not very reliable. The only reason we include it is to keep some comparability to previous works which only report the maximum performance. We do acknowledge that we cut back on explaining some other advantages of CenterNet, which are not related to its suitability for SNNs, due to the page limitation. We will try to remedy this.\n\nExploring the effects on deeper networks is of interest to us but they introduce significant challenges. First, due to the additional time dimension the hardware requirements rise significantly, making training iterations more difficult. Second, the error due to the approximate nature of surrogate gradients occurs in each layer and thus accumulates with increased depth of the network. Additionally, the spike-based nature makes sustaining gradients over deep networks difficult. This means deeper SNNs are generally harder to train by sheer computational limits, even without factoring in the inference of a teacher model. However, it is true that KD might be help with some of these issues. \n\nOur chosen identity mapping between encoder and decoder is the same for both the ANN and SNN as the skip signal is concatenated to the decoder signal. It thus does not change the binary nature of the SNN skip signal. In contrast, residual identity connections are usually added to a module's output to let it learn the residuals, and thus introduces non-binary results after that addition which violate the binary nature of our spikes. Boolean alternatives which would preserve the binary nature severely limit the advantage of these connections and were not helpful in our initial trials.\n\nWe observed that while our KD did increase the energy consumption, the resulting network is still vastly more energy-efficient than the ANN alternative based on the number of MAC and AC operations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229088790,
                "cdate": 1700229088790,
                "tmdate": 1700229088790,
                "mdate": 1700229088790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v5XxMwR7op",
                "forum": "NQUXBoGiDU",
                "replyto": "xz2SoEqE99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Reviewer_DzYu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Reviewer_DzYu"
                ],
                "content": {
                    "title": {
                        "value": "I stuck with the initial score."
                    },
                    "comment": {
                        "value": "The author's responses address my concerns, but there's room for optimization in the network structure's design and writing."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534817639,
                "cdate": 1700534817639,
                "tmdate": 1700534817639,
                "mdate": 1700534817639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VuEg7bVoGi",
            "forum": "NQUXBoGiDU",
            "replyto": "NQUXBoGiDU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_Dq9d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_Dq9d"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"Spiking CenterNet,\" a novel object detection approach that leverages the energy-efficiency of Spiking Neural Networks (SNNs). Positioned as a solution for the growing demand in edge devices and self-driving cars, this method combines an SNN-based adaptation of CenterNet with an M2U-Net-based decoder. Significantly, the authors incorporate Knowledge Distillation to further enhance the performance of their SNN, making their model stand out in object detection tasks using event data. The primary contribution lies in merging the benefits of SNNs with knowledge distillation to optimize object detection in energy-constrained environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The integration of an SNN adaptation of CenterNet with an efficient M2U-Net-based decoder is a novel approach in the object detection domain.\nThe model not only addresses the energy efficiency challenge but also outperforms comparable object detection models on event data."
                },
                "weaknesses": {
                    "value": "1.\tIn principle, there\u2019s no new architecture built for object detection\n2.\tThe preprocessing of event-data makes it quite similar to binarized conventional videos, instead of digging into the intrinsic benefits of asynchronous properties. \n3.\tThe improvements made by Knowledge Distillation are quite limited, while the whole paper emphases on KD a lot. More importantly, since the idea of KD is guiding SNN models by ANN models, how to obtain the unique advantages of SNNs from KD.\n4.     The sparsity listed in table 2 is not the sparsity but the density (higher sparsity means sparser)."
                },
                "questions": {
                    "value": "See the weaknesses, also:\n\n1. what's the reason for choosing the architecture? Is that possible to employ other kinds of structures?\n2.  How to guide an efficient SNN by an ANN teacher, in terms of the spatio-temporal representation ability, the non-linear firing dynamics etc?\n3. Why emphases on KD a lot, given the benefits of KD are quite limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591440185,
            "cdate": 1698591440185,
            "tmdate": 1699636294112,
            "mdate": 1699636294112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cPXV1My9ZK",
                "forum": "NQUXBoGiDU",
                "replyto": "VuEg7bVoGi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We are proposing a new architecture as we not only created a spiking version of CenterNet, but also fundamentally changed its decoder part with structures inspired by M2U-Net. We focused on keeping the basic structures simple to maximize the likelihood that it will be supported by future neuromorphic SNN hardware.\n\nWe acknowledge that the preprocessing we took from Cordone et al. 2022 makes less use of the asynchronous properties of the event data. However, our main goal was solving this very difficult task (that only one previous work with a pure SNN approach has addressed) and improving on the previous results at all while developing a more flexible architecture.\n\nSNN training due to the on-off behaviour of the spiking neurons is notoriously unstable and difficult. We have seen that while a single training run without Knowledge Distillation (KD) came close to our KD-based training, in the vast majority of cases our KD-based training was significantly better. This can be seen in the more reliable mean performance over our 5 trainings runs, rather than the maximum prone to outliers. We also see that while KD with an ANN teacher decreases the energy efficiency of the SNN, the SNN is still vastly more efficient than the non-spiking ANN as the sparse nature of the SNN has not changed much.\n\nRegarding the 'sparsity', we have used the interpretation of this term established in previous work (Cordone et al, 2022) to avoid confusion when comparing the works. We acknowledge, however, that we find this interpretation also rather unintuitive and will change it accordingly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228151464,
                "cdate": 1700228151464,
                "tmdate": 1700228151464,
                "mdate": 1700228151464,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l9IxlAr3vx",
            "forum": "NQUXBoGiDU",
            "replyto": "NQUXBoGiDU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_gBpC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3426/Reviewer_gBpC"
            ],
            "content": {
                "summary": {
                    "value": "The paper does knowledge distillation on SNNs"
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Not applicable"
                },
                "weaknesses": {
                    "value": "This paper has no contributions besides just applying knowledge distillation on SNN. Further there are many works that have shown ANN-to-SNN distillation can help. I recommend the authors to look at all the SNN work out there that focus on improving SNN performance using interesting SNN optimization techniques such as those from the research group of Priya Panda, Guoq Li, and many others...."
                },
                "questions": {
                    "value": "See weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3426/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781141040,
            "cdate": 1698781141040,
            "tmdate": 1699636294040,
            "mdate": 1699636294040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A52Znoaj6p",
                "forum": "NQUXBoGiDU",
                "replyto": "l9IxlAr3vx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3426/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We would like to point out three things: First, we introduce a novel SNN architecture based on several suitable ANN models which significantly outperforms comparable purely SNN-based approaches. Second, we also are, to the best of our knowledge, the first ones to utilize Knowledge Distillation for SNN-based object detection, a vastly more complex problem than the image classification task that is indeed well-researched with both SNNs and Knowledge Distillation. Lastly, we are actually citing three of Guoqi Li's works. Among other things, we use the SpikingJelly framework developed by his group as the core of our SNN models (see Section 3.1). Please let us know whether this clears up your concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3426/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227429457,
                "cdate": 1700227429457,
                "tmdate": 1700227429457,
                "mdate": 1700227429457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]