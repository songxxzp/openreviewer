[
    {
        "title": "Deep graph kernel point processes"
    },
    {
        "review": {
            "id": "a18mPZgavK",
            "forum": "c1QBcYLd7f",
            "replyto": "c1QBcYLd7f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_XHVx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_XHVx"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces GNN into the spatio-temporal point process framework to enhance the expression ability of point processes on graph data. The idea of introducing graph neural networks to model the kernel of point processes is novel. The experimental results sufficiently support the effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The idea of modeling kernels in point processes by GNNs is novel and meaningful to the development of the community. The introduction of localized graph filters improves the scalability of our model when applied to large graphs.\n3. The experiments are sufficient and support the main idea of this paper."
                },
                "weaknesses": {
                    "value": "The main concern is the contribution of this article. Many previous works have tried to study point processes in the context of GNN. (For example, see [1]) From this perspective, this article is not innovative enough. Among them, the main idea of equation (2) comes from [2]. This article focuses on expressing the h_d function using localized graph filters. Although effective, the contribution appears to be limited.\n\n\n[1] Pan Z, Wang Z, Zhe S. Graph-informed Neural Point Process With Monotonic Nets[J]. 2022.\n[2] Dong Z, Cheng X, Xie Y. Spatio-temporal point processes with deep non-stationary kernels[J]. arXiv preprint arXiv:2211.11179, 2022."
                },
                "questions": {
                    "value": "1. Based on comparison with existing work, would you mind further elaborating on the contribution of this article?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Reviewer_XHVx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673018805,
            "cdate": 1698673018805,
            "tmdate": 1699636148775,
            "mdate": 1699636148775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uakyfhs8NJ",
                "forum": "c1QBcYLd7f",
                "replyto": "a18mPZgavK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Detailed comparison with literature"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments and confirmation of our experiments. \n\nRegarding weakness, we have now provided a comprehensive comparison with related works in the literature, in Table A.1, and have explained the contribution/novelty of our paper in detail in Appendix A. \n\nWe want to point out that the extension to the graph kernel is non-trivial, and a lot of modeling and experiment validation development needs to be done. We have considered various types of graph kernels based on various GNNs and also consider the case when the graph topology is known, and when the graph topology is unknown, then we use the GAT. We provided extensive numerical experiments and compared them with multiple baselines and SOTA. In particular, since point process data over graph is so widely available, and there is still a lack of principled graph kernel design, we believe this work will be a valuable addition to the literature."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492330915,
                "cdate": 1700492330915,
                "tmdate": 1700492330915,
                "mdate": 1700492330915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4nSeR2PDBv",
            "forum": "c1QBcYLd7f",
            "replyto": "c1QBcYLd7f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_nvzU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_nvzU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes using graph neural networks to define the inter-mark relationship in triggering kernels of marked Hawkes processes. The proposed model represents the triggering kernels by localized graph filter bases, which permit flexible modeling of inter-event-category dependence, including non-stationary, multi-hop exciting, and inhibiting effects. The validity of the model was confirmed by the experiments on synthetic and three real-world data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. The relationship with the related works is clearly presented.\n- Experimental results support that the proposed model provides good predictive performance.\n- The training algorithm scales linearly with the data size.\n- The validity of the proposed model was evaluated on several real-world data."
                },
                "weaknesses": {
                    "value": "- The good accuracy achieved by the proposed model is practically important, but the technical contribution of the model seems to be somewhat marginal because it is a reasonable but mediocre idea to use a GNN to model the inter-mark relationship in inference kernels, and any technical difficulty is not seen in the training algorithm.\n- Any limitations of the proposed model are not discussed. For example, it seems that the intensity function of the proposed model could be negative even if the log-barrier method is adopted, while the conventional methods (e.g., RMTTP) are designed not to worry about it.\n- (Minor comment) There are typos in Reference (e.g., rules of upper/lowercase is not consistent)."
                },
                "questions": {
                    "value": "- To the best of my knowledge, (Omi et al., 2019) doesn\u2019t consider marks of each event in the model. How was FullyNN implemented in the experiment as a marked point process model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2156/Reviewer_nvzU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818990507,
            "cdate": 1698818990507,
            "tmdate": 1699636148679,
            "mdate": 1699636148679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JDe4TuT4nf",
                "forum": "c1QBcYLd7f",
                "replyto": "4nSeR2PDBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Technical novelty"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. We now have clarified in Appendix A that incorporating GNN in the graph-based influence kernel is crucial and non-trivial, requiring a carefully designed model. Our paper is the first one to incorporate GNN in the influence kernel without any parametric constraints."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492006471,
                "cdate": 1700492006471,
                "tmdate": 1700492006471,
                "mdate": 1700492006471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ly7TMorjSu",
                "forum": "c1QBcYLd7f",
                "replyto": "4nSeR2PDBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussing limitation of proposed model"
                    },
                    "comment": {
                        "value": "We now have discussed the potential limitation in the Conclusion of the paper. As we have explained to the above reviewer, current kernel representation still assumes that the influence over events is \"additive,\" which is common in the kernel-based type of methods for point processes. To be more general, one can potentially adopt a non-additive model over events that can be more complex, and we leave it for future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492062200,
                "cdate": 1700492062200,
                "tmdate": 1700492062200,
                "mdate": 1700492062200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lZlIj2S1Uk",
                "forum": "c1QBcYLd7f",
                "replyto": "4nSeR2PDBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ensuring non-negativity of intensity function of the proposed model"
                    },
                    "comment": {
                        "value": "In our paper, in our log-barrier function equation in the appendix (E.1) and (E.2), we used a small constant b<0 inside the log-barrier, which means that we can allow the intensity to have small non-negativity. However, we do this to get an easy initialization: we can start with a reasonable guess that allows for a small amount of negativity and tune b to diminish to 0 as the training process continues, and in the end, we observe in the experiment in the end the learned intensity function induced by the log-barrier is always non-negative."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492114628,
                "cdate": 1700492114628,
                "tmdate": 1700492114628,
                "mdate": 1700492114628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z4bCSTGsn3",
                "forum": "c1QBcYLd7f",
                "replyto": "4nSeR2PDBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Typos in Reference (e.g., rules of upper/lowercase is not consistent)"
                    },
                    "comment": {
                        "value": "Thank you very much. We have fixed these typos in the reference."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492149153,
                "cdate": 1700492149153,
                "tmdate": 1700492149153,
                "mdate": 1700492149153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q2TrghtNmR",
                "forum": "c1QBcYLd7f",
                "replyto": "4nSeR2PDBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison with (Omi et al., 2019)"
                    },
                    "comment": {
                        "value": "Indeed, the reference (Omi et al. 2019) did not consider marks, and their model can only be used for temporal point process. In order to compare with this method for the graph setting, we further extend their method to model the mark distribution on top of FullyNN. We have now provided complete details of the extended FullyNN in Appendix H.2."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492224775,
                "cdate": 1700492224775,
                "tmdate": 1700492224775,
                "mdate": 1700492224775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xFN1PWDtYp",
                "forum": "c1QBcYLd7f",
                "replyto": "Q2TrghtNmR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Reviewer_nvzU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Reviewer_nvzU"
                ],
                "content": {
                    "title": {
                        "value": "Response to author comments"
                    },
                    "comment": {
                        "value": "Thank you for the authors\u2019 clarification. I stand by the point that the technical contribution of the paper is valuable but not impactful, and keep my score. \n\nComments to the discussion about non-negativity of intensity: I understand that no negative intensity values were observed in the experiments, but it necessarily ensures that the learned intensity function model does not have any negative values on a new event data. For example, if unexpectedly frequent events are emitted from a mark who has a negative impact on a new test data, then the intensity function could be negative. I believe the practical values of the proposed model, but some caution should be exercised when using it."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518132376,
                "cdate": 1700518132376,
                "tmdate": 1700518132376,
                "mdate": 1700518132376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aKQje5jToY",
            "forum": "c1QBcYLd7f",
            "replyto": "c1QBcYLd7f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_o26h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_o26h"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel point process model for discrete event data over graphs. It extends the influence kernel-based formulation by Hawkes to include Graph Neural Networks (GNN), aiming to capture the influence of historical events on future events' occurrence. The model combines statistical and deep learning approaches to enhance model estimation, learning efficiency, and predictive performance. The paper also demonstrates the superiority of the proposed model over existing methods through comprehensive experiments on synthetic and real-world data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper introduces an innovative approach by integrating GNN with the classic influence kernel-based formulation, offering a new perspective in point process modeling over graphs.\n2.\tThe experimental setup and the methodologies used are sound and well-executed."
                },
                "weaknesses": {
                    "value": "1.\tThe proposed approach appears to be a combination of existing methods, which raises questions about its novelty. To provide a valuable contribution to the field, it is crucial to address the limitations and shortcomings of existing approaches, such as kernel-based, deep neural network-based, and graph-based models. Additionally, the authors should clearly articulate the advantages of this new method compared to GNN-based models [1,2,3,4].\n\n- [1] Learning neural point processes with latent graphs\uff0cWWW2021\n- [2] Graph neural point process for temporal interaction prediction\uff0cTKDE2022\n- [3] Spatio-temporal point processes with deep non-stationary kernels\uff0cICLR2022\n- [4] Gaussian process with graph convolutional kernel for relational learning\uff0cKDD2021\n\n2.\tGraph construction, including the definition of nodes and edges, is pivotal in GNN-based methods. The paper needs to provide a detailed explanation of how edges between nodes are established. Are the edge construction methods applicable universally across different scenarios, or do they require case-specific adaptations? While the process is well-defined for synthetic datasets, it is less clear in real-world data scenarios. The assertion that edges represent potential interactions should be elaborated upon to ensure clarity.\n3.\tThe authors use THP-S and SHAP-G as baselines. However, these baselines employ powerful transformers and graph attention mechanisms. It would be expected that they offer significant flexibility. This work should provide a robust justification for why the proposed model outperforms these baselines. Is there a theoretical basis for the superior performance?\n4.\tNotably, the synthetic data is primarily generated using kernel methods. Hence, I think the proposed model should fit this data well. However, the observed improvement in likelihood compared to the baseline is very minimal and possibly not statistically significant. The authors should offer a convincing explanation for this limited improvement. \n5.\tThis concern follows question 4. The discrepancy between the log-likelihood, which is similar to the baseline, and the substantially improved time prediction MAE is perplexing. I am suspicious about the accuracy of the MAE calculation. I recommend the authors clarify how this probabilistic prediction model is employed for predictions \u2013 whether it is based on random sampling or mean predictions.\n6.\tIn the evaluation using real-world data, Table 3 indicates that the advantage of the graph-based approach diminishes as the number of nodes increases. This observation runs counter to my expectation that graph modeling should excel in complex graph structures. The authors should provide a detailed analysis and potential explanations for this outcome.\n7.\tLack of crucial baseline comparisons. A robust baseline comparison is essential for a comprehensive evaluation of the proposed method. The absence of such comparisons is a notable limitation in the paper. For instance, even without incorporating a graph, models like THP [5], and DAPP[6] possess the inherent capability to capture inter-event interactions. Additionally, there are intensity-free methods [7] available in the literature. Hence, it is necessary to compare the proposed model against state-of-the-art point process models [1,2,3,4,5,6,7].\n- [5] Zuo, Simiao, et al. \"Transformer hawkes process.\" International conference on machine learning. PMLR, 2020.\n- [6] Zhu, Shixiang, et al. \"Deep fourier kernel for self-attentive point processes.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n- [7] Shchur, Oleksandr, Marin Bilo\u0161, and Stephan G\u00fcnnemann. \"Intensity-Free Learning of Temporal Point Processes.\" International Conference on Learning Representations. 2019.\n8.\tSimilarly, in real-world data evaluation, there is an absence of comparisons with state-of-the-art STPP models and their mentioned baselines. The authors should consider referring to recent literature [8,9,10] for these comparisons to provide a more comprehensive evaluation.\n- [8] Zhou, Zihao, et al. \"Neural point process for learning spatiotemporal event dynamics.\" Learning for Dynamics and Control Conference. PMLR, 2022.\n- [9] Chen, Ricky TQ, Brandon Amos, and Maximilian Nickel. \"Neural Spatio-Temporal Point Processes.\" International Conference on Learning Representations. 2020.\n- [10] Yuan Yuan, et al. \u201cSpatio-temporal Diffusion Point Processes.\u201d In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '23)."
                },
                "questions": {
                    "value": "Please see my listed weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698948193768,
            "cdate": 1698948193768,
            "tmdate": 1699636148609,
            "mdate": 1699636148609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I6eMOOs0qB",
                "forum": "c1QBcYLd7f",
                "replyto": "aKQje5jToY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Novelty"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comment. We have clearly summarized the novelty of the method, which is not simply an addition of existing methods in fact, there are non-trivial challenges to overcome and carefully design the model considering the graph structure of the data in a principled manner while maintaining the statistical/stochastical law of the point process, which is achieved through the kernel representation. Moreover, as also mentioned by the other reviewer, the core novelty is \"The idea of introducing graph neural networks to model the kernel of point processes is novel.\" \"The idea of modeling kernels in point processes by GNNs is novel and meaningful to the development of the community.\" We will also provide answers to the remaining questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699815173545,
                "cdate": 1699815173545,
                "tmdate": 1699818310153,
                "mdate": 1699818310153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vYF48Bkjm0",
                "forum": "c1QBcYLd7f",
                "replyto": "aKQje5jToY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o26h"
                    },
                    "comment": {
                        "value": "> The proposed approach appears to be a combination of existing methods, which raises questions about its novelty.\n\nWe have now provided a comprehensive comparison with related works in the literature in Table A.1, and have explained the contribution/novelty of our paper in detail in Appendix A.\n\n> The paper needs to provide a detailed explanation of how edges between nodes are established.\n\nWe would clarify that for real-world datasets, we adopt GAT in our framework to infer the underlying graph structure and node interactions based on the learned attention weights from data, since the underlying graph structure is unobserved in real-world datasets and we have no prior knowledge about the graphs. The latent graph structure is constructed upfront in synthetic datasets.\n\n> Is there a theoretical basis for the superior performance against the THP-S and SHAP-G baselines? These baselines employ powerful transformers and graph attention mechanisms.\n\nWe first would point out that the model performance of neural point processes is affected by various factors (*e.g.*, deep neural network architectures, model formulation of the point process, specific problems), and so far, a general theoretical basis can hardly be established.\n\nMeanwhile, we would clarify there is no GNN architecture incorporated in \\texttt{THP-S} and \\texttt{SHAP-G}, and they model the conditional intensity function instead of the influence kernel, which may not be able to capture the repeated influence patterns effectively like our method.\n\n> The observed improvement in likelihood compared to the baseline is very minimal and possibly not statistically significant.\n\nWe clarify that we not only report the average metrics from multiple independent runs but also the standard errors of those metrics. We believe that the improvement is significant both from statistical and practical perspectives.\n\n> Clarify how this probabilistic prediction model is employed for predictions \u2013 whether it is based on random sampling or mean predictions.\n\nThe predictability of the proposed model is evaluated through sampling. As we clarify in the paper, we use the learned model on the training set to generate new event sequences over the entire time horizon, and compare the generated sequences with the testing sequences, in terms of the predicted event frequency and the predicted distribution of event types. The time MAE is calculated as the mean absolute error between the temporal frequency of generated events and testing events.\n\n> Table 3 indicates that the advantage of the graph-based approach diminishes as the number of nodes increases, which runs counter to my expectation.\n\nWe would point out that the comparison of the absolute difference of performance metrics across different point process datasets has little practical meaning, for they have diverse problem settings and data features. More importantly, our proposed method enjoys superior performance advantages against baselines consistently across different datasets.\n\n> Lack of crucial baseline comparisons.\n\nWe would highlight that we have already compared with several state-of-the-arts that the reviewer listed, and have highlighted the results in the paper's experiments (Section 4).\n\n> Similarly, in real-world data evaluation, there is an absence of comparisons with state-of-the-art STPP models and their mentioned baselines. The authors should consider referring to recent literature for these comparisons to provide a more comprehensive evaluation.\n\nWe would highlight that we have compared with state-of-the-art STPP models in the ablation study (Section 4.2.1), which has clearly demonstrated the superior performance of our method against modeling graph point process using models constructed in the Euclidean space."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713408743,
                "cdate": 1700713408743,
                "tmdate": 1700713408743,
                "mdate": 1700713408743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JWC45lwd09",
            "forum": "c1QBcYLd7f",
            "replyto": "c1QBcYLd7f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_E4mM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2156/Reviewer_E4mM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a flexible deep graph kernel point process model for marked point process data, where marks are treated as \"nodes\" in a graph. The interactions between marks are modeled through Graph Neural Networks (GNN), which are very flexible to capture various types of interactions over a graph. The kernel representation of the marked point process is then constructed through a convolution of kernels over the time domain and the graph. The effectiveness of the proposed method is demonstrated through extensive simulation studies and some benchmark data points, which outperforms many existing methods for marked point process data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I found the idea of treating marks as graph nodes to be very interesting, which opens the door for many future research directions. The proposed kernel representation of the marked point process is straightforward but powerful, suggesting a wide range of possible applications. The paper is very well written and the presentation is clear. The numerical experiments are impressive and convincing."
                },
                "weaknesses": {
                    "value": "It will be good to add some background information on the graph-based kernels so that the paper is more self-contained. It will also be helpful if the authors can comment on what types of network interactions can be modeled and what the limitations of such representation are. This way, the readers will have a better idea of the advantages and limitations of the proposed model."
                },
                "questions": {
                    "value": "1. Some relevant recent work on Hawke's process on networks is missing. For example, [1]. In [1], the graph structure (i.e., the adjacency matrix) is observed, can you comment on how one can use this information in the proposed model? [1] also handles the heterogeneity of the nodes in the graph, can similar things be done using the proposed model?\n\n2. As I mentioned before, it will be good to add some background information on the graph-based kernels so that the paper is more self-contained. It will also be helpful if the authors can comment on what types of network interactions can be modeled and what the limitations of such representation are. This way, the readers will have a better idea of the advantages and limitations of the proposed model.\n\n3. On page 4, the basis functions of $g_d(\\cdot,\\cdot)$ typically need to be orthogonal. Can you comment on how to ensure the orthogonality of basis functions using the fully connected neural network? If orthogonality is not imposed, how do you ensure the uniqueness of the decomposition?\n\nReference:\n\n[1]. Fang, G., Xu, G., Xu, H., Zhu, X., & Guan, Y. (2023). Group network Hawkes process. Journal of the American Statistical Association, (just-accepted), 1-78."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699373376440,
            "cdate": 1699373376440,
            "tmdate": 1699636148524,
            "mdate": 1699636148524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UstWuIx2RP",
                "forum": "c1QBcYLd7f",
                "replyto": "JWC45lwd09",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Add some background information and kernel representation"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comment and acknowledge the value of our proposed model. We have added additional background information on the graph-based kernels to make the paper more self-contained. \n\nRegarding \"what types of network interactions can be modeled and what the limitations of such representation are.\" Currently, we assume the kernel is not \"rank-one,\" with more than one pair of basis functions (over time and graph), the temporal kernel can allow non-stationarity over time, and the graph basis is completely general to enable efficient computational; we assume each basis function is decoupled over \"time\" and \"graph\" which is also intuitive and commonly used in the literature; compared with prior models, which usually is \"rank-one\" our model can be much more expressive while striking similar computational complexity. \n\nOne potential limitation is that the current kernel representation still assumes that the influence over events is \"additive,\" which is common in the kernel-based type of methods for point processes. To be more general, one can potentially adopt a non-additive model over events that can be more complex, and we leave it for future work. We have now discussed the limitation in the Conclusion section."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491774393,
                "cdate": 1700491774393,
                "tmdate": 1700680516923,
                "mdate": 1700680516923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G0HSs9Wxeb",
                "forum": "c1QBcYLd7f",
                "replyto": "JWC45lwd09",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Graph structure"
                    },
                    "comment": {
                        "value": "We want to emphasize that our framework can directly incorporate known graph structure; in particular, the observed graph structure information can be leveraged in the construction of the localized graph filter $B_r(v', v)$ in the influence kernel based on the specific formulation of the GNN structures discussed in Appendix CB (\\textit{e.g., Chebnet, GAT, L3net, GPS Graph Transformer}). Indeed, we have demonstrated this using synthetic data experiments in Section 4.1.2; the localized graph filters are created based on the observed graph structures.\n\nWe currently do not directly handle\n``heterogeneity of nodes in graph,'' we admire this approach in [1] and believe our model can potentially be extended in the future to account for this, for instance, by introducing latent structures to the nodal feature."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491855264,
                "cdate": 1700491855264,
                "tmdate": 1700491855264,
                "mdate": 1700491855264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C4b1i3Fa5e",
                "forum": "c1QBcYLd7f",
                "replyto": "JWC45lwd09",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2156/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Basis function orthogonality"
                    },
                    "comment": {
                        "value": "Although the design of the kernel is motivated by Mercer's theorem explained in Sec. 3.2, and the basis function is represented using neural networks. In practice, we do not need orthogonality for the basis function. The reason can be explained potentially by the fact that as long as the basis function spans the space, it will be a good representation of the kernel; the training of the neural networks is done using stochastic gradient descent, which rarely leads to any nearly linearly dependent basis functions. In our experiment, we have chosen the rank $r$ to be large enough such that the empirical performance is good and validated in Appendix G."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491889426,
                "cdate": 1700491889426,
                "tmdate": 1700491889426,
                "mdate": 1700491889426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]