[
    {
        "title": "Bi-Directional Goal-Conditioning on Single Policy Function for State Space Search"
    },
    {
        "review": {
            "id": "4SGSgsmNjO",
            "forum": "vSBB2nRaoj",
            "replyto": "vSBB2nRaoj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_hi7c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_hi7c"
            ],
            "content": {
                "summary": {
                    "value": "This method proposes a framework called `SRE-DQN` which is a Goal-Conditioned Reinforcement Learning (GCRL) based framework where \ngoal states are sampled to create both a forward and a reverse task from the primary state space search. They introduce hindsight re-labeling for the forward tasks and a concept called `Foresight` for reverse tasks. To learn reverse moves from the reverse paths, agents are initiated from these paths to gather forward moves. All these components are glued together in a framework named `Scrambler-Resolver-Explorer (SRE)`. \nExperiments are ran on toy problems with discrete state and action : `NChain` and a `Simple Gridworld`."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces an interesting exploration technique termed as `Scrambler-ResolverExplorer (SRE)`. I think the idea of learning forward and backward representations (i.e. learning how to reverse an action) is interesting in general. And this idea has similarity to [1] where a forward and backward representation of the reward is being learned.\n- It is also a nice concept to try to have a distinction between an explorer, scrambler and resolver module where the primary focus of exploration of each module is distinct.\n\n\n\n\n\n[1] Learning One Representation to Optimize All Rewards - https://arxiv.org/pdf/2103.07945.pdf"
                },
                "weaknesses": {
                    "value": "- The experimental results presented in Figure 3 and 4 are not convincing at all. The tasks `NChain` and `Simple GridWorld` are too simple and despite that the results seem very unstable.\n- All the results are single-seeded and there is no measure of variance among multiple runs. I recommend the authors to present multi-seeded results over at least `10` random seeds per run to ensure reproducibility.\n- `Figure 3` : All methods including the `HER` and `RE` baselines seem to work on-par and `SRE_NITR` diverges on the 10Chain. And on 15Chain all methods perform the same.\n- `Figure 4` : Mixed and unstable results. The proposed method `SRE_NC` seems to have divergence issues.\n- There has been more work for exploration in RL, looking at expanding trees and search - for example [1] which can potentially be a baseline.\n- The framework still operates under a simple discrete states and action setting building on `DQN`. The authors claim that nothing is preventing \nthe framework to being extended to a continuous setting. However given the performance on the simpler setting, I'm not convinced it can readily be extended.\n- HER has been tried on many grid world navigation tasks. I recommend to the authors to redo a literature survey on this to pull more related work.\n- The writing and flow of the related work can be improved significantly. For instance, there is a related work section on `Surrogate Objective for Goal-conditioned RL` going through equations for the Lipschitz continuity assumption and Wassersetein distance and its not even relevant to their framework. Overall this makes reading the paper more difficult because I don't need to see the equations for related work.\n\n[1] Probabilistic Planning with Sequential Monte Carlo methods - ICLR 2019 - https://openreview.net/pdf?id=ByetGn0cYX"
                },
                "questions": {
                    "value": "- Why is the HER baseline failing in Figure 4 ? If the reward is sparse and binary, HER should be resampling goals, why does it fail completely?\n- Did the authros try other exploration mechanisms apart from `\u03f5-greedy` ? For instance `Curiosity-driven Exploration` , or `Max-Entropy-RL` methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685153786,
            "cdate": 1698685153786,
            "tmdate": 1699636770697,
            "mdate": 1699636770697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ndfjGEcVCS",
                "forum": "vSBB2nRaoj",
                "replyto": "4SGSgsmNjO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your thorough and insightful review of our manuscript, \"Bi-Directional Goal-Conditioning on Single Policy Function for State Space Search\". We appreciate the time you took to evaluate our work and provide constructive feedback. Below, we address each of your points to clarify and improve our manuscript.\n\n**1. Experimental Results' Robustness (Figures 3 and 4):**\nWe acknowledge your concern regarding the robustness of our experimental results. HER and other methods used here are well-established algorithms that can solve this state space search problem (which is a simple MDP) very well. Due to this reason, we\u2019ve not added a best effort implementation. When all variants perform well, it might become hard to explicitly understand the performance of agents when using samples from a bigger task space. We observe this in Fig 3. NChain experiments. To differentiate the performances better, we use these experiment parameters (function approximator size, number of updates to the UVFA, exploration parameters, number of episodes and so on..) borrowed from NChain experiments to varying complexities of GridWorld. Fortunately, we were able to see clear differences in performance of the models and have reported the results. The code will also be made available under anonymity and a link to the same will be added to the abstract of the updated paper. Thank you for your suggestion on changing the experiment from single-seeded to multi-seeded results, we will modify our results based on this and update our results.\n\n**2. More Exploration Settings & Planning Literature**\nThank you for pointing out some interesting literature. The primary research focus of our work hasn\u2019t been to come up with the most optimal exploration/planning setting for state space search problems, but instead, just to check if defining new tasks from an existing task (if the original/forward task is to go from a start state to a goal state) like backward or intermediate tasks (start from goal state and go to the original start state/ or an intermediate state to another intermediate state) and overload all these tasks on a single policy function (by extending GCRL) helps the agent perform better in the original task. We\u2019ve designed our baselines in a manner to exactly understand how different task definitions & formulations and training affects the performance in the main task. \n\nThank you for pointing us to multiple exploration ideas. As part of our future work, we plan on extending this way of generating new tasks and training the agent, by combining it with various existing exploration and planning ideas.\n\n**3. Writing and Structure of Related Work:**\nWe agree that the clarity of the related work section can be improved. We have revised this section, focusing on directly relevant literature and removing unnecessary technical details, such as the equations for the Lipschitz continuity assumption and Wasserstein distance. This revision would definitely make the paper more accessible and focused on our framework.\n\nThank you again for your valuable feedback, which has undoubtedly helped improve the quality and clarity of our work.\n\nBest regards,\nAuthors"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621531354,
                "cdate": 1700621531354,
                "tmdate": 1700621531354,
                "mdate": 1700621531354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8x4OHSKgJ0",
            "forum": "vSBB2nRaoj",
            "replyto": "vSBB2nRaoj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_mR2i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_mR2i"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposes Scrambler-Resolver-Explorer (SRE), which extends hindsight experience replay (HER) with bi-directional goal conditioning. SRE consists of three modules, Explorer, Scrambler, and Resolver for both the usual exploration and backward trajectory sampling from (original) goal states. It aims at gathering more samples close to the goal state region for more efficient training. They evaluate the proposed approach and compare it with baseline methods in NChain and GridWorld."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The GCRL problem is an important problem, especially in terms of having controllability connecting the agent and its state in the environment.\n- Given the employed assumptions, the proposed method that uses both directions for more effective GCRL is somewhat novel and might be useful.\n- The manuscript is easy to read and follow.\n- The baselines for the experiments are formed to examine the importance of some of the proposed modules."
                },
                "weaknesses": {
                    "value": "- I believe the biggest weakness is that the empirical evaluation is done in simple environments, NChain and GridWorld. I believe that GCRL, which is about overcoming difficulties in reaching different goal states, needs environments with complexities in their dynamics to some degree (e.g., locomotion environments from MuJoCo) as testbeds.\n- I have some concerns about the main assumption of the ability to spawn agents at arbitrary states, especially in the GCRL setting, where *reaching* specific goal states is the objective. Taking advantage of the simulated environments, if spawning at arbitrary states can be done without any costs, some combination of local exploration and spawning might be effective for both exploration and gathering various samples for re-labeling and goal-conditioned training."
                },
                "questions": {
                    "value": "Please take a look at the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820059101,
            "cdate": 1698820059101,
            "tmdate": 1699636770579,
            "mdate": 1699636770579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kV52Y8Pu6E",
                "forum": "vSBB2nRaoj",
                "replyto": "8x4OHSKgJ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your constructive feedback on our submission \u201cBi-Directional Goal-Conditioning on Single Policy Function for State Space Search\n\u201c. We appreciate your acknowledgment of the novelty and readability of our manuscript, as well as the importance of the Goal-Conditioned Reinforcement Learning (GCRL) problem.\n\nRegarding your concerns:\n\n1. **Applicability to State Space Search Problems**: \n\nWe build our idea on a fundamental assumption that if there is a path existing from start state to goal state, there is also a path back from the goal state to the start state. We tried to exploit the strength of GCRL, to train an agent to go from any state to any state, which implicitly leads to an assumption of action reversibility for each state-action pair. Unfortunately, this doesn\u2019t hold true for typical RL benchmark environments. Due to this reason, we stick to typical state space search problems (problems like NChain and GridWorld)\n\nWe agree that these environments (problems like NChain and GridWorld) used in our empirical evaluation are relatively simple. However, these simpler environments satisfy our reversibility assumption and also help us demonstrate our hypothesis of multi-task (start state -> goal state, goal state -> start state, intermediate states -> goal state and so on) definition learning and compare it with linearly increasing problem complexity. We acknowledge the need to test our approach in more complex environments, such as MuJoCo's locomotion tasks, which could better highlight the challenges and advantages in combining multi-task reformulations with GCRL. In future work, we plan to extend our evaluation to these more complex scenarios (that still remain reversible) to further validate the effectiveness of our approach.\n\n2. **Assumption of Spawning Agents at Arbitrary States**: \n\nIt is true that the ability to spawning at arbitrary states when combined with the GCRL setting will definitely improve the quality of samples obtained (after relabelling) and this is what we wish to fundamentally explore. Although, to prevent any unplanned correlations between the task definitions, we don\u2019t allow the agents to explicitly remember/control the spawning state sets. \n\nOur primary research objective in this work has been to check, if the multiple different tasks defined during the exploration phase (starting from any state s\u2019 and reaching new state s\u2019\u2019) generating more samples with rewards, enables the agent to learn more about the whole environment better, and if this knowledge will help in solving the primary task (start state and goal state of the original state space search problem) and from our set of experiments, there seem to be some potential improvement which deserves further and in-depth study. We believe this ideation and formulation of the GCRL problem will be further broken-down and studied by the community.\n\nThank you once again for your insightful feedback and for pointing out these critical aspects of our research.\n\nSincerely,\nAuthors"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621465937,
                "cdate": 1700621465937,
                "tmdate": 1700621465937,
                "mdate": 1700621465937,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZnAiCV1vji",
            "forum": "vSBB2nRaoj",
            "replyto": "vSBB2nRaoj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_bwK3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6710/Reviewer_bwK3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a way to sample and relabel goals to increase the sample efficiency of an off policy goal reaching agent. To do this, the authors propose three different state-goal samplers. The explorer, the scrambler and the resolver. \n\nThe explorer tries to solve the original task of reaching desired goals from the starting states.  The scrambler inverts this problem. The scrambler is started from the desired goal states of the original task and tries to reach the starting states of the original task. The resolver samples subgoals or waypoints which balance two objectives:\n1) are reachable from the original start states and,\n2) the original goals are reachable from them.\n\nThe authors instantiate this method using a DQN agent and perform experiments on many grid world MDPs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea of sampling waypoints can be very beneficial for sample efficient goal reaching. The paper recognizes this correctly."
                },
                "weaknesses": {
                    "value": "Weakness:\n1) It seems as if there is an implicit assumption (which should be explicitly stated) that the environment is reversible. That is, the start states can always be reachable from the goals states. This is not always true, for example you can't \"uncook\" food.\n3) Theoretically, there is no proof provided that the sampling method proposed can learn a meaningful goal conditioned policy. I understand it is difficult to provide any theoretical guarantees for relabeling methods, and perhaps this is out of scope of the paper. But, see the next point.\n4) All experiments are performed on extremely simple toy MDPs where judging the benefits of a complex goal sampling technique can be difficult. Moreover, the proposed method SRE-DQN doesn't perform the best in any of the tasks. \n5) It is surprising that the success rate on such simple MDPs is lower than 20%. For example in a 3x3 MDP, both the SRE-DQN and HER get around 0 success rate. I suspect that an error in the implementation is causing this. \n5) Implementation details as well as the code have not been provided."
                },
                "questions": {
                    "value": "Suggestions for improvement and minor questions:\n1) After reading just Section 1, it is unclear how the agent can collect trajectories starting from the goal state. If this is a assumption that the authors are making, then they should state it clearly in the introduction. They should also state why this assumption makes sense, and what are its limitations.\n2) In Section 4, 2nd para the authors state : \"Scrambler generated backward trajectories from goal state adversarial to the explorer\". Why are the goal state adversarial to the explorer?\n3) What are \"state space search\" problems? Is this the same as goal conditioned RL problems where the goal space is equal to the state space?\n\n\nTypos:\n1) Section 2, 2nd para: simiilar -> similar\n2) Section 3, 4th para: In the reward definition, $+1$ should be in the subscript. \n3) Section 4, 3rd para : generated -> generates\n3) Section 5.1, 4th para:  multi-directional) -> there is no corresponding opening bracket.\n4) In Section 3, para Surrogate Objective for Goal-conditioned RL:  $\\tau$ -> $\\tau^*$"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6710/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6710/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6710/Reviewer_bwK3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6710/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699201065868,
            "cdate": 1699201065868,
            "tmdate": 1699636770476,
            "mdate": 1699636770476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZlRHgh5LPs",
                "forum": "vSBB2nRaoj",
                "replyto": "ZnAiCV1vji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6710/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your constructive feedback on our submission \u201cBi-Directional Goal-Conditioning on Single Policy Function for State Space Search \u201c. We appreciate your acknowledgment of the novelty and readability of our manuscript, as well as the importance of the Goal-Conditioned Reinforcement Learning (GCRL) problem.\n\n**Assumption of Environment Reversibility**\n\nOur main assumption would be that there exists a path between goal state and start state (essentially, every new task we define, there exists a path from the start state and goal state of that particular task). The point you\u2019ve mentioned is very true, since as we increase the number of tasks, our assumption does directly converge to reversibility of state-action pairs. We acknowledge that we did not explicitly state this assumption. Thank you very much for pointing that the point is missing and the necessity to make it clear and explicit. We will add a small paragraph to discuss this limitation & assumption and make it explicit. We will also add a small part explaining how, these specific MDPs where path from any state to another and the goal state set is a subset of the original state space, are what we call \u2018state space search problems\u2019 in our work.\n\n**Environments, Performance of SRE-DQN not being the best & relative poor performance of other agents:**\n\nWe agree that a broader range of experiments is necessary. We agree that these environments (problems like NChain and GridWorld) used in our empirical evaluation are relatively simple. However, these simpler environments satisfy our reversibility assumption and also help us demonstrate our hypothesis of multi-task (start state -> goal state, goal state -> start state, intermediate states -> goal state and so on) definition learning and compare it with linearly increasing problem complexity. \n\nWe acknowledge the need to test our approach in more complex environments, such as MuJoCo's locomotion tasks, which could better highlight the challenges and advantages in combining multi-task reformulations with GCRL. In future work, we plan to extend our evaluation to these more complex scenarios (that still remain reversible) to further validate the effectiveness of our approach.\n\n**Regarding Low Success Rates:**\n\nAs you have mentioned, HER and other methods used here are well-established algorithms that can solve this state space search problem (which is a simple MDP) very well. Due to this reason, we\u2019ve not added a best effort implementation. When all variants perform well, it might become hard to explicitly understand the performance of agents when using samples from a bigger task space. We observe this in Fig 3. NChain experiments. To differentiate the performances better, we use these experiment parameters (function approximator size, number of updates to the UVFA, exploration parameters, number of episodes and so on..) borrowed from NChain experiments to varying complexities of GridWorld. Fortunately, we were able to see clear differences in performance of the models and have reported the results. The code will also be made available under anonymity and a link to the same will be added to the abstract of the updated paper.\n\n**Regarding SRE\u2019s Performance:**\n\nOur primary research focus in this paper has been about generating new \u2018task\u2019 definitions within the same state space, training the model on these newly defined tasks, and seeing if that helps the agent in solving the primary state space better. The full SRE agent uses forward, backward and multiple intermediate task definitions (multi-directional). Following the main claim of the paper (bi-directionality/two-task definitions), we see that SRE_NC (variant that only uses bi-directionality and does not use intermediate candidates) seems to be working well for our experiment setting.\n\n**Other Suggestions & Corrections**\n\nSince the UVFA acting as a policy function takes in two inputs (current state (s1) and desired state (s2)) and provides the optimal/desired action, to generate backward trajectories, we start the agent from the original goal state and provide the original goal state as the desired state into the policy function (UVFA). This way, in an abstract manner, we can say that we are requesting the agent to start from the goal state and traverse towards the start state. I understand this might have not been clear in section 1, in our new iteration that will be uploaded soon, I will make amends to the Section 1.1 to explain this. Thank you so much for pointing this out.\nWe apologize for the oversight. Producing adversarial trajectories would require exploiting the current policy (from forward) but we do not perform any such action.We have removed the part about the Scrambler being adversarial to the forward trajectories.\n\nWe look forward to the opportunity to contribute to this field. Thank you for your valuable feedback.\n\nSincerely,\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621876521,
                "cdate": 1700621876521,
                "tmdate": 1700621894908,
                "mdate": 1700621894908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1sYA3IiROj",
                "forum": "vSBB2nRaoj",
                "replyto": "ZnAiCV1vji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6710/Reviewer_bwK3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6710/Reviewer_bwK3"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for acknowledging my concerns. \n\n> Our main assumption would be that there exists a path between goal state and start state \n\nIs this the same as assuming reversibility of the MDP? Reversibility: Given that you can reach a goal state from a start state in an MDP, the assumption is that you can also reach the start state from the goal state.\n\n> However, these simpler environments satisfy our reversibility assumption and also help us demonstrate our hypothesis of multi-task\n\nYou could also extend the same toy MDPs to be more complex by increasing the size of the state-space. The example of solving a rubic's cube is, which you have included in the introduction can be a good task to test the improvement of your algorithm.\n\n> we\u2019ve not added a best effort implementation\n\nInstead of this, I think you should make the tasks more difficult. If you don't find a difference in 10x10 grids, then consider making the grid 100x100."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6710/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663875264,
                "cdate": 1700663875264,
                "tmdate": 1700663903618,
                "mdate": 1700663903618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]