[
    {
        "title": "GIFF: Generalized Inference Friendly Forward-Forward Algorithm"
    },
    {
        "review": {
            "id": "g4UkK3ZCM7",
            "forum": "wUKVia7J10",
            "replyto": "wUKVia7J10",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_V3Fd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_V3Fd"
            ],
            "content": {
                "summary": {
                    "value": "The present paper proposes a variant of the Forward-Forward (FF) algorithm [Hinton, 2022] which circumvents three issues: 1/ how the label is embedded and fed to the network, 2/ the poor performance of FF with conv layers, 3/ the inefficiency of the FF inference, which requires C forward passes for each class prediction, where C denotes the number of classes. The authors solve all three by proposing a separate label forward pathway, whereby the label is no longer fed only at the input layer through a single common data pathway, but broadcasted to all layers and embedded with small local auxiliary nets. This way, activations of the data forward pathway only need to be computed once and only the forward passes through the auxiliary nets which embed the label are computed C times. The authors claim superior performance to the standard FF algorithm, smaller memory footprint than BP and FLOPs for inference based on TinyML benchmarks (image classification, person detection and keyword spotting) on convolution-based architectures. \n\nMore precisely:\n- Section 3.1 and 3.2 reintroduce backprop and the FF algorithm.\n- Section 3.3 presents the core GIFF architectural ingredients, namely 1/ the separation of the feedforward pathway into one for the data, another one for the labels, 2/ a \"merging operation\" to blend the data activation and label information which is fed into the goodness function.\n- Section 3.4 presents results obtained on MNIST on MLPs, Convnets for GIFF, FF and BP in terms of number of parameters, peak memory and accuracy. It is claimed that GIFF maintains accuracy on par with BP while requiring much less memory (Table 2). Also it is shown that GIFF requires 10 times less MFLOPs than FF, as expected by construction of GIFF.\n- Section 4.1 introduces the experimental setup and provides a summary of the results obtained on the CIFAR10, VWW and GSC tasks for BP and GIFF. \n- Section 4.2 presents results obtained on CIFAR10 with. the ResNet-8 architecture, presenting the test accuracy vs peak memory tradeoff for various layer groupings (i.e. one auxiliary net for the label embedding might be used jointly for several layers, which reduces memory footprint of these auxiliary nets). It also shown that GIFF requires 10 times less MFLOPs than FF (as reported previously). \n- Section 4.3 and 4.4 report the same kind of study on the GSC and VWW tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written.\n- The use of auxiliary nets for the separate label forward pathway undeniably solves the inference problem of the FF algorithm.\n- GIFF is tested across various models and datasets, and various model setups (e.g. layer groupings). \n- I like that an alternative to BP is motivated for tinyML with quantitative metrics rather than fuzzy biologically plausible arguments."
                },
                "weaknesses": {
                    "value": "- While I am confident on the FLOPs reported for inference, I am much more skeptical on the memory peak evaluation (I detail in the next section why) and afraid it might skew all the presented results. I am really not sure about Eq. 9 of Appendix A.4. And in fact, it is up to 5% off the evaluation done by the C implementation on a tiny architecture used for MNIST. I am worried since based on this sole observation on the fully connected architecture used on MNIST, **\"[they] employ [their memory estimation method] for [their] results in the current work\"**. Therefore if the formula is incorrect, all the graphs (including the diamond graphs depicting accuracy vs memory peak utilization) are incorrect too. \n- When using layer groupings, for example for MobileNet ((1-3), (4-7), (8-13) groupings), **backpropagation might be used inside blocks**. While the authors say that non-gradient based techniques can be used for FF-based optimization, they need to clarify how the learning *within* each block has been achieved.\n- Please clarify why non-gradient based techniques (e.g. integer linear programming, evolutionary optimizers) can be used for FF-based optimization specifically.\n- Some quantitative comparisons are unfair (I detail below which ones).\n- No baseline experiments with the FF algorithm nor any FF variants are ever presented, and I think this is critically missing."
                },
                "questions": {
                    "value": "- Consider the following computational pathway for the data:\n```\n     G_1   G_2   G_3\n      \u2191     \u2191     \u2191\nx  \u2192 s_1 \u2192 s_2 \u2192 s_3\n      \u2191     \u2191     \u2191\n    w_1    w_2   w_3\n```\n\nFor simplicity, we omit the label pathway, non parametrized operations (e.g. normalization). For simplicity, let's assume that we consider a deep linear network such that $s_k = w_k \\cdot s_{k-1}$, $s_0=x$, $g_k = G(s_k, l_k)$ where $l_k$ denotes some label embedding. Computing the GIFF gradients amounts to compute $\\Delta w_k \\propto \\partial_{s_k} g_k \\cdot s_{k-1}^\\top$. Relating to your own terminology, the peak memory consumption for a layer $k$ takes into account $Param$ (to store $w_k$), $Grad$ (to store $\\Delta w_k$), $Act$ (to store $s_k$) and $Err$ (to store $\\partial_{s_k}g_k$). Below Eq. 9, you claim that \"the peak memory consumption for GIFF is lower since **GIFF can reuse the same memory space for gradients, activations, and errors of different layers**\". So this goes to say (if I understood correctly) that you can change *in-place* the memory allocated to neurons to store *either* activation, error signals or weight gradients.\n\nMy question is the following: could you please detail very clearly the computation pipeline (along with the memory units) for the simple example I proposed above to justify your claim and contrast it with backpropagation?\n\n- Please state clearly how you optimize the parameters **inside blocks**, when grouping multiple layers together. **Do you use backprop?**. \n\n- \"Table 2 shows that GIFF enables higher accuracy than FF (98.4% vs. 97.2%) using 279\u00d7 fewer parameters (2.82M vs. 10.1k) and 220\u00d7 smaller peak memory (15.3 MB vs. 73.4 kB)\". Here you compare GIFF **on a convnet** to FF **on a MLP**. Most of the memory savings merely come from using a convnet rather than using GIFF instead of FF. Please make a fair comparison based on the *same* architecture.\n\n- A comment on Table 2 : how is this possible that GIFF and FF use the exact same number of parameters, given that GIFF uses one extra parametrized layer *per* layer to compute the label embeddings?\n\n- You write multiple times in the paper that FF training on convnets is \"not supported\", is \"prevented\", \"FF does not work with any layers that use weight-sharing\". Although the statement is pretty assertive, the justification brought is a bit light and in fact there have been many studies investigating the FF algorithm on Convnets architectures, surely with a poor performance compared to backprop, but still with a decent accuracy (better than chance). I would really need to see convnets experiments with FF as well , **including most recent changes to the FF algorithm that could have improved its performance on convnets**, to be convinced that your approach truly solves this problem. Could you please explain why? \n\n- Your FF implementation, as well as GIFF, underperforms the FF implementation of this repo (https://github.com/loeweX/Forward-Forward)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698335230154,
            "cdate": 1698335230154,
            "tmdate": 1699636926126,
            "mdate": 1699636926126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "N5qmXw4j1G",
            "forum": "wUKVia7J10",
            "replyto": "wUKVia7J10",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_8xmW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_8xmW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generalized inference friendly forward-forward (GIFF) algorithm to address the limitations of the Forward-Forward (FF) algorithm by introducing a label branch, which can tackle inefficient inference passes and support conventional networks. The experiments show that GIFF achieves performs similar results with BP and use up to 43% less memory."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a label branch for FF, which can tackle inefficient inference passes and support conventional networks.\n\n- The proposed GIFF achieves performs similar results with BP and use less memory."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- The proposed GIFF is similar to a network with multiple losses at different layers. For example, GoogLeNet [r1] used the losses of the auxiliary classifiers\nto improve the accuracy of the classification. The main differences could be that GIFF only allows local (layer-wise) gradient.\n\n[r1] Going deeper with convolutions, 2014.\n\n- When applied to very deep networks, such as ResNet-8, the proposed method has to split it into several groups (2 groups in Figure A3). Within each group, it is unclear if the proposed method uses BP. If so, I do not think the proposed GIFF is a general FF method. Moreover, how to design networks heads (called Goodness in the paper? ) and losses in the lower layers? Is it very heavy? If we applied GIFF to each layer of a deep networks, is it contain lots of heavy heads?"
                },
                "questions": {
                    "value": "See [Weaknesses]"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654168081,
            "cdate": 1698654168081,
            "tmdate": 1699636926003,
            "mdate": 1699636926003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NURHvCcuWM",
            "forum": "wUKVia7J10",
            "replyto": "wUKVia7J10",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_QKxL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_QKxL"
            ],
            "content": {
                "summary": {
                    "value": "Forward-Forward (FF) is a recently proposed alternative to backpropagation (BP), which does not require propagation of a global error signal across layers.\nHowever, the authors identify 3 limitations of FF: it suffers from the label embedding problem, does not support convolutional layers, and is not efficient during inference for supervised learning tasks.\nThus, the authors propose Generalized Inference Friendly Forward-Forward (GIFF) to address these limitations.\nUnlike FF, GIFF uses a separate channel to pass the label through the network; by separating the data and label channels, GIFF does not constrain the type of layer used.\nThe authors empirically evaluate GIFF on the TinyML benchmark, and show that it is more computationally efficient than FF, and more memory efficient than BP while achievign similar test accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This paper is very well-written. It flows well, and the figures are clear and easy to follow.\n- The authors propose an approach that potentially improves the memory and computational efficiency of training neural networks. This is clearly an important research direction, particularly given the increasing climate change concerns of training large models."
                },
                "weaknesses": {
                    "value": "While this paper is well-written, I\u2019m not convinced on the novelty or importance of its contributions.\nThe authors claim that they alleviate 3 limitations of FF, namely the label embedding problem, lack of support for convolutional layers, and inefficiency during inference. But none of these appear to be real limitations of FF.\n- As far as I understand, the first 2 limitations both stem from the same problem, i.e. that for supervised learning, the original FF paper embeds the label as part of the input data.\nHowever, that was an ad-hoc solution Hinton made for the purpose of demonstrating how conceptually FF can be applied to supervised learning problems, rather than an actual design choice of FF.\nSince his setting was MNIST, and the border pixels of MNIST samples are not used to store useful information, he uses them to store the label, rather than code up a separate pipeline.\nBut there are no algorithmic limitations that prevent him from using a separate channel to store the labels; embedding them to the input was just a hacky solution for simplicity and convenience.\nThus, I don\u2019t agree that FF *requires* label embedding, or that label embedding is an issue that prevents it from being applied to other types of layers.\nTo support my point\u2014 the authors claim that \u201cGIFF can train ResNet and MobileNet models, which is not possible with FF,\u201d supposedly because of the label embedding issue.\nBut in the FF paper, Hinton states \u201cThere is clearly no problem adding skip-layer connections, but the simplest architecture is the easiest to understand and the aim of this paper is understanding, not performance on benchmarks.\u201d\n- For the 3rd limitation, the authors\u2019 claim that FF is inefficient during inference relies on the assumption that FF needs a separate inference pass for each class.\nBut the FF paper provides an alternative approach, which trades accuracy for efficiency: by initializing the input with a neutral label, it\u2019s possible to get the prediction with a single inference pass, just like GIFF.\nIf the authors want to claim computational efficiency, I think it\u2019s important to compare GIFF\u2019s performance to the efficient version of FF, rather than the inefficient version."
                },
                "questions": {
                    "value": "- I\u2019m happy to be proven wrong here, but it appears to me that the main reason you claim FF does not support convolutional layers is that it embeds the label in its input. Are there any other challenges that you had to overcome to implement convolutional layers that may be significant to you contributions, after moving the label to a separate channel?\n\n- Like FF, PEPITA (Dellaferrera & Kreiman, 2022) is another forward-only learning algorithm that utilizes 2 forward passes instead of a forward and backward pass.\nAs far as I\u2019m aware, like GIFF, PEPITA does not have the label embedding problem, supports convolutional layers, and does not require multiple inference passes for classification.\nGiven the similarities in the advantages these approaches offer, I\u2019d like to see the authors compare the two approaches in terms of design, performance, and limitations.\nIn other words, could you provide an argument of why we need GIFF given PEPITA exists?\n\n\nReferences:\n\n*Dellaferrera, G., & Kreiman, G. (2022, June). Error-driven input modulation: solving the credit assignment problem without a backward pass. In International Conference on Machine Learning (pp. 4937-4955). PMLR.*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7625/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7625/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7625/Reviewer_QKxL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806704206,
            "cdate": 1698806704206,
            "tmdate": 1699636925887,
            "mdate": 1699636925887,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "oGoeoSjS0l",
            "forum": "wUKVia7J10",
            "replyto": "wUKVia7J10",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_8ETu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7625/Reviewer_8ETu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GIFF to improve the existing FF algorithm that enables local optimization and continuous training on edge devices. It solves the update for convolutional layers and dependence on label embedding. However, the accuracy achieved is still obviously lower than the BP methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Proposes new methods to achieve optimization process without dependence on backward propagation.\n- Conduct experiments on three different tasks.\n- Consider the practical inference speed and complexity."
                },
                "weaknesses": {
                    "value": "- only conduct experiments on toy datasets.\n- the target accuracy value is lower than the value SOTA methods achieve."
                },
                "questions": {
                    "value": "- How about the method's effect on regression and generative methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7625/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836286018,
            "cdate": 1698836286018,
            "tmdate": 1699636925779,
            "mdate": 1699636925779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]