[
    {
        "title": "Structural Pruning of Large Language Models via Neural Architecture Search"
    },
    {
        "review": {
            "id": "CAghudiolv",
            "forum": "VAwgL8kPvr",
            "replyto": "VAwgL8kPvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_aZ9T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_aZ9T"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests integrating weight-sharing NAS for the compression of pre-trained language models. This approach consists of three components: a weight-sharing super-network trained using the sandwich rule in conjunction with an in-place knowledge distillation (KD) strategy; a sub-network selection based on the Pareto front; and a varied search space, extending from a larger scope with masks applied to each head/neuron to a smaller scale focusing solely on the quantity of heads, units, and layers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **NAS's Role in Compressing BERT**: A practical application for NAS is in the compression of BERT.\n2. **Method Advantages for Sub-Network Selection**: This technique facilitates a multi-objective search for choosing multiple sub-networks, unlike earlier methods which allowed only single-network pruning and selection at a time."
                },
                "weaknesses": {
                    "value": "1. **No experiments on LLM, but the topic of this paper is about LLM**. The title of the paper suggests a focus on Large Language Models , leading me to expect analyses or experiments involving LLMs like LLaMA, or at least T5-large, especially since the terms 'LLM' are predominantly used in the paper rather than 'language model' or 'pre-trained language model'. However, upon delving into the experimental section, it's surprising to find that the actual experiments exclusively involve **BERT**. There is no mention of LLMs in the experiment, nor is there any comparison or discussion on how the application of Neural Architecture Search might differ between LLMs and PLMs.\n\n2. **More Baselines are needed**. The authors appear to have selected some baselines that may be relatively less challenging to outperform, such as Retraining-Free Pruning and self-defined baselines. Retraining-Free Pruning, focusing on rapid compression of BERT without retraining, isn't directly comparable with the methods in this paper, which involve around 12 hours of training (as indicated by 50,000 seconds in Figure 5) on the MNLI dataset. Moreover, the proposed method underperforms DistillBERT in more than half of the datasets (including MRPC, COLA, SST2, QNLI, MNLI). Other baseline methods are self-created, and there's a notable omission of any recent advancements in structural pruning methods for BERT in the past five years (e.g., DynaBERT, CoFi). Comparison with those methods are needed. Additionally, the related work section only references four papers on structural pruning for BERT, whereas the authors could have expanded their literature scope by referring to a broader survey[1] on this topic.\n\n3. **Missing Important Comparison**. The paper misses an essential comparison with another study[2] that also employs NAS for compressing BERT. Given the relevance and slight methodological variation (NAS applied to pre-trained versus fine-tuned BERT) between the two studies, a comparison seems crucial. Both papers aim to achieve compression in downstream tasks, yet this paper lacks experimental evidence or analysis showing whether its method offers any advantages over the other one. This comparison is particularly pertinent since the approach to compression, whether on a pre-trained or fine-tuned model, could lead to different outcomes, and their exploration is essential for a comprehensive understanding.\n\n[1] Survey on Model Compression and Acceleration for Pretrained Language Models. \n[2] NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"
                },
                "questions": {
                    "value": "Could you clarify the process used to calculate the runtime for each technique as shown in Figure 5? The RFP (Retraining-Free Pruning) paper mentions that pruning takes merely 0.01 hour, approximately 36 seconds. However, in your study, this duration extends to about 20,000 seconds. What factors contribute to this substantial discrepancy in runtime measurements?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2340/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2340/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2340/Reviewer_aZ9T"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2340/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698381676198,
            "cdate": 1698381676198,
            "tmdate": 1699636166362,
            "mdate": 1699636166362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wn0cyYHSri",
                "forum": "VAwgL8kPvr",
                "replyto": "CAghudiolv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. \n\n1. To address their concerns, we updated our paper and replaced the term \u2018large language model\u2019 by \u2018pre-trained language model\u2019. Even though LLM with prompt-classification are increasingly popular, fine-tuning models such as BERT is still highly competitive on labeled datasets and often used in practice. We see the extension of our approach to LLM as a promising future work.\n2. We tried to benchmark against COFI using the official implementation but obtained inconclusive results compared to the original paper. We decided not to include these results yet to avoid an unfair comparison that would put COFI at a disadvantage. We reached out to the authors to understand this in detail and ensure a fair comparison;  if possible, will included the results of experiments for the camera ready version. However, we would like to stress that including additional structural pruning methods would not change the main argument of the paper: NAS allows for multi-objective optimization rather than constrained optimization to obtain the Pareto set of sub-networks. This captures the non-linear relationship between performance and compression ratio. For example, Figure 5 right shows that pruning the model on the MNLI by 50% leads to the same performance as pruning by 60%. Since we do not know this before, we either have to run the constraint optimization process (e.g. COFI or RFP) multiple times based on a grid of threshold values or stick to a sub-optimal solution based on an arbitrarily defined threshold. Multi-objective NAS on the other hand allows us to select the model without any additional overhead.\n3. Regarding the callout on comparison with distillation methods (NAS-BERT), we appreciate the feedback from the reviewer and included a more detailed discussion about the differences of distillation and pruning approaches in the related literature section. As we discuss in the related work section and also in the response to reviewer t4jT, in this setting, given a pre-trained model, the goal is to train a smaller model from scratch by distilling the knowledge of a larger pre-trained model. While distillation allows for flexible architectures, as is the case for NAS-BERT, its cost is in the same ballpark as pre-training the teacher model. In our case, we focus on pruning directly on the target task, which is the more appealing scenario for practitioners who do not have the compute to distill their own model and would rather fine-tune a pre-trained model. This makes it hard to perform a fair apple-to-apple comparison between these two approaches. In addition, the two approaches are complementary to each other, since we can always prune a distilled model. \n4. Question regarding runtime: For all methods we compute the total runtime to obtain a Pareto set of solutions. This includes also fine-tuning the original model for RFP and running it multiple times with different thresholds."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2340/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595612144,
                "cdate": 1700595612144,
                "tmdate": 1700595612144,
                "mdate": 1700595612144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CpeoswrgLR",
            "forum": "VAwgL8kPvr",
            "replyto": "VAwgL8kPvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_t4jT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_t4jT"
            ],
            "content": {
                "summary": {
                    "value": "The paper studys a important research direction for NAS: structural pruning using NAS. The paper explores weight-sharing based neural ar\u0002chitecture search (NAS) as a form of structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency. Authors valicate the effectiveness of the proposed method for fine-tuned BERT models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing of this paper is commendable as it is well-structured and easily comprehensible.  I believe that utilizing Neural Architecture Search (NAS) for pruning structured architectures is one of the crucial research directions in the field of NAS. This paper provides detailed experimental evidence of the effectiveness of their approach, particularly on the GLUE benchmark."
                },
                "weaknesses": {
                    "value": "One significant aspect that requires attention is the performance on the GLUE benchmark. It is worth considering an alternative branch of NAS, which involves directly searching for new architectures using distillation techniques for fine-tuned models such as AdaBERT, TinyBERT, and NAS-BERT. These methods have demonstrated the ability to achieve 50% pruning without any notable performance degradation and even achieve an impressive 80% reduction in parameters with minimal impact on performance. It would be beneficial to include and discuss these baselines in the paper. Moreover, it would be interesting to explore the potential combination of the proposed methods with these existing models and highlight the advantages of the proposed approach. I believe that incorporating these insightful discussions would greatly enhance the paper."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2340/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735988496,
            "cdate": 1698735988496,
            "tmdate": 1699636166275,
            "mdate": 1699636166275,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sldm8mKjsm",
                "forum": "VAwgL8kPvr",
                "replyto": "CpeoswrgLR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback and we are glad that the reviewer appreciated the experimental evidence of the effectiveness of our approach. We completely agree that compressing pre-trained language models is a crucial research direction for NAS. Indeed distillation techniques such as AdaBERT and TinyBERT are interesting alternatives to utilize NAS. However, there are several reasons why an experiment against distillation approaches would not be an apple-to-apple comparison: \n\n* We tackle the multi-objective setting where we obtain a Pareto set as a solution. Distillation on the other hand only provides a single solution. Providing a Pareto set of solutions is one of the key contributions of our work.\n* Distillation requires an expensive offline phase to train a single student model, which can then be fine-tuned on target tasks. Our pruning approach does incur an additional overhead compared to conventional fine-tuning on the target task, but omits the expensive distilation phase; pruning is substantially faster than distillation. Both approaches are valid but it depends on how many downstream tasks are considers (# tuning per experiment) to decide which approach is more useful. This makes it hard to compare the overall required compute time of both approaches. \n\nAs suggested by the reviewer, we added a detailed discussion of AdaBERT and TinyBERT to the related work section of our paper and expanded the discussion on differences between pruning and distillation methods. We already discuss NAS-BERT in our paper, which is orthogonal to our approach. Since our method is also independent of whether the network was pre-trained or distilled from another model, it is straightforward to combine it with methods such as NAS-BERT."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2340/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595482171,
                "cdate": 1700595482171,
                "tmdate": 1700595482171,
                "mdate": 1700595482171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qe1qqALEs8",
            "forum": "VAwgL8kPvr",
            "replyto": "VAwgL8kPvr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_2Az8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2340/Reviewer_2Az8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed weight-sharing-based NAS to compress fine-tuned pre-trained LLMs by slicing subnetworks. By utilizing a multi-objective approach, they can find the Pareto optimal set of architectures that balance model size and validation error. The NAS approach achieves up to 50% compression with less than 5% performance drop for a fine-tuned BERT model on 7 out of 8 text classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has detailed literature research and multiple baseline models for comparison and the selected topic is very important given that LLM is more and more important in our everyday life. Improving the LLM efficiency is critical.\n\nThe paper also provided in-depth ablation study."
                },
                "weaknesses": {
                    "value": "When looking the metrics, it seems the newly proposed NAS model mainly performs better when the model is pruned heavily. The model inference time is not the best when compared with other models."
                },
                "questions": {
                    "value": "In Figure 4, the graph shows: On 7 out of 8 dataset the new NAS strategy is able to prune 50% with less than 5% drop in performance (indicated by the dashed line) in performance. It seems more than 1 datasets dropped more than 5% when pruning 50% of parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2340/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813761231,
            "cdate": 1698813761231,
            "tmdate": 1699636166166,
            "mdate": 1699636166166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VWIWIpOvNy",
                "forum": "VAwgL8kPvr",
                "replyto": "qe1qqALEs8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2340/Authors"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful feedback and appreciate that they found the paper to be \u2018very interesting\u2019 and that they appreciate the in-depth ablation study of the paper. \n\n**Inference Time**: Could the reviewer clarify if they mean model inference time or the overall runtime of our method? We only report model inference time in Figure 11 in the appendix, where in fact the sub-networks found by our NAS approach are substantially faster than the quantized original network. Decreasing model inference time is one of the key benefits of our method and we use the network size as a proxy for infererence latency throughout the paper. We have added the following sentence in the results section to make it clear - note that parameter count is related to model inference time as discussed in Appendix.\n\nIn the case of runtime, our weight-sharing based NAS approach indeed induces some additional time overhead compared to just fine-tuning the model. However, we would like to point out that this time is significantly smaller (see Figure 5 left) than standard NAS or dropping layers. Our multi-objective approach returns a set of solutions such that the optimal compressed network can be selected post-hoc. This is in contrast to other structural pruning approaches, such as layer dropping or RFP, which require a pre-defined threshold on the remaining parameters. Since there is no simple relationship between pruning ratio (i.e., number of parameters left in the model) and performance, to find the optimal sub-network, we have to run these methods multiple times with different thresholds. Having said that, we do think there is room for further research to make weight-sharing based NAS more time efficient, for example by using non-uniform sampling of sub-networks during the fine-tuning of the super-network. \n\n**Question Figure 4**: We\u2019d like to ask the reviewer to clarify why it seems that more than 1 datasets dropped more than 5% when pruning 50% of parameters. Our experiments show that a performance drop higher than 5% was actually only observed on the COLA dataset."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2340/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595405893,
                "cdate": 1700595405893,
                "tmdate": 1700595405893,
                "mdate": 1700595405893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]