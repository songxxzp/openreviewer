[
    {
        "title": "IntentGPT: Few-Shot Intent Discovery with Large Language Models"
    },
    {
        "review": {
            "id": "SFnSN8tGbL",
            "forum": "2kvDzdC5rh",
            "replyto": "2kvDzdC5rh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_vjHF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_vjHF"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework for Intent Discovery by utilizing pre-trained LLM s like GPT 4 for few shot in context learning. The authors performed prompt engineering to guide LLMs to produce the desired generations. Experimental results show improvement over existing approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Good prompt engineering to guide GPT-4 to produce intents.\n\nIncorporation of few shot examples to improve performance.\n\nImproving performance without training overhead."
                },
                "weaknesses": {
                    "value": "Tested on only 2 datasets; difficult to not judge robustness of the proposed method without seeing results on different datasets.\n\nPrompt and few shot example usage might change depending on the data.\n\nKnown intent feedback might not be usable for zero shot context."
                },
                "questions": {
                    "value": "Please perform experiments on more datasets that have diverse input structure. (Many datasets are out there: https://paperswithcode.com/task/intent-detection)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6364/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587699627,
            "cdate": 1698587699627,
            "tmdate": 1699636702895,
            "mdate": 1699636702895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JKkVaRJUlL",
                "forum": "2kvDzdC5rh",
                "replyto": "SFnSN8tGbL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's time to read our paper and provide feedback. We are glad that the reviewer found the paper to provide good guidelines for how to prompt GPT-4 and discover intents without any training overhead. We respond to the comments in detail below.\n\n> Tested on only 2 datasets. Please perform experiments on more datasets that have diverse input structure.\n\nWe picked those two datasets for our paper because they're widely recognized as the standard benchmarks in open-set intent discovery [3, 4, 5], which makes it fair to compare our work to existing methods. However, the comment inspired us to extend our results to three new datasets. First, as was highlighted by reviewer kRkh, we have run our method on a multilingual dataset (MTOP) [6] and presented the results in our response to them. Please see our comment there for results on the MTOP dataset. To further strengthen our results we consider two additional datasets, namely SNIPS [1], and StackOverflow [2]. These datasets can be interesting because they show a much reduced number of categories, hence will allow us to judge the robustness of our model in a regime with a small number of categories. The results are shown as follows:\n\n\n| Model                | KIR | NMI   | ACC   | ARI   |\n|----------------------|-----|-------|-------|-------|\n| **SNIPS dataset**     |     |       |       |       |\n| DSSCC     | 0.75 | 90.44 | 94.87 | 89.03 |\n| CDAC+     | 0.75 | 89.30 | 93.63 | 86.82 |\n| IntentGPT3.5 (10 shot) (ours)    | 0.75| 85.26 | 89.14 | 80.48  |\n| IntentGPT-4 (10 shot) (ours)       | 0.75| **91.42** | **94.89** | **90.63**  |\n| **StackOverflow dataset**     |     |       |       |       |\n| DSSCC     | 0.75 | 77.08|**82.65**|68.67  |\n| CDAC+     | 0.75 | 69.84|73.48|52.59  |\n| IntentGPT3.5 (10 shot) (ours)      | 0.75| 80.56|81.69|71.57  |\n| IntentGPT-4 (10 shot)   (ours)     | 0.75| **81.78**|81.75|**76.85**  |\n\nWe compare IntentGPT-3.5 and IntentGPT-4 against available public results on two of the baselines presented in the paper, namely DSSCC [7] and CDAC+ [8]. Our model outperforms these methods except DSSCC on StackOverflow, which performs better for the ACC metric. These results required no human intervention in modifying the prompts, or training, showcasing the robustness of IntentGPT.\n\n> Prompt and few shot example usage might change depending on the data.\n\nWhile the IntentGPT prompt and few-shot examples might change across datasets our model is able to automatically adapt the prompts and in-context examples to a given dataset, hence generating more informative and valuable prompts. Furthermore, it does not require human effort when porting the model into a new dataset, as it is fully automated for generating the needed prompts given the new dataset, capturing the relevant nuances about the data domain. \n\n> Known intent feedback might not be usable for zero shot context.\n\nOur Known Intent Feedback technique is indeed usable in a zero-shot setting. In this setting, we inject the discovered intents back into the prompt, starting from an empty database. In fact, we have experiments available on the paper for the zero-shot setting case. We show the results of our method in Table 1 on the paper (see results for the unsupervised setting), and also in the first row of ablations in Table 2, for both GPT3.5 and GPT-4 on the two datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715155911,
                "cdate": 1700715155911,
                "tmdate": 1700715203722,
                "mdate": 1700715203722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "efWsDAgP6u",
            "forum": "2kvDzdC5rh",
            "replyto": "2kvDzdC5rh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_kRkh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_kRkh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes IntentGPT, a framework for few-shot intent discovery using large language models (LLMs) like GPT-3.5, GPT-4, and Llama-2."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Eliminates the need for training and extensive domain-specific data by leveraging the knowledge and generalization capabilities of LLMs through in-context learning. \n\n2. Introduces an automatic prompt generation module that leverages examples from the training set to create high-quality prompts tailored for intent discovery.\n\n3. Proposes techniques like Semantic Few-Shot Sampling to retrieve relevant examples and Known Intent Feedback to reuse discovered intents. \n\n4. Shows competitive performance compared to prior state-of-the-art semi-supervised methods on intent discovery benchmarks like CLINC and BANKING.\n\n5. Provides comprehensive analysis on the influence of various hyperparameters for few-shot in-context learning using frozen LLMs."
                },
                "weaknesses": {
                    "value": "1. The core idea of using LLMs for few-shot learning, while powerful, is not highly creative or novel at this point given extensive prior work on leveraging LLMs. The techniques like prompt engineering and few-shot sampling, while optimized for intent discovery, mostly draw from existing approaches for adapting LLMs.\n\n2. Is there a suspicion of cheating in the KIF mechanism? Is there any unfairness compared to traditional deep learning paradigms?\n\n3. The research is limited to English datasets, however, large language models can be easily extended to other languages, so it is necessary to test the performance of IntentGPT in other languages. I think the author may lack this necessary comparison."
                },
                "questions": {
                    "value": "See Above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6364/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764866555,
            "cdate": 1698764866555,
            "tmdate": 1699636702741,
            "mdate": 1699636702741,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bJ1b4tETzP",
                "forum": "2kvDzdC5rh",
                "replyto": "efWsDAgP6u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback. We are happy to see that the reviewer sees benefits to our Known Intent Feedback (KIF) mechanism and that we provide a comprehensive analysis of model hyper-parameters. We respond to the reviewer\u2019s comments in detail below.\n\n> The idea of using LLMs for few-shot learning is not highly creative or novel\n\nWhile LLM-based few-shot learning has received significant attention in recent literature[1, 2], the challenge of generating effective prompts for LLMs remains unsolved [3]. We recognized an opportunity to improve existing methods by proposing two valuable contributions. First, we introduce the use of an In-Context Prompt Generator (ICPG) LLM that can generate dataset-specific and informative prompts (ICP). Second, we introduce the technique of Known Intent Feedback (KIF) that can be extended for the general tasks of open-world class discovery. For the task of Intent Discovery, we see that these techniques can boost performance for our method, outperforming previous baselines using substantially less data, no training overhead, and minimal human effort.\n\n> Is Known Intent Feedback (KIF) unfair to previous deep learning approaches?\n\nWe respectfully disagree.  KIF is not unfair compared to previous methods as it does not provide our approach with unfair information. KIF works as a regularization in that it restricts the amount of discovered classes by making sure they are separate enough from each other. \n\n> The research is limited to English datasets. Can IntentGPT work with multilingual datasets?\n\nWe chose the English datasets since they seem to be the standard benchmarks in recent papers for the open-set intent discovery task [4, 5, 6].  However, your comment inspired us to test this method in a different language. IntentGPT can be seamlessly used in multiple languages supported by the underlying language model. Therefore,  we ran our method on the MTOP [7] dataset which consists of six languages: English, German, French, Spanish, Hindi, and Thai, and the results are shown below.\n\nTable: Results on MTOP dataset\n\n| Model                | KIR | NMI   | ACC   | ARI   |\n|----------------------|-----|-------|-------|-------|\n| **MTOP English**     |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 83.54 | 76.47 | 80.43 |\n| GPT-4 (10 shot)      | 0.75| 87.98 | 82.22 | 85.09 |\n| **MTOP Spanish**     |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 88.21 | 82.42 | 88.74 |\n| GPT-4 (10 shot)      | 0.75| 90.37 | 85.69 | 90.90 |\n| **MTOP French**      |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 88.97 | 83.43 | 88.44 |\n| GPT-4 (10 shot)      | 0.75| 84.34 | 76.69 | 80.62 |\n| **MTOP German**      |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 80.44 | 67.26 | 65.82 |\n| GPT-4 (10 shot)      | 0.75| 86.64 | 77.54 | 81.87 |\n| **MTOP Hindi**       |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 60.43 | 43.33 | 36.2  |\n| GPT-4 (10 shot)      | 0.75| 73.23 | 66.01 | 66.36 |\n| **MTOP Thai**        |     |       |       |       |\n| GPT3.5 (10 shot)     | 0.75| 28.38 | 20.66 | 5.66  |\n| GPT-4 (10 shot)      | 0.75| 23.72 | 17.67 | 7.42  |\n\nWe observe that IntentGPT is able to generalize to languages other than English without requiring extra human effort. The first LLM in charge of generating the prompt (ICPG) seems to perform well at automatically generating the prompt in the desired language (see next comment to observe the prompts generated by ICP in Spanish, French, and Hindi). \n\nFurthermore, quantitative results in the task of Intent Discovery displayed in the table show that our method works well in this multilingual setting, showing metrics in a similar range to the ones seen in other benchmarks, except for the case of Thai, where the GPT3.5 and GPT4 models struggle to handle. We leave as future work the comparison of IntentGPT with existing methods in this benchmark since existing methods do not report results on these datasets. We believe that these results reinforce the idea that IntentGPT is a robust approach for open-world class discovery tasks, and the novelty of the ICP and few-shot sampling techniques proposed. We have incorporated this new setting in the Appendix, including details on the automatically generated prompts."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709402837,
                "cdate": 1700709402837,
                "tmdate": 1700709402837,
                "mdate": 1700709402837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bqFn9BfJ8l",
                "forum": "2kvDzdC5rh",
                "replyto": "efWsDAgP6u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems (NeurIPS), 2020.\n\n[2] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\n\n[3] Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto, Karthik Raman, and Michael Bendersky. Ambiguity-aware in-context learning with large language models, 2023.\n\n[4] Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. Discovering new intents with deep aligned clustering. In AAAI Conference on Artificial Intelligence, 2021b.\n\n[5] Rajat Kumar, Mayur Patidar, Vaibhav Varshney, Lovekesh Vig, and Gautam Shroff. Intent detection and discovery from user logs via deep semi-supervised contrastive clustering. In PConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022.\n\n[6] Xiang Shen, Yinge Sun, Yao Zhang, and Mani Najmabadi. Semi-supervised intent discovery with contrastive learning. In Workshop on Natural Language Processing for Conversational AI, 2021.\n\n[7] Li, Haoran, et al. \"MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark.\" arXiv preprint arXiv:2008.09335 (2020)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709516319,
                "cdate": 1700709516319,
                "tmdate": 1700709542728,
                "mdate": 1700709542728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Eo6ZkYCH5",
            "forum": "2kvDzdC5rh",
            "replyto": "2kvDzdC5rh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_G7hj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_G7hj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a system of GPTs, called IntentGPT, in order to discover new intents in an open-world recognition system. The method of the paper consists of an LLM (or two) with a prompt to design an in-context learning prompt to do intent identification from utterances, with a special provision for labeling new intents that it has not seen before. This prompt is then augmented by including semantically relevant examples of labeled utterances to the test utterances and a list of possible intents and then handed to an LLM to perform the intent identification. The paper evaluates its method on two different benchmarks, with three different LLMs, and shows competitive, and often better, performance as compared to both semi-supervised and full unsupervised techniques for intent discovery."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has good empirical validation is clear in its explanations and figures and is attacking a difficult and significant problem.  The paper does a thorough empirical validation by comparing the multiple LLM models, including an open-source, one. This highlights that the proposed methodology is actually what is working for the task at hand. The paper also shows its proposed methods' performance relative to state-of-the-art methods which thoroughly validates the methods' effectiveness. Overall, given the context and high levels of linguistics understanding required for such a task as open-world intent discovery, the use of LLMs is a solid approach.\n\nThe paper also proactively attempts to answer questions that arise when using a GPT model, such as whether the model has seen the training data before in its pre-training."
                },
                "weaknesses": {
                    "value": "The paper is missing some grounding in previous literature and it's not clear what the value of all of the components of its proposed methodology is. First, the whole component of Semantic Few-shot Sampling (SFS) reads to me as just being Retrieval Augmented Generation or RAG. See https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/ for a description. The SFS component then is not a novel contribution and should be cited as RAG. \n\nSecond, it's not clear from the write-up what the value of the in-context prompt generator (ICPG) step is. The authors mention  that they do an ablation study with this module but do not provide what the alternative prompt is besides \u201cUtilizing the automatically generated In-Context Prompt (ICP) by IntentGPT proves superior to using a \\textit{simple prompt description}.\u201d From looking at the examples provided in the appendix, the crafted prompt looks like what I would have given the LLM as a base prompt for this task, so I am not convinced that the ICPG step is producing better prompts than what a human would give.\n\nThird, and this is more of a minor criticism and possibly something for future work, but why not test other prompting schemes? For example, especially given the ICPG step, why not tree a Chain of Thought style of prompt for this task? Doing such a prompting scheme combined with the ICL might deliver even better results."
                },
                "questions": {
                    "value": "There are a couple of questions about the manuscript, some of which have already been detailed earlier in this review.\n\n-\tWhat is the \u201csimple prompt description\u201d that was done in the ablation study of the ICPG module?\n-\tWhen doing empirical testing, since this is meant to be used in an open-world setting, was the inclusion of test examples that express no intent or multiple intents considered? If so, how did this method handle those cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6364/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767174140,
            "cdate": 1698767174140,
            "tmdate": 1699636702578,
            "mdate": 1699636702578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1T05DTaZoe",
                "forum": "2kvDzdC5rh",
                "replyto": "7Eo6ZkYCH5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for reviewing our paper. We are encouraged to read that the reviewer finds our paper is attacking a difficult problem, and our methodology performs well compared to previous baselines. We now respond to their questions below.\n\n> The Semantic Few-shot Sampling (SFS) component reads to me as Retrieval Augmented Generation or RAG, then is not a novel contribution.\n\nThe main difference is that Retrieval Augmented Generation (RAG) has been mainly explored in knowledge-intensive tasks like Question Answering [1], where factual information is retrieved to ground the generation, while we use it to extract few-shot examples, a much less explored problem [2, 3]. Therefore, we would rather consider the SFS and SKIF components as minor contributions to the paradigm of few-shot learning building upon the RAG model. We have updated our paper with this discussion, citing relevant work on RAG.\n\n[1] Lewis, Patrick, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" Advances in Neural Information Processing Systems 33 (2020): 9459-9474.\n\n[2] Liu, Jiachang, et al. \"What Makes Good In-Context Examples for GPT-$3 $?.\" arXiv preprint arXiv:2101.06804 (2021).\n\n[3] Izacard, Gautier, et al. \"Few-shot learning with retrieval augmented language models.\" arXiv preprint arXiv:2208.03299 (2022).\n\n> Why not test other prompting schemes like Chain of Thought?\n\nChain of Thought is an appealing research avenue in extending our research. Previous works that use Chain of Thought [1, 2], improve in-context learning capabilities by performing two forward passes: one for generating a rationale of the task and a second to perform the task using that rationale. We perform a similar approach by first using an LLM to generate an informed and domain-specific prompt, and then use it to perform the task with a second LLM. We coincide with the reviewer that exploiting this prompting scheme could further improve results. We have incorporated this direction for future work in the revised paper. \n\n[1] Wang, Xuezhi, et al. \"Self-consistency improves chain of thought reasoning in language models.\" arXiv preprint arXiv:2203.11171 (2022).\n\n[2] Lee, Harrison, et al. \"Rlaif: Scaling reinforcement learning from human feedback with ai feedback.\" arXiv preprint arXiv:2309.00267 (2023).\n\n > I am not convinced that the ICPG step is producing better prompts than what a human would give.\n\nAs seen in the generated prompts from the ICPG module (see Appendix A.2, Prompts 2 and 3), it creates new prompts with rich information about the dataset at hand, which is different from the \u201cvanilla prompt\u201d that a human can create (like the one seen in Appendix A.2. Prompt 1). As shown in responses to reviewers GqJc and NWU5 about the ablation studies depicted in Table 2,  the ICPG module is beneficial for obtaining performance boosts. Finally, let us show how the ICPG module creates better prompts than humans by testing it in a multi-lingual dataset, where a human might not know all the languages to manually create prompts. We have extended our experiments to the MTOP [1] dataset, and the results depict how IntentGPT's ICPG module generates a new prompt in the desired language automatically (see response to reviewer kRkh). \n\n[1] Li, Haoran, et al. \"MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark.\" arXiv preprint arXiv:2008.09335 (2020).\n\n> What is the \u201csimple prompt description\u201d that was done in the ablation study of the ICPG module?\n\nThe simple prompt description is included in Appendix A.2 (Prompt 5: E. Basic human-generated task description) to act as a baseline for our ablation analysis.\n\n> Did you consider the inclusion of test examples that express no intent or multiple intents?\n\nIn our experiments, we did not test instances that may not have an intent or have multiple intents associated. Nevertheless, to handle multiple intents, our method could be easily extended to handle multiple or no intents, by adjusting the original prompt description feed to the In-Context Prompt Generator and formatting the examples to support multiple intents in a single response. However, there are no public benchmarks for multiple intent or no intent, but we consider this as a very interesting task for future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719756456,
                "cdate": 1700719756456,
                "tmdate": 1700719786554,
                "mdate": 1700719786554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qkVOHn2WgE",
            "forum": "2kvDzdC5rh",
            "replyto": "2kvDzdC5rh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_NWU5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_NWU5"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates the power of pre-trained LLM like GPT to both detect known dialogue intents and more importantly discover intents not previously known. The paper shows how the in-context prompt of an LLM can be crafted using examples from known intents, and how certain prompt design decisions affect the performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tThis is a well-written paper, well-organized, and clear to read and follow.\n2.\tThere is a substantive literature review which is well presented, clearly comparing each work to the paper.\n3.\tThere are substantive appendices that are very helpful to the reader.\n4.\tThroughout all of it, there is obvious diligence, attention to detail, and care for the presentation.\n5.\tThis work could be useful to anyone new to the area of Intent Discovery, and in particular applying LLMs to this problem. \na.\tThis is not in conflict with the low contribution score above since the contribution question relates to research innovation value and the ICLR audience expectations - see below for more on this point."
                },
                "weaknesses": {
                    "value": "The first LLM does not appear to contribute any value to the model, as discussed below. If this criticism is correct, then this work becomes more of an application paper, a study of applying off-the-shelf LLM to a known problem, showing how readily available new technology outperforms previous methods while significantly increasing simplicity and flexibility. While valuable, it is probably not a good match for this conference/track.\n\nThe role of the first LLM (LLM1) in the model is to create the prompt for the second LLM (LLM2). LLM1 is given an original prompt created by a human and asked to craft an optimized prompt to be used with LLM2. This can be an effective method to optimize LLM performance, chaining LLMs like this. (The reviewer uses this technique quite a bit). However, LLM1 would usually use its intelligence to create something better, with a clear added value, compared to the original input. Often, LLM1 considers specialized data in this process that is later not observed by LLM2. None of this seems to be the case in the proposed model. The list below shows a complete generated prompt by LLM1 (from Appendix 2), and the original prompt given to LLM1 that was the basis for its generation. It is obvious that the generated prompt is at most a slight rewording of the original. Next, the auxiliary data (few shot examples, known intents) given to LLM1 are basically just passed to LLM2, i.e. they could just as well be given directly to it. In other words, nothing suggests that the performance would not be equivalent if the model just used one LLM, with the original prompt and the auxiliary data (few shot examples, known intents) added to it. At the very least, the paper should compare to this scenario. The ablation study result for ICP, if I understand it correctly, compares to using a \u201csimple prompt\u201d like Prompt 5.E. in Appendix 2 \u2013 which lacks the key addition of the auxiliary data (which are very informative to an LLM like GPT), and is therefore not a true comparison of using one vs two LLMs. \n\nLLM1 Generated Prompt: \u201cAs an AI language model, your task is to assign the correct intent to a given textual utterance.\u201d\t\nOriginal Prompt: \u201cYou are a helpful assistant and \u2026  You specialize in \u2026 the task of assigning textual utterances to specific intents\u201d\n\nLLM1 Generated Prompt: \u201cThe intent can be one of the pre-defined intents or a new one that you create based on the context and knowledge about the problem and specific data domain.\u201d\t\nOriginal Prompt: \u201c\u2026 some of which are pre-defined and others are not and have to be created\u201d\u2026 \u201csufficient context and knowledge about the problem and specific data domain.\u201d\n\nLLM1 Generated Prompt: \u201cYou should never assign an utterance to \u2019unknown\u2019.\u201d\t\nOriginal Prompt: \u201cnever assign a utterance to \u2019unknown\u2019\u201d\n\nLLM1 Generated Prompt: \u201cFor each utterance, analyze the context and the specific request or action implied.\u201d\t\nOriginal Prompt: \u201c\u2026acquire sufficient context\u2026\u201d\n\nLLM1 Generated Prompt: \u201cIf the utterance matches a known intent, assign it to that intent. If it doesn\u2019t match any known intent, create a new intent that accurately represents the request or action implied by the utterance.\u201d\t\nOriginal Prompt: \u201cyou need to be aware of the known intents and reuse them as much as possible, but need to create new intents when there are not known intents that fit the given utterance\u201d\n\nLLM1 Generated Prompt: \u201cRemember, the goal is to understand the user\u2019s intent as accurately as possible.\u201d\t\nOriginal Prompt: \u201cmaximizing the model\u2019s performance in the task\u201d\n\nLLM1 Generated Prompt: \u201cBe aware of the known intents and reuse them as much as possible, but don\u2019t hesitate to create new intents when necessary.\u201d\t\nOriginal Prompt: \u201cbe aware of the known intents and reuse them as much as possible, but need to create new intents\u201d\n\nThe other generated prompt example in Appendix 2 (Prompt 3.C) is basically equal to the original as above, with the exception of the following fragment: \u201cThe utterances can be questions, statements, or requests related to banking services like card transactions, account top-ups, refunds, identity verification, card delivery, transfer fees, and more.\u201d \u2013 But this is just a summary of the auxiliary example data and a single GPT would have it by default by having the auxiliary example data itself.\n\nA couple of smaller points:\n1.\tFor the \u201cSemantic Few-Shot Sampling\u201d the proposed model uses SentenceBERT to do the embeddings, which is a strange choice given that better quality embeddings can be attained with OpenAI\u2019s GPT3+ models, which are already used in this work in other ways.\n2.\tOn page 8, \u201cthat while SKIF\u201d should probably be \u201cthat SKIF\u201d"
                },
                "questions": {
                    "value": "Would you be able to run a comparison to a one-LLM setup as described above, i.e. by using the \u201cOriginal Prompt\u201d and adding the auxiliary data directly to it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6364/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812560407,
            "cdate": 1698812560407,
            "tmdate": 1699636702435,
            "mdate": 1699636702435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Lp450j0dr",
                "forum": "2kvDzdC5rh",
                "replyto": "qkVOHn2WgE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper very carefully. We are pleased to find that the reviewer feels the presentation of the paper and literature review were quite good, and the substantial appendices included are helpful for the reader. We now address their concerns regarding the contribution of our work.\n\n> Does the first LLM, In-Context Prompt (ICP), contribute to any value to the model?\n\nYes, the first LLM, In-Context Prompt Generator (ICPG), adds value to our model. The setup described by the reviewer is present in the paper. Please see our ablation studies, detailed in Table 2, which shows a comparison between the use of original prompts with auxiliary data (a single LLM with few-shot (FS) and Known Intent Feedback (KIF)) against our ICP-enhanced prompts. \n\nSpecifically, Rows 2, 3, and 4 of Table 2 of the paper (for both GPT3.5 and GPT4 versions) show the baseline performance using a single LLM, while Rows 5 and 6 highlight the improvements with the ICP module activated. The results show a consistent performance gain of 1-2 points across both datasets and various GPT models, underscoring the efficacy of our approach. \n\nAdditionally, thanks to a new experiment run on a multi-language dataset suggested by reviewer kRkh, we can show how the first LLM is able to handle the language adaptation without human effort, only relying on a few context examples from the train set. We find this case relevant for highlighting the contribution of ICP.\n\n> The choice of SentenceBERT to compute embeddings is a strange choice. Did you try OpenAI\u2019s GPT3+ models?\n\nWe tested text-embedding-ada-002 embeddings and we observed that they perform worse than SentenceBert (see table below). The most likely reason for OpenAI embeddings being worse is that it is primarily used to encode large contexts like documents for document retrieval, whereas SentenceBert is trained to efficiently encode short contexts, or sentences, like the size of our intents. We have incorporated this result in the Appendix of the paper.\n\n| Model                | Embeddings | KIR | NMI   | ACC   | ARI   |\n|----------------------|-----|-----|-------|-------|-------|\n|               |                    **CLINC dataset**     |           |       |       |       |\n| GPT-3.5 (10 shot)     |  text-embedding-ada-002   | 0.75| 88.64| 71.36| 61.42 |\n| GPT-3.5 (10 shot)     |   SentenceBert   | 0.75| 91.66| 78.99| 71.58 |\n| GPT-4 (10 shot)     |  text-embedding-ada-002   | 0.75| 90.73| 74.89| 65.99 |\n| GPT-4 (10 shot)     |   SentenceBert   | 0.75| 94.01| 83.96| 77.17 |\n|              |                   **BANKING dataset**     |     |     |     |       |       |       |\n| GPT-3.5 (10 shot)     |   text-embedding-ada-002  | 0.75| 69.46| 52.12| 39.51 |\n| GPT-3.5 (10 shot)      |  SentenceBert   |  0.75| 88.21 | 82.42 | 88.74 |\n| GPT-4 (10 shot)     |   text-embedding-ada-002  | 0.75| 88.64| 71.36| 61.42 |\n| GPT-4 (10 shot)      |  SentenceBert   |  0.75| 91.66| 78.99| 71.58 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712184853,
                "cdate": 1700712184853,
                "tmdate": 1700712184853,
                "mdate": 1700712184853,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2s6vdGSGVI",
            "forum": "2kvDzdC5rh",
            "replyto": "2kvDzdC5rh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_GqJc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6364/Reviewer_GqJc"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of intent classification/clustering and discovery using in-context learning and LLMs. It proposes 1) A method to discover useful prompts for the problem using an LLM 2) Use those prompts with few shot examples (based on semantic similarity) to classify utterances and discover new intents. The latter step is executed iteratively which leads to a growing database of novel intents."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is written well and cites the relevant literature\n2. Intent classification and discovery is an interesting problem on its own\n3. It also has many potential useful downstream applications \n4. The results improve upon prior baselines"
                },
                "weaknesses": {
                    "value": "While studying LLMs and their performance is an important endeavour, I am not convinced by the novelty of the proposed method or the efficacy of the individual components. For instance,\n\n1. The first part of the pipeline (ICP) seems to add little value. From row 3 vs row 5 in bottom panel of Table 2, it seems that on average ICP led to no performance boost.\n2. The semantic few shot sampling (SFS) also seems to add limited value (row 3 vs row 4)\n3. Intutitively \"Feedback\" should help, but I am not sure how is performance affected (see question below).\n4. Is row 1 (i.e. no tickmarks) of Table 2 (top and bottom panels) the right baseline to compare with? Based on the appendix it seems that for row 1, the KIR ratio is 0 (since the prompt contains no information about the space of labels) while the remaining rows presumably have a KIR of 0.75.\n5. How does 50-shot vanilla GPT4 with KIR = 0.75 perform? Is that closer to row 3 of table 2 or row 0? \n \nAs mentioned in the appendix, GPT4 was trained on an unknown data distribution. The authors point out, that it clearly knows about the dataset. The arguments presented in the appendix about GPT4 not having seen the test/train split are unconvincing. I am not sure what a Frechet distance of 0.54 means without a control, but Table 5 seems to suggest that for 5 out of 12 categories, the predicted intent exactly matches the ground intent. Is that evidence that GPT4 knows a lot more than just the name of the datasets?\n\nIs the fact the the optimal clusters (155 and 88) are so close to the ground truth numbers (150 and 70) an indication of dataset leakage or is it just an inevitable side effect of the evaluation criteria?"
                },
                "questions": {
                    "value": "1. How does performance change if only the feedback part is ablated (i.e. the database of intents is constant) from the entire pipeline?\n2. It would be nice to have a longer version on Appendix A.7 with more examples and details about the statistics of discovered intents. How close are they semantically/syntactically to the ground truth labels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6364/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698980584380,
            "cdate": 1698980584380,
            "tmdate": 1699636702322,
            "mdate": 1699636702322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ldZZiMTGj",
                "forum": "2kvDzdC5rh",
                "replyto": "2s6vdGSGVI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time spent reviewing the paper. We are encouraged to read that you see our method has useful downstream application potential and outperforms existing baselines. We now respond to their comments in detail below.\n\n> The In-Context Prompt (ICP) seems to add little value. From row 3 vs row 5 in the bottom panel of Table 2, it seems that on average ICP led to no performance boost.\n\nWhile the difference between rows 3 vs row 5 does not lead to a significant average performance boost, the scores between rows 4 and row 6 when adding ICP (both using SFS) indicate that it boosts performance. Notably, comparing rows 2, 3, and 4 (which do not use ICP) with rows 5 and 6 (which use the ICP), as a whole, shows a consistent improvement between 1-2 points in all the settings (across datasets and GPT models), except for the case of ARI on CLINC dataset for GPT3.5, where results remain similar (see also responses to reviewers NWU5 and kRkh on this matter). As this is an ablation study, a component may not boost performance in all scenarios but when combined with the other parts of our pipeline we show that ICP does help. ICP aims to redirect the prompt towards a specific domain without human labor, offering a boost in performance as seen by our ablations.\n\n> Is row 1 of Table 2  the right baseline to compare with? Iit seems that for row 1, the KIR ratio is 0 while the remaining rows have a KIR of 0.75.\n\nAs noted by the reviewer, row 1 in the ablation studies from Table 2 displays the zero-shot setting, which implies a KIR of 0, while others use a KIR of 0.75. While this comparison may be not well-suited for judging performance on the main metrics (i.e. NMI, ACC, ARI) it is interesting to observe how the Number of Discovered Intents (NDI) explodes. This is because this simple setting performs as a zero-shot, and there\u2019s no regularization of the label space introduced by the Known Intent Feedback technique. Additionally, this setting shows how other metrics downgrade because of it. We have updated the paper by specifying that this row shows a zero-shot setting, and clarifying that a more fair comparison in terms of Known Intent Ratio can be done starting from rows 2-7. \n\n> How does 50-shot vanilla GPT4 with KIR = 0.75 perform? Is that closer to row 3 of table 2 or row 0?\n\nA similar question was asked by reviewer NWU5, where they asked for 10-shot vanilla GPT4 with KIR = 0.75. This ablation is shown in rows 2, 3, and 4 of Table 2 in the paper, which presents the performance boosts of including the ICP module when comparing against rows 4 and 6 (Please see our comment to reviewer NWU5 highlighting the results). Further, to answer your question, we also ran an experiment with 50-shot vanilla GPT4 and KIR=0.75. We present the results as follows:\n\n\n| Model                 | KIR | NMI   | ACC   | ARI   |\n|----------------------|-----|-------|-------|-------|\n|               |                    **CLINC dataset**     |           |       |       |       |\n| GPT-3.5 (50 shot)       | 0.75| 90.38|75.11|67.43 |\n| GPT-4 (50 shot)       | 0.75| 92.59|81.38|71.43 |\n|              |                   **BANKING dataset**     |     |     |     |       |       |       |\n| GPT-3.5 (50 shot)     |   0.75| 77.29|60.85|45.9  |\n| GPT-4 (50 shot)     |    0.75| 81.72|67.76|55.7 |\n\nComparing this result with Table 2, this setting would perform similarly to rows 2 and 3, and far from the top positions. Even though this setting uses a lot of few-shot samples, it is not able to get better results because of the vanilla prompt. Note that when adding the ICP to the 50-shot setting, we obtain our best results, with an NMI=96.06 on CLINC, showing the contribution of having the ICPG component."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724006806,
                "cdate": 1700724006806,
                "tmdate": 1700724006806,
                "mdate": 1700724006806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GDPsr3GE63",
                "forum": "2kvDzdC5rh",
                "replyto": "2s6vdGSGVI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6364/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> How does performance change if only the feedback part is ablated (i.e. the database of intents is constant) from the entire pipeline?\n\nWe have run this experiment and present the results below. \n\n| Model                 | KIR | NMI   | ACC   | ARI   |\n|----------------------|-----|-------|-------|-------|\n|               |                    **CLINC dataset**     |           |       |       |       |\n| GPT-3.5 (10 shot)       | 0.75| 87.62|57.71|54.77 |\n| GPT-4 (10 shot)       | 0.75| 87.76|59.69|56.57 |\n|              |                   **BANKING dataset**     |     |     |     |       |       |       |\n| GPT-3.5 (10 shot)     |   0.75| 77.98|40.99|35.87  |\n| GPT-4 (10 shot)     |    0.75| 76.54|37.27|32.66 |\n\nHere, we study the impact of maintaining a constant intent database, only composed of the known intents at initialization, and not growing the database during the evaluation. We observe a degradation of the performance in all metrics, compared to rows 2-7 in the ablation table (consider these rows also in GPT-4). However, this setting outperforms the baseline shown in row 1 of the ablation, as we include the mechanism of injecting Intents into the prompt. We also display NDI, which explodes due to the lack of intent reuse. We have added the findings from this additional experiment to our ablation study in the paper.\n\n> The optimal clusters (155 and 88) are so close to the ground truth (150 and 70). Is this an indication of dataset leakage?\n\nWe do not use GPT-4 or a trained model to determine the optimal number of clusters. We utilize DBSCAN on the predicted intent embeddings to determine the optimal K for k-means clustering. Therefore, there can not be dataset leakage.\n\n> Sometimes the predicted intents match exactly the ground intent. Is that evidence that GPT4 knows a lot more than just the name of the datasets?\n\nWe thank the reviewer for taking the time to think about this problem in our Intent Discovery setup. We find that this concern is valid. Therefore, we propose the Frechet Bert Distance as a reasonable measure to compute the similarity between the predicted intent and the ground truth distributions to identify whether the model is regurgitating its predictions from memorization. We see that there is an acceptable level of dissimilarity between the real and predicted intents, with a score of 0.54. A perfect match would be 0 and a complete dissimilarity would be 1. As shown in the paper, we are not able to reproduce utterances or labels from the test sets by manually interacting with the model. \n\nNotably, the problem of detecting pre-train data in LLMs has recently received attraction [1, 2], however, there is not yet a standard approach on how the decide that a text example was part of the pre-training data. We will explore this direction in future work.\n\n[1] Shi, Weijia, et al. \"Detecting Pretraining Data from Large Language Models.\" arXiv preprint arXiv:2310.16789 (2023).\n[2] Meeus, Matthieu, et al. \"Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models.\" arXiv preprint arXiv:2310.15007 (2023).\n\n> It would be nice to have a longer version of Appendix A.7 with more examples and details about the statistics of discovered intents. How close are they semantically/syntactically to the ground truth labels?\n\nWe have included a complete list of results in the supplementary material (see results.csv). We also extended Appendix 7 with a study on the complete sets of discovered intents and ground truth intents. In that study, we compute SentenceBert embeddings from the two sets and visualize them using TSNE in a 2D grid. We observe that some intents coincide, and many others do not. The ones that coincide correspond to samples with a straightforward intent naming choice, while the others require more naming creativity. Note that the required capability for a good performance is to use intents robustly, assigning the same intent to utterances with the same ground truth intent, regardless of the exact name.\n\nIn the extended list of discovered intents, we also observe that there is a reasonable difference between the predicted intents and the ground truth despite their semantic similarity. We consider that IntentGPT-3.5 and IntentGPT-4, conditioned with the proposed prompt scheme, offer sufficient generalization capability to not solve the task by memorization."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6364/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725549102,
                "cdate": 1700725549102,
                "tmdate": 1700727934494,
                "mdate": 1700727934494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]