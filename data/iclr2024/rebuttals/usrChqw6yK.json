[
    {
        "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors"
    },
    {
        "review": {
            "id": "MNKzZEFz5j",
            "forum": "usrChqw6yK",
            "replyto": "usrChqw6yK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_E6zT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_E6zT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Descriptor-Enhanced Open-Vocabulary Detector (DVDet) which introduces a conditional visual (region) prompting and textual descriptor dictionaries by using Large Language Models. The use of textual descriptors is motivated by the observation that the VLMs are good at capturing the fine-grained attributes of the objects. The textual descriptor dictionary is dynamically updated during training by tracking how frequent each descriptors are used, and by prompting what visual descriptors can help distinguishing the confusing categories. In addition, this work proposes a conditional region prompt to help the alignment between the detected region and text embeddings. When using both the textual descriptors and conditional visual prompts, DVDet improves the modern open-vocabulary detectors on the OV-COCO and OV-LVIS benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The use of Large Language Models to enrich the vocabulary description is well motivated.\n\n* The proposed conditional visual prompt is useful in bridging the gap between the image-text and region-text alignment.\n\n* The proposed DVDet method can be plugged into other SOTA open-vocabulary detection methods and consistently improve the performance."
                },
                "weaknesses": {
                    "value": "* The use of region-level prompting for open-vocabulary detection is previously proposed in CVPR 2023 paper \"CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching\". The comparison (both at a idea level and a numeric comparison) with CORA work seems necessary regarding the technical similarity.\n\n* It is not clear what the optimal number of iterations is for the dictionary update. Is \"the number of high-frequency descriptors\" in Table 7 the number of iterations?\n\n* What is the effect of using the LLM on the running speed of the DVDet detector (using no LLM vs N-th interaction of LLM dictionary)? While the running speed might not be the main focus of open-vocabulary detection research, the multiple iterations of LLM use could make the speed not very favorable.\n\n* There are non-negligible number of hyper parameters (number of descriptors, number of update iterations, thresholds)."
                },
                "questions": {
                    "value": "* The gain in the base vs novel categories are similar (e.g., + 2.7 novel AP and +2.2 base AP in Table 1). This leads to a question whether the DVDet method is beneficial in standard detection instead of open-vocabulary detection. That is, the proposed method benefits from augmenting the vocabulary description (text embeddings) in general, not being specifically helpful for the novel categories. What is the motivation of evaluating on open-vocabulary detection? How would DVDet perform in the standard detection setting?\n\n* Please compare with CORA (CVPR 2023) which also uses visual region prompting for open-vocabulary detection.\n\n* Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721338063,
            "cdate": 1698721338063,
            "tmdate": 1699636756040,
            "mdate": 1699636756040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HX8VUb3mp2",
                "forum": "usrChqw6yK",
                "replyto": "MNKzZEFz5j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer E6zT (Part I)"
                    },
                    "comment": {
                        "value": "We are heartened by the reviewer's recognition of our method's motivation and utility. We value your insightful summary and constructive feedback and address each point as follows:\n\n> Q1: Comparison with region-level prompting in CORA (CVPR 2023)\n\nIn contrast to CORA's approach, our visual prompts integrate background information from images. Table 9 of our paper shows that incorporating background information leads to performance improvements, illustrating the effectiveness of our method. We believe that this design offers a more comprehensive and contextual understanding of the image, aligning more closely with the visual prompt concept. Here are the experimental results:\n\n|  CCP  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    w/o Background Information               \t| 33.4      \t| 51.8  |\n| DVDet          \t| **34.6**      \t| **52.8**  |\n\nThis distinction in prompt design underlines the originality of our approach and its effectiveness in enhancing detection performance.\n\n> Q2: Lack of Parameters Experiment on Update Frequency for Dictionary Update\n\nWe would clarify that the update frequency of the descriptors dictionary is not equal to \"the number of high-frequency descriptors\". We conduct new experiments to assess its impact on performance. As the table below shows, DVDet maintains stable performance when the dictionary update frequency remains within a certain range\n\n|  Update Frequency of Dictionary |   0  |   100  |   200  |   400  |\n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel               \t| 33.2      \t| **34.8**  | 34.6      \t| 34.6  |\n| mAP_base          \t| 51.9     \t| **53.2**  | 52.8      \t| 52.6  |\n\n> Q3: Effect of LLM on DVDet's Running Speed\n\n* We appreciate the reviewer pointing out the need for experimental analysis on efficiency. In our study, the primary factor impacting training efficiency is the update of category descriptors. Notably, the update time for the descriptor dictionary accounts for only **20.16%** of the total training time when the update frequency is set at every 100 iterations in our implementation. This ratio demonstrates that our approach achieves an efficient balance, being comprehensive while remaining resource-effective.\n\n* We further conducted experiments to evaluate this, as shown in the table below. The results indicate that within a certain range, reducing the update frequency of the dictionary allows DVDet to maintain stable performance.\n\n|  Update Frequency of Dictionary |   0  |   100  |   200  |   400  |\n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel               \t| 33.2      \t| **34.8**  | 34.6      \t| 34.6  |\n| mAP_base          \t| 51.9     \t| **53.2**  | 52.8      \t| 52.6  |\n\n> Q4: Handling Multiple Hyperparameters\n\nActually, our method focuses on three key parameters: the length 'm' of the enlarged proposals, the update frequency 'N' of descriptors, and the number 'TopN' of high-frequency descriptors. However, our experimental data indicate that the algorithm's performance is not highly sensitive to these parameters. This is supported by the results shown in Table 6 of our paper regarding the 'TopN' parameter. In response to Q2, we have analyzed the impact of the Descriptor Update Frequency 'N'. Additionally, to further demonstrate the importance of the parameter 'm', which determines the width and height of the proposal, we present the following experimental results:\n\n|  the values m and n  |   0  |   20  |   30  |   40  | \n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel     | 33.4    | 34.6  | 34.8  | 34.4  |\n|  mAP_base  |  51.8      \t|  52.8  |  52.9  | 52.4  |\n\nMoving forward, we plan to implement learnable parameters and structures, aiming to reduce the necessity for manual hyperparameter tuning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417453031,
                "cdate": 1700417453031,
                "tmdate": 1700417512037,
                "mdate": 1700417512037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qm2XRQAr8D",
            "forum": "usrChqw6yK",
            "replyto": "usrChqw6yK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_6yi2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_6yi2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DVDet, a Descriptor-Enhanced Open Vocabulary Detector featuring two innovative components: conditional context prompts and hierarchical textual descriptors. The conditional context prompts function in the Region of Interest (RoI) align to expand each region proposal's area, incorporating more context information and converting it into an image-like feature preferred by CLIP. Meanwhile, the hierarchical textual descriptors dynamically acquire fine-grained descriptors for each category through interaction with the Language Model (LLM) during training. Extensive experiments indicate that DVDet significantly enhances the performance of various Open-Vocabulary Detectors on two benchmarks, MS COCO and LVIS."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I greatly value the use of LLM to dynamically generate detailed descriptors of categories. This approach inspires a novel method of employing LLM to enhance the performance of OVD tasks. \n\n2. The concept presented is both straightforward and potent, with the paper being lucid and easy to navigate. The author's introduction of Framework Figures (Fig 2) and the examples of iterative fine-grained descriptors (Figure 3) are commendable as they significantly aid the reader in grasping the idea more effectively.\n\n3. The experiments conducted are thoroughly robust. First, the author meticulously scrutinizes each component of their methodology, as detailed in Table 3 and Table 9. Second, the author conducts comprehensive experiments on a variety of OVD detectors and datasets to demonstrate that DVDet significantly enhances performance."
                },
                "weaknesses": {
                    "value": "1. A comparison of computation costs with the baseline has not been conducted.\n\n2. The contribution of the technique is somewhat restricted.\n\n3. The prompt generated by LLM may possess an element of randomness, potentially making it challenging to reproduce.\n\n4. No ablation experiment about the hyper-parameter $m$ of Conditional Context Prompt"
                },
                "questions": {
                    "value": "1. How about the extra computation overhead?\n\n2. LLM may generate different prompts. Does it significantly affect the performance?\n\n3. How about different $m$ in Conditional Context Prompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817973161,
            "cdate": 1698817973161,
            "tmdate": 1699636755923,
            "mdate": 1699636755923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ApWApMh99W",
                "forum": "usrChqw6yK",
                "replyto": "qm2XRQAr8D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 6yi2"
                    },
                    "comment": {
                        "value": "We are grateful that the reviewer recognizes the value, straightforwardness, and effectiveness of our method. We appreciate your concerns about the algorithm's efficiency increase and stability and will address each of your points in turn.\n\n> Q1: A comparison of computation costs with the baseline.\n\n* We appreciate the reviewer pointing out the need for experimental analysis on efficiency. In our study, the primary factor impacting training efficiency is the update of category descriptors. Notably, the update time for the descriptor dictionary accounts for only **20.16%** of the total training time when the update frequency is set at every 100 iterations in our implementation. This ratio demonstrates that our approach achieves an efficient balance, being comprehensive while remaining resource-effective.\n\n\n* We further conducted experiments to evaluate this, as shown in the table below. The results indicate that within a certain range, reducing the update frequency of the dictionary allows DVDet to maintain stable performance.\n\n|  Update Frequency of Dictionary |   0  |   100  |   200  |   400  |\n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel               \t| 33.2      \t| **34.8**  | 34.6      \t| 34.6  |\n| mAP_base          \t| 51.9     \t| **53.2**  | 52.8      \t| 52.6  |\n\n> Q2: Prompt Generation Randomness from LLMs\n\nWe are pleased that the reviewer has pointed out the impact of descriptor randomness on experimental performance. In fact, DVDet's interactive mechanism was designed with the generative model's randomness in mind. We have implemented two relevant mechanisms:\n\n* Descriptor Update Mechanism: By retaining high-frequency descriptors and discarding low-frequency ones, we ensure the visual relevance of the descriptors.\n* Descriptor Selection Mechanism: During the inference phase, we discard irrelevant descriptors, thereby enhancing stability.\n* We further present the performance variability across five experimental runs to demonstrate the robustness of our algorithm.\n\n|  LVIS  |   mAP_novel  |   mAP_all  |   \n|:--------:|:---------:|:---------:|\n|    VLDet      | 27.5  $\\pm$ 0.2      \t| 41.8 $\\pm$ 0.3 |\n\n> Q3: Lack of Ablation Experiment on Hyper-Parameter m of Conditional Context Prompt:\n\n* Firstly, in Table 9 of our paper, we provided ablation studies regarding background information, showing the experimental outcomes when parameter m is set to 0.\n\n* We conducted further experiments about the parameter m. As the table below shows, DVDet's performance remains stable when m lies within a certain range:\n\n|  the values m and n  |   0  |   20  |   30  |   40  | \n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel     | 33.4    | 34.6  | 34.8  | 34.4  |\n|  mAP_base  |  51.8      \t|  52.8  |  52.9  | 52.4  |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417527176,
                "cdate": 1700417527176,
                "tmdate": 1700417527176,
                "mdate": 1700417527176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7hMuS0pLZX",
            "forum": "usrChqw6yK",
            "replyto": "usrChqw6yK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_shSW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_shSW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to boost open-vocabulary object detector with fine-grained language descriptors of the categories. The motivation is that existing works learn to align region embeddings with category labels only, disregarding the capability of VLM to align with object parts and other fine-grained descriptions. The fine-grained text descriptors are generated with an LLM in an interactive manner. In addition, the paper introduces conditional context prompt to augment region embeddings with contextual cues and make them more image-like for open-vocabulary detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The two approaches presented by this paper are intuitive and effective. The context prompt makes the region more image-like to aid open-vocabulary recognition and the use of LLM provides additional features for VLM to reliably detect novel classes. The main results in Table 1 and 2 show the effectiveness of proposed approaches on LVIS and COCO benchmarks based on many existing methods. Ablations show that each part is effective on OV-COCO."
                },
                "weaknesses": {
                    "value": "1. The idea of using LLM to generate more textual description has been explored in recent/concurrent work [2]. It'd be great to see a comparison with the existing ideas. Similarly, the idea of context regional prompt has been explored in recent/concurrent work [1], although the exact instantiation may differ. It'd be helpful to tease apart the contribution of context (which makes the crop more image-like) vs prompting (which adapts the features for detection use cases) through an ablation, and compare/discuss the similarity/differences with existing work. I understand the references listed are very recent/concurrent works, but I think it'd still be valuable to have some comparison with them for scientific understanding.\n\n2. Although DVDet shows gains for all methods in Table 1 and 2. The gains on base categories are non-trivial and even comparable to the novel categories in most cases. For example, DVDet + Detic has +1.8 boost on mAPf and +1.3 on mAPr, and +2.5 on Base AP vs +1.7 on Novel AP. Given the method is based on Detic, I'm wondering what the implication of the gains on base vs novel categories are, since the method is primarily designed for open-vocabulary detection. \n\n3. Table 3 (row 2) shows that adding fine-grained descriptors by itself without prompting hurts the performance of the model. This seems counter-intuitive to me. It'd be helpful to see if the same observation holds for other OVD models e.g. ViLD or Detic.\n\n4. Table 4 shows transfer detection from COCO to PASCAL and LVIS. It's nice to see a clear boost there. I'd recommend trying out the LVIS-trained model on Objects365 instead since it's a more commonly used transfer detection setting by e.g. ViLD, DetPro, F-VLM. \n\n5. In Eq (2), the $\\textit{m, n}$ are said to be constants. How are they set? I'm wondering if they should be set proportional to the width and height of the ROI. Another option is to use the whole image box as a baseline and see how that performs. Some ablations/analysis on the choice of context would be interesting.\n\nReferences:\n1. CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching (CVPR 2023)\n2. Multi-Modal Classifiers for Open-Vocabulary Object Detection (ICML 2023)"
                },
                "questions": {
                    "value": "See weaknesses. Point 1-3 are more important in my view.\n\nWhat's the variance of the proposed method on LVIS open-vocabulary benchmark over e.g. 3 or 5 independent runs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870498477,
            "cdate": 1698870498477,
            "tmdate": 1699636755808,
            "mdate": 1699636755808,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "El0AEFADz5",
                "forum": "usrChqw6yK",
                "replyto": "7hMuS0pLZX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer shSW (Part. I)"
                    },
                    "comment": {
                        "value": "We are really encouraged that the reviewer recognizes our method to be intuitive and effective.\n\nWe thank the valuable comments and insightful suggestions, and we hope our detailed responses below can address your concerns.\n\n> W1: Discussion with Concurrent Work [1] and [2].\n\nAs the reviewer noted, our work mainly involves the generation of category descriptors and the regional visual prompt. We appreciate the reviewer pointing out the need to compare our approach with the latest methods CORA [1] and MCVOD [2], and we provide a comparison with each below.\n\nFirstly, regarding the generation of descriptors, our approach differs significantly from MCVOD [2] in the way of interacting with LLMs. Specially, we leverage the interactive capabilities of LLMs to dynamically generate visually relevant descriptors for each category, rather than treating the process as a one-time interaction. As shown in Table 5 of our paper, introducing this interaction mechanism generally leads to a performance gain of around 1.5%. The experimental results are as follows:\n\n|  Interaction Strategy  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    Static               \t| 33.2      \t| 51.9  |\n| Interactive (DVDet)          \t| **34.6**      \t| **52.8**  |\n\nSecondly, regarding visual prompts, our design differs from the prompts in CORA [1] by integrating background information from images. As Table 9 of our paper shows, we observe further performance improvements when the background information is incorporated. As the reviewer pointed out, prompts without background information can be considered tailored for detection tasks (**prompting**). Incorporating background information further merges contextual information (**context**), making it more consistent with the image. The experimental results are as follows:\n\n|  CCP  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    w/o Background Information               \t| 33.4      \t| 51.8  |\n| DVDet          \t| **34.6**      \t| **52.8**  |\n\nWe will add the discussion with the concurrent methods[1, 2] in our updated manuscript.\n\n[1] Wu X, Zhu F, Zhao R, et al. CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching[C]. CVPR 2023.\n\n[2] Kaul P, Xie W, Zisserman A. Multi-Modal Classifiers for Open-Vocabulary Object Detection [C]. ICML 2023\n\n\n> W2: Gains on Base vs. Novel Categories.\n\nThank you for highlighting this important point. We acknowledge that our method does not show much more improvements in novel categories compared to base categories. Nonetheless, it's notable that our algorithm achieves consistent enhancements in both. \n\nAs illustrated in Figure 1 of our paper, unlike classification models using VLMs, current open-vocabulary detectors struggle to align visual features with descriptions of categories. Our ablation studies in Table3 of our paper, visualized in the results, demonstrate that category descriptors do not boost the pretrained model in detection tasks as effectively as they do in classification tasks [3], occasionally leading to reduced performance. \n\nIn our training process, we focus on aligning both base categories and descriptors, endowing them with better cross-modal alignment capability. This approach further yields improvements in novel categories, thereby confirming our method's effectiveness in enhancing open-set detection performance. The visualizations in Figure 5 of our paper further demonstrate how descriptors progressively refine detection results in new categories.\n\n[3] Menon S, Vondrick C. Visual classification via description from large language models[J]. ICLR2023.\n\n> W3: Decreased Performance with Descriptors without Prompt Learning\n\nThank you very much for the constructive suggestion. The reason for this phenomenon, where adding fine-grained descriptors by itself without prompting hurts the performance of the model, lies in the disparity between the capabilities of VLMs and current open vocabulary detectors in cross-modal alignment. As shown in Figure 1 of our paper, the visual features extracted by VLMs maintain high consistency with the textual embeddings of descriptors. Therefore, in classification tasks [3], they can directly enhance performance in the inference stage without additional training. However, this is challenging to achieve in detection tasks. To make this conclusion more convincing, we have also compiled results on Detic and provided the following ablation study:\n\n|  Detic  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    w/o Prompt Learning               \t| 25.5      \t| 47.9  |\n| Prompt Learning          \t| **27.8**      \t| **51.1**  |\n\nWe will update the experiments and discussions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417346336,
                "cdate": 1700417346336,
                "tmdate": 1700417346336,
                "mdate": 1700417346336,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UU6D7dFeFu",
            "forum": "usrChqw6yK",
            "replyto": "usrChqw6yK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_AtsG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_AtsG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to a novel open vocabulary detection method which uses a fine-grained descriptor for a better region-text alignment and open vocabulary detection. It consists of two parts. First part is to transform region embeddings to image-like representation, second part is to use LLM  to generate fine-grained descriptors. The method improves open vocabulary detection consistently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method proposed in the paper looks interesting by using LLM as a better fine-grained descriptor. The idea is reasonable and the presentation is good.\n2. The progress from iterative extraction of fine-grained LLM descriptor is interesting and looks interesting to me.\n3. The results of experiments are good and the ablation study is convincing to me."
                },
                "weaknesses": {
                    "value": "1. when LLM interacts with VLM, Could you give more examples of prompts to use and how the prompt may influence the final results?\n2. when iterative updating of LLM descriptor, It seems to just give a general description of objects. Then what will happen if we just let LLM do some general description of objects without seeing the objects?\n3. if the regional prompt is not accurate, how will the performance look like when interacting with LLM?"
                },
                "questions": {
                    "value": "please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887716560,
            "cdate": 1698887716560,
            "tmdate": 1699636755700,
            "mdate": 1699636755700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DXxu9qI9QW",
                "forum": "usrChqw6yK",
                "replyto": "UU6D7dFeFu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer AtsG"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's interest in understanding how descriptors and regional prompts influence detection results. We address each point of interest in turn.\n\n> W1: when LLM interacts with VLM, Could you give more examples of prompts to use and how the prompt may influence the final results:\n\nIn this paper, we utilize three prompts to interact with LLMs to gather descriptors, 'Template N' only includes the category name, \u2018Template H\u2019 [1] includes high-frequency descriptors and the category name, and \u2018Template C\u2019 includes confusing categories and the category name. The performance using different prompts is as follows:\n\n|  Interaction Strategy  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    VLDet               \t| 32.0      \t| 50.6  |\n|    +template N               \t| 33.1      \t| 51.2  |\n|    +template C               \t| 33.8      \t| **52.3**  |\n|    +template H               \t| **34.0**      \t| 52.2  |\n\nOur experimental findings show that our designed templates, namely templates C and H, are more effective than the template N used in [1] for prompting LLMs. This results in the generation of effective descriptors and an overall improvement in performance, highlighting the efficacy of our method in descriptor generation.\n\n[1] Menon S, Vondrick C. Visual classification via description from large language models[J]. ICLR2023.\n\n\n> W2: What will happen if we just let LLM do some general description of objects without seeing the objects?:\n\n* Firstly, in our method, the Large Language Models (LLMs) understand objects through their interaction with Vision Language Models (VLMs). So, during training, the LLMs don't see novel classes. The improvement in detecting these novel classes, which are unknown to LLMs, shows how effective our generated descriptors are.\n\n* Secondly, we present experimental results where all classes were unknown to the LLMs. The results from Table 5 of our paper are as follows:\n\n|  Interaction Strategy  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    VLDet               \t| 32.0      \t| 50.6  |\n|    Static               \t| **33.2**      \t| **51.9**  |\n\n\nThese results indicate that even without interacting with detection models, the descriptors from LLMs still improve the performance of these models.\n\n> W3: if the regional prompt is not accurate, how will the performance look like when interacting with LLM?\n\nAs the reviewer correctly points out, the accuracy of prompts can indeed affect the algorithm's performance. As shown in Table 3 of our paper, we present the performance of DVDet with Prompt Learning. To further illustrate the impact of prompt accuracy, we added a new experiment using a Random Prompt. This additional experiment clearly demonstrates that inaccurate, randomly selected prompts lead to decreased performance. In contrast, our original approach with prompt learning significantly improves detection accuracy.\n \n|  Regional Prompt  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    VLDet               \t| 32.0      \t| 50.6  |\n|    +random prompt | 27.3      \t| 45.2  |\n|    +prompt learning | **33.1**      \t| **51.2**  |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417261595,
                "cdate": 1700417261595,
                "tmdate": 1700417261595,
                "mdate": 1700417261595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1gUHy3QDKW",
            "forum": "usrChqw6yK",
            "replyto": "usrChqw6yK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_qAZ8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6622/Reviewer_qAZ8"
            ],
            "content": {
                "summary": {
                    "value": "The submission introduces Descriptor-Enhanced Open Vocabulary Detection (DVDet), a method that integrates fine-grained descriptors from Vision Language Models (VLMs) into open vocabulary object detection (OVOD) using a Conditional Context visual Prompt (CCP). This approach utilizes large language models to generate descriptors without extra annotations and features a hierarchical update mechanism for descriptor refinement. The paper claims improvements in OVOD tasks through extensive experiments and presents three main contributions: a new feature-level visual prompt, an update mechanism for descriptor management, and empirical evidence of enhanced detection performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Introduction of Descriptor-Enhanced Open Vocabulary Detection (DVDet) could address existing granularity challenges in object detection, making it a potentially transformative method.\n+ Leveraging large language models for descriptor generation without additional annotations presents a cost-effective solution."
                },
                "weaknesses": {
                    "value": "- The paper does not sufficiently address the analysis of descriptors produced by Large Language Models, which is crucial for understanding the quality and relevance of the generated descriptors in enhancing object detection.\n- The demonstrated enhancements (~2% on average) in object detection performance, while positive, do not represent a substantial leap forward when considering the benchmarks established by current leading methods."
                },
                "questions": {
                    "value": "1. How does the descriptors generation process affects the training efficiency?\n2. Can the authors provide insights into any observed trade-offs between the complexity of the Descriptor-Enhanced Open Vocabulary Detection (DVDet) and its performance gains?\n2. How does the system perform under varying conditions, such as different object scales, occlusions, and lighting? Are there specific scenarios where the performance improvement is more pronounced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6622/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699317162906,
            "cdate": 1699317162906,
            "tmdate": 1699636755567,
            "mdate": 1699636755567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dMlTmTgW12",
                "forum": "usrChqw6yK",
                "replyto": "1gUHy3QDKW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6622/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qAZ8"
                    },
                    "comment": {
                        "value": "We thank the valuable comments and insightful suggestions, and we hope our detailed responses below can address your concerns.\n\n> W1: Analysis of Descriptors from LLMs, essential for evaluating their quality and impact on improving object detection.\n\nThank you for highlighting the importance of descriptor analysis, which is indeed a central aspect of our work. Effectively generating descriptors through LLMs forms the core of our approach.\n*  Firstly, we designed an interactive mechanism for refining descriptors through ongoing interactions with LLMs. In contrast to static descriptors, which are produced through a one-time interaction, our interactive approach updates existing descriptors based on their contributions in enhancing detection performance. This point is well demonstrated in the experiment table below:\n\n|  Interaction Strategy  |   mAP_novel  |   mAP_base  |   \n|:--------:|:---------:|:---------:|\n|    Static               \t| 33.2      \t| 51.9  |\n| Interactive          \t| **34.6**      \t| **52.8**  |\n\n* Secondly, our semantic selection mechanism chooses visual-related descriptors, thereby avoiding the influence of descriptors that are not visible in the current sample. This design highlights the importance of the relevance of the descriptors as validated in the table below. \n\n|  TopN  |   0  |   1  |    3  |   5  |  10  |   15  | \n|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_base               \t| 50.2      \t| 51.2  | 52.0    | **52.8**  |51.9      \t| 51.7  |\n| mAP_novel          \t| 32.0      \t| 32.4  |34.4      \t| **34.6**  |33.0      \t| 32.2  |\n\n* Lastly, we provide extensive visualization in Figures 7 and 8 of our paper, which illustrate how the descriptors improve detection performance. These visualizations clearly demonstrate the effectiveness of our descriptor generation in enhancing object detection performance.\n\n\n> W2: The performance enhancements (~2% on average) in object detection.\n\nWe would like to share that a 2% improvement in performance metrics is quite acceptable in the current setting. For instance, methods like VLDet [1] which incorporates caption data also achieve only around 2% improvement compared to those without caption data. However, we would like to reiterate two key points: first, our method employs lightweight prompt learning, and second, it achieves consistent improvements across multiple detectors.\n\n[1] Lin C, Sun P, Jiang Y, et al. Learning object-language alignments for open-vocabulary object detection[J]. ICLR, 2023.\n\n> Q1: The training efficiency affected by the descriptors generation process.\n\nWe appreciate the reviewer pointing out the need for experimental analysis on efficiency. In our study, the primary factor impacting training efficiency is the update of category descriptors. Notably, the update time for the descriptor dictionary accounts for only **20.16%** of the total training time when the update frequency is set at every 100 iterations in our implementation. This ratio demonstrates that our approach achieves an efficient balance, being comprehensive while remaining resource-effective.\n\n> Q2: Complexity and Performance Trade-offs:\n\nBuilding on our previous discussion, we identified that the frequency of dictionary updates (i.e., the frequency of interactions with LLMs per number of iterations) is a key factor influencing training efficiency. We conducted experiments to evaluate this, as shown in the table below. The results indicate that within a certain range, reducing the update frequency of the dictionary allows DVDet to maintain stable performance.\n\n|  Update Frequency of Dictionary |   0  |   100  |   200  |   400  |\n|:--------:|:---------:|:---------:|:---------:|:---------:|\n|    mAP_novel               \t| 33.2      \t| **34.8**  | 34.6      \t| 34.6  |\n| mAP_base          \t| 51.9     \t| **53.2**  | 52.8      \t| 52.6  |\n\n> Q3: Performance Under Varying Complex Conditions\n\nIn Figure 7 of our paper, we demonstrate how fine-grained descriptors correct the model's detection results. Further visualizations of this process are provided in Figure 8 of our paper. These visualizations clearly show that our approach significantly improves detection, particularly in challenging scenarios. These scenarios include (1) detecting distant or occluded objects, and (2) distinguishing objects with small inter-class variations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6622/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417226600,
                "cdate": 1700417226600,
                "tmdate": 1700417226600,
                "mdate": 1700417226600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]