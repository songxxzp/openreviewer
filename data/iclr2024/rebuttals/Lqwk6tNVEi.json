[
    {
        "title": "Neighborhood-Informed Diffusion Model for Source-Free Domain Adaptation: Retrieving Source Ground Truth from Target Query's Neighbors"
    },
    {
        "review": {
            "id": "RDOzvgtQu0",
            "forum": "Lqwk6tNVEi",
            "replyto": "Lqwk6tNVEi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_Wxvo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_Wxvo"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies source-free domain adaptation without accessing the source labelled data when conducting target adaptation. Specifically, the authors propose to utilize diffusion models to generate positive key features for facilitating the unsupervised clustering in target adaptation. The whole framework consists of three key components: 1) the source representation learning; 2) diffusion model learning and 3) target model adaptation. Experimental results on several public datasets demonstrate that the proposed model can outperform recent baselines with different gains."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)\tThis paper investigates source-free domain adaptation, which is a much more practical setting compared with source-need domain adaptation.\n2)\tA diffusion model is employed in the domain adaptation framework, which is less explored in the scenario of source-free DA.\n3)\tAblation studies are given to show the effectiveness of the proposed components."
                },
                "weaknesses": {
                    "value": "1)\tAlthough the diffusion models are less explored in the scenarios of source-free DA, the technical contribution of this paper is quite limited. No new diffusion model is proposed to address the domain shift problem in DA and the authors simply use an existing model in this step.\n2)\tAs this paper generates examples in the adaptation procedure, it is not clear what are the advantages of using diffusion model compared with other generative models like GAN. There are also lots of baselines that generate samples in the adaptation process and the authors did not discuss and compare with them. See references below.\n\n[1] Qiu Z, Zhang Y, Lin H, et al. Source-free domain adaptation via avatar prototype generation and adaptation. IJCAI 2021.\n\n[2] Li R, Jiao Q, Cao W, et al. Model adaptation: Unsupervised domain adaptation without source data[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 9641-9650.\n\n3)\tThe source-free setting is a little different from existing work, as this paper uses source data to train the diffusion model. However, existing source free models do not use any source data to train the generative models. \n4)\tThe experimental results are not convincing. The authors directly copy the results from the baselines; however, their network backbones are different. Thus, the comparisons are not fair. For example, in Table 2, baseline DaC\u2019s results are directly cited from its original paper, and it uses the backbone of ResNet-50. However, the authors use ResNet-101 in this paper. I strongly recommend the authors to rerun the experiments.\n5)\tMore ablation studies should be given to verify the effectiveness of the proposed diffusion model. What if we directly use the target\u2019s kNN samples as the positive keys?\n\nThere are lots of typos in the paper. The authors need to carefully read and polish the paper. Some of the typos are listed as follows:\n\u201cTo transition from $z_0$\u201d should be \u201cTo transit from $z_0$\u201d.\n\n\u201cWe use an SGD optimizer\u201d should be \u201cWe use a SGD\u201d.\n\nIn equation (1), the norm should be $\\Vert \\cdot \\Vert_2$."
                },
                "questions": {
                    "value": "1.)\tMore ablation studies should be given to verify the effectiveness of the proposed diffusion model. What if we directly use the target\u2019s kNN samples as the positive keys? It is not clear how different number of k-nearest neighbors will affect the model\u2019s performance.\n\n2.)\tThere are also lots of baselines that generate samples in the adaptation process and the authors did not discuss and compare with them. \n\n3.)\tThe experimental results are not convincing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698480036727,
            "cdate": 1698480036727,
            "tmdate": 1699636132961,
            "mdate": 1699636132961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ul3DUkm9MY",
                "forum": "Lqwk6tNVEi",
                "replyto": "RDOzvgtQu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wxvo (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's diligent effort in reviewing our manuscript. \n### **Weakness 1**\n\n> ### **Response to Reviewer\u2019s Concern on Technical Contribution:**\n\nWe appreciate the reviewer's acknowledgment of the novelty in our application of diffusion models in SFDA. We would like to clarify that our aim is not to propose new diffusion models but to explore their use as domain shift correctors for aligning feature distributions across domains without source data during target adaptation.\n\nExisting SFDA methods typically focus on enhancing unsupervised clustering (like NRC, AaD, SHOT, and G-SFDA) or using generative models for data augmentation (DaC), but they do not address quantifying domain shifts when source data is unavailable during target adaptation.\n\nOur main contribution lies in applying neighborhood searching to parameterize the prior distribution in diffusion models. This approach aligns feature distributions across domains during adaptation without source data, ensuring a correspondence between the source knowledge retrieved from diffusion and encoded target features, a novel advancement in SFDA. Our diffusion model, distinct from its usual role in image generation, refines target features using source knowledge under latent geometry guidance, differing from typical diffusion model applications.\n\nIn summary, we have innovatively adapted diffusion models to address a fundamental problem in SFDA, effectively quantifying domain shift without source data. This unique application explores new ground in SFDA, representing a significant contribution to the field.\n\n### **Weakness 2 and Question 2**\n\n> ### **Advantages Compared to Other Generative Models:**\n\nOur diffusion model, unlike traditional generative models, acts as a **domain shift corrector**, refining target features with source knowledge and aligning feature distributions during target adaptation, without needing source data. It does not generate new images. \n\nOur diffusion model's key benefit is its ability to quantify domain shift during target adaptation using latent geometry, a feature not seen in other generative models where the prior distribution plays a different role. By integrating semi-supervised learning's neighborhood searching, we ensure that the target adaptation is guided by the latent geometries' similarity between the two domains, ensuring correspondence between transformed and encoded target features. \n\nOur novel application of diffusion models not only quantifies domain shift but also aligns domains effectively when source data is absent. This significant impact of the prior distribution on the diffusion process is unique to our model and is a primary reason for choosing diffusion models over others like GANs in our research.\n\n**Comparison with The Work Listed by The Reviewer:** We have added these to **Related Work** section of our revised manuscript, particularly focusing on the works you mentioned. This expanded section, highlighted in blue under source-free domain adaptation using generative models, offers a thorough discussion. Additionally, we have detailed the differences between our method and the referenced works below.\n\n**Comparison with [1]:** [1]'s Contrastive Prototype Generation and Adaptation (CPGA) method generates class-specific avatar feature prototypes using a prototype generator trained with contrastive learning, informed by a pre-trained source model's classification boundaries. It aligns pseudo-labeled target features with these source prototypes for class-wise domain alignment in target domains without source data. Our method, in contrast, uses diffusion models as domain shift correctors to directly transform target features into source-resembling ones, aligning domain-specific feature distributions without source data. Unlike CPGA's focus on prototype and pseudo-label generation, we use neighborhood searching for parameterizing the diffusion process, allowing feature distribution alignment without source data. This fundamental difference from CPGA's prototype-based approach marks our method's distinct approach to domain adaptation.\n\n**Comparison with [2]:** We summarize the 3C-GAN [2], which combines a discriminator, a generator for target-style samples based on random labels, and a pre-trained predictor. The generator and predictor improve performance during training, using generated data to enhance target domain accuracy. However, 3C-GAN requires careful regularization to balance generative and classification objectives. In contrast, our method uses diffusion models and neighborhood searching to transform target features into source-like features. Our diffusion model serves as a domain shift corrector, not an image generator, which obviates the need for complex regularization strategies. By focusing on parameterizing the diffusion model's prior density for feature transformation, our approach significantly diverges from existing methods, representing a novel contribution to domain adaptation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467808849,
                "cdate": 1700467808849,
                "tmdate": 1700605658918,
                "mdate": 1700605658918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sBRdK45uwz",
                "forum": "Lqwk6tNVEi",
                "replyto": "RDOzvgtQu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wxvo (2/3)"
                    },
                    "comment": {
                        "value": "### **Weakness 3**\n\nTo address the reviewer\u2019s concerns, we approach it from two angles. Firstly, we ensure that our DND method adheres to all SFDA settings and principles. Secondly, we address the concern regarding the use of source data to train generative models.\n\n> ### **Concerning the SFDA Problem Setting**\n\nIn response to the first aspect of your concern, we direct the reviewer to our official comment in section **C1**, which is detailed in our response to all reviewers.\n\n> ### **Existing Source-Free Models Do Not Train Generative Model Using Source Data**\n\nOur DND, integrating a diffusion model with the ResNet backbone, conforms to the SFDA principle of using source data only for initial pre-training. Unlike typical SFDA methods, we utilize the diffusion model for feature refinement, not data generation, aligning with SFDA guidelines and avoiding data privacy issues. Our method does not directly use or store source data for target adaptation but relies on latent geometry informed by k-NNs, ensuring influence from source domain knowledge without direct access to source data. While SFDA methods typically refrain from using source data to train generative models, we see no violation or drawback in training our diffusion module with source data, given its role in enhancing target feature refinement.\n\nThis implementation of the diffusion model in DND significantly diverges from conventional SFDA approaches focused on data augmentation. By employing diffusion models for domain-specific feature alignment, our method demonstrates their potential in SFDA, extending beyond traditional generative uses.\n\nWe hope that our comprehensive explanation adequately addresses the reviewer's concern regarding our adherence to SFDA settings."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467922639,
                "cdate": 1700467922639,
                "tmdate": 1700669469129,
                "mdate": 1700669469129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A1GjvD6AJE",
                "forum": "Lqwk6tNVEi",
                "replyto": "RDOzvgtQu0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wxvo (3/3)"
                    },
                    "comment": {
                        "value": "### **Weakness 4 and Question 3**\n\n> ### **Response to Reviewer\u2019s Questions about Encoder G and Performance Comparisons**\n\nWe understand the reviewers' concern about using ResNet-101 as the encoder for all datasets, unlike other SFDA methods that typically use ResNet-50 for Office-31 and Office-Home. Our experiments showed that both ResNet-50 and ResNet-101 have their merits, with no definitive superior choice for Office-31 and Office-Home. We chose ResNet-101 due to its consistent performance across different datasets and to standardize the backbone architecture in our experiments.\n\nAs suggested, we have re-conducted our experiments on Office-31 and Office-Home using ResNet-50 and have also re-run the baseline methods using ResNet-101. The results and further details in response to this concern are included in our official comment to all reviewers in section **C2**.\n\n### **Weakness 5 and Question 1**\n\n> ### **Additional Ablation Studies**\n\nThank you for suggesting additional ablation studies, which have greatly improved our research. We have conducted the suggested ablation study on the large-scale **VisDA-2017** dataset. The results and a more detailed response can be found in our official comment to all reviewers in section **C3**.\n\n### **Weakness 6**\n\n> ### **Response to Typos**\n\nWe respectfully request the reviewer to re-examine the noted typos. We believe our use of English, specifically the phrase \"to transition from,\" is correct, as \"transition\" can function as a verb in standard English writing. Additionally, regarding the use of articles with abbreviations, the choice between \"a\" and \"an\" depends on the starting sound of the acronym. For SGD, which begins with a consonant sound when spoken, the use of \u201can\u201d is appropriate rather than \u201ca\u201d. Furthermore, we would like to clarify that using \u2016\u22c5\u2016 is a standard notation for expressing the L2 norm, particularly in the context of cosine similarity. Our use of this notation aligns with the conventional representation of the L2 norm in related literature on cosine similarity. Regardless, we will continue to diligently review our manuscript for any other typos or grammatical errors."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468109997,
                "cdate": 1700468109997,
                "tmdate": 1700712591331,
                "mdate": 1700712591331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sz6iMkeoDu",
            "forum": "Lqwk6tNVEi",
            "replyto": "Lqwk6tNVEi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_oyCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_oyCL"
            ],
            "content": {
                "summary": {
                    "value": "The paper highlights challenges in diffusion models for source-free domain adaptation and introduces discriminative neighborhood diffusion (DND) as a solution. \nBy leveraging pre-trained source representations, DND facilitates unsupervised clustering through its latent k-nearest\nneighbors and significantly enhances performance in SFDA scenarios.\nExtensive evaluations demonstrate the discriminative potential and state-of-the-art effectiveness of DND across various benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the idea of using diffusion models for source-free domain adaptation sounds interesting and reasonable\n\n- the paper is overall well-written and easy to follow\n\n- the results on three widely used domain adaptation datasets are impressive"
                },
                "weaknesses": {
                    "value": "- In source-free domain adaptation, the suggested approach demands an extra diffusion model and necessitates the storage of source data features, leading to substantial efforts in the source domain. This situation renders the term \"free\" somewhat unrealistic, presenting a major concern.\n\n- A recent work [a] also uses diffusion models for test-time adaptation, which is similar to source-free domain adaptation as depicted in a recent survey [b]. Could the proposed method work for single-epoch target adaptation, and how about the comparison?\n\n- Another concern is that only three small datasets are used to evaluate the performance of the proposed method, large-scale datasets like DomainNet [c] are also important. Also, target adaptation under class shift (e.g., partial-set domain adaptation in SHOT (ICML-2020)) is not studied in the experiment.\n\n- Since previous SFDA methods typically adopt the ResNet-50 backbone, the comparisons are not fair in these tables (the proposed method is based on ResNet-101). And how is the diffusion model used in the source domain, would the pre-trained diffusion model bring additional gains?\n\n\n\n\n\n[a]. Gao, Jin, et al. \"Back to the source: Diffusion-driven adaptation to test-time corruption.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[b]. Liang, Jian, et al. \"A comprehensive survey on test-time adaptation under distribution shifts.\" arXiv preprint arXiv:2303.15361 (2023).\n\n[c]. Peng, Xingchao, et al. \"Moment matching for multi-source domain adaptation.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019."
                },
                "questions": {
                    "value": "pls see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728208016,
            "cdate": 1698728208016,
            "tmdate": 1699636132892,
            "mdate": 1699636132892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "frNfPJUppA",
                "forum": "Lqwk6tNVEi",
                "replyto": "Sz6iMkeoDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oyCL (1/4)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's diligent effort in reviewing our manuscript. Below, we provide our detailed response to your concerns.\n\n### **Weakness 1:**\n\nBased on our understanding of the reviewer's concern, we have split it into two parts and addressed each in our official comment to all reviewers:\n\n> ### **Response to Concerns about Violation of SFDA Problem Settings**\n\nIn response to this concern, we direct the reviewer to our official comment in section **C1**, which is detailed in our response to all reviewers.\n\n> ### **Response to Concerns about Additional Model and Time Complexity**\n\nIn response to this concern, we direct the reviewer to our official comment in section **C4**, which is detailed in our response to all reviewers.\n\n### **Weakness 2**:\n\n> ### **Experiments on Single-Epoch Target Adaptation:**\n\nThank you for the suggestion. We would like to clarify that the focus of our paper is source-free domain adaptation instead of test-time adaptation. However, as suggested by the reviewer, our method can also be seamlessly applied to single-epoch target adaptation. We would like to thank the reviewer for the important suggestion to broaden the impact of our work. \n\nFirst, we would like to differentiate our method from the one in [a], where the diffusion model is used to project target domain data back to source domain data during target adaptation. This method relies on generating data density. A key limitation of [a] is the unclear correspondence between the generated target data and the original source data. We address this in our approach by integrating latent geometry into the diffusion process to guide this correspondence. Crucially, instead of generating data, our diffusion model operates at the feature level, infusing source-like traits into the encoded target features. This novel application of diffusion models represents a significant shift from traditional approaches, offering a unique contribution to the field of domain adaptation and filling a previously unexplored gap in the community.\n\nNext, we would like to discuss the specifics of implementing our method for single-epoch target adaptation. It is important to note that the source pre-training stage adheres to our approach for SFDA. This experimental framework aligns with the one employed in [a]:\n\n**Single-Epoch Target Domain Adaptation:** Our method performs one epoch target adaptation using contrastive learning. It utilizes our DND-generated positive key to align target features more closely with the source feature space, focusing on domain-specific feature alignment and domain shift correction. We use ResNet-50 as the backbone, consistent with baseline methods, and set $k$ for neighborhood searching to 5 with 16 diffusion steps. The one-epoch adaptation uses an SGD optimizer, a learning rate of $5e^{-3}$, and a batch size of $128$. Due to time constraints, we conducted experiments only on the large-scale ImageNet-C dataset, with the results displayed below. \n\nWe have incorporated the aforementioned discussion and experiments into Appendix K of the revised manuscript and highlighted them in blue.\n\n**Table. One-Epoch Target Adaptation Experiments on ImageNet-C (ResNet-50).**\n|  Method | ImageNet-C |\n|:---------------:|:------:|\n| MEMO [6] | 24.7 | \n| DiffPure [7] | 16.8 | \n| DDA [a] | 29.7 |\n| DND (ours) | **32.6** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467120150,
                "cdate": 1700467120150,
                "tmdate": 1700669438167,
                "mdate": 1700669438167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PDDRUmU9h7",
                "forum": "Lqwk6tNVEi",
                "replyto": "Sz6iMkeoDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oyCL (2/4)"
                    },
                    "comment": {
                        "value": "### **Weakness 3**\n\nWe appreciate the reviewer's suggestion to broaden our work's impact but wish to clarify that our research is focused on SFDA, not domain generalization or partial-set domain adaptation. Our goal is to demonstrate how diffusion models can assist in the discriminative process, retrieving ground truth knowledge without source data, thus extending their use beyond traditional generative roles. We would like to clarify that in the domain adaptation community, VisDA-2017 is consistently considered a large-scale dataset, comprising approximately 280,000 images. It is also recognized as one of the most challenging datasets in domain adaptation, primarily due to its significant domain shift. Additionally, DomainNet is more frequently employed in domain generalization studies, rather than in source-free domain adaptation (SFDA).\n\n> ### **DomainNet Experiments**\n\nSince other reviewers mentioned the possibility of applying our DND to domain generalization, as suggested by the reviewer, we have included implementation details and experimental results of using our DND in source-free domain generalization on the large-scale DomainNet dataset.\n\n>  ### **Further Experiments on DomainNet Dataset:**\n\nWe have completed the experiments on DomainNet, and the details of our experimental settings and results are provided below. We have added the experiments on domain generalization to Appendix J of the revised manuscript, with the relevant sections highlighted in blue for easy reference.\n\nIn applying our method to domain generalization, we focused on the target inference stage, keeping the source pre-training stage consistent with our approach for SFDA. This approach aligns with the popular source-free domain generalization setting discussed in [1].\n\nInference for Domain Generalization: Contrary to SFDA, where the model will be fine-tuned on target domain data, source-free domain generalization involves no training during target testing. Our SiLGA method is used for latent augmentation, specifically to transform encoded target features during the target testing phase, without changing any model parameters:\n\n$\\mathbf{\\hat{z}}^{t,i} := \\frac{(\\mathbf{z}_{1}^{t,i}+\\mathbb{N}_k (\\mathbf{z}^{t,i}))}{k+1}$. \n\nHere, $\\mathbf{\\hat{z}}^{t,i}$ are the transformed target features, $\\mathbf{z}_{1}^{t,i}$ are DND-generated features guided by the prior density parameterized by target latent geometry, and \\mathbb{N}_k (\\mathbf{z}^{t,i})) is the centroid of the latent $k$-NNs of the target features. \n\nDomain Generalization Experimental Setup: Our method utilizes a single hyperparameter, $k$, which is used for both exploring latent geometry and parameterizing the Gaussian prior, and is set to a value of 5. SiLGA's effectiveness was demonstrated on the large-scale DomainNet for domain generalization, suggesting potential for further exploring our DND in this area. Time constraints during the author's response period limited deeper investigation into domain generalization techniques like batch normalization averaging [2] or additional regularization and augmentation. Our approach primarily involved using DND for feature transformation during target testing.\n\n**Table. Domain generalization experiments on DomainNet (ResNet-50).**\n|  Method | DomainNet |\n|:---------------:|:------:|\n| ZS-CLIP(C) [3] | 45.6 | \n| CAD [4] | 45.5 | \n|ZS-CLIP(PC) [3] | 46.3 |\n| PromptStyler [1] | 49.5 |\n| DND (ours) | **50.8** |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467326592,
                "cdate": 1700467326592,
                "tmdate": 1700635971885,
                "mdate": 1700635971885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0DXyMfKdKy",
                "forum": "Lqwk6tNVEi",
                "replyto": "Sz6iMkeoDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oyCL (3/4)"
                    },
                    "comment": {
                        "value": "### **Weakness 3 (continued)**\n\n> ### **Partial-Set Domain Adaptation Experiments**\n\nAs suggested, we have expanded our research to include experiments in partial-set domain adaptation (PDA). PDA involves adapting models from a source domain with a wide range of classes to a target domain with fewer classes, focusing on common classes and disregarding source-exclusive ones. Our DND, influenced by the target domain's latent geometry, aligns source-like features with the target's feature space during adaptation. This approach minimizes the generation of features for source-exclusive classes, addressing class mismatches between domains. Our rationale is that source labels not present in the target domain will have distinct latent geometry, leading our DND to be less likely to generate irrelevant source-like features, staying within the target domain's label space.\n\nWe validated our claim using the Office-Home dataset. For PDA, following [9], we chose the first six classes alphabetically as target categories and used their samples for both adaptation and testing. Due to time constraints, we used the SHOT codebase for implementation without changing any hyperparameters. The results displayed below demonstrate that our DND effectively addresses class mismatches during the adaptation process. This highlights our method's capability to retrieve relevant source ground truth based on target latent geometry, thus minimizing negative transfer by avoiding the use of non-relevant source information in target adaptation.\n\nWe have included the aforementioned discussion and experimental results in **Appendix M** of the revised manuscript, highlighted in blue for easy reference.\n\n**Table. Partial-set domain adaptation on Office-Home (ResNet-50).**\n|  Method | Ar\u2192Cl| Ar\u2192Pr | Ar\u2192Rw | Cl\u2192Ar | Cl\u2192Pr | Cl\u2192Rw | Pr\u2192Ar | Pr\u2192Cl | Pr\u2192Rw | Rw\u2192Ar | Rw\u2192Cl | Rw\u2192Pr | Avg. |\n|:---------------:|:------:|------:|------:|-------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n| ResNet-50 | 46.3 | 67.5 | 75.9 | 59.1 | 59.9 | 62.7 | 58.2 | 41.8 | 74.9 | 67.4 | 48.2 | 74.2 | 61.3 | \n| IWAN [8] | 53.9 | 54.5 | 78.1 | 61.3 | 48.0 | 63.3 | 54.2 | 52.0 | 81.3 | 76.5 | 56.8 | 82.9 | 63.6 | \n| SAN [9] | 44.4 | 68.7 | 74.6 | 67.5 | 65.0 | 77.8 | 59.8 | 44.7 | 80.1 | 72.2 | 50.2 | 78.7 | 65.3 |  \n| SAFN [10] | 58.9 | 76.3 | 81.4 | 70.4 | 73.0 | 77.8 | 72.4 | 55.3 | 80.4 | 75.8 | 60.4 | 79.9 | 71.8 | \n| SHOT [5]| 64.8 | 85.2 | 92.7 | 76.3 | **77.6** | 88.8 | 79.7 | 64.3 | 89.5 | **80.6** | 66.4 | 85.8 | 79.3 |\n| DND (Ours)| **66.2** | **86.8** | **93.2** | **78.1** | 77.2 | **90.2** | **80.5** | **66.8** | **90.4** | 79.3 | **67.2** | **89.4** | **80.4** |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467453215,
                "cdate": 1700467453215,
                "tmdate": 1700636042982,
                "mdate": 1700636042982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hOJ54b7x8b",
                "forum": "Lqwk6tNVEi",
                "replyto": "Sz6iMkeoDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oyCL (4/4)"
                    },
                    "comment": {
                        "value": "### **Weakness 4**\n\n> ### **Response to Reviewer\u2019s Questions about Encoder G and Performance Comparisons**\n\nWe understand the reviewers' concern about using ResNet-101 as the encoder for all datasets, unlike other SFDA methods that typically use ResNet-50 for Office-31 and Office-Home. Our experiments showed that both ResNet-50 and ResNet-101 have their advantages, with no definitive superior choice for Office-31 and Office-Home. We chose ResNet-101 due to its consistent performance across different datasets and to standardize the backbone architecture in our experiments.\n\nAs suggested, we have re-conducted our experiments on Office-31 and Office-Home using ResNet-50 and have also re-run the baseline methods using ResNet-101. The results and further details in response to this concern are included in our official comment to all reviewers in **C2**.\n\n> ### **Response to the Additional Gain of Employing Our Diffusion Model for Testing in the Source Domain**\n\nWe appreciate the reviewer's acknowledgment of our diffusion model's potential to improve classification performance in general. In our original manuscript, we included an analysis, found in **Appendices D** and **E**, on using our DND-generated features for source domain testing. This comprehensive experimentation demonstrated that our DND enhances supervised classification as a latent augmentation module. Our testing extended beyond domain adaptation datasets to datasets for standard classification like **CIFAR-10, CIFAR-100, and ImageNet**. The results confirm our DND's versatility, effective both as a domain shift corrector in domain adaptation and as a latent augmentation technique in supervised classification.\n\n> ### **Reference:**\n\n[1] Cho, Junhyeong, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, and Suha Kwak. PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pp. 15702-15712. 2023.\n\n[2] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test-time adaptation. International Conference on Learning Representations (ICLR), 2023.\n\n[3] AlecRadford, JongWook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen  Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021.\n\n[4] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Optimal Representations for Covariate Shift. In International Conference on Learning Representations (ICLR), 2022.\n\n[5] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pp. 6028\u20136039. PMLR, 2020.\n\n[6] Zhang, Marvin, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems (NeurIPS) (2022): 38629-38642. \n\n[7] Nie, Weili, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460 (2022).\n\n[8] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for partial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8156\u20138164, 2018.\n\n[9] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with selective adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2724\u20132732, 2018.\n\n[10] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1426\u20131435, 2019."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467582539,
                "cdate": 1700467582539,
                "tmdate": 1700636026976,
                "mdate": 1700636026976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2nqcTINXNV",
            "forum": "Lqwk6tNVEi",
            "replyto": "Lqwk6tNVEi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_VVzG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_VVzG"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to source-free domain adaptation using diffusion models. the diffusion models are built using the intuitive idea that \"you are close to your neighbors\". Experiments on three standard domain adaptation datasets are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Source-fee domain adaptation is a challenging problem. The proposed solution is novel and effective. Experiments validate the effectiveness of the proposed approach. Overall a good paper."
                },
                "weaknesses": {
                    "value": "I do not see any."
                },
                "questions": {
                    "value": "How will your approach work for the domain generalization problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794824362,
            "cdate": 1698794824362,
            "tmdate": 1699636132787,
            "mdate": 1699636132787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "szb6CDRZ4j",
                "forum": "Lqwk6tNVEi",
                "replyto": "2nqcTINXNV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VVzG (1/1)"
                    },
                    "comment": {
                        "value": "We would like to greatly appreciate the reviewer\u2019s positive and constructive feedback on our manuscript. Your recognition of our work and insightful suggestions have been invaluable in enhancing the quality and impact of our work. Thank you again for the supportive and encouraging words, and for contributing significantly to our research journey. \n\n> ### **Suggesting Investigation of DND for Domain Generalization Applications:**\n\nThank you for your insightful suggestion to conduct domain generalization experiments, which has helped broaden the impact of our method. We have completed these experiments, and the details of our experimental settings and results are provided below. We have added the experiments on domain generalization to **Appendix J** of the revised manuscript, with the relevant sections highlighted in blue for easy reference.\n\nFirst, we would like to clarify that source-free domain adaptation (SFDA) and source-free domain generalization (SFDG) have distinct objectives. SFDA is about adapting a model initially trained on a source domain to a different target domain, and this adaptation occurs without access to the original source data. On the other hand, SFDG involves training a model in one or multiple source domains with the goal of effective generalization across various unknown and unseen target domains, without requiring any specific adaptation training for these target domains. Therefore, our method, which is tailored for SFDA, cannot be directly applied to SFDG without modifying its framework to fulfill the more expansive generalization objectives of SFDG.\n\nIn applying our method to domain generalization, we focused on the target inference stage, keeping the source pre-training stage consistent with our approach for SFDA. This approach aligns with the popular source-free domain generalization setting discussed in [1].\n\n**Inference for Domain Generalization:** Contrary to SFDA, where the model will be fine-tuned on target domain data, source-free domain generalization involves no training during target testing. Our SiLGA method is used for latent augmentation, specifically to transform encoded target features during the target testing phase, without changing any model parameters:\n\n$\\mathbf{\\hat{z}}^{t,i}=\\frac{\\mathbf{z}_{1}^{t,i}+\\mathbb{N}_k (\\mathbf{z}^{t,i})}{k+1}$\n\nHere, $\\mathbf{\\hat{z}}^{t,i}$ are the transformed target features, $\\mathbf{z}_{1}^{t,i}$ are DND-generated features guided by the prior density parameterized by target latent geometry, and $\\mathbb{N}_k (\\mathbf{z}^{t,i})$ are the latent $k$-NNs of the target features. \n\n**Domain Generalization Experimental Setup:** Our method utilizes a single hyperparameter, $k$, which is used for both exploring latent geometry and parameterizing the Gaussian prior, and is set to a value of 5. SiLGA's effectiveness was demonstrated on the large-scale DomainNet for domain generalization, suggesting potential for further exploring our DND in this area. Time constraints during the author's response period limited deeper investigation into domain generalization techniques like batch normalization averaging [2] or additional regularization and augmentation. Our approach primarily involved using DND for feature transformation during target testing.\n\n**Table. Domain generalization experiments on DomainNet (ResNet-50).**\n|  Method | DomainNet |\n|:---------------:|:------:|\n| ZS-CLIP(C) [3] | 45.6 | \n| CAD [4] | 45.5 | \n|ZS-CLIP(PC) [3] | 46.3 |\n| PromptStyler [1] | 49.5 |\n| DND (ours) | **50.8** |\n\n\n> ### **Reference:**\n\n[1] Cho, Junhyeong, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, and Suha Kwak. PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pp. 15702-15712. 2023.\n\n[2] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test-time adaptation. International Conference on Learning Representations (ICLR), 2023.\n\n[3] AlecRadford, JongWook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen  Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021.\n\n[4] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Optimal Representations for Covariate Shift. In International Conference on Learning Representations (ICLR), 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466816718,
                "cdate": 1700466816718,
                "tmdate": 1700635938580,
                "mdate": 1700635938580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1oE2gs8vVB",
            "forum": "Lqwk6tNVEi",
            "replyto": "Lqwk6tNVEi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method: discriminative neighborhood diffusion (DND). DND formulates a diffusion model using pre-trained source domain representation and combine it with contrastive learning to promote unsupervised clustering of the target domain in the domain adaptation process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces the diffusion model into the SFDA problem, and the proposed method is simple and effective."
                },
                "weaknesses": {
                    "value": "1) DND may violate the problem setting of SFDA, i.e., learning a target model with only a pre-trained source model and target data introduced by SHOT.\n2) The writing logic of the paper is chaotic and difficult to read, especially in the introduction section. In addition, it is not advisable to use a large space in the method section to introduce existing work: IADB, and a brief explanation is sufficient.\n3) This method requires a large number of hyperparameters, and it seems difficult to quickly find suitable parameters. And there is a lack of hyperparameter sensitivity experiments.\n4) The persuasiveness of conducting ablation experiments on a relatively simple dataset, Office-31, is not strong. It is recommended to supplement the results of ablation experiments on the Office-Home or VisDA-C."
                },
                "questions": {
                    "value": "1) Does diffusion model learning violate or relax the problem setting of SFDA, because the diffusion model pre-training requires source data. Other SFDA methods only use source data to pre-train a source model, but DND uses source data to train a source model and a diffusion model.\n2) You maintain ResNet-101 as the encoder G across all datasets, but ResNet-50 is used as the encoder by other SFDA methods on both Office-31 and Office-Home. So are the performance comparisons on Office-31 and Office-Home unfair (Table 1,2)? And whether the results of other methods in Tables 1,2,3 are your reproduced results? Some of which are different from the results in the original paper (such as NRC++, original paper: 88.1, you reported: 87.8).\n3) To our knowledge, the training of the diffusion model is very time-consuming, could you conduct a runtime analysis between DND and other SFDA methods (eg. DaC and NRC++)?\n4) The target adaptation part in Figure 1 mistakenly divides the inverted triangles into class 0 and the diamonds into class 1.\n5) There is an error in the pseudocode of algorithm 1: if the maximum value of t is T in algorithm 1, then z_{\\alpha_{T+1}} is obtained, but in reality, the algorithm should end after z_{\\alpha_{T}} is obtained."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2015/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2015/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2015/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699624988612,
            "cdate": 1699624988612,
            "tmdate": 1699636132711,
            "mdate": 1699636132711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6yMed7m76y",
                "forum": "Lqwk6tNVEi",
                "replyto": "1oE2gs8vVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6XN2 (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's diligent effort in reviewing our manuscript. Below, we provide our detailed response to your concerns.\n\n\n> ### **Response to Concerns about Violation of SFDA Problem Settings (weakness 1 and question 1)**\n\nIn response to this concern, we direct the reviewer to our official comment in section **C1**, which is detailed in our response to all reviewers.\n\n> ### **Clarity and Structure of the Paper (weakness 2):**\n\nThank you for raising concerns about the clarity of our writing, particularly in the introduction and methods sections. As suggested, we have revised these sections and highlighted the major changes in blue in the revised paper for clarity and conciseness (please download the updated PDF file. It now contains our revised manuscript).\n\nThe revised Introduction concisely presents our research problem and approach, and the Methods section now focuses on our DND method's novelty, reducing the extent of the existing work explanation. We have limited the discussion about IADB to only the key details that are important for understanding our DND. You can see these updates marked in blue in our revised manuscript.\n\n\n> ### **Hyperparameter Sensitivity Analysis (weakness 3):**\n\nThank you for suggesting a hyperparameter sensitivity analysis for our DND and raising concerns about the challenge of selecting suitable parameters. We would like to clarify that we were aware of the potential complexities in diffusion models while developing our DND, which led us to focus on designing a lightweight model with few hyperparameters. Our primary goal is to show how diffusion models can be used as domain shift correctors for SFDA challenges, beyond their traditional role in data density generation, rather than just maximizing performance. We selected hyperparameters to keep the model light, and consequently, our diffusion model implementation involves just three hyperparameters: (1) the number of diffusion steps; (2) the number of neighbors ($k_s$) for source pre-training; and (3) the number of neighbors ($k_t$) for target adaptation.\n\nAs suggested, we have conducted a detailed sensitivity analysis of the three hyperparameters to evaluate their impact on target-domain classification accuracy. This analysis is described in **Appendix H** of our revised manuscript, with the relevant parts marked in blue for ease of reference. Our results suggest that changes in hyperparameter values have a minimal effect on performance. We hope this explanation and our hyperparameter sensitivity analysis address the concerns about the complexity and choice of hyperparameters in our DND.\n\nIn each experiment, we controlled variables as follows: we set $k_s$ to 5 while evaluating $k_t$ and the diffusion steps; $k_t$ was fixed at 5 during the evaluation of $k_s$ and the diffusion steps; and the number of diffusion steps was maintained at 16 when assessing $k_s$ and $k_t$. The analysis results are shown below.\n\n**Table. sensitivity analysis for $k_s$ and $k_t$ on VisDA-2017 (ResNet-101).**\n|  Hyperparameter | 3 | 4 | 5 | 6 | 7 | \n|:------------:|:--------:|:-------:|:--------:|:-------:|:--------:|\n| $k_t$| 87.9 | 88.5 | 89.2 | 88.7 | 88.4 |\n| $k_s$| 88.2 | 88.6 | 89.2 | 88.5 | 88.3 |\n\n**Table. sensitivity analysis for the number of diffusion steps on VisDA-2017 (ResNet-101).**\n|  Hyperparameter | 4 | 8 | 16 | 32 | 64 | \n|:------------:|:--------:|:-------:|:--------:|:-------:|:--------:|\n| diffusion steps | 88.2 | 88.6 | 89.2 | 88.4 | 88.1 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465784876,
                "cdate": 1700465784876,
                "tmdate": 1700669411477,
                "mdate": 1700669411477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aMPLnBGoiN",
                "forum": "Lqwk6tNVEi",
                "replyto": "1oE2gs8vVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6XN2 (2/2)"
                    },
                    "comment": {
                        "value": "> ### **Ablation Study on A More Complex Dataset (weakness 4):**\n\nThank you for recommending the inclusion of ablation studies on a more complex dataset. As suggested, we have now added new ablation studies on the **VisDA-C dataset**, known for its large scale and significant domain shifts, to our updated manuscript. The original Office-31 studies have been moved to **Appendix G**.\n\n**Table. Ablation studies on VisDA-2017 (ResNet-101).**\n|  Method | plane |  bcycl | bus | car | horse | knife | mcycl | person | plant | sktbrd | train | truck | Avg |\n|:------------:|:--------:|:-------:|:--------:|:-------:|:--------:|:-------:|:--------:|:-------:|:--------:|:-------:|:--------:|:-------:|-------:|\n| ResNet-101 | 55.1 | 53.3 | 61.9 | 59.1 | 80.6 | 17.9 | 79.7 | 31.2 | 81.0 | 26.5 | 73.5 | 8.5 | 52.4 |\n| K$-NN only | 97.5 | 91.1 | 88.6 | 74.6 | 97.4 | 96.2 | 90.8 | 81.6 | 92.6 | 92.8 | 91.5 | 49.9 | 87.1 |\n|DND (without Gaussian Prior)| 97.4 | 92.4 | 89.6 | 78.2 | 97.7 | 95.8 | 89.8 | 85.4 | 94.9 | 93.2 | 90.4 | 49.6 | 87.9 |\n|DND (without SiLGA)| 97.5 | 92.7 | 89.2 | 78.7 | 97.1 | 95.2 | 86.6 | 85.4 | 93.8 | 92.7 | 92.3 | 50.9 | 87.7 |\n|DND (Ours)| 98.4 | 92.1 | 86.0 | 83.6 | 98.1 | 96.5 | 93.5 | 82.9 | 97.0 | 95.2 | 92.6 | 54.6 | 89.2 |\n\n> ### **Response to Reviewer\u2019s Questions about Encoder G and Performance Comparisons (question 2)**\n\nWe understand the reviewers' concern about using ResNet-101 as the encoder for all datasets, unlike other SFDA methods that typically use ResNet-50 for Office-31 and Office-Home. Our experiments showed that both ResNet-50 and ResNet-101 have their merits, with no definitive superior choice for Office-31 and Office-Home. We chose ResNet-101 due to its consistent performance across different datasets and to standardize the backbone architecture in our experiments.\n\nAs suggested, we have re-conducted our experiments on Office-31 and Office-Home using ResNet-50 and have also re-run the baseline methods using ResNet-101. The results and further details in response to this concern are included in our official comment to all reviewers in **C2**.\n\n> ### **Response to Concerns about Misreported Results for NRC++ (question 2)**\n\nWe thank the reviewer for pointing out the mismatch in reporting NRC++ results, which were incorrectly taken from the NRC results. We have corrected our tables to accurately reflect NRC++ results and thoroughly reviewed all tables to ensure precise reporting and consistency between cited methods and their results.\n\n> ### **Response to Concerns about Time Complexity (question 3)**\nIn response to this concern, we direct the reviewer to our official comment in section **C4**, which is detailed in our response to all reviewers.\n\n\n> ### **Figure 1 Typo (question 4):**\n\nHuge thanks for the detailed examination by the reviewer.  As suggested, we have updated Figure 1 in our revised manuscript. Now, diamonds represent class 0 and inverted triangles represent class 1. We have highlighted Figure 1 in blue for easier reference in the revised paper.\n\n\n> ### **Algorithm 1 Typo (question 5)**: \n\nWe appreciate the reviewer's careful examination. In response to the suggestion, we have updated Algorithm 1 in the revised manuscript, with the changes highlighted in blue for clarity. For your reference, we have updated Line 2 of Algorithm 1 in the revised manuscript, changing the ending from T to T-1.\n\n\n> ### **Reference:**\n\n[1] Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen Li, Siyuan Li, Liang Lin, and Guanbin Li. Divide\nand contrast: Source-free domain adaptation via adaptive contrastive learning. In Conference on\nNeural Information Processing Systems (NeurIPS), 2022.\n\n[2] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, Shangling Jui, and Jian Yang. Trust your good friends: Source-free domain adaptation by reciprocal neighborhood clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466325630,
                "cdate": 1700466325630,
                "tmdate": 1700605575216,
                "mdate": 1700605575216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2XXklHCdQ1",
                "forum": "Lqwk6tNVEi",
                "replyto": "aMPLnBGoiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
                ],
                "content": {
                    "comment": {
                        "value": "We have already acknowledged the work 3C-GAN that you mentioned. Although they introduce an additional generative model G, G is only trained with target data in the target adaptation phase. In another word, they merely have access to the pre-trained prediction model, which is the setting that all other SFDA methods follow and cannot be changed. Thus, we think 3C-GAN does not deviate from SFDA fundamental settings. Different from 3C-GAN, the diffusion model in DND is trained with source data in the pre-training phase, which is not practical in the real application, because we cannot require the source owner to pre-train any modules except the prediction model for us, including the additional diffusion model in DND."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642106313,
                "cdate": 1700642106313,
                "tmdate": 1700642106313,
                "mdate": 1700642106313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O98PITNUlR",
                "forum": "Lqwk6tNVEi",
                "replyto": "1oE2gs8vVB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 6XN2"
                    },
                    "comment": {
                        "value": "We appreciate your prompt response. \n\nWhile it is true that a source provider needs to perform the additional task of training the DND module, whether this is practical or not can vary depending on perspective.\n\n1. If, as per your assumption, there is a separate provider for the classifier, and they wish for their classifier to adapt well to different target domains, they can simply attach and train the DND module before providing it. Our DND functions as a supplementary module that is integrated with the main classification model. While this requires additional work, it does not significantly add to the overall effort. For instance, when using ResNet-101 as the backbone on the largest benchmark dataset, **VisDA-2017**, the time needed for the source diffusion process is only about 3672.4 seconds for 15 epochs (15 epochs is sufficient for its convergence) ($\\approx$ 1 hour).\n\n2. Additionally, as highlighted in **Appendix E**, incorporating our DND module also improves performance in the source domain. Thus, from the source provider\u2019s standpoint, adding DND could be considered beneficial not just for target domain adaptation but also for enhancing source domain performance.\n\nOur understanding of the definition of source-free domain adaptation (SFDA) is to adapt a model trained on one source domain to a different target domain, without having access to the original source data. This problem focuses on addressing the privacy concern of releasing training data to the public. We would be grateful if the reviewer could share their understanding of SFDA and kindly point out any specific aspects where our paper might diverge from the typical SFDA settings."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675287209,
                "cdate": 1700675287209,
                "tmdate": 1700712440715,
                "mdate": 1700712440715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGDwDIaeep",
                "forum": "Lqwk6tNVEi",
                "replyto": "O98PITNUlR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewers",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission2015/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2015/Reviewer_6XN2"
                ],
                "content": {
                    "comment": {
                        "value": "Our understanding of the definition of SFDA is also to adapt a model (but only one model and just the prediction model) trained on one source domain to a different target domain, without having access to the original source data. The main issue is not whether the training of the diffusion model is easy or not for the source provider. The main point that we want to express is that the whole source pre-training phase is not controlled by us (the target users) but the source provider, in the real world, although all SFDA methods pre-train the source model by themselves for experiments. Thus, for us, we should design the algorithm from the perspective of the target users so that we should focus on the target adaptation phase and cannot do any change for the pre-training phase. We think the point that DND deviates from SFDA settings is the diffusion model is trained with source data and in the pre-training phase. We think DND is more likely belonging to standard UDA methods (with access to both source and target data in the whole training phase).\n\nMoreover, as we all know, the diffusion model is a kind of deep generative model, which has great power for data recovery. If the source provider uses his/her private data to train the diffusion model and provides the whole model to the target users, does this run counter to his/her intention of data privacy protection? It may be a question deserve to think for the authors.\n\nThanks for the author's responses to my comments. Based on the comments of all reviewers and the author's reply, I decided to keep my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2015/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720667053,
                "cdate": 1700720667053,
                "tmdate": 1700720667053,
                "mdate": 1700720667053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]