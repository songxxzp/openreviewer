[
    {
        "title": "DISTA: DENOISING SPIKING TRANSFORMER WITH INTRINSIC PLASTICITY AND SPATIOTEMPORAL ATTENTION"
    },
    {
        "review": {
            "id": "2EZUB7j7zz",
            "forum": "mjDROBU93g",
            "replyto": "mjDROBU93g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_kKsP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_kKsP"
            ],
            "content": {
                "summary": {
                    "value": "this work introduces a Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal Attention. The work did experiments on Cifar."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the work provides two types of spatiotemporal attentions: intrinsic neuron-level attention and network-level attention.\n2.  the work also provides an efficient nonlinear denoising mechanism."
                },
                "weaknesses": {
                    "value": "1. The main weakness of the paper is the novelty, the  SpatioTemporal Attention has been proposed in prior work[1].\n2. The imagenet dataset is widely used in the SNN field, but the work misses it.\n3. Other results in the SPIKFORMER should be compared in the work.\n4. Some recent work is missing, e.g., [2,3]\n[1]Spatial-Temporal Self-Attention for Asynchronous Spiking Neural Networks\n[2]Surrogate Module Learning: Reduce the Gradient Error Accumulation in Training Spiking Neural Networks\n[3]Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network"
                },
                "questions": {
                    "value": "see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698493456726,
            "cdate": 1698493456726,
            "tmdate": 1699636766394,
            "mdate": 1699636766394,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B9VQsn4LC0",
                "forum": "mjDROBU93g",
                "replyto": "2EZUB7j7zz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful review!"
                    },
                    "comment": {
                        "value": "Q1: The main weakness of the paper is the novelty, the SpatioTemporal Attention has been proposed in prior work[1].\n\nA1: We apologize for our lack of knowledge of the reference paper the reviewer provided during the development of our work.  We will reference it in the paper. Nevertheless,  we would like to highlight the key contributions of our paper. \n \n1. Two-level spatiotemporal attention based on neuron-level intrinsic plasticity and network-level spatiotemporal attention: DISTA uniquely integrates the network-level spatiotemporal attention with the intrinsic plasticity mechanism, allowing neurons within the spiking neural network to adaptively adjust their firing thresholds. This feature significantly enhances the model's ability to process and learn from spatiotemporal data, setting it apart from previous models that do not include such a mechanism. As the reviewer has pointed out, the spirit of our network-level spatiotemporal attention is similar to that of the reference paper [1]. Nevertheless, we want to present the difference between our approach and [1]. In [1], spatiotemporal attention is introduced when computing the attention map using keys and queries. Our approach, however, considers spatiotemporal attention when computing keys, queries, and values. As such, these two approaches are complementary. The benefit of the spatiotemporal attention scheme of [1] can be leveraged by our work, which may further improve accuracy. \n\n2. Denoising Approach: DISTA is distinctively designed with a denoising approach tailored for spiking neural networks. This aspect of the architecture aids in reducing the impact of spiking noise, thereby enhancing the robustness and performance of the model, especially in scenarios with varying data quality. In addition, we enable the attention map non-linearity by our proposed denoising method, in which we truncate the values that are smaller than the noise threshold to zero. Furthermore, denoising significantly reduces hardware overhead in the attention layers as shown in the general responses.\n\nQ2: The ImageNet dataset is widely used in the SNN field, but the work misses it.\n\nA2: We conducted an experiment on ImageNet-100 and presented the results in the general responses at the very beginning. \n        \nQ3: Other results in the spikformer should be compared in the work.\n\nA3: We added a detailed hardware energy analysis  in the general responses at the very beginning. \n\nQ4: Some recent work is missing, e.g., [2,3]\n\nA4: Thank you for pointing these out. We will add a review of these works to our paper!\n\n[1]Spatial-Temporal Self-Attention for Asynchronous Spiking Neural Networks \n[2]Surrogate Module Learning: Reduce the Gradient Error Accumulation in Training Spiking Neural Networks \n[3]Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737461946,
                "cdate": 1700737461946,
                "tmdate": 1700742268282,
                "mdate": 1700742268282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5FSEKOwYSt",
            "forum": "mjDROBU93g",
            "replyto": "mjDROBU93g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_5bGj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_5bGj"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DISTA, a novel neural network architecture that synergizes the strengths of Vision Transformer (ViT) with the potential of spiking neurons. DISTA stands out with its \"Denoising Spiking Transformer\" design that integrates Intrinsic Plasticity and Spatiotemporal Attention, optimizing its computational prowess for vision tasks. A distinct innovation lies in its noise-reducing mechanism for computed spatiotemporal attention maps, ensuring more refined performance. Demonstrating its efficacy, DISTA yields notable results on benchmark datasets such as CIFAR10 and CIFAR10-DVS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tIt changed the Linear layer into production of matrix, and the complexity is indeed O(ND^2) or O(TND^2).\n2.\tThis work adopted PLIF to improve the performances.\n3.\tThis work explores two types of spatiotemporal attentions.\n4.\tThis paper is well written."
                },
                "weaknesses": {
                    "value": "The biggest flaw is training for 1000 Epochs on CIFAR100 and CIFAR10.\n\nThe algorithm demonstrates improvements on performance on both CIFAR10 and CIFAR100 datasets; however, the improvement on CIFAR100 is not that significant, and it has not been assessed on larger datasets like ImageNet.\n\nLacking details on complexity, parameters etc."
                },
                "questions": {
                    "value": "Why the model converge so slow (1000 Epochs)\n\nHow's the performance on ImageNet\n\nIs there any improvements on PLIF or just grab it without any changes\n\nPlease add the analysis on complexity, parameters etc"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6684/Reviewer_5bGj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760952117,
            "cdate": 1698760952117,
            "tmdate": 1699636766268,
            "mdate": 1699636766268,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aDcfbyqoi0",
                "forum": "mjDROBU93g",
                "replyto": "5FSEKOwYSt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful review!"
                    },
                    "comment": {
                        "value": "Q1: Why does the model converge so slowly?\n\nA1: We would like to point out that slow convergence is a general property of transformers instead of a characteristic of our DISTA. For comparison, we use spikformer as a baseline to show the relationship between model performance and epoch number. It can be seen that our method has better results and faster convergence speed than spikformer at any epoch checkpoint. The table below shows model test accuracy(%) during training on CIFAR100.\n| Method\\Epoch       | 200  | 300  | 400  | 500  | 600  | 700  | 800  | 900  | 1000 |\n|--------------|------|------|------|------|------|------|------|------|------|\n| Spikeformer  | 72.21| 72.96| 73.76| 74.29| 75.30| 76.17| 77.12| 78.13| 78.36|\n| DISTA        | 72.59| 73.02| 73.77| 74.66| 75.47| 76.57| 77.88| 78.85| 79.15|\n\nQ2: How's the performance on ImageNet?\n\nA2: We conducted an experiment on ImageNet-100 and presented the results in the general responses.\n\nQ3: Are there any improvements on PLIF or just grab it without any changes?\n\nA3: We adopt the Parameterized LIF model as a tool that provides a trainable membrane potential constant in SNNs to support our neuron-level spatiotemporal attention.  Our experiment proves that, together with our Network-level spatiotemporal attention mechanism and denoising function, a trainable membrane potential constant improves the performance of spiking transformers.\n\nQ4: Please add the analysis on complexity and parameters.\n\nA4: We provided a detailed hardware energy and space complexity analysis in the general responses at the very beginning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737011838,
                "cdate": 1700737011838,
                "tmdate": 1700742158850,
                "mdate": 1700742158850,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WnvRdj5wSe",
            "forum": "mjDROBU93g",
            "replyto": "mjDROBU93g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_DFpL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_DFpL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DISTA, a novel architecture for spiking neural networks (SNNs) aiming to improve their performance on vision tasks by utilizing spatiotemporal attention mechanisms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Innovative Architecture: DISTA innovates by integrating spatiotemporal attention at both neuron and network levels, along with a denoising mechanism, which leads to state-of-the-art performance on several datasets.\n\n+ Energy Efficiency: By leveraging the inherent efficiencies of SNNs and denoising mechanisms, DISTA promises to improve energy efficiency, which is crucial for neuromorphic computing.\n\n+ Biologically Inspired: The approach takes inspiration from the biological brain, potentially opening up new avenues for understanding neural processing.\n\n+ High Accuracy: DISTA achieves remarkable accuracy improvements on the CIFAR10 and CIFAR100 datasets, demonstrating the potential of SNNs in complex tasks.\n\n+ Backpropagation Integration: The paper integrates backpropagation-based synaptic and intrinsic plasticity into SNNs, enhancing their learning capabilities."
                },
                "weaknesses": {
                    "value": "- Novelty: I may have missed something, but most of the approach is taken from the spikeformer paper. This work mainly taken the temporal attention window which results in an increase in number of parameters and that may be the reason why the authors get accuracy improvement. I am not sure if the temporal attention has anything to do with it. If yes, it ll be good if the authors can elaborate. Further,there is no result on imagenet dataset. I feel the reason why authors didn't do it was the temporal attention ll explode the overall parameter requirement which becomes unmanageable in alarge dataset. On the other hand, the original spikeformer work has imagenet results. Kindly clarify the scalability of your approach or discuss its limitations.\n\n- Specificity of Application: While DISTA shows significant improvements in vision tasks, its adaptability to other types of tasks is not demonstrated. Can the authors comment on this?\n\n- Overfitting Risk: There's a potential risk of overfitting as the model complexity increases with multiple attention mechanisms, though this is not explicitly discussed in the paper. Can the authors provide their opinion on this?\n\n- Can the authors do some off the shelf energy estimations maybe by using some spiking hardware simulator tools like [1] (its understandable if the authors are not able to do it fully, a qualitative discussion on the efficiency benefits- as in whether they expect sparsity advantage at compute level or memory storage reduction from a hardware point of view will be useful) ? I will be interested to see if their DISTA is actually energy efficient (specifically in terms of memory energy).\n \n[1] https://github.com/RuokaiYin/SATA_Sim"
                },
                "questions": {
                    "value": "see weaknesses above. I am giving a rating of 6 mainly because of novelty concerns. If the authors can provide suitable justification, I ll change my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778640586,
            "cdate": 1698778640586,
            "tmdate": 1699636766143,
            "mdate": 1699636766143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NPRFbwIZdW",
                "forum": "mjDROBU93g",
                "replyto": "WnvRdj5wSe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful review!"
                    },
                    "comment": {
                        "value": "Q1:  I may have missed something, but most of the approach is taken from the spikeformer paper. This work mainly taken the temporal attention window which results in an increase in the number of parameters and that may be the reason why the authors get accuracy improvement. I am not sure if the temporal attention has anything to do with it. If yes, it ll be good if the authors can elaborate. Further, there is no result on the ImageNet dataset. I feel the reason why authors didn't do it was the temporal attention ll explode the overall parameter requirement which becomes unmanageable in a large dataset. On the other hand, the original spikformer work has ImageNet results. Kindly clarify the scalability of your approach or discuss its limitations.\n\nA1: We conducted an experiment on ImageNet-100 and presented the results in the general responses.\n\nQ2: While DISTA shows significant improvements in vision tasks, its adaptability to other types of tasks is not demonstrated. Can the authors comment on this?\n\nA2: Although we only conducted experiments on visual tasks in the paper, our three core methods: neuron-level spatiotemporal attention, network-level spatiotemporal attention, and spatiotemporal attention denoising. They are in principle universal for all transformer architectures and are not limited to the visual realm. In principle, our method could be directly applied to spiking versions of transformers in other fields, such as Natural Language Processing (Bert, GPT), Reinforcement Learning (Decision Transformer), and Time Series Prediction (Temporal Fusion Transformers). We plan to explore some of these additional applications in our future work. \n\nQ3: There's a potential risk of overfitting as the model complexity increases with multiple attention mechanisms, though this is not explicitly discussed in the paper. Can the authors provide their opinion on this?\n\nA3: Although the extra parameters we added in DISTA brought the risk of overfitting, it did not actually happen. Theoretically, neuron-level spatiotemporal attention enhances the nonlinear processing capabilities of neurons, network-level spatiotemporal attention provides short-term memory of past spiking information, and spatiotemporal attention denoising provides noise reduction capabilities as an alternative to softmax. Experimentally, the decent test accuracy and learning curve also proved that overfitting did not occur.\n\nQ4: Can the authors do some off-the-shelf energy estimations maybe by using some spiking hardware simulator tools like [1] (it's understandable if the authors are not able to do it fully, a qualitative discussion on the efficiency benefits- as in whether they expect sparsity advantage at compute level or memory storage reduction from a hardware point of view will be useful)?\n\nA4: We provided a detailed hardware energy analysis in the general responses at the very beginning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736605605,
                "cdate": 1700736605605,
                "tmdate": 1700742107277,
                "mdate": 1700742107277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YMM5qQhXuG",
            "forum": "mjDROBU93g",
            "replyto": "mjDROBU93g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_AvKg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6684/Reviewer_AvKg"
            ],
            "content": {
                "summary": {
                    "value": "The authors have tackled a rather interesting and important problem, which is to design/improve spiking neural networks-based vision transformers. More specifically, the authors have proposed a spiking transformer model that provides node-level and network-wide spatiotemporal attention with denoising. In this work, they consider, a simple yet effective, neuromorphic model called Leaky Integrate-and-Fire that makes use of the temporal information that decays over time. They incorporated a learnable time constant to be used in LIF and a thresholding-based efficient denoising technique. The experiments show a significant improvement over their predecessor, Spikformer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality \n- Incorporating spiking neural networks with the vision transformers is a very recent yet challenging issue. The authors are expanding on the work of Spikformer [1]. Even that work is very recent. The main objective of the paper is to make efficient spiking transformers that take advantage of temporal information.  Previous work is lacking in this regard; however, the authors have tackled this issue and improved the approach. They also involve a denoising technique for the accumulated spatiotemporal maps since they do not use a rather complicated function Softmax. Therefore, I believe this work is reasonably new and the concept itself deserves more attention. \n\nQuality\n- Spiking neural networks, more or less, mimic biological networks. Of course, it all depends on the biological plausibility of the models. For example, leaky integrate and fire models are very efficient neuromorphic models; however, compared to the Hodgkin-Huxley model, they are comparatively less biologically plausible. Thus said, current neuromorphic hardware does take advantage of the concept of LIF models to develop an SNN-based chip, and these models are not just space-efficient but also energy-efficient.\n- Given the progression in transformers, the authors of this paper have shown that making transformers more biologically plausible can indeed have significant advantages over the other neuromorphic approaches. They do so by comparing against the most recent Spikformer [1], and other neuromorphic approaches with variable time steps.\n- Specifically, the authors have integrated learnable time-constant (tau), which is a very important parameter in LIF that decides the significance of the input at the current time-step to the future ones. Further, to make the spatiotemporal attention mechanism less computationally expensive, the spiking networks have discarded Softmax masking, instead, the authors have used a threshold-based masking technique that significantly reduces the computational overhead.\n- The authors have shown significant improvement over the other baseline approaches, and have done extensive ablation studies. \n\nClarity\n- The paper is well-written and straightforward. The objective is clearly stated and tackled reasonably well throughout the paper.\n\nSignificance\n- Since this approach is tackling a very recent and significant challenge of making neural networks more energy-efficient using spiking neural networks, it could have a good impact on the current research.\n\nReferences:\n1. Zhou, Zhaokun, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Li Yuan. \"Spikformer: When spiking neural network meets transformer.\" arXiv preprint arXiv:2209.15425 (2022)."
                },
                "weaknesses": {
                    "value": "Concerns:\n- There are a few things that the authors discussed in the paper; however, are not fully tackled. \n  - Spiking networks by nature use less energy to function since they take advantage of the temporal coding. The authors have discussed this and it is one of the core reasons the community is interested in this, it would be interesting to perform a comparative analysis on hardware to test this approach. The authors have only discussed the accuracy of the approach.\n  - Another thing would be space complexity. There should also be a comparative analysis of the space complexity of the approach. \n- To get a better idea of the significance of the approach, comparative analysis on large benchmarks such as ImageNet would also be interesting to observe.\n  - There are many hyperparameters involved: \ud835\udf2d, u, t, etc."
                },
                "questions": {
                    "value": "The approach signifies the importance of spiking transformers; however, to fully exploit the potential of this approach, I believe, there are certain things to be considered:\n- As mentioned in the weakness section, the paper does lack a rather important comparative analysis of energy-efficiency as mentioned. Please consider that as a measure of performance since it is one of the most crucial properties of the SNNs. \n- I believe, with further analysis of the efficiency, this paper could be a good contribution to the vision community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787513963,
            "cdate": 1698787513963,
            "tmdate": 1699636766002,
            "mdate": 1699636766002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ktNM5FhliK",
                "forum": "mjDROBU93g",
                "replyto": "YMM5qQhXuG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful review!"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful and insightful questions! \n\nQ1: Spiking networks by nature use less energy to function since they take advantage of the temporal coding. The authors have discussed this and it is one of the core reasons the community is interested in this, it would be interesting to perform a comparative analysis on hardware to test this approach. The authors have only discussed the accuracy of the approach. \n\nA1: We provided a detailed hardware energy analysis in the general responses at the very beginning. \n\nQ2: Another thing would be space complexity. There should also be a comparative analysis of the space complexity of the approach.\n\nA2: We provided a detailed space complexity analysis in the general responses.\n\nQ3: To get a better idea of the significance of the approach, comparative analysis on large benchmarks such as ImageNet would also be interesting to observe.\n\nA3: We conducted an experiment on ImageNet-100 and presented the results in the general responses."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736045257,
                "cdate": 1700736045257,
                "tmdate": 1700742029414,
                "mdate": 1700742029414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]