[
    {
        "title": "GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings"
    },
    {
        "review": {
            "id": "1FbOoyCKlH",
            "forum": "c56TWtYp0W",
            "replyto": "c56TWtYp0W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_MEmY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_MEmY"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, authors present a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. A key aspect of contributions lies in the creation of a specialized group embedding scheme designed specifically for transformer architectures. This scheme enables the adaptive learning of concise grouping tokens, which encompass both channel and temporal dimensions. By incorporating group-aware structural information into the representation space, GAFormer enhances the overall understanding and encoding of data patterns."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The method proposed by the authors is intuitive and validated on multiple datasets, and the experimental results prove the effectiveness of GAFormer.\n\n2.GAFormer achieves the adaptive discernment of channel-wise and temporal groupings without relying on any predefined structure or additional supervision beyond classification or regression tasks. Through the assignment of group embeddings to tokens, GAFormer enhances the interpretability of the model's representations."
                },
                "weaknesses": {
                    "value": "1.The GAFormer goes through the SGE and then the TGE, what happens if the order of the two modules is reversed? Why is it modeled from the spatial point of view first?\n\n2.Can the author explain Figure 4 again, I don't really understand the interpretability of the authors' proposal.\n\n3.Table 4 shows that SGE is more effective than TGE, can the authors explain why?"
                },
                "questions": {
                    "value": "See Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563405500,
            "cdate": 1698563405500,
            "tmdate": 1699637055769,
            "mdate": 1699637055769,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GePQut4JqI",
                "forum": "c56TWtYp0W",
                "replyto": "1FbOoyCKlH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MEmY"
                    },
                    "comment": {
                        "value": "Dear reviewer MEmY, \n\nWe appreciate your time and positive comments on the \"effectiveness of GAFormer\" and enhancement of the \"interpretability of the model\u2019s representations\". Based upon your comments, we ran an additional experiment to swap the order of the SGE and TGE operations in GAFormer and report these results below along with additional point-by-point replies to your review.\n\n\n**1. Swapping the order of the SGE and TGE encoders**\n\nThank you for your insightful question! To better understand the effectiveness of SGE and TGE, we conducted experiments to switch the order of our group embedding layers in GAFormer. In this case, we compare our model with SGE followed by TGE to a model where we apply TGE and then SGE.\n\n| | SGE -> TGE  | TGE -> SGE | \n| -------- | ------- | -------- | \n| MotorImagery | 61.0 | 58.0 | \n| SelfRegSCP2 | 56.11 | 48.33 | \n\nIn this experiment, we find that there is a large drop in performance when we switch the order of both operations. This also seems to align with our previous finding inTable 4 where we ablate TGE and SGE and find that SGE provides more significant improvements. Thus, we think that adding SGE at the input layer can have more transformative effects on the model performance, as it learns a discriminative representation for each timepoint by attending to multi-channel relations, which is the basis of the the following TGE block which models inter-timepoint relations. Additionally, we adopt the SGE-TGE architecture to make the model adaptive to different tasks. Some tasks require time-wise representations and some tasks require global representation. We get time-wise representations from the last layer of our model and simply average all timepoints to get global representation. \n\n**2. How group embedding assignments provides insights into the underlying spatial and temporal structure in timeseries datasets**\n\nThank you for your question, which is an important question in our rebuttal. Please refer to general response 2 for an example of neural spiking data. We show that the learned temporal groups can indicate temporal structures in timeseries data in Appendix Figure 6.\n\nTo explain Figure 4, we want to recall that there are 64 channels and 100 timepoints (after the patching layer proposed by [1]) for a single sample in the MotorImagery dataset. For SGE figures (upper row), we have 10 group embeddings and each channel will be assigned with 1 group embedding (the value of Matrix(i, j) indicates the probabilities that channel i is assigned with group j). The visualizations indicate how the 64 channels are grouped within each sample based on the token similarity. Similarly, TGE visulaizations (lower row) indicate how the timepoints are grouped within each sample. And we can also get insights of inter-sample correlations by comparing their visulizations because all samples share the same set of group embeddings.\n\nIn contrast to a standard position embedding (PE) which is **fixed** for each datapoint in the dataset, GE is **adaptive to data** and thus for each token and sequence, we build a data-adaptive embedding through a mixture of the learned group tokens. This in turn reveals different grouping assignments for each input token. We find that when we use a GE, we often obtain sparse assignments across the different group tokens, which provides insights into **how different channels or time points are grouped**. In the temporal embeddings, we find contiguous groups that divide the sequence into different phases: the results on neural population dynamics contains integer-valued bins and also results in very sparse and nearly binary group assignments and often correspond to visible transitions in the data (Appendix Figure 6). This point will be elaborated upon in the revision. \n\n**3. Why SGE is more effective than TGE?**\n\nThank you for your insightful question! We hypothesize that because the temporal structure is naturally preserved across nearby samples in the input sequence, spatial information is more important to harness and learn to inject into the input. Additionally, we hypothesize that the channel-wise relationship is even more critical for classification and regression tasks. This is in contrast to timeseries forecasting tasks [1], where performance remains consistent irrespective of whether the spatial relationship across channels is taken into account. A recent paper [2] also provides extensive discussion on the importance of modeling inter-channel relations.\n\n**Reference**\n\n[1] Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" arXiv preprint arXiv:2211.14730 (2022).\n\n[2]Yong Liu, et al. \u201ciTransformer: Inverted Transformers Are Effective for Time Series Forecasting.\u201d arXiv preprint arXiv:2310.06625."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268352986,
                "cdate": 1700268352986,
                "tmdate": 1700269474308,
                "mdate": 1700269474308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W0r6Mp8mgK",
                "forum": "c56TWtYp0W",
                "replyto": "1FbOoyCKlH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks so much for your constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer MEmY,\n\nThanks so much for your comments and feedback. We hope that our additional experiments on reversed SGE, TGE modules, responses on inter-channel relation modeling and explaination of Figure 4 address your concerns. Let us know if there\u2019s anything further that you would like to see or that we can do. Thanks for all your efforts and time throughout the review process!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602082523,
                "cdate": 1700602082523,
                "tmdate": 1700602082523,
                "mdate": 1700602082523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PbjNqlVAZl",
            "forum": "c56TWtYp0W",
            "replyto": "c56TWtYp0W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_Hkvr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_Hkvr"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel technique for improving the application of transformers to timeseries datasets by learning spatiotemporal structures. The proposed framework employs group tokens and an instance-specific group embedding layer to integrate structure into the learning process. The newly devised Group-Aware transFormer (GAFormer) demonstrates superior performance across various timeseries tasks, offering enhanced interpretability and the ability to discern latent structures in intricate multivariate datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of using *groupe mbedding*(GE) technique to learn both spatial and temporal structure in multi-variate timeseries datasets is novel.\n- The proposed model achieves the state-of-the-art performance on a number of time- series classification and regression tasks"
                },
                "weaknesses": {
                    "value": "Though the approach claims to offer \"a more interpretable decomposition,\" this is a subjective claim. The degree of interpretability might vary based on the user or the specific use case."
                },
                "questions": {
                    "value": "- How to tokenise the MTS data?\n\n- What's the different between a text tokeniser and a MTS tokeniser?\n\n- How to determine K? \n\n- What's the effect of different K?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620852042,
            "cdate": 1698620852042,
            "tmdate": 1699637055644,
            "mdate": 1699637055644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aOVQECMo30",
                "forum": "c56TWtYp0W",
                "replyto": "PbjNqlVAZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hkvr"
                    },
                    "comment": {
                        "value": "Dear reviewer Hkvr, \n\nWe appreciate your time and positive comments to our work. Especially, thank you for identifying our proposed approach as \"novel\", and recognizing the empirical performance of our model.\n\n**1. How to define interpretability**\n\nThanks for your insightful question on the definition of interpretability! Please refer to general response 2 for an example of neural spiking data which can provide some intuition of learned groups. We found the learned temporal groups can indicate the temporal structure of the timeseries data. \n\nWhile we do agree that interpretability is often subjective, the challenges in understanding and extracting the spatiotemporal structure of timeseries data is an outstanding challenge, especially given no pre-assumed underlying structure across channels. We hope our work could further advocate the research along this line, and thus is important to the community.\n\nTo address the reviewer\u2019s concern, we attempted to quantify the interpretability through an additional zero-shot fine-grained classification experiment on a synthetic many-body dataset. Specifically, we only train the model to classify the total energy of the system (high energy v.s. low energy, a binary classification task), and later freeze the weight of the transformer encoder. At test time, we extract the token representation of each channel (object), and then train a KNN classifier (k=5, 8-class problem to classify the x, y axis of 4 objects) to cluster and classify the type of each object using the learned token representation. In this case, we can see how GE impacts the learned token representation without providing any information of the type of channel or additional training.\n\nClassification accuracy with positional embedding and group embedding:\n\n| | Positional Embedding  | Group Embedding |\n| -------- | ------- | -------- | \n| Acc | 61.65 | 71.23 |\n\n\nWe can see that, with group embeddings, the learned token representation contains more information about the type of the channel in our synthetic setting, demonstrating its interpretability. We believe our architecture can be used to understand the spatiotemporal structure of timeseries datasets, giving the potential to perform zero-shot fine-grained classification.\n\n**2. Details of the Time-Series Tokeniser**\n\nThank you for your insightful question! We believe that the choice of a \u201cdefault tokenizer\u201d for time-series is still an active research question, and thus explored two different versions of the tokenizer to make sure that our approach generalizes to both cases. Specifically, we attempted (i) a single-layer MLP to create token representation from a patch of continuous time points of a temporal window from each channel (adopted by PatchTST [1] and GAFormer in our paper); and (ii) a single-layer MLP that creates token representation from all channels of the timeseries at one time point (adopted by MVTS [2], one of the baselines in our paper). The MLP layer is trained end-to-end. Text tokenizer usually builds on a predefined vocabulary and there are discrete word embeddings for each word/token. Different from text tokenizer which process discrete words, time-series tokenizer processes overlapping patches of timeseries, which map continuous timepoints into a higher latent space.\n\n\n\n\n**3. Determine the appropriate size of K**\n\nWe conducted additional experiments to examine the robustness of our model as we vary the number of groups (K). The results are provided for MotorImagery and FaceDetect datasets below. \n\n| Values of K | 5  | 10 | 15 | 20 | 25 |\n| -------- | ------- | -------- | ------- | ------- | ------- |\n| MotorImagery | 55.0 | 61.0 | 62.0 | 63.0 | 58.0 |\n| FaceDetect | 67.02 | 67.99 | 67.08 | 68.53 | 68.19 |\n\nWe found that our method is relatively stable across different values of K, with only small performance degradation when K is too small or too large. When visualizing the different group assignments it appears that larger values of K will produce a number of tokens with very sparse/few assignments, suggesting that additional unnecessary tokens may not be used and thus similar performance is obtained for the overcomplete case. \n\nWe will include these results in an updated version of the paper. Thanks for your suggestion!\n\n**Reference**\n\n[1] Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" arXiv preprint arXiv:2211.14730 (2022).\n\n[2] George Zerveas, et al. \"A transformer-based framework for multivariate time series representation learning.\" In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pp.2114\u20132124 (2021)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267961837,
                "cdate": 1700267961837,
                "tmdate": 1700269447865,
                "mdate": 1700269447865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83p5Y1Yliu",
                "forum": "c56TWtYp0W",
                "replyto": "PbjNqlVAZl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks so much for your constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer Hkvr,\n\nThanks so much for your comments and feedback. We hope that our additional experiments on different K selection and responses on MTS tokenization address your concerns. Let us know if there\u2019s anything further that you would like to see or that we can do. Thanks for all your efforts and time throughout the review process!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601870232,
                "cdate": 1700601870232,
                "tmdate": 1700601870232,
                "mdate": 1700601870232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9S4NdqYGdt",
            "forum": "c56TWtYp0W",
            "replyto": "c56TWtYp0W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to learn PEs for transformers in the multivariate temporal data setting. Specifically, the authors propose a Group Embedding (GE) to capture the group aware dynamics along the channels and also the temporal dimension. This Group Embedding is then induced into the transformer architecture called GAFormer to learn the spatial and temporal structure of the multivariate time series datasets. Experimental results on the synthetic dataset show the robustness of the proposed GE over baseline PEs. Moreover, when GE is induced in other transformer architectures for the univariate datasets the performance is enhanced. GAFormer also achieves SoTA performance on the multivariate datasets tested. The authors also claim the interpretability of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Promising PE for multivariate time-series data. The PE show improved performance over the tasks.\n- The proposed GAFormer also achieves SoTA performance on the multivariate datasets tested."
                },
                "weaknesses": {
                    "value": "- Since the model is designed to learn the group structure in a data-dependent manner, it may not be able to capture these properties effectively in the low data regime as the baseline PEs would.\n- The group structure exhibited in Figure 4 is claimed to provide interpretability of the model. However, these are some groups that have been learnt and the interpretability aspect is not clear as to what it means if some channels or time signals are grouped. This is similar to the groups formed in any attention mechanism and the method doesn\u2019t seem to provide any added explanations or interpretability to the decisions. Thus it may not be right to claim interpretability based on group embedding structure."
                },
                "questions": {
                    "value": "- The authors don\u2019t seem to have mentioned the computational complexity of the method in the paper. From the description, it seems to inherit the quadratic computational complexity of the transformer which is prohibitive for large timesteps and multi chaneled data\n- The group structure exhibited in Figure 4 is claimed to provide interpretability of the model. However, these are some groups that have been learnt and the interpretability aspect is not clear as to what it means if some channels or time signals are grouped. Further analysis and explanation by the authors may help.\n- As the authors have mentioned other mechanisms (spot attention etc.) to induce group structure, I would expect some empirical study to compare the proposed method with these PEs. Specifically, is the proposed method a unique way to induce PEs for multivariate time series forecasting or could other techniques provide similar or better performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8459/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8459/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800417925,
            "cdate": 1698800417925,
            "tmdate": 1699637055496,
            "mdate": 1699637055496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4GWFxqXzGM",
                "forum": "c56TWtYp0W",
                "replyto": "9S4NdqYGdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cXyc"
                    },
                    "comment": {
                        "value": "Dear reviewer cXyc, \n\nWe appreciate your time and feedback on our work. Specifically, thank you for commenting on our approach as \"promising\", and recognizing the improved performance. We provide point-by-point responses to your comments below.\n\n**1. Sensitivity to amount of training data**\n\nOur results are provided for datasets across a wide range of sizes, lengths, and numbers of channels. In all of these cases, we find improvements over standard positional embedding (PE) which we think highlights the flexibility of the approach and its ability to work well even in limited data regimes. For example, the MotorImagery dataset has 278/100 training/test samples, SelfRegSCP2 has 200/180 training/test samples, NLB-RTT has 810/270 training/test samples. Timeseries datasets are usually small compared to the large-scale  text or image datasets.  Exploring the sample efficiency of GE vs PE would be an interesting line of future work.\n\n**2. Interpretability**\n\nWe thank you for your comments and agree that our argument for GE providing enhanced interpretability could be explained more clearly. Please refer to general response 2 for an example of neural spiking data. We show that the learned temporal groups can indicate temporal structures in timeseries data in Appendix Figure 6.\n\nIn particular, we hope to better highlight the ability of the model to capture both temporal structure and spatial structure in the data. Our approach can do this in two ways: (i) through the choice of assignments to group tokens, and (ii) by improving the representations learned by the model to enhance interpretability or explainability in other ways. \n\nIn our extended supplemental figures, we now include more examples and visualizations of the time-series data along with the group assignment coefficients. In these examples, we can see how the switches in dynamics across the time-series are also captured in the group assignment matrix.\n\n**3. Transformer for large timesteps and multi channel data**\n\nThanks for your question. While our proposed approach does inherit the quadratic computational complexity of the transformer, we would like to point out there are other potential (and maybe better) methods to build GE and learn the group assignment, e.g., CNN, Linear layer, which can be more computing-efficient than transformer. We adopt transformer to build GE because we want to keep the model architectures concise and consistent. The main point of this paper is that group embeddings are effective for enhancing timeseries transformers. Therefore we didn\u2019t search a lot on the building blocks. Designing efficient GE module remains an open question and will be explored in future works.\n\nIn the case of large timesteps and multivariate data, PatchTST [1] introduced the method of patching a block of timesteps into a token. This approach significantly reduces the number of input tokens and concurrently enhances model generalization. Our method incorporates a plug-and-play GE module which is relatively light-weight compared to the foundation transformer backbone where it\u2019s added on. Also, the number of tokens for timeseries data are still much lower than the tokens of LLMs, so we don\u2019t worry about the computability of timeseries transformers.\n\n\n**4. Comparison to traditional PE** \n\nWe would like to clarify that our baselines and ablations provide different variants of position embedding both in time and in space. However, comparisons with other methods mentioned like slot attention have not been applied to time series datasets and extending them to work across channels is not a trivial task.  To the best of our knowledge, we are the first to explore the concept of group embedding in time-series where there is no spatial information or known correlations across \u201cnearby channels\u201d and thus there are no other PE baselines to compare against.\n\n**Reference**\n\n[1] Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" arXiv preprint arXiv:2211.14730 (2022)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267806396,
                "cdate": 1700267806396,
                "tmdate": 1700269430979,
                "mdate": 1700269430979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ljl2ZwxM8c",
                "forum": "c56TWtYp0W",
                "replyto": "9S4NdqYGdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks so much for your constructive feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer cXyc,\n\nThanks so much for your comments and feedback. We hope that our updates of appendix and responses on sensitivity of training data, computational complexity and computability for large timesteps/channels address your concerns. Let us know if there\u2019s anything further that you would like to see or that we can do. Thanks for all your efforts and time throughout the review process!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601759168,
                "cdate": 1700601759168,
                "tmdate": 1700601759168,
                "mdate": 1700601759168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xdnHpOXuCC",
                "forum": "c56TWtYp0W",
                "replyto": "ljl2ZwxM8c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Reviewer_cXyc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my comments and addressing all of them. After reading the paper and rebuttal once again, I find no need for further clarification and will maintain the current score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692781582,
                "cdate": 1700692781582,
                "tmdate": 1700692781582,
                "mdate": 1700692781582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rpQoKtw8P8",
            "forum": "c56TWtYp0W",
            "replyto": "c56TWtYp0W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_q3iU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_q3iU"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to improve the transformers to time-series datasets by learning spatiotemporal structure. Specifically, the authors introduce an instance-specific group embedding layer that assigns input tokens to a small number of group tokens to incorporate structure into learning. Based on the group embedding layer, they introduce the GAFormer to incorporate both spatial and temporal group embeddings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies the multivariate time series which is an interesting and important problem.\n2. The paper provides a detailed literature review and\n3. The paper discusses its limitations.\n4. The paper provides extensive experimental results to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1, The proposed methodology is not well motivated and built on the existing Transformer. The paper tries to introduce the group embedding layer but does not provide clear motivation for the group embedding, i.e. how to define the \"group\" and why we need the group embedding.\\\n2. The explanation of the group embedding is vague. It is not easy to understand what the group embedding stands for.\\\n3. The paper aims to learn generalizable representation within multivariate datasets. I did not find the related experiments to demonstrate the generalization ability."
                },
                "questions": {
                    "value": "1. The authors claim that the group embedding layer can assign input tokens to a small number of group tokens to incorporate structure into learning. I wonder how the structure information can be incorporated using group embedding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902052968,
            "cdate": 1698902052968,
            "tmdate": 1699637055385,
            "mdate": 1699637055385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OqKJvQT1N1",
                "forum": "c56TWtYp0W",
                "replyto": "rpQoKtw8P8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer q3iU"
                    },
                    "comment": {
                        "value": "Dear reviewer q3iU, \n\nWe appreciate your time and insightful comments on our work. We are pleased that you identified our problem as \u201cinteresting and important\u201d, and recognized the \u201cextensive experimental results to demonstrate the effectiveness of the proposed method\u201d. In what follows, we provide a point-by-point response to your questions and concerns.\n\n**1. Additional motivation**\n\nThank you for raising this point about needing to improve the explanation behind group embeddings (GE). We plan to revise the introduction and methods to provide more intuition and explanation of why group embeddings are needed for multivariate timeseries.\n\nOur approach is centered on developing data-adaptive spatiotemporal embeddings, which augment the conventional positional embeddings typically employed in sequential data processing. This is achieved by creating a compact set of universal embeddings that are dynamically combined, often through a sparse linear composition. This process results in customized positional embeddings, varying across different sequences, allowing the model to apply a \"group embedding\" either to specific channels or points in time. This can be perceived as a form of soft clustering, where input tokens are aligned with group tokens, leading to a shared or similar learned positional embedding. This methodology enables diverse group assignments across various channels and temporal segments when applied to multivariate timeseries, facilitating efficient grouping or segmentation through our approach.\n\nWe believe our results underscore the need for new methods to learn spatiotemporal structures that are distinct for each instance, and can be adapted to produce embeddings that are not tied to their absolute position in the sequence. This is particularly pertinent in multivariate timeseries data, where the significance of channels may fluctuate between different classes and across training and testing datasets. Consequently, we believe that both our group embedding approach and the GAFormer architecture present significant advancements, particularly for the timeseries analysis community, where standard Positional Embeddings (PEs) may prove inadequate.\n\n**2. Generalizable representation**\n\nThanks for your question. By generalization, we were referring to the model\u2019s ability to perform well at test time on a new set of sequences which were not seen by the model in training time. In timeseries, the noise distribution and channel relevance can change abruptly and thus models that aren\u2019t robust to these types of changes in the input may suffer at test time. We will make this clear in the revised paper. Thank you!\n\n**3. Clarification of how we incorporate structure into learning through the GE**\n\nThank you for your question about \u201chow the structure information can be incorporated using group embedding\u201d. Intuitively, when we solve this assignment problem, multiple channels (SGE) or timepoints (for TGE) will have similar group embeddings and adding this to each input token will induce some underlying shared structure across the tokens with the same group assignments. We will make this more clear in our revision. \nAdditionally, we ran two new experiments on our synthetic many-body dataset to provide further insight into how GEs shape learning. Specifically, we train the model to classify the total energy of the system (high energy v.s. low energy, a binary classification task), and then freeze the weight of the transformer encoder. At test time, we extract the token representation of each channel (object), and then train a KNN classifier (k=5, 8-class problem to classify the x, y axis of 4 objects) to classify the type of each object using the learned token representation. In this case, we can see how GE impacts the learned token representation without providing any information of the type of channel or additional training. \n\nClassification accuracy with positional embedding and group embedding:\n| | Positional Embedding  | Group Embedding |\n| -------- | ------- | -------- | \n| Acc | 61.65 | 71.23 |\n\n\nWe find that there is a nearly 10\\% improvement in our prediction of the channel type when we add GE. These results show that GE shapes the tokens learned at the input to be more aware of structure in the channels, resulting in improved accuracy in the task the model is trained on and in this auxiliary task of interest."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267567042,
                "cdate": 1700267567042,
                "tmdate": 1700267567042,
                "mdate": 1700267567042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ovxt3WliQr",
                "forum": "c56TWtYp0W",
                "replyto": "rpQoKtw8P8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks so much for your constructive feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer q3iU,\n\nThanks so much for your comments and feedback. We hope that our additional experiments on synthetic many-body dataset and responses on motivation behind GE and generalizable representation address your concerns. Let us know if there\u2019s anything further that you would like to see or that we can do. Thanks for all your efforts and time throughout the review process!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601404916,
                "cdate": 1700601404916,
                "tmdate": 1700601404916,
                "mdate": 1700601404916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K0C3Kgqksc",
            "forum": "c56TWtYp0W",
            "replyto": "c56TWtYp0W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_Jppq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8459/Reviewer_Jppq"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel transformer architecture called GAFormer, which models the interations of spatial and temporal groups separately. The proposed model demonstrates effectiveness on both synthetic experiments and real-data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is clearly written and motivated. \n2. The group embedding is novel and very effective as shown by synthetic experiments."
                },
                "weaknesses": {
                    "value": "1. The details of transformer used seems to be missing. see questions\n2. Why does this model not compare to (Nie et al.), which seems to be an important related work."
                },
                "questions": {
                    "value": "1. Does this model consisted of 2 layers of transformer? SGE and TGT?\n\n2. What's the specification of these transformers? It should be stated in the main paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699495913563,
            "cdate": 1699495913563,
            "tmdate": 1699637055267,
            "mdate": 1699637055267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DiDR9Mu2My",
                "forum": "c56TWtYp0W",
                "replyto": "K0C3Kgqksc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jppq"
                    },
                    "comment": {
                        "value": "Dear reviewer Jppq, \n\nWe appreciate your time and positive comments on our work. Thank you for recognizing our proposed strategy as \u201cnovel and very effective\u201d! We provide point-by-point replies to your concerns and questions below.\n\n**1. Details and specifications of our architecture**\n\nOur GAFormer architecture is built based on a cascaded Spatial transformer and a Temporal transformer, with group embedding modules (SGE, TGE) added to both encoders. Specifically, our SGE/TGE module is typically a 3-layer, 8-head, 256-dim transformer with K=10 groups. Please refer to the updated Appendix for more details on the hyperparameters used in our experiments. We will also make sure to include these details in the revised paper. Thanks for your feedback!\n\n**2. Comparison to PatchTST**\n\nWhile PatchTST has a similar channel separated encoder design, because it is designed for forecasting, we couldn\u2019t directly report numbers from their work on the classification and regression tasks considered here. For our initial submission, we implemented a supervised variant of PatchTST [2] and reported those results in Table 2, where we demonstrate superior performance. Thanks to your comment, we ran additional experiments to also compare with supervised PatchTST for the univariate datasets in Table 1 and the neural datasets in Table 3 (see below). We find that across the board, GAFormer and group embeddings maintain their top performance and the patching-based model without group embeddings. We believe that these results help to further demonstrate the power of our approach.\n\nAdditional Baselines for Table 1:\n| | InlineSkate  | Earthquakes | Adiac |\n| -------- | ------- | -------- | ------- |\n| PatchTST | 21.64 | 68.35 | 65.47 |\n| PatchTST + TGE | 21.64 | 69.06 | 69.82 |\n| $\\Delta$ | $\\uparrow$ 0.00 | $\\uparrow$ 0.71 | $\\uparrow$ 4.35 |\n\nAdditional Baselines for Table 3:\n\n| | C-1  | C-2 | M-1 | M-2 | NLB-Maze | NLB-RTT |\n| -------- | ------- | -------- | ------- | -------- | ------- | -------- |\n| PatchTST (P=5) | 59.38 | 83.33 | 78.57 | 67.44 | 87.57 | 33.40 |\n| PatchTST (P=1) | 78.13 | 86.11 | 88.10 | 72.09 | 88.19 | 43.16 |\n| GAFormer (P=1) | 81.25 | 94.44 | 92.86 | 88.37 | 91.36 | 54.33 |\n\n\n**References**\n\n[1] Liu, Ran, et al. \"Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers.\" Advances in neural information processing systems 35 (2022): 2377-2391.\n\n[2] Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" arXiv preprint arXiv:2211.14730 (2022)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700266660269,
                "cdate": 1700266660269,
                "tmdate": 1700266676682,
                "mdate": 1700266676682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6VXfAPm3J7",
                "forum": "c56TWtYp0W",
                "replyto": "K0C3Kgqksc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks so much for your constructive feedback"
                    },
                    "comment": {
                        "value": "Thanks so much for your comments and feedback. We hope that our additional experiments on PatchTST comparison and updates of model architectures address your concerns. Let us know if there\u2019s anything further that you would like to see or that we can do. Thanks for all your efforts and time throughout the review process!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600907151,
                "cdate": 1700600907151,
                "tmdate": 1700600907151,
                "mdate": 1700600907151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]