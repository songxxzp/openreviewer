[
    {
        "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"
    },
    {
        "review": {
            "id": "DxjwRhZDNd",
            "forum": "FAO4VS9QRV",
            "replyto": "FAO4VS9QRV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission671/Reviewer_SSSB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission671/Reviewer_SSSB"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new efficient fine-tuning method for large language models. Compared to LoRA, Delta-LoRA will fine-tune both the adapter and the original weight of the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing of this paper is good, making the paper easy to follow. \nThe method is reasonable and easy to reproduce, and the experiments demonstrate the efficiency of the method.\nThe challenge with directly fine-tuning a large language model is the memory cost. This method provides a way to update LoRA to fine-tune the backbone. In this case, the model can train the backbone directly without incurring a significant GPU memory cost."
                },
                "weaknesses": {
                    "value": "Since LoRA is a framework, it would be beneficial if the author could apply Delta-LoRA to more large language models.\nThe paper doesn't clearly explain why Delta-LoRA works.\nIn terms of novelty and significance, I believe it is borderline."
                },
                "questions": {
                    "value": "Can the author apply Delta-LoRA to llama2 and provide the experimental results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Reviewer_SSSB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission671/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698296491978,
            "cdate": 1698296491978,
            "tmdate": 1699635994294,
            "mdate": 1699635994294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5cmWoAKlwG",
                "forum": "FAO4VS9QRV",
                "replyto": "DxjwRhZDNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer SSSB"
                    },
                    "comment": {
                        "value": "We are grateful for your constructive comments. We have revised our paper according to your constructive suggestions.\n\n\n**Strengths:**\n*The writing of this paper is good, making the paper easy to follow. The method is reasonable and easy to reproduce, and the experiments demonstrate the efficiency of the method. The challenge with directly fine-tuning a large language model is the memory cost. This method provides a way to update LoRA to fine-tune the backbone. In this case, the model can train the backbone directly without incurring a significant GPU memory cost.*\n\n**Question 1:** *Since LoRA is a framework, it would be beneficial if the author could apply Delta-LoRA to more large language models. The paper doesn't clearly explain why Delta-LoRA works. In terms of novelty and significance, I believe it is borderline.*\n\n**Our Reply:**\n\n\nWe have used a variety of language models(e.g. GPT-2 and Bart-Large as well as LLaMA-7B with 7 Billions parameters) to conduct experiments with our Delta-LoRA. Based on the experimental results, we believe that our Delta-LoRA can generalize well on other LLMs.\n\n\nWe would like to clarify the following three strengths of Delta-LoRA.\n- **Mathmaticlly, $g_W =  g_{AB}$.** Therefore, it is reasonable to update the pretrained $W$ by $\\Delta AB$.\n- **Delta-LoRA showcases the lowest cosine similarity with the pretrained model.** According to our understanding experiment in Section 5.4, we found that Delta-LoRA has the lowest similarity with the pretrained model among all baselines, underscoring that our approach induces the most significant modifications to the final matrix $\\boldsymbol{W}^{\u2217}$.\n- **Delta-LoRA shows better performance compared with LoRA on LLaMA-7B.** According to the evaluation experiment in our paper, we found that Delta-LoRA can yield higher evaluation result than LoRA with Alpaca dataset. \n\n**Question 2:** *Can the author apply Delta-LoRA to llama2 and provide the experimental results?*\n\n**Our Reply:**\nThank you very much for your suggestion. We have tested the LLaMA-7B, following the setting in most papers. The results can be found in Section 5.1 in the revised manuscript. We can see that our Delta-LoRA has around 10\\% improvement compared with LoRA evaluated by LLMs.\n\n*We will really appreciate if you can reconsider your score based on our revisions.*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551431255,
                "cdate": 1700551431255,
                "tmdate": 1700551431255,
                "mdate": 1700551431255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wW3aswsNDu",
                "forum": "FAO4VS9QRV",
                "replyto": "DxjwRhZDNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Questions?"
                    },
                    "comment": {
                        "value": "*We hope our response addresses your concerns; if so, we would really appreciate it if you would reconsider your score accordingly. Please let us know if you have additional questions.*"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701468080,
                "cdate": 1700701468080,
                "tmdate": 1700701468080,
                "mdate": 1700701468080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "16TNo3w6f0",
                "forum": "FAO4VS9QRV",
                "replyto": "wW3aswsNDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_SSSB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_SSSB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response; I would like to maintain my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710205347,
                "cdate": 1700710205347,
                "tmdate": 1700710205347,
                "mdate": 1700710205347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SF1cgqgOzG",
            "forum": "FAO4VS9QRV",
            "replyto": "FAO4VS9QRV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve LoRA by also updating the pretrained weights $W$ in addition to the low-rank adapters $A$ and $B$: $W^{(t+1)} = W^{(t)} + \\text{coefficient} \\cdot (A^{(t+1) B^{(t+1)} - A^{(t) B^{(t)}})$. Experiments are conducted to compare the proposed method (Delta-LoRA) and other baselines including LoRA, DyLoRA, and AdaLoRA"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is very simple but seems to be effective as shown in the experiments.\n2. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. [Major] **Delta-LoRA seems to be fundamentally equivalent to LoRA.** I am very unclear about the fundamental difference between updating the pretrained weights using Delta-LoRA and LoRA. Basically, **both of them are performing low-rank updates**. Updating the pretrained weights does not seem to have any benefits, and if we treat W+AB as a whole $\\hat{W}$, then, what you are doing can be viewed as LoRA but applied with a different learning rate. Please correct me if I am wrong. \n2. [Major] **Many experiment details are not justified.**\n    1. Neither DyLoRA nor AdaLoRA outperforms LoRA as they claimed in their paper. I noticed that the authors chose different settings with the AdaLoRA, this might be the reason for the bad performance. However, the authors do not provide any explanation on why they chose this different setting. This makes the outperformance of Delta-LoRA reported in the tables less convincing. \n    2. Why does the author only try the experiments with cases with rank = 8? How about other cases? \n    3. Why is the learning rate fixed? In my intuition, the only difference between the Delta-LoRA seems to be the resulting learning rate as I explained in the first bullet point. Therefore, it is unclear whether the reported performance of Delta-LoRA happen to outperform LoRA in this learning rate. \n    4. Why are the trainable parameters of Delta-LoRA set to be higher than LoRA in multiple experiments? (Table 1, 4). It is very unclear whether the improvement comes from the additional trainable parameters or extra updatable parameters (or dropouts). \n3. [Medium] The improvements look not significant. \n4. [Medium] The code is not publically available thus I cannot verify the reproducibility of the experiment results. \n5.  [Minor] Listing extra updatable parameters as a column in the tables looks very confusing. For example, Table 4 made me think that Delta-LoRA uses 72M updatable parameters but achieves worse results than Fine-Tuning in the third row."
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission671/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705970391,
            "cdate": 1698705970391,
            "tmdate": 1700678545636,
            "mdate": 1700678545636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IPD6h9X8Gt",
                "forum": "FAO4VS9QRV",
                "replyto": "SF1cgqgOzG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer e3re (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable and constructive comments. \n\n**Question:** \n*[Major] Delta-LoRA **seems to be fundamentally equivalent to LoRA**. I am very unclear about the fundamental difference between updating the pretrained weights using Delta-LoRA and LoRA. Basically, **both of them are performing low-rank updates**. Updating the pretrained weights does not seem to have any benefits, and if we treat W+AB as a whole, then, what you are doing can be viewed as LoRA but applied with a different learning rate. Please correct me if I am wrong.*\n\n\n\n\n**Our Reply:**\nWe would like to clarify some points and help you understand our Delta-LoRA better. We want to point out that Delta-LoRA is fundamentally different from LoRA in the following three aspects:\n\n - Given $\\boldsymbol{W + AB}$, $\\boldsymbol{W}$ is fixed in LoRA, while $\\boldsymbol{W}$ will be updated in our Delta-LoRA. This is **the largest difference between LoRA and Delta-LoRA**. This modification can enhance the representative ability of the model.\n - $\\text{Rank}(\\Delta \\boldsymbol{W_{Delta-LoRA}}) = \\text{Rank}(\\boldsymbol{W}^{(T)} - \\boldsymbol{W}^{(0)} + \\boldsymbol{AB})>\\text{Rank}(\\Delta \\boldsymbol{W}_{LoRA}) = \\text{Rank}(\\boldsymbol{AB})$. The rank of the learned incremental weight matrix in our Delta-LoRA is larger than that in the original LoRA.\n - The gradient flow is different between LoRA and Delta-LoRA. We can know that $\\frac{\\partial \\mathcal{L}}{\\partial A} = B^{\\top}\\cdot\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial B} = \\frac{\\partial \\mathcal{L}}{\\partial W}\\cdot A^{\\top}$ according to the chain rule.\n\nHere is the **back-propagation process of LoRA**: $\\frac{\\partial \\mathcal{L}}{\\partial A^{(t+1)}}= \\frac{\\partial \\mathcal{L}}{\\partial W^{(t+1)}}\\cdot (B^{(t+1)})^{\\top}=\\frac{\\partial \\mathcal{L}}{\\partial W^{(0)}+(A^{(t)}+\\Delta A^{(t)})(B^{(t)} + \\Delta B^{(t)})}\\cdot (B^{(t)}+\\Delta (B^{(t)}))^{\\top}$\n$\\frac{\\partial \\mathcal{L}}{\\partial B^{(t+1)}} = (A^{(t+1)})^{\\top} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(t+1)}}=(A^{(t)}+\\Delta (A^{(t)}))^{\\top} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(0)}+(A^{(t)}+\\Delta A^{(t)})(B^{(t)} + \\Delta B^{(t)})}$ \nHere is the **back-propagation process of Delta-LoRA**:\n$\\frac{\\partial \\mathcal{L}}{\\partial A^{(t+1)}} = \\frac{\\partial \\mathcal{L}}{\\partial W^{(t+1)}}\\cdot (B^{(t+1)})^{\\top}=\\frac{\\partial \\mathcal{L}}{\\partial ((W^{(t)}+ \\lambda \\Delta A^{(t)}B^{(t)}) + (A^{(t)}+\\Delta A^{(t)})(B^{(t)}+\\Delta B^{(t)})) }\\cdot (B^{(t)}+\\Delta (B^{(t)}))^{\\top}$ \n$\\frac{\\partial \\mathcal{L}}{\\partial B^{(t+1)}} = (A^{(t+1)})^{\\top} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(t+1)}} = (A^{(t)}+\\Delta (A^{(t)}))^{\\top} \\cdot \\frac{\\partial \\mathcal{L}}{\\partial ((W^{(t)}+ \\lambda\\Delta A^{(t)}B^{(t)}) + (A^{(t)} + \\Delta A^{(t)})(B^{(t)} + \\Delta B^{(t)})  )}$.\n\n\n\n\n**Question 2.1:** *Neither DyLoRA nor AdaLoRA outperforms LoRA as they claimed in their paper. I noticed that the authors chose different settings with the AdaLoRA, this might be the reason for the bad performance. However, the authors do not provide any explanation on why they chose this different setting. This makes the outperformance of Delta-LoRA reported in the tables less convincing.*\n\n\n\n**Our Reply:**\n\n***For a fair comparison, we use the same training setups as LoRA. The AdaLoRA and DyLoRA chose different training settings as LoRA. In the following, we want to clarify the differences among them.*** \n\nFirst, we want to point out that\n\n1. DyLoRA aims to improve the performance of lower rank when inference rather than the rank they selected at the training stage. That means the performance they claimed in their paper was based on a higher rank, which can yield a much more memory overhead. For a fair comparison, we report their results based on the low-rank setting. \n\nSecond, AdaLoRA uses a different setting from LoRA, the differences are summarized as:\n\n1. The AdaLoRA fine-tuned each layer of the pretrained matrices in Transformer. This operation will cause extra **activation memory** consumption (i.e. **19.79 GB extra GPU memory** for LLaMA-7B with batch size of 1024 and 1024 max length implemented by Pytorch). Therefore, we do not choose the similar training setups with AdaLoRA. Instead, we used the same training setups with LoRA.\n2. AdaLoRA has 7 extra hyper parameters (i.e. target_rank, reg_orth_coef, init_warmup, final_warmup, mask_interval, beta1, beta2). We cannot search the best hyper-parameters for the different datasets and models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551211764,
                "cdate": 1700551211764,
                "tmdate": 1700551211764,
                "mdate": 1700551211764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u5hNmCzPIF",
                "forum": "FAO4VS9QRV",
                "replyto": "SF1cgqgOzG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer e3re (Part 2)"
                    },
                    "comment": {
                        "value": "**Question 2.2:** *Why does the author only try the experiments with cases with rank = 8? How about other cases?*\n\n**Our Reply:**\n\nThanks for your constructive comments. We would like to emphasize that we use the same training setups with LoRA. We use the rank=8 on 8 datasets in GLUE benchmark, while rank=4/8 for NLG tasks. *We explore more ranks in the following works. The table in the following is the experiment that we use different ranks on CoLA dataset.* \n\n| Rank | LoRA | Delta-LoRA |\n|----------|----------|----------|\n| 8 | 63.17 | **63.82** |\n| 16 | 63.06 | **64.30** | \n| 32 | 63.81 | **64.79** | \n| 64 | 63.39 | **65.59** | \n\nWe can see a better choice of rank can further improve Delta-LoRA.\n\n**Question 2.3:** *Why is the learning rate fixed? In my intuition, the only difference between the Delta-LoRA seems to be the resulting learning rate as I explained in the first bullet point. Therefore, it is unclear whether the reported performance of Delta-LoRA happen to outperform LoRA in this learning rate.*\n\n**Our Reply:**\n\nTo make fair comparison, we use the same learning rate as reported in the original paper and code(e.g. 2e-4 on E2E Challenge dataset). Besides, we apply linear learning rate scheduler at the training stage, which is same as the setting of LoRA. We could use better hyper-parameters to further improve the results of our Delta-LoRA with standard errors. \n\n\n| MNLI| SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B | AVG|\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| 87.62 $\\pm$ 0.21 | 95.29 $\\pm$ 0.23 | 90.60 $\\pm$ 0.14 | 64.64 $\\pm$ 0.86 | 93.09 $\\pm$ 0.15 | 91.01 $\\pm$ 0.06 | 87.00 $\\pm$ 0.36 | 91.61 $\\pm$ 0.04 | 87.60 |\n\n\n\n\n**Question 2.4:** *Why are the trainable parameters of Delta-LoRA set to be higher than LoRA in multiple experiments? (Table 1, 4). It is very unclear whether the improvement comes from the additional trainable parameters or extra updatable parameters (or dropouts).*\n\n**Our Reply:**\n\nThe **Extra Updatable Trainable Parameters** means the adjustable pretrained parameters, which we update by using the delta of low rank matrices, without extra cost of GPU resources. For example, since LoRA couldn't update the pretrained $\\boldsymbol{W}$, so that its **Extra Updatable Trainable Parameters** are **0**. The Table 6 shows the **Delta-LoRA+LoRA module**, which means that we add the Dropout layer in Delta-LoRA module. As we reported, only using the Delta-LoRA update strategy indeed prompts the model to have better performance, while removing the Dropout layer can have more performance gain.\n\n\n**Question 3.1:** *[Medium] The improvements look not significant.*\n\n**Our Reply:**\n\nWe have decent improvement on most NLG datasets (e.g. 1.24 on E2E, 0.91 on WebNLG, 10.4\\% on LLaMA-7B with Alpaca dataset), and some NLU datasets (e.g. 0.65 on CoLA). More importantly, we use the same learning rate and same model architecture with LoRA, we do not search the best hyper-parameter and show the real performance of our Delta-LoRA in our paper. The Delta-LoRA has consistent performance improvement over 4 NLG datasets and 8 NLG datasets, which is a general method and can be applied to multiple domains.\n\n\n**Question 3.2:** *[Medium] The code is not publically available thus I cannot verify the reproducibility of the experiment results.*\n\n\n**Our Reply:**\n\nWe will release our models and all codes after the decision of this conference.\n\n\n\n**Question 3.3:** *[Minor] Listing extra updatable parameters as a column in the tables looks very confusing. For example, Table 4 made me think that Delta-LoRA uses 72M updatable parameters but achieves worse results than Fine-Tuning in the third row.*\n\n\n**Our Reply:**\n\nWe would like to modify our paper to clarify the main ideas and contributions as well. Generally, **The Extra Updatable Trainable Parameters** claimed in our paper means the adjustable pretrained parameters **but does not consume more memory**, which we update by using the delta of low rank matrices. \n\n*We will really appreciate if you can reconsider your score based on our revisions.*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551302239,
                "cdate": 1700551302239,
                "tmdate": 1700551302239,
                "mdate": 1700551302239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wPKURlZPUp",
                "forum": "FAO4VS9QRV",
                "replyto": "u5hNmCzPIF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Thanks for your clarification! Now I understand that Delta-LoRA is not fundamentally equivalent to LoRA. \n\nQ2.1: Thanks for the clarification. Delta-LoRA uses less memory overhead than DyLoRA. Then DyLoRA is probably not a good baseline? Then I think the authors should use other common parameter-efficient fine-tuning methods such as (IA)^3 proposed by \u201c[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)\u201d.\n\nAlso, directly using LoRA\u2019s hyperparameters for AdaLoRA looks a bit unfair since the hyperparameters might be optimized for LoRA, not the other methods. Therefore, this comparison is not that convincing. I feel like more experiments are needed for giving a more convincing conclusion. \n\nQ2.2: Thanks for running experiments on COLA. It seems on full-rank, Delta-LoRA outperforms the full fine-tuning (64.57). Do you have any intuition about why this happens? Otherwise, I think more experiments are important to empirically understand everything happening here. \n\nQ2.3: Thanks for the additional experiments. It seems hyperparameters still have a fair impact on the performance. \n\nQ2.4: How about Table 4?\n\nIn summary, my major concerns still remain, and I will keep my original score for now."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634896493,
                "cdate": 1700634896493,
                "tmdate": 1700634896493,
                "mdate": 1700634896493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ttY23zq23",
                "forum": "FAO4VS9QRV",
                "replyto": "qcVFCAdyTq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Reviewer_e3re"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Q2.4: I am confused \u2014 I mean in Table 4, LoRA and Delta-LoRA are using different trainable parameters, right?\n\nBased on the response, I am happy to increase my score to 5."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678526059,
                "cdate": 1700678526059,
                "tmdate": 1700678526059,
                "mdate": 1700678526059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fZjs6VLvpv",
            "forum": "FAO4VS9QRV",
            "replyto": "FAO4VS9QRV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission671/Reviewer_rQDs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission671/Reviewer_rQDs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DeltaLORA, a novel variant of the popular PEFT technique LoRA, which updates not only the low rank adapter matrices, but also propagates updates to the underlying pretrained matrix $W$. Their strategy helps alleviate the problem of the being limited to low rank representations which may be insufficient to adapt to downstream tasks while maintaining computational parity with LoRA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-organized and easy to follow.\n- Extensive evaluation on NLU, NLG tasks as well as ablation studies to emphasize the efficacy of their formulation. \n- The proposed algorithm is well-motivated, it builds very naturally upon the baseline LORA and differentiates itself against the baseline (Section 5.3) to answer a natural question - \"Does the performance stem from the additional parameters or the updates applied to the underlying weight matrix?\"\n- It accomplishes it's goals without sacrificing the key advantages of LORA - low memory consumption"
                },
                "weaknesses": {
                    "value": "## Major\n\n- The only major drawback seems to be that the differences in empirical results do not seem statistically significant (and do not have any errors provided). Given the incremental increase, it is not clear whether the performance of the algorithm stems from real gains or is noise from insufficient tuning.\n## Minor \n\n- The whole idea is pretty much a heuristic which works very well in practice (but the same can be said of LoRA)\n- The overt emphasis on the fact that $g_W = g_{AB}$ seems unnecessary (specifically Section 4.1) since the results stems from fairly straightforward linear algebra and calculus. It might be better to reduce that section, and allocate space to more impactful discussion/experiments.\n\nIt is certainly true that the work's novelty is primarily in engineering a practical method, rather than introducing a new direction of research. The advantage of method is clearly empirical in nature. \n\nI am very divided on accepting this paper simply due to the fact that all algorithms seem very close to each other in empirical results, the differences being marginal in most cases. However, due to the fact that this method does provide positive gains across the board, I vote to accept \nthe paper."
                },
                "questions": {
                    "value": "- Can you provide standard errors on some of your empirical results?\n- It would help to incorporate some experiments in Section 7 of the LoRA paper to help understand the kind of updates induced by DeltaLoRA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission671/Reviewer_rQDs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission671/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936416846,
            "cdate": 1698936416846,
            "tmdate": 1699635994111,
            "mdate": 1699635994111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b75nZQg7cB",
                "forum": "FAO4VS9QRV",
                "replyto": "fZjs6VLvpv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer rQDs"
                    },
                    "comment": {
                        "value": "Thanks for your detailed reviews and valuable advice for our submitted paper. \n\n**About the Strengths:**\n\n- *The paper is well-organized and easy to follow.*\n\n- *Extensive evaluation on NLU, NLG tasks as well as ablation studies to emphasize the efficacy of their formulation.*\n\n- *The proposed algorithm is well-motivated, it builds very naturally upon the baseline LORA and differentiates itself against the baseline (Section 5.3) to answer a natural question - \"Does the performance stem from the additional parameters or the updates applied to the underlying weight matrix?\"*\n\n- *It accomplishes its goals without sacrificing the key advantages of LORA - low memory consumption.*\n\n\n**Our Reply:**\n\nThanks for your recognition, we are encouraged by your comments. We have made some corresponding revisions according to your comments and concerns.\n\n**Question 1:** *The only major drawback seems to be that the differences in empirical results do not seem statistically significant (and do not have any errors provided). Given the incremental increase, it is not clear whether the performance of the algorithm stems from real gains or is noise from insufficient tuning.*\n\n\n\n**Our Reply:**\nWe would like to emphasize the effectiveness of the proposed Delta-LoRA from the following points:\n\n\n- **We kept the similar training settings as LoRA**, such as learning rate, weight decay, rank number, training epochs and random seed. We do not use a parameter search and select the random seed (we kept all experiments with random seed=0). Therefore, the improvement that claimed in our paper is convincing. \n\n- **We achieved significant improvement compared with AdaLoRA and DyLoRA.** We used the same training setting to evaluate the two methods. We observed that our method could have pronounced performance gain.\n\n- **We witnessed consistent improvement across multiple datasets.** According to our experiment, we achieved improvement across 8 NLU datasets and 4 NLG datasets, which proves that our method works well across different tasks. \n\n**Question 2:** *The whole idea is pretty much a heuristic which works very well in practice (but the same can be said of LoRA)*\n\n**Our Reply:**\nThanks for your comment. Our initial motivation was indeed heuristic, while we gave a mathematical formulation and presented our analysis of its rationality. We supplied a more theoretical proof to further demonstrate the difference between Delta-LoRA and LoRA in Sec. A.4 of the revised version.\n\n\n\n\n**Question 3:** *The overt emphasis on the fact that \n seems unnecessary (specifically Section 4.1) since the results stem from fairly straightforward linear algebra and calculus. It might be better to reduce that section, and allocate space to more impactful discussions/experiments. It is certainly true that the work's novelty is primarily in engineering a practical method, rather than introducing a new direction of research. The advantage of method is clearly empirical in nature.*\n\n**Our Reply:**\n\nThanks for your kind advice. We appropriately reduce this statement in our revised paper. We modified some content in our revised paper, and marked these descriptions with blue color.\n\n\n**Question 4:** *Can you provide standard errors on some of your empirical results?*\n\n**Our Reply:**\n\nWe provide the standard errors on GLUE benchmark in the following table. We used better learning rate and start steps here which can be found in the Section A.5 of our revised paper.\n\n| MNLI| SST-2 | MRPC | CoLA | QNLI | QQP | RTE | STS-B | AVG|\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| 87.62 $\\pm$ 0.21 | 95.29 $\\pm$ 0.23 | 90.60 $\\pm$ 0.14 | 64.64 $\\pm$ 0.86 | 93.09 $\\pm$ 0.15 | 91.01 $\\pm$ 0.06 | 87.00 $\\pm$ 0.36 | 91.61 $\\pm$ 0.04 | 87.60|\n\nA better choice of learning rate and start steps can further improve the performance of Delta-LoRA.\n\n**Question 5:** *It would help to incorporate some experiments in Section 7 of the LoRA paper to help understand the kind of updates induced by Delta-LoRA.*\n\nThanks for your reminder. We moved these experiment tables to a more conspicuous place. We provided the ablation study and parameter sensitivity in Section 5.3 and Appendix A.5 with 4 experiments in the revised paper.\n\n*We will really appreciate if you can reconsider your score based on our revisions.*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551051873,
                "cdate": 1700551051873,
                "tmdate": 1700551051873,
                "mdate": 1700551051873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iNcl6OL6a2",
            "forum": "FAO4VS9QRV",
            "replyto": "FAO4VS9QRV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission671/Reviewer_p8Lv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission671/Reviewer_p8Lv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Delta-LoRA. Unlike other low-rank adaptation techniques, it updates both low-rank matrices and pre-trained weights. The authors claim this approach tackles the issue of inadequate learning representations and matches LoRA in terms of memory and computational costs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is clear and easy to read with straightforward derivations. The proposed method, Delta-LoRA, is original in its approach to update both the low-rank matrices and the pre-trained weights, which could potentially address the limitations of existing low-rank adaptation methods."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is the lack of experiments on large models where the memory efficiency of Delta-LoRA would be most beneficial. The models tested in the paper are relatively small, which makes the memory-saving advantage less meaningful. Additionally, there is a performance loss compared to fine-tuning, which is expected. However, the paper should have reported the performance of full finetuning with stateless optimizers (such as SGD) and showed that Delta-LoRA outperforms this baseline."
                },
                "questions": {
                    "value": "- Could you provide more evidence to support the claim that Delta-LoRA can nearly match the performance of full finetuning with \"large\" models?\n- How does Delta-LoRA's performance compare to full finetuning with stateless optimizers? (Note -- the memory footprint is actually slightly lower with full finetuning with stateless optimizers)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission671/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698955784630,
            "cdate": 1698955784630,
            "tmdate": 1699635994022,
            "mdate": 1699635994022,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2bDclYjUo2",
                "forum": "FAO4VS9QRV",
                "replyto": "iNcl6OL6a2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer p8Lv"
                    },
                    "comment": {
                        "value": "Thank you very much for recognizing the novelty of this paper. We have revised our paper according to your comments.\n\n**Strengths:** *The paper is clear and easy to read with straightforward derivations. The proposed method, Delta-LoRA, is original in its approach to update both the low-rank matrices and the pre-trained weights, which could potentially address the limitations of existing low-rank adaptation methods.*\n\n**Our Reply:**\nDelta-LoRA introduces a simple but efficient strategy to finetune the low-rank matrices and the pre-trained weights. Delta-LoRA improves the original LoRA from two perspectives: performance and memory consumption.\n\n**Question 1.1:** *The main weakness of the paper is the lack of experiments on large models where the memory efficiency of Delta-LoRA would be most beneficial. The models tested in the paper are relatively small, which makes the memory-saving advantage less meaningful.*\n\n**Our Reply:**\nThanks a lot for your suggestions! We have evaluated the performance of LLaMA-7B model on Alpaca dataset, in the revised manuscript, we have clarified the experimental results. We find that, using evaluation of GPT-4, Delta-LoRA improves  LoRA by around 10\\%, which proves the effectiveness of our Delta-LoRA.\n\n\n\n\n\n**Question 1.2:** *Additionally, there is a performance loss compared to fine-tuning, which is expected.*\n\n**Our Reply:**\nFor this question, we would like to point out there exist two exceptions: \n\n1. **The gap between the finetuned data and the pretrained data is large.** If the training data in downstream task is much different from the pretrained dataset, the full-finetuned can defeat LoRA and other PEFT methods. However, if training data is similar to the pretrained dataset, the fully finetuned method can overfitted fast and suffer from catastrophic forgetting.\n2. **The data scale of the finetuned data is small**. If the data scale of the finetuned data is small, full finetuning mode will overfit the finetuned data quickly. In this way, Delta-LoRA and LoRA will show better results. This is consistent with our results.\n\n\n\n**Question 1.3 and Question 2.2:** \n- *The paper should have reported the performance of full finetuning with stateless optimizers (such as SGD) and showed that Delta-LoRA outperforms this baseline.*\n- *How does Delta-LoRA's performance compare to full finetuning with stateless optimizers? (Note -- the memory footprint is actually slightly lower with full finetuning with stateless optimizers)* \n\n**Our Reply:**\n\nThe AdamW is widely used in the training of transformers, but SGD is not so popular. In NLP and multimodal tasks, most people choose AdamW instead of SGD, since it has a stable performance and converges fast. As the reasons described before, we chose the AdamW optimizer to conduct experiments to make fair comparison. We run the experiments with SGD optimizer on CoLA dataset with rank=8, $\\alpha$=16, learning rate=0.01 and momentum=0.9. LoRA achieves 63.67 while Delta-LoRA has 64.32. Besides, we observed that the training process occurred crash after 57 epochs for the LoRA's training, which emphasizes the instablity of the SGD optimizer for current NLP tasks.\n\n\n**Question 2.1:** *Could you provide more evidence to support the claim that Delta-LoRA can nearly match the performance of full finetuning with \"large\" models?*\n\n**Our Reply:**\nWe have tested the effectiveness of Delta-LoRA with LLaMA-7B on Alpaca dataset. We found that the memory cost of Delta-LoRA is basically same as the LoRA, while our method outperforms the LoRA for around 10\\% under the evaluation of LLMs.\n\n*We will really appreciate if you can reconsider your score based on our revisions.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550939063,
                "cdate": 1700550939063,
                "tmdate": 1700550939063,
                "mdate": 1700550939063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sjg8O8KFvU",
                "forum": "FAO4VS9QRV",
                "replyto": "iNcl6OL6a2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission671/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional questions?"
                    },
                    "comment": {
                        "value": "*We hope our response addresses your concerns; if so, we would really appreciate it if you would reconsider your score accordingly. Please let us know if you have additional questions.*"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission671/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701405584,
                "cdate": 1700701405584,
                "tmdate": 1700701405584,
                "mdate": 1700701405584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]