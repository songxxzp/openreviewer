[
    {
        "title": "Posterior Probability-Based Label Recovery Attack in Federated Learning"
    },
    {
        "review": {
            "id": "2NiK6wbO2M",
            "forum": "cJ7XuW5JaH",
            "replyto": "cJ7XuW5JaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for extracting batch label counts from gradients. The method uses auxiliary data to estimate the network probabilities for data coming from different classes and uses that knowledge to approximate the batch label counts. Additionally, the authors show that their framework is more generic and allows attacking more than the softmax + CE losses used in previous work, instead also handling the focal family of losses. Finally, they provide a theoretical analysis of the label leakage problem through the lens of the exponential family. They demonstrate that their algorithm recovers label counts efficiently in several settings, including under class imbalance, label smoothing, different network activations, different model depths, and different batch sizes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good practical results - beats SOTA everywhere\n- Experiments  with many different settings on the CE + softmax loss \n- Provides extension of the method to the focal loss - not considered before in the literature\n- Possibly an interesting idea to look at the exponential family and their respective losses to explain label leakage, however, the analysis is currently too simplistic"
                },
                "weaknesses": {
                    "value": "- **The focal loss**:    \nThe focal loss definition is confusing. It doesn't seem to be internally consistent throughout the paper, and it does not seem to be exactly following [1] either. This makes it very hard to check the associated maths. In particular, in Eq. 1, $t$ is just an index of the sum, while in the first paragraph of page 4, it is the \"target class.\" Further, the sum over $t$ in Eq. 1 does not appear in the original description in [1]. As currently defined, where $y$ appears in Eq. 1 is not clear - according to the text on page 4, it is embedded in $p_t$ (in a different way from how it is embedded in [1]), but according to the proof in Appendix A.1 that is not the case and $p_t$ is simply the post-softmax probability. According to [1], $\\alpha_t$ depends on $y$, but the paper specifies nothing about this. Further, the temperature parameter $\\tau$ and the smoothed labeling options are only passingly defined. Finally, in the proof in Appendix A.2, the sum over $t$ that is given in Eq. 1 seems to disappear.    \nThe problem is further exacerbated by inconsistent notations. In particular, $i$s and $t$s are used interchangeably in the first paragraph on page 4, even though the authors themselves claim to differentiate them. For example, see the definition of $p_t$, and $\\mathcal{L}_\\text{CE} (p_t)$ .  \nFinally, in [1], the focal loss is defined in the case of multiple foreground classes and a single background class, which, to my understanding, are treated slightly differently from each other and which also results in loss slightly different than $\\mathcal{L} _\\text{CE} (p_t)$ when $\\gamma=0$ and $\\alpha_t=1$. Do the authors consider the same setup or not, and how are these discrepancies resolved?\n - **Theoretical analysis of the label leakage:**   \nThe authors claim their analysis gives valuable insights regarding the origin of label leakage. I find their analysis falls very short from this promise for a few reasons.   \nOn the one hand, the derivation of the gradient of the network loss is not done for the full exponential family but only for a single member (the exponential probability that gives rise to the softmax and cross-entropy loss), jeopardizing the generality of the proposed conclusions.\nOn the other hand, even in that one case, the proposed derivations are not novel, as they follow directly from plugging the results of the standard theory for deriving the cross entropy loss and the softmax, which the authors should more explicitly refer to, into the gradient of these functions which have been derived in the context of label leakage multiple times ( as the authors acknowledge in their background section ).   \nThis leaves as possible contributions of the authors' analysis only describing the gradient of the logits of softmax in the context of the exponential family notions and definitions. This is also not a strong contribution, however, as the authors do not spend more than two sentences on this interpretation. In particular, the authors do not discuss how computation is saved by the exponential family at all and do not explain how the exponential family poses a privacy threat beyond the CE + softmax combination. \n- **Missing Comparisons and Citations:**\n1. The authors should cite [3] and [4] as relevant prior work on label leakage. They are both very relevant as [4] works on non-positive activations similarly to this work, while [3] talks about the possibility of using auxiliary data to estimate quantities that can be used to estimate the label counts. I think comparing to [3] will be good also. \n2. The paper will benefit from better discussion about differences to prior work and, in particular, [2], [3], and [5]. All these works, similarly to the proposed method, estimate a quantity that they later plug into $p - y$ to compute the label counts $\\lambda_j$. Further, at least [3] and [5] talk about using auxiliary data to do so, again similarly to this work.\n3. In the background section, the paper can benefit from discussing the how auxiliary data have been used in gradient inversion attacks before - e.g. [8-11]\n- **Proposed additional evaluations and missing evaluation details:**\n1. What dataset/model/batch size/epoch/label distribution is used in Table 3? Are $\\tau$ and $\\epsilon$ assumed to be known by the attacker? Can you redo the experiments in a setting where we do not get 100% accuracy?\n2. What is the performance if there is a distribution shift between the auxiliary data used for label recovery and the client data?\n3. In Sec 6.1/6.2, do you use the focal or cross-entropy loss? \n4. [2] has a proposed version that works on models trained for several epochs. The authors should compare their method to it. \n5. [3] and [5] can be applied using auxiliary data. The authors should compare those methods to the proposed method under the same auxiliary data.\n6. Why in Sec. 6.3 do the authors use models trained for one epoch?\n7. In Sec. 6.3 the authors say, \"Since we have the prior distribution of the training data, we can constrain and regularize the estimated labels to improve the success rate of label recovery.\" Is that something the authors do in their experiments, and if so, how exactly?\n8. What label distributions do the authors use in 6.1? Uniform at random?\n- **Typos and other Nits**:\n1. Figure 2 will benefit from a box plot instead of the current figure, as the variance of the positives is huge, and it becomes hard to judge where means/medians are.\n2. Bolding in Table 2 should be applied to all 1.000, not only the proposed method for consistency. \n3. On the first line of the derivation of $\\nabla_{z_j}\\mathcal{L}_{FL}$ in Appendix A.1, there is missing $z_t$ in the last sum\n4. On the last line of the derivation of $\\nabla_{z_j}\\mathcal{L}_{FL}$ in Appendix A.1, $\\Psi\\rightarrow \\Phi$\n5. Second summand in the second line of the derivation of $\\nabla b_j$ in Appendix A.2, $(B-\\lambda)\\rightarrow(B-\\lambda_j )$\n6. First part of the derivation of $\\lambda_j$  in Appendix A.2, there is no division by $\\varphi_j$\n- **Other:**  \nThe terminology used in the paper is slightly non-standard and this makes the abstract and intro hard to read. In particular, without reading deep into the paper, it is not clear what is meant by \"classification variants\" - the authors can just say different classification losses and network activation functions. Further, talking about posterior probability distributions is a bit confusing too - you can just call them the probabilities predicted by the network or post-softmax probabilities. Finally, \"approximate probability distributions\" do not clearly convey that you are talking about approximating from data the distributions of the elements of the output of the softmax."
                },
                "questions": {
                    "value": "- Can the authors fix consistently throughout the paper and appendix the definition and usage of the focal loss?\n- Can the authors provide an extension of the current theoretical analysis to the general exponential family or at least to the subset of the exponential family covering the focal loss? \n- Can the authors expand the discussion about the interpretation of their theoretical findings, in particular explaining in more detail their computational and privacy arguments in the context of the full exponential family?\n- Can the authors expand the focal loss experiments to more settings, especially ones where the success rate is not 100%? For a paper that focuses on proposing a generic method that works on multiple losses that are subversions of the focal loss, the paper provides surprisingly few experiments with that loss.\n- Can the authors provide a discussion about why the focal loss is significant in particular? Are there other common classification losses that it covers as sub-cases? Are there other common classification losses not covered by the focal loss that might be more secure?\n- Can the authors provide a comparison with [2] for networks trained for several epochs?\n- Can the authors provide a discussion of how their method compares to prior work - e.g. [2,3,5]? All these methods and the author's proposed method in practice approximate some quantities related to $p_t$ and plug them in the same formula to get $\\lambda_j$. Thus, as of now, I have no idea why the author's proposed approximation is better than prior work.\n- Can the authors provide a comparison with [3] or [5] when auxiliary data is used for estimating their parameters?\n- Can the authors test their methods in a FedAvg setting (check [3,6,7])?\n- Do the authors know why the SELU activation results in so much more variance in the estimates of its posterior probability distribution?\n- Can the authors provide the missing information mentioned in the weakness section?\n- Can the authors provide code? \n\nAll in all, I find the experimental part of this paper strong. Thus, I am leaning toward acceptance. However, the focal loss definition problem is significant, as it makes it hard to fully check the mathematics. Further, the theoretical analysis in its current form does not contribute to the paper despite its interesting idea. Additionally, I really want to see a discussion about the differences to prior work ( discussion also sorely missed in prior work ), as many works in the field are slight variations of each other, and it is hard to understand where improvements between the papers, including this one, comes from. Finally, additional discussion of why the focal loss matters and more experiments with it just make a lot of sense in the context of this paper as it is one of the biggest claimed contributions.\n\n[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2980\u20132988, 2017.  \n[2] Kailang Ma, Yu Sun, Jian Cui, Dawei Li, Zhenyu Guan, and Jianwei Liu. Instance-wise batch label restoration via gradients in federated learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[3] Geng, Jiahui, et al. \"Towards general deep leakage in federated learning.\" arXiv preprint arXiv:2110.09074 (2021).   \n[4] Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter Chin, and Franc\u00b8oise Beaufays. Revealing and protecting labels in distributed training. Advances in Neural Information Processing Systems, 34:1727\u20131738, 2021.   \n[5] Aidmar Wainakh, Fabrizio Ventola, Till Mu\u00dfig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, and Max Muhlh \u00a8 auser. User-level label leakage from gradients in federated learning. Proceedings on Privacy Enhancing Technologies, 2:227\u2013244, 2022.  \n[6] Dimitrov, Dimitar Iliev, et al. \"Data leakage in federated averaging.\" Transactions on Machine Learning Research (2022).  \n[7] Zhu, Junyi, Ruicong Yao, and Matthew B. Blaschko. \"Surrogate model extension (SME): A fast and accurate weight update attack on federated learning.\" arXiv preprint arXiv:2306.00127 (2023).  \n[8] Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu. Auditing privacy defenses in federated learning via generative gradient leakage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10132\u201310142, 2022.  \n[9] Wu, Ruihan, et al. \"Learning To Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning.\" Uncertainty in Artificial Intelligence. PMLR, 2023.  \n[10] Dongyun Xue, Haomiao Yang, Mengyu Ge, Jingwei Li, Guowen Xu, and Hongwei Li. Fast generation-based gradient leakage attacks against highly compressed gradients. IEEE INFO316 COM 2023 - IEEE Conference on Computer Communications, 2023.  \n[11] Garov, Kostadin, et al. \"Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning.\" arXiv preprint arXiv:2306.03013 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors should include an ethics statement discussing how their work can be used by malicious actors and how to defend against it."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo",
                        "ICLR.cc/2024/Conference/Submission4626/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697549251883,
            "cdate": 1697549251883,
            "tmdate": 1700740881714,
            "mdate": 1700740881714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eBmJWQKf3E",
                "forum": "cJ7XuW5JaH",
                "replyto": "2NiK6wbO2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gCmo"
                    },
                    "comment": {
                        "value": "We greatly appreciate your meticulous reading this paper and providing so many valuable suggestions. Regardless of the final result, we are very lucky to have a reviewer like you! We carefully read your comments and rethink about how to improve this work. We will try our best to answer your questions and concerns. We are pleased to offer more explanation or revisions if you have any other recommendations.\n\n> Q1: Can the authors fix consistently throughout the paper and appendix the definition and usage of the focal loss?\n\nIn Appendix A.1, we step-by-step derive the Focal Loss in multi-class scenarios from the original binary Focal Loss in [1].\n\nSome important points we would like to emphasize:\n\n\n- **Difference in probabilities**: $p_i$ is the post-softmax probability, while $p_t$ represents the automatic determined confidence of the input at class $i$, where $t=i$.\n- **Mechanism of weight**: For an easy-to-learn sample, $p_t$ might be close to the target label. So, Focal Loss assigns a small coefficient $(1-p_t)^{\\gamma}$ as the weight. However, for a hard-to-learn sample, $p_t$ may be close to 0. Then $(1-p_t)^{\\gamma}$ a relative large weight to enhance the ratio of the these samples in the total loss.\n- **Summation sign**: Since the binary Focal Loss [1] just has one output, the summation is not necessary. In the multi-class case, we use the summation to cover all the classes $t\\in[1,K]$, and aim to derive a general conclusion in Theorem 1.\n- **Why Focal Loss**: To the best of our knowledge, Focal Loss has the general form in the cross-entropy loss variants and it can be converted to CE loss or BCE loss by setting different $\\alpha$ and $\\gamma$. We aim to derive a general form of label leakage from gradient, so we choose the Focal Loss.\n\n[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2980\u20132988, 2017.\n\n> Q2: Can the authors provide an extension of the current theoretical analysis to the general exponential family or at least to the subset of the exponential family covering the focal loss?\n\nWe would like to explain the reason of introducing the Exponential Family. Actually, we introduce the Exponential Family to better understand **why the combination of cross-entropy and normalization functions (softmax / sigmoid) leads to the conclusions of Theorem 1** in this paper. Although the studies like iDLG have deduced some conclusions about cross-entropy and softmax, they do not delve into the underlying reasons. For illustration, we also find that sigmoid + cross-entropy also satisfies this conclusion. **We also wonder if there are other functions that can replace softmax and whether the label leakage still holds?** So we ask ourselves the following three questions to:\n\n1. How did softmax (or sigmoid) originate and why is it applied to classification problems?\n2. Why can cross-entropy serve as a loss function for classification problems?\n3. When softmax and cross-entropy are combined, why does the term $(\\boldsymbol p-\\boldsymbol y)$ appear?\n\nThe exponential family can provide satisfactory answers to these questions. The Focal Loss is a variant of the common CE loss, and its unique term $(1-p_t)^{\\gamma}$ represents the auto-determined weight for class imbalance. We find that from the perspective of exponential family, the cross-entropy loss and softmax (or sigmoid) are naturally combined with each other. **To this end, we make sure that no other functions can replace softmax (or sigmoid), and the occurence of label leakage is in evitable in a classification model shared gradients.**\n\nThe Exponential Family also encompasses the Bernoulli, Poisson and Gaussian distributions. However, these distributions do not support the conclusion proposed in this paper. Therefore, the conclusion is only applicable to the Categorical distribution within the Exponential Family.\n\n> Q3: Can the authors expand the discussion about the interpretation of their theoretical findings, in particular explaining in more detail their computational and privacy arguments in the context of the full exponential family?\n\nIn Appendix A.4, we expand the derivation of the theorectical findings. Please refer to that part."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662590568,
                "cdate": 1700662590568,
                "tmdate": 1700662590568,
                "mdate": 1700662590568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8wNas3UQR7",
                "forum": "cJ7XuW5JaH",
                "replyto": "2NiK6wbO2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gCmo (Part 2)"
                    },
                    "comment": {
                        "value": "> Q4: Can the authors expand the focal loss experiments to more settings, especially ones where the success rate is not 100%?\n\nHere, we present additional experiments conducted on the Focal Loss. We mainly test the parameters of $\\tau$, $\\gamma$ and $\\varepsilon$ on an untrained model, and average the experiments over 10 trials. Focusing on temperature $\\tau$, we present several cases where the accuracy is not 100%. In addition, by varying $\\gamma$ and $\\varepsilon$ on these setting, the accuracies are not affected.\n\nTemperature $\\tau$ (batch size=64, activation=ReLU):\n\n| Dataset   | Model     | $\\tau$ | Our ClsAcc | Our InsAcc |\n| --------- | --------- | ------ | ---------- | ---------- |\n| MNIST     | LeNet     | 0.5    | 0.980      | 0.906      |\n|           |           | 0.75   | 0.980      | 0.945      |\n|           |           | 0.9    | 0.990      | 0.954      |\n|           |           | 1.25   | 0.990      | 0.983      |\n|           |           | 1.5    | 1.000      | 0.998      |\n| CIFAR-10  | ResNet-18 | 0.5    | 0.990      | 0.960      |\n|           |           | 0.75   | 1.000      | 0.994      |\n|           |           | 0.9    | 1.000      | 1.000      |\n|           |           | 1.25   | 1.000      | 1.000      |\n|           |           | 1.5    | 1.000      | 1.000      |\n| CIFAR-100 | ResNet-50 | 0.5    | 1.000      | 1.000      |\n|           |           | 0.75   | 1.000      | 1.000      |\n|           |           | 0.9    | 1.000      | 1.000      |\n|           |           | 1.25   | 1.000      | 1.000      |\n|           |           | 1.5    | 1.000      | 1.000      |\n\nWe have observed that the temperature parameter, $\\tau$, significantly impacts smaller datasets. When $\\tau$ is smaller, the space of logits expands, complicating the estimation of batch posterior probabilities. Consequently, as $\\tau$ decreases, label accuracy deteriorates. For the large datasets with more classes, such as CIFAR-10, the logits space is hardly influenced by changing different $\\tau$.\n\n> Q5: Can the authors provide a discussion about why the focal loss is significant in particular? Are there other common classification losses that it covers as sub-cases? Are there other common classification losses not covered by the focal loss that might be more secure?\n\n- To the best of our knowledge, Focal Loss has the general form in the cross-entropy loss variants and it can be converted to CE loss or BCE loss by setting different $\\alpha$ and $\\gamma$. We aim to derive a general form of label leakage from gradient, so we choose the Focal Loss.\n- In supervised learning, we currently did not find other common classification losses that can cover the Focal Loss.\n- There are indeed some other classification losses not covered by the Focal Loss, such as the the Hinge Loss used in Support Vector Machines or the Kullback-Leibler Divergence used in some probabilistic models. However, this paper cannot conclude they are more or less secure than the Focal Loss. Because this paper only covers the classification loss function with cross-entropy and softmax (or sigmoid)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666551301,
                "cdate": 1700666551301,
                "tmdate": 1700666551301,
                "mdate": 1700666551301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5880siPbw4",
                "forum": "cJ7XuW5JaH",
                "replyto": "2NiK6wbO2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gCmo (Part 3)"
                    },
                    "comment": {
                        "value": "> Q8: Can the authors provide a comparison with [3] or [5] when auxiliary data is used for estimating their parameters?\n\nWe name ZLG for [3] and LLG for [5]. The following experiments compare the label recovery accuracies with different model architectures, batch sizes and activation functions.\n\n- Batch size=64, activation=ReLU:\n\n  | Dataset   | Model     | ZLG ClsAcc | ZLG InsAcc | LLG ClsAcc | LLG InsAcc | Our ClsAcc | Our InsAcc |\n  | --------- | --------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n  | MNIST     | LeNet     | **1.000**  | 0.996      | **1.000**  | **1.000**  | 0.995      | 0.955      |\n  | CIFAR-10  | ResNet-18 | **1.000**  | 0.916      | 1.000      | 0.914      | **1.000**  | **1.000**  |\n  | CIFAR-100 | ResNet-50 | 0.985      | 0.893      | 0.816      | 0.633      | **1.000**  | **1.000**  |\n\n- Batch size=256, activation=ReLU:\n\n  | Dataset   | Model     | ZLG ClsAcc | ZLG InsAcc | LLG ClsAcc | LLG InsAcc | Our ClsAcc | Our InsAcc |\n  | --------- | --------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n  | MNIST     | LeNet     | **1.000**  | 0.977      | **1.000**  | **0.982**  | **1.000**  | 0.951      |\n  | CIFAR-10  | ResNet-18 | **1.000**  | 0.905      | **1.000**  | 0.919      | **1.000**  | **0.977**  |\n  | CIFAR-100 | ResNet-50 | 0.998      | 0.881      | **1.000**  | 0.868      | **1.000**  | **1.000**  |\n\n- Batch size=64, activation=ELU:\n\n  | Dataset   | Model     | ZLG ClsAcc | ZLG InsAcc | LLG ClsAcc | LLG InsAcc | Our ClsAcc | Our InsAcc |\n  | --------- | --------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n  | MNIST     | LeNet     | **1.000**  | 0.948      | 0.580      | 0.283      | 0.990      | **0.969**  |\n  | CIFAR-10  | ResNet-18 | **1.000**  | 0.902      | 0.980      | 0.697      | **1.000**  | **0.972**  |\n  | CIFAR-100 | ResNet-50 | 0.985      | 0.897      | 0.845      | 0.603      | **1.000**  | **1.000**  |\n\n> Q9: Can the authors test their methods in a FedAvg setting (check [3,6,7])?\n\nWe present a concept for a label attack within the Federated Averaging (FedAvg) setting. Drawing inspiration from [7], we leverage a surrogate model to estimate the unshared gradients, denoted as $G_t$, during local training with FedAvg. At each iteration $t$, we update the model $M_t$ by considering the previous estimated state $M_{t-1}$, the update $G_{t-1}$, and the learning rate $\\eta$. This update is represented by the equation $M_{t} = M_{t-1} - \\eta \\cdot G_{t-1}$. Utilizing the updated model $M_{t}$ and the estimated gradients $G_{t}$, we can calculate positive ($\\hat p_j^+$) and negative ($\\hat p_j^-$) probabilities for each class $j$. These probabilities are derived through our formulated expression for FedSGD algorithm. At this point, we can restore all the labels during the local updates.\n\n[7] Zhu, Junyi, Ruicong Yao, and Matthew B. Blaschko. \"Surrogate model extension (SME): A fast and accurate weight update attack on federated learning.\" arXiv preprint arXiv:2306.00127 (2023).\n\n> Q10: Do the authors know why the SELU activation results in so much more variance in the estimates of its posterior probability distribution?\n\nWe address this phenomenon by examining the output range $a$ of various activation functions. The input is denoted as $x$, and we all use the default settings in PyTorch.\n\n- Sigmoid: $a\\in(0,1)$\n- Tanh: $a\\in(-1,1)$\n- ReLU: $a\\in[0, x]$\n- ELU: $a\\in(-1,x]$\n- SELU: $a\\in(-1.76,x]$\n\nObserving these output ranges, it becomes evident that SELU has the widest output space among these activation functions. Consequently, the output of each layer in the network may undergo larger shifts compared to the others. This increased shift results in a posterior probability distribution with higher variance, posing a greater challenge for attackers attempting estimation. Understanding this aspect sheds could explain why the attacks under SELU activation have a relative worse performance compared to the other activation functions."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666613414,
                "cdate": 1700666613414,
                "tmdate": 1700666613414,
                "mdate": 1700666613414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GWlvMcvNFd",
                "forum": "cJ7XuW5JaH",
                "replyto": "2NiK6wbO2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "content": {
                    "title": {
                        "value": "Response to Response to Reviewer gCmo"
                    },
                    "comment": {
                        "value": "In the interest of providing the authors with quick feedback before the end of the discussion period. I have skimmed their responses and the new pdf without going through it in detail yet. My feedback follows:\n\n**Response to Q1:** I am thankful to the authors. This indeed drastically helps the readability of the paper. While I haven't yet looked through all derivations from scratch, one thing I want to point out/ask about is interpreting $p_t$ as the softmax at the start of the proof on page 13. I thought, it could also be one minus the softmax instead, depending on the ground truth label. Can the authors elaborate on this/fix the derivations?\n\n**Response to Q2:** I am sure the authors should be able to find an intuitive interpretation within the exponential family for the focal loss ( given its closeness to CE ) and equal vulnerability. A quick google search suggested the following article [1]. Again, due to the interest of time, I haven't gone through it in detail to see if it exactly relevant to the author's use case.\n\n**Response to Q3:** In Appendix A.4. the authors discuss the computational savings of the gradient function of the softmax+CE combo. Are those savings realized by the backward pass of, say, pytorch? Are the savings significant for the training time? Can the authors give an example of another exponential family member's final gradient and how it does not have the same computation savings (both in terms of the formula and computation complexity)?\n\n[1] https://towardsdatascience.com/cross-entropy-classification-losses-no-math-few-stories-lots-of-intuition-d56f8c7f06b0"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668008244,
                "cdate": 1700668008244,
                "tmdate": 1700669380331,
                "mdate": 1700669380331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dkCYWZs2z6",
                "forum": "cJ7XuW5JaH",
                "replyto": "GWlvMcvNFd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "content": {
                    "title": {
                        "value": "Response to Response to Reviewer gCmo"
                    },
                    "comment": {
                        "value": "**Response to Q4:** I am thankful to the authors for providing this experiment. I am more confident now that the attack on the Focal Loss is as effective as the one on CE.\n\n**Response to Q8:** I assume all three methods use the same auxiliary data in this experiment? If so, I would argue that the shown results are very good and the **other reviewers** should also take a look at them, as they constitute one of the most important results of the paper.  \n\nThat said, I still suggest the authors, before the end of the discussion period, to write a paragraph pointing out where the big difference between them and prior work comes from."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669327629,
                "cdate": 1700669327629,
                "tmdate": 1700669327629,
                "mdate": 1700669327629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GQRLPEelHw",
                "forum": "cJ7XuW5JaH",
                "replyto": "2NiK6wbO2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_gCmo"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I really appreciate Appendix B. It really helps me to evaluate the paper's contribution better. \n\nFor the next revision, I will suggest authors provide a version of Table 5-7 including the main results -e.g iLRG and the version of LLG/ZLG that does not use auxiliary data (but dummy data), as I think it will benefit the community as a whole, as these works have been inconsistent at comparing and citing related work. I also think trying the author's approach on dummy data might be interesting.\n\nFor the next revision, I will also encourage the authors to decrease the prominence of the exponential family discussion in the main paper and/or to extend it to the focal loss. \n\nI am very thankful to the authors for their rebuttal engagement as a whole, and I have raised my grade to 6. I might raise it further after going through the paper fully with the new information supplied. I could not do this in the short time frame given."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740639162,
                "cdate": 1700740639162,
                "tmdate": 1700741407706,
                "mdate": 1700741407706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2B1SKkmzDF",
            "forum": "cJ7XuW5JaH",
            "replyto": "cJ7XuW5JaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_zaUc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_zaUc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a label recovery attack by estimating the posterior probabilities. The authors first obtain the relationship between the gradient of focal loss and posterior probability. Then, they explain the essential reasons for such findings from the perspective of the exponential family. They also empirically observe that positive (negative) samples of a class have approximate probability distributions. Experiments on different datasets validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed method is well supported by theoretical analysis.\n* The proposed method empirically works."
                },
                "weaknesses": {
                    "value": "* The proposed method requires an auxiliary dataset with the same distribution of training data. This requirement is impractical in FL.\n* The writing is not clear. For example, I cannot find the exact definition of 1) negative probabilities and positive probabilities in Sec 5.1, class-wise labels in Theorem 2.\n* [Minor] The second equation in the proof for theorem 3, Appendix A.1 should be\n\n$$\n\\nabla_{z_j}L_{FL}\n=\\sum_{t=1}^K\\alpha_t\\frac{\\partial\\bar{h}}{\\partial z_j}\\log\\sum_{k=1}^Ke^{z_k}+\\sum_{t=1}^K\\alpha_t(1-p_t)^{\\gamma}p_j-\\sum_{t=1}^K\\alpha_t\\frac{\\partial\\bar{h}}{\\partial z_j}\\textcolor{red}{z_t}-\\alpha_j(1-p_j)^\\gamma.\n$$\n\n$z_t$ is missing in the equation."
                },
                "questions": {
                    "value": "The performance of the proposed attack in Theorem 2 relies on the quality of posterior probability. To my understanding, The posterior probability increases when the training epoch increases. Therefore, the performance should increase with the training epoch. However, in Figure 4, the performance decreases as the training epoch increases. Could you please provide more explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721734868,
            "cdate": 1698721734868,
            "tmdate": 1699636441783,
            "mdate": 1699636441783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "miSJpN1c8q",
                "forum": "cJ7XuW5JaH",
                "replyto": "2B1SKkmzDF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zaUc"
                    },
                    "comment": {
                        "value": "Thanks for appreciation of our theorectical analysis and empirical evaluations, and pointing out the imperceptible mistakes. We give a point-by-point response to the weaknesses and questions, and hope to clear up your concerns.\n\n> Weakness 1: The proposed method requires an auxiliary dataset with the same distribution of training data. This requirement is impractical in FL.\n\nWe would like to highlight the following points to justify this assumption:\n\n1. **Realistic Threat Model:** In many practical scenarios, attackers often have access to auxiliary data sources that closely resemble the distribution of the target training data. This assumption aligns with the notion that attackers may gather information from publicly available sources or other related datasets to enhance the effectiveness of their attacks.\n2. **Common Practice in Security Analysis:** Making use of auxiliary data with a similar distribution is a standard practice in security analysis, especially in the research fields of *Membership Inference Attacks* [1, 2] and *Gradient Inversion Attacks* [3, 4]. This enables a more thorough evaluation of the model's robustness, covering a broader range of potential adversary scenarios.\n3. **Worst-Case Scenario Exploration:** Assuming auxiliary data with a matching distribution, our analysis explores a worst-case scenario, where the adversary accesses highly relevant information. This reveals insights into the FL's vulnerabilities under challenging conditions, providing a comprehensive evaluation of its security properties.\n\n[1] Hu, Hongsheng, et al. \"Membership inference attacks on machine learning: A survey.\" *ACM Computing Surveys (CSUR)* 54.11s (2022): 1-37.\n\n[2] Carlini, Nicholas, et al. \"Membership inference attacks from first principles.\" *2022 IEEE Symposium on Security and Privacy (SP)*. IEEE, 2022.\n\n[3] Jeon, Jinwoo, et al. \"Gradient inversion with generative image prior.\" *Advances in neural information processing systems* 34 (2021): 29898-29908.\n\n[4] Balunovic, Mislav, et al. \"Lamp: Extracting text from gradients with language model priors.\" *Advances in Neural Information Processing Systems* 35 (2022): 7641-7654.\n\n> Weakness 2: I cannot find the exact definition of negative probabilities and positive probabilities in Sec 5.1, class-wise labels in Theorem 2.\n\nHere, we add the definition and explanation of the positive and negative probabilities, and the class-wise labels.\n\nIn a multi-class classification problem, each instance in the dataset belongs to one of several classes. Let's denote the set of classes as $K$ and a particular class of interest as $k\\in K$. In this context, we can define positive and negative samples for class $k$.\n\n1. **Positive Samples** ($X_{k}^{+}$): The positive samples of class $k$ satisfy that: $X_{k}^{+}=\\{x_i|y_i=k\\}$, where $x_i$ is the input and $y_i$ is the corresponding label.\n2. **Negative Samples** ($X_{k}^{-}$): Similarly, the negative samples of class $k$ satisfy that: $X_{k}^{-}=\\{x_i|y_i\\neq k\\}$.\n\nAccording to the positive and negative samples, we can then get the positive and negative probability for class $k$.\n\n1. **Positive Probability** ($p_{k}^{\\text{+}}$): When a positive instance is fed into the model, the predicted probability of class  $k$ is termed the positive probability. Since the Softmax activation function is used in the output layer, the output posterior probability $p^{+}$ is a vector of length $k$. Therefore, the positive probability for class $k$ can be expressed as $p_{k}^{+}$.\n2. **Negative Probability** ($p_{k}^{-}$): Similarly, when a negative sample is input into the model, the $k$th element of the output probability vector represents the negative probability, denoted as $p_{k}^{-}$. It's essential to note that any negative sample associated with the other $(K-1)$ classes contributes to $p_{k}^{-}$.\n\nWhen using an auxiliary dataset to esimate the probabilities of the target training batch in FL, we denote the estimated positive and negative probabilities as $\\hat p_{k}^{+}$ and  $\\hat p_{k}^{-}$, respectively.\n\nIn a batch size of $B$, we aim to recover the labels of each instance within the batch, i.e., $\\mathbf{y}=[y^{(1)},y^{(2)},\\cdots,y^{(B)}]$. As this is a multi-class classification problem, the ground-truth labels $\\mathbf{y}$ can also be represented the occurences of each class: $\\mathbf{y}=[n_1,n_2,\\cdots,n_K]$, where $n_k$ is the number of samples belonging to class $k$ and $K$ is the number of total classes.\n\n- **Class-wise Labels**: In a batch data, the class-wise labels can be defined as: $n_k=\\sum_{i=1}^{B}\\delta(y^{(i)}=k)$. Here, $n_k$ is the number of samples belonging to class $k$, B is the batch size, $y^{(i)}$ is the true class label of the $i$th instance in the batch, and $\\delta(\\cdot)$ is the Kronecker delta function, which equals 1 if the condition inside is true and 0 otherwise."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471066379,
                "cdate": 1700471066379,
                "tmdate": 1700471066379,
                "mdate": 1700471066379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mgFu6n7cif",
                "forum": "cJ7XuW5JaH",
                "replyto": "OEX6Mmc1We",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_zaUc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_zaUc"
                ],
                "content": {
                    "title": {
                        "value": "Thank for the responses"
                    },
                    "comment": {
                        "value": "Thanks to the author for the responses.\n\nThe revision is not provided and the original manuscript is hard to understand due to unclear notation.\nThe phenomenon that the learned posterior probabilities are worse than random guesses is quite confusing. Further justification or calibration techniques might be needed.\n\nTherefore, I would like to keep my rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645487211,
                "cdate": 1700645487211,
                "tmdate": 1700645487211,
                "mdate": 1700645487211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "slIEDpadda",
            "forum": "cJ7XuW5JaH",
            "replyto": "cJ7XuW5JaH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_AjeW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4626/Reviewer_AjeW"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyses root cause of label leakage from gradients and propose a novel attack which estimates the posterior probabilities from an auxiliary dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Analysis of label leakage is novel and insightful, and the conclusion is interesting: combining cross-entropy loss and Softmax is intended to reduce computation but opens a backdoor to privacy attacks.\n2. Novelty is clear. \n3. Writing is easy to follow."
                },
                "weaknesses": {
                    "value": "A small weakness: the attack assumes adversary can use an auxiliary data with the same distribution of training data, which in reality is not always true. I hope there can be some more discussions on how the results can be if auxiliary datasets and training datasets have considerable distribution shift."
                },
                "questions": {
                    "value": "1. MNIST and CIFAR10 are relatively small and easy datasets. I wonder if the posterior probability estimation accuracy can scale up, if the data is more complex and the task is more challenging? For example, in ImageNet there are more vague and hard samples, the variance in the data distribution will be larger. Can you show us what will be the case in ImageNet or other large datasets?\n\n2. In Figure 4 rightmost subfigure, why instance accuracy drops with model training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4626/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4626/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4626/Reviewer_AjeW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4626/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699208319636,
            "cdate": 1699208319636,
            "tmdate": 1699636441696,
            "mdate": 1699636441696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fUI75ZR1oi",
                "forum": "cJ7XuW5JaH",
                "replyto": "slIEDpadda",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AjeW"
                    },
                    "comment": {
                        "value": "We appreciate your praise for the novelty and inspiration of our paper and some concerns about the assumption and experiments. We give point-by-point responses below. We are pleased to offer more explanation or revisions if you have any other recommendations.\n\n> Weakness 1: The attack assumes adversary can use an auxiliary data with the same distribution of training data, which in reality is not always true.\n\nWe would like to highlight the following points to justify this assumption:\n\n1. **Realistic Threat Model:** In many practical scenarios, attackers often have access to auxiliary data sources that closely resemble the distribution of the target training data. This assumption aligns with the notion that attackers may gather information from publicly available sources or other related datasets to enhance the effectiveness of their attacks.\n2. **Common Practice in Security Analysis:** Making use of auxiliary data with a similar distribution is a standard practice in security analysis, especially in the research fields of *Membership Inference Attacks* [1] and *Gradient Inversion Attacks* [2]. This enables a more thorough evaluation of the model's robustness, covering a broader range of potential adversary scenarios.\n3. **Worst-Case Scenario Exploration:** Assuming auxiliary data with a matching distribution, our analysis explores a worst-case scenario, where the adversary accesses highly relevant information. This reveals insights into the FL's vulnerabilities under challenging conditions, providing a comprehensive evaluation of its security properties.\n\n[1] Hu, Hongsheng, et al. \"Membership inference attacks on machine learning: A survey.\" *ACM Computing Surveys (CSUR)* 54.11s (2022): 1-37.\n\n[2] Jeon, Jinwoo, et al. \"Gradient inversion with generative image prior.\" *Advances in neural information processing systems* 34 (2021): 29898-29908.\n\n> Weakness 2: I hope there can be some more discussions on how the results can be if auxiliary datasets and training datasets have considerable distribution shift.\n\nTo address concerns about significant distribution shifts, we conducted supplementary experiments, averaging results over 10 trials.\n\n1. In the first set of experiments, we alternately used CIFAR-10 and CINIC-10 [3] as the training and auxiliary datasets. CINIC-10 extends CIFAR-10 by incorporating downsampled ImageNet images. While there is some overlap in the distributions due to similar object categories, there is a notable bias in the data features. We employed an untrained VGG model with a batch size of 64, and the results are presented in the table below.\n\n   | Training Data | Auxiliary Data | Model | Our ClsAcc | Our InsAcc |\n   | ------------- | -------------- | ----- | ---------- | ---------- |\n   | CIFAR-10      | CINIC-10       | VGG   | 1.000      | 1.000      |\n   | CINIC-10      | CIFAR-10       | VGG   | 1.000      | 1.000      |\n\n2. Similarly, the second set of experiments involves alternating between MNIST and Fashion-MNIST as the training and auxiliary datasets. Despite both datasets having 10 classes, the objects they represent are entirely different. MNIST dataset contains a lot of handwritten digits, while Fashion-MNIST represents the article of clothing. Employing an untrained LeNet model with a batch size of 64, the results are presented in the table below.\n\n   | Training Data | Auxiliary Data | Model | Our ClsAcc | Our InsAcc |\n   | ------------- | -------------- | ----- | ---------- | ---------- |\n   | MNIST         | Fashion-MNIST  | LeNet | 1.000      | 0.969      |\n   | Fashion-MNIST | MNIST          | LeNet | 1.000      | 0.948      |\n\n\nThis phenomenon can be explained as follows: In the initial phase of model training, the model lacks the ability to differentiate between samples from each class, assigning similar output probabilities to all fitted samples (i.e., $1/K$). For different training datasets like Fashion-MNIST and MNIST, the model projects input figures to similar output probabilities with slight variations. This benefits our label recovery attack as it becomes easier to estimate the posterior probabilities of the target batch.\n\nGiven that Fashion-MNIST is more intricate than MNIST, utilizing MNIST as the auxiliary dataset poses challenges in accurately estimating the probability distribution of batch training samples. This difficulty accounts for the marginal decrease in InsAcc. However, since CIFAR-10 and CINIC-10 exhibit similarities, there is no difference in InsAcc. The outcomes of these experiments illustrate that if an attacker initiates an attack during the early training stages, having an auxiliary dataset with an identical distribution to the training dataset may not be essential.\n\n[3] Darlow, Luke N., et al. \"Cinic-10 is not imagenet or cifar-10.\" *arXiv preprint arXiv:1810.03505* (2018)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469998881,
                "cdate": 1700469998881,
                "tmdate": 1700469998881,
                "mdate": 1700469998881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4FDdIJmX7H",
                "forum": "cJ7XuW5JaH",
                "replyto": "G6XXwk8fsC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_AjeW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4626/Reviewer_AjeW"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I would like to express my gratitude to the authors for providing a thorough response to my questions, which effectively addressed my concerns. My stance remains in favor of acceptance, and therefore, I will maintain my original score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4626/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631116785,
                "cdate": 1700631116785,
                "tmdate": 1700631116785,
                "mdate": 1700631116785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]