[
    {
        "title": "HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs"
    },
    {
        "review": {
            "id": "dIHNA2c4Dz",
            "forum": "DZUzOKE6og",
            "replyto": "DZUzOKE6og",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_EPYj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_EPYj"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a novel self-supervised learning (SSL) task, hyperedge filling. The authors also give the relationship between hyperedge filling and node classification. Based on the SSL task, the authors propose HypeBoy, which is composed of 3 steps, hypergraph augmentation, hypergraph encoding, and hypergraph filling. The authors demonstrate the effectiveness of HypeBoy under multiple SSL tasks on multiple datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The article proposes a novel SSL task, hyperedge filling, which is a supplement to the general node classification task.\n2. The hyperedge-filling task proposed in the article can obtain more accurate node features, thereby helping other tasks."
                },
                "weaknesses": {
                    "value": "1. In the hypothesis, the premise of the article is that each hyperedge contains most of the same category vertices. However, there are hyperedges that do not satisfy this situation. For example, a hyperedge of size 10 contains 5 nodes of category A and 5 category B.\n2. One of the main contributions of the paper is hyperedge filling, but using only hyperedge filling (v1) in the ablation experiment is the worst among v1-v4. The effectiveness of using hyperedge filling alone is questionable, or it needs to be bound to feature reconstruction and projection heads."
                },
                "questions": {
                    "value": "1. In BASIC SETTING, one of the assumptions is that the homophily ratio of each hyperedge is in [0.5, 1]. In graphs, the homophily rate is defined as the proportion of intra-class edges to all edges. So how does this definition apply to the hypergraph? Do all datasets in the experiments satisfy this assumption?\n2. I noticed that in section 5.1, the article uses a setting of 1% of the training set, which is different from most articles. Most other papers use fixed division or 5/10 nodes per category. I'm curious about what the considerations are for such an experimental setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Reviewer_EPYj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482390234,
            "cdate": 1698482390234,
            "tmdate": 1700659067242,
            "mdate": 1700659067242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "krK6kxffni",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer EPYj"
                    },
                    "comment": {
                        "value": "Dear Reviewer EPYj,\n\nWe appreciate your constructive review. We provide our responses to each of your comments below. First, however, please let us make clarifications about the homophily ratio, which would serve to better clarify our responses."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218327007,
                "cdate": 1700218327007,
                "tmdate": 1700218327007,
                "mdate": 1700218327007,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HwaMBW7O0o",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification for Homophily Ratio $\\mathscr{P}$"
                    },
                    "comment": {
                        "value": "- **Clarification 1.** Our theoretical analysis covers both homophily and non-homophily scenarios.\n    - Specifically, our theorems (Theorems 1 and 2) are valid in both homophilic and non-homophilic cases.\n        - The (expected) general-sense homophily (i.e. the same class nodes tend to interact with each other) is in a symmetric U-curve relation w.r.t. $\\mathscr{P} \\in [0,1]$.\n            - Homophily decreases at $\\mathscr{P} \\in [0, 0.5]$ and increases at $\\mathscr{P} \\in [0.5, 1]$.\n            - Homophily is maximized at either $\\mathscr{P} =1$ or$\\mathscr{P} = 0$ and minimized at $\\mathscr{P} = 0.5$.\n        - Our theorems (Theorems 1 and 2) hold for any $\\mathscr{P} \\in [0,1]$, i.e., both homophilic and non-homophilic cases.\n- **Clarification 2.** $\\mathscr{P}$ is a parameter that controls the number of particular class nodes in a hyperedge.\n    - Specifically, homophily does not monotonically increase as $\\mathscr{P}$ increases (see Clarification 1 above).\n    - We apologize for the confusion seemingly caused by our verbal expression of $\\mathscr{P}$ (we used the term \u201chomophily ratio\u201d).\n    - We have renamed the $\\mathscr{P}$ as \u201c***affinity parameter***\u201d and modified it in the revised manuscript with additional descriptions.\n    - Details regarding $\\mathscr{P}$ are described below:\n        - In our theoretical analysis, we assume two types of hyperedges (Type 0 and Type 1).\n        - (Homophilic) As $\\mathscr{P} \\rightarrow 1$, Type 0 hyperedges tend to be filled with class-0 nodes, and Type 1 hyperedges are filled with class-1 nodes.\n        - (Homophilic) As $\\mathscr{P} \\rightarrow 0$, Type 0 hyperedges tend to be filled with class-1 nodes, and Type 0 hyperedges are filled with class-0 nodes.\n        - (Non-Homophilic) As $\\mathscr{P} \\rightarrow 0.5$, for each hyperedge, half of it tends to be filled with class-1 nodes, and the rest half tends to be filled with class-0 nodes.\n    - Our data model is symmetric about $\\mathscr{P} = 0.5$. In other words, $\\mathscr{P}$ and $1 - \\mathscr{P}$ are expected to produce equally homophilic hypergraphs.\n        - Considering the entire hyperedges, the tendency of node classes being mixed within hyperedges is equivalent at hypergraphs that are generated with $\\mathscr{P}$ and those with $1-\\mathscr{P}$."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218368437,
                "cdate": 1700218368437,
                "tmdate": 1700218368437,
                "mdate": 1700218368437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dn07pAOYUX",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1 [Premise on Homophily]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`In the hypothesis, the premise of the article is that each hyperedge contains most of the same category vertices. However, there are hyperedges that do not satisfy this situation. For example, a hyperedge of size 10 contains 5 nodes of category A and 5 category B.`\n\n---\n\n### Response\n\n- We apologize for the confusion. The detailed descriptions regarding $\\mathscr{P}$ are provided in \u201cClarification for Homophily Ratio\u201d.\n- **Clarification W1.1:** Our theoretical analysis covers both general-sense homophily and non-homophily (i.e. homophily means that the same-class nodes tend to interact with each other).\n    - Both Theorem 1 and 2 hold for any $\\mathscr{P} \\in [0, 1]$, i.e., both homophilic and non-homophilic cases.\n    - With $\\mathscr{P} = 0.5$, each hyperedge is expected to contain half class-0 nodes and class-1 nodes.\n- **Experiment W1.2**: To demonstrate HypeBoy\u2019s efficacy when the homophily assumption of input data is violated, we conduct an additional experiment, described in the tables below.\n    - **Results**: HypeBoy is the most robust to decreasing homophily. Its performance gap to the SOTA baselines tends to increase as the homophily assumption of the input data is increasingly violated.\n    - The experimental details are as follows:\n        - We randomly choose two hyperedges ($e_{1}$ and $e_{2}$), and choose one node each from the chosen hyperedges ($v_{1} \\in e_{1}$ and $v_{2}\\in e_{2}$).\n        - Then, we swap two chosen nodes: $e^\\prime_1 = (e_1 \\setminus \\{v_1\\})  \\cup \\{v_2\\}$ and $e^\\prime_2 = (e_2 \\setminus \\{v_2\\}) \\cup \\{v_1\\}$, then $e_{1} = e^\\prime_{1}$ and $e_{2} = e^\\prime_{2}$.\n        - We repeat this process. With more swapping, a hypergraph is expected to get more non-homophilic.\n    - The results are added in the revised manuscript as Table 10.\n- **Argument W1.3**: HypeBoy is more effective than its competitors in less homophilic hypergraphs.\n    - The proposed method (HypeBoy) outperforms the baseline methods in non-homophilic benchmark datasets. See more details in the response for Q1.\n\n---\n\n### Node Swapping Experimental Results\n    \n**Cora (Value in a bracelet indicates the performance gap from that of the original hypergraph)**\n    \n|  | Original | Shuffle 50 | Shuffle 100 | Shuffle 150 | Shuffle 200 |\n| --- | --- | --- | --- | --- | --- |\n| No Pretraining | 0.485 | 0.467 (-0.018) | 0.447 (-0.038) | 0.435 (-0.050) | 0.409 (-0.076) |\n| TriCL | 0.602 | 0.591 (-0.011) | 0.561 (-0.041) | 0.557 (-0.045) | 0.536 (-0.065) |\n| HyperGCL | 0.603 | 0.574 (-0.029) | 0.537 (-0.066) | 0.520 (-0.083) | 0.500 (-0.103) |\n| HypeBoy (Ours) | 0.623 | 0.617 (**-0.006**) | 0.592 (**-0.031**) | 0.580 (**-0.043**) | 0.567 (**-0.056**) |\n    \n**Citeseer (Value in a bracelet indicates the performance gap from that of the original hypergraph)**\n    \n|  | Original | Shuffle 50 | Shuffle 100 | Shuffle 150 | Shuffle 200 |\n| --- | --- | --- | --- | --- | --- |\n| No Pretraining | 0.442 | 0.414 (-0.028)  | 0.390 (-0.052) | 0.359 (-0.083) | 0.338 (-0.104) |\n| TriCL | 0.517 | 0.511 (-0.006) | 0.475 (-0.042) | 0.452 (-0.065) | 0.418 (-0.099) |\n| HyperGCL | 0.470 | 0.458 (-0.012) | 0.433 (-0.037) | 0.409 (-0.061) | 0.377 (-0.093) |\n| HypeBoy (Ours) | 0.567 | 0.565 (**-0.002**) | 0.542 (**-0.025**) | 0.508 (**-0.059**) | 0.486 (**-0.081**) |"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218548671,
                "cdate": 1700218548671,
                "tmdate": 1700219383812,
                "mdate": 1700219383812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u9iwNGvKLE",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2 [Effectiveness of Hyperedge Filling Alone]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`One of the main contributions of the paper is hyperedge filling, but using only hyperedge filling (v1) in the ablation experiment is the worst among v1-v4. The effectiveness of using hyperedge filling alone is questionable, or it needs to be bound to feature reconstruction and projection heads.`\n\n---\n\n### Response Summary\n\n- We show that the *hyperedge filling* task is a better SSL task compared to other SSL tasks, including feature reconstruction and contrastive learning.\n- We clarify that it is common to employ additional parameters (e.g., projection heads) to complement an SSL task, and our competing SSL methods also utilize such additional parameters.\n\n### Response\n\n- We clarify the relationship between additional parameters (e.g. projection heads) and SSL methods:\n    - **Clarification W2.1**:  Projection heads are widely used tools to complement SSL methods, enhancing their performances.\n    - **Clarification W2.2**: Typical SSL pretext tasks, *contrastive learning* and *feature reconstruction*, also require additional parameters such as projection heads, decoders, and view generators.\n- Under such context, we have conducted two additional experiments to address the reviewer\u2019s concerns:\n    - We compared the *hyperedge filling* task with other SSL tasks under two settings: (1) encoder only, i.e., without additional parameters; (2) encoder + additional parameters. Under both settings, hyperedge filling is more effective than other SSL tasks.\n    - **Experiment W2.3 [Encoder-Only]**:\n        - **Summary**: When an encoder alone is employed, the *hyperedge filling* task is more effective than other hypergraph SSL tasks.\n        - **Setting**: We compare various SSL methods by removing their additional parameters.\n            - *Hyperedge filling*: It employs an encoder without projection heads (equivalent to V1 in the main paper).\n            - *Feature reconstruction*: We remove a decoder from the feature reconstruction method. Specifically, we make the embedding dimension of the encoder the same as the node feature dimension (other parts are the same as our feature reconstruction scheme).\n            - *Contrastive learning*: We remove projection heads from TriCL [1] (contrastive loss and augmentation steps are equivalent to TriCL).\n        - **Outcome details**: As shown in the below table, *hyperedge filling* without any parameters (V1) outperformed (1) *feature reconstruction* (V3) or (2) *contrastive learning* methods.\n        - **Revision**: We have added the results to the revised manuscript as Table 8.\n    - **Experiment W2.4 [Encoder + Additional Parameters]**:\n        - **Summary**: When additional parameters are also employed, the *hyperedge filling* task is more effective than other hypergraph SSL tasks.\n        - **Setting**: We compare various SSL methods that employ additional parameters.\n            - *Hyperedge filling*: This method employs an encoder with projection heads (equivalent to V2 in the main paper).\n            - *Feature reconstruction*: We employ both encoder and decoder (equivalent to V3 in the main paper).\n            - *Contrastive learning*: We employ TriCL and HyperGCL [2].\n        - **Outcome details**: By comparing the performance of *hyperedge filling* + projection head (V2) with that of V3, TriCL, and HyperGCL, we have verified that V2 outperforms other methods in terms of the average rank (this result can be obtained from Table 1 and 3).\n\n---\n\n### SSL methods W/O additional parameters\n\n|  | Cora | Citeseer | Pubmed | Cora-CA | DBLP |\n| --- | --- | --- | --- | --- | --- |\n| Contrastive learning W/O projection heads | 0.592 (0.067) | 0.445 (0.102) | 0.752 (0.040) | 0.621 (0.053) | 0.867 (0.005) |\n| Feature reconstruction W/O decoder | 0.586 (0.080) | 0.515 (0.092) | 0.747 (0.051) | 0.619 (0.073) | 0.873 (0.006) |\n| V1 (Hyperedge filling W/O projection heads) | **0.607 (0.082)** | **0.516 (0.112)** | **0.762 (0.036)** | **0.635 (0.060)** | **0.881 (0.005)** |"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218639999,
                "cdate": 1700218639999,
                "tmdate": 1700218639999,
                "mdate": 1700218639999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "siWhlQXYcU",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 1 [Hyperedge Homophily Ratio]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`In BASIC SETTING, one of the assumptions is that the homophily ratio of each hyperedge is in [0.5, 1]. In graphs, the homophily rate is defined as the proportion of intra-class edges to all edges. So how does this definition apply to the hypergraph? Do all datasets in the experiments satisfy this assumption?`\n\n---\n\n### Response Summary\n\n- Our theoretical analyses hold for both general-sense homophily- and non-homophily (i.e. homophily means that the same-class nodes tend to interact with each other).\n- Empirically, in both homophilic and non-homophilic hypergraphs, our method (HypeBoy) outperforms other SSL methods.\n\n---\n\n### Response\n\n- **Clarification Q1.1 [Theory]**:\n    - We are sorry for the confusion. The additional descriptions for $\\mathscr{P}$ are provided in Clarification for Homophily Ratio.\n    - We again clarify that our theorem is valid at any $\\mathscr{P} \\in [0,1]$. In other words, Theorem 1 and 2 hold for both homophilic and non-homophilic hypergraphs.\n- **Clarification Q1.2 [General-sense Homophily Measure]**:\n    - We have used one of the widely used hypergraph homophily analysis methods, proposed by Veldt et al. 2023 [3].\n    - The homophily ratio the reviewer has mentioned can be directly extended to hypergraphs as the proportion of hyperedges that consist of the same-class nodes to all hyperedges.\n    - However, this has limitations in measuring the extent of homophily within each hyperedge.\n        - For instance, consider two hyperedges $e_{i}$ and $e_{j}$ whose constituent node labels are $\\{{0,0,0,0,0,1}\\}$ and $\\{{0,0,0,1,1,1}\\}$, respectively.\n        - Although their label-mixed-extents significantly differ, both hyperedges are counted as non-same-class-node hyperedges.\n        - In this manner, this extension may fail to measure the higher-order homophily in a hypergraph.\n    - Notably, the homophily analysis method (Veldt et al. 2023) generalizes the homophily metric the reviewer has mentioned (i.e., the proportion of intra-class edges to all edges).\n- **Experiment Q1.3 [Homophily Levels in Hypergraphs]**:\n    - Since the homophily analysis is based on visualization analysis, we could not report a single value that represents the homophily of a dataset. Instead, analysis should be conducted by each hyperedge size (each hyperedge size has a separate plot). For each size plot, the line color represents the node class.\n        - An ***upward trend line*** indicates the corresponding class nodes exhibit higher-order homophilic characteristics in the corresponding size hyperedges.\n        - A ***downward trend line*** indicates the corresponding class nodes exhibit higher-order non-homophilic characteristics in the corresponding size hyperedges.\n    - We analyze three benchmark datasets: Cora, IMDB, and AMiner. The results are in Figure 6 of the revised manuscript (Appendix). Notably, these three datasets exhibit different homophilic characteristics. Details regarding the homophily analysis are as follows.\n        - **Cora** (*Homophilic*): Most classes show an upward trend.\n        - **IMDB** (*Non-Homophilic*): Most classes show a downward trend.\n        - **AMiner** (*Mixed*): Some classes show an upward trend and some classes show a downward trend.\n- **Experiment Q1.4 [Performance on Heterophilic Hypergraph]**:\n    - In short, HypeBoy outperforms other baseline methods in these three datasets: homophilic, non-homophilic, and partially homophilic hypergraphs (Table 1 of the main paper).\n    - Thus, we have verified that the empirical efficacy of HypeBoy is not limited to homophilic hypergraphs.\n    - This analysis is added to the revised manuscript."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218728332,
                "cdate": 1700218728332,
                "tmdate": 1700218728332,
                "mdate": 1700218728332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O1OfKw3z6K",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 2 [Experimental Setup]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`I noticed that in section 5.1, the article uses a setting of 1% of the training set, which is different from most articles. Most other papers use fixed division or 5/10 nodes per category. I'm curious about what the considerations are for such an experimental setup?`\n\n---\n\n### Response\n\n- **Experiment Q2.1**: In the reviewer\u2019s suggested setting, HypeBoy outperforms two SOTA hypergraph SSL methods (TriCL and HyperGCL).\n    - Since a fixed division split that is widely used for hypergraph node classification is unknown, we have employed the second setting: 5/10 training nodes per category.\n    - The results are shown below, where each method employs UniGCNII [4] as a backbone encoder.\n    - We have added this result to the revised manuscript as Table 11.\n- **Clarification Q2.2**: Moreover, we would like to clarify that our used setting is equivalent to the setting of Wei et al., 2022. [2].\n    \n---\n\n### Additional Data Splits Experimental Results\n    \n**5 training nodes**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP |\n| --- | --- | --- | --- | --- | --- |\n| No Pretraining | 0.537 (0.068) | 0.637 (0.038) | 0.716 (0.046) | 0.607 (0.049) | 0.778 (0.047) |\n| TriCL | 0.601 (0.052) | 0.710 (0.031) | 0.719 (0.038) | 0.676 (0.029) | 0.805 (0.025) |\n| HyperGCL | 0.570 (0.056) | 0.703 (0.029) | 0.722 (0.044) | 0.615 (0.044) | 0.795 (0.037) |\n| HypeBoy (Ours) | **0.631 (0.046)** | **0.720 (0.024)** | **0.729 (0.032)** | **0.678 (0.024)** | **0.815 (0.024)** |\n\n**10 training nodes**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP |\n| --- | --- | --- | --- | --- | --- |\n| No Pretraining | 0.627 (0.038) | 0.728 (0.022) | 0.737 (0.030) | 0.677 (0.030) | 0.824 (0.019) |\n| TriCL | 0.663 (0.026) | 0.757 (0.023) | 0.745 (0.031) | 0.697 (0.022) | 0.836 (0.019) |\n| HyperGCL | 0.644 (0.039) | 0.749 (0.021) | 0.743 (0.031) | 0.690 (0.034) | 0.836 (0.016) |\n| HypeBoy (Ours) | **0.677 (0.026)** | **0.758 (0.027)** | **0.749 (0.027)** | **0.709 (0.017)** | **0.846 (0.015)** |"
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218889892,
                "cdate": 1700218889892,
                "tmdate": 1700218889892,
                "mdate": 1700218889892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TenZLKTPx2",
                "forum": "DZUzOKE6og",
                "replyto": "dIHNA2c4Dz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "`[1]: Dongjin Lee and Kijung Shin. I\u2019m me, we\u2019re us, and i\u2019m us: Tri-directional contrastive learning on hypergraphs. In AAAI, 2023.`\n\n`[2]: Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, and Zhangyang Wang. Augmentations in hypergraph contrastive learning: Fabricated and generative. In NeurIPS, 2022.`\n\n`[3]: Nate Veldt, Austin R Benson, and Jon Kleinberg. Combinatorial characterizations and impossibilities for higher-order homophily. Science Advances, 9(1):eabq3200, 2023.`\n\n`[4]: Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. In IJCAI, 2021.`"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218907395,
                "cdate": 1700218907395,
                "tmdate": 1700219284228,
                "mdate": 1700219284228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "djaFnd2oP9",
                "forum": "DZUzOKE6og",
                "replyto": "dn07pAOYUX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_EPYj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_EPYj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response, and I have read via the feedback. I maintain my review score."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659645040,
                "cdate": 1700659645040,
                "tmdate": 1700659645040,
                "mdate": 1700659645040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZFL4FndF6A",
            "forum": "DZUzOKE6og",
            "replyto": "DZUzOKE6og",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_4MNM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_4MNM"
            ],
            "content": {
                "summary": {
                    "value": "The paper makes an in-time contribution to studying the generative pretraining strategy for hypergraph neural networks. Specifically,  a novel generative SSL task on hypergraphs, hyperedge filling, is proposed, with the sound analysis demonstrating its effectiveness for node classification tasks. Extensive experiments are performed to support the claim.\n\nThe main intuition behind the magic of hyperedge filling is that the hyperedge, when it satisfies the homophily assumption, is indicative of the node membership and eventually helps node classification. This analysis supports the intuition. Although there might be some inconsistency between theory and practice, I appreciate the analysis part a lot."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Nice and reasonable intuition to motivate the algorithm: The forming of hyperedge indicates the nodes share the same (or similar) membership. This might be motivated by the stochastic block models. It is a great intuition and I think it fits many applications such as social networks.\n- Sound theoretical analysis to support the intuition. This is done by analyzing how the representations change to optimize hypergraph filling loss can improve node classification results.\n- Extensive empirical results are provided. More appreciatively, the numerical characteristics, such as proximity, are examined to reach certain conclusions."
                },
                "weaknesses": {
                    "value": "I do not have major criticisms for this paper. I only have some questions regarding the analysis part, which might result from missing some points during reading.\n\n-  In the hyperedge filling analysis, it seems to not relate to the neural network architecture. How do authors think the choice of the hypergraph neural networks would affect the performance?\n- In the hyperedge filling process (F2), the representation is updated with a gradient w.r.t. the hyperedge filling loss and a step size $\\gamma$. Per my reading of the proof, the value of $\\gamma$ (only need to be greater than 0) seems to not affect the result, while in practice this might not be true. Do I miss some points here?\n- Is the edge filling optimal to perform as it is in hypergraphs? Considering I am gonna perform clique expansion to get a graph and perform edge filling. How would the result be different?"
                },
                "questions": {
                    "value": "Please see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Reviewer_4MNM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611353963,
            "cdate": 1698611353963,
            "tmdate": 1700667760795,
            "mdate": 1700667760795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zf5G2Vd7mT",
                "forum": "DZUzOKE6og",
                "replyto": "ZFL4FndF6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer 4MNM"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4MNM,\n\nWe appreciate your generous review. We provide our responses to each of your comments below."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218005288,
                "cdate": 1700218005288,
                "tmdate": 1700218005288,
                "mdate": 1700218005288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Egvnews77Z",
                "forum": "DZUzOKE6og",
                "replyto": "ZFL4FndF6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1 [Choice of Hypergraph Neural Network]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`In the hyperedge filling analysis, it seems to not relate to the neural network architecture. How do authors think the choice of the hypergraph neural networks would affect the performance?`\n\n---\n\n### Response\n\n- **Argument W1.1**: We have verified that a neural network that preserves the higher-order information of each hyperedge aligns well with HypeBoy.\n- **Empirical Support W1.2**: We have employed four different encoders as the backbone encoders of HypeBoy. Among the four, the encoder that uses higher-order information the least, HGNN [1], shows the worst performance.\n    - Specifically, the encoders are HGNN [1], MeanPoolingConv [2], SetGNN [3], and UniGCNII [4].\n    - A noticeable difference between HGNN and the other encoders is their use of input data structure: HGNN transforms a hypergraph into a graph via clique-expansion and performs graph convolution, while the others conduct message passing on the hypergraph structure itself (details are described in Section 2.1 of the main paper).\n    - As discussed by Dong et al., [5], clique expansion may cause significant information loss in higher-order interactions, potentially causing suboptimal performance of a machine learning model.\n    - To effectively solve the hyperedge filling task, an encoder should be capable of learning higher-order interactions, and such a requirement may cause a suboptimal performance of HGNN.\n\n---\n\n### Encoder Experiments\n\n|  | Average ranking | Citeseer | Cora | Cora-CA | Pubmed | DBLP-P |\n| --- | --- | --- | --- | --- | --- | --- |\n| HGNN + HypeBoy | 3.4 | 0.521 (0.091) | 0.611 (0.098) | 0.600 (0.060) | 0.768 (0.043) | 0.874 (0.005) |\n| MeanPoolingConv + HypeBoy | 2.2 | 0.545 (0.083) | 0.612 (0.078) | 0.630 (0.044) | 0.773 (0.034) | 0.868 (0.005) |\n| SetGNN + HypeBoy | 3.0 | 0.538 (0.091) | 0.623 (0.064) | 0.610 (0.039) | 0.734 (0.034) | 0.842 (0.007) |\n| UniGCNII + HypeBoy | **1.4**|  **0.567 (0.098)** | **0.623 (0.077)** | **0.663 (0.046)** | **0.770 (0.034)** | **0.882 (0.004)** |"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218097670,
                "cdate": 1700218097670,
                "tmdate": 1700218188333,
                "mdate": 1700218188333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nhrrPOnWlH",
                "forum": "DZUzOKE6og",
                "replyto": "ZFL4FndF6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2 [Step Size in Practice]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`In the hyperedge filling process (F2), the representation is updated with a gradient w.r.t. the hyperedge filling loss and a step size. Per my reading of the proof, the value of\u00a0(only need to be greater than 0) seems to not affect the result, while in practice this might not be true. Do I miss some points here?`\n\n---\n\n### Response\n\n- **Clarification W2.1**: Theorem 1 only states that the updated embedding is more beneficial and does not discuss the extent of benefit, which is expected to be influenced by the step size, as the reviewer pointed out.\n- **Clarification W2.2**: In practice, as the reviewer has mentioned, the efficacy of the hyperedge filling task is influenced by the step size. On the other hand, with a fixed step size of 0.001, HypeBoy outperforms existing hypergraph SSL methods in most of the datasets (in all our experiments, we have fixed the step size of 0.001, while that for other methods are tuned. Details are in Appendix).\n- **Argument W2.3**: Thus, we conclude that HypeBoy is relatively robust to the step size.\n- **Clarification W2.4:** Apart from the theoretical results, we delineate some possible reasons why step size may affect the node classification performance\n    - Real-world datasets may not (perfectly) align with our data assumptions that are used in theoretical analysis.\n    - We use a hypergraph neural network to obtain representations of nodes in practice, while we have assumed each representation as a free variable that is directly being updated."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218176731,
                "cdate": 1700218176731,
                "tmdate": 1700218176731,
                "mdate": 1700218176731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oy3pZK4rfN",
                "forum": "DZUzOKE6og",
                "replyto": "ZFL4FndF6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "`[1]: Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI, 2019.`\n\n`[2]: Dongjin Lee and Kijung Shin. I\u2019m me, we\u2019re us, and i\u2019m us: Tri-directional contrastive learning on hypergraphs. In AAAI, 2023.`\n\n`[3]: Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework for hypergraph neural networks. In ICLR, 2022.`\n\n`[4]: Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. In IJCAI, 2021.`\n\n`[5]: Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn: Hypergraph networks with hyperedge neurons. In ICML Workshop on Graph Representation Learning and Beyond (GRL+), 2020.`\n\n`[6]: Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.`"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218289001,
                "cdate": 1700218289001,
                "tmdate": 1700219302310,
                "mdate": 1700219302310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T5QHwplbqV",
                "forum": "DZUzOKE6og",
                "replyto": "ZFL4FndF6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_4MNM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_4MNM"
                ],
                "content": {
                    "title": {
                        "value": "Thank You for the Response"
                    },
                    "comment": {
                        "value": "I acknowledge the response and would like to increase my positive rate 6 --> 8."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575413657,
                "cdate": 1700575413657,
                "tmdate": 1700667831028,
                "mdate": 1700667831028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QwzwVMgSWi",
            "forum": "DZUzOKE6og",
            "replyto": "DZUzOKE6og",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_ctQb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_ctQb"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a generative self-supervised learning task called \"hypergraph filling\"  and explores generative self-supervised learning on hypergraphs. The author focuses on addressing issues related to overemphasized proximity, dimensional collapse, and non-uniformity/alignment problems in learned representations. To tackle these issues, the author proposes the \"HYPEBOY\" strategy for hypergraphs, both in theory and through empirical experiments. Furthermore, the author demonstrates the effectiveness of this approach in tasks such as node classification and link prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author effectively identifies the issue of overemphasized proximity and demonstrates the beneficial impact of augmentation. Additionally, to address the problem of dimensional collapse, the author introduces a two-stage training scheme, which helps reduce the reliance on projection heads."
                },
                "weaknesses": {
                    "value": "1. The rationale behind employing generative SSL for hypergraph representation is not convincingly established, and it confronts several challenges, including dimensional collapse.\n2. The author devises a SSL strategy for hypergraphs by utilizing existing encoders and decoders such as UniGCNII, HNN, and MLP, without introducing any novel model designs. The concept of the projection head for hypergraph encoding is inspired by Deep Sets[1].\n3.The author utilizes UniGCNII[2] as an encoder for HYPERBOY, and primarily focuses on homogeneous hypergraphs. However, it's important to note that heterogeneous hypergraphs are also prevalent, and the embedding method for hyperedges is not discussed.\n\n[1] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. NeurIPS 2017: 3391-3401.\n[2] Jing Huang, Jie Yang. UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks.IJCAI 2021: 2563-2569."
                },
                "questions": {
                    "value": "1. It's not straightforward to extend edge reconstruction methods to hyperedges in SSL. Considering this challenge, why did the author choose to employ SSL for hypergraphs without addressing the embeddings of hyperedges explicitly.\n2. Could you elaborate on the real-world applications of the hypergraph filling task? How is this task practically relevant?\n3. The author uses Gaussian distribution, Bernoulli sampling, and binomial distribution in the method. Could you explain the reasoning behind these choices and how they compare to more conventional methods like attention strategies and neural network approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Reviewer_ctQb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828542671,
            "cdate": 1698828542671,
            "tmdate": 1699636089278,
            "mdate": 1699636089278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14oG58rjdX",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer ctQb,"
                    },
                    "comment": {
                        "value": "Dear Reviewer ctQb,\n\nWe express our gratitude for your considerate review. We summarize our response to each of your comments below. In doing so, please let us first make an overarching clarification."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216615631,
                "cdate": 1700216615631,
                "tmdate": 1700216615631,
                "mdate": 1700216615631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5xHeuKWwnZ",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Overall Clarification"
                    },
                    "comment": {
                        "value": "- We clarify that one of the primary goals of Self-Supervised Learning (SSL) is to empower a machine learning model to better solve some ***downstream** **tasks***.\n- To this end, the majority of SSL methods solve ***pretext** **tasks***, which are distinct from the target ***downstream tasks***.\n- Our primary contribution is devising a ***pretext task*** for hypergraph representation learning, called the hyperedge filling task.\n    - **The goal of our pretext task**: By solving the hyperedge filling task, we mainly aim to better solve the node classification task, which is one of the most popular downstream tasks in the hypergraph representation learning field.\n    - **Theoretical connection**: While our hyperedge filling task is seemingly not related to node classification, we have theoretically analyzed that solving the hyperedge filling task is beneficial for node classification, as described in Section 3.2 of the main paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216646977,
                "cdate": 1700216646977,
                "tmdate": 1700216646977,
                "mdate": 1700216646977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aiPJyJJOTA",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1 [Rationale of Hyperedge Filling Task]"
                    },
                    "comment": {
                        "value": "### Comment\n`The rationale behind employing generative SSL for hypergraph representation is not convincingly established, and it confronts several challenges, including dimensional collapse.`\n\n---\n\n### Response\n- As described in our **Overall Clarification**, we aimed to better solve the node classification task by empowering a hypergraph neural network with the proposed hyperedge filling task.\n- **Hyperedge filling helps node classification (Sec. 3)**: Theoretically, solving the proposed SSL pretext task, hyperedge filling, can improve node classification performance. Moreover, we have experimentally demonstrated that our method HypeBoy outperforms other baseline methods.\n- **Findings in other domains (Sec. 1)**: The effectiveness of generative SSL has been demonstrated in many downstream tasks for a range of domains [1, 2].\n- **Utility in label-scarce scenarios (Sec. 1)**: Efficacy of (semi-)supervised learning (e.g., by directly aiming at node classification) is limited when available labels are scarce. Generative SSL tackles such a label-scarcity issue by providing abundant labels from the hypergraph structure itself.\n    - We experimentally validate this with our experiments in Table 1. See higher average ranks of SSL methods over the (Semi-)Supervised ones."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216710222,
                "cdate": 1700216710222,
                "tmdate": 1700217521503,
                "mdate": 1700217521503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WbKqzB9tIo",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2 [Contribution on Neural Network]"
                    },
                    "comment": {
                        "value": "### Comment\n`The author devises a SSL strategy for hypergraphs by utilizing existing encoders and decoders such as UniGCNII, HNN, and MLP, without introducing any novel model designs. The concept of the projection head for hypergraph encoding is inspired by Deep Sets[1].`\n\n---\n\n### Response\n\n- We make the following clarifications:\n    - **Clarification W2.1**: We emphasize that our main contribution lies in ***devising a pretext task*** for hypergraph SSL, as described in Section 2.1 of the main paper, not proposing a novel neural network architecture for hypergraphs.\n    - **Clarification W2.2**: Moreover, as evidenced in the below table, the proposed hyperedge filling task can empower various hypergraph neural networks, not only UniGCNII, in the node classification task.\n        - We have additionally employed three hypergraph neural networks, which were used as backbone encoders of SOTA hypergraph SSL methods (TriCL [3] and HyperGCL [4]).\n        - As shown below, HypeBoy outperforms other SSL methods in most of the settings.\n- As such, we argue the following:\n    - **Argument W2.3**: Devising a pretext task, hyperedge filling, that can empower a wide range of hypergraph neural networks is our significant contribution.\n        - Furthermore, given HypeBoy\u2019s potential applicability to a broad spectrum of hypergraph neural networks, our contribution is on par with introducing a novel neural network architecture.\n    - **Argument W2.4**: Devising an SSL method (consisting of augmentation, encoding, and loss function) that satisfies the desired properties (Property 1 - 3 in Section 4 of the main paper) is our non-trivial contribution.\n    - **Argument W2.5**: Thus, our contributions are valid even if we employ existing neural networks as the hypergraph encoders.\n\n---\n\n### Additional encoder experimental results\n\n**Encoder: HGNN [5], used as the encoder for TriCL.**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.419 (0.078) | 0.500 (0.072) | 0.729 (0.050) | 0.502 (0.057) | 0.853 (0.008) |\n| TriCL | 0.493 (0.103) | 0.569 (0.010) | 0.743 (0.041) | 0.572 (0.061) | **0.874 (0.005)** |\n| HyperGCL | 0.472 (0.093) | 0.595 (0.076) | 0.763 (0.043) | 0.533 (0.069) | 0.858 (0.004) |\n| HypeBoy (Ours) | **0.521 (0.091)** | **0.611 (0.098)** | **0.768 (0.043)** | **0.600 (0.060)** | **0.874 (0.005)** |\n\n**Encoder: MeanPoolingConv [3], used as the encoder for TriCL.**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.399 (0.100) | 0.485 (0.089) | 0.724 (0.040) | 0.506 (0.075) | 0.857 (0.007) |\n| TriCL | 0.466 (0.083) | 0.598 (0.089) | 0.740 (0.038) | 0.573 (0.048) | **0.870 (0.005)** |\n| HyperGCL | 0.432 (0.094) | 0.596 (0.076) | 0.742 (0.043) | 0.550 (0.067) | 0.857 (0.009) |\n| HypeBoy (Ours) | **0.545 (0.083)** | **0.612 (0.078)** | **0.773 (0.034)** | **0.630 (0.044)** | 0.868 (0.005) |\n\n**Encoder: SetGNN [6], used as the encoder for HyperGCL.**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.436 (0.063) | 0.489 (0.092) | 0.698 (0.045) | 0.507 (0.067) | 0.828 (0.010) |\n| TriCL | 0.489 (0.076) | 0.577 (0.084) | 0.720 (0.045) | 0.578 (0.049) | **0.851 (0.005)** |\n| HyperGCL | 0.468 (0.085) | 0.546 (0.075) | 0.731 (0.039) | 0.569 (0.053) | 0.836 (0.007) |\n| HypeBoy (Ours) | **0.538 (0.091)** | **0.623 (0.064)** | **0.733 (0.039)** | **0.610 (0.039)** | 0.842 (0.007) |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216760664,
                "cdate": 1700216760664,
                "tmdate": 1700217583551,
                "mdate": 1700217583551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IQALbuOyX1",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 3 [Heterogeneous Hypergraphs and Hyperedge Embedding]"
                    },
                    "comment": {
                        "value": "### Comment\n`The author utilizes UniGCNII[2] as an encoder for HYPERBOY, and primarily focuses on homogeneous hypergraphs. However, it's important to note that heterogeneous hypergraphs are also prevalent, and the embedding method for hyperedges is not discussed.`\n\n---\n\n### Response\n- To address your concerns, we conducted further experiments about heterogeneous hypergraphs. We summarize the results below:\n    - **Experiment W3.1:** In a heterogeneous hypergraph dataset, we have verified that HypeBoy outperforms two SOTA hypergraph SSL methods.\n        - **Dataset**: We found one heterogeneous hypergraph dataset (the ACM dataset [7]) that meets the following criteria: (1) exhibiting heterogeneity, (2) having node attributes and labels, and (3) being publicly accessible.\n        - **Results**: HypeBoy achieves the best performance among the SOTA SSL baselines. The results are provided in the below table. We have added this result to the revised manuscript as Table 12.\n- Further, we make the following clarification w.r.t. hyperedge embeddings:\n    - **Clarification W3.2:** Our primary goal is to obtain node representations for node classification. In this process, we do not explicitly obtain or utilize hyperedge embeddings.\n    - **Clarification W3.3:** Potentially, however, we can obtain good hyperedge embedding by appropriately employing node representations obtained from a hypergraph neural network.\n\n---\n\n### Results on Heterogeneous Hypergraph (ACM)\n\n|  | UniGCNII | TriCL | HyperGCL | HypeBoy (Ours) |\n| --- | --- | --- | --- | --- |\n| ACM  | 0.400 (0.031) | 0.403 (0.024) | 0.408 (0.034) | **0.417 (0.026)** |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217231413,
                "cdate": 1700217231413,
                "tmdate": 1700218964981,
                "mdate": 1700218964981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZcsMlkxPp3",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 1 [Extending Edge Reconstruction Method and Hyperedge Embedding]"
                    },
                    "comment": {
                        "value": "### Comment\n`It's not straightforward to extend edge reconstruction methods to hyperedges in SSL. Considering this challenge, why did the author choose to employ SSL for hypergraphs without addressing the embeddings of hyperedges explicitly.`\n\n---\n\n### Response \n\n- We make the following clarifications:\n    - **Clarification Q1.1:** Explicit hyperedge embeddings are not required for both hyperedge filling and node classification, which are the focus of this paper.\n        - Hyperedge filling task\n            - This task aims to fill a correct node to a given subset of a hyperedge. To this end, we employ representations of a node and a subset, whose steps are described in Section 4.2.\n            - Thus, we do not need hyperedge embeddings to perform hyperedge filling.\n        - Node classification task\n            - This task aims to correctly classify a node. To this end, we employ a node representation.\n            - Thus, we do not need hyperedge embeddings to perform node classification.\n    - **Experiment Q1.2**: Our proposed *hyperedge filling* method outperforms the *hyperedge reconstruction* method, which is a direct extension of graph edge reconstruction.\n        - *Hypergraph reconstruction* method details are as follows:\n            - For each SSL training epoch, we create negative-sample hyperedges by using the size-negative-sampling strategy, which is described in Appendix D.3.\n            - Then, we encourage an encoder to maximize the probability of the real hyperedges, while minimizing that of the negative-sample hyperedges.\n            - To obtain the probability of a hyperedge, we (1) obtain a hyperedge embedding via sum-pooling of embeddings of nodes that are included in the corresponding hyperedge and (2) feed the hyperedge embedding to the MLP classifier.\n        - We have added this result to the revised manuscript as Table 9.\n- Under such context, we make the following argument:\n    - **Argument Q1.3:** The superiority of the *hyperedge filling* task, which we have theoretically shown the effectiveness on the node classification, over the *hyperedge reconstruction* method is evidenced by the results in (**Experiment Q1.2**).\n\n---\n\n### Hyperedge Reconstruction SSL Method\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| Hyperedge Reconstruction | 0.439 (0.076) | 0.569 (0.097) | 0.730 (0.049) | 0.591 (0.065) | 0.858 (0.006) |\n| HypeBoy (Ours) | **0.567 (0.098)** | **0.623 (0.077)** | **0.770 (0.034)** | **0.663 (0.046)** | **0.882 (0.004)** |"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217728486,
                "cdate": 1700217728486,
                "tmdate": 1700218954148,
                "mdate": 1700218954148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HB4NVJKLV3",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 2 [Application of Hyperedge Filling Task]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`Could you elaborate on the real-world applications of the hypergraph filling task? How is this task practically relevant?`\n\n---\n\n### Response\n\n- As described in the **Overall Clarification**, our primary objective is to empower a hypergraph neural network to better solve the node classification task. That is, we employed the proposed *hyperedge filling* task as a means to achieve this goal, rather than considering it as the final objective.\n- While *hyperedge filling* is not our ultimate goal, *hyperedge filling* itself has potential applications described below. We have added these potential applications in our revised manuscript.\n    - **Email recipient recommendation**\n        - Given recipients of an email, recommend a user likely to be added as a recipient of the email.\n        - Nodes in a hypergraph indicate a user and each hyperedge indicates an email, consisting of a sender, recipients, and CCs.\n    - **Item recommendation**\n        - Given a shopping cart list, recommend an item to be co-purchased with listed items.\n        - Nodes in a hypergraph indicate an item and each hyperedge contains a set of co-purchased items."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217782864,
                "cdate": 1700217782864,
                "tmdate": 1700217925604,
                "mdate": 1700217925604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XrQWhh8gJ5",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Question 3 [Assumptions]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`The author uses Gaussian distribution, Bernoulli sampling, and binomial distribution in the method. Could you explain the reasoning behind these choices and how they compare to more conventional methods like attention strategies and neural network approaches?`\n\n---\n\n### Response\n\n- We clarify that the mentioned conditions are only assumed for the simplicity of our theoretical analysis (Section 3.2), and they are not utilized in our design of HypeBoy, which is our proposed SSL method.\n- We have clarified this in our revised manuscript."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217912152,
                "cdate": 1700217912152,
                "tmdate": 1700219028845,
                "mdate": 1700219028845,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B4YmbTRJdv",
                "forum": "DZUzOKE6og",
                "replyto": "QwzwVMgSWi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "`[1]: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022.`\n\n`[2]: Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In KDD, 2022.`\n\n`[3]: Dongjin Lee and Kijung Shin. I\u2019m me, we\u2019re us, and i\u2019m us: Tri-directional contrastive learning on hypergraphs. In AAAI, 2023.`\n\n`[4]: Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, and Zhangyang Wang. Augmentations in hypergraph contrastive learning: Fabricated and generative. In NeurIPS, 2022.`\n\n`[5]: Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI, 2019.`\n\n`[6]: Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework for hypergraph neural networks. In ICLR, 2022.`\n\n`[7]: Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In WWW, 2019.`"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217972740,
                "cdate": 1700217972740,
                "tmdate": 1700218330273,
                "mdate": 1700218330273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8E48UI8cLz",
                "forum": "DZUzOKE6og",
                "replyto": "En74pUOAET",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_ctQb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_ctQb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses, they have enhanced my understanding of the paper's intent. While I grasp the concept that hyperedge filling contributes to node classification, I believe merely relying on experiments and results to demonstrate the motivation behind generative Semi-Supervised Learning (SSL) may be insufficient. The issue of dimensional collapse introduced by generative SSL lacks a reasoned explanation.\nThe authors advocate for concentrating on the tasks of hyperedge filling and node classification. Complementary experiments have indeed confirmed the superior performance of the hyperedge filling method compared to the hyperedge reconstruction method, but this claim is based solely on homogeneous hyperedge experiments. How does this method fare in the context of heterogeneous hyperedges?"
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726870292,
                "cdate": 1700726870292,
                "tmdate": 1700726870292,
                "mdate": 1700726870292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "caIir2Re1J",
            "forum": "DZUzOKE6og",
            "replyto": "DZUzOKE6og",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_3LGX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1610/Reviewer_3LGX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the first generative SSL methods, especially designed for hypergraph learning. The proposed self-supervised task is to fill in the missing node from an incomplete hyperedge. The authors show theoretically and empirically that, unde some conditions, this self-supervised task is well aligned with hypernode classification, achieving better results than other contrastive SSL methods from the hypergraph literature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper identify and fill in a gap existent in the hypergraph literature: designing generative SSL tasks for hypergraph representation learning. The proposed hyperedge filling task is intuitive and well-suited for hypergraph representation learning. The theoretical finding enhances confidence in the approach.\n- The experimental section is comprehensive, demonstrating the individual contribution of each component.\n- The paper is generally well written."
                },
                "weaknesses": {
                    "value": "- All the SSL-based results presented in the paper uses UniGCNII as a backbone. The authors motivate this decision by saying that this combinations achieves best overall performances. While I consider the comparison fair (since all the reported baselines uses UniGCNII as well), it is essential to conduct experiments that demonstrate the method's advantages when applied with various backbones. This additional experiments would clearly demonstrate the advantages of the proposed method. \n- While the authors did a good job in explaining the intuition behind this, it is somewhat discouraging to observe that the suggested approach performs noticeably worse when the feature reconstruction warmup is omitted (Table 3)\n- Minor: It would be useful to include the UNIGCNII baseline (without any of the 3 component) as a line in Table 3\n- Given the big improvement brought by the augmentation (masking) scheme (Table 6 in appendix), I am curious to know if the other baseline methods benefit from a similar augmentation step."
                },
                "questions": {
                    "value": "Please see the Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1610/Reviewer_3LGX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1610/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851795886,
            "cdate": 1698851795886,
            "tmdate": 1700557822366,
            "mdate": 1700557822366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZQQKtt1lUp",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer 3LGX"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3LGX,\n\nWe deeply appreciate your constructive review. Below, we provide our response to each of your comments. Our empirical outcomes have become more solid, thanks to your comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215726548,
                "cdate": 1700215726548,
                "tmdate": 1700215726548,
                "mdate": 1700215726548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZL25jdeKAd",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 1 [Various Backbone Encoders]"
                    },
                    "comment": {
                        "value": "### Comment\n\n`All the SSL-based results presented in the paper uses UniGCNII as a backbone. The authors motivate this decision by saying that this combinations achieves best overall performances. While I consider the comparison fair (since all the reported baselines uses UniGCNII as well), it is essential to conduct experiments that demonstrate the method's advantages when applied with various backbones. This additional experiments would clearly demonstrate the advantages of the proposed method.`\n\n---\n\n### Response\n\n- We additionally have employed three hypergraph encoders used in SOTA hypergraph SSL methods (TriCL [1] and HyperGCL [2]).\n    - **Additional encoders:** HGNN [3], MeanPoolingConv [1], and SetGNN [4]\n- Our method (HypeBoy) outperformed all SOTA hypergraph SSL methods in all the used encoders. The results are presented in the below tables.\n    - **Details**: Compared to the 2nd-best SSL method (TriCL), HypeBoy achieves up to 4.2% better accuracy in all the encoder settings.\n- We have added these results in Table 7 of our revised manuscript.\n\n---\n\n### Encoder Experiments\n\n**Encoder: HGNN [3], used in TriCL [1].**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.419 (0.078) | 0.500 (0.072) | 0.729 (0.050) | 0.502 (0.057) | 0.853 (0.008) |\n| TriCL | 0.493 (0.103) | 0.569 (0.010) | 0.743 (0.041) | 0.572 (0.061) | **0.874 (0.005)** |\n| HyperGCL | 0.472 (0.093) | 0.595 (0.076) | 0.763 (0.043) | 0.533 (0.069) | 0.858 (0.004) |\n| HypeBoy (Ours) | **0.521 (0.091)** | **0.611 (0.098)** | **0.768 (0.043)** | **0.600 (0.060)** | **0.874 (0.005)** |\n\n**Encoder: MeanPoolingConv [1], used in TriCL [1].**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.399 (0.100) | 0.485 (0.089) | 0.724 (0.040) | 0.506 (0.075) | 0.857 (0.007) |\n| TriCL | 0.466 (0.083) | 0.598 (0.089) | 0.740 (0.038) | 0.573 (0.048) | **0.870 (0.005)**|\n| HyperGCL | 0.432 (0.094) | 0.596 (0.076) | 0.742 (0.043) | 0.550 (0.067) | 0.857 (0.009) |\n| HypeBoy (Ours) | **0.545 (0.083)**| **0.612 (0.078)** | **0.773 (0.034)** | **0.630 (0.044)** | 0.868 (0.005) |\n\n\n**Encoder: SetGNN [4], used in HyperGCL [2].**\n\n|  | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P |\n| --- | --- | --- | --- | --- | --- |\n| W/O Pretraining | 0.436 (0.063) | 0.489 (0.092) | 0.698 (0.045) | 0.507 (0.067) | 0.828 (0.010) |\n| TriCL | 0.489 (0.076) | 0.577 (0.084) | 0.720 (0.045) | 0.578 (0.049) | **0.851 (0.005)**|\n| HyperGCL | 0.468 (0.085) | 0.546 (0.075) | 0.731 (0.039) | 0.569 (0.053) | 0.836 (0.007) |\n| HypeBoy (Ours) | **0.538 (0.091)** | **0.623 (0.064)** | **0.734 (0.034)** | **0.610 (0.039)** | 0.842 (0.007) |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216007988,
                "cdate": 1700216007988,
                "tmdate": 1700217402500,
                "mdate": 1700217402500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NVzuGQBoxl",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 2 [Limited Performance of Only Hyperedge Filling Method]"
                    },
                    "comment": {
                        "value": "### Comment\n`While the authors did a good job in explaining the intuition behind this, it is somewhat discouraging to observe that the suggested approach performs noticeably worse when the feature reconstruction warmup is omitted (Table 3).`\n\n---\n\n### Response\n\n- We make the following clarifications:\n- **Clarification W2.1**: Our method outperforms two SOTA hypergraph SSL methods, even without feature reconstruction warmup.\n    - In terms of the average ranks, *HypeBoy variant V2* (no feature reconstruction) outperforms *TriCL* and *HyperGCL* (the SOTA hypergraph SSL methods).\n    - This can be derived by comparing Tables 1 and 3 of the main paper.\n- **Clarification W2.2**: Moreover, the *hyperedge filling* task is more effective than *feature reconstruction*.\n    - In terms of the average ranks, *V2* (hyperedge filling with projection heads) outperforms *V3* (only feature reconstruction).\n- **Clarification W2.3**: Lastly, feature reconstruction on hypergraphs is also our contribution, since we are the first to employ the feature reconstruction scheme on hypergraphs, to our best knowledge."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216125329,
                "cdate": 1700216125329,
                "tmdate": 1700217383641,
                "mdate": 1700217383641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vVykb8H3Dc",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 3 [Adding UNIGCNII also in ablation study]"
                    },
                    "comment": {
                        "value": "### Comment\n`Minor: It would be useful to include the UNIGCNII baseline (without any of the 3 component) as a line in Table 3`\n\n---\n\n### Response\n- We have revised Table 3 based on your comment: UniGCNII [5] is added as an additional baseline in Table 3.\n- As we originally concluded, we find that each component of HypeBoy contributes to its performance gain.\n\n---\n\n### Updated Table 3 (The first line \u201cNA\u201d is added).\nThe ablation study with four variants of HypeBoy on node classification under the fine-tuning protocol.\nF.R., H.F., and P.H. denote Feature Reconstruction, Hyperedge Filling, and Projection Heads, respectively. A.R. denotes the average ranking among all methods.\n***NA denotes no pretraining***. HypeBoy outperforms others in most datasets, justifying each of its components.\n\n|  | F.R. | H.F. | P.H. | Citeseer | Cora | Pubmed | Cora-CA | DBLP-P | DBLP-A | AMiner | IMDB | MN-40 | 20 News | House | A.R. |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| NA | \u2717 | \u2717 | \u2717 | 0.442 (0.090) | 0.485 (0.074) | 0.741 (0.039) | 0.548 (0.070) | 0.874 (0.006) | 0.658 (0.039) | 0.325 (0.017) | 0.425 (0.039) | 0.908 (0.011) | 0.709 (0.010) | 0.508 (0.043) | 5.5 |\n| V1 | \u2717 | \u2713 | \u2717 | 0.516 (0.112) | 0.607 (0.082) | 0.762 (0.036) | 0.635 (0.060) | 0.881 (0.005) | 0.785 (0.029) | 0.335 (0.028) | 0.468 (0.031) | 0.900 (0.011) | 0.774 (0.009) | 0.685 (0.045) | 4.3 |\n| V2 | \u2717 | \u2713 | \u2713 | 0.527 (0.096) | 0.597 (0.092) | 0.767 (0.032) | 0.635 (0.060) | 0.882 (0.005) | 0.791 (0.039) | 0.338 (0.022) | 0.469 (0.033) | 0.906 (0.010) | 0.770 (0.009) | 0.696 (0.049) | 3.3 |\n| V3 | \u2713 | \u2717 | \u2717 | 0.520 (0.093) | 0.589 (0.082) | 0.741 (0.039) | 0.612 (0.066) | 0.878 (0.004) | 0.799 (0.023) | 0.339 (0.021) | 0.463 (0.027) | 0.914 (0.009) | 0.775 (0.009) | 0.701 (0.048) | 3.6 |\n| V4 | \u2713 | \u2713 | \u2717 | 0.560 (0.099) | 0.618 (0.085) | 0.765 (0.031) | 0.653 (0.043) | 0.880 (0.004) | 0.803 (0.024) | 0.340 (0.020) | 0.475 (0.023) | 0.908 (0.010) | 0.774 (0.010) | 0.693 (0.050) | 2.5 |\n| Ours | \u2713 | \u2713 | \u2713 | 0.567 (0.098) | 0.623 (0.077) | 0.770 (0.034) | 0.663 (0.046) | 0.882 (0.004) | 0.806 (0.023) | 0.341 (0.022) | 0.476 (0.025) | 0.904 (0.009) | 0.776 (0.009) | 0.704 (0.048) | 1.4 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216351190,
                "cdate": 1700216351190,
                "tmdate": 1700219137398,
                "mdate": 1700219137398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CM2YK60IBW",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weakness 4 [Augmentation of Baseline Methods]"
                    },
                    "comment": {
                        "value": "### Comment\n`Given the big improvement brought by the augmentation (masking) scheme (Table 6 in appendix), I am curious to know if the other baseline methods benefit from a similar augmentation step.`\n\n---\n\n### Response\n\n- **Clarification W4.1:** HyperGCL and TriCL (the SOTA hypergraph SSL baseline methods) also employ augmentation strategies, and their results were obtained with the augmentations.\n    - Specifically, both HyperGCL and TriCL are contrastive learning methods that create hypergraph views by augmenting input hypergraphs.\n    - To this end, they utilize the below methods:\n        - HyperGCL: This method generates augmented views of a hypergraph by using a hypergraph VAE model. The hypergraph VAE model is being trained with contrastive loss.\n        - TriCL: This method masks certain columns of the input node feature matrix and drops a certain number of nodes from each hyperedge.\n- **Argument W4.2:** Thus, we emphasize the *fairness* of our experiments and the *superiority* of our proposed model and self-supervision loss."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216398620,
                "cdate": 1700216398620,
                "tmdate": 1700217439862,
                "mdate": 1700217439862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PONP5vG56F",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "`[1]: Dongjin Lee and Kijung Shin. I\u2019m me, we\u2019re us, and i\u2019m us: Tri-directional contrastive learning on hypergraphs. In AAAI, 2023.`\n\n`[2]: Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen, Jingrui He, and Zhangyang Wang. Augmentations in hypergraph contrastive learning: Fabricated and generative. In NeurIPS, 2022.`\n\n`[3]: Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI, 2019`\n\n`[4]: Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function framework for hypergraph neural networks. In ICLR, 2022.`\n\n`[5]: Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. In IJCAI, 2021.`"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217811392,
                "cdate": 1700217811392,
                "tmdate": 1700218310675,
                "mdate": 1700218310675,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X7HIKaGtEk",
                "forum": "DZUzOKE6og",
                "replyto": "caIir2Re1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_3LGX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1610/Reviewer_3LGX"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reply"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for carefully addressing the review. \n\nBefore the rebuttal, my main concern regards the absence of experiments involving multiple backbones. The additional experiments included in the response demonstrate that the proposed method yields significant improvements irrespective of the hypergraph processing method. I believe that this is an important empirical contribution, which strengthen and validates the proposed approach. \n\nIn light of this, I decided to increases my score to 6: marginally above the acceptance threshold.\n\nBest,\nReviewer 3LGX"
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1610/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557769237,
                "cdate": 1700557769237,
                "tmdate": 1700557790984,
                "mdate": 1700557790984,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]