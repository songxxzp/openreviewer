[
    {
        "title": "Discovering Logic-Informed Intrinsic Rewards to Explain Human Policies"
    },
    {
        "review": {
            "id": "Gd6P7jlCvX",
            "forum": "ZdvI91pInB",
            "replyto": "ZdvI91pInB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_1taW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_1taW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an inverse reinforcement learning method to discover a logic-informed reward function. Assuming demonstrations were generated by experts following an optimal energy-based policy, it alternates between learning a neural logic tree and learning policy until convergence with a GAN-framework from a previous work. The energy function is parameterized using logic-informed features given a set of generated logic rules. Specifically, a transformer-based reader encodes the observed state-action demonstrations to a set of Boolean logic variables (predicates), then a decoder predicts the next predicate based on the previously generated partial symbolic tree. Experiments on toy games and two highly correlated real healthcare datasets show overall improvement from several benchmarks under two metrics each."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. proposed a novel approach to IRL.\n2. the way the logic rules inform the energy function and consequently the policy makes sense to me."
                },
                "weaknesses": {
                    "value": "1. lack of clarity and justification. E.g., why is the traversal pre-order, how is the set of predefined labeling functions predefined, what is grounded predicate sequence, why grounded predicate is divided into characters by the 'first' block of the abstract symbolic tree reader. If the tree is based on transformer, perhaps the authors could focus more on how the tree is built upon transformer and the difference between the two. See more major ones in my questions.\n2. since the contribution is on logic-informed IRL, would be good to show and analyze the logic rules discovered along a trajectory as opposed to a single snapshot."
                },
                "questions": {
                    "value": "1. what is the numerical form of the tree? Eq. 6 only specifies the likelihood of the tree in terms of pre-order traversal sequence, but not the structure thereof (i.e. parent/children nodes).\n2. are the cardinalities of the index sets $I_k^1$ and $I_k^0$ same across $k$?\n3. how is the node chosen for expansion and how is it expanded? By what criterion you'd know the tree cannot be further expanded?\n4. does the order of the predicates generated matter? at the same tree horizon?\n5. how is the goal $X^0$ decided, is it always success v. fail? Is it constant throughout a trajectory or changing over timesteps? If it's the former how can you make sure it is the right goal for all timesteps (one step of mishap would not necessarily reasult in an overall failure and vice versa), if the latter then how is the step-wise goal specified without too much human knowledge?\n6. in estimating the overall energy function, since the tree generator is amortized, why you can use the top-K logic trees with generated probabilities as the probabilities may change later? And can you not use unweighted trees (e.g. taking average of all trees) to approximate the expectation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Reviewer_1taW",
                        "ICLR.cc/2024/Conference/Submission2185/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588111417,
            "cdate": 1698588111417,
            "tmdate": 1700591833882,
            "mdate": 1700591833882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qeonti25oy",
                "forum": "ZdvI91pInB",
                "replyto": "Gd6P7jlCvX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1. Why is the traversal pre-order?**\n\nA1:The choice of pre-order traversal for processing the logic tree is guided by its inherent property to prioritize root nodes before leaves. This sequence aligns with how our model hierarchically processes predicates, ensuring that higher-level decisions inform the evaluation of subsequent conditions. Pre-order traversal is well-suited for generating a sequence of predicates that reflects the structured reasoning our model embodies, mirroring human decision-making patterns where fundamental judgments precede more detailed considerations. Moreover, this approach is consistent with methodologies used in previous research [1], particularly in the context of natural language processing.\n\n[1]Sun, Zeyu, et al. \"Treegen: A tree-based transformer architecture for code generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. 2020.\n\n**Q2. How is the set of predefined labeling functions predefined?**\n\nA2: The labeling functions are predefined based on a combination of domain expertise and pretrained model. Domain experts identify relevant features that could potentially serve as informative predicates. These are then validated against the data to ensure they are significant for the model's predictive performance. We implement an iterative process where the initial set of labeling functions is refined through model feedback, emphasizing the most discriminative features for the logic tree construction. This ensures that our predefined labeling functions are both theoretically sound and empirically robust, leading to the creation of a logic tree that is well-founded and effective for the task at hand.\n\n**Q3. What is a grounded predicate sequence?**\n\nA3: A grounded predicate sequence in logic trees is a sequence where each predicate has specific, often binary, values assigned to its variables. This transforms abstract logic rules into concrete, evaluable conditions. In the context of a logic tree, this sequence forms a distinct path marked by predicates grounded with actual data. It's crucial for model interpretability, as it offers a clear and logical flow of deductions leading to a specific outcome.\n\n**Q4.Why grounded predicate is divided into characters by the 'first' block of the abstract symbolic tree reader.**\n\nA4: The division of grounded predicates into characters within the 'first' block of our abstract symbolic tree reader is a strategy to encapsulate raw trajectory data, specifically state and action information, into the logic tree structure. This process facilitates the embedding of concrete trajectory instances, preserving the integrity of state/action sequences within the decision-making framework. \n\n**Q5: If the tree is based on transformer, perhaps the authors could focus more on how the tree is built upon transformer and the difference between the two.**\n\nA5: Our logic tree, while drawing inspiration from transformer architectures, is specially crafted to enhance interpretability in decision-making. Transformers adeptly map global dependencies, yet our logic tree is devised to present decisions in a hierarchical and intelligible fashion. This tailored design facilitates transparent and traceable reasoning, essential in critical domains. Additionally, our supplementary material details a transformer architecture incorporating a tree convolution layer. This layer is pivotal in grasping the contextual relationships of nodes within the logic tree, marking a key innovation from traditional transformer models.\n\n**Q6: Since the contribution is on logic-informed IRL, would be good to show and analyze the logic rules discovered along a trajectory as opposed to a single snapshot.**\n\nA6: Thank you very much for your valuable recommendation. You raise an important point regarding the potential insights that could be gained from analyzing the evolution of logic rules along a trajectory in our logic-informed IRL study. Currently, our methodology centers around a logic tree that primarily captures static conjunctions ('and' relations) among various logical components. Our predicates are not time-based, which means that our current framework might not be ideally suited to capture the dynamic evolution of these rules over time. However, your suggestion has certainly inspired us to consider the possibilities of integrating spatio-temporal logic rules in our future research."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538847554,
                "cdate": 1700538847554,
                "tmdate": 1700538847554,
                "mdate": 1700538847554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HgDyz0q5vo",
                "forum": "ZdvI91pInB",
                "replyto": "iR24OwV1OC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_1taW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_1taW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for your response. I acknowledge that most of my concerns have been addressed.\n\n\nTrivial further questions:\n\nin A10, if the order of predicates at the same level is irrelevant, how would you ensure the pre-order traversal is insensitive to it?\n\nin A11, how would you compute the cumulative progress towards the goal?\n\n\nI am raising my score. However, the method depends largely on human picked features which could be multitudinous, leading to a trade-off between inaccuracy and human involvement."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591787354,
                "cdate": 1700591787354,
                "tmdate": 1700591787354,
                "mdate": 1700591787354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AV8eRqbn8I",
                "forum": "ZdvI91pInB",
                "replyto": "Gd6P7jlCvX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your insightful inquiries and interest in our research. We are pleased to provide further clarification on your questions as follows:\n\n**Q13: If the order of predicates at the same level is irrelevant, how would you ensure the pre-order traversal is insensitive to it?**\n\nA13: To ensure that pre-order traversal is insensitive to the order of predicates at the same level in a tree structure, the use of position and depth embeddings is key. We have introduced them in the section 2 of our supplementary file. Here's how it works:\n\n**1.Position Embeddings**: These assign a unique identifier to each node based on its position in the pre-order traversal. This helps in capturing the sequence information of the nodes.\n\n**2.Depth Embeddings**: These embeddings encode the depth of each node in the tree. Nodes at the same level will have identical depth embeddings, making the representation insensitive to their order.\n\nBy combining position and depth embeddings, the representation of each node incorporates both its sequence in the traversal and its hierarchical level in the tree. This approach ensures that the tree representation remains consistent regardless of the order of nodes at the same level, focusing instead on their structural positions.\n\n**Q14: how would you compute the cumulative progress towards the goal?**\n\nA14: In our framework, the primary stages are logic tree learning and policy learning, which mirror backward and forward reasoning, respectively. The neural logic tree generator sequentially composes logic variables, starting from the goal, to establish strategic rules. In the policy learning stage, the agent is optimized to follow the most favorable path for forward chaining, as dictated by these rules.** These stages are synergistically linked by a neural symbolic energy function, denoted as Eq. (7). This function serves as a critical intermediary, assessing the extent to which rule conditions are met and guiding the GAN in refining the policy learning process**. Specifically, **each summand in Eq. (7) acts as a soft approximation of a clause in the logic tree**, facilitating a nuanced and flexible approach to rule application. Through an iterative process of alternating between logic tree generation and policy optimization, our framework progressively converges on the most effective set of logical rules and corresponding policies."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632189787,
                "cdate": 1700632189787,
                "tmdate": 1700632498538,
                "mdate": 1700632498538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tpif8PxJGr",
            "forum": "ZdvI91pInB",
            "replyto": "ZdvI91pInB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a novel logic-informed Inverse Reinforcement Learning (IRL) framework is introduced. The approach embodies inverse optimal control through policy optimization, where logic rules are learned from expert trajectories and serve as the energy function. The policy is optimized to estimate the energy model's partition function. Essentially, the policy is trained to generate state-action trajectories that minimize the energy function encoded from the currently learned logic rules, ensuring better adherence to the logic rules. The paper employs a GAN-style training scheme to update these logic rules by discerning trajectories generated by the policy from expert trajectories. The framework utilizes a neural logic tree generator to sequentially derive logic rules from goal variables, mimicking backward reasoning, and employs policy learning to determine the most effective path to achieve the end goal based on the current logic rules, akin to forward reasoning. This alternating process of backward and forward reasoning continues until convergence is attained, enabling the method to potentially learn the optimal probabilistic distribution of logic trees and the policy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Interpretability: The paper introduces a novel Inverse Reinforcement Learning (IRL) framework that learns both logical reasoning processes employed by experts and policies from observational data. This dual-learning approach improves policy interpretability, distinguishing it from traditional black-box solutions. Logic rules learned from this framework can be used to explain the observational state-action trajectories from expert demonstrations.\n\n* Reward Recovery: The paper introduces a reward learning framework that appears to be both manageable and effective. This framework facilitates the automatic exploration of intrinsic logical knowledge, as manifested in the symbolic logic trees implicitly employed by experts for guiding reward design.\n\n* The experiment results regarding policy and logic rules learning seem convincing."
                },
                "weaknesses": {
                    "value": "I have a positive view of this work. However, the reason for not assigning a positive score to the paper lies in the absence of a clear evaluation of the reward discovery aspect (as mentioned in Section 4.2, a claimed contribution of this paper). While Section 4.2 provides informative content and closely follows the prior work of deep PQR (Geng et al., 2020b), the central focus appears to be on the relationship between the discovered rewards and the learned logic rules. Unfortunately, the paper lacks concrete examples or evaluations to support this argument. Particularly, there is a lack of evaluation regarding whether it is possible to predict the decision-making of the experts using the estimated reward functions on benchmark tasks.\n\nSimilarly, there has been no evaluation conducted on the quality of the learned logic rules."
                },
                "questions": {
                    "value": "For practical application of this approach, users are required to define predicate sets beforehand to facilitate the learning of logic rule-informed energy functions. I am curious whether there has been an ablation study conducted to assess the algorithm's performance concerning the quality and suitability of the provided predicates.\n\nIs there any evaluation regarding predicting the decision-making of experts using the estimated reward functions on your benchmarks?\n\nCould you assess the precision and recall of your learned logic rules in comparison to expert decisions on the provided benchmarks?\n\nWhy do Sec 5.1 and Sec 5.2 use different baselines?\n\nWas the evaluation conducted in a fair manner for all the baselines? For instance, NLRL does not utilize expert trajectories and learns directly from an MDP. In contrast, your approach benefits from access to expert trajectories."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808076554,
            "cdate": 1698808076554,
            "tmdate": 1699636152087,
            "mdate": 1699636152087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OMMH4cJ1jG",
                "forum": "ZdvI91pInB",
                "replyto": "Tpif8PxJGr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: I have a positive view of this work. However, the reason for not assigning a positive score to the paper lies in the absence of a clear evaluation of the reward discovery aspect...**\n\nA1: Thanks for your suggestions. To evaluate the estimated reward functions, we will compare the decisions made by experts with the decisions predicted by the estimated reward functions. Although our benchmarks do not contain the decisions made by experts, we follow [1] to evaluate our method against GAIL and AIRL on the Swimmer dataset. We predict the decision-making using the estimated reward function. Numerical results are presented in the following table. Our method can also obtain comparable scores in the Swimmer dataset.\n\nTable1: Results on swimmer tasks. Negative log-likelihood (lower is better) are reported across 5 runs.\n\n|  Methods   | Negative log-likelihood  |\n| :---  | ---:  |\n| AIRL | 0.39 |\n| GAIL | 0.34 |\n| Ours | 0.26 |\n\n[1] Fu J, Luo K, Levine S. Learning robust rewards with adversarial inverse reinforcement learning[J], 2017.\n\n\n**Q2: Similarly, there has been no evaluation conducted on the quality of the learned logic rules.**\n\nA2: Thanks for your advice. In the real-world dataset, there is no ground-truth of learned logic rules, so we follow [3] to verify our model\u2019s rule discovery ability on synthetic datasets with a known set of ground-truth rules and weights. Note that it was originally utilized for the temporal point process, so we modify it by adding spatial variables (such as \u201cleft, right, front, and behind\u201d) to fit in our settings. The weight learning results on 4 synthetic datasets are shown in the following table.\n\nTable2: Rule discovery and weight learning results (GT weights/learned weights) on 4 synthetic datasets.\n\n|  Weights   | Dataset-1  | Dataset-2  | Dataset-3  | Dataset-4  |\n| :---  | ---:  | ---:  | ---:  | ---:  | \n| w0| 1.00/0.98 | 0.50/0.45 | 1.50/1.47 | 2.00/1.82 |\n| w1| 1.00/0.91 | 0.50/0.40 | 1.50/1.44 | 1.00/0.97 |\n| w2| 1.00/0.81 | 0.50/0.34 | 1.50/1.39 | 1.00/0.92 |\n\n[3] Li S, Feng M, Wang L, et al. Explaining point processes by learning interpretable temporal logic rules[C]/ICLR. 2021.\n\n**Q3:I am curious whether there has been an ablation study conducted to assess the algorithm's performance concerning the quality and suitability of the provided predicates.**\n\nA3: Thanks for your suggestions. Our generator takes the observed state-action demonstrations as input, and initially encodes them into the symbolic predicate space. So the input predicate has a great influence on the final results. Moreover, we removed different portions (denoted as p) of the input predicate, and the results are shown in the following table. we can see that when we remove some input predicates, the performance of R@10 and R@20 drops significantly.\n\nTable 3: Ablation study of removing different portions (denoted as p) of the input predicate. Note that p=10% means that we remove 10% predicates from the framework. We show diagnosis prediction results on MIMIC-III using w-F1 (%) and R@k (%).\n|  Metric | p=0%  | p=10%  | p=20%  | p=30%  | p=40% |\n| :---  | ---:  | ---:  | ---:  | ---:  | --:  | \n| R@10 | 29.01(0.11) | 25.32(0.16) | 23.21(0.18) | 19.93(0.08) | 18.24(0.09) |\n| R@20 | 38.10(0.08) |33.45(0.14)  | 32.81(0.07) | 31.64(0.11) | 30.83(0.06) |\n\n**Q4: Is there any evaluation regarding predicting the decision-making of experts using the estimated reward functions on your benchmarks?**\n\nA4: Be the same as Q1.\n\n**Q5: Could you assess the precision and recall of your learned logic rules in comparison to expert decisions on the provided benchmarks?**\n\nA5: Be the same as Q2.\n\n**Q6: Why do Sec 5.1 and Sec 5.2 use different baselines?**\n\nA6: The difference of baselines results from the different datasets. We compare our method with several baselines (e.g., NLM and MemNN) in the Blockworld dataset because their authors have evaluated the performance in the Blockworld dataset in published papers. Moreover, HiTANet and Chet are also representative baselines in the MIMIC dataset, so we compared our method with it to demonstrate our performance in healthcare event prediction and meanwhile provide strategic explanations.\n\n**Q7: Was the evaluation conducted in a fair manner for all the baselines? **\n\nA7: Our evaluation was designed to demonstrate the enhanced learning potential when expert trajectories are incorporated, as opposed to a direct comparison of learning methods. While NLRL learns solely from an MDP, our approach leverages expert trajectories for a more guided learning process. This difference in methodology is crucial for the context of our study, as it highlights the advantages of integrating expert knowledge. The evaluation is thus fair within the scope of our research objectives, which is to showcase the benefits of using expert data in learning algorithms."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574045004,
                "cdate": 1700574045004,
                "tmdate": 1700575712606,
                "mdate": 1700575712606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vbpm8nEjBb",
                "forum": "ZdvI91pInB",
                "replyto": "Tpif8PxJGr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I thank the authors' for the detailed response.\n\n**To evaluate the estimated reward functions, we will compare the decisions made by experts with the decisions predicted by the estimated reward functions.**\n\nThe presented results in the rebuttal is on the swimmer task, which is not listed in the original paper. I would appreciate it if you could consider conducting experiments with the algorithmic and healthcare benchmarks from the paper (instead of using ad hoc new benchmarks). Doing so would make it possible to integrate the results into the paper. \n\n**Evaluation conducted on the quality of the learned logic rules.**\n\nSimilarly, the experiments on the quality of the learned logic rules are not satisfactory. The results were obtained on a new set of benchmarks rather than the benchmarks specified in the paper. Is it possible to evaluate how often the logic rules can deduce the same decision actions as those conducted in the expert demonstrations?\n\nGiven the aforementioned concerns, I maintain my opinion that the paper falls below the threshold for acceptance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598266801,
                "cdate": 1700598266801,
                "tmdate": 1700598295890,
                "mdate": 1700598295890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oHzlaWaJeA",
                "forum": "ZdvI91pInB",
                "replyto": "JN0GXXj6XZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_5AXy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I'm curious if it would be feasible to predict the decision-making of experts using the estimated reward function with the benchmarks provided in the paper. The absence of an evaluation like this to assess the quality of the reward functions is currently the main limiting factor for me to support accepting this manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685625616,
                "cdate": 1700685625616,
                "tmdate": 1700685625616,
                "mdate": 1700685625616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qmH2qfFyLd",
            "forum": "ZdvI91pInB",
            "replyto": "ZdvI91pInB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_Neph"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_Neph"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new technique for Inverse Reinforcement Learning grounded in constructing Logic Trees that can help with interpreting a given expert policy dataset while also yielding the policies deployed by the expert. It does this by means of a neural rule generator that creates a tree in a top down fashion. These trees are modeled to fit the trajectory data by means of a logic-informed energy function which is further combined with a GAN-based framework to determine the parameters matching the data. This is then used to get a policy matching the logic tree following which the reward distribution can be estimated as well. Experimental results show the resulting policies are interpretable while also being more efficient in several RL-based settings. Real-world results on the MIMIC-III and MIMIC-IV datasets also demonstrate sufficient performance while maintaining interpretability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Logic based tree formulation is simple yet informative on the exact decision making process by the expert policies.\n- Strong results in the RL-based experiments (Table 1) showing promise of performance while maintaining interpretability."
                },
                "weaknesses": {
                    "value": "- Results in Table 2 on Diagnosis Prediction may be too close to edge out competing methods (Chet [1]) albeit being more interpretable by means of the Logic Tree.\n- It is not entirely clear how to determine the  predicates for any given problem. The quality of these predicates will largely determine the quality of the logic tree and output policy.\n\nReferences:\n\n[1] Context-aware Health Event Prediction via Transition Functions on Dynamic\nDisease Graphs, Lu et al., 2022"
                },
                "questions": {
                    "value": "1. How does the algorithm handle redundant sets of nodes in a given Logic tree? Is there a pruning procedure?\n2. How are the predicate variables determined? Are the functions of the observation space manually provided by the user for a given dataset? E.g. Above(x,y) in the Blockworld experiments\n3. Could there be any additional experiments showing the effect of input predicate set choice on the final result?\n4. How are the competing algorithms being shown fairly since they are not provided these informative predicates? Could they be included as part of the observation space and run again?\n\nMinor Typos:\n\nIntroduction (paragraph 4) : \u201cOur ILR involves\u201d\n\nFig. 3 \u201cSucess\u201d\n\nFig. 4 \u201cTemperture\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Reviewer_Neph"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811308349,
            "cdate": 1698811308349,
            "tmdate": 1699636152008,
            "mdate": 1699636152008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YxdSeyK7G6",
                "forum": "ZdvI91pInB",
                "replyto": "qmH2qfFyLd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1: Results in Table 2 on Diagnosis Prediction may be too close to edge out competing methods (Chet [1]) albeit being more interpretable by means of the Logic Tree.**\n\nA1:Thank you for your insightful comment regarding the results presented in Table 2. While the performance enhancements over Chet [1] may seem modest, the significant increase in interpretability achieved through our Logic Tree method represents a crucial advancement. In domains where the stakes of decisions are high, the ability to comprehend and trust the decision-making process is invaluable. Our approach underscores the importance of explainable AI, balancing a slight uptick in performance with a marked improvement in transparency. This balance is further supported by [2], which articulates the often necessary trade-off between precision and interpretability in model design.\n\n[1] Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs, Lu et al., 2022\n\n[2] Rudin C. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. Nat Mach Intell. 2019.\n\n**Q2: It is not entirely clear how to determine the predicates for any given problem. The quality of these predicates will largely determine the quality of the logic tree and output policy.**\n\nA2: Thank you for your insightful comment regarding predicate determination. In our approach, we rely on domain expertise, which is a widely adopted practice within this field, to guide the selection of predicates. While there is existing research on automated predicate invention, such as Meta-Interpretive Learning (MIL) [3], these methods often face limitations in complex domains like healthcare, where the intricacies of the data require expert knowledge. Predicates crafted with the input of medical professionals ensure reliability and relevance, leading to more meaningful and actionable logic rules. \n\n[3] Muggleton S H. Meta-interpretive learning of higher-order dyadic datalog: Predicate invention revisited[J]. Machine Learning, 2015.\n\n**Q3: How does the algorithm handle redundant sets of nodes in a given Logic tree? Is there a pruning procedure?**\n\nA3: Our algorithm proactively minimizes redundancy in logic trees through the use of expertly crafted predicates, ensuring tree compactness from the outset. Additionally, our tree generator operates within an amortized framework, which facilitates the concurrent optimization of the tree structure and the parameters of an energy-based Generative Adversarial Network (GAN) during policy function estimation. This integrated approach effectively prunes unnecessary nodes, maintaining the efficiency and relevance of the logic tree representation. \n\n**Q4: How are the predicate variables determined?**\n\nA4: Be the same as Q2.\n\n**Q5: Could there be any additional experiments showing the effect of input predicate set choice on the final result?**\n\nA5: Thanks for your suggestions. Our generator takes the observed state-action demonstrations as input, and initially encodes them into the symbolic predicate space. So the input predicate has a great influence on the final results. Moreover, we removed different portions (denoted as p) of the input predicate, and the results are shown in the following table. we can see that when we remove some input predicates, the performance of R@10 and R@20 drops significantly.\n\nTable 1: Ablation study of removing different portions (denoted as p) of the input predicate. Note that p=10% means that we remove 10% predicates from the framework. We show diagnosis prediction results on MIMIC-III using w-F1 (%) and R@k (%).\n|  Metric   | p=0%  | p=10% | p=20% | p=30% | p=40%|\n| :---  | ---:  | :--: |  :--: |  :--: |  :--: | \n| R@10  | 29.01(0.11) |25.32(0.16)|23.21(0.18)|19.93(0.08)|18.24(0.09)|\n| R@20  | 38.10(0.08)|33.45(0.14)|32.81(0.07)|31.64(0.11)|30.83(0.06)|\n\n**Q6: How are the competing algorithms being shown fairly since they are not provided with these informative predicates? Could they be included as part of the observation space and run again?**\n\nA6: Thank you for your recommendation. It is important to note that the effectiveness of informative predicates is not solely reliant on their addition to the model; the relationships between these predicates (relations) are more crucial. Our plan follows the logic tree structure, and simply augmenting the state space with additional predicates without considering their dynamic interrelations would not be beneficial. We emphasize estimating dynamics that underpin these relations, which is a critical aspect of our framework. This approach ensures that our model not only incorporates informative predicates but also captures the essential relational dynamics for accurate predictions.\n\n**Q7:Minor Typos.**\n\nA7: These typos have been modified in a revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538474824,
                "cdate": 1700538474824,
                "tmdate": 1700538474824,
                "mdate": 1700538474824,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "65s4zNhaAO",
            "forum": "ZdvI91pInB",
            "replyto": "ZdvI91pInB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_wzfu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2185/Reviewer_wzfu"
            ],
            "content": {
                "summary": {
                    "value": "Authors introduce a method that learns a logic tree and a policy using inverse reinforcement learning from observational data from experts, which is ultimately presented as a set of logical rules. Further, this logic tree allows the recovery of the reward function, using q-function estimation, which is then used for the reward estimation. State-action demonstrations are used as the raw input of the method. The logic tree is then generated in a top-down manner, through a transformer based model, which is then fed to a GAN model to estimate the agent policy. Evaluation was done both on synthetic (3 datasets) and real-world (1) datasets. Authors evaluate the method in a healthcare context and use MIMIC-3 and MIMIC-4 datasets. The F1 score and recall is measured for the predictions of the diagnosis, across 6 baseline models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-Extracting logic rules in high-stakes domains is valuable and can facilitate better decision making.\n-A good selection of baseline methods are compared, showing the strength of the proposed model."
                },
                "weaknesses": {
                    "value": "There are several weaknesses of the method, detailed below.\n\n-The major weakness I see in the paper is in the framing of the problem as an interpretability problem, where authors acknowledge the lack of interpretability in black box policies and how in high-stakes domains. The logic rules are presented as the high level explanations that can provide interpretability. While this is a reasonable assumption, this is not detailed further in the paper, or discussed in the evaluation section. E.g. A large logic tree might not be inherently interpretable, where the selection of the rules to be explained can be important.\n- While there is a good selection of baseline methods compared, the synthetic/toy datasets need further additions of planning domains/datasets.\n\n---- after the rebuttal ---- \n\nI have adjusted the scores accordingly after considering the rebuttal, and further considering the rebuttals and concerns of other reviewers."
                },
                "questions": {
                    "value": "-What are the computation times for the MIMIC and synthetic datasets?  It will be helpful for the reader to understand the computation requirements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2185/Reviewer_wzfu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2185/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847325260,
            "cdate": 1698847325260,
            "tmdate": 1700694932396,
            "mdate": 1700694932396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zUj5ahFhtF",
                "forum": "ZdvI91pInB",
                "replyto": "65s4zNhaAO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**Q1:The major weakness I see in the paper is in the framing of the problem as an interpretability problem, where authors acknowledge the lack of interpretability in black box policies and how in high-stakes domains.**\n\nA1: Thank you for your valuable feedback. In our paper, we constrain the size of the logic tree to enhance interpretability. Each branch within the tree uses a conjunction (\"and\") to maintain simplicity and directness in the reasoning process. Disjunctions (\"or\") are used between separate trees, indicating alternative reasoning paths to the root node. This structured approach ensures that only when all predicates in a path are true, a conclusion at the root is reached. Each distinct tree represents a unique reasoning pathway, simplifying the interpretability of complex decisions. \n\n**Q2: While there is a good selection of baseline methods compared, the synthetic/toy datasets need further additions of planning domains/datasets.**\n\nA2: Thanks for your suggestions. To demonstrate the performance of our framework, we evaluate our methods in benchmark tasks (the shifting maze [1]) for planning datasets.  This task involves a 2D point mass navigating to a goal position in a small maze when the position of the walls is changed between train and test time. At test time, the agent cannot simply mimic the actions learned during training, and instead must successfully infer that the goal in the maze is to reach the target. In this task, a reward is learned via IRL in the training environment, and the reward is used to reoptimize a new policy on a test environment. We compare our method with several competing IRL algorithms, including NLM, MemNN, MaxEnt-IRL, Deep PQR and AIRL. The results are shown in the following table. Our method still obtains the best performance.\n\nTable1: Results on planning dataset (the shifting maze).\n|  Methods   | NLM  | MemNN | MaxEnt-IRL | Deep PQR | AIRL | Ours |\n| :---  | ---:  | :--: | :--: | :--: | :--: | :--: |\n|  Reward  | 393.5 |372.8|353.8| 440.6| 384.4| 468.3|\n\n\n\n[1] Fu J, Luo K, Levine S. Learning robust rewards with adversarial inverse reinforcement learning[J]. arXiv preprint arXiv:1710.11248, 2017.\n\n**Q3: What are the computation times for the MIMIC and synthetic datasets? It will be helpful for the reader to understand the computation requirements.**\n\nA3: Following the reviewer\u2019s suggestion, we show the runtime for all methods averaged on BlocksWorld tasks. Note that all methods are tested on an RTX 2080Ti GPU. The following table shows the time of 100 evaluation episodes. Our method is simpler to train at the same time achieving even better performance. \n\n\nTable2: Comparison of runtime.\n|  Methods   | NLM  | MemNN | MaxEnt-IRL | Deep PQR | AIRL | Ours |\n| :---  | ---:  | :--: | :--: | :--: | :--: | :--: |\n|  Runtime| 66.9s |101.3s|32.6s| 161.0s | 70.8s | 57.2s|"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537957671,
                "cdate": 1700537957671,
                "tmdate": 1700537957671,
                "mdate": 1700537957671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZPImSXHE9C",
                "forum": "ZdvI91pInB",
                "replyto": "65s4zNhaAO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Invitation for further discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nI hope this message finds you well. Following your valuable feedback, we have diligently made revisions based on your previous feedback and are now eager to understand if there are any aspects that may still require refinement or additional clarification.\n\nYour insights are very important to us, and we eagerly await your response.\n\nThank you for your time and effort.\n\nBest regards,\n\nThe authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632874534,
                "cdate": 1700632874534,
                "tmdate": 1700632874534,
                "mdate": 1700632874534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EAl14T5zf7",
                "forum": "ZdvI91pInB",
                "replyto": "zUj5ahFhtF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_wzfu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2185/Reviewer_wzfu"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "I thank the authors for providing additional results shifting domain and computation times. I have adjusted\u00a0the scores accordingly after considering the rebuttal, and further considering\u00a0the\u00a0rebuttals\u00a0and concerns of other\u00a0reviewers.\u00a0\n\nFor the updated manuscript, please add the\u00a0commentary around interpretability."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2185/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694854639,
                "cdate": 1700694854639,
                "tmdate": 1700694854639,
                "mdate": 1700694854639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]