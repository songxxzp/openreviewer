[
    {
        "title": "Understanding Community Bias Amplification in Graph Representation Learning"
    },
    {
        "review": {
            "id": "1BCOk2Q7hG",
            "forum": "T8RiH35Hy6",
            "replyto": "T8RiH35Hy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_vF6X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_vF6X"
            ],
            "content": {
                "summary": {
                    "value": "For the graph learning, this paper discovers the phenomenon which this paper calls community bias amplification. This paper provides theoretical analysis on this phenomenon. Using this theoretical insights, this paper proposes a graph learning algorithm to mitigate this bias. The experimental results show that the proposed method outperforms the existing ones."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A problem this paper focuses on is interesting. Seeing Fig.1, it is convincing that a bias between classes surely exists, and this itself is novel as far as I believe, at least in the GNN realm."
                },
                "weaknesses": {
                    "value": "1. I do not agree with the authors claim on where the bias comes from. \nThe datasets for illustrative examples in Fig.1 contain the independent components. \nOono and Suzuki (2020) and its earlier work [1] shed a light on the phenomenon over-smoothing -- if we stack layers, the stacked adjacency matrix is dominant by the eigenspace associated with the largest eigenvalues. In the Cora and Citeseer cases, that space is a set of indicate vectors of independent components. \nAlso, the claim is this worsens the performance, which is understandable, since the only this eigenspace of the independent components are too simplified as an underlying structure. \nHowever, the underlying graph structure does not necessarily correspond to the classes, while of course graph structure and the classes are loosely related. \nIf they are, we observe somewhat comparative performance only using the graph of Cora and Citeseer, but we do not observe such performance by conducting for example the simple spectral clustering on graph. \nThus, the community amplification bias of the underlying graphs is not the primal reason why we observe the unfairness of Fig. 1. \nInstead, I believe that the bias is more nuanced -- hope to see what is the dominant.\n\n\n2. Also, even if the community bias were primal reason, the argument of Eq. (3) is weak since they only compare the value of the eigenvalues of two clusters. Also, how the example of Appendix A reflects the Cora and Citeceer dataset? Do we observe such things in the real datasets? How do you argue that?\n\n---\n\n[1] Li et al. Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. Proc AAAI 2018."
                },
                "questions": {
                    "value": "How do you defend that the community amplification bias of the underlying graph is the primal reason we observe an unfairness between classes? As stated in the weakness section, I feel like there exists some gap between them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Reviewer_vF6X"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698079929183,
            "cdate": 1698079929183,
            "tmdate": 1699636556906,
            "mdate": 1699636556906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vy8ehAnpGM",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vF6X[1/3]"
                    },
                    "comment": {
                        "value": "Thank you for your review. It seems you may have misunderstood some part of our motivation and main contribution. Therefore, we would like to clarify more in the rebuttal. \n\n\n\nOne of our main contribution is identifying a novel phenomenon in GCL named community bias amplification, which is the observation that class imbalance increases after applying GNN encoders (compared to MLP). Understanding this amplification effect is the problem we investigated. However, it seems that the reviewer has misunderstood the goal of our work is to interpret why class imbalance exists in the downstream task.   **Note that we focus on the \"amplification\" effect of GNN**: whether there is bias in the original features or not, we always observe that GNN makes the accuracy imbalance between classes worse. Moreover, in the review, the reviewer keep referring to \"community bias amplification\" studied in this work as \"community amplification bias\", which suggests there may be some misunderstandings. \n\n\n\n**Research background and problem.** We use the term \"community bias amplification\" to describe exacerbation of class performance gap caused by the graph structure information.  Specifically, for instance, the maximum performance gap when classifying different classes of nodes using MLP is $\\Delta_0$, and the maximum performance gap when classifying the embeddings learned by the GNN encoder is $\\Delta_1$. We then studied the reasons why $\\Delta_1 > \\Delta_0$ and how to reduce $\\Delta_1$ - $\\Delta_0$. We did not focus on the reasons for $\\Delta_0$.\n\n\n\nNext, we will address each of the issues raised by the reviewer one by one.\n\n\n\n**W1.1: I do not agree with the authors claim on where the bias comes from. The datasets for illustrative examples in Fig.1 contain the independent components.**\n\nIn our analysis, we always assume the graph is connected. Different communities in our analysis are not disconnected, but are clusters in a connected graph. Although real-world graph datasets may consist of multiple components, there is always a main component (the largest connected subgraph) and this largest connected subgraph contains the vast majority of nodes. Our analysis interprets the reason why community bias amplification occurs on this single component. This conclusion also holds for graphs with multiple connected components, as each component can be analyzed in the same way independently. \n\n**W1.2: Oono and Suzuki (2020) and its earlier work [1] shed a light on the phenomenon over-smoothing -- if we stack layers, the stacked adjacency matrix is dominant by the eigenspace associated with the largest eigenvalues. In the Cora and Citeseer cases, that space is a set of indicate vectors of independent components. Also, the claim is this worsens the performance, which is understandable, since the only this eigenspace of the independent components are too simplified as an underlying structure.**\n\nAs clarified above, our analysis is for a single connected graph. So the comments here actually misinterprets our theoretical analysis. Moreover, we have emphasized in the submission that existing spectral analysis on over-smoothing, which assumes the number of layers goes to infinity, is not suitable in our setting. Here, we would like to clarify more on this issue.\n\n**Distinct from oversmoothing.** We want to emphasize that what we are studying are the phenomena that occur **when oversmoothing does not happen**. Oversmoothing is a phenomenon that is only observed when there are enough stacked adjacency matrices present. In our theoretical analysis, we are considering more realistic shallow GCN encoders. This is also in line with reality, as in common GCL models, GCN encoders usually only have few layers of graph convolution, at which point oversmoothing does not occur. At a high level, our justification for bias amplification is that when oversmoothing has not occurred, the level of smoothness at different regions of the graph could differ significantly due to disparities in local structures. Overall, the reviewer's hypothesis that community bias amplification could be caused by oversmoothing is not valid."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466686646,
                "cdate": 1700466686646,
                "tmdate": 1700466686646,
                "mdate": 1700466686646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TfnBwXzX5A",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vF6X[2/3]"
                    },
                    "comment": {
                        "value": "To further explain this distinction, we tested the specific performance of each class for MLP and DGI (with graph convolution layers of 1, 2, 4, 16). We can see that for MLP, the maximum performance gap between classes is only 0.20. For DGI, When L=1 and 2, the performance of each class of nodes has improved, but the maximum performance gap between classes has risen to 0.24 and 0.25. Clearly, the performance gap between classes at this time does not come from oversmoothing, because oversmoothing has not occurred. When L=4 and L=16, oversmoothing gradually occurs (the overall performance decreases), but the maximum performance difference does not increase significantly. **Therefore, the main problem we are studying has no relation to oversmoothing.**\n\nTable 1:  Performance of each class for MLP and DGI.\n\n|           | C0   | C1   | C2   | C3   | C4   | C5   | C6   | $\\Delta$ |                    |\n| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | -------- | ------------------ |\n| MLP       | 0.49 | 0.68 | 0.69 | 0.51 | 0.61 | 0.58 | 0.59 | 0.20     | no oversmoothing   |\n| DGI(L=1)  | 0.69 | 0.87 | 0.93 | 0.73 | 0.81 | 0.75 | 0.82 | 0.24     | no oversmoothing   |\n| DGI(L=2)  | 0.71 | 0.89 | 0.96 | 0.75 | 0.81 | 0.76 | 0.82 | 0.25     | no oversmoothing   |\n| DGI(L=4)  | 0.69 | 0.86 | 0.94 | 0.74 | 0.80 | 0.74 | 0.83 | 0.25     | mild oversmoothing |\n| DGI(L=16) | 0.66 | 0.78 | 0.92 | 0.68 | 0.72 | 0.65 | 0.80 | 0.27     | oversmoothing      |\n\n\n\n\n**W1.3: If they are, we observe somewhat comparative performance only using the graph of Cora and Citeseer, but we do not observe such performance by conducting for example the simple spectral clustering on graph. Thus, the community amplification bias of the underlying graphs is not the primal reason why we observe the unfairness of Fig. 1. Instead, I believe that the bias is more nuanced -- hope to see what is the dominant.**\n\n\nThese GCL models are usually applied to homophily graphs, in which intra-class edges significantly outnumber inter-class edges, hence the graph structure and classes are strongly correlated. \n\nWhat we want to express through Figure 1 is that the performance gap $\\Delta_1$ of the GCL model is significantly higher than the performance gap $\\Delta_0$ of MLP. We are studying the reasons why $\\Delta_1 > \\Delta_0$, and how to reduce this community bias amplification, but not the main reason for $\\Delta_0$.\n\n\n\n**W2:Also, even if the community bias were primal reason, the argument of Eq. (3) is weak since they only compare the value of the eigenvalues of two clusters. Also, how the example of Appendix A reflects the Cora and Citeceer dataset? Do we observe such things in the real datasets? How do you argue that?**\n\nAfter recognizing the community bias **amplification** in GCLs, we summarize the evidences for this phenomenon as three folds:\n\n(1) Eq 3 shows that a upper bound of the representation density shrinks with a rate that depends on the **second** largest eigenvalue. We would like to point out that the rationale on two clusters can also be extended to multiple clusters with one-vs-the-rest (OvR) multiclass strategy. Secondly, The comparison of eigenvalues is not ``weak\" due to the structural properties they imply [1], and it is widely used in the analysis of GNNs [2-5].\n\n(2) **For random graphs:** In appendix A, we show the proof for bias amplification on the CSBM. Random graphs like CSBM is widely adopted for GNN analysis (e.g., [6-8]), and it is very suitable for illustrating bias amplification. \n\n(3) **Emprically:** We would like to highlight Table 2 in our original submission. In Table 2, we show the standard deviation of community density, computed on the node representations produced by various GCL models. It is clear that, for the embeddings of baseline models, **densities differ a lot among communities.** This is a direct evidence of community bias on two real datasets, Cora and Citeceer. Furthermore, it shows that our method RGCCL effectively mitigates this issue."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466777030,
                "cdate": 1700466777030,
                "tmdate": 1700466777030,
                "mdate": 1700466777030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jz9cjdy8D4",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vF6X[3/3]"
                    },
                    "comment": {
                        "value": "**Q1: How do you defend that the community amplification bias of the underlying graph is the primal reason we observe an unfairness between classes? As stated in the weakness section, I feel like there exists some gap between them.**\n\nIn summary, we discover a phenomenon called community bias amplification: After applying a **shallow** GNN, the representation densities of communities differs a lot due to their distinct structure (which is validated both theoretically and empirically, c.f., our response to W2). Different representation densities are proved to be unfair in Proposition 1 and Figure 3. The reviewer's concern on oversmoothing and eigenvalues are carefully addressed in our response to W1.2 and W2, respectively: (1) oversmoothing will not occur in **shallow** GNN; (2) the **second** largest eigenvalue contains rich information of structural properties, thus comparison on it is not ''weak'' and is widely adopted in the literature.\n\n\n\n----------------------------------\n\nThank you again for your review! We have attempted to address your questions in our reply.  If you have any further doubts, please contact us. we will gladly address any additional questions/concerns you may have.\n\n\n[1] Cvetkovi\u0107, D., \\& Simi\u0107, S. (1995). The second largest eigenvalue of a graph (a survey). Filomat, 449-472.\n\n[2] Chen, M., Wei, Z., Huang, Z., Ding, B. and Li, Y. Simple and deep graph convolutional networks. ICML 2020.\n\n[3] Keriven, N. Not too little, not too much: a theoretical analysis of graph (over) smoothing. Neurips 2022.\n\n[4] Rong, Y., Huang, W., Xu, T. and Huang, J. Dropedge: Towards deep graph convolutional networks on node classification. ICLR 2020.\n\n[5] He, M., Wei, Z. and Xu, H. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Neurips 2021.\n\n[6] Wei, R., Yin, H., Jia, J., Benson, A. R., \\& Li, P. Understanding non-linearity in graph neural networks from the bayesian-inference perspective.  Neurips 2022.\n\n[7] Wu, X., Chen, Z., Wang, W. W., \\& Jadbabaie, A. A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks. ICLR 2023.\n\n[8] Su, J., Zou, D., Zhang, Z., \\& Wu, C. Towards Robust Graph Incremental Learning on Evolving Graphs. ICML 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466804390,
                "cdate": 1700466804390,
                "tmdate": 1700473773003,
                "mdate": 1700473773003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GN9l5CjPnv",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer vF6X,\n\nWe thank you again for your review. We have worked hard and have thoroughly addressed your comments in the rebuttal.\n\nAs the discussion period soon comes to an end, we are looking forward to your feedback to our response and revised manuscript. Many thanks again for your time and efforts.\n\nBest regards,\n\nAuthors of Submission 5464"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538661645,
                "cdate": 1700538661645,
                "tmdate": 1700538661645,
                "mdate": 1700538661645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d1puQMZiGx",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer vF6X,\n\nThe rebuttal phase ends today and we have not yet received feedback from you. We believe that we have addressed all of your previous concerns. We would really appreciate that if you could check our response and updated paper.\n\nLooking forward to hearing back from you.\n\nBest Regards,\n\nAuthors"
                    },
                    "title": {
                        "value": "A friendly reminder for discussion"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674207989,
                "cdate": 1700674207989,
                "tmdate": 1700674549576,
                "mdate": 1700674549576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5AEEbj8MZy",
                "forum": "T8RiH35Hy6",
                "replyto": "d1puQMZiGx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Reviewer_vF6X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Reviewer_vF6X"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the review. \n\nI apologize for the \"community amplification bias.\" However, other than that, I believe that I understood in a way like the authors wrote, especially on the observation on accuracy difference is big for some pair of the classes. Not on the accuracy itself.\n\n\nThe major problem of this paper is inconsistent analyses around \"connected\" assumptions. The connection of underlying graph has a major impact on the eigenspace. However, while authors did analysis on a connected graph as stated in a rebuttal, the illustration Fig. 1 is demonstrated on Cora, whose graph is not connected. Also, none of the datasets used for experiments is connected. We may observe that the bias \"community amplification bias\" authors claim in Sec. 3, but in this paper we do not see this empirically since  Fig.1 is demonstrated on disconnected graph and again none of the datasets in the experiments is connected. For this inconsistency only, I vote for rejection.\n\nAlso, as far as I understand, in the response the authors does not rebut on \"However, the underlying graph structure does not necessarily correspond to the classes, while of course graph structure and the classes are loosely related.\" \nAgain, the classes for the dataset are not strictly corresponds to the community structure. Therefore, we do not know if $|\\hat{B}_{1}|$ and $|\\hat{B}_{2}|$ are close to 0. For some case we may consider for the citation network is that writing is similar but citation topology is different; this is what we observe in the research community.\nMy suggestion is that you would do this analysis not the typical GNN datasets which contains features and graph. Instead, authors may want to use community detection datasets which only contains graph."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730144470,
                "cdate": 1700730144470,
                "tmdate": 1700730144470,
                "mdate": 1700730144470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oWQmK3HFHb",
                "forum": "T8RiH35Hy6",
                "replyto": "1BCOk2Q7hG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer vF6X[1/2]"
                    },
                    "comment": {
                        "value": "\"especially on the observation on accuracy difference is big for some pair of the classes. Not on the accuracy itself.\"\n\n**Sorry, we have to emphasize again that we are not focusing on the phenomenon of \"accuracy difference is big\". We investigate the reason why the difference becomes even bigger after applying graph convolutions.** The feed-forward pass of a GCL encoder is typically a combination of graph convolution and MLP. We compared the \"accuracy difference\" of MLP and that of GCL (i.e., graph convolution+MLP). The accuracy difference becomes bigger for GCL, and we hypothesize that the graph convolution operation \"amplifies the bias\". Please see our detailed explanation on the problem definition in the previous rebuttal. \n\n**Q1:\"The major problem of this paper is inconsistent analyses around \"connected\" assumptions. ... For this inconsistency only, I vote for rejection.\"**\n\nWe respectfully disagree with this. There is no inconsistency here, as we have already explained why analyzing a connected graph is without loss of generality in the previous rebuttal. Here we would like to clarify more.  \n\nSuppose the graph contains multiple components. In the computation of a GNN encoder, computations on different components are uncorrelated. For example, the propagation matrix corresponds to a graph with two connected components is a block diagonal matrix: $\n\\begin{bmatrix}\n  \\hat{A_1} & \\\\\\\\\n  & \\hat{A_2}   \n\\end{bmatrix}\n$. \nApply the operator $k$ times results in \n$\\begin{bmatrix}\n  \\hat{A_1}^k & \\\\\\\\\n  & \\hat{A_2}^k   \n\\end{bmatrix}\n$.\n\nSo $\\hat{A_1}$ and $\\hat{A_2}$ can be analyzed independently. Our analysis works for both $\\hat{A_1}$ and $\\hat{A_2}$, and thus our conclusion holds on each of the components, i.e., community bias amplification can happen on each component. To sum up, although we only present an analysis on connected graphs, the conclusion extends easily to disconnected graphs, so there is inconsistency between of analysis and the empirical results presented in Fig 1. To empirically support that our conclusion holds on each component, we single out the largest connected components on Cora and Citeseer; the accuracy on these two connected graphs are shown in Table 1 and Table 2, which is consistent with our analysis. \n\n\n\nTable 1. The performance on Cora.\n|           | C0   | C1   | C2   | C3   | C4   | C5   | C6   | $\\Delta$ |  \n| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | -------- | \n| MLP       | 0.47 | 0.66 | 0.65 | 0.46 | 0.54 | 0.57| 0.45 | 0.21|  \n| GGD  |0.71 | 0.90 |  0.96|0.80 |0.85|0.80 | 0.86 | 0.25  |\n\nTable 2. The performance on Citeseer.\n|           | C0   | C1   | C2   | C3   | C4   | C5   |  $\\Delta$ |  \n| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| MLP       | 0.21 | 0.23 | 0.41 | 0.32 | 0.56 | 0.34| 0.35| \n| GGD  | 0.28| 0.59 | 0.83 | 0.71|0.85| 0.75| 0.55 |\n\nAmong the datasets used for the experiments, the largest dataset Ogbn-arxiv is connected (thus the statement of the reviewer \"none of the datasets used for experiments is connected\" is wrong). And in the remaining datasets, the largest connected component also dominates. \n\n**Q2: \"Also, as far as I understand, in the response the authors does not rebut on \"However, the underlying graph structure does not necessarily correspond to the classes, while of course graph structure and the classes are loosely related.\" Again, the classes for the dataset are not strictly corresponds to the community structure. Therefore, we do not know if $|\\hat{B_1}|$ and $|\\hat{B}_{2}|$ are close to 0. \"**\n\nAs we stated in the response to W1.3 (see Response to Reviewer vF6X[2/3]), the homophily of the graph is the most common property in graph datasets. These GCL models are usually applied to homophily graphs, in which intra-class edges significantly outnumber inter-class edges, hence the graph structure and classes are strongly correlated. And all the datasets we used also satisfy this condition. We acknowledge that the homophily assumption is not universal, and investigating bias amplification on heterophily graphs is an interesting future direction. \n\nFor the spectral analysis part, our conclusion is that when the connectivity of different parts of the graph differs drastically, the distributions of embeddings computed by GNN encoders can exhibit very different level of concentration. Note, this result only concerns embedding distributions and has nothing to do with classes of nodes."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740816964,
                "cdate": 1700740816964,
                "tmdate": 1700742771772,
                "mdate": 1700742771772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0zhBpvIjOK",
            "forum": "T8RiH35Hy6",
            "replyto": "T8RiH35Hy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_bi59"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_bi59"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors explore a phenomenon called \"community bias amplification\" in graph representation learning. This phenomenon refers to the exacerbation of performance bias between different classes by graph representation learning methods. The researchers conduct a thorough theoretical investigation of this phenomenon, approaching it from a novel spectral perspective. Their analysis reveals that the structural bias between communities leads to varying local convergence speeds for node embeddings, resulting in biased classification outcomes in downstream tasks. To address this issue, the authors propose a solution called random graph coarsening. This technique is demonstrated to be effective in mitigating the problem of bias amplification. Furthermore, the authors introduce a novel graph contrastive learning model named Random Graph Coarsening Contrastive Learning (RGCCL). This model utilizes random graph coarsening as a form of data augmentation and alleviates community bias by contrasting the coarsened graph with the original graph. Extensive experiments conducted on various datasets highlight the effectiveness of their proposed method in addressing community bias amplification in graph representation learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem of community bias amplification in GRL is very interesting and has not been extensively studied before.\n\n- The analysis is theoretically sound with appropriate discussion and remarks.\n\n- Experiments on several benchmark graphs show the effectiveness (better node representations) and efficiency (less memory usage) of the proposed method."
                },
                "weaknesses": {
                    "value": "- One of the research questions has not been answered in a good way, i.e., why community bias amplification exists in existing GCL method? Although some theoretical analyses have been provided in this paper, they are based on general (and simplified) graphs and it is not clear why there is such bias in GCL methods.\n\n- How to quantitatively measure the (community) bias amplification is not clear in the experiments. More analysis should be conducted to better illustrate why the proposed method is able to mitigate the issue of bias amplification. For example, some measures [1] can be used for the quantitative analysis.\n\n[1] Angelina Wang and Olga Russakovsky.Directional Bias Amplification.ICML 2021"
                },
                "questions": {
                    "value": "- What are the answers to the first research question, i.e., why community bias amplification exists in existing GCL method?\n\n- It is possible to give some quantitative analysis on how the proposed method can mitigate the issue of community bias amplification rather than simply comparing the performance between different classes/communities?\n\n- Discussion on extending this key idea to general GRL. I asked this question because the theoretical analysis is general enough on any GRL methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Reviewer_bi59"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791590533,
            "cdate": 1698791590533,
            "tmdate": 1700699828755,
            "mdate": 1700699828755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "boC0LSORKP",
                "forum": "T8RiH35Hy6",
                "replyto": "0zhBpvIjOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bi59"
                    },
                    "comment": {
                        "value": "**W1\\&Q1: One of the research questions has not been answered in a good way, i.e., why community bias amplification exists in existing GCL method? Although some theoretical analyses have been provided in this paper, they are based on general (and simplified) graphs and it is not clear why there is such bias in GCL methods.**\n\nThanks for the comments. Yes, our analysis is for embeddings produced by general GCN encoders, which provides insights on why community bias amplification exists in typical GCN encoders. Since most mainstream and competitive GCL models all use GCN and its variants as the encoder; thus current GCL models suffers community bias amplification. \n\nLikewise, supervised GNN as long as GCN is used as the encoder, community bias amplification will occur. The reason why we only focus on GCL is that supervised GNNs is also provided with label information at training time, and thus various techniques designed for handling class imbalance in supervised GNNs can be effectively applied, e.g., TAM [1] and GraphENS [2]. On the contrary, since there is no label information during the training process for unsupervised GCL, it is more difficult to mitigate the imbalance of embeddings from GCN encoders. And our paper is the first work that explicitly identifies this problem in GCL and provides an effective solution.\n\n\n\n**W2\\&Q2: It is possible to give some quantitative analysis on how the proposed method can mitigate the issue of community bias amplification rather than simply comparing the performance between different classes/communities?**\n\nThanks for the question, and we agree that simply comparing the performance is not enough. We would like to highlight **Table 2 in our original submission.** In this table, we computed representation densities for each communities, and reported the average and standard deviation of these densities among communities. A lower standard deviation means that different communities are with similar degree of concentration, which is more fair for classification as illustrated in Figure 3 and Proposition 1 in our original submission. The results in Table 2 quantitatively shows that the proposed method RGCCL can mitigate the issue of community bias amplification (which is also proved by Lemma 4.1 in our original submission).\n\n\n\nSince the metrics in [3] are primarily about fairness and causal inference, they are not applicable to our task. Furthermore, suggested by reviewer K8c3, we conducted experiments using the  Matthew's coefficient to measure bias. We present the results of representative GCL models and our RGCCL regarding the Matthew's coefficient in Table 1. The results also demonstrate the effectiveness of RGCCL in mitigating the amplification of community bias.\n\nTable 1: Matthew\u2019s coefficient for RGCCL and baselines.\n\n|         | Cora         | CiteSeer     | PubMed       |\n| ------- | ------------ | ------------ | ------------ |\n| DGI     | 73.0$\\pm$1.5 | 64.5$\\pm$1.2 | 57.8$\\pm$4.3 |\n| GRACE   | 67.9$\\pm$1.6 | 60.0$\\pm$2.0 | 62.3$\\pm$3.8 |\n| CCA-SSG | 74.5$\\pm$1.6 | 64.6$\\pm$1.4 | 64.0$\\pm$4.1 |\n| GGD     | 77.0$\\pm$1.6 | 64.9$\\pm$1.2 | 63.7$\\pm$4.1 |\n| GRADE   | 75.7$\\pm$1.5 | 62.1$\\pm$1.3 | 58.1$\\pm$3.4 |\n| RGCCL   | 78.9$\\pm$0.9 | 66.3$\\pm$0.8 | 65.6$\\pm$4.2 |\n\n**Q3:Discussion on extending this key idea to general GRL. I asked this question because the theoretical analysis is general enough on any GRL methods.**\n\n\n\nAs we answered in Q1, for general GRLs, such as supervised GCN, GAT models, etc., since we can use label information during the training process, we can refer to methods like TAM [1], GraphENS [2], etc., to mitigate the community bias amplification. As for classic unsupervised general GRLs like Node2vec [4] and SDNE [5], since they do not use GCN encoders, community bias amplification does not exist.\n\n\n\n[1] Song, J., Park, J. and Yang, E. TAM: topology-aware margin loss for class-imbalanced node classification. ICML 2022.\n\n[2] Park, J., Song, J. and Yang, E. Graphens: Neighbor-aware ego network synthesis for class-imbalanced node classification. ICLR 2022.\n\n[3] Angelina Wang and Olga Russakovsky.Directional Bias Amplification.ICML 2021.\n\n[4] Grover, A. and Leskovec, J. node2vec: Scalable feature learning for networks. KDD 2016.\n\n[5] Wang, D., Cui, P. and Zhu, W. Structural deep network embedding. KDD 2016."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465029124,
                "cdate": 1700465029124,
                "tmdate": 1700465029124,
                "mdate": 1700465029124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QJMMQx3DUz",
                "forum": "T8RiH35Hy6",
                "replyto": "0zhBpvIjOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder for discussion and we appreciate your input"
                    },
                    "comment": {
                        "value": "Dear Reviewer bi59,\n\nWe thank you again for your insightful and constructive review. We have worked hard and have thoroughly addressed your comments in the rebuttal.\n\nAs the discussion period soon comes to an end, we are looking forward to your feedback to our response and revised manuscript. Many thanks again for your time and efforts.\n\nBest regards,\n\nAuthors of Submission 5464"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538566216,
                "cdate": 1700538566216,
                "tmdate": 1700538566216,
                "mdate": 1700538566216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ODuDnurV18",
                "forum": "T8RiH35Hy6",
                "replyto": "0zhBpvIjOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer bi59,\n\nThe rebuttal phase ends today and we have not yet received feedback from you. We believe that we have addressed all of your previous concerns. We would really appreciate that if you could check our response and updated paper.\n\nLooking forward to hearing back from you.\n\nBest Regards,\n\nAuthors"
                    },
                    "title": {
                        "value": "A friendly reminder for discussion"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674383241,
                "cdate": 1700674383241,
                "tmdate": 1700674536714,
                "mdate": 1700674536714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mzNw2GoXSW",
                "forum": "T8RiH35Hy6",
                "replyto": "ODuDnurV18",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Reviewer_bi59"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Reviewer_bi59"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the authors' responses especially the added tables with more quantitative results. These responses addressed some of my concerns. For Q3, I may not agree with the claim that \"since they do not use GCN encoders, community bias amplification does not exist\". Overall, I would like to increase my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699799841,
                "cdate": 1700699799841,
                "tmdate": 1700699799841,
                "mdate": 1700699799841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9uX8QOoABs",
            "forum": "T8RiH35Hy6",
            "replyto": "T8RiH35Hy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_K8c3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5464/Reviewer_K8c3"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies a problem in graph representation learning on graphs with community structures. When the communities have different strengths (e.g., edge densities), the representations of nodes from different communities convergence at different speeds. In the resulting representations, this may lead the representations of nodes from weaker communities to be further apart, which may result in poor classification results on these classes downstream. \nThe paper introduces a contrastive learning approach using random graph coarsening to ameliorate this issue.\nExperiments show that this learning model outperforms existing representation learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written, with only few language errors and typos.\n\nThe problem that is identified is interesting and subtle.\n\nThe proposed solution seems elegant."
                },
                "weaknesses": {
                    "value": "It took me quite a while into reading the paper before I understood what was actually meant with this community bias. I think the introduction of this bias can be made a bit more concrete to improve this.\n\nThe message passing operator $\\hat{A}$ is mentioned but not introduced properly. Is this supposed to be the renormalized version of $\\tilde{A}$ that is shown in (1) or should it be something else?\n\nThe performance in the experiments are measured by the Accuracy and Macro-F1 measure. Both of these methods have biases w.r.t. class sizes [1,2]. I understand why you use Accuracy, as you also use this to motivate the community bias, but perhaps you could replace Macro-F1 by the Matthew's coefficient.\n\n[1] Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC genomics, 21(1), 1-13.\n[2] G\u00f6sgens, M., Zhiyanov, A., Tikhonov, A., & Prokhorenkova, L. (2021). Good classification measures and how to find them. Advances in Neural Information Processing Systems, 34, 17136-17147."
                },
                "questions": {
                    "value": "Consider removing \"Understanding\" from the title, as it sounds a bit generic and makes the title seem less strong.\n\nInstead of using this contrastive learning approach, can't we just cluster the obtained representations by a density-based clustering method like DBSCAN? The problem of different clusterings having different densities doesn't seem like a new problem in clustering.\n\nIt would be interesting to compare the performance of these representation learning methods to community detection methods like the Louvain algorithm [1] or Bayesian community detection methods [2]. This latter method also addresses the issue of different communities having different densities, but does so in a statistical framework.\n\n[1] Blondel, V. D., Guillaume, J. L., Lambiotte, R., & Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10), P10008.\n[2] Zhang, L., & Peixoto, T. P. (2020). Statistical inference of assortative community structures. Physical Review Research, 2(4), 043271."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5464/Reviewer_K8c3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5464/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841310611,
            "cdate": 1698841310611,
            "tmdate": 1699636556713,
            "mdate": 1699636556713,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LgMhJ9rBNu",
                "forum": "T8RiH35Hy6",
                "replyto": "9uX8QOoABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments. We would like to address your questions/concerns below:\n\n**W1\\&Q1: It took me quite a while into reading the paper before I understood what was actually meant with this community bias. I think the introduction of this bias can be made a bit more concrete to improve this. I think the introduction of this bias can be made a bit more concrete to improve this. Consider removing \"Understanding\" from the title, as it sounds a bit generic and makes the title seem less strong.**\n\nThank you for your feedback. We will improve the readability of the paper in the revised version and appropritely modify our title.\n\n**W2:The message passing operator $\\hat{A}$ \n is mentioned but not introduced properly. Is this supposed to be the renormalized version of $\\tilde{A}$ that is shown in (1) or should it be something else?**\n\nYes, $\\hat{A}$ is the renormalized version of $\\tilde{A}$. We will add its introduction in the revised version.\n\n**W3:The performance in the experiments are measured by the Accuracy and Macro-F1 measure. Both of these methods have biases w.r.t. class sizes [1,2]. I understand why you use Accuracy, as you also use this to motivate the community bias, but perhaps you could replace Macro-F1 by the Matthew's coefficient.**\n\nWe agree with your statement that Matthew's coefficient will better eliminate the bias of classification results w.r.t class sizes. \nWe present the results of representative GCL models and our RGCCL regarding the Matthew's coefficient in Table 1. The results also demonstrate the effectiveness of RGCCL in mitigating community bias amplification.\n\n\n\nTable 1: Matthew\u2019s coefficient for RGCCL and baselines.\n|         | Cora         | CiteSeer     | PubMed       |\n| ------- | ------------ | ------------ | ------------ |\n| DGI     | 73.0$\\pm$1.5 | 64.5$\\pm$1.2 | 57.8$\\pm$4.3 |\n| GRACE   | 67.9$\\pm$1.6 | 60.0$\\pm$2.0 | 62.3$\\pm$3.8 |\n| CCA-SSG | 74.5$\\pm$1.6 | 64.6$\\pm$1.4 | 64.0$\\pm$4.1 |\n| GGD     | 77.0$\\pm$1.6 | 64.9$\\pm$1.2 | 63.7$\\pm$4.1 |\n| GRADE   | 75.7$\\pm$1.5 | 62.1$\\pm$1.3 | 58.1$\\pm$3.4 |\n| RGCCL   | 78.9$\\pm$0.9 | 66.3$\\pm$0.8 | 65.6$\\pm$4.2 |\n\n**Q2:Instead of using this contrastive learning approach, can't we just cluster the obtained representations by a density-based clustering method like DBSCAN? The problem of different clusterings having different densities doesn't seem like a new problem in clustering.**\n\nGCL and clustering methods have fundamental differences. The goal of GCL is to learn node embeddings for various downstream tasks, including node classification, clustering and link prediction. Clustering is typically an unsupervised method. Although GCL learns embeddings in an unsupervised manner, in a downstream task, say node classification, a classifier will be trained in a supervised manner, and thus can achieve much better accuracy than purely unsupervised methods. In this paper, our objective is to address the bias present in the learned node embeddings by GCL. We aim to make embeddings learned by GCLs more unbiased. We think combining ideas from density-based clustering methods, as suggested by the reviewer, with graph representation learning is an interesting direction to address the bias problem. We will investigate how to use such clustering methods in end-to-end GCL models in future work.\n\n**Q3:It would be interesting to compare the performance of these representation learning methods to community detection methods like the Louvain algorithm [1] or Bayesian community detection methods [2]. This latter method also addresses the issue of different communities having different densities, but does so in a statistical framework.**\n\nWe appreciate the reviewer's insightful observation highlighting the common concern of community bias in representation learning and the variations in community densities encountered in community detection. However, it is crucial to acknowledge the inherent distinctions between these tasks. In graph representation learning, our focus is on learning node embeddings for various downstream tasks, not limited to clustering tasks alone. Integrating these community detection algorithms into the GCL model is one of our future directions of research."
                    },
                    "title": {
                        "value": "Response to Reviewer K8c3"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464584866,
                "cdate": 1700464584866,
                "tmdate": 1700464679996,
                "mdate": 1700464679996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xdz067uWRu",
                "forum": "T8RiH35Hy6",
                "replyto": "9uX8QOoABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder for discussion and we appreciate your input"
                    },
                    "comment": {
                        "value": "Dear Reviewer K8c3,\n\nWe thank you again for your insightful and constructive review. We have worked hard and have thoroughly addressed your comments in the rebuttal.\n\nAs the discussion period soon comes to an end, we are looking forward to your feedback to our response and revised manuscript. Many thanks again for your time and efforts.\n\nBest regards,\n\nAuthors of Submission 5464"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538506615,
                "cdate": 1700538506615,
                "tmdate": 1700538506615,
                "mdate": 1700538506615,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TwKhuntBTp",
                "forum": "T8RiH35Hy6",
                "replyto": "9uX8QOoABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5464/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer K8c3,\n\nThe rebuttal phase ends today and we have not yet received feedback from you. We believe that we have addressed all of your previous concerns. We would really appreciate that if you could check our response and updated paper.\n\n Looking forward to hearing back from you.\n\nBest Regards,    \n\nAuthors"
                    },
                    "title": {
                        "value": "A friendly reminder for discussion"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5464/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674096657,
                "cdate": 1700674096657,
                "tmdate": 1700674518995,
                "mdate": 1700674518995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]