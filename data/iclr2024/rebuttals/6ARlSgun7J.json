[
    {
        "title": "Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction"
    },
    {
        "review": {
            "id": "kpgvNw0JN9",
            "forum": "6ARlSgun7J",
            "replyto": "6ARlSgun7J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_UBHy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_UBHy"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates a novel factor, label variance, in the context of tail performance in extreme classifiers. To address this issue, the authors propose LEVER, a knowledge distillation framework that leverages a robust teacher model to reduce label variance. Experimental results demonstrate that LEVER significantly improves tail performance, achieving approximately 5% and 6% increases in PSP and Coverage metrics, respectively, when integrated with state-of-the-art extreme classifiers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper give a proof of the correlation between the generalization performance of a classifier and the variance in labels.\n2. Use a Siamese- style model as a teacher to help reduce the label variance effects.\n3. The extensive experiment results are promising.\n4. The paper contribute two new datasets to the field."
                },
                "weaknesses": {
                    "value": "1. The readability of the paper is not strong, and the formatting is uncomfortable. For example, the abstract should generally be a single paragraph, there are too many blank spaces before and after Section 2 Related Work, and the spacing of equations is large. With ample space in the appendix, the full text does not fill up the nine pages. Improvements are needed in basic writing formatting and readability.\n2. The paper has limited novelty. From a personal perspective, the innovation lies in using a teacher model to predict probabilities. The rest of the paper mainly demonstrates the significant impact of tail classes. From the perspective of innovative design, it is not convincing. It is also unclear why the teacher model can provide accurate estimates of $p_x$.\n3. There is limited description regarding fair comparisons. After introducing the teacher model, the training process and time will be affected. The main part should devote more space to introducing the teacher model, analyzing its performance, and comparing the training time with other models."
                },
                "questions": {
                    "value": "1. Can the teacher model be adapted to the current task? How does its performance compare? What are the differences between directly using a better model for distillation and utilizing the results of other models as auxiliary information in this paper?\n2. Refer to the weaknesses mentioned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Reviewer_UBHy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697620328049,
            "cdate": 1697620328049,
            "tmdate": 1699637170872,
            "mdate": 1699637170872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Sa7aGI5Y1",
                "forum": "6ARlSgun7J",
                "replyto": "kpgvNw0JN9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. \n\n**Can the teacher model be adapted to the current task?  How does its performance compare?**\n\nThe teacher model is adapted to the current task, the Siamese model is trained on the same dataset on which the classifier-based methods were trained. The teacher model (Siamese network) is much better than the student model (classifier) on the tail (indicated by superior PSP Coverage), while it lags behind significantly in Precision. Using the superior performance of the teacher on the tail, LEVER improves the OvA classifier\u2019s tail performance. We have updated the draft to include a comparison between teacher and student in Table 4 in the main paper.\n\n**What are the differences between directly using a better model for distillation and utilizing the results of other models as auxiliary information in this paper?**\n\nWe request the reviewer to clarify the question. From what we understand, the reviewer is asking the effect of using a better model for distillation.\nIf so, it is the case that a better teacher model results in better performance of classifiers when combined with LEVER. Table 13 in the supplementary shows the effect of using different teachers. We experiment with 3 teachers: Astec, MiniLM 3-layer, and 6-layer DistilBERT.However, it is important to note that even our best teacher, NGAME 6-layer DistilBert, performs much inferior to classifier-based methods on precision.\n\n**Readability of paper not strong and uncomfortable formatting. Full text not filling 9 pages. More discussion around train time and teacher model performance.**\n\nWe apologize for the concerns around formatting. We have made the following changes:\n(i): Abstract has been made concise, \n(ii): Empty space around Section 2 related work and  equations has been cut down,\n(iii): We have devoted the additional space obtained to discuss the train time overhead of using LEVER (Table 3) and Comparison with the teacher model (Table 4)\n\n**Limited novelty: the innovation lies in using a teacher model to predict probabilities. From a personal perspective, the innovation lies in using a teacher model to predict probabilities From the perspective of innovative design, it is not convincing.**\n\nWhile the approach might be simple, we believe the concept of label variance we propose is novel. Additionally, we would like to highlight the difference between our setup and conventional distillation, which usually relies on a teacher model that is uniformly better than the student. In our work, we show that a teacher model that excels on a subpopulation (tail-labels) can also be leveraged to improve the performance of OvA classifiers for XMC tasks. Further, the simplicity of the approach should rather be viewed as a strength as it allows for easy and effective combinations with multiple SOTA classifier-based methods. \n\n**Why does the Siamese model provide accurate estimates of $p_x$?**\n\nRecent studies have shown that Siamese Networks, when used as input encoders, exhibit impressive performance on tail labels (Dahiya et al., 2021a; 2022; Jain et al., 2023). This success can be attributed to the ability of Siamese encoders to learn correlations by utilizing label-side features. Further, we consume only high confidence estimates of $p_x$ from Siamese teacher models: \u201cFor each label l, the \u03c4 nearest points from the pool are selected and added as ground truth.\u201d\n\n---\n\nWe hope this rebuttal clarifies the concerns you had about the paper. We would be happy to discuss any further questions about the work, and would really appreciate an appropriate increase in the score if the reviewer\u2019s concerns are adequately addressed to facilitate the acceptance of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378603159,
                "cdate": 1700378603159,
                "tmdate": 1700378603159,
                "mdate": 1700378603159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8az92Q3ydM",
                "forum": "6ARlSgun7J",
                "replyto": "4Sa7aGI5Y1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_UBHy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_UBHy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, but I still choose to keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481954994,
                "cdate": 1700481954994,
                "tmdate": 1700481954994,
                "mdate": 1700481954994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jns9oikZjC",
            "forum": "6ARlSgun7J",
            "replyto": "6ARlSgun7J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Kx6v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Kx6v"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve the tail labels performance of the extreme multi-label classification (XMC) problems. The paper propose LEVER, a knowledge distillation framework, that learns student OVA classifiers with binary relevance true labels and soft-labels generated by teacher bi-encoders. The author claims that bi-encoders have better performance on tail labels, and hence using their soft labels as additional signals to learn the student OVA classifiers can help reduce the variance for tail labels. Empirically, LEVER demonstrated consistent improvement upon competitive OVA classifiers on a wide-range of XMC datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper writing is easy to follow\n2. Empirical results are strong"
                },
                "weaknesses": {
                    "value": "1. Its questionable that using \"pairwise\" logistic loss (Eq.10) lead to calibrated scores\n2. Some experiment settings are not clear from the main text. See detailed questions below.\n3. Missing proper evaluation metrics (i.e., Macro-average Precision/Recall/F1) for tail-labels performance"
                },
                "questions": {
                    "value": "## 1. Calibrated Scores\nIts well known that pairwise ranking losses (e.g., Equation (10) of this submission) is shift-invariant hence may not produce calibrated scores, as pointed out by [1]. Why not considering point-wise loss functions or hybrid objectives [1] (listwise Softmax + pointwise BCE) to produce more calibrated scores?\n\n## 2. Experiment Settings and Results\n(1) What's the model size for each method in Table 1 and Table 2? The model size should include every component needed to do inference. For example, does ELIAS and ELIAS+LEVER have the same model size, as LEVER is just in-place modifying the OVA classifiers of LEVER? If not, I am concern about the performance gain of LEVER is due to additional model capacity. \n\n(2) In Table 1, ELIAS+LEVER achieved >20% absolute gain on LF-AOL-270K in P@3 and P@5. If not a typo, any insight why such significant gain? Similar questions to ELIAS+LEVER on LF-Wiki-1M, PSP metrics.\n\n(3) On page 9, the author claims the training time of LEVER only increased by at most 2x. Does that include (a) training time of teacher bi-encoders (b) prediction time of teacher bi-encoder to generate soft-labels (c) training time of student OVA classifiers which are trained by not only sparse ground truth labels but also dense soft-labels? If so, what's the detailed breakdown in terms of those three components?\n\n(4) Suppose the ground truth label matrix is a sparse $N \\times L$ matrix. How dense are the soft-labels generated by LEVER? Does the performance of LEVER vary when using different top-$k$ soft-labels per input?\n\n## 3. Macro-averaged Evaluation Metrics\nTo properly measure the performance of long-tailed labels, text classification community often consider Macro-average Precision/Recall/F1 metrics [2,3,4]. The author should also report Macro-averaged metrics to further validate the major claim of LEVER, which is the the performance gain on tailed-labels.\n\n\n## Reference\n\n[1] Yan et al. Scale Calibration of Deep Ranking Models. KDD 2022.\n\n[2] Zhang et al. Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions. EACL 2023.\n\n[3] Yang et al. A re-examination of text categorization methods. SIGR 1999.\n\n[4] Lewis et al. RCV1: A New Benchmark Collection for Text Categorization Research. JMLR 2004."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Reviewer_Kx6v"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790360127,
            "cdate": 1698790360127,
            "tmdate": 1700638413725,
            "mdate": 1700638413725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5budJXqnPi",
                "forum": "6ARlSgun7J",
                "replyto": "jns9oikZjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part-1 of Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We request the reviewer to clarify the ethical concerns with our submission as we see our paper has been flagged for ethics review. \n\nBelow we clarify some of the concerns.\n\n**Calibration concerns**\n\nThank you for bringing this paper [1] to our notice. Since our loss is shift-invariant, the scores produced by the model are not calibrated, as pointed out in [1]. If $s_x =z_l^{T}z_x$  is the score output by the Siamese model for a label $l$ (with embedding $z_l$) and document $x$ with embedding $z_x$ this would mean that $s_x$ is not calibrated, we agree. However as indicated in theorem 2, we make use of probabilities $p_x = 1/(1 + e^{-(ms_x + c)})$ where $m,c$ are hyper-parameters for training OvA classifiers. Note that this  is in line with the post-hoc calibration strategies explored in [2], where a model is learned and then a parametrized Sigmoid function is fit to learn the relevance probabilities, i.e., given an uncalibrated score $s_x$ we learn $p_x$ by fitting the sigmoid function $p_x = 1/(1 + e^{-(ms_x +c)})$ where $m, c$ are model hyper-parameters.\n\n**Why not use BCE Loss or methods proposed in [1] to achieve calibration?**\n\nWe acknowledge that exploring the techniques in [1] could certainly be an interesting direction for future investigation to understand the effect of teacher models trained with different loss functions on student model performance, and we will add this as a future direction in our updated draft.\nAdditionally, from our experience using BCELoss while training Bi-encoders leads to training instability. Hence we resorted to using pair-wise losses and posthoc calibration techniques \n\n[1]: Scale Calibration of Deep Ranking Models, Le Yan et al, KDD 2022\n\n[2]: Probabilistic Outputs for SVMs and Comparisons to Regularized Likelihood Methods, Platt 2000\n \n**Experimental settings and results**\n\n**Model Size**\n\nPlease note that the inference pipeline is unchanged with the addition of LEVER (both in terms of model capacity and inference time). LEVER only affects the training strategy by the addition of soft labels. So, the model size is the same for the base classifier and its LEVER counterpart.\nThe below table shows the model size (in Millions of parameters) for the different models and datasets. Note the smaller model size in ELIAS for LF-AmazonTitles-1.3M is due to using 512 dimension classifiers, (as 768D led to OOM issues).\n\n||Renee|ELIAS|CascadeXML|\n|-|-|-|-|\n|LFAT-13K|167.03M|168.73M|268.94M|\n|LFA-131K|167.03M|168.73M|268.94M|\n|LF-AOL-270K|275.26M|277.14M|377.95M|\n|LF-WikiSeeAlso-320K|312.12M|315.59M|408.33M|\n|LF-Wikipedia-500K|451.90M|457.93M|553.47M|\n|LF-WikiHierarchy-970K|815.93M|829.51M|918.86M|\n|LF-AmazonTItles-1.3M|1064.76M|746.25M|1171.11M|\n\n\n\n**Significant gain in  P@3/5 AOL in ELIAS**\n\n\nWe apologize for the typo here, we accidentally entered values for ELIAS + LEVER on LFAT-1.3M here. The correct values were present in the appendix (Table 7), and now they have been added to the main paper in Table 1. as well. Here are the values for your reference\n\n|LF-AOL-270K|P@1|P@3|P@5|PSP@1|PSP@3|PSP@5|\n|-|-|-|-|-|-|-|\n|ELIAS |40.83 |22.33 |14.91 |13.29 |21.46 |25.22 | \n|ELIAS + LEVER |40.85 |22.83 |15.57 |13.68 |24.30 |30.43|\n\n**Significant gain in PSP metrics in WikiHierarchy in ELIAS**\n\nIn ELIAS there is a bigger drop in head performance as compared to Renee when LEVER is applied.  This therefore translates to a much larger increase in ELIAS+LEVER\u2019s tail performance for this dataset, note that for ELIAS the performance improvement starts from the torso labels itself. This is illustrated in Figure 8 in the supplementary which compares decile wise plot ELIAS, ELIAS + LEVER and Renee, Renne+LEVER."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379541834,
                "cdate": 1700379541834,
                "tmdate": 1700379541834,
                "mdate": 1700379541834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fQx39BRvhQ",
                "forum": "6ARlSgun7J",
                "replyto": "RdASL7hszP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_Kx6v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_Kx6v"
                ],
                "content": {
                    "comment": {
                        "value": "I am satisfied with the authors response and appreciate the additional experiment results. I am happy to increase the rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638390697,
                "cdate": 1700638390697,
                "tmdate": 1700638390697,
                "mdate": 1700638390697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O6ZTIrtlbZ",
            "forum": "6ARlSgun7J",
            "replyto": "6ARlSgun7J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Cq1J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Cq1J"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new extreme multi-label classification algorithm called LEVER, that aims to improve the existing methods by proposing training with a loss function that combines loss calculated on observed hard labels and soft labels coming from the teacher model, which is assumed to have superior performance on the tail labels. The teacher model used by the authors in the empirical part of the paper is a Siamese-style neural network that leverages label features and is trained with a logistic-loss-based objective, which was found to yield calibrated estimates of the marginal probabilities of labels. The authors demonstrate the attractiveness of the proposed approach in the exhaustive empirical comparison when they used 4 popular XMLC benchmarks, introduced 2 new benchmarks, and combined the proposed approach with 3 SOTA methods. The proposed approach, in many cases, significantly improves the performance on standard precision@k, propensity-scored precision@k, and coverage@k."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The proposed method is simple to apply to a wide range of XML classifiers.\n3. The exhausting empirical comparison proved the attractiveness of the proposed method.\n4. In addition to the main results, the authors provide a wide array of additional experiments in the appendix.\n5. Two new datasets additionally strengthen the contribution of this work."
                },
                "weaknesses": {
                    "value": "1. The proposed method is just a combination of loss with hard and soft labels, which is a simple idea.\n2. I find the theoretical results rather simplistic, expected, and being there to serve as just justification for the applied method rather than its original motivation.\n3. This part of the paper especially confirms that for me:\n\n   > Notably, if we have precise estimates of marginal relevance, denoted by $p_x = E[y|x]$, we can replace $y$ with $p_x$, effectively reducing the variance term to 0 and thereby improving classifier generalization. This principle forms the foundation for the LEVER framework, which employs an additional teacher network to provide accurate estimates of $p_x$.\n\n   If we have a model that provides good estimates $p_x$ then we don't need to train another one, the XMLC task is already solved! The strength of the method seems to lay in the properly selected trade-off between loss calculated on hard observed labels and soft labels coming from the teacher network that seems to be much better on tail-labels thanks to leveraging labels-side information. \n\n4. From the appendix, I understand that $\\lambda = 0.05$ (the variable that weights the hard and soft part of the loss) was used for all the experiments. Since the choice of $\\lambda$ seems to be crucial for the method. I find the lack of further comments on it and experiments demonstrating its impact the biggest weakness of this paper.\n\nMy score is 6, but I believe the paper is even a bit above that."
                },
                "questions": {
                    "value": "1. As I mentioned in the weakness section, I would like to see how different values of $\\lambda$ impact the final performance and what the trade-off curve between head-labels and tail-labels performance looks like. Could the authors comment on that?\n2. On the LF-AOL-270K dataset, LEVER combined with ELIAS yields extremely high improvement on standard precision@k, especially at @5. These seem almost unrealistic when compared with scores of other methods. Are these numbers for sure correct? If yes, do the authors have any explanation for this result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699286099960,
            "cdate": 1699286099960,
            "tmdate": 1699637170621,
            "mdate": 1699637170621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cALwRAOyWZ",
                "forum": "6ARlSgun7J",
                "replyto": "O6ZTIrtlbZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "content": {
                    "title": {
                        "value": ""
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. Below, we provide clarifications.\n\n**The proposed method is just a combination of loss with hard and soft labels, which is a simple idea.**\nYes, while the idea is a combination of hard and soft labels, we would like to highlight that this simplicity allows the easy and effective combination of LEVER with any OvA-based XMC approach. However, unlike standard distillation where logits for all classes from teacher are used, in our case, for each label l, only the \u03c4 nearest points from the pool are selected and added as ground truth for distillation. This is how we have adapted distillation for our multi-label scenario. Table 22 and Figure 7 in supplementary show that increasing \u03c4 improves performance on tail, while it hurts head or torso label\n\n**If we have a model that provides good estimates then we don't need to train another one; the XMLC task is already solved!**\n\nOur intention for writing \u201cif we have precise estimates of marginal relevance.. Reducing the variance to 0\u201d was to convey that as the teacher gets more accurate, the variance term reduces to zero, and thus the upper bound on the true risk reduces, thereby improving generalization error of the classifier. However, a perfect teacher model is not available in practice. The NGAME model we use is a proxy for the perfect teacher and note that it is more accurate than the OvA classifier only on tail labels. Leveraging the strength of this Siamese model on the tail to reduce the label variance of OvA classifiers on the tail is what LEVER aims to achieve.\n\n**From the appendix, I understand that $\\lambda$ (the variable that weights the hard and soft part of the loss) was used for all the experiments..**\n\nWe think the reviewer has confused the $\\lambda$ for ELIAS mentioned in Section D.2 of supplementary with the $\\lambda$ for LEVER. We have renamed the $\\lambda$ used in Section D.2 to $\\lambda_{elias}$. The $\\lambda$ used in LEVER is 0.5 (equal weight to both hard & soft labels).\n\n**I would like to see how different values of $\\lambda$ impact the final performance and what the trade-off curve between head-labels and tail-labels performance looks like**\n\nIn table below, we show the effect of varying $\\lambda$ when LEVER is combined with Renee on 3 datasets: LF-AmazonTitles-131K, LF-Wikipedia-500K, & LF-AOL-270K. Note that $\\lambda$=0.33 weighs the soft labels with double the weight as compared to hard labels. \nOur observations are:\nThe equal weightage (0.5) gives the best performance.\nIncreasing $\\lambda$ weighs the hard labels more and this results in performance that gets closer to the base classifier, i.e., PSP worsens, and Precision remains flat or drops slightly.\nAdditionally, decreasing $\\lambda$ helps only up to a certain point, i.e., $\\lambda$=0.5; this is because our teacher is not perfect we need to strike a balance between hard and soft labels.\n\n|LF-AmazonTitles-131K||||||||||\n|-|-|-|-|-|-|-|-|-|-|\n|$\\lambda$|P@1|P@3|P@5|PSP@1|PSP@3|PSP@5|C@1|C@3|C@5|\n|0.33|46.15|30.76|21.87|39.70|45.33|50.13|32.56|54.48|61.24|\n|0.50|46.44|30.83|21.92|39.70|45.44|50.31|32.82|55.11|61.94|\n|0.66|46.57|30.87|21.93|39.59|45.46|50.36|32.31|54.49|61.48|\n|0.80|46.58|30.88|21.92|39.37|45.39|50.33|32.06|54.38|61.42|\n\n|LF-Wikipedia-500K||||||||||\n|-|-|-|-|-|-|-|-|-|-|\n|$\\lambda$|P@1|P@3|P@5|PSP@1|PSP@3|PSP@5|C@1|C@3|C@5|\n|0.33|84.66|65.94|51.54|40.51|53.40|58.75|26.88|56.01|67.94|\n|0.50|85.02|66.42|52.05|42.50|54.86|60.20|29.46|58.53|70.29|\n|0.66|84.96|66.51|52.18|39.34|53.53|59.46|25.66|55.78|68.33|\n|0.80|84.84|66.42|52.14|38.66|53.09|59.16|24.96|55.08|67.79|\n\n|AOL-270K||||||||||\n|-|-|-|-|-|-|-|-|-|-|\n|$\\lambda$|P@1|P@3|P@5|PSP@1|PSP@3|PSP@5|C@1|C@3|C@5|\n|0.33|41.14|24.00|16.50|17.24|32.31|40.02|14.14|36.67|45.84|\n|0.50|41.70|24.78|17.07|20.38|37.07|45.13|17.43|42.54|52.01|\n|0.66|41.44|24.41|16.76|16.76|32.69|40.47|14.16|37.38|46.54|\n|0.80|41.17|24.04|16.49|15.59|30.30|37.64|13.16|34.62|43.25|\n\nThis table has been added as Table 16 in the supplementary material. For decile-wise plots showing head-tail variation, please refer to Fig. 4, 5, 6 in the supplementary material\n\n**On the LF-AOL-270K dataset, LEVER combined with ELIAS yields extremely high improvement on standard precision@k..**\n\nThanks for pointing this out. This is a typo; the number from the LF-AmazonTitles-1.3M ELIAS+LEVER row was mistakenly copied to the ELIAS+LEVER row in LF-AOL-270K. The supplement however has the correct values for LF-AOL-270K without this typo. We fixed this in the main table of updated draft.\nHere are the values for your reference\n|LF-AOL-270K|P@1|P@3|P@5|PSP@1|PSP@3|PSP@5|\n|-|-|-|-|-|-|-|\n|ELIAS |40.83 |22.33 |14.91 |13.29 |21.46 |25.22 |\n|ELIAS + LEVER |40.85 |22.83 |15.57 |13.68 |24.30 |30.43|\n\n---\nWe hope this rebuttal clarifies your concerns. We would be happy to discuss any further questions, and would appreciate an appropriate increase in the score if the reviewer\u2019s concerns are addressed to facilitate the acceptance of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378208281,
                "cdate": 1700378208281,
                "tmdate": 1700378208281,
                "mdate": 1700378208281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bPwOn1D8KL",
            "forum": "6ARlSgun7J",
            "replyto": "6ARlSgun7J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Uo9h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9297/Reviewer_Uo9h"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a methodology for improving performance on tail-labels. The main observation behind the proposed approach is that there is a significant variance in the label distribution due to the finite and even scarce number of data points for tail-labels in the extreme classification. The paper builds a connection with an existing work (Menon et al. 2021), on which it heavily relies, for most of the motivation, to propose a variance reduction strategy (Theorem 1 in the paper & discussion thereafter) and hence improvement in the generalization error. \n\nThe focus of the approach (generalization analysis) is on the last/classification layer of the network. The above generalization analysis leads to an augmented objective which, in addition to standard hard labels, also consists of a loss term consisting of soft labels from a teacher model, for which recent frameworks based on Siamese training are exploited. The proposed approach is tested on a range of datasets from the extreme classification repository, and it is shown that the proposed approach leads to significant improvements in the prediction performance in terms of P@k and PSP@k metrics. The augmentation strategy is also compared to existing methods for data augmentation in extreme classification to further demonstrate its general applicability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper attempts to address a key shortcoming of existing methods in extreme classification i.e. performance of sota methods on tail-labels, which despite its importance, doesn\u2019t get much focus of research.\n\n2. The experimental results of the paper are quite impressive, and significant gains on a range of datasets, and methods are shown, thereby demonstrating its general applicability. Overall, the experimental setup is quite detailed.\n\n3. The paper also contributes two additional datasets (LF-AOL-270K, and LF-WikiHierarchy-1M), which may be helpful for the community."
                },
                "weaknesses": {
                    "value": "1. The main shortcoming of the paper is the lack of novelty in the main idea and the approach :\n\n1a) In terms of the content i.e. the main theoretical results such as Theorem 1, its proof, the idea to reduce variance for improving generalization, using soft-labels in teacher-student setup has already been explored in the existing work of Menon et al 2021. It seems that the paper attempts to build a loose bridge between this earlier theoretical work to build a weighting scheme and soft-labels (equation 9). \n\n1b) The paper claims that the variance issue for tail-labels has not been explored so far. However, this seems not quite true, as a similar concern has been raised in the earlier work Babbar and Scholkopf 2019, where there was a similar argument that due to the lack of data in tail-labels, for a given class label, there is high variance between the input features between two samples. \n\n2) There also seems to be a lack of consistency in terms of using the symbols and notation in many places. For instance, X is used in Section 3.1 to denote a (random?) instance in the input space $\\mathcal{X}$. However, the same is used to limit the norm of x in the same section. For equation (1), should the LHS also not be conditioned on x i.e. V[y|x]. It is not clear why there are two different symbols for the sub-script (small-case) x. Why the equation (6) is defined in terms of a trained classifier w*, while the standard generalization results are stated in terms of all possible classifiers in the hypothesis class. The theorem statement needs to be on a better formal footing as done in the original paper Menon et al. 2021, the current version is incomplete and unclear.\n\n3. In terms of properly citing related work, the paper falls somewhat short. For instance, the paper introduces the need to use calibrated losses without a proper justification to use calibrated losses or a reference thereof. It does not seem to be in the sense of the word used in the last sentence of page 1. Another concern is the lack of setting the right context for the cited papers.  As mentioned above, the high variance problem for tail-labels has also been explored. As another instance, the work of Schultheis et al. 2022 is cited but the correct context is in the related work section when discussing Missing and Tail-labels in Section 2.2, and not only later down in discussing coverage as a metric. Even though the paper has been cited, it is not in the right context. In terms of the main classifier, the focus of the paper is on one-vs-rest approaches, while DiSMEC which initiated this approach in extreme settings isn\u2019t referred."
                },
                "questions": {
                    "value": "As mentioned above in the weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9297/Reviewer_Uo9h"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699538361824,
            "cdate": 1699538361824,
            "tmdate": 1700734785102,
            "mdate": 1700734785102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9HrmB4lqcE",
                "forum": "6ARlSgun7J",
                "replyto": "bPwOn1D8KL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. Below, we provide clarifications:\n\n>  Concerns around novelty: The paper claims that the variance issue for tail labels has not been explored so far. However, this seems not quite true, as a similar concern has been raised in the earlier work by Babbar and Scholkopf 2019\n\n[1] discusses the issue of variance from the point of view of a lack of commonality between features of training instances and those between training and test instances. However, the definition of variance we use is different, and we highlight it with an example below:\n\nConsider a dataset where data points are user queries and labels are advertisements shown in response to user queries. \nNow, if we consider a label with the text \u201cinsurance,\u201d there could be a wide variety of user queries (or documents), such as \u201chospital bills\u201d and \u201ccar repair,\u201d that are relevant to this label. This lack of commonality between documents associated with a single label has been discussed in [1].\n\nIn recommendation tasks, dataset rely on click signals to build the ground truth, i.e., if the user clicks on an advertisement for a particular query, then the query-advertisement pair is considered relevant (1 in ground truth). However, feedback in the form of user clicks is subject to variability as a user\u2019s interests can fluctuate over time; for example, for the query \u201chospital bills,\u201d there may or may not be a click on the \u201cinsurance\u201d ad. Approximating the relevance using click signals that are binary leads to inaccuracies in the dataset, which we quantify using the term label variance.\n\nIn regards to the novelty of our work, we would like to highlight a few points:\n\n1.\tWe propose the concept of label variance, which, to the best of our knowledge, has not been explored in the domain of extreme classification.\n\n2.\tWhile Menon et al. discuss the teacher-student setup, their teacher is better than the student overall (as indicated by superior Precision values in Table 1). While such a strong teacher might not always be available, we demonstrate the potential of improving the performance of OvA classifiers by proposing a Siamese teacher that excels only  on tail labels. Note that the Siamese teacher is inferior in precision compared to the OvA student model. To the best of our knowledge, we are the first to propose distilling from a teacher that excels only on a subset of the dataset population (tail labels) for XMC tasks.\n\n3.\tWhile the idea is simple, we believe the method's simplicity is a strength as it allows for an easy and effective combination with state-of-the-art one-versus-all methods, as highlighted in our results.\n\n[1]: \u201cData scarcity, robustness, and extreme multi-label classification\u201d (Babbar and Scholkopf 2019)\n\n> Inconsistent notation in Section 3.1, and better presentation of improving presentation of Theorem 1.\n\nThanks for highlighting this. We will work on addressing the issues in our final draft.\n\n> Citing related works\n\n**Justification for the use of calibrated losses**\n\nIn Section 3.3, we discuss the need to use calibrated losses. In summary, Siamese networks trained using triplet loss have shown to do well on tail labels. However, the scores output by these models are not well calibrated, i.e., they do not have a probabilistic interpretation). For training OvA classifiers with soft targets, we need calibrated scores, and Theorem 2 in our paper shows how we can obtain calibrated scores from the Siamese teacher.  Note that this is in line with the posthoc calibration strategy mentioned in [3], where a model is learned. Then a parametrized Sigmoid function is fit to learn the relevance probabilities, i.e., given an uncalibrated score $s_x$ we get $p_x$ by fitting the sigmoid function $p_x = 1/(1 + e^{-(ms_x +c)})$ where $m, c$ are model hyper-parameters.\nWe will clarify the definition of calibration used in the current context and add a citation to [2].\n\n**Meaning of calibration used at the end of Pg1?**\n\nAt the end of Pg 1. We discuss the strategies that rebalance the loss functions to tackle the problem of extreme class imbalance in XMC datasets. We will clarify this in the final version of the paper.\n\n**Citing work of Schultheis et al. 2022 in the correct context and missing citation of DiSMEC**\n \nIn our revised draft, we have cited the work of Schultheis et al. 2022 in Section 2.2 while discussing \u201cBias due to Missing Lables,\u201d and  DiSMEC has now been cited in the introduction while discussing about OvA classifiers in the revised draft.\n\nWe hope this rebuttal clarifies the concerns you had about the paper. We would be happy to discuss any further questions about the work. We would really appreciate an increase in the score if the reviewer\u2019s concerns are adequately addressed to facilitate the acceptance of the paper.\n\n[2]: Scale Calibration of Deep Ranking Models, Le Yan et al, KDD 2022\n[3] Probabilistic Outputs for SVMs and Comparisons to Regularized Likelihood Methods, Platt 2000"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409886155,
                "cdate": 1700409886155,
                "tmdate": 1700409886155,
                "mdate": 1700409886155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gE6OTJgu3C",
                "forum": "6ARlSgun7J",
                "replyto": "9HrmB4lqcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_Uo9h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9297/Reviewer_Uo9h"
                ],
                "content": {
                    "title": {
                        "value": "post author rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the response. While this does help somewhat in improving my score, there are still some concerns/clarifications required :\n1. Variance issues : In my opinion, both the earlier work [1] and this one focus on different aspects of the same issue i.e. data scarcity leading to shift in the observed empirical distribution between different sub-samples. In [1] the change in the empirical distribution (between train and test) of the inputs x_i for a given label, and in this case, possible change in the observed label y_i for an instance x_i over time or different annotators. In the second scenario, however, it seems like a missing label problem, even though the authors mention that this does not include any systematic bias, and hence they claim that it is not a missing labels problem, to which I am not sure if I entirely agree. Despite that PSP metrics are used for evaluation, which were designed for missing label problem in the first place, and the usage for measuring the tail-label performance as a metric is rather secondary (as per Schultheis et al 2022). Apparently, we seem to be going in circles, and this requires some clarification/discussion in the paper. \n\n2. The overlap with Menon et al. 2021 remains a concern, and the paper in its current form lacks theoretical soundness in comparison to that paper. On the positive side, it has strong empirical performance and addresses a key problem in XMLC literature."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9297/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734744012,
                "cdate": 1700734744012,
                "tmdate": 1700734744012,
                "mdate": 1700734744012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]