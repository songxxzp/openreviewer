[
    {
        "title": "Enhancing Neural Subset Selection: Integrating Background Information into Set Representations"
    },
    {
        "review": {
            "id": "Omfs3dIcbU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
            ],
            "forum": "eepoE7iLpL",
            "replyto": "eepoE7iLpL",
            "content": {
                "summary": {
                    "value": "This paper proposes a neural subset selection method based on deep sets. This model is inspired by a theoretical perspective to include information from supersets to achieve better performance. Experiments on common benchmarks show SOTA performance compared to several recent baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea to include information from superset is simple and effective as shown by the experiment results\n2. Theoretical discussions are provided."
                },
                "weaknesses": {
                    "value": "1. Equation 4 describes the neural network construction. However, I am unclear about the objective function to optimize the neural network. Also, after optimization, how do you use this neural network to select a subset?\n\n2. In equation 4, how do you divide a superset into several subsets? There are an exponential number of combinations.\n\n3. What is the number of learnable parameters for each baseline method and the proposed method?"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9406/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697428066328,
            "cdate": 1697428066328,
            "tmdate": 1700014592522,
            "mdate": 1700014592522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VAHKGn2TpF",
                "forum": "eepoE7iLpL",
                "replyto": "Omfs3dIcbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to Reviewer miuk (part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate the time and effort you have invested. In response to your concerns, we have provided detailed clarifications.\n### ***Comments 1: The optimization objective and how to select an optimal subset during inference?***\n***ANSWER:*** Thank you for your insightful question, as the construction of an optimization objective and inference process are indeed key aspects of neural subset selection [1, 2, 3]. We aimed to balance detailed explanation with readability for a broad audience, which is why we initially provided a high-level overview in the **Introduction Section**. To specifically address your concerns, we are now including more detailed descriptions of the optimization and inference processes.\n\nOur formulation of the optimization objective is based on the framework established in [1]. Specifically, the optimization objective is to address Equation 1 in our paper by adopting an implicit learning strategy grounded in probabilistic reasoning. This approach can be succinctly formulated as follows:\n$$ argmax_\\theta\\ \\mathbb{E}_{\\mathbb{P}(V, S)} [\\log p _\\theta (S^{*}| V)] $$\n$$s.t.  p _\\theta (S | V) \\propto  F _\\theta (S ; V), \\forall  S \\in 2^V, $$\n\nThe important step in addressing this problem involves constructing an appropriate set mass function $p_\\theta (S|V)$ that is monotonically increasing in relation to the utility function $F_\\theta (S;V)$. To achieve this, we can employ the Energy-Based Model (EBM):\n$$\np_\\theta (S|V) = \\frac{\\mathrm{exp}( F_\\theta (S; V))}{Z}, \\; Z := \\sum\\nolimits_{S'\\subseteq V}  \\mathrm{exp}( F_\\theta (S'; V)),\n$$\nIn practice, we approximate the EBM by solving a variational approximation\n$$\n    \\phi^* =  argmin_{\\phi} D(q_\\phi(Y|S,V)) || p_\\theta (S|V)),\n$$\nDuring the training phase, we need an EquiNet, denoted as $Y = \\operatorname{EquiNet}(V;\\phi): 2^V \\rightarrow [0,1]^{|V|}.$ This network takes the ground set $V$ as input and outputs probabilities indicating the likelihood of each element $x \\in V$ being part of the optimal subset $S^*$. In the inference stage, EquiNet is employed to predict the optimal subset for a given ground set $V$, using a TopN rounding approach. For detailed information on the implementation and derivation of the aforementioned objective, please refer to [1].\n\n[1] Ou Z, Xu T, Su Q, et al. \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022.\n\n[2] Tschiatschek S, Sahin A, Krause A. \"Differentiable submodular maximization.\" IJCAI, 2018.\n\n[3] Zhang D W, Burghouts G J, Snoek C G M. \"Set prediction without imposing structure as conditional density estimation.\" ICLR, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699951742381,
                "cdate": 1699951742381,
                "tmdate": 1699954988035,
                "mdate": 1699954988035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HfXtgwreK9",
                "forum": "eepoE7iLpL",
                "replyto": "Omfs3dIcbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to Reviewer miuk (part 2)"
                    },
                    "comment": {
                        "value": "### ***Comments 2: In equation 4, how do you divide a superset into several subsets?***\n***ANSWER:*** Theorem 3.5 and Eq.4 are general frameworks to establish the relationship between $Y$ and $(S,V)$. This approach **does not necessitate** dividing a superset into multiple subsets; instead, it requires processing only once for a specific pair of $(S,V).$ In the context of neural subset selection with the optimal supervision (OS) oracle, addressing the variational approximation employs Monte-Carlo (MC) sampling. For a given $V_i,$ we only generate $m$ subsets during training, consistently setting this number to $5$ across various tasks. Consequently, this **eliminates the need for an exponential number of combinations**. It is important to highlight that one of the foremost advantages of Neural Subset Selection in the context of OS Oracle is its ability to **significantly reduce the computational burden** associated with processing an exponential number of $(S,V).$\n\n### ***Comments 3: What is the number of learnable parameters for baselines and the proposed method?***\n***ANSWER:*** In regard to the parameters, we have already done ablation studies in **Table 3** and provided a discussion in **Section 4.4** of our paper. To further demonstrate that the improvements achieved by our method are **not merely due to additional parameters**, we present an additional table  here using the CeleA dataset. This table compares EquiVSet (v1) and EquiVSet (v2) \u2014 variants of EquiVSet where we have incorporated a Conv(32, 3, 2) layer and a Conv(64, 4, 2) layer into the EquiVSet backbone, respectively. Detailed descriptions of these backbones are available in Appendix E.2. Notably, despite having the largest number of parameters, EquiVSet (v2) is outperformed by INSET, indicating that **our method's efficacy is not solely parameter-dependent**.\n\n|  | DeepSet | Set-Transformer | EquiVSet | EquiVSet-v1  | EquiVSet-v2 | INSET\n|--|--|--|--|--|--|--|\n| Parameter |  651181 | 1288686 | 1782680 | 2045080 | **3421592** | 2162181\n|MJC| 0.440$\\pm$0.006| 0.527$\\pm$0.008 | 0.549$\\pm$0.005 | 0.554$\\pm$0.007 | 0.560$\\pm$0.005 | **0.580$\\pm$0.012**\n\nThank you in advance for dedicating your time and attention to our response. We are confident that the clarifications and additional information provided here comprehensively address your concerns. With this in mind, we respectfully and earnestly request that you re-evaluate our work, considering the explanations we have offered."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699954290689,
                "cdate": 1699954290689,
                "tmdate": 1699955011079,
                "mdate": 1699955011079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VHHAi48v3L",
                "forum": "eepoE7iLpL",
                "replyto": "HfXtgwreK9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
                ],
                "content": {
                    "title": {
                        "value": "Need more clarification on Comments 2"
                    },
                    "comment": {
                        "value": "Thanks for your response! I have one more question regarding Comments 2. By stating \"we only generate m\n subsets during training\", do you mean that during each training iteration, you randomly select m subsets?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988013828,
                "cdate": 1699988013828,
                "tmdate": 1699988013828,
                "mdate": 1699988013828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iHWiIIuFsv",
                "forum": "eepoE7iLpL",
                "replyto": "Omfs3dIcbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your prompt reply"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response! You are correct that during each training iteration, we randomly select $m$ subsets for each ground set $V$. Increasing the value of $m$ leads to an improvement in performance. To ensure a **fair comparison**, we adhere to EquiVSet's protocol by setting the sample number $m$ to 5 **across all tasks and datasets**. Even when varying the value of $m$, the results **consistently** demonstrate that INSET **significantly outperforms** EquiVSet. In the following table, we report the performance of EquiVSet by selecting the best results achieved after tuning the value of $m$ within the range of 1 to 10.\n\n| | EquiVSet | m=1 | m=2 |m=5 | m=7 |m=8 | m=10 \n|--|--|--|--|--|--|--|--|\n| Toys |  70.4$\\pm$0.004 | 75.2$\\pm$0.006 | 75.3$\\pm$0.005 | 76.9$\\pm$0.005 | 76.8$\\pm$0.003 |76.7$\\pm$0.003| **77.1$\\pm$0.004**\n| Gear| 74.5$\\pm$0.013 |78.8$\\pm$0.015|  77.5$\\pm$0.020| 80.8$\\pm$0.012 | 81.3$\\pm$0.010 | 82.1$\\pm$0.015 | **84.6$\\pm$0.011**\n\nThank you for your time. If you have any additional questions, we would be delighted to discuss them further."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011960419,
                "cdate": 1700011960419,
                "tmdate": 1700012082101,
                "mdate": 1700012082101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sIlnAhy7I9",
                "forum": "eepoE7iLpL",
                "replyto": "iHWiIIuFsv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Reviewer_miuk"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I am satisfied with the clarification and increased my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014561762,
                "cdate": 1700014561762,
                "tmdate": 1700014561762,
                "mdate": 1700014561762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xi9RToIk6l",
                "forum": "eepoE7iLpL",
                "replyto": "Omfs3dIcbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Huge thanks for your super quick reply and for raising the score!"
                    },
                    "title": {
                        "value": "Thanks for raising the score!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015512571,
                "cdate": 1700015512571,
                "tmdate": 1700090048958,
                "mdate": 1700090048958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "shUSKCqL1J",
                "forum": "eepoE7iLpL",
                "replyto": "Omfs3dIcbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised Manuscript Incorporating Your Suggestions."
                    },
                    "comment": {
                        "value": "Dear Reviewer miuk,\n\nWe would like to extend our heartfelt gratitude for your active engagement and valuable suggestions. Thanks to your insightful feedback, we have made some revisions to our manuscript.\n\nFirstly, we have incorporated the optimization objective and inference process into Appendix D.2, allowing for a more comprehensive understanding of our proposed approach. Additionally, we have included the extra experiments in Appendices F.2 and F.4, providing further supporting evidence for our findings. These revisions have been highlighted in purple for readers' convenience.\n\nWe greatly appreciate your continued support and acknowledgement of our response. Moreover, we are truly grateful for the time and consideration you have invested in reviewing our manuscript.\n\nSincerely,\n\nThe Authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710651994,
                "cdate": 1700710651994,
                "tmdate": 1700710651994,
                "mdate": 1700710651994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lN5nKGKcEZ",
            "forum": "eepoE7iLpL",
            "replyto": "eepoE7iLpL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_vcJR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_vcJR"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles neural subset selection. In particular, they tackle the issue that current methods do not consider the properties of the superset while constructing subsets. Their theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written.\n- The related work covers enough ground for a new researcher to understand a high level idea of this field.\n- The experiments include multiple baselines."
                },
                "weaknesses": {
                    "value": "- Lack of ablation studies.\n- The proposed method is not evaluated on a wide distribution of datasets.\n- Will similar findings hold if the dataset contains imbalance? If so, what degree of imbalance do the guarantees still hold?"
                },
                "questions": {
                    "value": "- Baselines do not consider the information from superset, but these baselines be improved by adding the invariant sufficient statistic of the superset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Reviewer_vcJR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9406/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699741480766,
            "cdate": 1699741480766,
            "tmdate": 1699741480766,
            "mdate": 1699741480766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "29tZ7OiyiO",
                "forum": "eepoE7iLpL",
                "replyto": "lN5nKGKcEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the time and effort you have invested. In response to your concerns, we have provided detailed clarifications and additional experimental results. For your convenience, these results are presented in tabular format. We will incorporate these results, along with details and plots into the appendix of the revised version.\n\n### ***Comments 1: Lack of ablation studies***\n\n***ANSWER***: Thank you for highlighting the absence of ablation studies in our work. Indeed, our method, INSET, **does not introduce any new hyperparameters** to the EquiVSet [1] framework. We use the **exact same** hyperparameters as EquiVSet in all of our experiments, ensuring a **fair comparison**. Meanwhile, INSET can significantly outperform baseline models across various datasets and tasks, demonstrating its **substantial efficacy**.\n\nTo further verify the robustness of INSET, we have now conducted ablation studies focusing on the Monte-Carlo (MC) sample numbers for each input pair $\\{(V_i, S_i^*)\\}$. In the context of neural subset selection tasks, our primary aim is to train the model  $\\theta$ to predict the optimal subset $S^*$  from a given ground set $V$.  During training, we sample m subsets from $V$ to optimize our model parameters  $\u03b8$, thereby maximizing the conditional probability distribution $p_\\theta (S^* | V)$ among of all pairs of $(S,V)$ for for a given V. In our main experiments, we adhere to EquiVSet's protocol by setting the sample number $m$ to 5 across all the tasks. Even with varying the value of $m$, the results **consistently** demonstrate that INSET **significantly outperforms** EquiVSet. Please note that the performance of EquiVSet is reported by **selecting the best results** achieved after tuning the value of $m$.\n\n| | EquiVSet | m=1 | m=2 |m=5 | m=7 |m=8 | m=10 \n|--|--|--|--|--|--|--|--|\n| Toys |  0.704$\\pm$0.004 | 0.752$\\pm$0.006 | 0.753$\\pm$0.005 | 0.769$\\pm$0.005 | 0.768$\\pm$0.003 | 0.767$\\pm$0.003| **0.771$\\pm$0.004**\n| Gear| 0.745$\\pm$0.013 | 0.788$\\pm$0.015|  0.775$\\pm$0.020| 0.808$\\pm$0.012 | 0.813$\\pm$0.010 | 0.821$\\pm$0.015 | **0.846$\\pm$0.011**\n| Bath| 0.820$\\pm$0.005 | 0.821$\\pm$0.010|  0.851$\\pm$0.008| 0.862$\\pm$0.005| 0.874$\\pm$0.006 | 0.861$\\pm$0.005 | **0.874$\\pm$0.003**\n| Health| 72.0$\\pm$0.010 | 0.749$\\pm$0.015|  0.763$\\pm$0.012| 0.812$\\pm$0.005 | **0.824$\\pm$0.008** | 0.808$\\pm$0.005 | 0.811$\\pm$0.005\n\n###  ***Comments 2: The proposed method is not evaluated on a wide distribution of datasets.***\n\n***ANSWER***: It is important to clarify that our experiments encompass three tasks: product recommendation, set anomaly detection, and compound selection, which involve the processing of tabular data, images, and 3D Cartesian coordinates. Specifically, we conduct extensive experiments on **these tasks using six datasets**. Notably, for the product recommendation task, the datasets consist of 12 categories, effectively representing **12 sub-datasets**.\n\nMoreover, we have also conducted synthetic experiments in Appendix F.1 to assess INSET's capability to learn complex set functions.  Additionally, to provide further evidence of INSET's effectiveness, we have performed set anomaly detection tasks using the CIFAR-10 dataset. We are also incorporating additional filters for compound selection tasks for a wider distribution of datasets. For more information, please refer to Appendix F.3 and F.5 in the revised submission.\"\n\n|  | Random | PGM | DeepSet | Set-Transformer | EquiVSet | INSET\n| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| CIFAR-10 | 0.193 |  0.450$\\pm$0.020 | 0.316$\\pm$0.008 | 0.654$\\pm$0.023 | 0.603$\\pm$0.012 | **0.742$\\pm$0.020**\n| PDBBind | 0.073 | 0.350$\\pm$0.009 | 0.323$\\pm$ 0.004 | 0.355$\\pm$0.010 | 0.357$\\pm$0.005 | **0.371$\\pm$0.010**\n| BindingDB | 0.027 | 0.176$\\pm$0.006 | 0.165$\\pm$0.005 | 0.183$\\pm$0.004 | 0.188$\\pm$0.006 | **0.198$\\pm$0.005**\n\nThe latest results provide further evidence of INSET's superior performance compared to the baselines. Furthermore, it is worth mentioning that our experimental setup includes a **significantly larger number** of experiments compared to DeepSet (Sec. 4.3) [2] and PGM (Sec. 5.3) [3].\n\n[1] Ou Z, Xu T, Su Q, et al. \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022.\n\n[2] Zaheer M, Kottur S, Ravanbakhsh S, et al. \"Deep Sets.\" NeurIPS, 2017.\n\n[3] Tschiatschek S, Sahin A, Krause A. \"Differentiable submodular maximization.\" IJCAI, 2018."
                    },
                    "title": {
                        "value": "Answers to Reviewer vcJR (part 1)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699941991641,
                "cdate": 1699941991641,
                "tmdate": 1700384082828,
                "mdate": 1700384082828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2kWyNTwmG6",
                "forum": "eepoE7iLpL",
                "replyto": "lN5nKGKcEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "###  ***Comments 3: Will similar findings hold if the dataset contains imbalance? What degree of imbalance do the guarantees still hold?***\n\n***ANSWER***: INSET is designed to significantly enhance the models' capacity to effectively learn $P(Y|S,V)$ or $F(S,V)$. According to Theorem 3.5, this enhancement **holds true consistently** when the tasks involve modeling the relationship between (S,V) and $Y$. To provide empirical evidence, we conduct additional experiments that demonstrate INSET's consistent superiority over the baselines, even in scenarios with imbalanced ground set sizes. Specifically, we train the model on the two-moons datasets (for detailed information, please refer to Appendix F.1) using fixed ground set sizes of 100, and evaluate its performance on various data sizes ranging from 200 to 1000.\n\n|  | 200  | 400 | 600 | 800| 1000 |\n|--|--|--|--|--|--|\n|EquiVSet | 0.538 $\\pm$ 0.002  | 0.513 $\\pm$ 0.003 | 0.482 $\\pm$ 0.002 | 0.473  $\\pm$ 0.005 | 0.471 $\\pm$ 0.003\n|INSET |  0.547 $\\pm$ 0.002 | 0.518 $\\pm$ 0.005 | 0.502 $\\pm$ 0.003 | 0.486 $\\pm$ 0.002 | 0.485 $\\pm$ 0.002\n\nThe results clearly show that INSET **consistently enhances** the performance of EquiVSet, regardless of any imbalances.\n\n\n### ***Comments 4: Can baselines be improved by adding the invariant sufficient statistic of the superset?***\n\n***ANSWER:*** Certainly, Theorem 3.5 offers a comprehensive framework for modeling the relationship between $Y$ and $(S,V)$, which is also **applicable to the baselines**. However, integrating this invariant sufficient statistic directly into DeepSet and Set-Transformer presents challenges, as they do not explicitly learn a neural subset function $F(S,V)$. Our method, INSET, has employed DeepSet as its backbone. To demonstrate that Set-Transformer can also derive benefits from INSET, we utilize Set-Transformer as our backbone to showcase this.\n\n|  | Random  |  Set-Transformer | Set-Transformer + INSET\n|--|--|--|--|\n| Toys |0.083 | 0.625 $\\pm$ 0.020 | **0.769 $\\pm$ 0.010**|\n| Gear | 0.077 | 0.647 $\\pm$ 0.006| **0.825 $\\pm$ 0.021** |\n| Carseats | 0.066 | 0.220 $\\pm$ 0.010| **0.230 $\\pm$ 0.031**|\n| Bath | 0.076 | 0.716 $\\pm$ 0.005| **0.862 $\\pm$ 0.005**|\n| Health |0.076  |0.690 $\\pm$ 0.010 | **0.852 $\\pm$ 0.009** |\n| Diaper | 0.084 |0.789 $\\pm$ 0.005 | **0.896 $\\pm$ 0.005** |\n| Bedding | 0.079 | 0.760 $\\pm$ 0.020| **0.885 $\\pm$ 0.013** |\n| Feeding | 0.093 | 0.753 $\\pm$ 0.006| **0.902 $\\pm$ 0.004** |\n\nBy employing Set-Transformer as our backbone, we enable it to explicitly learn the relationship between $Y$ and $(S,V)$. The empirical results clearly demonstrate a **significant improvement** in performance as a result.\n\nThank you for your time and thoughtful consideration. If you have any concerns or questions, please don't hesitate to reach out to us."
                    },
                    "title": {
                        "value": "Answers to Reviewer vcJR (part 2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943444867,
                "cdate": 1699943444867,
                "tmdate": 1700384113185,
                "mdate": 1700384113185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jZucIfWVeG",
                "forum": "eepoE7iLpL",
                "replyto": "lN5nKGKcEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of our response"
                    },
                    "comment": {
                        "value": "Dear Reviewer vcJR:\n\nThank you again for your valuable time and efforts in reviewing our manuscript. Since our previous response was a bit long, we provide a summary below:\n\n- **Ablation studies:** We have clarified that our method INSET does not introduce new hyper-parameters. Additionally, we have included an ablation study focusing on the number of Monte-Carlo (MC) samples. Detailed responses to these points are available in our feedback to Comment 1.\n\n- **Datasets:** We have elaborated on the usage of our datasets, encompassing 3 tasks across 6 datasets in three different modalities. Besides, we have also presented more experiments in our feedback to Comment 2.\n\n- **Questions:** We also provide new experiments to answer your thoughtful questions on the imbalance and baseline.\n\nOur method not only demonstrates an impressive empirical performance, with up to a 23% improvement over the best baselines, but it is also underpinned by rigorous theoretical analysis and a strong foundational concept. We are also grateful for your recognition of our work\u2019s Soundness, Presentation, and Contribution as being satisfactory. \n\nConsidering these aspects, we respectfully and kindly invite you to re-evaluate the rating of our submission. We eagerly anticipate any further feedback from you."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104183503,
                "cdate": 1700104183503,
                "tmdate": 1700275671129,
                "mdate": 1700275671129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y0KuvCEkeD",
                "forum": "eepoE7iLpL",
                "replyto": "lN5nKGKcEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update Appendix in Response to Your Concerns"
                    },
                    "comment": {
                        "value": "Dear Reviewer vcJR,\n\nThanks for your time and consideration. We have revised the Appendix of our manuscript to address your concerns. Regarding your comments on ablation studies, we have conducted additional experiments, detailed in Appendix F.4, and in the first answer of our initial [response](https://openreview.net/forum?id=eepoE7iLpL&noteId=29tZ7OiyiO). Concerning the distribution of datasets, we invite you to review Appendix F.3 and F.5, along with the second answer in our initial [response](https://openreview.net/forum?id=eepoE7iLpL&noteId=29tZ7OiyiO). For your insightful questions, please refer to the second part of our initial [response](https://openreview.net/forum?id=eepoE7iLpL&noteId=2kWyNTwmG6). A summary of our previous response can be found in the [paragraph](https://openreview.net/forum?id=eepoE7iLpL&noteId=jZucIfWVeG). We would appreciate knowing if you have any additional feedback or suggestions.\n\nSincerely,\n\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408388298,
                "cdate": 1700408388298,
                "tmdate": 1700408388298,
                "mdate": 1700408388298,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ghE2kdXE3e",
                "forum": "eepoE7iLpL",
                "replyto": "lN5nKGKcEZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder of the Revision Deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer vcJR,\n\nThank you once again for your time! We understand that you have a busy schedule, and we kindly remind you that the revision deadline is approaching. If you have any suggestions or feedback on how we can improve our manuscript, we would greatly appreciate your input. We eagerly await your response.\n\nSincerely,\n\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664488650,
                "cdate": 1700664488650,
                "tmdate": 1700664488650,
                "mdate": 1700664488650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oQLbqcGMhF",
            "forum": "eepoE7iLpL",
            "replyto": "eepoE7iLpL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_PoVi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9406/Reviewer_PoVi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an optimal subset selection method based on neural networks, which is designed to learn a permutation invariant representation of both the subset of interest $S$ and the ground superset $V$. The authors highlight that prior works for neural subset selection (e.g., DeepSet) do not account for the superset $V$, and both theoretically and empirically demonstrate that jointly modeling the interactions between $S$ and $V$ leads to improved performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The writing is generally easy to follow, and the paper includes a sufficiently comprehensive discussion of relevant prior works. Experimental results are presented well.\n- The proposed method achieves strong empirical performance in terms of mean Jaccard coefficient (often with a fairly large gap) when compared against several optimal subset selection baselines (e.g., DeepSet, EquiVSet)."
                },
                "weaknesses": {
                    "value": "- The presentation of some of the mathematical details needs improvement. In particular, it seems that some of the notations are overloaded (i.e., the same notation is used with different interpretations) or not clearly defined. For example, the notation $S$ appears as a *subset* of the ground set $V$ in the Introduction, but in Section 3.1 (Background), the notation $S$ appears as an *element* of $V$ that takes a matrix form. The relationship between elements $x_i \\in \\mathcal{X}$ and $S_i$ is not clearly defined either. On another note, it is not entirely clear to me what the function value $Y \\in \\mathcal{Y}$ is really referring to, which also appears without an explicit discussion of its meaning in the Introduction as part of the variational distribution $q(Y|S,V)$. Is $Y \\in \\mathcal{Y}$ supposed to be the utility function value (which was also introduced with the notation $U = F_{\\theta}(S,V)$ in the Introduction)? The confusion arising from notational ambiguity makes the paper less readable."
                },
                "questions": {
                    "value": "- Can the authors clearly define what $Y$ is? The footnote mentions that $Y_i$ is the \"probability of element $i$ being selected\", but this description is ambiguous.\n- It looks like learning the neural network approximation in Eq. (4) is done via variational inference as in Ou et al. (2022). As I am not familiar with the cited work, it is unclear to me how $q(Y|S,V)$ is serves as an approximation for the subset likelihood $p(S|V)$ when the former is a distribution over $Y$ and the latter is a distribution over $S$. Can the authors provide clarifications on this?\n- How is the neural network construction in Eq. (4) explicitly related to $p_{\\theta}(S,V)$ (or $F_{\\theta}(S,V)$)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9406/Reviewer_PoVi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9406/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700549624075,
            "cdate": 1700549624075,
            "tmdate": 1700549624075,
            "mdate": 1700549624075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NCrlTHrYB8",
                "forum": "eepoE7iLpL",
                "replyto": "oQLbqcGMhF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to Reviewer PoVi"
                    },
                    "comment": {
                        "value": "We greatly appreciate the time and effort you have invested! In response to your concerns, we have provided clarifications here. We will also incorporate these clarifications into our revised version to enhance clarity.\n\n### ***Comments 1: Relationship Between $x_i, S_i,$ and $V.$***\n***ANSWER:*** We regard $V$ as a set composed of $n$ elements, denoted as $x_i$, i.e., $V=\\\\\\{x_1, x_2,..., x_n\\\\\\}$. In order to facilitate the proposition of Property 1, we describe $V$ as a collection of several disjoint subsets, specifically $V = \\\\\\{S_1, \\dots, S_m\\\\\\}$, where $S_i \\in \\mathbb{R}^{n_i \\times d}$. Here, $n_i$ represents the size of subset $S_i \\subset V$, that is, $S_i = \\\\\\{x_{1_i}, x_{2_i},..., x_{n_i}\\\\\\}.$\n\n### ***Comments 2: The definition of $Y.$***\n***ANSWER:*** The generality of our Theorem 3.1 allows it to be applied to both U=F(Y|S,V) and P(Y|S,V) for different tasks. Specifically, when considering the task of Neural Subset Selection in Optimal Subset (OS) oracles, which involves learning P(Y|S,V), we define Y as a $|V|$ independent Bernoulli distribution, which is parameterized by $Y \\in [0,1]^{|V|}$, representing the odds or probabilities of selecting element $x_i \\in V$ in a output subset $S$.\n\n### ***Comments 3: Why can $q(Y|S,V)$ serve as a variational approximation to $P(S|V)?$***\n***ANSWER:*** As discussed in the previous answer, $Y \\in [0,1]^{|V|}$. In practice, $S$ is represented as a binary vector (mask), denoted as $S := \\\\\\{0,1\\\\\\}^{|V|}$, where the $i$-th element is equal to $1$ if $i \\in S$ and $0$ otherwise. Therefore, it is natural to use $q(Y|S,V)$ to represent the variational distribution of $P(S|V)$.\n\n### ***Comments 4: How is the neural network construction in Eq. (4) explicitly related to $p_\\theta(S,V)$ or $F_\\theta(S,V).$***\n***ANSWER:*** Once neural networks are trained, their outputs become fixed for a given input, such as (S,V). Thus, Eq. (4) represents the explicit structure used to construct models for learning the deterministic function $\\theta(S,V)$ (to differentiate it from the utility function U=F(S,V)). Using this function, we can construct the conditional distribution $q(Y|S,V)$ according to Theorem 3.5. Specifically, we employ the Mean-Field Variational Inference (MFVI) method introduced by [1] (Section 3.2) to approximate the distribution $q(Y|S,V)$, referred to as $\\psi$ in [1]. \n\nTo prevent overwhelming readers with an abundance of notations and equations, we have deliberately omitted the detailed construction of q(Y|S,V) and the derivation of variational approximation in our paper. This decision was motivated by two factors. Firstly, our theorem and Eq. 4 offer a general framework for modeling the relationship between $Y$ and $(S,V)$, instead of focusing on the neural subset selection tasks. Secondly, in order to ensure clarity of our motivation, we have provided a high-level description of these concepts in the Introduction section. For readers interested in the details of these concepts, we strongly recommend referring to [1] (Section 3) for a more comprehensive understanding. For the implementation details of q(Y|S,V) and $\\theta(S,V)$, we suggest consulting our accompanying code located at (./model/modules.py).\n\nThanks for your time and suggestions again. We would appreciate knowing if you have any additional feedback or suggestions.\n\n[1] Ou Z, Xu T, Su Q, et al., \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585160366,
                "cdate": 1700585160366,
                "tmdate": 1700708444785,
                "mdate": 1700708444785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Jy1rhSj2D",
                "forum": "eepoE7iLpL",
                "replyto": "oQLbqcGMhF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revision of Manuscript Incorporating Your Suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer PoVi,\n\nWe sincerely appreciate your reviews and valuable suggestions. Taking into account your feedback, we have made refinements to the footnote in the Introduction Section and enhanced the description of $V$ and $S$ in Section 3.1. These revisions, highlighted in purple, will significantly enhance the clarity of our paper. Thank you once again for your time and contribution.\n\nBest regards,\n\nThe Authors."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710079348,
                "cdate": 1700710079348,
                "tmdate": 1700710079348,
                "mdate": 1700710079348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]