[
    {
        "title": "Stochastic Competition Networks for Deep Learning on Tabular Data"
    },
    {
        "review": {
            "id": "XTUC86exmz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_fLPM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_fLPM"
            ],
            "forum": "f8Jdtbey3B",
            "replyto": "f8Jdtbey3B",
            "content": {
                "summary": {
                    "value": "**Scope:** machine learning problems on tabular data (e.g. classification, regression).\n\n**Contribution:** *STab* -- a Transformer-like architecture for tabular data problems. There are four architectural elements that make STab different from a vanilla Transformer:\n1. *LWTA* (\"Local Winner Takes All\") -- a stochastic activation used instead of the ReLU activation in feed-forward blocks.\n2. *\"Embedding mixture layer\"* -- a LWTA-based embedding layer for scalar continuous features.\n3. *A trainable bias* (shape `N_features x N_features`) is added to attention maps in all heads of all attention blocks.\n4. *\"Parallel module\"* -- a LWTA-based feed-forward module in each Transformer block that that runs in parallel with the main block and enriches only the CLS embedding.\n\n**The main claim:** *\"we yield state-of-the-art performance and mark a significant advancement in applying deep learning to tabular data\"*."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The story is easy to follow.\n- Developing better tabular DL architectures is an interesting research direction.\n- Specifically, I appreciate that the paper aims at developing a variation of Transformer architecture that would be more tailored specifically for tabular data problems instead of being copied as-is from other fields, which can be a suboptimal approach.\n- In particular, the bias term in the attention module is indeed such an element that is motivated by the nature of tabular data.\n- I appreciate that the code is available, I managed to launch it"
                },
                "weaknesses": {
                    "value": "(1) Unfortunately, in my opinion, **the novelty is limited:**\n- The LWTA \"activation\" is not a new element as such (in general, applying existing techniques is fine if well-motivated or supported by strong empirical performance).\n- In particular, the \"stochastic feature embeddings\" is a `Linear-Activation-Linear` embeddings with LWTA used as `Activation`, which makes the proposed module very similar to the embeddings schemes analysed in \"On Embeddings for Numerical Features in Tabular Deep Learning\" by Gorishniy et al.\n- Although I like the idea of adding a bias to the attention map, I should say that it was previously explored in other fields (e.g. see \"Position Information in Transformers: An Overview\" by Dufter et al.).\n- The \"parallel module\" is technically new, but, subjectively, this is an incremental addition which is not enough to support the overall novelty.\n\n(2) In my opinion, **the proposed architectural elements need more motivation** (*specific* observations/analysis/experiments/theory/citations; heuristic claims are fine, but, in my opinion, they are not enough). The world of deep learning offers many techniques that can be used to slightly improve the performance of existing tabular models. Then, the question arises: why specifically for tabular data problems should we use, for example, LWTA/\"parallel module\"/etc.? Without answering this question, it can be hard to break the inertia of using simple well-established elements.\n\n(3) Subjectively, **the reported results are not fully in line with the main claim, and they do not compensate for other issues ((1), (2), and (4))**. Specifically, I refer to Table 2, Table 3, the reported margins, the number of wins, the benchmark size and the claim *\"we yield state-of-the-art performance and mark a significant advancement in applying deep learning to tabular data\"*.\n\n(4) **The paper adds new non-trivial complexity** to a well-established Transformer architecture, which is not a problem as such, but, in my opinion, the previous issues should be resolved to motivate the community to use a new non-trivial architecture.\n\n(5) Other smaller things in no particular order:\n- In my opinion, some parts of the story could be more compact or could be moved to the appendix. For example, Table 4. Similarly, Section 4.1 could be more compact. Perhaps, in the introduction, the nature of tabular data could be also explained in a more compact way.\n- I recommend increasing the resolution of the illustrations.\n- I recommend supporting the story in the introduction with more citations.\n- (Code) `STab/mainmodel.py`: there is a (currently, non-critical) bug on line 72: it should be `LWTA(U=U)`. I recommend tweaking the code editor to make it highlight such typing-related issues.\n- (Code) I tried launching `TrainAD.ipynb` but obtained suboptimal results. I appreciate that the code is marked as \"experimental\", but I am reporting this just in case. It seems that the number of epochs is too low.\n- I recommend proof-reading the paper for English style, vocabulary and grammar issues.\n- I see the place where the \"TDM\" abbreviation was introduced, but overall, this is the first time I see this abbreviation (this can be my fault though). Perhaps, it can be avoided."
                },
                "questions": {
                    "value": "How negative values in $\\eta$ are handled in Equation 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Reviewer_fLPM"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5805/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697325986363,
            "cdate": 1697325986363,
            "tmdate": 1699636611733,
            "mdate": 1699636611733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iJtHuN0E6m",
            "forum": "f8Jdtbey3B",
            "replyto": "f8Jdtbey3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_Nj1J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_Nj1J"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an interesting transformer architecture, that features 3 extra components. The first is a change for the linear layer in the transformer layer/block that is subtituted with an LWTA layer, which I would phrase as a \"guided dropout\". In the same way, a similar procedure is applied before feeding the data to the transformer, where LWTA is applied but in this case with layers and not distinct blocks inside a layer. Lastly, a parallel module is added to the transformer layer/block which projects the feature embeddings and then aggregates them, adding them to the output of the transformer layer.\n\nThe authors compare the proposed method against different deep learning baselines and tree-based methods on 8 diverse datasets (proposed from prior work) covering binary/multiclass classification and regression."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is written well and it has a good structure.\n- An ablation is performed for one of the proposed components."
                },
                "weaknesses": {
                    "value": "- **Until the close of the previous decade, deep learning methodologies for Tabular Data predominantly\ncentered around multi-layer perceptrons and similar rudimentary architectures. However, the recent\nyears have witnessed a surge in sophisticated neural network designs, yielding remarkable results.**\n\n    I would argue against the above paragraph, Kadra et al. (2021) [1] have shown that tabular resnets\n    when carefully regularized manage to outperform specialized deep-learning architectures and tree-based methods.\n    An outcome that is further verified by Shwartz-Ziv et al. (2022) and Gorishniy et al. (2021) [2][3].\n\n    In this regard, the related work is not extensive and can be further improved.\n- I could not find an ablation of the LWTA instead of the linear layer as a component for the transformer layer. There additionally is no ablation regarding the proposed parallel module that projects the feature embeddings and aggregates them, finally summing them with the transformer layer.\n- The authors claim state-of-the-art results with only 8 datasets. \n- The authors reuse results for the baselines, which can potentially propagate failures in the setup of competitor methods, depending on where the results were taken. At the same time, it is not clearly indicated which results were taken from where. \n- It is not clear whether the authors use default hyperparameters for the baselines or tuned hyperparameters by reading the core manuscript. For the proposed methods the hyperparameters are tuned, however no information is given on what procedure was used and how much time it took.\n- No information is provided regarding the runtime of the proposed method or the competitor baselines.\n\n[1] Kadra, Arlind, et al. \"Well-tuned simple nets excel on tabular datasets.\" Advances in neural information processing systems 34 (2021): 23928-23941.\n\n[2] Shwartz-Ziv, Ravid, and Amitai Armon. \"Tabular data: Deep learning is not all you need.\" Information Fusion 81 (2022): 84-90.\n\n[3] Gorishniy, Yury, et al. \"Revisiting deep learning models for tabular data.\" Advances in Neural Information Processing Systems 34 (2021): 18932-18943."
                },
                "questions": {
                    "value": "- Should the numerator in Equation 2 not go through a specific $u$ value, it is slightly confusing as $k$ is used for the distinct blocks, however, $\\xi_k$ represents the one hot vector inside a block of $u$ units if I am not mistaken.\n\n- **For established benchmarks, we cite results from\nexisting literature as provided in Gorishniy et al. (2021); Rubachev et al. (2022); Gorishniy et al.\n(2023), or Somepalli et al. (2021). This approach not only conserves computational resources but\nalso ensures impartiality through third-party verification of performance metrics.**\n\n    Could the authors describe for which baselines were the results reused? I think it would be beneficial that all methods be run by the authors since in many cases, it can happen that competitor baselines can be run in the wrong way. \n\n- **In Table 4, we list the main hyper-parameters of the proposed model for each dataset, corresponding to the experimental results presented.**\n\n    Pending on my previous question, I assume the authors at least partially (if not for all) have not performed hyperparameter tuning for the considered baselines (since they reuse results). In contrast, they have performed hyperparameter tuning for their method.\n\n- Could the authors provide an ablation of all the proposed components? It would be nice if it could be provided for all datasets and as distribution plots for the different values over the datasets, that way it would be easier to observe the differences.\n- Could the authors provide the ranks that all methods achieve over the datasets to aggregate the results?\n- It would be interesting to compare the runtimes of the different methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5805/Reviewer_Nj1J"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5805/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698326891553,
            "cdate": 1698326891553,
            "tmdate": 1699636611603,
            "mdate": 1699636611603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Sc7uoAnC1x",
            "forum": "f8Jdtbey3B",
            "replyto": "f8Jdtbey3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_Fv9b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_Fv9b"
            ],
            "content": {
                "summary": {
                    "value": "This research addresses the underexplored use of deep learning in handling tabular data, a prevalent and significant data format in various industries. Historically, techniques like gradient-boosted decision trees (GBDT) have dominated this field. However, recent models are narrowing this gap, surpassing GBDT in different scenarios and gaining increased attention. The study introduces a novel deep learning model designed specifically for tabular data, using a Transformer-based architecture that has been modified to accommodate the unique characteristics of tabular data. These modifications include incorporating elements like \"Local Winner Takes All\" mechanisms and a novel Mixture Embedding layer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It seems to me that the Mixture Embedding layer is new."
                },
                "weaknesses": {
                    "value": "Four runs were conducted, but standard deviation data is not available. The AUC is also considered a relevant metric. Overall, I don't see the justification for using more complex models with significant resource requirements over a less interpretable model like XGBoost. The figures have low quality; please consider using 'TikZ' or 'tikzpicture' in LaTeX for better quality figures."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5805/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571219790,
            "cdate": 1698571219790,
            "tmdate": 1699636611484,
            "mdate": 1699636611484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "owRtYNJVCo",
            "forum": "f8Jdtbey3B",
            "replyto": "f8Jdtbey3B",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_tizg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5805/Reviewer_tizg"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose an approach to apply a vision transformer architecture to tabular data. More specifically, the authors introduce in a transformer-based architecture three mechanisms. The first is the \"Local winner takes all\" previously proposed by Panousis et al., which applies a heavy non-linearity to select the firing output in parallel branches and smoothed through the Gumbel softmax. Then, an embedding mixture layer is introduced, and finally, the hybrid transformer module is assembled. The experimental benchmarks are inspired by similar works in the literature."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Deep learning-based techniques for tabular data are not very popular, and this work goes in this direction\n- the paper is clear in most of its parts"
                },
                "weaknesses": {
                    "value": "- the ablation study is incomplete\n- the confidence intervals in the main results are missing- this questions the superiority of the proposed approach\n- the model and the training complexity are in general not evaluated when results are presented\n- some parts are not very clear and require some interpretation to be understood (Sec. 3.3)"
                },
                "questions": {
                    "value": "- can you provide the ablation where you show that every introduced component is clearly improving the performance of a vanilla transformer-based approach? The current ablation study does it just partially\n- can you provide standard deviations for the main results?\n- can you evaluate the trade-off between training/inference complexity and final performance? it feels that the proposed approach has longer training/inference time and occupies much more space, for a marginal improvement"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5805/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772971693,
            "cdate": 1698772971693,
            "tmdate": 1699636611388,
            "mdate": 1699636611388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]