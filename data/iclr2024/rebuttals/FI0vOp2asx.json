[
    {
        "title": "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging"
    },
    {
        "review": {
            "id": "gCrhye8Rvg",
            "forum": "FI0vOp2asx",
            "replyto": "FI0vOp2asx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_bAjN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_bAjN"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the robustness, efficiency, and accuracy of current snapshot compressed imaging reconstruction networks. The major contribution of the paper is designing a prompt network which automates the process of aligning a measurement based on its corresponding measurement model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles an interesting distribution shift, that is a shift in the measurement model of the compressed sensing task.\n\nThe paper is overall well-written (although in some parts difficult to read).\n\nThe idea of the prompt network to tackle distribution shifts is very interesting. If I understood correctly, without a prompt network, fine-tuning is needed for new measurement models. Yet with the prompt network the process of measurement alignment with the measurement model is automated for any measurement model.\n\nThe experiments are interesting and carefully designed in the sense that reasonable baselines and datasets are chosen for evaluation."
                },
                "weaknesses": {
                    "value": "The major contribution of the paper is not well-justified. i.e., the crucial need for the proposed method (as opposed to training from the scratch for every new measurement model) is not well-supported. e.g., the reviewer still finds it very convenient to train a model for every new sets of measurement models for a new organization of interest from a practical perspective. i.e., all it takes is a few hours (days) of training for the new set (note that this automatically addresses the other concern raised by the authors regarding privacy constraints, in that each organization has access to its own data and device sets).\n\nThe comparisons are not fair in Fig. 1 (also please see my question regarding Fig. 1 below). Clearly, joint training should serve as an upper bound on the performance when the test set contains the same measurement models as training. \n\nThe results (especially the quantitative ones) do not yield the conclusion that FedAVG is outperformed by FedHP. The major advantage of FedHP seems to be its 4x more efficient training time compared to FedAVG. This is fine and improving the efficiency is valuable from a practical point of view, but the paper isn\u2019t oriented around this conclusion; the paper emphasizes the value of prompt networks and FedHP in the form of accuracy and robustness gains, whereas FedAVG enjoys those traits, too!\n\nMinor:\nOn Tab. 2, FedHP is highlighted as the best-performing method in terms of SSIM (0.8481), whereas FedAVG should be highlighted (0.8496)."
                },
                "questions": {
                    "value": "How\u2019s Fig. 1 obtained? Is it evaluated on the same measurement model set used during training of each setup? Or are all models evaluated on the same predefined test set of measurement models?\n\nAs mentioned in the strengths section, the idea of the prompt network is interesting. However, the biggest question raised is whether that network induces another source of instability to the overall model. Specifically, what guarantees that the prompt network doesn\u2019t do a terrible alignment for measurement models deviating from the training distribution?\n\nWhat is the source of inconsistency between Tab. 1 and Fig. 3? What we see in Fig. 3 flags for a higher PSNR difference than 0.14 dB between FedAVG and FedHP\u2026 It\u2019s understandable to argue that quantitative metrics such as PSNR or SSIM don\u2019t perfectly capture the true quality, but the visual difference on Fig. 3. is too large not to be captured by those metrics.\n\nWhy isn\u2019t the deep unfolding network included in all the results and only reported as a short paragraph at the end? \n\nAny intuition on why FedAVG is so much slower to train than FedHP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697641956905,
            "cdate": 1697641956905,
            "tmdate": 1699637162684,
            "mdate": 1699637162684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uvCgRuZ4TB",
                "forum": "FI0vOp2asx",
                "replyto": "gCrhye8Rvg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate that Reviewer bAjN finds our method tackles an interesting problem, proposes an interesting prompt network, experiment is carefully designed with reasonable baselines!\n\n`Weakness 1`: The major contribution of the paper is not well-justified. i.e., the crucial need for the proposed method (as opposed to training from scratch for every new measurement model) is not well-supported. e.g., the reviewer still finds it very convenient to train a model for every new set of measurement models for a new organization of interest from a practical perspective. i.e., all it takes is a few hours (days) of training for the new set (note that this automatically addresses the other concern raised by the authors regarding privacy constraints, in that each organization has access to its own data and device sets).\n\n`A1`: Thanks for this useful question! \n\nThe conventional setting where each organization employs its local data and device to perform independent training has practical limitations for the current SCI research study and prevents its further real-world applications due to the following reasons.\n* Data-starving challenge of the client. Some clients may have limited training data (e.g., only several scenes) that the training cannot converge.\n* Efficiency concern. We find training a local model from scratch takes $3.54$ days ($10.62/3$ as shown in Table 3) on our platform. It only takes less than $1$ day ($2.86/3$ in Table 3) to adapt the model to a new device/client using FedHP.  We have put this illustration into the manuscript. The training time can be even longer if the reconstruction backbone becomes larger. \n\nIn summary, considering the data-starving nature of the client, it may be impractical for a new client to train a model. Besides, there is a training efficiency concern when there is enough data for the client. Plus, the reconstruction model trained with a single well-calibrated hardware instance is hard to adapt to new hardware. This work proposed FedHP to practically enable the deployment of reconstruction models on new clients. \n\n`Weakness 2`: The comparisons are not fair in Fig. 1 (also please see my question regarding Fig. 1 below). Clearly, joint training should serve as an upper bound on the performance when the test set contains the same measurement models as training.\n\n`A2`:  We appreciate the reviewer\u2019s valuable insight in improving this work!\n\nIn general, joint training serves as an upper bound for federated learning when the test set contains the same measurement models as training. \n\nHowever, in Fig. 1, the test set consists of random measurement models that are unseen for training. Specifically, all results (1 to 5) are evaluated by sampling from the same mask pool to test the trained models. Note that this mask pool contains masks from three different distributions, i.e., $P_1$, $P_2$, $P_3$ as we plotted in Fig. 1. Since we always sample non-overlapped masks, the masks used for testing are generally unseen to the models. This is quite challenging and can well simulate a real-world scenario.  We have added more illustrations about the settings to the caption of Fig. 1.\nIn this case, our experiments find that simply combining all data from different hardware to jointly train does not work well (as shown in Fig. 1, over $0.6$dB lower than FedHP and $0.3$dB lower than FedAvg).  \n\nWe appreciate the reviewer's help in distinguishing this work from general federated learning endeavors!  \n\n`Weakness 3`: The results (especially the quantitative ones) do not yield the conclusion that FedAVG is outperformed by FedHP. The major advantage of FedHP seems to be its 4x more efficient training time compared to FedAVG. This is fine and improving the efficiency is valuable from a practical point of view, but the paper isn\u2019t oriented around this conclusion; the paper emphasizes the value of prompt networks and FedHP in the form of accuracy and robustness gains, whereas FedAVG enjoys those traits, too!\n\n`A3`: We thank the reviewer bAjN for helping us summarize this contribution! \n\nWe find that FedAvg serves as a very strong baseline by even working better than recent  FL methods, such as SCAFFOLD and FedProx. Thus, it is non-trivial to achieve further performance boost. By comparison, FedHP can bring a consistent performance boost. For example, $+0.14$dB in Table 1, $+0.35$dB in Table 2, and $+0.27$dB in Table 4 (a) with more clients. \n\nBesides, by jointly introducing the hardware prompt network and adapter, the proposed method also benefits from the efficiency advantage by only requiring adapting the pre-trained models in the system to the new clients. We have also emphasized this point by mentioning the advantage of efficiency in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524766232,
                "cdate": 1700524766232,
                "tmdate": 1700585460424,
                "mdate": 1700585460424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoS1EfGF2V",
                "forum": "FI0vOp2asx",
                "replyto": "gCrhye8Rvg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors (Part 2)"
                    },
                    "comment": {
                        "value": "`Weakness 4`: Minor: On Tab. 2, FedHP is highlighted as the best-performing method in terms of SSIM (0.8481), whereas FedAVG should be highlighted (0.8496).\n\n`A4`:  Thanks for the useful comments! We modified the annotation typos in Table 1  in our manuscript.\n\n`Question 1`: How\u2019s Fig. 1 obtained? Is it evaluated on the same measurement model set used during training of each setup? Or are all models evaluated on the same predefined test set of measurement models?\n\n`A5`: Thanks for the great question! \n\nIn Fig. 1, all results are evaluated by sampling from the same mask pool to test the trained models. Note that this mask pool contains masks from three different distributions, i.e., $P_1$, $P_2$, $P_3$ as we plotted in Fig. 1. Since we sample non-overlapped masks, the masks used for testing are unseen to the models and can be regarded as a zero-shot testing. We add this illustration into the caption of Fig. 1.  In computational imaging, previous work [1,2] has shown that the change of mask (e.g., shift, perturbation) will cause large performance degradation. \n\n`Question 2`:  The biggest question raised is whether that network induces another source of instability to the overall model. Specifically, what guarantees that the prompt network doesn\u2019t do a terrible alignment for measurement models deviating from the training distribution?\n\n`A6`:  Thanks for the valuable insight! \n\nWe provide an ablation study in Table 3 to discuss the effect of prompt networks. FedHP experiences large performance degradation without using a prompt network. One thing to note is that the prompt network is learned on local but will be aggregated to the server, and then distributed to clients at each global round. This makes sure that the prompt network does not deviate from the global learning objective. \n\n`Question 3`: What is the source of inconsistency between Tab. 1 and Fig. 3? What we see in Fig. 3 flags for a higher PSNR difference than 0.14 dB between FedAVG and FedHP\u2026 It\u2019s understandable to argue that quantitative metrics such as PSNR or SSIM don\u2019t perfectly capture the true quality, but the visual difference on Fig. 3. is too large not to be captured by those metrics.\n\n`A7`: The difference of $0.14$dB is an averaged result on ten testing scenes as provided in Table 1. In Fig.3, we select one example of Scene $7$ to visually compare between different methods. This should correspond to a PSNR gap of $0.43$dB and $0.0073$ in SSIM. Besides, the visual quality in different regions can vary.  In the supplementary material, we provide more visual comparisons of different scenes. \n\n`Question 4`: Why isn\u2019t the deep unfolding network included in all the results and only reported as a short paragraph at the end?\n\n`A8`: Thanks for the insightful questions!\n\nThe deep learning-based reconstruction methods and model-based methods (e.g., deep unfolding) methods demonstrate promising performance for the SCI study. This work mainly focuses on deep learning-based methods. \n\nBesides the performance comparison in main tables, we also would like to compare with a state-of-the-art deep unfolding method [3]. As shown in Table 4b, FedHP also brings a notable performance boost ($+0.28$dB/$0.0038$ in PSNR/SSIM) with a much smaller model size. \n\n`Question 5`: Any intuition on why FedAVG is so much slower to train than FedHP?\n\n`A9`:   Thanks for the useful question!\n\nThere are two designs that make FedHP more efficient than FedAvg. \n\nFirstly, FedHP does not need to train every local client model from scratch, as long as there is a pre-trained model, we can easily adapt it to new clients under FedHP. By comparison, for any new client, FedAvg is required to train the model from scratch, this can be computationally cumbersome.\n\nThe other reason is that the proposed method has a much lower communication burden. FedHP only needs to aggregate and distribute a lightweight hardware prompt network and adaptors. By comparison, FedAvg has to do full model training and communication to adapt to different masks, otherwise, the performance will be significantly degraded (e.g., $<20$dB). In contrast, we introduce a hardware prompt to address this issue in a smart way.  \n\n[1] Modeling mask uncertainty in hyperspectral image reconstruction. ECCV 2022.\n\n[2] Metasci: Scalable and adaptive reconstruction for video compressive sensing. CVPR 2021.\n\n[3] Deep unfolding for snapshot compressive imaging. IJCV 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525671926,
                "cdate": 1700525671926,
                "tmdate": 1700584683848,
                "mdate": 1700584683848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TidVLnIPS3",
            "forum": "FI0vOp2asx",
            "replyto": "FI0vOp2asx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_X7WS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_X7WS"
            ],
            "content": {
                "summary": {
                    "value": "This work develops a federated hardware-prompt learning (FedHP) method for the task of snapshot compressive imaging (SCI). Existing reconstruction methods generally consider a single well-calibrated hardware configuration for network learning, inducing a highly coupled relationship between the reconstruction model and hardware settings. Differently, this work adopts federated learning to coordinate multiple clients with variant hardware settings and proposes a hardware-oriented solution to mitigate heterogeneous data issues."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022 The motivation of this work is impressive, pointing out a very practical problem for snapshot compressive imaging. Both the hardware cooperation and hardware heterogeneous problems are underexplored.  This work solves the heterogeneous issue accounting for the special characteristics of SCI.\n\u2022 The design of the hardware prompter bridges the hardware and software in a novel way, which could be easily incorporated into optimizing diverse set-ups in SCI. \n\u2022 Experimental results are abundant and have shown a clear performance boost over previous methods. Extensive ablation studies and model discussions have also been provided."
                },
                "weaknesses": {
                    "value": "\u2022 It remains unclear if the proposed method can adopt a larger client number. A detailed discussion on the number of clients should be given to demonstrate the practicality of the proposed method and to enhance the soundness of the work. \n\u2022 Is it possible to apply the proposed method to other hyperspectral image datasets? \n\u2022 It seems that a competitive method of FedGST for comparison was a centralized learning strategy, is it a fair comparison or what are the modifications toward this method? Please provide more details."
                },
                "questions": {
                    "value": "\u2022 Is the dataset split of the centralized learning the same as the federated learning? Please provide more illustrations and details. \n\u2022 There are some typos in the manuscript, for example, Fig.3 caption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570222896,
            "cdate": 1698570222896,
            "tmdate": 1699637162556,
            "mdate": 1699637162556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jPYukZyNa2",
                "forum": "FI0vOp2asx",
                "replyto": "TidVLnIPS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We appreciate that Reviewer X7WS finds our method well-motivated, solves a very practical problem, experimental results are abundant and clear. \n\n`Weakness 1`: It remains unclear if the proposed method can adopt a larger client number. A detailed discussion on the number of clients should be given to demonstrate the practicality of the proposed method and to enhance the soundness of the work.\n\n`A1`: We provide an experiment with more clients, e.g., C=8, as follows. We train the proposed FedHP under the same setting as Table 4a. \n\n| #Clients      | FedAvg   | FedHP | $\\Delta$ |\n| :-------------:|:-------------:|:-----:|:-----:|\n| 3      | 31.21 $\\pm$  0.10 \\ 0.8959 $\\pm$ 0.0017  | 31.35 $\\pm$ 0.10 \\ 0.9033 $\\pm$  0.0014| 0.14dB/0.0074  |\n| 4      | 31.06 $\\pm$  0.10 \\ 0.8955 $\\pm$ 0.0018  | 31.33 $\\pm$ 0.13 \\ 0.9023 $\\pm$ 0.0018 | 0.27dB/0.0068  |\n| 5      | 31.05 $\\pm$  0.10 \\ 0.9025 $\\pm$ 0.0014  | 31.32 $\\pm$ 0.19 \\ 0.9029 $\\pm$ 0.0019 | 0.27dB/0.0004  |\n| 8      | 31.17 $\\pm$  0.10 \\ 0.9033 $\\pm$ 0.0014  | 31.42 $\\pm$ 0.11 \\ 0.9043 $\\pm$ 0.0010 | 0.25dB/0.0010  |\n\n*Table R1: Comparison between FedAvg and FedHP with different number of clients. The last column denotes the performance gap.*\n\nAs shown in Table R1, the proposed method can achieve a consistent performance boost over FedAvg at a larger number of clients. \n\n`Weakness 2`: Is it possible to apply the proposed method to other hyperspectral image datasets?\n\n`A2`: We perform experiments on another hyperspectral dataset [1] with 24 spectral channels with the number of client C=3. As the table R2 shown below, the proposed method enables a performance boost over FedAvg. \n\n| Metrics     | FedAvg   | FedHP | \n| :-------------:|:-------------:|:-----:|\n| PSNR      | 29.97 $\\pm$  0.31    | 30.55 $\\pm$ 0.20  |\n| SSIM       |   0.8442 $\\pm$ 0.0026 |  0.8471 $\\pm$  0.0035  |   \n\n*Table R2: Comparison between FedAvg and FedHP on different hyperspectral dataset [1].*\n\n\n`Weakness 3`: It seems that a competitive method of FedGST for comparison was a centralized learning strategy, is it a fair comparison or what are the modifications toward this method? Please provide more details.\n\n`A3`: Thanks for the valuable suggestion!\n\nGST [2] is a centralized learning strategy to handle various hardware masks from the same distribution. In this work, we insert this model directly into the federated framework to enable hardware cooperation. We have added more illustrations about this method into the manuscript. Since all of the methods are compared under the federated learning framework and adopt the same hardware instances, we can thus perform a fair comparison. Actually, considering GST adopts a self-tuning network besides the reconstruction backbone, it requires more training cost to converge. \n\n`Question 1`: Is the dataset split of the centralized learning the same as the federated learning? Please provide more illustrations and details.\n\n`A4`: We split the training data according to the number of clients for federated learning. We keep the total amount of training data the same for both centralized learning and federated learning, for a fair comparison. We have put the above illustration into the manuscript. \n\n`Questions 2`: There are some typos in the manuscript, for example, Fig.3 caption.\n\n`A5`: Thanks for the useful comments! We have revised the typos in Fig. 3 caption. \n\n\n[1] l-net: Reconstruct hyperspectral images from a snapshot measurement. ICCV 2019. \n\n[2] Modeling mask uncertainty in hyperspectral image reconstruction. ECCV 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523692643,
                "cdate": 1700523692643,
                "tmdate": 1700581457789,
                "mdate": 1700581457789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MePSfXD9td",
                "forum": "FI0vOp2asx",
                "replyto": "jPYukZyNa2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_X7WS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_X7WS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors response and all my concerns have been well addressed. I have no further comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669140018,
                "cdate": 1700669140018,
                "tmdate": 1700669140018,
                "mdate": 1700669140018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gONoByJYsN",
            "forum": "FI0vOp2asx",
            "replyto": "FI0vOp2asx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_UbMX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_UbMX"
            ],
            "content": {
                "summary": {
                    "value": "This paper has studied a new problem for snapshot compressive imaging (SCI) by optimizing a cooperative network across different hardware configurations (coded apertures). A new hardware prompt learning module has been proposed and integrated into the FedAvg algorithm to enable co-optimizing multi-hardware and the global model for a computation imaging task. Extensive experimental results were provided on simulated and real data, compared with several federated baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1\uff09It is interesting and practical to leverage a federated learning framework to address hardware shifts across different systems while preserving the privacy of each system\u2019s local data. Plus, the paper has collected data from multiple real hardware systems to empirically validate the proposed method. \n2\uff09The proposed hardware prompt is a novel and efficient solution to mitigate data heterogeneity for developing deep SCI models in a federated learning framework, especially to enable co-optimizing multiple hardware and a global model across systems. A detailed ablation study has also been provided to clearly show the improvement given this prompt design. \n3\uff09 A multi-hardware dataset has been collected and built for this new problem, which could broadly benefit the SCI community. Extensive experimental results on multiple settings were provided in terms of both quantitive and qualitative evaluation.\n4\uff09Several state-of-the-art federated learning methods have been developed for a computational imaging task and been involved in the experiment comparison."
                },
                "weaknesses": {
                    "value": "1\uff09 While federated learning is a good choice, it remains unclear if the proposed problem setting can be directly solved by some other simple solutions, such as meta learning or deep ensemble. \n2\uff09Despite the large improvement given by the hardware prompt, it lacks further analysis of how this design works for different hardware. For example, will different hardware lead to different prompts? What these \u201chardware prompt\u201d look like? Is the prompt network only implemented by an attention block?"
                },
                "questions": {
                    "value": "1\uff09What are the benefits of introducing adaptors? Why not directly update the full model?\n2\uff09What\u2019s the main reason for setting C=3 in the experiment? \n3\uff09In Eq (9), is there any other way to impose a prompt on the measurements? For example, can the concatenation operation be applied?\n4\uff09It would be better to directly explain the settings of different hardware shits in the captions of Table \u00bd."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653070007,
            "cdate": 1698653070007,
            "tmdate": 1699637162419,
            "mdate": 1699637162419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eth2Ukxc5X",
                "forum": "FI0vOp2asx",
                "replyto": "gONoByJYsN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We appreciate that the reviewer UbMX finds our method novel, solves a practical problem, also provides a new multi-hardware dataset! \n\n`Weakness 1`: While federated learning is a good choice, it remains unclear if the proposed problem setting can be directly solved by some other simple solutions, such as meta learning or deep ensemble.\n\n`A1`: One key reason we adopt federated learning is to solve the privacy concern. To the best knowledge, both meta learning and deep ensemble are centralized learning strategies and require seeing all of the data to train each client, which makes it hard to perform the hardware corporations among different institutions. \n\nBesides, we additionally conduct a new experiment using meta learning. Specifically, we integrate MAML [1] into the federated learning framework, termed as FedMAML. We perform experiments using the same setting as Table 1, such that #client=3.   As shown in Table R1 below,  FedMAML method gives limited performance compared with FedAvg and the proposed FedHP. \n\n| Metrics     | FedMAML   | FedAvg | FedHP |\n| :-------------:|:-------------:|:-----:|:-----:|\n| PSNR      | 29.00 $\\pm$  1.44    | 31.21 $\\pm$ 0.10   | 31.35 $\\pm$ 0.10   |\n| SSIM       | 0.8532 $\\pm$ 0.0304 |  0.8959 $\\pm$  0.0017 | 0.9033 $\\pm$ 0.0014 | \n\n*Table R1: Comparison between FedAvg and FedHP, and FedMAML*\n\n`Weakness 2`: It lacks further analysis of how this design works for different hardware. For example, will different hardware lead to different prompts? What these \u201chardware prompt\u201d look like? Is the prompt network only implemented by an attention block?\n\n`A2`: Thanks for the detailed suggestion! \n\nThere will be only one prompt network obtained as a function to handle different input  masks, as shown in Fig. 2. Thus, different input hardware will lead to different prompts, which will have the same dimensionality as the input mask but with different pixel values. The prompt network only contains one attention block, which per our observation, can effectively cooperate among clients. \n\n`Question 1`: What are the benefits of introducing adaptors? Why not directly update the full model?\n\n`A3`: Directly updating the full model can cause cumbersome computational cost and communication cost. As exemplified by Table 3, directly learning client backbones from scratch under a federated framework (FedAvg) can result in $14$ times training time. Together with the prompt network,  adaptor can help enhance efficient fine-tuning performance of pre-trained backbones. As shown in Table 3, the adapter can bring $0.016$dB improvement in PSNR and $0.0037$ boost in SSIM. \n\n`Question 2`: What\u2019s the main reason for setting C=3 in the experiment?\n\n`A4`: Thanks for the valuable question!\n\nWe previously collected $5$ different real hardware instances. Considering the computational cost and our limited resources, we choose to adopt a number of $3$ clients in the main table. In Table 4a, we also report results under more clients, such as C=$4$ and C=$5$. We are still working on collecting more real hardwares. \n\n`Question 3`: In Eq (9), is there any other way to impose a prompt on the measurements? For example, can the concatenation operation be applied?\n\n`A5`: Thanks for the insightful question! \n\nDirectly using the concatenation will cause the dimensionality inconsistent with the backbones and cannot be used. Besides, there is no learnable module for concatenation, which lacks flexibility in handling different data distributions. \n\n`Question 4`: It would be better to directly explain the settings of different hardware shits in the captions of Table \u00bd.\n\n`A6`: Thanks for the useful suggestion! \n\nWe have added more explanations of the settings of different hardware shifts in captions of Table 1 and 2. Specifically, for Table 1, we provide the explanation that: For different clients, we sample non-overlapping masks from the same mask distribution to train the model and use unseen masks randomly sampled from all clients for testing. For Table 2, we provide the explanation that: Masks from each client are sampled from a specific distribution for training. We  randomly sample non-overlapping masks (unseen to training) from all distributions for testing.\n\n[1] Model-agnostic meta-learning for fast adaptation of deep networks. ICML 2017."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521991450,
                "cdate": 1700521991450,
                "tmdate": 1700522057353,
                "mdate": 1700522057353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cz2156VsN5",
                "forum": "FI0vOp2asx",
                "replyto": "eth2Ukxc5X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_UbMX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_UbMX"
                ],
                "content": {
                    "title": {
                        "value": "Feedback"
                    },
                    "comment": {
                        "value": "Thanks for your responses. My concerns have been well solved, and thus I have no further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660993659,
                "cdate": 1700660993659,
                "tmdate": 1700660993659,
                "mdate": 1700660993659,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UjAgTI9uyK",
            "forum": "FI0vOp2asx",
            "replyto": "FI0vOp2asx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_99wV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9239/Reviewer_99wV"
            ],
            "content": {
                "summary": {
                    "value": "Motivated by recent success of Federated Learning (FL) and Prompt Tuning, this paper proposes a deep neural network framework, named as FedHP, that can take into account diverse sensor acquisitions for spectral snapshot compressive imaging (Spectral SCI). The primary distinction from existing FL methods lies in the inclusion of a measurement enhancement network that considers both degraded observations and the physical forward model pattern across clients. Experimental results demonstrate its effectiveness of FedHP on both simulation dataset and real-world SCI dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1), This paper is overall well written and easy to flow. It clearly introduces the motivation and problem formulation, making the method accessible to non-SCI experts.\n\n2), Both simulation and real-world datasets are considered, making a better practical contribution.\n\n3), The experimental comparison is comprehensive, and baseline methods are up to date."
                },
                "weaknesses": {
                    "value": "1), The technical contribution to more general computational imaging seems to be limited or at least not well supported by this paper\u2019s current state.\n\n2), Likewise, the main deep learning technic behind this proposal, FedAvg, is already well known, which makes the technical contribution to deep learning community also marginal.\n\n3), The idea of using another learning-based module that can consider additional forward-model settings seems not new to model-based deep learning methods for computational imaging. Moreover, it is difficult to evaluate the proposed \u201ccorrection\u201d module indeed robust to distribution shift. At least, there is no clear evidence presented in this paper."
                },
                "questions": {
                    "value": "1), Figure 1. It is difficult to find differences between 4. FedAvg and 5. FedHP, the method instruction plot.\n\n2), The authors did not discuss a lot about why their method robust to the codec pattern shift, both intuitively and theoretically. What if the new module $\\phi$ cannot handle very new coded aperture $\\bf M$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9239/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9239/Reviewer_99wV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698867791865,
            "cdate": 1698867791865,
            "tmdate": 1700692616149,
            "mdate": 1700692616149,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F2hFUg7LM4",
                "forum": "FI0vOp2asx",
                "replyto": "UjAgTI9uyK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We appreciate that Reviewer 99wV finds our method easy to follow, provides comprehensive comparison, and is well presented!\n\n`Weakness 1`: The technical contribution to more general computational imaging seems to be limited or at least not well supported by this paper\u2019s current state.\n\n`A1`: Thanks for the valuable comments! \n\nOur key technical contribution is to provide a new multi-hardware optimization framework adapting to hardware shift by only accessing local data.  The principle underlying the proposed FedHP can be potentially extended to broad SCI applications. However, due to the practical cost of data acquisition and building optics systems, this study explores one specific direction following previous works in the field, where we focus on spectral SCI and collecting optical masks and real data from multiple hardware. \n\nExploiting the hardware collaboration of computational imaging systems is still in an early stage. This work serves as a proof of concept to inspire future endeavors in a more general scope. We list several potential related applications that might benefit from the proposed method, such as Lensless camera [1], LiDAR [2], HDR camera [3], or CT-Reconstruction [4], cooperating multiple imaging systems via aligning forward models, etc. We have put this discussion into the related work.\n\n`Weakness 2`: FedAvg, is already well known, which makes the technical contribution to deep learning community also marginal.\n\n`A2`: Thanks for the useful comments!\n\nWe find that FedAvg serves as a very strong baseline by even working better than recent  FL methods, such as SCAFFOLD and FedProx. Thus, it is non-trivial to achieve further performance boost. By comparison, FedHP provides an encouraging performance. \n\nWe kindly summarize the technical contributions of this work as follows:\n* We introduce a hardware prompt network to capture the hardware perturbations/replacement. \n* We improve the efficiency of the  federated learning in SCI, achieving performance boost with much lower training time cost. \n* We collect and will release a heterogeneous dataset that covers multiple hardware configurations, which to the best knowledge, is the first one in SCI.  \n \n`Weakness 3`: The idea of using another learning-based module that can consider additional forward-model settings seems not new to model-based deep learning methods for computational imaging. Moreover, it is difficult to evaluate the proposed \u201ccorrection\u201d module indeed robust to distribution shift. At least, there is no clear evidence presented in this paper.\n\n`A3`: Thanks for the valuable suggestion!\n\nFirstly, we compared with a state-of-the-art model-based method of GAP-Net [5] in Table 4b. We find that the proposed method brings a notable performance boost ($+0.28$dB/$0.0038$ in PSNR/SSIM) with a much smaller model size.  Secondly, we provide empirical evidence that the proposed method can handle new masks from distinct distributions in Table 2. During training, masks from different clients are sampled from different distributions. During testing, we randomly sample non-overlapping masks (unseen to models) from different distributions of all clients.\n\n `Questions 1`: Figure 1. It is difficult to find differences between 4. FedAvg and 5. FedHP, the method instruction plot.\n\n`A4`: Thanks for the useful suggestion!\n\nFig. 1 aims to show different settings for different types of solutions. For federated learning methods including FedAvg and FedHP, we make the settings the same to avoid misunderstanding. We have made it clear by modifying the Fig.1 captions. \n\n`Question 2`: Authors did not discuss a lot about why their method robust to the codec pattern shift, both intuitively and theoretically. What if the new module $\\phi$ cannot handle very new coded aperture $\\mathbf{M}$?\n\n`A5`: Thanks for the valuable insight\uff01 \n\nIntuitively, one of the key reasons why the proposed method can handle coded aperture shifts lies in the design of the hardware prompt learning model.  The hardware prompt learning aligns the input data distributions, solving the heterogeneity rooted in the input data space. We have provided the above discussion in Section 3.3.\n\nBesides, in Table 2, we provided the results that FedHP can handle very new coded apertures $\\mathbf{M}$. Specifically,  mask distributions from different clients are drastically different (as the distributions we shown in Fig.1, also shown in the supplementary).  The proposed method enables a significant performance boost over compared methods. For example, FedHP  improves $0.35$dB compared with FedAvg (Table 2). \n\n[1] A simple framework for 3D lensless imaging with programmable masks. CVPR 2021.\n\n[2] LiDAR-in-the-loop Hyperparameter Optimization. CVPR 2023.\n\n[3] End-to-end high dynamic range camera pipeline optimization. CVPR 2021.\n\n[4] DOLCE: A model-based probabilistic diffusion framework for limited-angle ct reconstruction. ICCV 2023. \n\n[5] Deep unfolding for snapshot compressive imaging. IJCV 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520671805,
                "cdate": 1700520671805,
                "tmdate": 1700585131310,
                "mdate": 1700585131310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FagKXanrEP",
                "forum": "FI0vOp2asx",
                "replyto": "UjAgTI9uyK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_99wV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9239/Reviewer_99wV"
                ],
                "content": {
                    "title": {
                        "value": "Thank You for Addressing The Reviewer's Comments"
                    },
                    "comment": {
                        "value": "Thank you to the authors for addressing my concerns regarding the general applicability of this proposal to computational imaging. I've no other questions. I'll increase my score by 1."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692594825,
                "cdate": 1700692594825,
                "tmdate": 1700692594825,
                "mdate": 1700692594825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]