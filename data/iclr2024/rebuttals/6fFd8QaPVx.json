[
    {
        "title": "OneBNet: Binarized Neural Networks using Decomposed 1-D Binarized Convolutions on Edge Device"
    },
    {
        "review": {
            "id": "0pYfMLPbmY",
            "forum": "6fFd8QaPVx",
            "replyto": "6fFd8QaPVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied binarized convolutional neural network and proposed to replace nxn 2D BNN by two 1D BNN (nx1 row-wise BNN and 1xn column-wise BNN). Further, the paper designed two basic BNN blocks, 1-D binarized convolutional layer in Figure 1 and downsampling 1-D binarized convolutional layer in Figure 2. Adjustment of activation distribution are analyzed in Section 3.3. By combining above two basic blocks and adjustment of activation distribution, the paper built the architecture based on ResNet18 for image classification tasks.The experimental results validate the effectiveness of the proposed binarized convolutional neural network."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provided detailed  analysis of proposed binarized convolutional network network from both theoretical and experimental perspectives.\n2. The designed architectures achieved SOTA performances on CIFAR10 and imagenet. \n3. The author's writing is very good, and the entire paper is relatively easy to understand.\n4. simple algorithm, easy to follow."
                },
                "weaknesses": {
                    "value": "1. from engineering viewpoints, the proposed basic blocks seems obviously due to 1\uff09the work of binarizing nx1 and 1xn convolutional neural network existed, and 2) adjustment of activation distribution also existed. The combination of them is not novel enough for top conferences. Thus, the contribution of this method is not very important.\n2. The reference format in this paper can be improved, e.g. \n - \"Reactnet: Towards precise binary neural network with generalized activation functions\" is an ECCV paper, \n - \"Recu: Reviving the dead weights in binary neural networks\" is an ICCV paper.\n3. In provided tables, some columns are left aligned, while others are center aligned. It is best to use a consistent format"
                },
                "questions": {
                    "value": "see weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697767859464,
            "cdate": 1697767859464,
            "tmdate": 1699636394870,
            "mdate": 1699636394870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "omXztn5lxa",
                "forum": "6fFd8QaPVx",
                "replyto": "0pYfMLPbmY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Review by Reviewer HiaQ"
                    },
                    "comment": {
                        "value": "* **Question:**\nThe works of binarizing $1 \\times N$ and $N \\times 1$ convolutional neural network and adjustment of activation distribution existed. The combination of them is not novel enough. \n* **Answer:** \nThanks for your sharp comments. In this paper, the effects of adjusting the activation distribution are doubled on the decomposed 1-D binarized convolutions. Although 1-D binarized convolution was considered in an existing work (BNSC-Net), its performance was only under 60% Top-1 accuracy. When the ideas of 1-D convolution and the adjustments of activation distributions are used in the proposed OneBNet, we did not naively combine the ideas and performed the model developments and structural analysis as follows:  \n**Firstly**, a new downsampling layer using 1-D binarized convolutions was proposed, so that the proposed model can have dramatical changes based on backboned ResNet, compared with the model naively using 1-D binarized convolution.  \n**Secondly**, the effects of the 1-D binarized convolutions having the adjustments and shortcuts can be doubled. The detail structure for the doubled effects was proposed. To show which layers can be critical, model structures were analyzed in terms of Top-1 accuracy and latency. We showed that the proposed 1-D binarized convolutions in downsampling (denoted as 1-D DS in Table 1) can be effective in enhancing Top-1 accuracy. Besides, when the number of channels was great (256 or 512) and feature map size was small, the proposed model can be effective, which was explained in the previous draft as:  \n***\"Table 1 shows the effectiveness of 1-D binarized convolutions in downsampling, where deep convolutional layers with many channels and small activation map can have significant benefits for enhancing performance.\"***\n---\n* **Question:**\nThe reference format in this paper can be improved.\n* **Answer:** \nIn agreement with your concerns, the references of officially published papers have been changed in **Reference** section of the revised version as:  \n***\"Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions. In European Conference on Computer Vision (ECCV), pp. 143-159, 2020.\"  \n\"Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, and Rongrong Ji. Recu: Reviving the dead weights in binary neural networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 5198-5208, 2021.\"***  \nBesides, the references of several works have been revised if they were officially published.\n---\n* **Question:**\nIn provided tables, some columns are left aligned, while others are center aligned. It is best to use a consistent format. \n* **Answer:** \nAccording to your concerns, left-alignments have been applied to all Tables in the revised version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143346828,
                "cdate": 1700143346828,
                "tmdate": 1700143346828,
                "mdate": 1700143346828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7yFbvoO6QS",
                "forum": "6fFd8QaPVx",
                "replyto": "omXztn5lxa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback and clarification. I agree that this paper has certain technical contributions and practical significance.\n\nI insist that the technology used in the paper mainly comes from existing literature, and the contribution of the technology combination lacks persuasiveness. For top-level conferences, current paper manuscripts have limited novelty and lack new insights. In addition, considering that other reviewers have already pointed out some modifications and clarifications, I believe that this article is not comprehensive enough.\n\nTherefore, I insist on my rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532212711,
                "cdate": 1700532212711,
                "tmdate": 1700532212711,
                "mdate": 1700532212711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t3xJrARyzQ",
            "forum": "6fFd8QaPVx",
            "replyto": "6fFd8QaPVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new binary neural network called OneBNet, which mainly replaces NxN 2D binarized convolution by Nx1 row-wise and 1xN column-wise 1D binarized convolutions. The proposed model shows strong performance on ImageNet, i.e., ResNet18-based model obtains 63.9% Top-1 accuracy by training from scratch and 68.4% Top-1 accuracy by applying teacher-student training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The proposed method is easy to follow.\n+ Latency on Raspberry Pi is reported, which is beneficial for the BNN community.\n+ The proposed model shows strong performance on ImageNet, i.e., ResNet18-based model obtains 63.9% Top-1 accuracy by training from scratch and 68.4% Top-1 accuracy by applying teacher-student training."
                },
                "weaknesses": {
                    "value": "-\tIt is not new to replaces NxN 2D binarized convolution by Nx1 and 1xN 1D binarized convolutions. For example, SqueezeNext (CVPR\u201918 workshop) decompose the KxK convolutions into two separable convolutions of size 1xK and Kx1. Although this paper focuses on binary neural network, the novelty of using such strategy for BNNs is still quite unclear.\n-\tCould the proposed method also suitable for object detection tasks using binary neural network?\n-\tSeveral recent methods reported in Table 2 are not included in Table 3."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659166464,
            "cdate": 1698659166464,
            "tmdate": 1699636394797,
            "mdate": 1699636394797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XIpdMe6wfL",
                "forum": "6fFd8QaPVx",
                "replyto": "t3xJrARyzQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Review by Reviewer 6xWT"
                    },
                    "comment": {
                        "value": "* **Question:**\nIt is not new to replaces $N \\times N$ 2-D binarized convolution by $N \\times 1$ and $1 \\times N$ 1-D binarized convolutions. \n* **Answer:** \nThanks for your novel comments. In agreements with your concerns, 1-D convolutions were proposed in SqueeNext and Inceptionv3. I agree that the naive application of 1-D convolutions to BCNNs cannot be a main contribution, as shown in **Introduction** section of the previous version. \nHowever, we provide several unique things in this paper as: the downsampling layer using 1-D binarized convolutions was proposed, so that the proposed model can have significant internal differences based on backboned ResNet, compared with the model naively using 1-D binarized convolution. Besides, the effects of adjusting the activation distribution and non-linear activation function can be doubled by applying specific layers for BCNNs to the decomposed 1-D convolutions in the proposed OneBNet. In order to avoid the confusion of the main contribution, the explanation has been rewritten in **Abstract** as:  \n***\"It was known that the decomposed 1-D convolutions can replace the spatial 2-D convolutions in several convolutional neural networks (CNNs) for computer vision. However, the proper usage of 1-D convolutions was not shown in the field of binarized CNNs. This paper shows This paper proposes a new structure called OneBNet to maximize the effects of 1-D binarized convolutions, thus producing  excellent performance on CPU-based edge devices. To double the effects of adjusting the activation distribution and non-linear activation function, specific layers for BCNNs are doubled by applying them to  both $n \\times 1$ row-wise and $1 \\times n$ column-wise 1-D binarized convolutions. The proposed 1-D downsampling can perform information compression gradually through two 1-D convolutions, which can contribute tremendously to the performance improvement in binarized convolutional neural networks (BCNNs) in our analysis.\"***\n---\n* **Question:**\nCould the proposed method also suitable for object detection tasks using binary neural network?\n* **Answer:** \nThanks for your good recommendation for model applications. At this time, we did not verify the proposed OneBNet to object detection in the experiments. However, because similar works using BCNNs can produce acceptable performance in object detection, we expect that the proposed model could be applicable. For example, BiDet: An Efficient Binarized Object Detector (CVPR 2020) and Recurrent Bilinear Optimization for Binary Neural Networks (RBONN in ECCV 2022) applied BCNNs such as XNOR-Net, Bi-RealNet, and ReActNet in the object detection. Notably, when RBONN was applied to the image classification, its Top-1 performance (66.7\\%) on ImageNet dataset cannot reach our results. \nAlthough the experimental results for the object dection were not listed,\nthe successful application with the counterparts and the outstanding image classification of the proposed model make sure that the proposed OneBNet could be useful in object detection.\n---\n* **Question:**\nSeveral recent methods reported in Table 2 are not included in Table 3.\n* **Answer:**\nTable 3 listed the comparison with several BCNNs if a model has difference in layer structure and **the specific layer supported by Larq compute engine**.\nWhen the specific layer was not supported by Larq, we cannot evaluate the latency using Larq. For example, XNOR-NET++ has row-wise and column-wise scaling parameters in binarized convolutions, which cannot be supported in Larq and Tensorflow Lite. In RB-Net, the reshaped output could not be supported by Larq. Therefore, we have listed models that can be developed by Larq in Table 3."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143093700,
                "cdate": 1700143093700,
                "tmdate": 1700143093700,
                "mdate": 1700143093700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "haAjkoL1Vl",
                "forum": "6fFd8QaPVx",
                "replyto": "XIpdMe6wfL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Overall my concerns about limited novelty and more experiments on downstream tasks have not been fully addressed, thus I tend to keep my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663286034,
                "cdate": 1700663286034,
                "tmdate": 1700663286034,
                "mdate": 1700663286034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BqYl8M9ssS",
            "forum": "6fFd8QaPVx",
            "replyto": "6fFd8QaPVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to decompose a 2D convolution into two 1D convolutions on a Binarized neural network model to improve inference speed and model accuracy on edge devices. On the basis of the previous Binarized ResNet, the 3x3 convolutional kernel was changed to two sets of 1x3 and 3x1 convolutional kernels, achieving higher accuracy."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is easy to read and understand. The method in this paper is relatively simple, clear, and easy to reproduce. The experimental data in the paper indicates that the improved model outperforms the previous Binarized neural network model in terms of speed and accuracy."
                },
                "weaknesses": {
                    "value": "1. The contribution and innovation of this paper are insufficient. The decomposition of 2D convolutions into two 1D convolutions used in this article is not a new idea, but a widely studied method. Although its combination with Binarized neural networks may make it more effective, it is easy to consider or attempt.\n\n2. The generalizability of this method has not been verified. The author's experiment only trained and tested the smaller ResNet model, and the dataset only included CIFAR10 and ImageNet.\n\n3. The basic theory of the method in this paper is not sound enough. Binary quantization and 2D convolutional decomposition are methods that sacrifice accuracy for less computational complexity. Why can a combination of the two achieve better accuracy? Substantive improvements may come from element wise calculations and learnable bias in more layers after decomposition, but this is not without cost, as FLOPs cannot accurately reflect the additional hardware overhead this brings. The explanation of Figure 3 also lacks quantitative data support.\n\n4. This method lacks a determination method for parameter selection. The paper mentions that not all convolutional layers of blocks are suitable for such transformations. In Table 1, several selection combinations are attempted, and the best ones are selected for subsequent comparison. This will bring difficulties to practical applications. If the model structure is different and there are more blocks with different channel numbers, it will not be suitable for such selection.\n\n5. The comparison of experiments lacks fairness and universality. Compared to other methods in Table 3, they are all different binary quantization methods and will not significantly change the model structure. This paper essentially changes the structure of the model orthogonal to previous work. Unless compared with other structural optimizations, such changes are unfair."
                },
                "questions": {
                    "value": "In experimental environments, it said \"Like other BCNN models, the first convolutional and last fully-connected layers adopted FP32\nweights and activations.\" Is the latency data measured End2End? If not, are non binarized layers becoming performance bottlenecks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672249585,
            "cdate": 1698672249585,
            "tmdate": 1699636394723,
            "mdate": 1699636394723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A8QQ3NfkTJ",
                "forum": "6fFd8QaPVx",
                "replyto": "BqYl8M9ssS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Review by Reviewer w6Pw"
                    },
                    "comment": {
                        "value": "* **Question:**\nThe contribution and innovation of this paper are insufficient. \n* **Answer:** \nIn agreements with your concerns, 1-D convolutions were proposed in previous works such as SqueeNext and Inceptionv3. I agreed that the naive application of 1-D convolutions to BCNNs cannot be a main contribution, as shown in Introduction section.  \nOn the other hand, our works proposed the downsampling layer using 1-D binarized convolutions, so that the proposed model can have significant internal changes. Besides, the effects of adjusting the activation distribution and non-linear activation function can be doubled by applying specific layers for BCNNs to the decomposed 1-D convolutions. To focus on the main contribution, the explanation has been rewritten in **Abstract** as:  \n***\"It was known that the decomposed 1-D convolutions can replace the spatial 2-D convolutions in several convolutional neural networks (CNNs) for computer vision. However, the proper usage of 1-D convolutions was not shown in the field of binarized CNNs. This paper proposes a new structure called *OneBNet* to maximize the effects of 1-D binarized convolutions, thus producing excellent performance on CPU-based edge devices. To double the effects of adjusting the activation distribution and non-linear activation function, specific layers for BCNNs are doubled by applying them to both $n \\times 1$ row-wise and $1 \\times n$ column-wise 1-D binarized convolutions. The proposed 1-D downsampling can perform information compression gradually through two 1-D convolutions, which can contribute tremendously to the performance improvement in binarized convolutional neural networks (BCNNs) in our analysis.''***\n---\n* **Question:**\nGeneralizability: Only smaller ResNet model, and the dataset only included CIFAR10 and ImageNet.\n* **Answer:** \nOur evaluations were based on baselined ResNet18 and ResNet20. Target datasets included FashionMNIST, CIFAR10, and ImageNet. In agreement with your concerns, we adopted the multiplier $m$ for the number of channels in Appendix A.3 of the revised version, . Both the number of input and output channels can be scaled by multiplying with $m$. New Table 7 shows the summary of Top-1 accuracies by varying multiplier on CIFAR10 dataset.\n---\n* **Question:**\nWhy can a combination of Binary quantization and 2D convolutional decomposition achieve better accuracy? \n* **Answer:** \nBecause the newly added adjustments and functions are doubled, we concluded that the doubled effects can mitigate the degradation. I agree that good performance of the proposed method required additional computational costs. Therefore, we want to show the trade-off between the additional costs and enhanced performance to provide better model structure, in Figure 5 and Table 1. \nWe agree with you that OPs in Figure 5 cannot be accurate to estimate the latencies. Additionally, Table 1 compares the latencies on Raspberry Pi when1-D binarized convolutions were adopted, so that the guideline for developing structures were concluded in terms of accuracies and latencies.\n---\n* **Question:**\nThis method lacks a determination method for parameter selection. \n* **Answer:** \nAlthough it is hard to extract any quantitative metric, we think that the below guideline can introduce the priority to apply the proposed structure. In the second paragraph of **Experimental Results and Analysis** section, we concluded the meaning of data as:  \n***\"Table 1 shows that the effectiveness of 1-D binarized convolutions in downsampling, where deep convolutional layers with many channels and small activation map can have significant benefits for enhancing performance.\"***  \nThe above conclusion gave the guideline to determine which layer adopted 1-D binarized convolutions considering latencies and Top-1 accuracies.\n---\n* **Question:**\nIn Table 3, unless compared with other structural optimizations, such changes are unfair.\n* **Answer:** \nThe proposed OneBNet adopted 1-D binarized convolutions, which bring structural changes. However, many characteristics based on the baselined ResNet were equally applied; the first FP32 convolutional and last fully-connected layers were the same in Table 3. the numbers of downsamplings and output channels in their basic blocks can be the same. In several existing works such as Inceptionv3 and BNSC-Net, the computations and accuracies of the structures having decomposed 1-D convolutions are compared with those of models using 2-D convolutions. Besides, we think that the differences of the latencies in Table 3 are due to the structural changes using the specific layers. For example, Real-to-Bin had self-attention layer. Whereas XNOR-Net adopted doubled skipped shortcuts, other BCNNs had single skipped shortcuts. In **Related Works** section, the model structures were briefly summarized.\n---\n* **Question:**\nIs the latency data measured End2End?\n* **Answer:** \nThe latencies included the delay in the first and fully connected layers."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142926311,
                "cdate": 1700142926311,
                "tmdate": 1700142926311,
                "mdate": 1700142926311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CD8daodgs1",
                "forum": "6fFd8QaPVx",
                "replyto": "A8QQ3NfkTJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply and explanation to my question. Inserting more adjustment layers does bring benefits. However, I believe that the overall novelty of this paper **is still limited**, as almost all components come from existing research.\n\nI think it is possible to revise my rating to 5 if more test data is provided on other baseline models or different types of tasks. This is because I agree that the author's current work has good practical significance, but it is still not sufficient as a good paper with in-depth insights or new contributions.\n\nIn addition, 1D convolutional decomposition is not the core contribution point, so I think an interesting perspective is the effectiveness and universality of inserting adjustment layers after this decomposition. As you mentioned, *\"Table 1 shows that the effectiveness of 1-D binarized conversions in downsampling, where deep convolutional layers with many channels and small activation map can have significant benefits for enhancing performance\"*\nFor the Binary quantization model, the rule of inserting adjustment layers after decomposition calculation is applicable to ResNet, but is it applicable to the Inception, MobileNet structure or even Transformer? If so, this will significantly increase the significance of the work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240855177,
                "cdate": 1700240855177,
                "tmdate": 1700240855177,
                "mdate": 1700240855177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zJk7fqsTFn",
            "forum": "6fFd8QaPVx",
            "replyto": "6fFd8QaPVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_38Cc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4273/Reviewer_38Cc"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes decomposing 2-D binarized convolution into two 1-D convolutions in different directions to reduce model complexity. Then, the paper investigates the settings where such replacement can be beneficial, followed by experimental verification of the architecture performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work identified the proper settings where replacing 2D convolutions with 1D convolutions can be beneficial. The model performance shown in the paper is promising."
                },
                "weaknesses": {
                    "value": "Overall the paper should improve on the presentation befoer it is ready for publication. At current state, I am unsure about many technical details. Please also see below."
                },
                "questions": {
                    "value": "P3: section 3.1 The section is very difficult to follow, please rewrite this.\n\nP4: \"However, it shrinks the receptive fields\", I am not sure whether this is true, please verify.\n\nFig. 2. It is not clear how the results from 1D convolution are summed together with ouput of the 1x1 FP32 convolution, given they are of different shape.  \n\nP5: not sure about \" It removes the negative part with \u03b2\" \n\nP5: Not sure why there should be both  \u03b6 and \u03b1\n\nFigure 5: please change Fig. (a) to (a)\n\nTable 1 is very confusing, please clear it up.\n\nDistillation is quite a standard approach for improving model performance. Therefore, I suggest the authors move the results from the distillation experiment to the appendix. \n\nI suggest adding a model performance comparison with a similar OP level. \n\nReActNet presented results of much higher performance (probably higher complexity); I suggest including an experiment that compares those results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740124136,
            "cdate": 1698740124136,
            "tmdate": 1699636394624,
            "mdate": 1699636394624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RFxoY8rUz9",
                "forum": "6fFd8QaPVx",
                "replyto": "zJk7fqsTFn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Review by Reviewer 38Cc"
                    },
                    "comment": {
                        "value": "Thanks for your review. We have addressed your concerns and upload new PDF. Please check them. \n* **Question:**\nSection 3.1 is very difficult.\n* **Answer:** \nAccording to your recommendation, **Motivations** section has been modified.\nNotably, the second paragraph was rewritten as:  \n **\"*To double the effects of the adjustment of activation distribution, we apply the adjustment of activation distribution to the decomposed 1-D binarized convolutions. Whereas the conventional 2-D convolutions use 3 \u00d7 3 kernels, the decomposed convolutions use 3 \u00d7 1 and 1 \u00d7 3 kernels. The adjustment of activation distribution is applied to each 1-D convolution. Although the decomposition had the same receptive field with 2-D convolution (Szegedy et al., 2016), it requires additional element-wise operations for the adjustments and activation functions. This paper explains the structure of 1-D convolutional layer. Then, it analyzes the model structures in terms of latency and accuracy and shows the idea to deploy 1-D binarized convolutions on the above explained pros and cons*.\"**\n---\n  \n* **Question:**\nI am not sure \"However, it shrinks the receptive fields\"  is true.  \n* **Answer:** \nAccording to Inceptionv3, when $1 \\times n$ and $n \\times 1$ convolutions are used, the receptive fields are the same. The expression has been modified as:  \n***\"Although the decomposition had the same receptive field with 2-D convolution (Szegedy et al., 2016), it shrinks the receptive fields and requires additional element-wise operations for the adjustment.\"***\n---\n* **Question:**\nFig. 2 needs to be clarified.\n* **Answer:** \nThe numbers of channels and size of the feature map after 1x1 FP32 and binarized convolutions have been denoted in Fig.2.\n---\n* **Question:**\nThe expression \"It removes the negative part with $\\beta$\"\n* **Answer:** \nThe expression has been rewritten as:  \n***\"When $x_i - \\gamma_i$ is negative, learnable parameter $\\beta_i$ scales it.\"***\n---\n* **Question:**\nNot sure why there should be both $\\zeta$ and $\\alpha$.\n* **Answer:** \nThe learnable parameter $\\alpha$ can produce the difference between shortcut and input of binarized convolutions. If the learnable parameter does not exist, the input activations for the shortcut and binarized convolutions are the same. The learnable parameter can adjust the input distribution for the binarized convolution. Although $\\zeta$ can change the distribution for the shortcut and input of binarized convolutions, it is known that the additional adjustments with $\\alpha$ are necessary in the ablation study of ReActNet (Liu et al., 2020).\n---\n* **Question:**\nIn Figure 5, change Fig. (a) to (a)\n* **Answer:** \nWe have revised the expression in the caption of Figure 5.\n---\n* **Question:**\nTable 1 is very confusing.\n* **Answer:** \nTo avoid confusion in Table 1, the inference latencies of all structures have been listed in the revised version.\n---\n* **Question:**\nI suggest adding a model performance comparison with a similar OP level.\n* **Answer:** \nTo show the comparison with BCNN models having similar OPs, counterparts listed in Table 3 were compared in the previous manuscript because they have the same first FP32 convolutional and last fully connected layers on the same baseline model. \nInstead, I think that your recommendation means the comparison with mobile-friendly FP32 models such as MobileNet or ShuffleNet. Theoretically, TypeI had about **$170 \\times 10^6$** OPs. On the other hand, MobileNetv1 and MobileNetv2 has about **500MFLOPs and 300MFLOPs** with 70.6 and 72.0 Top-1 accuracies. The inference latency on a single thread of MobileNetv2 was **117ms** using Larq. Considering the results, we can conclude that the proposed OneBNet can consume small computational resources, although accuracies were degraded. \nThe performance comparison with other mobile-friendly models requires huge space because there have been too many researches. Besides, additional model optimization related to number of channels and feature size should be done. Thanks for recommendation.\n---\n  \n* **Question:**\nExperiment that compares those results with ReActNet.\n* **Answer:** \nThanks for the good recommendation. ReActNet and its following works based on had better Top-1 accuracy, but the models were based on a modified MobileNet. Although the theoretical OP of ReActNetA was smaller than BCNNs based on ResNet18, our experiment shows that its latency was significantly longer than the proposed model. We have added the experimental results and discussion in **Appendix A.4** as:  \n***\"We have added the inference latency of ReActNetA (Liu et al., 2020) in Table 7. When using 4 threads on RaspberryPi 4, the inference latency of ReActNetA was 86.9ms on the ImageNet dataset. With a single thread, the inference latency was 123.3 ms. It is noted that ReActNetA was developed based on MobileNet (Howard et al., 2017). Although the estimated OPs from the binarized operations were small in ReActNetA, it showed longer latency.\"***"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141263919,
                "cdate": 1700141263919,
                "tmdate": 1700239324497,
                "mdate": 1700239324497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]