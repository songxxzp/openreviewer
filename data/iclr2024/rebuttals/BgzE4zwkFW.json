[
    {
        "title": "Curriculum Reinforcement Learning via Morphology-Environment Co-Evolution"
    },
    {
        "review": {
            "id": "8XOwHBIyc5",
            "forum": "BgzE4zwkFW",
            "replyto": "BgzE4zwkFW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a curriculum reinforcement learning approach, MECE, which optimizes an RL agent's morphology and environment through co-evolution. The authors train two policies to automatically modify the morphology and change the environment, creating a curriculum for training the control policy. Experimental results demonstrate that MECE significantly improves generalization capability compared to existing methods and achieves faster learning. The authors emphasize the importance of the interplay between morphology and environment in brain-body co-optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-structured, with a relatively clear introduction.\n\n2. The paper includes comprehensive experiments on rigid robot co-design tasks, demonstrating the superiority of the proposed algorithm. The ablation studies effectively isolate each component's contribution and provide valuable insights into the algorithm's effectiveness."
                },
                "weaknesses": {
                    "value": "The significance of the paper's contributions is a bit unclear. It is not the first to propose using co-evolution method to co-design brain, body and environment. The proposed methods should be compared with more strong baselines. Curiously, can and how this system extend to the real world?"
                },
                "questions": {
                    "value": "1. How general is the proposed approach, beyond the tasks and environments considered in the experiments?\n\n2. Is the proposed MECE method computationally efficient?\n\n3. Have you encountered any scalability issues when applying MECE to more complex tasks or environments?\n\n4. It is not clear to me how environments are produced and how the agents perform in your environment (Figure 4), do you have a video?\n\n5. It seems that MECE's performance is not much better than Transform2Act, can you provide more results on different tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1960/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1960/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698595118245,
            "cdate": 1698595118245,
            "tmdate": 1699651836294,
            "mdate": 1699651836294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gok8XGkXgY",
                "forum": "BgzE4zwkFW",
                "replyto": "8XOwHBIyc5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments and suggestions!"
                    },
                    "comment": {
                        "value": "Thank you for your comments and suggestions! We address your main concerns as below:\n\n> The significance of the paper's contributions is a bit unclear.\n- To the best of our knowledge, MECE is the first paper to discuss the co-evoltuion between the agent's morphologies and training environments and use the co-evolution to create an efficient curriculum for training embodied agent for better generalization. Would you mind kindly providing papers that have previously examined this problem?\n- Our baseline methods include the SOTA algorithms of training agent to adpat to diverse environments (POET), the SOTA morphology optimization method utilizing RL to evolve the agent's morphology in a fixed environment (Transform2Act), and a widely-studied method that models an agent's morphology as GNN (NGE).\n\n\n> How general is the proposed approach, beyond the tasks and environments considered in the experiments?\n- In MECE, the simulation is Mujoco, which is a widlely used simulator in RL.\n- We did not design special constraints for environments because the learned $\\pi_E$ automatically avoids generating unlearnable environments. Furthermore, tasks are not accompanied by any form of extrinsic reward signal. Hence, MECE has the potential to be applied to a wide range of tasks and circumstances, extending beyond the confines of experimental settings.\n\n> Is the proposed MECE method computationally efficient?\n- MECE training is more computationally efficient than all the compared baselines. For all the test environments, MECE took around 44 hours to train on a standard server with 8 CPU cores and an NVIDIA RTX 3090 Ti GPU, while Transform2Act requires around 63 hours on the same server.\n\n> Have you encountered any scalability issues when applying MECE to more complex tasks or environments?\n- The 3d-locomotion environment is a complex environment for RL navigation tasks. If we do not set any contraints on $\\pi_e$, in earlier stages, the unmatural environment policy may change the environment at a large scale, which can make the training environments overly hard/simple. Hence, we limit the scale of changes by $\\pi_e$ to environment in each step.\n\n\n> It is not clear to me how environments are produced and how the agents perform in your environment (Figure 4), do you have a video?\n- We introdce how $\\pi_E$ generates and controls the environments in Appendix A.\n- We will release the source code with videos.\n\n> It seems that MECE's performance is not much better than Transform2Act, can you provide more results on different tasks?\n- We respectfully disagree with the assertion that this is \"not much superior to Transform2Act.\" In Figure 2, MECE outperforms the modified Transform2Act (trained on diverse environments) by around 20% in terms of rewards and outperforms Transform2Act-Original by approximately 60%."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897371597,
                "cdate": 1699897371597,
                "tmdate": 1699897371597,
                "mdate": 1699897371597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3BN4bZopR4",
                "forum": "BgzE4zwkFW",
                "replyto": "8XOwHBIyc5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Would you mind confirming if your concerns have been addressed by our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWould you mind validating whether our rebuttal addresses your concerns:\n\n- We explained the generalization and contribution of MECE.\n- We explained that MECE is not computationally expensive.\n\nPlease feel free to ask any additional queries after reading our rebuttal. During this author-reviewer discussion period, we hope to catch and answer any questions.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393939718,
                "cdate": 1700393939718,
                "tmdate": 1700393939718,
                "mdate": 1700393939718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lvt8PxTrH7",
                "forum": "BgzE4zwkFW",
                "replyto": "tQJIW3VlgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_cQrp"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response, I have no further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644935585,
                "cdate": 1700644935585,
                "tmdate": 1700644935585,
                "mdate": 1700644935585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uCO3Ds5Hhe",
            "forum": "BgzE4zwkFW",
            "replyto": "BgzE4zwkFW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of joint optimization of the policy and the morphology of a learning agent. The authors\u2019 motivation is described in the claim written in the introduction: \u201ca good morphology should improve the agents\u2019 adaptiveness and versatility, i.e., learning faster and making more progress in different environments.\u201d To realize it, the authors propose the novel framework where the morphology and the training environment are jointly evolved. In the proposed MECE scheme, three policies are introduced: one for the control of an agent\u2019s action, one for the evolution of the morphology, one for the evolution of the training environment. Inside this scheme, the authors define reward functions for the training of the morphology policy and for the training of the environment policy. The authors have performed comparison with several baseline approaches on three control tasks and ablation studies have been conducted to confirm the effectiveness of each algorithmic component."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A novel framework for morphology optimization aiming at obtaining a morphology under which a control policy  can be quickly adapted to an unseen environment.\n\n2. Promising empirical results compared to baseline approaches, but the experimental procedure is questionable, see below."
                },
                "weaknesses": {
                    "value": "1. As far as I understand from what is written in the introduction, the motivation of the morphology optimization in this paper is to obtain a morphology with which the agent can adapt its policy quickly to unseen tasks. It is also written in the second question of the experiments. However, it seems that the reported results in figures are average performances of the agent obtained at each training time step on randomly-selected environment. Therefore, the performance evaluated in this paper is the one for domain randomization. It is different from the motivation. The efficiency of the adaptation of the policy under the obtained morphology is not evaluated. My understanding might be wrong as the evaluation procedure was not clearly stated. Please clarify this point. \n\n2. It could be better if the design choices of the proposed approach is more elaborated. In particular, it is not clear how the reward functions (1) and (2) reflect the author\u2019s hypotheses \u201ca good morphology should improve the agent\u2019s adaptiveness and versatility, i.e., learning faster and making more progress in different environments\u201d and \u201ca good environment should accelerate the evolution of the agent and help it find better morphology sooner\u201d. It is also not clear why the authors want to train policies for morphology evolution and environment evolution instead of just optimizing the probability distributions over these spaces, despite the fact that these policies are not used afterwards and only the obtained morphology is used in the test phase. \n\n3. The clarity of the explanations could be improved. First, the notation inconsistencies makes it confusing. For example, r^m vs r_m, r^E vs r_e, and E and Env. If they are the same, please use the same notation. Algorithm 2 was also not very clear. How could pi_m be updated by using D where transition history doesn\u2019t necessarily have a reward information r_m? The same applies for pi_e."
                },
                "questions": {
                    "value": "Please clarify the points given in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799894030,
            "cdate": 1698799894030,
            "tmdate": 1699636127380,
            "mdate": 1699636127380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YbzeTL7Q8j",
                "forum": "BgzE4zwkFW",
                "replyto": "uCO3Ds5Hhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments and suggestions!"
                    },
                    "comment": {
                        "value": "Thank you for your comments and suggestions! We will correct the typos following your suggestions. We address your main concerns as below:\n\n> The setting of experiments is different from the motivation.\n- Our evaluation is consistent with the motivation and the 2nd question at the beginning of the experiments: \"Does our method create agents more adaptive and generalizable to diverse new environments?\". As stated in the 2nd line of Section 5.2, we evaluated MECE and its learned morphology in 12 diverse and unseen environments (random variants of 2d-locomotion, 3d-locomoation, or Gap-crosser) under 6 random seeds with the morphology fixed. The experimental results show that the morhology evolved by MECE can generalize better to the tasks in diverse unseen environments.\n- MECE does not require finetuning for the unseen environments in the test, unlike the Transform2Act and NGE methods, which still require this additional step. Furthermore, we present the performance of baselines without finetunes in the table below.\n \n|Methods| Performance|\n|:--|:--|\n|MECE| 4252.26 $\\pm$ 393.45|\n|Transform2Act| 3644.28 $\\pm$ 518.77 |\n|Transform2Act without finetune|1990.54 $\\pm$ 424.03| \n|NGE|1874.65 $\\pm$ 816.51|\n|NGE - without finetune|942.30 $\\pm$ 566.22|\n\n> How do $r_m$ and $r_E$ contribute to our statement on \"a good morphology\" and \"a good environment\"? + Why we need morphology policy $\\pi_m$ and environment policy $\\pi_e$?\n\n- We explain the motivation of designing $r_m$ and $r_e$ in the 4th and 5th paragraphs of Section 3, in particular, the text above Eq. (1) and Eq. (2). They are in line with the statement of \"a good morphology should make the agent learn faster and make more progress in different environments.\" (i.e., greater progress of control policy $\\pi$ in Eq. (1)) and \"a good environment should accelerate the evolution of the agent and help it find better morphology sooner\" (i.e., greater progress of morphology policy $\\pi_m$ in Eq. (2)). \n\n- The training and co-evolution of $\\pi_e$ and $\\pi_m$ is the key for the morphology optimization and gaining generaliation to diverse unseen environments. They jointly create a special adaptive curriculum for the training phase for the agent to evolve its morphology and generalization capability. In particular, $\\pi_e$ creates a curriculum of environments for optimizing the morphology and control policy $\\pi$, while $\\pi_m$ creates a curriculum for the environment changing and control policy learning. They improve the efficency of exploration through (1) interactions with MDP using different morphologies and environments; (2) structural constraint and correlation captured by the GNN; (3) more directed evolution than random mutation.\n- In our thorough ablation study-\u2160,\u2161,\u2163, we have extensively evaluated the importance and impacts of $r_E$ and $r_m$, $\\pi_e$ and $\\pi_m$ in morphology optimization and generalization to new environments. \n\n> The clarity of the explanations could be improved.\n- Thank you for this suggestion. We have updated all unclear notations and expressions following your suggestions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897256837,
                "cdate": 1699897256837,
                "tmdate": 1699897256837,
                "mdate": 1699897256837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "54ZnO9iIFu",
                "forum": "BgzE4zwkFW",
                "replyto": "uCO3Ds5Hhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Would you mind confirming if your concerns have been addressed by our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWould you mind validating whether our rebuttal addresses your concerns:\n\n- The consistency of our experiments and motivation, and we provide additional results.\n- The design of the reward function for $\\pi_e$ and $\\pi_m$.\n\nPlease feel free to ask any additional queries after reading our rebuttal. During this author-reviewer discussion period, we hope to catch and answer any questions.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393787957,
                "cdate": 1700393787957,
                "tmdate": 1700393787957,
                "mdate": 1700393787957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oR0lRLEHRF",
                "forum": "BgzE4zwkFW",
                "replyto": "YbzeTL7Q8j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_jNfL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for our response"
                    },
                    "comment": {
                        "value": "Thank you for your clarification. However, I am even more puzzled by the response to the first question. If I understand correctly what you say, the objective of the joint optimization of the morphology and the policy is to have a high zero-shot performance, rather than to obtain a morphology with which a policy can be quickly learned to an unseen task. That is, the objective is the same as that of domain randomization, curriculum learning, etc., without morphology optimization, i.e., fixed morphology.  I believe that such baseline approaches, including (the original) enhanced POET, must be included and the advantage of the proposed approach over these baseline must be empirically shown."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459273460,
                "cdate": 1700459273460,
                "tmdate": 1700459273460,
                "mdate": 1700459273460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J4wgFZ5zOT",
                "forum": "BgzE4zwkFW",
                "replyto": "uCO3Ds5Hhe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for your response. We would want to further address your concern as follows:\n- The evaluation tasks presented in the paper are characterized by a higher level of difficulty and generality compared to either morphology optimization alone or domain-specific generalization. Specifically, our objective is to acquire **both a general morphology and general policy** that can be directly applied to unseen environments and tasks without the need for further fine-tuning. To the best of our knowledge, the majority of existing work, including the baselines we have evaluated, does NOT directly tackle this problem and was NOT specifically developed to address this particular issue. To provide equitable comparisons, it is necessary to make some modifications to some baselines in experimental comparisons. **The comparisons have already incorporated enhanced POET, as indicated in Figure 2 and the table provided in our rebuttal.** \n- Our current discussion is about \"what is a good morphology or a policy for the test/inference phase\". In our introduction section (and many places), we are discussing the **good morphologies that exhibit higher learning efficiency during the training phase, rather than the test phase** (since the adaption_cost =0 for the morphology evolved by MECE in the test). The MECE framework constructs a mechanism that encourages $\\pi_m$ to discover the morphology that can achieve higher learning progress in diverse environments, utilizing the reward design $r_m$.\n\nPlease let us know if we misunderstood your comment regarding our response to your first question."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482825172,
                "cdate": 1700482825172,
                "tmdate": 1700495618135,
                "mdate": 1700495618135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AI2yo9zU9I",
            "forum": "BgzE4zwkFW",
            "replyto": "BgzE4zwkFW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to co-optimize both the morphology and environments of robots. The morphology and controller of the robot is updated, while the environment is progressively changed. The result of the employed co-evolutionary approach are environments that progressively get more complex, providing a good learning signal for the agent. The approach is compared to ablated versions, which demonstrate that the co-evolution of morphology and environment is beneficial, in addition to comparisons with modifications of methods such as POET, which typically only optimize the robot\u2019s controller but not its morphology."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting approach that could make robots more robust to varying environments\n- Good ablation baseline comparisons"
                },
                "weaknesses": {
                    "value": "- Environment modifications seem limited (e.g. only environment roughness in the case of the 2D environment)\n- Comparisons to other methods are a bit ad hoc, e.g. as the authors note, POET was not developed to deal with changing morphologies. In addition to randomly sampling environments here, I would suggest a slightly more advanced baselines that samples environments of increasing complexity\n \nMinor comment:\n\n\"CMA-ES (Luck et al., 2019) optimizes robot design via a learned value function.\u201d -> their method is not called CMA-ES. CMA-ES is used an evolution strategy for  optimisation"
                },
                "questions": {
                    "value": "- \"When the control complexity is low, evolutionary strategies have been successfully applied to find diverse morphologies in expressive soft robot design space\u201d -> how does the control complexity in this paper compare to the one by Cheney et al.? One could say the soft robots in Cheney et al. (2013) are more complex than the robots co-evolved in this paper.\n- How expensive is the approach of co-evolving the three different policies? And how does the computational complexity compare to the other baseline approaches?\n- It would be good to see some pictures of the evolved environments\n- What would happen if you start 3d-locomotion and gap-crossover with the same initial robot as in 2d-locomotion? There already seems to be a lot of bias given with the initial design."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833013742,
            "cdate": 1698833013742,
            "tmdate": 1699636127300,
            "mdate": 1699636127300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y4HwUoh4UV",
                "forum": "BgzE4zwkFW",
                "replyto": "AI2yo9zU9I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments and suggestions!"
                    },
                    "comment": {
                        "value": "Thank you for the suggestions! we have revised the draft by following your comments. We address your questions as below:\n\n> The control complexity compared to Cheney et al..One could say the soft robots in Cheney et al. (2013) are more complex than the robots co-evolved in this paper.\n- Whether a soft-bodied robot is more complex than a rigid-bodied robot is still an open problem with different opinions from different studies. In this paper, we only focus on the optimization in the space of rigid-bodied robots. We will make it clear in the new version. \n\n> How expensive is MECE co-evolving the three different policies?\n- Training MECE is not costly compared to baselines. For example, MECE took around 44 hours to train on a standard server with 8 CPU cores and an NVIDIA RTX 3090 Ti GPU, while Transform2Act took around 63 hours on the same server.\n\n> Comparison with POET.\n- Yes, we agree that POET is not designed for changing morphologies. But this baseline was required by a previous reviewer. For ensure a fair comparison, we have made several modifications (detailed in point (3) of the ''Baseline'' paragraph in Section 5.1).\n\n> \"I would suggest a slightly more advanced baselines that samples environments of increasing complexity\"\n- Thank you for the suggestion! We already included such a baseline in our comparison (e.g., in Figure 2): Enhanced POET starts with a simple environment and then gradually creates and adds new environments with increasing diversity and complexity.\n- As mentioned in the paper, MECE stands as the pioneering approach in discussing co-evolution between the agent's morphology and the training environment. Finding baselines that align properly with this settings might be challenging.\n\n> What happened if starting with the same initial robot of 2d-locomotion in other environments?\n- The initial agent in gap-crosser is the same as that in 2d-locomotion. We tried to initialize the same agent in 3d-locomotion but it didn't work. This is because 3d-locomotion has one more dimension (XYZ-plane) and it is hard to ensure the final performance of evolving morphologies without initial limitations. \n\n> \"It would be good to see some pictures of the evolved environments.\"\n- We will release the code with videos."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897050468,
                "cdate": 1699897050468,
                "tmdate": 1699897050468,
                "mdate": 1699897050468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q6EK6T6Van",
                "forum": "BgzE4zwkFW",
                "replyto": "AI2yo9zU9I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Would you mind confirming if your concerns have been addressed by our rebuttal?"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWould you mind validating whether our rebuttal addresses your concerns:\n\n- We have explained the issues of the experiments in MECE.\n- The computation of MECE is not cost.\n- We will release the source code with videos.\n\nPlease feel free to ask any additional queries after reading our rebuttal. During this author-reviewer discussion period, we hope to catch and answer any questions.\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393618835,
                "cdate": 1700393618835,
                "tmdate": 1700393618835,
                "mdate": 1700393618835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6OS6Fw1Ix1",
                "forum": "BgzE4zwkFW",
                "replyto": "Y4HwUoh4UV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1960/Reviewer_QUnH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I'm still in doubt about \"There already seems to be a lot of bias given with the initial design.\" One could say that the final designs in your paper are only (slight) modifications of the designs already given at the start. You say \"We tried to initialize the same agent in 3d-locomotion but it didn't work. This is because 3d-locomotion has one more dimension\" but shouldn't the algorithm be able to evolve such a creature? It seems rather limiting if most of the desig has to be given by the human experimenter.\n\nI saw the POET baseline but this is not exactly the same as \"I would suggest a slightly more advanced baselines that samples environments of increasing complexity\"? I had a much simpler approach than POET in mind, something more similar to adaptive domain randomisation (e.g. used here: https://arxiv.org/abs/1910.07113)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513275223,
                "cdate": 1700513275223,
                "tmdate": 1700513275223,
                "mdate": 1700513275223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]