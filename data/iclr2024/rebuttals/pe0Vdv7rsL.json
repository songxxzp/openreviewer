[
    {
        "title": "Graph Transformers on EHRs: Better Representation Improves Downstream Performance"
    },
    {
        "review": {
            "id": "QuXagndXKc",
            "forum": "pe0Vdv7rsL",
            "replyto": "pe0Vdv7rsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_CybP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_CybP"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a BERT-based model to represent EHR with a graph-based time-aware visit embedding to better capture the implicit graphical structure of EHR data and the temporal relationships between visits. The paper shows better performances in multiple tasks on the MIMIC-IV dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper demonstrates the importance of temporal effects in the EHR. This is insightful for research on EHR.\n\n- Comprehensive studies are conducted including various tasks and ablation settings. The confidence intervals are provided."
                },
                "weaknesses": {
                    "value": "- The proposed method is a combination of building blocks originated from several previous papers, e.g. graph transformer, BEHRT. It is a good application paper that utilizes all these aspects, but the methodological impact is limited.\n\n- In the ablation study section, the author claims that \"both pre-training strategies seem to have a great impact on the overall results, as showcased by the second, fourth, and fifth rows.\". However, in fact, Table 3 shows the performance differences between experiments with and without NAM, MNP, and GT are marginal. The confidence interval of the last row and the first row (the simplest version) even overlap. The simple linear model seems to already perform very well compared to GT. A better way of justifying the impact of pretraining can be directly using linear probing on the frozen pre-trained representation.\n\n- The power of pretraining seems to be limited by the size of the dataset (around 20K patients). Under this condition, pretraining a large model may not significantly outperform a simple linear model. It will be more exciting to apply the proposed method to a larger dataset."
                },
                "questions": {
                    "value": "- How is the F1 score calculated? Which threshold is picked? Some experiments with similar AUPRC result in substantially different F1 scores.\n\n- What is the difference between NAM and MNP? It seems they can be merged into one loss."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607734632,
            "cdate": 1698607734632,
            "tmdate": 1699637091415,
            "mdate": 1699637091415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JLO0aVByzT",
                "forum": "pe0Vdv7rsL",
                "replyto": "QuXagndXKc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Good application paper, limited methodological impact**\nWhile our study, at its core, is application-driven (i.e., EHR analysis), we believe it presents several fundamental algorithmic contributions. A key contribution is presenting an effective method to extract visit-level embeddings for the transformer-based methods, instead of the common code-level embeddings. Making such pipeline work required many engineering and design considerations, especially using a graph-based approach. We also show that our pipeline helps with the quadratic complexity of transformer-based methods. Moreover, we present two new pretraining methods, that are specifically tailored to our design. We have made adjustments to the Intro to more clearly present our technical contributions. \n\n**Ablation study clarification**\nWe agree that the linear model already performs well in comparison to the other baselines, however, we believe that the impact of pre-training should be compared using the same architecture baseline. That is, one should compare the results of the linear model with pre-training (2nd row) and without pre-training (1st row), and compare the GT model with pre-training (4th/5th row) and without (3rd row) pre-training. By doing so, it can be seen that when using GT, 8 of the 12 reported metrics using the pre-training outperform the GT baseline (these differences are also statistically significant using a t-test). Additionally, we believe that the true impact of the pre-training process is better highlighted by the final version of the model, which uses the entire two-step pre-training process. Nevertheless, we agree that the impact of pre-training on the linear model, while showing some improvements, remains marginal and we have made this aspect clearer in the manuscript.\n \n \n**Use a larger dataset**\nWe have now run all experiments on a large cohort of the All of Us dataset (a very large dataset of ~1 million individuals). The Experiments section is updated accordingly. \n \n \n \n**F1 score clarification**\nFor all experiments run, the F1 scores were calculated using the \u201ctorcheval\u201d library, using the standard formula and a threshold of 0.5. The disparity in the F1 scores can probably be explained by a poorer calibration of the models where the threshold of 0.5 is suboptimal. We added a short note to the Experiments section about this. \n \n**NAM vs MNP**\nWhile NAM and MNP share some similarities, they are two different tasks. NAM is in essence similar to Masked Language Modeling, where for each graph, we randomly mask one or more nodes and use the nodes\u2019 neighborhoods to retrieve the original nodes\u2019 attributes. In MNP, we remove one node from the graph and use the graphs\u2019 embeddings to retrieve the missing nodes. Thus NAM is a node-level task, while MNP is a graph-level task. Additionally, NAM is performed using the Graph Transformer only, while MNP uses the entire network, having access to the past/future visits. To address the confusion, we have modified Figure 4 and changed the description of this section to make more references to the illustration for better understanding."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723199348,
                "cdate": 1700723199348,
                "tmdate": 1700723199348,
                "mdate": 1700723199348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ppqmwdilni",
            "forum": "pe0Vdv7rsL",
            "replyto": "pe0Vdv7rsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_efXj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_efXj"
            ],
            "content": {
                "summary": {
                    "value": "The authors presented an approach towards extending graphical neural networks to temporal domain using transformers for modeling EHRs. Past research has shown the importance of modeling EHR modalities as GNN that can better capture the inherent correlation better than a flat data structure. However, nuance of EHR data, especially around longitudinal aspects, makes it important to also capture the temporal dynamics. The authors compared their proposed method with several baselines and reported strong results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Some of the key strengths of the paper are as below\n- the authors have proposed a hybrid architecture combing GNN and transformer architectures to capture both the spatial and temporal dynamics of EHR data. Key contributions around this proposed architecture are around identifying some of the issues for extending GNN to temporal domain and propose a multi stage pre-training method to capture the dynamics accurately\n- The authors compared their methods against strong baselines and reported significant performance improvements\n- they key insights around the model being able to capture longer medical histories is very interesting and adds makes the model more applicable\n- The authors have also added ablation studies to capture the importance of their training strategy"
                },
                "weaknesses": {
                    "value": "The paper can be improved upon by addressing the following aspects\n - The method description can be improved upon. Consider adding an illustration of the training strategy and describing the methods using the illustrations\n- the authors can also consider adding sub-group analysis to further strengthen the claims around model performance\n- While being cognizant of the page limits, it would have been interesting to analyze some of the inferred graphical networks at individual example level"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764622052,
            "cdate": 1698764622052,
            "tmdate": 1699637091279,
            "mdate": 1699637091279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VHzcnVUcIU",
                "forum": "pe0Vdv7rsL",
                "replyto": "Ppqmwdilni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Improve Method, new illustration**\nWe replaced Figure 1 (architecture visualization) with a new improved one. We have also updated the Method section to improve the descriptions. Moreover, we added a new section (Appendix B) containing the new Figure 4, to improve the illustration of the training strategy. \n \n**Add sub-group analysis**\nWe have performed a series of new experiments on several patient subgroups (sex and race/ethnicity). We report the results in the Appendix (due to space limitations) and refer to them in the main text. \n \n**Inferred graphical networks at individual level**\nWe added a new section (C) in the Appendix to address this suggestion. The section includes Figure 5, which visualizes the learned graphical structure for an individual patient in two different formats (a heatmap and a graph)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723170854,
                "cdate": 1700723170854,
                "tmdate": 1700723170854,
                "mdate": 1700723170854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qhMqbizG52",
            "forum": "pe0Vdv7rsL",
            "replyto": "pe0Vdv7rsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_omW6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_omW6"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on patient representations and developed a tailor transformer architecture leveraging both graph transformer and BERT-like encoder only transformer. Evaluation on MIMIC-III and eICU showed improved performance on two tasks, mortality and length of stay."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written and the developed method is easy to follow.\n\nThe architecture sounds reasonable to me."
                },
                "weaknesses": {
                    "value": "Baselines are quite old. The most recent baseline is 2021.More literature research is necessary. For example,  [1,2,3] have dome similar things. Consequently, I am not convinced by the advantage of bringing GNN into BERT, \n\n[1] Hypergraph Transformers for EHR-based Clinical Predictions\n[2] EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models\n[3] Unsupervised pre-training of graph transformers on patient population graphs"
                },
                "questions": {
                    "value": "What do authors would like to convey in the title \"BETTER REPRESENTATION IMPROVES DOWNSTREAM PERFORMANCE\"\nWhat's the definition of better and how to obtain it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795047419,
            "cdate": 1698795047419,
            "tmdate": 1699637091157,
            "mdate": 1699637091157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7NnN3on6nd",
                "forum": "pe0Vdv7rsL",
                "replyto": "qhMqbizG52",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Title clarification**\nWe were referring to a \u201cbetter representation\u201d of EHRs in the embedding space compared to the representations achieved by the current mainstream transformer-based or GNN-based methods (as discussed in the Intro). We added a sentence to the abstract to clarify this. Nevertheless, we appreciate specific suggestions from the reviewer to improve the title. \n\n**Older baselines**\nWe have expanded and improved our literature research. We discuss our rationale for the current baselines we picked (pls, see response to 74P1, too). We now added two new (2022 and 2023) baselines (a total of 7), as well as a large new dataset. \n\nOut of the three baselines suggested by the reviewer, we now include the first one (thanks!). The second study (EHRSHOT) came out this summer. Except for a few days, the complete data/model was never released due to the additional approvals needed (pls check the study's GitHub repo). The third study [3] is indeed very relevant, but many methodological details and several key preprocessing steps are missing from the presented code, and despite our efforts, we could not reproduce the results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723147515,
                "cdate": 1700723147515,
                "tmdate": 1700723147515,
                "mdate": 1700723147515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W2JxvHeHgM",
            "forum": "pe0Vdv7rsL",
            "replyto": "pe0Vdv7rsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_74P1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8705/Reviewer_74P1"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents GT-BEHRT, an innovative approach that integrates graph-based and transformer-based methodologies to enhance the analysis and predictive accuracy of electronic health records (EHRs). The technique specifically addresses the sparsity in EHR data and the need for capturing complex, graph-type relationships. GT-BEHRT combines graph transformer-derived embeddings for individual visits with a BERT-based framework, facilitating richer patient representations over extended EHR sequences. This method also features a novel two-step pre-training process that further refines the model\u2019s capacity to decode both graphical and temporal patterns in the data. As a result, GT-BEHRT achieves leading performance across diverse medical predictive tasks, indicating its robustness and versatility.\n\nMain Contributions:\n\nGT-BEHRT Design: A new hybrid model that integrates graph transformer and BERT-based architectures to better handle the temporal and graphical nature of EHR data.\n\nTwo-Step Pre-training Strategy: A unique pre-training process that enhances the model's ability to understand complex relationships in EHRs, improving performance on predictive tasks.\n\nSuperior Predictive Performance: Through its innovative approach, GT-BEHRT sets a new benchmark for state-of-the-art results in various standard medical predictive tasks, demonstrating its potential to significantly impact healthcare analytics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present a commendable effort in intertwining GNN and Transformer methodologies, showcasing an innovative approach to a topic gaining traction in the field. Their literature review is generally thorough, but it's surprising to note the omission of recent contributions from Jure Leskovec's lab, which bear resemblance to this work. While the essence of the paper is intriguing, a deeper exploration of their model's specifics would have provided more comprehensive insights. Overall, the strength of this paper lies in its novel perspective on a burgeoning issue, even if there are areas left to be further elaborated."
                },
                "weaknesses": {
                    "value": "The paper, while advancing an intriguing new architecture, does not fully address the breadth of state-of-the-art works within the Graph Neural Network (GNN) sphere. A more exhaustive acknowledgment and discussion of leading GNN research would have provided a richer context for the authors' contributions. Additionally, while the authors assert that their proposed architecture outperforms existing models, the paper falls short in offering visual depictions of the architecture. Such illustrations are crucial for readers to fully grasp the design and the innovations it purports to bring. Furthermore, the rationale behind the selection of certain models as baselines is not sufficiently elucidated. This lack of detailed justification and visual support may leave the reader questioning the thoroughness of the comparative analysis and the foundation of the authors' claims. The paper would benefit from more comprehensive visual materials and a deeper discussion on model selection to solidify its standing within the current scientific discourse."
                },
                "questions": {
                    "value": "Why did you only used one dataset to test you model?\nHave you looked at Michele Moore works?\nWhy you didn't compare your results with some of the state of the art works such as GMAI?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823898133,
            "cdate": 1698823898133,
            "tmdate": 1699637091004,
            "mdate": 1699637091004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vETvRdhlSI",
                "forum": "pe0Vdv7rsL",
                "replyto": "W2JxvHeHgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8705/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**More on GNN research** \nWe have expanded our Related Work section to further discuss the leading GNN studies and provide more context on our work.\n\n**Visual depictions of the architecture**\nWe replaced the original Fig 1 with a new one, containing improvements to clarify the process. We have also added a new Fig 3 (in the Appendix) to clarify the pretraining process (also part of our contributions). \n\n**Rationale for baselines**\nOut of many available choices, we selected a subset (seven) baselines that i) use either transformer or GNN-based architectures, ii) are recent (2021+) or popular (Dipole), and iii) offer source codes. We also added two new baselines. We added a short note to the paper discussing this. \n\n**Why one dataset**\nWe now added the very large and public All of Us dataset. We note that many of the publicly accessible EHR datasets are not quite a good fit, as they contain ICU data (like those on PhysioNet; hence have only short temporal patterns) or do not contain rich code representations for the visits (like Synthea; hence have weak graph representations).\n\n**Moore\u2019s work, SOTA, and GMAI**\nThe Perspective study by Moore et al. refers to the growing interest in using foundation models as generalist medical AIs (GMAIs). While very interesting and relevant, only a couple of studies have so far presented an actual model demonstrating generalist-level capabilities (most notably Google\u2019s Med-PaLM). These studies (i.e., those that are truly internet-scale) do not offer their codes (API access also has limitations, especially on patient data). Plus, almost all of these studies focus on unstructured data (like text and images), and none readily uses structured data (like those we use). Inputting EHRs as prompts to such LLMs is doable, but needs separate engineering. We briefly discuss encoder-based (like ours) vs decoder-based (like GMAIs) in the Intro. \nNevertheless, we now added two new (SOTA) baselines (total 7 baselines)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723065005,
                "cdate": 1700723065005,
                "tmdate": 1700723065005,
                "mdate": 1700723065005,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]