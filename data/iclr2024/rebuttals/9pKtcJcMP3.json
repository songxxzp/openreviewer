[
    {
        "title": "Video Language Planning"
    },
    {
        "review": {
            "id": "MJEdoyogFt",
            "forum": "9pKtcJcMP3",
            "replyto": "9pKtcJcMP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe an algorithm for video language planning (VLP) that takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP leverages (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP is evaluated on simulated and real-world robotic platforms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Overall well-written paper, which is easy to follow\n2. Great to see real-world evaluation despite the simplicity of tasks"
                },
                "weaknesses": {
                    "value": "1. One of the key contributions of VLP is the use of text-to-video models as dynamics models. But authors experiments and baselines are not convincing about the value of using these models as dynamics models. \n* Authors use relatively simple tasks to evaluate VLP. Why not use a text-based LLM or something like this: https://arxiv.org/abs/2106.00188 as a dynamics model instead of the text-to-video model given the simplicity of the evaluation tasks? Would have been great to see such a baseline to really understand the edge that video dynamic models provide.\n * It would have also been great to see VLP being applied to more complex tasks e.g., procedural planning (https://arxiv.org/abs/1907.01172). Generating a walk-through plan for a procedural task is a convincing use case for video dynamics model as compared to say an LLM-based dynamics model.\n2. It is also not clear which use-cases will really require VLP as compared to existing LLM-based planning models or more generalized agent architectures such as GATO (https://arxiv.org/abs/2205.06175) or decision transformers. To that end, it would have been useful to see a broader set of baselines in the paper. Some baselines to consider -- RAP: https://arxiv.org/abs/2305.14992 (uses MCTS + LLM dynamics model similar to VLP's tree search+dynamics model), Visual Language Planner: https://arxiv.org/pdf/2304.09179.pdf (learns a multimodal dynamics model and policy by finetuning an LLM), VIMA: https://arxiv.org/pdf/2210.03094.pdf (no dynamics model but multimodal planner, could truly bring out the value of video-based dynamics via such a comparison?)\n* Authors also say HiP is the closest to their work in terms of an approach but do not provide a performance comparison in their evaluation. Why? Would love to see it.\n3. VLP's inference is slow.  Given that VLP primarily focuses on robotic applications right now, it is unclear whether VLP is suitable for real world deployment. Slow runtime should also be reported as limitations in the main paper (rather than in the appendix).\n4. It is unclear if the authors are considering open-sourcing the code and the models. Without that the reproducibility and value of the work for the community reduces. I encourage the authors to discuss open-sourcing plans in their rebuttal."
                },
                "questions": {
                    "value": "- Unclear how long of a video does the video models produce. \n- Authors say that they train separate models per domain but it is unclear what is a domain. Is tabletop manipulation in sim and real world considered same or different domain?\n- In table. 1, why not ablate VLP without the video dynamics model? Given my concerns in the weakness section, would have loved to see this ablation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733410447,
            "cdate": 1698733410447,
            "tmdate": 1699636455935,
            "mdate": 1699636455935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aL9HFLiNZI",
                "forum": "9pKtcJcMP3",
                "replyto": "MJEdoyogFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments. We respond to each concern below.\n\n**Text LLM for Model Dynamics.** To effectively use a LLM to model dynamics and plan on the precise manipulation settings we consider, the LLM must be able to accurately simulate changes in the visual state subject to each possible natural language action. This requires the first a text-based serialization of the objects in an image as well as resultant simulation based off this serialization. In appendix A.1 (Figure XV), we illustrate that existing LLMs, i.e. GPT-3.5, are unable to accurately simulate the fine dynamics changes of such a serialization. \n\n**Complex Tasks.** In our paper, we illustrate the ability to construct complex video plans across three separate platforms.  in Language Table, we illustrate how our approach can synthesize complex plans to precisely manipulate blocks to form a line. On a  7 DoF mobile manipulator, we illustrate how our approach can synthesize a complex plan to place fruits into a cabinet by first opening the cabinet, putting fruits in it, and finally closing it. Finally, on a 14 DoF dexterous manipulator,  we illustrate how our approach can synthesize  multi-camera video plans of dexterous manipulation involving stacking utensils, cups, and bowls on top of each other. We believe this already represents several axes of difficulty in synthesizing video plans. \n\n**Baseline Comparisons.** Since LLMs are unable to accurately simulate the detailed video dynamics for object manipulation, we cannot apply RAP to our setting. In terms of GATO and VIMA, we already compare VLP against a multimodal transformer, RT-2 which directly predict actions (which both GATO and VIMA can be seen as instances of)  and find that our approach substantially outperforms this baseline. We have added references to each of these papers in the baseline section and clarified our comparisons against them.\n\n**Comparison with HiP.** HiP leverages consensus between a text-to-video and LLM to synthesize a possible language instruction given the current visual observation of the scene. In VLP, we directly learn a VLM to predict text actions to execute given an image observation, which serves as an upper bound on the consensus operation in HiP. Since HiP does not do planning, its performance is upper bounded by the VLP (No Value Function) base which VLP substantially outperforms.  We have clarified this in the main paper.\n\n**Inference Speed.** We have added slow inference as a limitation of our approach. We believe that inference speed can be substantially improved by either distilling the diffusion model, or by using alternative generative decoding methods such as that in Phenalki.\n\n**Code and Models.** We will release an open-source codebase for reproducing the results in VLP by the camera-ready version of the paper. In terms of models \u2013 they are trained using a company's internal codebase, so we are unable to directly open-source the models that are trained in the paper. However, we will aim to release pretrained weights for models trained using the released open-source codebase."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506159415,
                "cdate": 1700506159415,
                "tmdate": 1700506159415,
                "mdate": 1700506159415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DnYqvsBbTG",
                "forum": "9pKtcJcMP3",
                "replyto": "aL9HFLiNZI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications.\n\nI am glad to see the actual numbers on how long does it take for the approach to generate the videos (30 minutes), but it is still not clear to me how long the videos are in the first place.\n\nI also appreciated a comparison to a text-based approach for dynamics simulation/state change prediction using GPT in the appendix. However, currently, this example is zero-shot/prompt-based. Given that video dynamics models used by the authors' approach are trained, a true comparison to text-based dynamics approach would have been something that also has access to some training examples -- at the least via in-context learning. I would encourage the authors to think of a fair apples-to-apples comparison between text-based dynamics models and their video dynamics models instead of their current straw-man comparison.\n\nLastly, while considering complex real world tasks as in procedural planning might be out of scope for this work, I would have appreciated atleast a comment on how complex of videos can the approach handle. While I agree that the task of putting fruit in a cabinet is a complex \"planning\" task,  the vision-based dynamics/videos of this task do not seem complex in comparison to what one would see/expect in real world environments -- think cluttered kitchen counter vs. mono-color table tops with sparse objects in the authors experiments. I'd like to understand if video dynamics models are really the answer to planning in such situations and I don't think the authors provide me that understanding right now.\n\nFinally, how would the authors approach compare to models that also learn to generate multimodal tokens e.g. this one: https://arxiv.org/pdf/2304.09179.pdf ?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696795342,
                "cdate": 1700696795342,
                "tmdate": 1700696795342,
                "mdate": 1700696795342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9p6MA2mDNq",
                "forum": "9pKtcJcMP3",
                "replyto": "MJEdoyogFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your response, please see our additional clarifications below.\n\n**Duration of Videos.** 30 minutes corresponds to the long horizon video plans shown on the website at https://vlp-iclr2024.github.io/ and corresponds to approximately 15-second videos (for example depicting a robot arranging a set of blocks into a line or moving blocks to a corner of the board). \n\n**Comparisons to Text-Based Dynamics Simulation.** We agree it would be better to compare with a text-based dynamics approach with access to training examples of serialized dynamics predictions. However, our existing training dataset only consists of sequences of images, as opposed to a text serialization, and there is no easy way to convert an image directly into a text representation to train a dynamics model. The most straightforward manner would be directly to tokenize images into multimodal tokens and predict dynamics in this manner and use this for planning (this is actually the first approach we experimented with -- see our response in generating multimodal tokens). If the reviewer believes this baseline is important, we are happy to add such a comparison in the final version of the paper.\n\n**Complexity of Videos.** The video models in our approach are based on those from [1], and as demonstrated in [1] on the Bridge dataset, the video models are able to accurately synthesize dynamics in cluttered kitchen counters as long as there is not significant partial observability (i.e. limited camera movement). However, we found that our approach does not work well when there is substantial camera movement (i.e. when someone is walking down a hallway into another room) as it can not preserve the world state across video frames.\n\n**Generating Multimodal Tokens.** The first approach we explored in our project was actually to directly use the existing PALM-E model to generate image goals using multimodal tokens. Unfortunately, we found that this approach was unable to accurately synthesize image goals in high resolution, perhaps due to the challenge of predicting 256 accurate tokens. Furthermore, this approach could not synthesize video due to context limits as decoding a single video required 16*256=4096 tokens (16x16 tokens per image multiplied by 16 frames).\n\n[1] Learning Universal Policies through Text-Guided Video Generation"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704939720,
                "cdate": 1700704939720,
                "tmdate": 1700711087562,
                "mdate": 1700711087562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "we64bdap7v",
            "forum": "9pKtcJcMP3",
            "replyto": "9pKtcJcMP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of long-horizon visual planning tasks by utilizing the robust generative capabilities of large vision-language models (VLM) and text-to-video models. The authors introduce the Video Language Planning (VLP) algorithm, which takes both a visual observation and a natural language goal as inputs and subsequently processes a series of instructions, video frames, and low-level controls. The effectiveness of their proposed heuristic value function and tree search procedure is well demonstrated through extensive long-horizon robot manipulation tasks across three hardware platforms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The long-horizon tasks pose significant challenges and matter in the field of robot manipulation. The proposed VLP algorithm has the potential to significantly improve the success rate of these tasks, as demonstrated in both simulated and real-world experiments in this paper\n- Collecting large-scale robot manipulation datasets can be a costly endeavor. VLP, on the other hand, harnesses the impressive generative and generalization capabilities of the latest VLMs and text-to-video models that are pre-trained on Internet-scale data. This approach can serve as valuable inspiration for other researchers looking to tackle challenging robot manipulation tasks without the need for expensive data collection.\n- The experimental results regarding the relationship between execution accuracy and planning budget provide valuable insights into the efficiency of the proposed VLP algorithm."
                },
                "weaknesses": {
                    "value": "The primary concern regarding this work is its potential for reproduction and adaptation to, e.g., other hardware platforms and low-cost budgets.\n- While the study tests the VLP algorithm on three hardware platforms, they are either relatively simple (i.e., having just one end effector) or self-designed. There is a question as to whether the well-trained VLP model could generalize or adapt quickly to other popular platforms. This limitation may reduce the paper's overall impact.\n- While video model inference is computationally expensive, its scalability is also a concern."
                },
                "questions": {
                    "value": "- Table 1 clearly illustrates that the Value Function utilized in VLP significantly enhances result accuracy. However, in many robotic tasks, it is common to have multi-modal policies. For instance, in the \"make a horizontal line\" task shown in Figure 2, there could be multiple ways to manipulate objects, resulting in various possible remaining steps, especially for images that are far from the final goal. It would be helpful to see whether the PaLM-E model can be fine-tuned to accommodate multi-modal trajectories.\n- While the paper properly discusses a few limitations, the limitation of the task horizon is unclear in this paper. In the appendix, the authors provided both VLP and other baselines with 1500 timesteps to complete a task. Does this imply that VLP may not be suitable for handling longer-horizon tasks? For example, a long-distance task, such as moving an object over a long distance (e.g., 10 meters) from one location to another.\n- In the introduction, the authors emphasize the potential benefits of VLP when working with incomplete videos that lack corresponding language labels. It would be helpful to see more detailed descriptions and accompanying experimental results to further demonstrate this capability, which are not found in this paper.\n- The authors have highlighted the issue of overestimation with the Value Function. It would be interesting to see if out-of-distribution images or goals might also contribute to this problem. If so, could the application of certain offline reinforcement learning algorithms, such as CQL, potentially offer a solution for addressing this issue?\n- Minor issues that may need careful proofreading:\n  - Sec. 2: a image goal-conditioned -> an\n  - as planning submodules Sec. 2.1 -> in Sec. 2.1"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815840654,
            "cdate": 1698815840654,
            "tmdate": 1699636455829,
            "mdate": 1699636455829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pPSpi641EA",
                "forum": "9pKtcJcMP3",
                "replyto": "we64bdap7v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and positive evaluation. We respond to each question below.\n\n**Multimodal Value Function.** We would like to clarify that in our experiments with PALM-E, our demonstrations to accomplish long-horizon tasks were already multimodal. For instance, demonstrations for the task of making a line were made by 10 different teleop contractors with very different strategies to arrange a line. To deal with this multimodality, our PALM-E model formulates the task of predicting the number of steps left to finish a task as a language prediction problem. We notice that when sampling from the model, it would thus generate multiple different predictions on the number of steps left to finish a task \u2013 corresponding to the different ways to solve this task. To get a single deterministic answer, we directly sampled to 0 temperature decoding of the number of steps left to finish a task, which corresponds to the most likely strategy to finish the task. We have added additional details in Appendix A.4.\n\n**Long Horizon Tasks.** We found that VLP had no difficulties with longer horizon tasks \u2013 we artificially set the task length to 1500 so that the evaluation of an episode did not take too long (as a single environment evaluation with 1500 steps already take 1 hour). \n\n**Incomplete Videos Lacking Language Labels.** We would like to clarify that the videos used to train VLP in both Language Table, as well as the 14-DoF dexterous manipulation setting, already lack full language labels. In both settings, videos of the full long-horizon task are collected by teleop workers. However, there is no way then to directly segment videos into corresponding language labels, as there is no clear language label for each consecutive temporal segment of the video. Instead, we then asked a separate group of workers to go through video and select and label any sub-snippets of the video that had coherent language labels. These sub-snippets would often overlap in time in the video and many parts of the video would not have consistent language instructions.  We have clarified this in the main paper as well as in Appendix A.3.\n\n**Overestimation of Value Function.** Yes, we indeed sometimes found that out-of-distribution goals would lead to overestimation of value function . We have added a reference to the CQL paper and methods in offline RL as ways to avoid the exploration of out-of-distribution states.\n\n**Typos.** Thank you, we have fixed the typos you mentioned."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506068229,
                "cdate": 1700506068229,
                "tmdate": 1700506068229,
                "mdate": 1700506068229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oewyrbTY4h",
                "forum": "9pKtcJcMP3",
                "replyto": "pPSpi641EA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\nI appreciate the additional illustrations provided to enhance the clarity of this paper. \nIt would be great if the fine-tuned 12B PALM-E and the RT2-PaLM-E model could be released."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739904037,
                "cdate": 1700739904037,
                "tmdate": 1700739904037,
                "mdate": 1700739904037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3d7alSwxMU",
            "forum": "9pKtcJcMP3",
            "replyto": "9pKtcJcMP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_uBLi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_uBLi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a general-purpose framework solving for robotics tasks in which planning is done using a video generative model, and then using the sampled video as a target, a low-level controller selects actions to be executed by the robot in order to reproduce the video. To improve the quality of long-term video generation, the authors propose a hierarchical approach in which a list of commands are generated in text-format in addition to the video, and together optimized to minimize an estimate of how close the task is to being completed. The authors demonstrate impressive performance for multiple synthetic and real-world robotics tasks. Importantly, due to the use of internet-scale pre-training for the vision-language model (VLM) and video generative model, the approach generalizes well in a zero-shot setting to novel lighting conditions, objects, and tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A great challenge for the machine learning community is to determine how recent advances in generative modeling can be used to advance robotics. I think the method proposed in this paper makes great strides towards this end. Both vision and language foundation models are combined to produce an impressive model-based goal-conditioned planner for robotics. Additionally, I found the paper very well written; I'm not an expert in robotics, but I found the paper easy and interesting to read. The discussion of the limitations at the end is especially insightful, as well as the visuals of video generative model failures in the appendix. The method is stated clearly and its efficacy is backed up with substantial qualitative and quantitative results."
                },
                "weaknesses": {
                    "value": "I think the main weakness of the current submission is that it does not include measurements of runtime for the proposed approach, for instance in Table 3. Planning in video space seems very expensive which could limit how well this approach could be used for robotics tasks that need to be executed quickly. Of course, these runtimes can always be improved, but it would be helpful to see what the runtime is with today's hardware.\n\nRelatedly, I'm curious if the authors have considered optimizing only the text sequence with planning in Algorithm 1, then following with per-clip video generation? It seems much more efficient to me to search through text instead of pixels. Based on Table 3, it seems there are large gains to be had from a more exhaustive search, which I'm guessing is much easier to do if the need to render pixels is eliminated."
                },
                "questions": {
                    "value": "In addition to the two main points mentioned in the weaknesses section, I have a few other minor questions:\n\n- Section 2.2 \"Vision-Language Models as Heuristic Functions.\" - is this model trained with regression, or the tokenized representation of the number of steps with the standard language modeling objective? If it's with the language modeling objective, do the authors choose the mode of the distribution over predicted number of steps when using the heuristic to plan in Algorithm 1?\n- Section 2.3 \"Replanning\" - is there a way to make this re-planning rate dynamic? Ideally, it would be possible to sense when reality has diverged from the video plan in a significant way that is not recoverable.\n- A.4 \"Video Models\" - what's the frequency of these videos, e.g. how many seconds does 16 frames correspond to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699220487187,
            "cdate": 1699220487187,
            "tmdate": 1699636455736,
            "mdate": 1699636455736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2VpXKEGS1k",
                "forum": "9pKtcJcMP3",
                "replyto": "3d7alSwxMU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and positive evaluation. We respond to each question below.\n\n**Runtime.** We reported the runtime of our approach in the original submission in Appendix A.2. Our approach takes approximately 30 minutes to synthesize a full long-horizon video and approximately 1 hour to execute a long-horizon task such as making a line of blocks. To make this more visible, we have explicitly added this sampling speed to the limitations sections of the paper. We believe that the runtime can be substantially improved by either distilling the diffusion process or using alternative decoding procedures such as that done in Phenalki.\n\n**Searching with Text.** While it would be desirable to search first for a text plan and then through video prediction, this is difficult because the precise text action to execute next in the text plans depends heavily on the exact continuous state the previous text plan ends on. As an example, consider the text instruction \u201cpush the green circle to the left\u201d. If the final goal is to group blocks by color, if the continuous execution of the text instruction pushes it to the left of the green square the next text instruction becomes \u201cpush the green circle slightly to the right\u201d, while if its still to the right of the green square the, the text instruction becomes \u201cpush the green circle slightly to the left\u201d. Thus to effectively plan, it is important to plan with both text and videos. \n\n**Heuristic Function.** The VLM is trained to predict the number of steps left to execute a task through language prediction on a tokenized representation of the steps left. This allows the VLM to sample a multimodal set of possible number of steps left until task completion. In practice, during search, we sample at 0 temperature from the VLM, corresponding to the mostly likely probability mode of the number of steps left to finish execution of the task. We have clarified this in Appendix A.4\n\n**Replanning.** Yes, replanning can be made dynamic by making replanning occur based off the pixel MSE distance between the current observation and the goal image. We have added a discussion of this in the method section of the paper.\n\n**Video Model Clarification.** The videos are approximately at 15FPS, so 16 frames of video corresponds to one second of video. The long video rollouts shown in the submission cite correspond to between 10-15 video rollouts chained together."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505967926,
                "cdate": 1700505967926,
                "tmdate": 1700505967926,
                "mdate": 1700505967926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "klXykK3FMt",
            "forum": "9pKtcJcMP3",
            "replyto": "9pKtcJcMP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework for planning in the space of videos and language by leveraging recent text-to-video models (over vision-language models (VLMs) that are trained on static images) for incorporating scene dynamics and object motions. They take as input an initial observation and a long-horizon language instruction and output a plan in the form of video and language specifications, by first prompting a VLM for language actions and rolling out a language-conditioned video model whose outputs are subsequently assessed by the VLM again for favourability towards task progress using a learned heuristic."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is easy to follow and well-structured with the main contributions listed clearly in the introduction alongwith grounding in related works and building blocks that make up this method (VLM, a video prediction model, and an inverse-dynamics model).\n2) The authors present strong model performance compared to 4 baseline methods on object rearrangement tasks, as well as deploy their method on multiple real-robot platforms."
                },
                "weaknesses": {
                    "value": "While the tasks chosen for this paper are claimed to be long-horizon, they are not challenging enough to showcase a big leap in this realm of tasks. For example, when grouping blocks by color, while the task may seem long-horizon given there are large number of blocks on the table to manipulate, there is no dependency between subgoal successes/failures. This makes the task solvable without any need to retain information for long horizons. Hence, I believe the chosen suite of tasks does not evaluate the model adequately for solving long-horizon tasks."
                },
                "questions": {
                    "value": "Are there any limitations induced by video prediction models, which as the authors identified can generate out-of-manifold frames during long-horizon video prediction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699258372231,
            "cdate": 1699258372231,
            "tmdate": 1699636455655,
            "mdate": 1699636455655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rLTGYNxOYc",
                "forum": "9pKtcJcMP3",
                "replyto": "klXykK3FMt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4737/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments. We address each of the above issues below.\n\n**Long Horizon Tasks.** In the Language Table domains considered by this work, we believe that there is still some dependency between subgoal successes/failures. In the pair blocks by colors task, a move on one colored block significantly impacts changes the behavior necessary to push another block to pair it. This is further amplified in the make line task, where one incorrect move in one block significantly changes the downstream means necessary in all other blocks. The difficulty of both this pair by color task and the make line task is analyzed in the MAGICAL robust imitation learning benchmark on synthetic 2D shapes [1].  Even in this simplified setting, this benchmark found that existing imitation learning techniques (without planning) get essentially 0 performance on both tasks,  indicating the necessity of reasoning between subgoal successes/failures.\n\nIn addition, outside of the Language Table domain, we also evaluate our approach on two other long-horizon tasks. We first illustrate how our approach to solve the task of placing all the fruit on a cabinet on a 7 DoF mobile manipulator, which requires reasoning about both when to open and close cabinets and what objects to put in a cabinet.  We further show how we can synthesize long-horizon video plans for 14-DoF dexterous manipulator to place utensils, cups and bowls together on top of each other. This task again requires a model to be able to reason about which bowls to place on top of each other, as will as cups and utensils. \n\n**Limitations.** We found several limitations with our method.  First, as an image is not a full state representation of the world, our approach cannot deal with occluded objects. Second, our video prediction model does not perfectly preserve the shapes of objects, and they often warp their shape over the video rollout.  Finally, the underlying generation speed of video is very slow (as we run a diffusion process to generate each video snippet) preventing the approach from be used for realtime control. We have added additional limitations in the limitations section of the paper in Section 5.\n\n[1] Toyer et al. The MAGICAL Benchmark for Robust Imitation"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4737/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505895697,
                "cdate": 1700505895697,
                "tmdate": 1700505895697,
                "mdate": 1700505895697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]