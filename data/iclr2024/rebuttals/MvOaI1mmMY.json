[
    {
        "title": "Learning Label Refinement and Thresholds for Imbalanced Semi-Supervised Learning"
    },
    {
        "review": {
            "id": "uIP1QxWfRA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_PXgw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_PXgw"
            ],
            "forum": "MvOaI1mmMY",
            "replyto": "MvOaI1mmMY",
            "content": {
                "summary": {
                    "value": "The authors propose an algorithm termed SEVAL for imbalanced SSL which trains the algorithm on class imbalanced dataset consists of both labeled and unlabeled sets. To relieve classifier bias and thereby enhance the quality of pseudo-labels, SEVAL develop a curriculum for adjusting logits, and they also establish a curriculum for class-specific thresholds. Experimental results on CIFAR-10, 100 and STL-10 verify effectiveness of the proposed algorithm."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Code is submitted.\n\nMany studies are reviewed in Section 2.\n\nThe paper is easy to read."
                },
                "weaknesses": {
                    "value": "There is no theoretical ground for the proposed algorithm. \n\nFor CIFAR-10, the experimental settings are not enough. For example, other LTSSL (long tailed semi supervised learning) studies such as  DARP, DASO, SAW conducted experiments on CIFAR-10 with changing both imbalanced ratio of labeled and unlabeled sets.\n\nThe proposed algorithm appears to require extensive hyperparameter tuning.\n\nThe proposed algorithm and baseline algorithms are combined with only FixMatch, without being combined with ReMixMatch (other LTSSL studies such as DARP, ABC, SAW and CoSSL conducted experiments with combining their algorithms with both FixMatch and ReMixMatch).\n\nThe proposed algorithm make class balanced validation set from the train set, which seems not realistic. For example, for the iNaturalist dataset, the most minor class has only 2 samples, and in this case, it would be hard to make validation set from the 2 samples."
                },
                "questions": {
                    "value": "I have no question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1062/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1062/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1062/Reviewer_PXgw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697371822051,
            "cdate": 1697371822051,
            "tmdate": 1699937030241,
            "mdate": 1699937030241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UfLf3F47P5",
                "forum": "MvOaI1mmMY",
                "replyto": "uIP1QxWfRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PXgw"
                    },
                    "comment": {
                        "value": "> Q1. There is no theoretical ground for the proposed algorithm.\n \nNow we rephrase our method section and supply SEVAL with its theoretical motivation. We also derive the principal hypothesis behind the success of SEVAL. Like other SSL algorithms, SEVAL is mostly heuristic. However, we believe it is valuable as our analysis sheds new lights on the problem of pseudo-label refinement and we validate the effectiveness with extensive experiments.\n \n> Q2. For CIFAR-10, the experimental settings are not enough. For example, other LTSSL (long tailed semi supervised learning) studies such as DARP, DASO, SAW conducted experiments on CIFAR-10 with changing both imbalanced ratio of labeled and unlabeled sets.\n \nDue to page limitations, we only report key experiment results in main texts. We provide plenty of experiments in section B. For the experiments of CIFAR10-LT, now we provide the results when $\\gamma_l \\neq \\gamma_u$ and $\\gamma_l=\\gamma_u=150$. Take this into account, our experiments of CIFAR10-LT are equally or more extensive than previous works including DARP, DASO and SAW.\n \n> Q3. The proposed algorithm appears to require extensive hyperparameter tuning.\n \nSEVAL requires three core hyperparameters, including $t$ and two moments for curriculum learning ($\\rho_\\pi $ and $ \\rho_\\tau$). Other parameters, such as the length of the acquired curriculum ($L$), predominantly affect SEVAL's learning speed with minimal influence on the outcomes. Based on empirical observations, we reduce $L$ as the number of classes increases to enhance training efficiency. \n\nWhen compared with FixMatch, we only add two hyper-parameters because $t$ is also required for FixMatch. Considering that it is reasonable to keep $\\rho_\\pi $ and $\\rho_\\pi $ the same, we believe that SEVAL does not require extensive hyper-parameter tuning.\n \nNow we conduct sensitivity analysis of these parameters in section B.6 and we find SEVAL is not sensitive to hyper-parameters. We summarize the detailed hyper-parameter in section C.4 to facilitate reproducibility.\n \n \n> Q4. The proposed algorithm and baseline algorithms are combined with only FixMatch, without being combined with ReMixMatch (other LTSSL studies such as DARP, ABC, SAW and CoSSL conducted experiments with combining their algorithms with both FixMatch and ReMixMatch).\n \nWe think there might be some misunderstandings. We integrate SEVAL with Mean Teacher, MixMatch and ReMixMatch and report experimental results in Figure 4(a), showing better performance than other methods. Now we report more detailed and complete results in section B.4 and summarize implementation details in section C.4.\n \n> Q5. \u00ad\u00ad\u00adThe proposed algorithm make class balanced validation set from the train set, which seems not realistic. For example, for the iNaturalist dataset, the most minor class\u00ad has only 2 samples, and in this case, it would be hard to make validation set from the 2 samples.\n \nWe also think there might be some misunderstandings. As we demonstrate in the beginning of section 4, we make no assumption regarding the distribution of the validation set. To achieve so, we make our optimization function class-balanced, c.f. Eq.3 and Eq. 4. In fact, as we describe in the method section, we partition the training dataset into two subset with the same number of samples. Therefore, in most of our experiments, the validation set is also (highly) imbalanced.\n \nRegarding sample efficiency, we validate our algorithms using Semi-Aves, derived from the mentioned iNaturelist dataset. We find SEVAL works well under such settings and summarize the results in section B.2. SEVAL achieves this with frequency normalization and group-wise optimization, as mentioned in section 4.2 and described in details in section C. Now we also supply extreme experiment settings when very few labelled samples are available (tailed class contains 2 labelled samples or 40 labelled samples in total) in section B.3. We find SEVAL works well under such scenarios, demonstrating that SEVAL is labelled sample efficient.\n \n \n*We kindly ask the reviewer to reassess the merit of our work, considering both our technical contributions and the effectiveness of our method.*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359961307,
                "cdate": 1700359961307,
                "tmdate": 1700360157450,
                "mdate": 1700360157450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ecBOcglpc",
                "forum": "MvOaI1mmMY",
                "replyto": "UfLf3F47P5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Reviewer_PXgw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Reviewer_PXgw"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Your response has addressed many of my concerns. Therefore, I would like to raise my score from 3 to 4.\n\nHowever, since there is no score of 4 in the ICLR scoring system, I will keep it as 3 for now. It would be appreciated if the meta reviewer could consider reflecting my score as 4 in the final decision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364457918,
                "cdate": 1700364457918,
                "tmdate": 1700364457918,
                "mdate": 1700364457918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wOzS978NjK",
            "forum": "MvOaI1mmMY",
            "replyto": "MvOaI1mmMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_axcv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_axcv"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the imbalance issue in semi-supervised learning. They develop a curriculum for adjusting logits before generating pseudo labels from biased models. Then, they build a curriculum for class-specific thresholds. They evaluate it on FixMatch over several datasets and show the results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-structured and easy to follow. The method is clearly presented. The algorithm can be useful for deploying semi-supervised learning in real-world scenarios."
                },
                "weaknesses": {
                    "value": "1. Lack of technical novelty. All technical components have been established in the prior works, such as FlexMatch [1], Dash [2]. Specifically, Sec 4.1 -> [3]; Sec 4.2 -> [2, 4]; Sec 4.3 -> [1]. \n2. Lack of literature review. This work missed a couple of important baselines to compare with. For example, the methods presented in [4,5,6,7, 8]. \n3. Lack of experimental design. This work only evaluated it on FixMatch. However, in other pieces of literature [3,5,6,8], they evaluated on various SSL algorithms to show the effectiveness of the method, such as MixMatch and ReMixMatch. It would be helpful if the authors can present the results of these SSL algorithms. Moreover, FixMatch was proposed three years ago. It would be interesting to apply SEVAL to more recent algorithms, such as FlexMatch, CoMatch, or other SSL algorithms. \n4. The motivation is not clear to me. The main goal of SSL is to reduce the labeling effort. However, the experimental setting shows they still use a large amount of labeled data in the training data and a labeled validation set. FixMatch can achieve ~90% accuracy when only using 40 labeled data. It would be helpful if the authors could do experiments where the labeled data are scarce, e.g., 40 labeled data in total, while the unlabeled data can be highly imbalanced. \n5. The parameter is tuned on the validation set, which is not practical in my view. How do we prepare a labeled validation set in real-world scenario? If we have such labeling budget, why don't we choose other learning algorithms but stick to SSL? \n\n\n[1] Zhang, Bowen, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. \"Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling.\" Advances in Neural Information Processing Systems 34 (2021): 18408-18419.\n\n[2] Xu, Yi, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. \"Dash: Semi-supervised learning with dynamic thresholding.\" In International Conference on Machine Learning, pp. 11525-11536. PMLR, 2021.\n\n[3] Lai, Zhengfeng, Chao Wang, Sen-ching Cheung, and Chen-Nee Chuah. \"SAR: Self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4091-4100. 2022.\n\n[4] Guo, Lan-Zhe, and Yu-Feng Li. \"Class-imbalanced semi-supervised learning with adaptive thresholding.\" In International Conference on Machine Learning, pp. 8082-8094. PMLR, 2022.\n\n[5] Chen, Hao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Marios Savvides, and Bhiksha Raj. \"An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning.\" arXiv preprint arXiv:2211.11086 (2022).\n\n[6] Lai, Zhengfeng, Chao Wang, Henrry Gunawan, Sen-Ching S. Cheung, and Chen-Nee Chuah. \"Smoothed adaptive weighting for imbalanced semi-supervised learning: Improve reliability against unknown distribution data.\" In International Conference on Machine Learning, pp. 11828-11843. PMLR, 2022.\n\n[7] Wei, Tong, and Kai Gan. \"Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3469-3478. 2023.\n\n[8] Kim, Jaehyung, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. \"Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning.\" Advances in neural information processing systems 33 (2020): 14567-14579."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613085314,
            "cdate": 1698613085314,
            "tmdate": 1699636032617,
            "mdate": 1699636032617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1HNIa4c5do",
                "forum": "MvOaI1mmMY",
                "replyto": "wOzS978NjK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer axcv (Part 1/2)"
                    },
                    "comment": {
                        "value": "> Q1. Lack of technical novelty. All technical components have been established in the prior works, such as FlexMatch [1], Dash [2]. Specifically, Sec 4.1 -> [3]; Sec 4.2 -> [2, 4]; Sec 4.3 -> [1].\n[1] Zhang, Bowen, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. \"Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling.\" Advances in Neural Information Processing Systems 34 (2021): 18408-18419.\n[2] Xu, Yi, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. \"Dash: Semi-supervised learning with dynamic thresholding.\" In International Conference on Machine Learning, pp. 11525-11536. PMLR, 2021.\n[3] Lai, Zhengfeng, Chao Wang, Sen-ching Cheung, and Chen-Nee Chuah. \"SAR: Self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4091-4100. 2022.\n[4] Guo, Lan-Zhe, and Yu-Feng Li. \"Class-imbalanced semi-supervised learning with adaptive thresholding.\" In International Conference on Machine Learning, pp. 8082-8094. PMLR, 2022.\n[5] Chen, Hao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Marios Savvides, and Bhiksha Raj. \"An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning.\" arXiv preprint arXiv:2211.11086 (2022).\n[6] Lai, Zhengfeng, Chao Wang, Henrry Gunawan, Sen-Ching S. Cheung, and Chen-Nee Chuah. \"Smoothed adaptive weighting for imbalanced semi-supervised learning: Improve reliability against unknown distribution data.\" In International Conference on Machine Learning, pp. 11828-11843. PMLR, 2022.\n[7] Wei, Tong, and Kai Gan. \"Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3469-3478. 2023.\n[8] Kim, Jaehyung, Youngbum Hur, Sejun Park, Eunho Yang, Sung Ju Hwang, and Jinwoo Shin. \"Distribution aligning refinery of pseudo-label for imbalanced semi-supervised learning.\" Advances in neural information processing systems 33 (2020): 14567-14579.\n \nWe appreciate the reviewers for providing valuable insights from the literature. We do discuss and compare with [1, 2, 4, 5, 7, 8] in the manuscript and now add the discussion of [3, 6] in section 2. In particular, now we clearly show the differences between SEVAL and its counterparts either based on pseudo-label refinement or threshold adjustment in section 4.\n \nWe would like to underscore that our principal contribution centers around systematically reviewing and enhancing existing methods within a unified framework. We observe that these methods are suboptimal due to issues related to either uncalibrated confidence or unreliable difficulty estimation. We elucidate our discoveries using Bayes' theorem and illustrate them through the two-moon example. This provides novel insights into the challenges of imbalanced semi-supervised learning and guides the development of SEVAL, a straightforward yet effective technique for optimization.\n \nThe derived method, SEVAL, although share some similarity with the methods the reviewers mentioned (as they all refine pseudo-label or/and adjust thresholds), has some unique properties including building a theoretically more accurate classifier for unlabelled data (c.f. section 4.1) and accommodate more thresholding scenarios (c.f. section 4.2). We prove the feasibility of learning a curriculum of label refinement and thresholding parameters with a separate learning process using a partition of training dataset (c.f section 4.3). We think the technical contributions are therefore substantial. The extensive experiments further support the efficacy of SEVAL.\n \n> Q2. Lack of literature review. This work missed a couple of important baselines to compare with. For example, the methods presented in [4,5,6,7, 8].\n \nWe think there might be some misunderstandings. We compare with Adsh [4], ACR [7], DARP [8] and report the results under the same setting mainly in Table 2 and also other tables. We discuss their differences with SEVAL in related work and method sections. We also mention [5] in related work, and compare with a very similar work (CreST) along most tables. Now we add a more detailed discussion of SAW [7] in section 2, which focuses on reweighting of unlabelled samples, therefore is orthogonal to our studies which focus on pseudo-label refinement. We think SEVAL and SAW can work well together and leave the investigation for future works."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359880887,
                "cdate": 1700359880887,
                "tmdate": 1700359880887,
                "mdate": 1700359880887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8PaLq5e7Iv",
                "forum": "MvOaI1mmMY",
                "replyto": "wOzS978NjK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Reviewer_axcv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Reviewer_axcv"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Your response has addressed many of my concerns. I would like to raise my score from 3 to 3.5. However, since there is no score of 3.5 in the ICLR scoring system, I will keep it as 3 for now. It would be appreciated if the meta reviewer could consider reflecting my score as 3.5 in the final decision.\n\n However, the comparison is still not enough compared to the existing papers, especially to SimiS [5]. In [5], they did a comprehensively job in benchmarking all settings and algorithms in Table 1. I'm interested in the results of SEVAL under the settings of Table 1 & 3 in [5] to have a comprehensive comparison with previous methods."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414898225,
                "cdate": 1700414898225,
                "tmdate": 1700414920107,
                "mdate": 1700414920107,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2y2E45nJqs",
            "forum": "MvOaI1mmMY",
            "replyto": "MvOaI1mmMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_TEBc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_TEBc"
            ],
            "content": {
                "summary": {
                    "value": "The performance of pseudo label based semi-supervised learning (SSL) methods heavily depends on the quality of pseudo labels. In order to alleviate the adverse impact of class imbalance on the quality of pseudo labels, a class-specific calibration and a class-specific threshold are learned, thereby improving the performance of imbalanced SSL methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is of universality and can be embedded into other SSL methods to improve their performance on imbalanced classification problems.\n2. A large number of experiments are conducted to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The discussion of relevant work was not sufficient, so that the progressiveness and novelty of the article could not be highlighted. As mentioned in the experimental section, some existing methods also include both pseudo label refinement (PLR) and threshold adjustment (THA) modules. A detailed discussion is needed on the differences between the proposed method and these methods.\n2. Relevant theoretical analysis is needed to ensure the rationality of the proposed method. Otherwise, the proposed method is more like a combination of techniques.\n3. The expression of the article needs to be improved, such as\n(a) grammar error, e.g., in Page 4, \u201cIt which comprise two...\u201d\n(b) spelling error, e.g., in Page 9, \u201cWe compared with the ... inf Figure 8.\u201d\n(c) Figure 9 is not referenced in the main text.\n(d) inconsistent expression, e.g., $n_i $ and $n1$ are both appeared.\n(e) improper use of terminology, e.g., in Page 3, the meaning of \"data loader\" is unclear."
                },
                "questions": {
                    "value": "1. In experimental section, why not record specific evaluation criteria for imbalanced classification, such as precision, recall, and F1 score?\n2. Is the accuracy recorded in the experiment evaluated on the unlabeled samples used during the training phase? Is it evaluated on a test set that is not used during the training phase?\n3. In the experimental section, is the class distribution on the data set used in the test phase the same as the labeled sample set used in the training phase? In real applications, how to ensure that the class distribution on a small number of labeled samples is the same as the real class distribution?\n4. In Table 1, why is the imbalanced ratio of unlabeled data set unknown on the STL10-LT?\n5. In Table 1, what does $n_1$ mean? Is it the number of labeled samples from majority class or the number of labeled samples from minority class?\n6. On line 16 of Algorithm 2, why not use the final $\\pi$ and $\\tau$ learned in the previous stage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652671645,
            "cdate": 1698652671645,
            "tmdate": 1699636032544,
            "mdate": 1699636032544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zTEzmwQvnx",
                "forum": "MvOaI1mmMY",
                "replyto": "2y2E45nJqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TEBc (Part 1/2)"
                    },
                    "comment": {
                        "value": "> Q1. The discussion of relevant work was not sufficient, so that the progressiveness and novelty of the article could not be highlighted. As mentioned in the experimental section, some existing methods also include both pseudo label refinement (PLR) and threshold adjustment (THA) modules. A detailed discussion is needed on the differences between the proposed method and these methods.\n \nWe have rephrased our method section and highlighted the differences between SEVAL and other methods focusing on PLR and THA. We demonstrate distinct variations from both a theoretical standpoint and through easily comprehensible toy examples. In the context of PLR, SEVAL can yield more optimal results by leveraging Bayes' theorem, as elaborated in section 4.1. In the case of THA, SEVAL stands out as the sole method that estimates thresholds based on precision, offering a more reliable approach compared to existing methods, as discussed in section 4.2.\n \n> Q2. Relevant theoretical analysis is needed to ensure the rationality of the proposed method. Otherwise, the proposed method is more like a combination of techniques.\n \nWe have enhanced the method section, providing SEVAL with a robust theoretical foundation. Additionally, we have elucidated the core hypothesis underpinning SEVAL's effectiveness. Although SEVAL shares the heuristic nature typical of many SSL algorithms, its value lies in our analysis, offering novel insights into the pseudo-label refinement challenge. We substantiate SEVAL's efficacy through comprehensive experimental validation.\n \n> Q3. The expression of the article needs to be improved, such as (a) grammar error, e.g., in Page 4, \u201cIt which comprise two...\u201d (b) spelling error, e.g., in Page 9, \u201cWe compared with the ... inf Figure 8.\u201d (c) Figure 9 is not referenced in the main text. (d) inconsistent expression, e.g., n_{1} and n1 are both appeared. (e) improper use of terminology, e.g., in Page 3, the meaning of \"data loader\" is unclear.\n \nWe thank the reviewer for detailed suggestions. All the identified typos have been rectified.\n \n> Q4. In experimental section, why not record specific evaluation criteria for imbalanced classification, such as precision, recall, and F1 score?\n \nWe follow the literature and utilize accuracy as a main metric to evaluate the effectiveness of SSL algorithms. Of note, as the test data has a uniform distribution, the reported accuracy is equivalent to average recall of all classes and can represent the overall classification performance of all classes.\n \nWe agree with the reviewers on the importance of comprehensive evaluation metrics. Due to page limitations, we summarize the class-wise precision, recall and F1 score in appendix section E. From the detailed results, we find that SEVAL improves the classification model by being more sensitive to the minority classes.\n \n> Q5. Is the accuracy recorded in the experiment evaluated on the unlabeled samples used during the training phase? Is it evaluated on a test set that is not used during the training phase?\n \nWe report the accuracy on a test set that is not used during the training phase. We do not evaluate the performance on the unlabeled data as it is out of scope for the problem setting of SSL."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359750210,
                "cdate": 1700359750210,
                "tmdate": 1700359750210,
                "mdate": 1700359750210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iZgQUUjafW",
                "forum": "MvOaI1mmMY",
                "replyto": "2y2E45nJqs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TEBc (Part 2/2)"
                    },
                    "comment": {
                        "value": "> Q6. In the experimental section, is the class distribution on the data set used in the test phase the same as the labeled sample set used in the training phase?\n \nFollowing the literature, the distribution of test data is different from that of training data. In particular, the test data is assumed to follow a uniform distribution, because we care more about the overall classification performance.\n \n> Q7. In real applications, how to ensure that the class distribution on a small number of labeled samples is the same as the real class distribution?\n \nOur algorithms and experimental setting assume that the training data follows the long-tailed distributions, which is common for real world datasets. We make no assumption on the distribution of the validation dataset, as we declare in the beginning of section 4. This is achieved class-balanced optimization function, c.f. Eq.3 and Eq. 4.\n \n> Q8. In Table 1, why is the imbalanced ratio of unlabeled data set unknown on the STL10-LT?\n \nThe purpose of STL10 is to make use of the unlabelled data, which comes from a similar but different distribution from the labelled data, to learn better representation. Therefore, this dataset does not provide any class prior of unlabelled  samples, making it impossible for us to control the imbalanced ratio.\n \n> Q9. In Table 1, what does n1 mean? Is it the number of labeled samples from majority class or the number of labeled samples from minority class?\n \n$n_{1}$ represents the samples from the class that contains the most samples as $n_{c}$ is sorted in a descending order. Now we further clarify this in section 3.\n \n> Q10. On line 16 of Algorithm 2, why not use the final and learned in the previous stage?\n \nOn line 16 of Algorithm 2, we calculate the step of curriculum l. This is an index used to retrieve the learned parameters from the previous stage, i.e. line 18 and line 19. \n\nWe refrain from relying on the ultimate learned results due to the dynamic nature of optimal parameters throughout the training process. The shifting model bias and evolving classification capability during learning necessitate an alternative approach. Thus, in section 4.3, we introduce curriculum learning, aiming to acquire a set of parameters that adapt to the evolving learning process."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359776660,
                "cdate": 1700359776660,
                "tmdate": 1700360393198,
                "mdate": 1700360393198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ipXcj5FErj",
            "forum": "MvOaI1mmMY",
            "replyto": "MvOaI1mmMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_S1He"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1062/Reviewer_S1He"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a solution for threshold adjustment for pseduo-labeling based semi-supervised learning (SSL) method when there exists class-imbalance. The proposed solution leverages a seperately labeled validation set to optimize the thresholds, and can be applied as an add-on to existing SSL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper identified an important yet understudied question - how to learn pseudo-label thresholds when there exists class-imbalance.\n\n2. This paper proposed a novel threshold adjustment scheme that is suitable for label imbalanced scenarios.\n\n3. The proposed method appears to have notable improvements over state-of-the-art baselines."
                },
                "weaknesses": {
                    "value": "Major issues:\n\n1. This paper mandates the strong assumption that there must exist a sufficient amount of labeled validation data per class (at least 10 per class), which could easily be violated under semi-supervised learning settings. \n\n2. Highly relevant and overlapping method such as Adsh [1] is not included in the experiments.\n\nMinor issues:\n\n3. The motivation is not strong enough, in the abstract, the authors mentioned that existing dynamic threshold methods can cause bias in the pseudo-labeling. However, there seems a lack of detailed justification to substantiate how that method can cause bias. In addition, approaches such as FlexMatch and FreeMatch use class-dependent thresholds, shouldn't this approach reduce the bias caused by class imbalance?\n\n4. The presentation could be further improved, specifically on the layout of Figures 3-5 and figure 7-9.\n\n5. This paper is overall heuristic, the proposed solution lacks theoretical justification.\n\n[1] Guo, Lan-Zhe, and Yu-Feng Li. \"Class-imbalanced semi-supervised learning with adaptive thresholding.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "questions": {
                    "value": "1. The authors are encouraged to improve the proposed method in situations where the labeled set is not big enough.\n\n2. I failed to conceive how curriculum learning reconciles with the threshold adjustment, is there a process where SEVAL reordering the data? Or the curriculum in this context is different from the commonly referred curriculum learning [1]?\n\n[1] Bengio, Yoshua, et al. \"Curriculum learning.\" Proceedings of the 26th annual international conference on machine learning. 2009."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749353626,
            "cdate": 1698749353626,
            "tmdate": 1699636032459,
            "mdate": 1699636032459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yPxMBOyCz7",
                "forum": "MvOaI1mmMY",
                "replyto": "ipXcj5FErj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S1He (Part 1/2)"
                    },
                    "comment": {
                        "value": "> Q1. This paper mandates the strong assumption that there must exist a sufficient amount of labeled validation data per class (at least 10 per class), which could easily be violated under semi-supervised learning settings.\n \nSEVAL does not request more labelled samples than other SSL algorithms. Our ablation study demonstrates that SEVAL remains stable with varying amounts of validation data, ranging from 10 to 500 per class.\nIn order to further demonstrate the data efficiency of SEVAL, now we also supply extreme experiment settings when very few labelled samples are available (tailed class contains 2 labelled samples or 40 labelled samples in total) in section B.3. The experimental results demonstrate that SEVAL is labelled sample efficient and works well under such scenarios.\n \n> Q2. Highly relevant and overlapping method such as Adsh [1] is not included in the experiments.\n[1] Guo, Lan-Zhe, and Yu-Feng Li. \"Class-imbalanced semi-supervised learning with adaptive thresholding.\" International Conference on Machine Learning. PMLR, 2022.\n \nFollowing the request of the reviewers, now we add the comparison with Adsh in the experiments. We now update the results in Table.2. We find that Adsh improves model performance in most cases. Our algorithm is consistently better than Adsh in all the settings.\n \n> Q3. The motivation is not strong enough, in the abstract, the authors mentioned that existing dynamic threshold methods can cause bias in the pseudo-labeling. However, there seems a lack of detailed justification to substantiate how that method can cause bias. \n\nThe bias in pseudo-labeling arises from disparities in class distributions between training and test data. As discussed in Theorem 1, the optimal decision boundary for test data should consider differences in marginal distributions. Current solutions fall short, either by neglecting the class prior of test data or lacking calibrated probabilities, as illustrated in section 4.1. \nWe have emphasized the reason for class bias and the problems of current methods in section 4.1.\n\n> Q4: In addition, approaches such as FlexMatch and FreeMatch use class-dependent thresholds, shouldn't this approach reduce the bias caused by class imbalance?\n\nCurrent dynamic threshold methods (including FlexMatch and FreeMatch) mostly adjust thresholding based on maximum class probability (MCP), which is an approximation of recall per class. However, as we analyze in Section 4.2, the optimal thresholds should correlate with precision rather than recall. Therefore, as we demonstrate in a two-moon toy example, existing MCP-based methods would fail two out of the four threshold adjustment scenarios, therefore not optimal. In contrast, SEVAL can accommodate all the four cases. We also validate this in CIFAR10-LT experiments in section D. Moreover, we  show that the selected pseudo-labels are more correct than FlexMatch and FreeMatch in Figure 3(a).\nWe have rephrased the method section to emphasize these differences with more analysis and evidence."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359662636,
                "cdate": 1700359662636,
                "tmdate": 1700359662636,
                "mdate": 1700359662636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]