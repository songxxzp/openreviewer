[
    {
        "title": "Towards Better Evaluation of GNN Expressiveness with BREC Dataset"
    },
    {
        "review": {
            "id": "C9h7GayYLq",
            "forum": "aqqE1yS3RY",
            "replyto": "aqqE1yS3RY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_D7WE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_D7WE"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of benchmarking GNNs, especially being concerned with their expressive power. The authors propose the BREC dataset to tackle this problem. The dataset aims to fix some issues in existing datasets, in particular, the authors argue that existing benchmarks are not very granular and too easy with models either achieving perfect accuracy or random guessing. The authors further propose an evaluation technique that is more principled than existing techniques, that takes also into account numerical errors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors are very thorough in their work and consider a number of interesting points when designing the dataset. I particularly enjoyed the section that takes into account numerical errors induced for instance by floating point arithmetic errors. I think that the authors propose a principled approach to tackle this issue that could be valuable in general, outside of the scope of this specific work as well."
                },
                "weaknesses": {
                    "value": "While the work is valuable, I believe that it is not novel enough in its current state. The tasks are very synthetic and even though I agree with the authors that the BREC dataset seems more interesting than existing benchmarks such as CSL, in my opinion it seems to be a marginal improvement over such datasets in terms of the novelty factor. There is definitely a need for such datasets in the community and the new evaluation technique is interesting, but I am not convinced that in its current state the work is fit for ICLR. Overall, this feels more of an \"extension\" to current datasets than something very novel."
                },
                "questions": {
                    "value": "Would the authors be able to clarify any interesting findings that come from Table 2? It is a bit challenging for me to spot any significant trends the way it is currently ordered. It might be useful to further group the models based on their type. \n\nWould it be possible to clarify the contributions of the work? I understand that the work is proposing a new dataset and a new evaluation technique for it (which I find to be interesting and valid), but are there further novel contributions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8818/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8818/Reviewer_D7WE",
                        "ICLR.cc/2024/Conference/Submission8818/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698183234930,
            "cdate": 1698183234930,
            "tmdate": 1700488542477,
            "mdate": 1700488542477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IYY62xnDJy",
                "forum": "aqqE1yS3RY",
                "replyto": "C9h7GayYLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D7WE Part 1/2"
                    },
                    "comment": {
                        "value": "We thank reviewer D7WE for acknowledging the intricate design and insightful evaluation techniques. We answer your questions and address your concerns as follows:\n\n> W1: While the work is valuable, I believe that it is not novel enough in its current state. The tasks are very synthetic and even though I agree with the authors that the BREC dataset seems more interesting than existing benchmarks such as CSL, in my opinion it seems to be a marginal improvement over such datasets in terms of the novelty factor. There is definitely a need for such datasets in the community and the new evaluation technique is interesting, but I am not convinced that in its current state the work is fit for ICLR. Overall, this feels more of an \"extension\" to current datasets than something very novel.\n\nR1: \n\nWe thank the reviewer for highlighting a pivotal criterion concerning benchmarks. BREC is not limited to an upgraded version of previous datasets but provides many additional perspectives in expressive power analysis. A critical difference between BREC and previous datasets is that BREC is a benchmark for testing \"practical expressiveness\" for the first time. However, previous datasets only serve to verify \"theoretical expressiveness\". We explain their difference in the following.\n\n1. Previous expressiveness analysis mainly focused on \"theoretical expressiveness\", which represents the theoretical upper bound of the model under strong assumptions (deep enough layers, universal approximation of MLP, etc.). However, whether these models' practical implementations can achieve their theoretical power is constantly questioned and worth studying. Previous expressiveness datasets only served as a simple and incomplete verification due to their toy and repetitive components, making models tend to easily achieve 100% accuracy. Still, BREC is designed to test the \"practical expressiveness\", i.e., whether the theoretically powerful models can practically reach their expressiveness under more diverse, more difficult and larger scale settings than previous datasets.\n2. The \"practical expressiveness\" results also lead to many interesting findings. For detailed analysis, please refer to R2.\n\nFurther, we would like to highlight again that BREC fundamentally differs from existing datasets in multiple aspects:\n\n1. New evaluation method. Previous datasets do not consider evaluation techniques from an expressiveness perspective. They assign a label for each graph and consider it a classification task. It fundamentally restricts extending a toy dataset to a practical one.\n2. Diversity in graphs. Previous toy datasets have too few components/graphs, where one set of hyperparameters can achieve perfect accuracy. BREC has many more pairs of distinct non-isomorphic graphs advanced in difficulty, granularity, and scale, which can test models' true expressivity.\n3. Worthwhile experiment results. BREC not only verifies theoretical expressiveness but also provides more insightful findings. For example, BREC can test models without theoretical expressiveness characterizations. Half of the baselines we tested in Table 2 do not have a precise expressiveness analysis, but our experiment results make the quantification and comparison feasible. In contrast, previous datasets are restricted to 1-WL and 3-WL bounded models as their graphs are designed strictly following 1-WL/3-WL distinguishability. They cannot compare models between 1-WL and 3-WL, or beyond 3-WL."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321285690,
                "cdate": 1700321285690,
                "tmdate": 1700321285690,
                "mdate": 1700321285690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sad9EF4Z7x",
                "forum": "aqqE1yS3RY",
                "replyto": "C9h7GayYLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D7WE Part 2/2"
                    },
                    "comment": {
                        "value": ">Q1: \u201cWould the authors be able to clarify any interesting findings that come from Table 2? It is a bit challenging for me to spot any significant trends the way it is currently ordered. It might be useful to further group the models based on their type.\u201d\n\nR2: \n\nWe thank the reviewer for the valuable suggestion. We have revised Table 2 in the updated paper by grouping each type of model. We also summarize the findings below.\n\n1. There is gap between theoretical expressiveness and practical accuracy in many models. For example, PPGN is theoretically the same as 3-WL. However, our tests firstly show that it cannot reach its theoretical expressive power, even weaker than some models proven to be less powerful than 3-WL (e.g., SSWL_P). The gap can also explain why some models with high theoretical expressiveness do not perform well in real-world graphs.\n2. Some techniques are very useful for improving the practical expressiveness. For example, distance encoding [1] greatly improves the performance of NGNN+DE, KPGNN, I2-GNN, SSWL_P, etc. In comparison, node labeling, which is proved to bring the same expressiveness improvement for subgraph-based models [2] as distance encoding, do not perform as well in practice. These findings can inspire new research on designing practically more powerful positional/structural encodings.\n3. Generally, higher expressive power leads to better real-world performance. For example, subgraph-based models show excellent expressiveness results in our experiments, which align well with their state-of-art real-world performance [3]. Our results can serve as an efficient tool for analysis. A more detailed analysis can be found at R2 to reviewer ZHhj.\n\nReferences\n\n[1] Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning https://arxiv.org/abs/2009.00142\n\n[2] A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests http://arxiv.org/abs/2302.07090\n\n[3] Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman https://arxiv.org/abs/2306.03266\n\n>Q2: \u201cWould it be possible to clarify the contributions of the work? I understand that the work is proposing a new dataset and a new evaluation technique for it (which I find to be interesting and valid), but are there further novel contributions?\u201d\n\nR3: \n\nWe thank the reviewer for the valuable question. Other than the new dataset and evaluation technique (our major contributions), we summarize some other contributions as follows.\n\n1. Discovering flaws of existing datasets: Previous datasets have limitations in their difficulty, granularity and scale. They cannot be used to evaluate newly proposed expressive models. As we pointed out in motivation, we are the first to discover inherent flaws of current datasets and propose a solution.\n2. Revealing the gap between \"theoretical expressiveness\" and \"practical expressiveness\": Our dataset provides many additional perspectives in expressive power analysis. By revealing the gap between theoretical and practical expressiveness, we can find the influence of different strategies, components, frameworks, and other factors that may be underestimated.\n3. We give the first and most thorough empirical expressiveness measurement results. The quantification is especially useful for models without precise expressiveness analysis. By comparing it with broader experiment results on real-world datasets, we can also capture what is needed regarding expressiveness. We hope users can utilize it and aid further research.\n\nFinally, we would like to respectfully point out that ICLR welcomes a broad range of subject areas, including datasets and benchmarks (explicitly listed in topics). As a benchmark paper, our BREC has a clear motivation, intricate designs, and a sound evaluation pipeline, and it also addresses an urgent need of the community, which makes us believe that BREC meets the acceptance bar of ICLR. We appreciate the review's invaluable questions and advice, and kindly request the reviewer to reevaluate the contribution and novelty of our paper, possibly from a benchmark paper standard. We are profoundly thankful for your time and effort!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321573452,
                "cdate": 1700321573452,
                "tmdate": 1700321573452,
                "mdate": 1700321573452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGcc7uIqPY",
                "forum": "aqqE1yS3RY",
                "replyto": "Sad9EF4Z7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Reviewer_D7WE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Reviewer_D7WE"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the thorough answer. I have thought about this for some time. I also thank the authors for grouping models in Table 2, I think it is a significant improvement.\n\nI agree with the points raised by the authors, but in my opinion, the improvement remains marginal over what exists in the literature. I also agree with the authors that the proposed evaluation procedure is interesting, but it feels more like a \"nice-to-have\" rather than tackling a concrete existing issue in current evaluations on expressiveness benchmarks.\n\nThe fundamental issue I see is that generating such datasets is not very challenging in regard to the effort necessary to collect such data or label it. There is of course non-trivial effort involved in your work to do such a job in a diligent manner, but I'm not convinced that it's a sufficiently novel benchmark, when compared to what currently exists. \n\nRegardless, I have raised my score as I do believe that the improved Table and the discussion have improved the work. I finally thank the authors for their effort in their rebuttal."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488524871,
                "cdate": 1700488524871,
                "tmdate": 1700488524871,
                "mdate": 1700488524871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CiD727rsCB",
            "forum": "aqqE1yS3RY",
            "replyto": "aqqE1yS3RY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_Bpfn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_Bpfn"
            ],
            "content": {
                "summary": {
                    "value": "The paper overcomes the limitations of previous expressiveness datasets in terms of difficulty, granularity, and scale by introducing 4 datasets. Each dataset covers different benchmarking purposes for comprehensive GNN expressiveness evaluations. The authors further introduce Reliable Paired Comparisons instead of applying traditional classification comparisons to eliminate possible spurious correlations that can lead to unfair comparisons."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is the first benchmark that can cover different difficulties with fine granularity.\n2. The authors are the first to implement CFI graphs that enable any k-WL tests.\n3. Applying pair-wise comparisons instead of classifications to eliminate influence from other factors is reasonable and rigorous.\n4. To overcome the dilemma between false negative and false positive, the authors propose RPC that includes similarity comparisons and internal fluctuation considerations. Moreover, the authors propose to adjust the similarity threshold adaptively.\n5. The code is well-organized."
                },
                "weaknesses": {
                    "value": "The overall benchmark is comprehensive and elaborated. I only have a few minor concerns/questions.\n1. Why do the authors adopt cosine similarity instead of other contrastive loss? Will this loss be possible to introduce any spurious correlation leading to biased results?\n2. Since PPGN was compared, I'm wondering why \"INVARIANT AND EQUIVARIANT GRAPH NETWORKS\" was not included in the comparisons."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738741064,
            "cdate": 1698738741064,
            "tmdate": 1699637108489,
            "mdate": 1699637108489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nJq5nQiizY",
                "forum": "aqqE1yS3RY",
                "replyto": "CiD727rsCB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Bpfn"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer Bpfn's valuable questions and address the concerns as follows:\n\n>W1: Why do the authors adopt cosine similarity instead of other contrastive loss? Will this loss be possible to introduce any spurious correlation leading to biased results?\n\nR1: \n\nWe appreciate the reviewer's meticulous examination of our evaluation methodology.Our choice of cosine similarity in the contrastive learning setting is notably simplified compared to common scenarios where batches of samples need to be separated. In our case, the objective is to distinguish only one sample from another, eliminating the necessity for certain properties, particularly the control of differences in one type. Unlike typical contrastive learning, which aims to maintain alignment and uniformity, and often introduces additional terms to regulate the distance between samples in one batch, such considerations are rendered unnecessary in our context. Consequently, we adhere to the simplest method to streamline the evaluation process.\n\n>W2: Since PPGN was compared, I'm wondering why \"INVARIANT AND EQUIVARIANT GRAPH NETWORKS\" was not included in the comparisons.\n\nR2: \n\nWe appreciate the reviewer's observation regarding the exclusion of \"INVARIANT AND EQUIVARIANT GRAPH NETWORKS\" from our comparisons. While this model introduces invariant and equivariant layers and holds significance in expressiveness analysis [1, 2, 3], the original model it proposes, IGN, does not exhibit high expressiveness. As highlighted in [1], the k-IGN variant can implement k-WL but is constrained by the need for iterating k-th and 2k-th bell numbers' operations (the 4th bell number is 15, and the 6th bell number is 203), limiting its applicability and scalability to k <= 2. Furthermore, 2-WL is only as expressive as 1-WL, indicating that 2-IGN cannot distinguish any pairs of graphs in BREC. Consequently, we have chosen not to include IGN in our testing. Instead, we have evaluated several models inspired by IGN, such as PPGN [1], ESAN (DSS-GNN) [2], SUN [3], among others.\n\nReferences\n\n[1] Provably Powerful Graph Networks https://arxiv.org/abs/1905.11136\n\n[2] Equivariant Subgraph Aggregation Networks https://arxiv.org/abs/2110.02910\n\n[3] Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries https://arxiv.org/abs/2206.11140"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320928993,
                "cdate": 1700320928993,
                "tmdate": 1700320928993,
                "mdate": 1700320928993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O5drAHGoXW",
                "forum": "aqqE1yS3RY",
                "replyto": "nJq5nQiizY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Reviewer_Bpfn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Reviewer_Bpfn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I lean to accept the paper but not confidently. The overall idea and contributions are interesting with the first implementation of CFI graphs that enable any k-WL tests. But as Reviewer D7WE said, this benchmark is \"nice-to-have\" but not necessary."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502008675,
                "cdate": 1700502008675,
                "tmdate": 1700502008675,
                "mdate": 1700502008675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q93WQJmDOH",
            "forum": "aqqE1yS3RY",
            "replyto": "aqqE1yS3RY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_ZHhj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8818/Reviewer_ZHhj"
            ],
            "content": {
                "summary": {
                    "value": "Previous datasets designed for evaluating the expressiveness of GNNs had limitations in terms of difficulty, granularity, and scale. The authors propose a new expressiveness dataset called BREC, which includes 400 pairs of non-isomorphic graphs carefully selected from different categories. The dataset offers higher difficulty, finer granularity, and larger scale compared to previous datasets. The authors conduct experiments on 23 models with expressiveness beyond 1-WL (Weisfeiler-Lehman) on the BREC dataset, providing a thorough measurement of their expressiveness and highlighting the gap between theoretical and practical expressiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It is a valuable problem for evaluating the expressiveness of GNNs.\n* The benchmark is extensive, including datasets with higher difficulty, finer granularity, larger scale and many models for comparisons.\n* It is promising for the BREC dataset to serve as a benchmark for testing the expressiveness of future GNNs."
                },
                "weaknesses": {
                    "value": "1. One concern is about the evaluation. It shows in Table 2 that only 400 samples are included in total. Including more samples in the datasets would make the experiments more convincing.\n\n2. What about the relationship about the performance on the proposed synthetic datasets and real-world datasets? In other words, is the model that  shows to more expressive in the benchmark performing better in real-world tasks? It would be better to provide more fine-grained analyses."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8818/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8818/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8818/Reviewer_ZHhj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8818/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818997534,
            "cdate": 1698818997534,
            "tmdate": 1699637108327,
            "mdate": 1699637108327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nmzZ3MGP4u",
                "forum": "aqqE1yS3RY",
                "replyto": "Q93WQJmDOH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8818/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZHhj"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer ZHhj's insightful comments and address the concerns as follows:\n\n> W1: One concern is about the evaluation. It shows in Table 2 that only 400 samples are included in total. Including more samples in the datasets would make the experiments more convincing.\n\nR1: \n\nWe thank the reviewer for emphasizing the importance of dataset size. We carefully considered how to construct the datasets initially and decided to use 400 pairs for coming reasons:\n\n1. 400 samples are not small for expressiveness. Recalling that each sample includes 2 quite similar but non-isomorphic graphs, and all 800 graphs are non-isomorphic. Thus, generating 319,600 pairs of graphs (from CSL and SR25 settings) or even more by adding noisy components (from EXP settings) will be easy. However, we limit the number to 400 pairs (each graph appears only once) to keep the most essential and difficult comparisons.\n2. Unlike large-scale real-world datasets, which require as many graphs as possible splited into train/val/test for testing generalization and scalability, our datasets aim to test expressiveness, where we do not need the model to generalize by training on big data. Instead, expressiveness datasets purely aim to measure models\u2019 distinguishing power. Furthermore, we provide a reliable and precise evaluation method to obtain the exact distinguishable number of graphs with nearly **no variance**. This indicates that the current number of graphs is **sufficient** to reflect the expressiveness gap between different methods.\n3. While it is feasible to expand the datasets by seamlessly sampling more graphs, certain graph types, such as Basic and Extension, lend themselves to exhaustive search sampling. However, this approach is not universally applicable, particularly for graphs like CFI or distance regular graphs, which are too rare to procure in larger quantities. Furthermore, enlarging the dataset in this manner may lead to an unbalanced distribution, disproportionately favoring simple graphs. Consequently, we have chosen to retain a set of 800 graphs.\n\n> W2: What about the relationship about the performance on the proposed synthetic datasets and real-world datasets? In other words, is the model that shows to more expressive in the benchmark performing better in real-world tasks? It would be better to provide more fine-grained analyses.\n\nR2: \n\nWe thank the reviewer for underscoring the significance of practical utility concerning expressiveness. Generally, graphs in chemical areas tend to have high expressiveness demand. There exist some structures that standard MPNNs can hardly recognize. For example, the benzene ring (6-cycle) is a common and essential molecule component that most simple GNNs (including all MPNNs) cannot recognize. It is inevitable those GNNs might learn some statistical patterns correlated to benzene rings, but they fundamentally fail to understand the structure and fail to make predictions causally. In real-world datasets, expressive GNNs also perform remarkably in molecular datasets. ZINC [1] is a widely used molecular property regression benchmark. Currently, the 2 SOTA models are GRIT [2] and N$^2$-GNN [3], both focusing on expressiveness.\n\nWe collect our tested models' performance on ZINC:\n\n| Model | BREC(all) | BREC(CFI Graph) | ZINC\u2193 |\n| --- | --- | --- | --- |\n| OSAN | 148 | 5 | 0.155 |\n| Graphormer | 79 | 10 | 0.122 |\n| DS-GNN | 222 | 16 | 0.116 |\n| GSN | 254 | 0 | 0.115 |\n| DE+NGNN | 231 | 21 | 0.111 |\n| DSS-GNN | 221 | 15 | 0.097 |\n| GNN-AK | 222 | 15 | 0.093 |\n| KP-GNN | 275 | 11 | 0.093 |\n| SUN | 223 | 13 | 0.083 |\n| I$^2$-GNN | 281 | 21 | 0.083 |\n| PPGN | 233 | 23 | 0.079 |\n| KC-SetGNN | 211 | 1 | 0.075 |\n| SSWL_P | 248 | 38 | 0.070 |\n| N$^2$-GNN | 287 | 27 | 0.059 |\n\n\nObservably, N$^2$-GNN and SSWL_P outperform other models on the ZINC dataset. Both models exhibit commendable performance on BREC, particularly on CFI Graph, known for its substantial size and asymmetric composition, necessitating the model to discover graph pattern differences in its layers iteratively. The ZINC dataset, with its regression target \"constrained solubility,\" incorporating water-octanol partition coefficient, synthetic accessibility score, and the number of cycles with more than six atoms, requires structure learning from a molecular accessibility perspective. Expressiveness, therefore, plays a pivotal role in enhancing model performance in the ZINC dataset.\n\nReferences\n\n[1] Benchmarking Graph Neural Networks https://arxiv.org/abs/2003.00982\n\n[2] Graph Inductive Biases in Transformers without Message Passing http://arxiv.org/abs/2305.17589\n\n[3] Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman https://arxiv.org/abs/2306.03266"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8818/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320680933,
                "cdate": 1700320680933,
                "tmdate": 1700320680933,
                "mdate": 1700320680933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]