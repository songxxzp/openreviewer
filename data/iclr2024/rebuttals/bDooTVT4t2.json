[
    {
        "title": "Universally Amplifying Randomized Smoothing for Certified Robustness with Anisotropic Noise"
    },
    {
        "review": {
            "id": "AL12ezkHqW",
            "forum": "bDooTVT4t2",
            "replyto": "bDooTVT4t2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_SMHf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_SMHf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to Certified Robustness via Randomized Smoothing, in which the noise distribution for the different data dimensions is allowed to vary, i.e. is anisotropic. Three example noise parameter generators are proposed, and experimental results given on standard datasets against SOTA methods."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well presented, clearly structured and well written, with a reasonably comprehensive set of experiments performed."
                },
                "weaknesses": {
                    "value": "My scoring reflects very much an issue raised in the Questions below. Clarification on that from the authors could substantially change my assessment.\n\u00a0\nThe paper presents only an incremental improvement on SOTA methods, and in many ways the main Theorem (3.2) is really no more than a simple Corollary from prior work with a simple affine transformation on the variables (which would usually be done anyway in dealing with non-image data with very different means and scales in each data dimension).\n\nThe main issues, written in questions to the authors, concern (a) the presentation of the comparison with isotropic SOTA, and (b) the validity of the input-dependent noise (the method introduced in section 4.3). This latter is also used as the basis for the main results in section 5.3. Hence my rating for the paper could be improved significantly in light of any author response/clarification to my questions.\n\nSome more minor comments here though:\n- The binary case rather than multi-class is used for Cohen. This gives poorer results, of course. Not an issue, as it is done consistently for the proposed and SOTA methods. But it should be at least clarified as the statement of Theorem 3.2 suggests that the p_B value will be used, but then in Table 1 it is not (ie it is replaced by p_B = 1 \u2013 p_A)\n- The definition of Acc in the Metrics section has it defined as a function of V\u2019_S. And yet V\u2019_S does not appear on the RHS!! (instead you have replaced it with the dth root of the product of the sigmas times R)\n- At small radii, the SOTA methods are better (on the graphs) than the proposed method. Some discussion would be welcome on this.\n- In section 5.1, it is mentioned that Cohen gives a tight radius. Again, as per above, this is really only in the non-binary form."
                },
                "questions": {
                    "value": "In the experiments, it seems that the results vs {min \\sigma_i} R should be presented (per Corollary 3.3) for a fair comparison, i.e. it is not clear to me that the proposed technique certifies a strictly larger L_p ball in the same conditions than SOTA (and what \u201csame conditions\u201d may mean is not clear e.g. it may mean isotropic sigma=1 and \\product \\sigma_i = 1 in anisotropic case). That said, the results in section 5.3 apparently include a certified accuracy wrt radius. It is not clear if this may be an answer to my query as it is not clear exactly what is being reported here.\n\nRe section 4.3, I am a bit confused. This input-dependent proposal would give an x-dependent sigma, and hence an x-dependent classifier (ie x-dependent noise, over and above the obvious dependence on x). Theorem 3.2 holds assuming the noise is constant in the ball around x. This proposal in 4.3 violates that surely. Hence it is no longer true that Theorem 3.2 guarantees that the classifier gives an unchanged output in the region claimed, and so the proposed classifier is not certified robust in the claimed reghion around x. I may have misunderstood. Clarification/explanation is welcome.\nPlease see aeXiv paper 2110.05365 for an example of work done with input-dependent noise."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697753919447,
            "cdate": 1697753919447,
            "tmdate": 1699636799675,
            "mdate": 1699636799675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RtLNPu0wiI",
                "forum": "bDooTVT4t2",
                "replyto": "AL12ezkHqW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments and please find our clarifications as below. We also make more clarifications in the paper to avoid confusion/misunderstanding. \n\n1. **On the Novelty of Theorem 3.2**:\n\n> The paper presents only an incremental improvement on SOTA methods, and in many ways the main Theorem (3.2) is really no more than a simple Corollary from prior work with a simple affine transformation on the variables (which would usually be done anyway in dealing with non-image data with very different means and scales in each data dimension).\n\nWe respectfully clarify that the derived universal theories are not simple (by covering both strict robustness w.r.t. heterogeneous dimensions and universality), as detailed in Appendix A. Even if its conclusion might be considered as simple, as noted by Reviewer v1kJ, simple-but-effective solutions are not generally considered as ``weakness'', but would be desirable in practice.\n\n2. **Clarification on Isotropic Results**:\n\n> In the experiments, it seems that the results vs $\\{min \\sigma_i\\} R$  should be presented (per Corollary 3.3) for a fair comparison, i.e. it is not clear to me that the proposed technique certifies a strictly larger $L_p$ ball in the same conditions than SOTA (and what \u201csame conditions\u201d may mean is not clear e.g. it may mean isotropic $\\sigma=1$ and $\\prod \\sigma_i = 1$ in anisotropic case). That said, the results in section 5.3 apparently include a certified accuracy wrt radius. It is not clear if this may be an answer to my query as it is not clear exactly what is being reported here.\n\nIn our experimental results, the \u201ccertified accuracy w.r.t radius\u201d refers to certified accuracy w.r.t. $\\min\\{\\sigma_i\\}R$, which equals to the $\\ell_p$ radius (e.g., the Gaussian $\\ell_2$ case). Therefore, we did achieve a strictly larger $\\ell_2$-ball than other SOTAs. In the entire paper (including Section 5.3), all the \u201cradius\u201d means traditional radius for the $\\ell_p$ ball. \n\n3. **Input-dependent Certification Validity**:\n\n> Re section 4.3, I am a bit confused. This input-dependent proposal would give an x-dependent sigma, and hence an x-dependent classifier (ie x-dependent noise, over and above the obvious dependence on x). Theorem 3.2 holds assuming the noise is constant in the ball around x. This proposal in 4.3 violates that surely. Hence it is no longer true that Theorem 3.2 guarantees that the classifier gives an unchanged output in the region claimed, and so the proposed classifier is not certified robust in the claimed region around x.\n\nRegarding the input-dependent certification, we noticed the discussion in Eiras et al. (2021). We respectfully clarify that our input-dependent noise method is still sound and does not breach certification principles. \n\nThe $x$-dependent classifier is fixed in the certification on $x$, that means with $g(x,\\mu(x),\\sigma(x))$ and $R$, we are guaranteeing $g(x+\\delta,\\mu(x),\\sigma(x))=g(x,\\mu(x),\\sigma(x))$ as described in Theorem 3.2 (note that noise is pre-computed and $\\mu(x),\\sigma(x)$ are actually constants). We do not change the $x$-dependent classifier during the certification, thus we achieve consistent prediction over such a specific classifier. This does not violate the certification of randomized smoothing, instead, it sticks to the inherent characteristics of the RS-based certification, i.e., input-dependence (The RS certification is inherently input-dependent since the radius $R(x)$ only works on the specific input $x$ and a specific classifier). Please also see the detailed explanation in our ``Common Concerns'' response.\n\n4. **Responses to Minor Comments**:\n\n- Binary Case: We appreciate this observation and have revised the caption of Table 1 accordingly.\n\n- Metrics Clarification: The metric named $V'_S$ is referred to as ALM throughout the paper. We have amended the symbols to ALM for consistency and clarity.\n\n- Performance at Small Radii: The reason SOTA methods perform better at smaller radii is due to the noise variance-prediction accuracy trade-off. Our method, with single $\\lambda\\approx 1$, competes with SOTA methods at various $\\lambda$ settings. Notably, our Input-dependent method outperforms these trade-offs even with a single setting.\n\n- Cohen's Tight Radius: We have revised the statement about Cohen's method to reflect its application more accurately."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011872144,
                "cdate": 1700011872144,
                "tmdate": 1700011961874,
                "mdate": 1700011961874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eVQS5AYvT9",
                "forum": "bDooTVT4t2",
                "replyto": "RtLNPu0wiI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_SMHf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_SMHf"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses. In line with reviewer v1kj I still am not convinced of the validity of the certification in the input-dependent case and maintain my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103161060,
                "cdate": 1700103161060,
                "tmdate": 1700103161060,
                "mdate": 1700103161060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5GSqA0mWOZ",
            "forum": "bDooTVT4t2",
            "replyto": "bDooTVT4t2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to shift and re-scale the noise in randomized smoothing in order to generate anisotropic robustness guarantees. The approach is universal in the sense that any randomized smoothing-based method can be transformed into a model with such anisotropic guarantees. Experiments on benchmark image classification datasets demonstrate increased certified accuracy curves compared to past works."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is very easy to read.\n2. The approach is simple to understand, and the resulting theoretical guarantee follows from past robustness guarantees in a very straightforward manner. This simplicity should be considered a strength of the method, not a weakness of the paper.\n3. The experiments are thorough, with comparisons to a wide range of prior methods and on high-dimensional image datasets (e.g., ImageNet)."
                },
                "weaknesses": {
                    "value": "See \"Questions\" section below."
                },
                "questions": {
                    "value": "1. \"However, its theory is based on assumptions and the universality is relatively limited.\" What assumptions? Please at least briefly mention them and why they are stringent.\n2. Definition 3.1 does not really appear to be a \"definition\" in mathematical terms. It looks more like you are re-stating the certified radius theorems for general distributions and norms. So, this should probably be labeled as a \"theorem\".\n3. Please move the definition/review of alternative Lebesgue measure to Section 3.1, where Table 1 appears with Alt. Lebesgue Measure as a column.\n4. In Section 4.2, the loss function is solely a function of the NPG parameters $\\theta_g$, correct? If so, it would be good to explicitly write $\\mathcal{L}(\\theta_g)$ to emphasize to the reader what you are optimizing over. Furthermore, $\\sigma$ and $\\mu$ would be functions of this parameter $\\theta_g$, right? If so, it would also be good to write $\\sigma(\\theta_g)$ and $\\mu(\\theta_g)$ in the smoothing loss expression.\n5. MOST IMPORTANT PROBLEM: Your highest performing approach, using input-dependent anisotropic smoothing parameters that are optimized per-input, breaks the robustness certificates. Namely, randomized smoothing robustness certificates intimately rely on the same model being used to predict at the nominal point $x$ and all perturbed versions $x+\\delta$ in the certified ball around $x$. However, if you optimize $\\mu,\\sigma$ at $x$, then the smoothing-based certificate only says that $x+\\delta$ will yield the same prediction if you also use the same parameters $\\mu,\\sigma$ to define the prediction at $x+\\delta$. But, according to your scheme, you actually re-optimize $\\mu,\\sigma$ at the perturbed test input $x+\\delta$ to generate the prediction at $x+\\delta$, meaning you are using a different model than what smoothing certifies at $x$. This mathematical breakdown of certified robustness for input-dependent smoothing has been noted before in past works, and is the reason why works like Eiras et al. (2022) augment their input-dependent scheme with \"memory.\" In order for your input-dependent smoothing scheme to work, you would also need to appeal to some \"fix\" like this memory method, which comes with its own issues (e.g., relating to dependency on input order, and increased memory overhead costs). Either you should fix this issue (and hopefully your certificates still provide substantial improvement over state-of-the-art), or you should remove this input-dependent part of the paper (which, in my opinion, would significantly reduce the contributions of the paper)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698435454680,
            "cdate": 1698435454680,
            "tmdate": 1699636799505,
            "mdate": 1699636799505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SFD6jm6Quq",
                "forum": "bDooTVT4t2",
                "replyto": "5GSqA0mWOZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments and please find our clarifications as below. We also make more clarifications in the paper to avoid confusion/misunderstanding. \n\n1. **On the Presenting of Related Works**:\n\n> \"However, its theory is based on assumptions and the universality is relatively limited.\" What assumptions? Please at least briefly mention them and why they are stringent.\n\nOur approach does not need the assumptions in Eiras et al., where the classifier's $L$-Lipschitz continuity is a prerequisite. This assumption imposes limitations on the universality of their theorem. Conversely, our method is designed to work universally with any classifier, thereby offering broader applicability.\n\n2 \\& 3. **Regarding the Paper Revisions**:\n\n> Definition 3.1 does not really appear to be a \"definition\" in mathematical terms. It looks more like you are re-stating the certified radius theorems for general distributions and norms. So, this should probably be labeled as a \"theorem\".\n> Please move the definition/review of alternative Lebesgue measure to Section 3.1, where Table 1 appears with Alt. Lebesgue Measure as a column.\n\nWe appreciate your suggestions for improving the paper's structure and clarity. We have revised the paper accordingly, particularly in relabeling Definition 3.1 as a theorem and relocating the alternative Lebesgue measure definition to Section 3.1, as suggested.\n\n4. **Clarifications in Section 4.2**:\n\n> In Section 4.2, the loss function is solely a function of the NPG parameters $\\theta_g$, correct? If so, it would be good to explicitly write $\\mathcal{L}(\\theta_g)$ to emphasize to the reader what you are optimizing over. Furthermore, $\\sigma$ and $\\mu$ would be functions of this parameter $\\theta_g$, right? If so, it would also be good to write $\\sigma(\\theta_g)$ and $\\mu(\\theta_g)$ in the smoothing loss expression.\n\nWe have updated our paper to clarify that the loss function is optimized over both $\\theta_f$ and $\\theta_g$. Additionally, we have made it explicit that $\\mu$ and $\\sigma$ are functions of the parameter $\\theta_g$. These revisions should enhance the clarity of our methodology.\n\n5. **On the Validity of Input-Dependent Randomized Smoothing**:\n\n> Your highest performing approach, using input-dependent anisotropic smoothing parameters that are optimized per-input, breaks the robustness certificates. \n\nWe noticed the discussion in Eiras et al. (2022) and understand your concerns regarding our input-dependent Randomized Smoothing (RS) approach. However, we respectfully clarify the soundness of this method.\n\n> Namely, randomized smoothing robustness certificates intimately rely on the same model being used to predict at the nominal point $x$ and all perturbed versions $x+\\delta$ in the certified ball around $x$.However, if you optimize $\\mu,\\sigma$ at $x$, then the smoothing-based certificate only says that $x+\\delta$ will yield the same prediction if you also use the same parameters $\\mu,\\sigma$ to define the prediction at $x+\\delta$. But, according to your scheme, you actually re-optimize $\\mu,\\sigma$ at the perturbed test input $x+\\delta$ to generate the prediction at $x+\\delta$, meaning you are using a different model than what smoothing certifies at $x$. This mathematical breakdown of certified robustness for input-dependent smoothing has been noted before in past works, and is the reason why works like Eiras et al. (2022) augment their input-dependent scheme with \"memory.\"\n\nIn this case, with $g(x,\\mu(x),\\sigma(x))$ and $R$ where $\\mu(x),\\sigma(x)$ are pre-computed (independent of $\\delta$) as constants, we are guaranteeing $g(x+\\delta,\\mu(x),\\sigma(x))=g(x,\\mu(x),\\sigma(x))$ as described in Theorem 3.2. Then, the models used for predicting $x$ and $x+\\delta$ are the same in the certification, thus do not break the certification. The entire certification process has nothing to do with $g(x+\\delta,\\mu(x+\\delta),\\sigma(x+\\delta))$ and the noise optimization is not based on $x+\\delta$. Please also see the detailed explanation in our **Common Concerns** response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011469992,
                "cdate": 1700011469992,
                "tmdate": 1700011469992,
                "mdate": 1700011469992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JO08UhARD3",
                "forum": "bDooTVT4t2",
                "replyto": "SFD6jm6Quq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "1. Notice that Eiras does not assume that the base classifier $f$ is Lipschitz, so their method is \"designed to work universally with any classifier\" as well. They prove that their smoothed classifiers $g$ are Lipschitz with respect to the anisotropic $\\ell_2$ and $\\ell_\\infty$ norms, no matter what the base classifier is. This is a classical property of randomized smoothing: convolving a general function with an adequately smooth probability density function or \"kernel\" gives rise to a new function that inherits smoothness from the kernel. Their theoretical robustness certificate holds very generally for Lipschitz classifiers, of which smoothed classifiers via randomized smoothing are a special case. Therefore, your assertion that their \"theory is based on assumptions that the networks are L-Lipschitz continuous and thus the universality is relatively limited\" is not well-justified (since it is implying that they assume the base classifiers are L-Lipschitz, which they do not), and I suggest removing it.\n\n2,3,4. Thank you for the revisions.\n\n5. I have read both your \"Common Concerns\" response, as well as your individual response to me. I am still not convinced that your robustness guarantee holds. Unless I'm misunderstanding something, then, at test time, the attacked input going to your model is $x+\\delta$. Therefore, your NPG will output $\\mu(x+\\delta)$ and $\\sigma(x+\\delta)$ as the noise parameters. These will in general define a different smoothing distribution than that corresponding to the clean test input $x$, which has parameters $\\mu(x)$ and $\\sigma(x)$ output by the NPG. Therefore, the certification condition, as you call it, would need to be that $g(x+\\delta,\\mu(x+\\delta),\\sigma(x+\\delta)) = g(x,\\mu(x),\\sigma(x))$, which is not what is guaranteed by the randomized smoothing framework. In other words, the actual output of your model at an attacked input $x+\\delta$ is $g(x+\\delta,\\mu(x+\\delta),\\sigma(x+\\delta))$, and your certificates do not guarantee that this output coincides with the output $g(x,\\mu(x),\\sigma(x))$ generated by the clean input $x$.\n\nGiven that my primary concern (shared with Reviewer SMHf) still remains, I maintain my original score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078494701,
                "cdate": 1700078494701,
                "tmdate": 1700078494701,
                "mdate": 1700078494701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VKc9YP8Mhm",
                "forum": "bDooTVT4t2",
                "replyto": "5GSqA0mWOZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I believe you are still misunderstanding my point. Let me try to make it more clear:\n\nI agree with you that, mathematically, the equality $g(x+\\delta,\\mu(x),\\sigma(x)) = g(x,\\mu(x),\\sigma(x))$ holds. However, my main point is that **this equality does not reflect how your classifier makes its predictions in reality, and thus provides no meaningful robustness guarantee**.\n\nTo see why this is the case, let $x$ be some fixed input. Your scheme then chooses a mean $\\mu(x)$ and covariance $\\sigma(x)$ based on this $x$. Then, to form the prediction for the input $x$, you compute $g(x,\\mu(x),\\sigma(x)) = \\arg\\max_{c\\in\\mathcal{C}} \\mathbb{P}_{\\epsilon \\sim D} (f(x + \\epsilon^\\top \\sigma(x) + \\mu(x))=c)$, where $D$ is some isotropic \"base distribution,\" such as an isotropic Gaussian. Now, your equality says that $g(x + \\delta,\\mu(x),\\sigma(x)) = g(x,\\mu(x),\\sigma(x))$ for all perturbations $\\delta$ inside of some region $\\mathcal{R}$ of the input space containing $x$. **This does not imply that your classification scheme's prediction is constant over this region**.\n\nSpecifically, consider a NEW input $x'$ (possibly different from $x$, and therefore I put the \"prime\") for which you are to make a prediction. Well, substituting $x'$ for $x$ in the above prediction rule (which we both have agreed upon) gives that the prediction is $g(x',\\mu(x'),\\sigma(x')) = \\arg\\max_{c\\in\\mathcal{C}} \\mathbb{P}_{\\epsilon \\sim D} (f(x' + \\epsilon^\\top \\sigma(x') + \\mu(x'))=c)$. In other words, to form a prediction for the input $x'$, you again compute the input-dependent mean and covariance, and perform the RS-based prediction at $x'$ using its optimized $\\mu(x')$ and $\\sigma(x')$.\n\nNow, suppose that I reveal to you that actually, I was an adversary, and the input $x'$ that I gave to you and asked you to classify was $x'=x+\\delta$ for some perturbation $\\delta$ that I chose within your region $\\mathcal{R}$. Then your prediction for this perturbed input was computed as $g(x',\\mu(x'),\\sigma(x'))=g(x+\\delta,\\mu(x+\\delta),\\sigma(x+\\delta)) = \\arg\\max_{c\\in\\mathcal{C}} \\mathbb{P}_{\\epsilon \\sim D} (f(x + \\delta + \\epsilon^\\top \\sigma(x+\\delta) + \\mu(x+\\delta))=c)$, which is **not** guaranteed to be equal to the prediction $g(x,\\mu(x),\\sigma(x))$ corresponding to $x$. Therefore, the equality $g(x+\\delta,\\mu(x),\\sigma(x)) = g(x,\\mu(x),\\sigma(x))$ does not say anything about the prediction/output $g(x+\\delta,\\mu(x+\\delta),\\sigma(x+\\delta))$ of an attacked input $x+\\delta$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160021877,
                "cdate": 1700160021877,
                "tmdate": 1700160860692,
                "mdate": 1700160860692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GohIk0b4Pa",
                "forum": "bDooTVT4t2",
                "replyto": "5GSqA0mWOZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for the prompt reply. First of all, with respect to your two proposed situations:\n\nAssuming that you always know the clean input $x$ is an unrealistic and very stringent assumption. If I gave you $x'=x+\\delta$, how would you know what the clean input $x$ is? If this were the case, then all of the recent work on certifiably robust machine learning in the presence of $\\ell_p$-norm bounded additive adversaries would be pointless, since one could simply reconstruct $x$ from $x'$, and then classify the clean input $x$ (which is much less likely to induce an error). So, this first situation is meaningless to consider in your setting of adversarial threats. Thus, the standard setting for papers in this area (and randomized smoothing, in particular), is to assume that all that you have at your disposal is whatever input you are given ($x'$ in the case of an attacked input), and you cannot distinguish whether this was attacked or not. In this standard setting, there is no possible way for you to take the attacked $x'$ as an input, and compute $\\mu(x),\\sigma(x)$ for the clean input.\n\nSo, there is an essential question to consider that points to the flaw at hand:\nWhat does your \"certificate\" $g(x+\\delta,\\mu(x),\\sigma(x)) = g(x,\\mu(x),\\sigma(x))$ represent? To me, this says that, if you predict the output of $x+\\delta$ using the smoothing distribution for $x$, then the prediction will match that of $x$. The problem is twofold: 1) you cannot compute $\\mu(x),\\sigma(x)$ (or even samples drawn from this distribution) unless you have access to the clean input $x$, which you do not, and 2) $g(x+\\delta,\\mu(x),\\sigma(x))$ is not even the prediction that your scheme would output if $x+\\delta$ was treated as its own input, since the smoothing distribution would be re-optimized for the input $x+\\delta$."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166894540,
                "cdate": 1700166894540,
                "tmdate": 1700166933057,
                "mdate": 1700166933057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sbPHFZrE0Z",
                "forum": "bDooTVT4t2",
                "replyto": "5GSqA0mWOZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "\"Let's assume that all we have at our disposal is whatever input we are given, then given arbitrary $x'$ by an adversary, what information about robustness does randomized smoothing provide?\"\n- Conventional randomized smoothing, with a smoothing distribution that is uniform over the entire input space, DOES give you that the classification of $x'$, namely, $g(x')$, is the same as that of the certified clean input $x$, even if you don't know $x$, so long as $\\lVert x-x'\\rVert \\le R$ with $R$ being the certified radius. This result intimately relies on the fact that $g(x')$ is computing using the same smoothing distribution as $g(x)$ is computed.\n\n\"The more realistic setting is the model owner holds a $x$, and then computes a certified radius $R$ on it, then given any input $x'$ by the adversary the model owner can compute $\\delta=x'-x$ and compare $\\lVert \\delta \\rVert_p$ to $R$ to pre-determine the prediction without execution\"\n\n- This is not how adversarial threat models are usually formulated. Can you point to a paper that assumes that they have access to the clean input $x$ at the time when they are predicting the class of an attacked input $x+\\delta$? The two closest things that come to mind are [1] and [2], which use memory-based techniques for input-dependent randomized smoothing, in particular as a method to overcome the exact issue that I am saying your approach suffers from. However, you do not make clear in your paper that you are using such a memory-based approach, and furthermore those memory-based approaches have their own limitations. For example, the classification of an input may in general be dependent on the order of inputs that were previously classified+certified, and furthermore you incur a memory cost that continually grows as you predict+certify more and more inputs.\n\n[1] \"ANCER: Anisotropic Certification via Sample-wise Volume Maximization\", Eiras et al.\n\n[2] \"Data Dependent Randomized Smoothing\", Alfarra et al."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174104785,
                "cdate": 1700174104785,
                "tmdate": 1700175015466,
                "mdate": 1700175015466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DuH0vG00Df",
                "forum": "bDooTVT4t2",
                "replyto": "sbPHFZrE0Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "Are you generating your \"certificates\" for the inputs in the training set, or the test set?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175470184,
                "cdate": 1700175470184,
                "tmdate": 1700175470184,
                "mdate": 1700175470184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kxep1sS8MW",
                "forum": "bDooTVT4t2",
                "replyto": "ag9t7SF0A8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_v1kJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further discussion. I am still not convinced by your arguments, and therefore I maintain my overall score.\n\nI encourage the authors to carefully read [1] (and in particular, Section 3.5) for more information on why robustness certificates for input-dependent randomized smoothing fail to hold without some careful modifications (like a memory-based approach), and to take these technicalities (and their associated limitations) into account in their future iterations of the project.\n\n[1] Alfarra et al., \"Data-Dependent Randomized Smoothing,\" UAI, 2022."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425108175,
                "cdate": 1700425108175,
                "tmdate": 1700425108175,
                "mdate": 1700425108175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9fsMYV3D55",
            "forum": "bDooTVT4t2",
            "replyto": "bDooTVT4t2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_eeVj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_eeVj"
            ],
            "content": {
                "summary": {
                    "value": "The authors interpret certified robustness from an anisotropic lens, with the aim of assessing how the performance of certification mechanisms within this context."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Well written, comprehensive experiments that match community expectations, nice visualisations that really break apart the differences between fixed pattern, universal, and input dependent noise. \n\nThe input-dependent component of the noise is interesting."
                },
                "weaknesses": {
                    "value": "My issues with this paper stem from two different directions, which I find to be significant hurdles to my ability to recommend this paper for publication.\n\nThe first is the fact that the contribution in constructing the anistropic noise measures (the core conceit of the paper) is essentially just a basic modification to extant techniques, with no other modifications being made. However, this is not in and of itself a reason for rejection - simple modifications can lead to impactful contributions. \n\nMy primary concern relates to the alignment of the chosen area of investigation to the broader problem space. Specifically, I do not think that there is any framework (either in the literature or suggested in the framework) that would case about the area of the region of certification. From the certifiers perspective, what information about the security of a model is gained by knowledge of the Lebesgue measure of the noise region (or any other measure of the area of certification)? The primary measure of risk is the nearest extant adversarial example - this is well established in the literature as a measure of the adversarial risk (see Gilmer's \"Motivating the Rules of the Game for Adversarial Example Research\", 2018), because this measures the effort required for an adversary to identify an adversarial example. Any other risk measure would need to be well justified and well posed, and this is not the case within this work. \n\nGiven that the certified distances to the nearest possible adversarial example are unchanged by your work, I do not see how this leads to an improved understanding of adversarial risk. I would argue that rather than significantly improving upon SOTA, you're introducing a new metric to mask the fact that you do not appear to produce any level of outperformance. \n\nFor a few minor issues:\n-The use of lambda as the scale parameter, and $\\sigma_i$ as the modification of the scale parameter. But $\\lambda$ is typically proportional to the standard deviation, so using $\\sigma_i$ as part of the notation is not as clear as it could be. \n- A secondary minor issue is that Table 1, I believe the Lee et. al PDF should be proportional to $||z||_{\\infty}$, rather than $||z / \\lambda||$.\n- The idea of including a headline figure of an 182.6% improvement over SOTA - anyone reading this who was not familiar with this field would assume that this would be an apples-to-apples comparison, but it's not. There's no SOTA for certification that cares about area driven measures of certification, and so claiming a comparison to these prior techniques is not reasonable or well justified."
                },
                "questions": {
                    "value": "Is there any justification for using the Lebesgue Measure as a proxy of adversarial risk?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Reviewer_eeVj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698472061310,
            "cdate": 1698472061310,
            "tmdate": 1699636799377,
            "mdate": 1699636799377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OKIDueDlj1",
                "forum": "bDooTVT4t2",
                "replyto": "9fsMYV3D55",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments and please find our clarifications as below. We also make more clarifications in the paper to avoid confusion/misunderstanding. \n\n1. **On the Contribution and Novelty**:\n\n>    The first is the fact that the contribution in constructing the anistropic noise measures (the core conceit of the paper) is essentially just a basic modification to extant techniques, with no other modifications being made.\n\nWe would like to respectfully clarify that our core contribution is not on the construction of the ALM measure but on the theoretical analysis of the robustness bound when injecting anisotropic noise, as well as the framework and novel methods for customizing anisotropic noise. ALM is one of our metrics for evaluating the certified robustness with anisotropic noise while we also derive the traditional $\\ell_p$ radii (Corollary 3.3) for the robustness region with anisotropic noise.\n\n>  My primary concern relates to the alignment of the chosen area of investigation to the broader problem space. Specifically, I do not think that there is any framework (either in the literature or suggested in the framework) that would care about the area of the region of certification. \n\nThe certified $\\ell_p$ radius can be seen as a measure for the area of the regular and symmetric robustness region within the overall robustness region (sub-region in $\\ell_{1,2,\\infty}$ shape). On the other hand, ALM provides a full capture of the overall robustness region (a comprehensive understanding of the theoretical guarantees and boundaries of the certified defense). These two metrics complement each other, and we have provided the results for both $\\ell_p$ radius and ALM as well as performed evaluations on both of them in the original submission. \n\n> From the certifiers perspective, what information about the security of a model is gained by knowledge of the Lebesgue measure of the noise region (or any other measure of the area of certification)? The primary measure of risk is the nearest extant adversarial example - this is well established in the literature as a measure of the adversarial risk (see Gilmer's \"Motivating the Rules of the Game for Adversarial Example Research\", 2018), because this measures the effort required for an adversary to identify an adversarial example. Any other risk measure would need to be well justified and well posed, and this is not the case within this work.\n\nThanks for the insightful comment. We know that the certified radius provides the knowledge that any perturbation within the $\\ell_p$ distance will not succeed. In practice, the defense may tolerate the perturbation of a larger distance in some dimensions (see Figure 1 (b)), how to measure this gain of robustness would be a problem. Therefore, we propose the complementary ALM metric to measure the full area of the (certified) robustness region. \n\nAs you stated, researchers may only care about the nearest extant adversarial example (AE), however, this AE indeed only measures the minimum effort required for finding an AE, while the ALM can be seen as a measure for showing the average effort required for finding an AE. These two metrics just reflect two different aspects of the certified defense, so we provide both the nearest measure ($\\ell_p$ radius in Corollary 3.3) and the average measure (ALM). Our method significantly improves the certification performance under both measures.\n\n> Given that the certified distances to the nearest possible adversarial example are unchanged by your work, I do not see how this leads to an improved understanding of adversarial risk. I would argue that rather than significantly improving upon SOTA, you're introducing a new metric to mask the fact that you do not appear to produce any level of outperformance.\n\nAs discussed before, our method drastically improves the robustness against both the certified distance to the nearest possible AE (please see ``certified accuracy w.r.t. radius'' in Table 2-4 in our original submission) and the average distance measure (ALM).\n\n> The idea of including a headline figure of an 182.6% improvement over SOTA - anyone reading this who was not familiar with this field would assume that this would be an apples-to-apples comparison, but it's not. There's no SOTA for certification that cares about area driven measures of certification, and so claiming a comparison to these prior techniques is not reasonable or well justified.\n\nThe 182.6% gain over SOTA is under the evaluation of a  **standard certified $\\ell_p$ radius** (derived in Corollary 3.3, and measured via the ``certified accuracy w.r.t. radius'' in Table 2-4), which is an apples-to-apples comparison.\n\n2. **Addressing Minor Technical Issues**:\n\nThanks for your observations regarding the minor issues. We have revised the notation to  \\$||z||_\\infty\\$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010298080,
                "cdate": 1700010298080,
                "tmdate": 1700012196798,
                "mdate": 1700012196798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RoJRGyrdMP",
                "forum": "bDooTVT4t2",
                "replyto": "OKIDueDlj1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_eeVj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_eeVj"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for response. I appreciate the effort placed into it, and have been following your discussions with the other reviewers closely. While you have resolved some of the issues I had originally held with the paper, the broader discussions around this paper do not leave me confident that changing my review would be justified, although I will keep an eye on this over the remainder of the review period. \n\nThere was one point in your rebuttal response that I wanted to directly address though. While I can see why you might argue that the ALM is a a measure of the average effort required for finding an AE, I think this is the kind of statement that would require significantly more evidence to establish - especially when adversarial examples often exist at a distance that can be well over an order of magnitude larger than the certified radii themselves (as shown in another ICLR paper under review). I would believe that the ALM only would measure the average effort required to attack a sample if the certifications were tight (which they aren't), or if the search algorithm was very naive."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363143793,
                "cdate": 1700363143793,
                "tmdate": 1700363143793,
                "mdate": 1700363143793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AcbxU6ZHbR",
            "forum": "bDooTVT4t2",
            "replyto": "bDooTVT4t2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_mJxe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6877/Reviewer_mJxe"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at improving certified robustness by randomized smoothing with anisotropic noise. The universal theory for certification with anisotropic noise has been provided. The authors consider three kinds of customizing anisotropic noises, and provide corresponding noise generation methods. The authors conduct experiment to demonstrate that the proposed UCAN method achieve state-of-the-art performance compared to existing randomized smoothing-based methods for certified robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method on smoothing with anisotropic noise is novel. It is interesting to see the expansion of RS-based methods from isotropic noises to anisotropic ones.\n2. This paper provides the theoretical guarantee of certified robustness under anisotropic randomized smoothing, and comprehensive analyses to transform existing randomized smoothing methods to anisotropic cases.\n3. Authors consider three different kinds of anisotropic noises, and provide a novel input-dependent one by optimizing $\\mu(x)$ and $\\sigma(x)$ by a multi-layer neural network."
                },
                "weaknesses": {
                    "value": "1. My major concern of this paper is the potential unfairness in evaluation on UCAN and existing RS-based methods. If I am not misunderstanding, the evaluation criterion is based on scaled radius, which has different weight in each dimension.\n\nI believe this is true because I am surprised to the evaluation results provided in Table 3 that the $l_2$ certification on CIFAR-10 reach over 70% even under radius 1.75. By simple calculation, for commonly-used $L_{\\infty}$ norm with budget 8/255, it achieves at most $\\sqrt{3072 \\times (\\frac{8}{255})^2} \\approx 1.74$, that means in your UCAN it achieves at least 70% robust accuracy for 8/255 $l_{\\infty}$ attacks. This result is unbelievable, because existing SOTA performance of CIFAR-10 robustness may only achieve about 60% if no further data augmentations are conducted (like diffusion model), let alone UCAN is only a certified method based on $l_2$ norm. Therefore, although the paper said they evaluate certified accuracy w.r.t radius, I am doubtful of this claim and I think the authors only consider scaled radius robustness.\n\nHowever, scaled radius certification seems not a fair criterion for certified robustness. It is reasonable that in some dimensions, the image is vulnerable to adversarial attacks, e.g., contour of an image. Reversely, in some dimension images are intrinsically robust to perturbations like background of the image. Therefore, I believe the corresponding variance $\\sigma$ is small when UCAN performs on these vulnerable dimensions, and gain robustness back in some ``unimportant\u2019\u2019 dimensions.\n\nOverall, the evaluation setting of this paper seems differently from existing RS methods. It is a consensus that using $l_p$ norm as constraint for images, the authors should provide corresponding evaluation on standard $l_p$ norm, or at least, provide the explanations or practical scenario on why using scaled radius as the evaluation criterion.\n\n2.\tIn Theorem 3.2, your certification using the p-norm of $\\frac{\\delta_i}{\\sigma_i},$ but it seems that $\\frac{\\delta_i}{\\sigma_i}$, is a one-dimension scalar as $\\delta_i$ is the i-th dimension of perturbation $\\delta$. Furthermore, this theorem is seemingly a direct corollary from Theorem 3.1, because your certification divides the variance $\\simga_i$ for each $\\delta_i$ (not anisotropic anymore?).\n\n3.\tThere might be missing of some baselines for $l_1$ [a] and $l_{\\infty}$ [b, c] certified robustness. It will be better to compare the UCAN with existing certified $l_1$ and $l_{\\infty}$ methods.\n\n[a] Levine et al. Improved, deterministic smoothing for L_1 certified robustness. In ICML 2021.\n[b] Zhang et al. Towards Certifying L_\u221e Robustness using Neural Networks with L_\u221e-dist Neurons. In ICML 2021.\n[c] Zhang et al. Boosting the Certified Robustness of L-infinity Distance Nets. In ICLR 2022."
                },
                "questions": {
                    "value": "1.\tWhy using 5-layers NN when generating universal/ input-dependent anisotropic noises? Is there some motivations or ablation studies for that?\n2.\tCould you provide more details on training of universal anisotropic noise? It seems that the variance loss is to optimize $\\sigma$ and smoothing loss containing $\\sigma$ when optimizing classifier $\\theta_f$. I believe the two losses are optimized alternately but not simultaneously.\n3.\tThe authors said that randomized smoothing achieved great success for certified adversarial robustness. Could RS really make classifier robust? Can you provide comparison of RS based model to the SOTA methods for achieving robustness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6877/Reviewer_mJxe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6877/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676089385,
            "cdate": 1698676089385,
            "tmdate": 1699636799222,
            "mdate": 1699636799222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NltlZbLeD7",
                "forum": "bDooTVT4t2",
                "replyto": "AcbxU6ZHbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful comments and please find our clarifications as below. We also make more clarifications in the paper to avoid confusion/misunderstanding. \n\n1. **Regarding the Evaluation Concerns Raised**:\n\n> If I am not misunderstanding, the evaluation criterion is based on scaled radius, which has different weight in each dimension.\n\n\nOur experimental evaluations are based on both 1) the **$\\ell_p$ radius** from Corollary 3.3 ($min  \\sigma_i  R$), which is a tight bound for the perturbation in anisotropic RS, and has the same weight in each dimension (please see Table 2-4 in the original submission); 2) the ALM metric ($\\sqrt[d]{\\prod^d_i \\sigma_i} R$), which is a measure on the volume of the irregular asymmetric robust region, and has different weights in each dimension.\n\n\n> I believe this is true because I am surprised to the evaluation results provided in Table 3 that the $\\ell_2$ certification on CIFAR-10 reach over $70$% even under radius 1.75.\n\nIn Table 3, the **70%** certified accuracy at $R=1.75$ on CIFAR10 is the performance based on the *$\\ell_p$ radius* ($min\\{\\sigma_i\\} R$), which is strictly derived in Corollary 3.3. This validates the significant improvement of RS with anisotropic noise over SOTA methods. \n\n> This result is unbelievable, because existing SOTA performance of CIFAR-10 robustness may only achieve about 60\\% if no further data augmentations are conducted (like diffusion model)\n\nWe agree with the reviewer that existing isotropic RS methods can hardly improve their performance to this level. However, we focus on a new dimension of RS (with anisotropic noise) that is orthogonal to existing RS methods, which can boost all the RS methods. Such results are based on $\\ell_p$ radius, rather than scaled radius. \n\n>Therefore, although the paper said they evaluate certified accuracy w.r.t radius, I am doubtful of this claim and I think the authors only consider scaled radius robustness.\n\n\nWe understand the reviewer's concern as our results drastically outperform the SOTA methods (evaluated on both $\\ell_p$ radius and ALM). To address the concern, we are providing our code in the supplemental material for transparency. Please see line 180-182 in certification\\_personalized.py for the implementation of our **$\\ell_p$ radius** (and corresponding evaluations).\n\n> However, scaled radius certification seems not a fair criterion for certified robustness. It is reasonable that in some dimensions, the image is vulnerable to adversarial attacks, e.g., contour of an image. Reversely, in some dimension images are intrinsically robust to perturbations like background of the image.\n> Overall, the evaluation setting of this paper seems differently from existing RS methods. It is a consensus that using $\\ell_p$ norm as constraint for images, the authors should provide corresponding evaluation on standard $\\ell_p$ norm, or at least, provide the explanations or practical scenario on why using scaled radius as the evaluation criterion.\n\nWe agree with the reviewer that the ALM (scaled radius) may result in some ``weak'' dimension for the adversary, which only provides the certified robustness in a specific shape (depends on $\\sigma$).\n\nWhile working on the paper earlier, we also understood that reviewers may concern that solely evaluating on the ALM might be unfair (though ALM is defined as a full capture of the robustness region). Thus, we have also derived and presented the $\\ell_{1,2,\\infty}$ radii (derived in Corollary 3.3), and evaluated the performance of our method and SOTA using the ``certified accuracy w.r.t. $\\ell_p$ radius'' (please see Table 2-4) in our original submission. We show that by customizing appropriate anisotropic noise, our method achieves significantly boosted performance on both the standard $\\ell_p$ radius and the new ALM metric."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009294860,
                "cdate": 1700009294860,
                "tmdate": 1700012118055,
                "mdate": 1700012118055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pvGCkREj41",
                "forum": "bDooTVT4t2",
                "replyto": "AcbxU6ZHbR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. **In Response to the Theoretical Concerns**:\n\n> In Theorem 3.2, your certification using the p-norm of $\\delta_i/\\sigma_i$ but it seems that $\\delta_i/\\sigma_i$, is a one-dimension scalar as is the i-th dimension of perturbation $\\delta$.\n\n\n\nThe Hadamard division of $\\delta$ over $\\sigma$ ($\\delta_i/\\sigma_i$) is formally defined in Theorem 3.2 and Appendix A. \n\n> Furthermore, this theorem is seemingly a direct corollary from Theorem 3.1, because your certification divides the variance $\\sigma_i$ for each $\\delta_i$ (not anisotropic anymore?).\n\n\nWe ensure that Theorem 3.2, while appearing to be straightforward, is a unique universal transformation from Definition 3.1, as detailed in Appendix A. We respectfully clarify that the derived theories in the theorem are not a direct corollary from Theorem 3.1 (since it covers both strict robustness w.r.t. heterogeneous dimension and universality). The guarantee in Theorem 3.2 is anisotropic w.r.t. $\\delta$, although our focus is not to derive an ``anisotropic radius'', but to extend isotropic noise to anisotropic noise for RS. The guarantee on the perturbation $\\delta$ can be either anisotropic (Theorem 3.2) or isotropic (Corollary 3.3).\n\n3. **As per the Suggestion to Compare with $\\ell_1$ and $\\ell_\\infty$ Methods**: \n\n\n> There might be missing of some baselines for $\\ell_1$ [a] and $\\ell_\\infty$ [b, c] certified robustness. It will be better to compare the UCAN with existing certified $\\ell_1$ and $\\ell_\\infty$ methods.\n\nOur method, when benchmarked against $\\ell_1$ and $\\ell_\\infty$ methods, shows significant improvements (see tables provided). This underscores our method's effectiveness across different metrics. We respectfully clarify that RS-based methods and $\\ell_\\infty$-distance neural network-based methods may not be directly comparable (due to their differences, e.g., RS can certify any classifier). To further address this concern, we still compare with them, and our method also demonstrates superior performance in these scenarios (a huge improvement space brought by the anisotropic noise for RS).\n\n| Methods                  | 0.0  | 0.5  | 1.0  | 1.5  | 2.0  |\n|--------------------------|------|------|------|------|------|\n| Yang et al. 2020          | 83%  | 43%  | 22%  | 14%  | 7%   |\n| Levine et al. 2021        | 79%  | 71%  | 61%  | 54%  | 49%  |\n| Ours ($\\ell_1$ radius)  | **85%** | **81%** | **77%** | **73%** | **68%** |\n\n*Table 1. Certified accuracy vs. $\\ell_1$ perturbation (CIFAR10)*\n\n\n| Methods                  | 0/255 | 1/255 | 2/255 | 3/255 | 4/255 | 5/255 | 6/255 | 7/255 | 8/255 |\n|--------------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| Yang et al. 2020         | 83%   | 63%   | 48%   | 36%   | 27%   | 20%   | 16%   | 13%   | 10%   |\n| Zhang et al. 2021         | 51%   | --    | --    | --    | --    | --    | --    | --    | 35%   |\n| Zhang et al. 2022         | 61%   | --    | 54%   | --    | --    | --    | --    | --    | 40%   |\n| Ours ($\\ell_\\infty$ radius)  | **85%** | **83%** | **82%** | **80%** | **78%** | **77%** | **75%** | **73%** | **70%** |\n\n*Table 2. Certified accuracy vs. $\\ell_\\infty$ perturbation (CIFAR10)*\n\n4. **Responses to Specific Questions**:\n\n> Why using 5-layers NN when generating universal/ input-dependent anisotropic noises? Is there some motivations or ablation studies for that?\n\nThe 5-layer MLP architecture in our Universal method is inspired by GANs (Goodfellow et al., Communications 2020), and the CNN structure in our Input-dependent method is designed per the dense blocks (Huang et al., CVPR 2017). \n\n> Could you provide more details on training of universal anisotropic noise? It seems that the variance loss is to optimize $\\sigma$ and smoothing loss containing $\\sigma$ when optimizing classifier $\\theta_f$. I believe the two losses are optimized alternately but not simultaneously.\n\n\nFor the training of NRG and the classifier, we employed simultaneous training, optimizing both for predictive accuracy and noise pattern enhancement. While alternate training is possible, our current approach promotes synergy between the classifier and NRG.\n\n> The authors said that randomized smoothing achieved great success for certified adversarial robustness. Could RS really make classifier robust? Can you provide comparison of RS based model to the SOTA methods for achieving robustness?\n\nPlease see the above experimental results and discussion. It is worth noting that RS methods can be universally applied to any classifier with different scales, and it is unfair to compare the robustness performance of different types of classifiers.\n\nThanks for the comments again. We hope these clarifications can address your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009995469,
                "cdate": 1700009995469,
                "tmdate": 1700010394587,
                "mdate": 1700010394587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hW0kAD74As",
                "forum": "bDooTVT4t2",
                "replyto": "NltlZbLeD7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_mJxe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6877/Reviewer_mJxe"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses.\n\nAlthough a part of my concerns has been solved, there are still some issues in the current version of this paper. \n\nFirst, I am still doubtful of the soundness of your experimental results. Specifically, as shown in your Tables 2 and 3, the certified accuracy w.r.t. radius is greater than whose w.r.t. ALM is confusing. The guarantee w.r.t. radius should always less than those w.r.t. ALM because $\\min\\{\\sigma_i\\} \\leq \\sqrt[d]{\\prod_{i=1}^n \\sigma_i}$.\n\nFurthermore, as reviewer SMHf and review v1KJ pointed out, the proposed certification may be not true. You said that what you guarantee is $g(x+\\delta, \\mu(x), \\sigma(x))= g(x, \\mu(x), \\sigma(x))$ rather than $g(x+\\delta, \\mu(x+\\delta), \\sigma(x+\\delta))= g(x, \\mu(x), \\sigma(x))$. However, what is your (smoothed) classifier? I believe your classifier is $h(x)=\\mathbb{E}_{\\mu, \\sigma} g(x+\\delta, \\mu(x), \\sigma(x))$, then the guarantee should be hold on $h(x+\\delta)$ for all $\\|\\| \\delta \\|\\|_p\\leq \\epsilon$."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6877/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700137568768,
                "cdate": 1700137568768,
                "tmdate": 1700137568768,
                "mdate": 1700137568768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]