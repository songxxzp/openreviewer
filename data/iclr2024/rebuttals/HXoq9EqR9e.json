[
    {
        "title": "FairVLM: Mitigating Bias In Pre-Trained Vision-Language Models"
    },
    {
        "review": {
            "id": "CsrrGrPdSc",
            "forum": "HXoq9EqR9e",
            "replyto": "HXoq9EqR9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_75gn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_75gn"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors address the fairness problems in visual-language models (VLMs). More specifically, they propose a framework for jointly debiasing VLMs\u2019 image and text representations. The proposed framework utilizes an alternating optimization-based approach to debias VLM representations. The authors evaluate their work using several datasets and show that FairVLM alleviates the debiasing problems of vanilla VLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Compared to previous debiasing works in VLMs, FairVLM results in the debiasing of both image and textual representations.\n\n2. FairVLM is agnostic to the availability of data labels and can generate debiased representations with or without labels.\n\n3. Using the properties of RKHS in mapping the original VLM representations to a debiased space is interesting."
                },
                "weaknesses": {
                    "value": "1. The authors did not compare their results with unimodal baselines, i.e., techniques that debias only the image/text representations of the VLM.\n\n2. While the authors argue that RKHS has nice universal approximation properties, it's unclear how and why they aid in debiasing the original representations."
                },
                "questions": {
                    "value": "Please refer to the weakness section for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698261051929,
            "cdate": 1698261051929,
            "tmdate": 1699636335672,
            "mdate": 1699636335672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67RT5NoEQg",
                "forum": "HXoq9EqR9e",
                "replyto": "CsrrGrPdSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments of Reviewer 75gn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below are our responses to individual questions.\n\n> ****W1**** The authors did not compare their results with unimodal baselines, i.e., techniques that debias only the image/text representations of the VLM.\n> \n\n****A1**** We compared our method with various baselines, both with and without ground truth $Y$. Among those, Orth-Cali (Chuang et al., 2023) only debiases the VLM\u2019s text representations, and Contrastive Adapter (Zhang & R\u00e9, 2022) debiases only the VLM\u2019s image representations.\n\nMoreover, FairVLM itself can be adapted to debias only one of the modalities by setting $\\tau_I$ or $\\tau_T$ to zero. In either case, we will have a closed-form solution for the encoder parameters in a single shot and will not need to perform any alternating optimization. \n\nOur motivation for debiasing text and image encoders stems from prior studies demonstrating bias in both image encoders [R1-R3] and text encoders [R4-R8]. Subsequently, to achieve our goal of proposing a method that can perform under different settings and conditions of debiasing, we made FairVLM controllable so that it can easily be adapted to an unimodal debiasing setting by only setting $\\tau_I$ or $\\tau_T$ to zero.\n\n> ****W2**** While the authors argue that RKHS has nice universal approximation properties, it's unclear how and why they aid in debiasing the original representations.\n> \n\n****A2**** The key to the effectiveness of FairVLM is debiasing with HSIC type measure of dependence, which measures non-linear dependence. HSIC is typically estimated through RKHS. And, as we explain on page 4, under the Choice of Encoder paragraph, RKHS lends itself to closed-form solutions for the optimization problem, which is vital when optimizing competing objectives. SGD-type algorithms are not well suited in such cases, as is common in GANs and other adversarial learning methods. We repeat our description in the paper below for completeness:\n\n\u201cOur choice of RKHS is motivated by several reasons. Debiasing is inherently an optimization problem with multiple competing objectives. In such cases, optimization is the primary bottleneck rather than model expressivity. This was also observed in Sadeghi et al. (2022). The closed-form solution afforded by our approach mitigates the optimization challenges (Sec. 4.3 and App. A.6).\u201d\n\nMoreover, in Section 3.1 and Figure 4, we provide a geometric illustration of how FairVLM works. Using a universal RKHS like RBF kernel maps the data into infinite dimensional space where all attributes are perhaps amenable to disentangling through linear mappings, thus allowing us to estimate the correct direction of $Y$ and $S$ labels. In this space, a linear correlation can capture all types of dependencies, i.e., all linear and non-linear dependencies.\n\nIf our responses addressed your initial comments, please consider raising the score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101586391,
                "cdate": 1700101586391,
                "tmdate": 1700101586391,
                "mdate": 1700101586391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bYuC0neSPb",
                "forum": "HXoq9EqR9e",
                "replyto": "67RT5NoEQg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_75gn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_75gn"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttal response. I will wait for the comments from all other reviewers. For now, I will keep my rating of \"weak accept.\""
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325651718,
                "cdate": 1700325651718,
                "tmdate": 1700325651718,
                "mdate": 1700325651718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rM8hiPzBaQ",
            "forum": "HXoq9EqR9e",
            "replyto": "HXoq9EqR9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_9VkB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_9VkB"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggested a VLM debiasing method that remove bias in visual and text representations jointly by using reproducing kernel Hilbert space and deploying statistical dependency measure in RKHS. This enables to considering nonlinearity between the representation and the attribute. This paper provides a closed form solution for such formulation, and theoretical analysis on its complexity. Experiments show that the suggested method can work well both with and without true labels setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Paper is well written and organization is clear.\n- While previous methods mainly depend on the linearity of representation and the attribute, the suggested method can overcome such linearity assumption using RKHS.\n- The suggested method is practically competitive in both of settings \u2014 w or w/o labels.\n- Ablation experiments cover various scenarios, providing solid understanding of variables that affect performance."
                },
                "weaknesses": {
                    "value": "While one of core features of VLM is zero-shot classification, the suggested method still requires parameter tuning (RBF kernel parameter) and stacking test data, which could be a limitation of the method."
                },
                "questions": {
                    "value": "- Can this method be extended to a single point debiasing? (i.e. a single point inference for online prediction?)\n- It looks like FairVLM sacrifices average scores more than other methods such as Contrastive Adapter in Table 2 w/ labels result. Furthermore, FairVLM works similarly in w/labels and w/o labels in CelebA. What\u2019s a good interpretation on this?\n- Table 3 CFD results look interesting! Does it imply FairVLM has its strength when the number of training samples is limited? How do other zero-shot methods in CFD dataset?\n- Is there any convergence guarantee for Algorithm 1 (FairVLM Training Without Labels)? Also, I am wondering if there can be a failure mode that the errors in the initialization step propagate further in iteration steps.\n- Possibly related works\n    - Chen, A. S., Lee, Y., Setlur, A., Levine, S., & Finn, C. (2023). Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features.\u00a0*arXiv preprint arXiv:2302.05441*.\n    - Adila, D., Shin, C., Cai, L., & Sala, F. (2023). Zero-Shot Robustification of Zero-Shot Models With Foundation Models.\u00a0*arXiv preprint arXiv:2309.04344*.\n    - An, B., Zhu, S., Panaitescu-Liess, M. A., Mummadi, C. K., & Huang, F. (2023, July). More Context, Less Distraction: Improving Zero-Shot Inference of CLIP by Inferring and Describing Spurious Features. In\u00a0*Workshop on Efficient Systems for Foundation Models@ ICML2023*."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Reviewer_9VkB",
                        "ICLR.cc/2024/Conference/Submission3787/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698686571436,
            "cdate": 1698686571436,
            "tmdate": 1700319566263,
            "mdate": 1700319566263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l0MFXbhGCA",
                "forum": "HXoq9EqR9e",
                "replyto": "rM8hiPzBaQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments of Reviewer 9VkB (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. In response to the question regarding the performance of the other zero-shot models on the CFD experiment, we now evaluated more than 100 publicly available VLMs in addition to CLIP on CFD. The results provided more insight into understanding the challenging nature of this particular CFD scenario. In the following, we respond to each comment.\n\n    \n\n> ****W1**** While one of core features of VLM is zero-shot classification, the suggested method still requires parameter tuning (RBF kernel parameter) and stacking test data, which could be a limitation of the method.\n> \n\n****A1**** Please recall that we consider two scenarios. One where we have ground truth $Y$ labels and the other without $Y$ labels. In both cases, ground truth $S$ labels are not available. \n\nIn both cases, we have a train-test split where we learn on the train split and evaluate on the test split. So, in both cases, there is no stacking of test data, and we can evaluate a single test sample at a time. We stack the train data in both cases and select the RBF kernel parameter through cross-validation. This experimental design choice was motivated by supervised baselines, which train with ground truth $Y$. We do not perform any online updates for FairVLM.\n\nThe reviewer\u2019s comment suggests an alternative scenario where one learns FairVLM at test time and applies the learned model to the same test samples. In this scenario, yes, we have to stack the test data. However, even in this scenario, it is possible to pre-train FairVLM on a separate set of training samples (only images, without labels) obtained through other means and apply it to the test sample without training. For a completely novel task where samples for training (only images, without labels) cannot be obtained, we will have to stack test data for learning FairVLM. And, as we demonstrate, FairVLM can learn from limited data (e.g., CFD dataset with 597 samples).\n\n> ******Q1******  Can this method be extended to a single point debiasing? (i.e. a single point inference for online prediction?)\n> \n\n****A2**** Yes, as we explained in response to the previous comment, we do not train at test time but instead train on a separate dataset. So at test time, we can debias a single point.\n\n> ****Q2**** It looks like FairVLM sacrifices average scores more than other methods such as Contrastive Adapter in Table 2 w/ labels result. Furthermore, FairVLM works similarly in w/labels and w/o labels in CelebA. What\u2019s a good interpretation on this?\n> \n\n****A3**** We note that the Contrastive Adapter is tailored for the supervised setting and employs a complex anchor-based loss function. So, it is critically dependent on access to ground-truth labels. Therefore, it cannot be easily adapted to scenarios without ground-truth labels. In contrast, FairVLM is designed to be flexible and equally applicable to settings with and without ground truth labels without customization. The anchor-based loss function in the Contrastive Adapter can be integrated into FairVLM to improve performance under supervised learning at the cost of losing the flexibility in adapting to all different scenarios in terms of label availability.\n\nIt is worth mentioning that even with this flexible approach, FairVLM can outperform Contrastive Adapter in both the CelebA and Waterbirds datasets of the CLIP ViT-L/14  model in terms of worst-group accuracy (WG) and Gap (Table 2) and also the Chicago Face Database (Table 3 (right)). Furthermore, since FairVLM can also mitigate the representation's unfairness and spurious correlations, it outperforms other baselines, including Contrastive Adapter, in fairness experiments such as CelebA for high cheekbones (Table 1).\n\nIn the experiments reported in Table 2 on the CelebA dataset, the target attribute ($Y$) is blonde hair or not. Since CLIP models are trained on large-scale and diverse datasets, their zero-shot prediction for the worst group on easy-to-predict attributes like blonde hair is closer to the average across all groups. So, there is no significant performance gap between learning FairVLM with and without labels.\n\nTo support this claim, we extract the zero-shot prediction accuracy of the target labels presented in Table 2 and report it below again (Table R1). We observe for CelebA that the gap between Avg. and the worst group is lower than Waterbirds, and the average accuracy is reasonably high (~87%). So FairVLM\u2019s performance with and without labels is similar for CelebA, while the difference is larger for Waterbirds. \n\n********Table R1:  $\\hat{Y}$ zero-shot prediction accuracy by CLIP ViT-L/14********  \n\n|  | Avg. | WG |\n| --- | --- | --- |\n| Waterbirds | 84.4 | 45.3 |\n| CelebA | 87.6 | 72.8 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101379063,
                "cdate": 1700101379063,
                "tmdate": 1700101379063,
                "mdate": 1700101379063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S8Ef5VzOSF",
                "forum": "HXoq9EqR9e",
                "replyto": "rM8hiPzBaQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_9VkB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_9VkB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed clarification and updated results; most of my initial comments are nicely addressed. I have a follow-up comment on A2.\n\n```\nA2. Yes, as we explained in response to the previous comment, we do not train at test time but instead train on a separate dataset. So at test time, we can debias a single point.\n```\n\nMy comment was about when such a separate dataset is unavailable, and we want to do zero-shot prediction for a single test data point. In my understanding, the answer is no. However, acquiring unlabeled dataset is typically not challenging, the proposed method remains valuable.\n\nWhile I find the technical contribution of this paper to be sound and solid, I suggest the authors provide further clarification regarding the specific category and limitations of their proposed method. When true labels are available, the method can be seen as an adapter approach that uses kernel methods outlined in the paper. In this view, it might be inappropriate to label it as \"zero-shot\" prediction, since the method requires a sort of adapter training on the specific task. In the absence of true labels, the method can be viewed as a test-time adaptation using pseudo-labels. One of potential limitation is that applying it to a single or a few test data points can be challenging.\n\nI have increased my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319545952,
                "cdate": 1700319545952,
                "tmdate": 1700319678842,
                "mdate": 1700319678842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sFmhTst7Fn",
            "forum": "HXoq9EqR9e",
            "replyto": "HXoq9EqR9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_2eeh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_2eeh"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces FairVLM, a novel approach designed to address bias in zero-shot predictions made by VLMs. FairVLM demonstrates versatility in mitigating bias arising from two primary sources: spurious correlations and intrinsic dependencies within the data. Moreover, it offers the flexibility to be trained with or without the presence of ground-truth labels."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper demonstrates that a single general method can debias the image and text features of VLMs under different scenarios more effectively than specialized solutions for each scenario. The scenarios include accounting for both spurious correlations and intrinsic dependencies, learning with and without ground-truth labels, and learning from small and medium-sized datasets\n2. The words are fluent."
                },
                "weaknesses": {
                    "value": "1. The experiment results on the datasets (w/ labels) are not good enough.\n2. The pare of the method is too complex."
                },
                "questions": {
                    "value": "I don't have any questions because I cannot understant the method part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no ethics concerns here"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Reviewer_2eeh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759848102,
            "cdate": 1698759848102,
            "tmdate": 1699636335521,
            "mdate": 1699636335521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w6tYD0MPTo",
                "forum": "HXoq9EqR9e",
                "replyto": "sFmhTst7Fn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments of Reviewer 2eeh"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below are our responses to individual questions.\n\n> ******W1****** The experiment results on the datasets (w/ labels) are not good enough.\n> \n\n****A1**** As pointed out by the reviewer as a strength, one of the main contributions of FairVLM is its ability to adapt to different settings to mitigate various types of biases. In this paper, we have conducted several experiments on FairVLM, including mitigating spurious correlation, mitigating intrinsic dependencies, and conducting experiments on datasets with limited training samples and both w/ and w/o ground-truth labels. In all of these settings, FairVLM either outperforms other baselines, e.g.,\n\n- In Table 2, WG and Avg. increased for all experiments (except CelebA on CLIP ResNet-50) in w/o label settings,\n- Chicago Face Dataset (CFD) in Table 3 (right)\n- Reduce MaxSkew fairness metric for the FairFace dataset in Table 3 (left).\n- Reduce the fairness metric EOD on the CelebA dataset in Table 1.\n- Improve the WG accuracy and decrease the Gap for experiments on CLIP ViT-L/14 in Table 2.\n\nOr it is second best in other experiments except for CelebA for CLIP ResNet-50 in w/ ground-truth labels.\n\nFurthermore, FairVLM enjoys computational advantages for training (see Table 4). On the Waterbirds dataset, FairVLM is 40 times more efficient than Contrastive Adapter, which has some accuracy advantages over FairVLM in two of the experiments on CLIP ResNet-50 and is five times more efficient than the other baselines. On the CelebA dataset that contains more samples, FairVLM is more than ten times faster than the model with the best accuracy, DRF (Upsample), and about 100 times faster than the Contrastive Adapter, which is the second-best method in terms of accuracy in that specific setting. It is worth mentioning that since the FairVLM employs kernel methods in its encoders, it can be scaled to medium-sized datasets using Random Fourier Features [R1], while the other baselines are not efficient even for the relatively small-sized datasets (CelebA and Waterbirds), we evaluated.\n\nIn conclusion, based on the information provided above, we maintain a robust belief that **suboptimal outcomes against specialized solutions occur in only two out of thirteen experiments.** These results amply demonstrate the overall value and effectiveness of FairVLM's contributions.\n\n> ******W2****** The pare of the method is too complex.\n> \n\n****A2**** Our solution employs kernel methods, which are part of the standard machine learning toolbox. Fig 4 in Section 3.1 provides a geometric illustration of FairVLM. Moreover, Fig. 2 provides an overview of the training and inference phases of the proposed method, and Fig. 3 explains our training algorithm as a figure. \n\n\nIf our responses addressed your initial comments, please consider raising the score.\n\n\n## ********************References********************\n\n[R1] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101198433,
                "cdate": 1700101198433,
                "tmdate": 1700101198433,
                "mdate": 1700101198433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EgV7yDhuMe",
            "forum": "HXoq9EqR9e",
            "replyto": "HXoq9EqR9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FairVLM, which is an additional module top on the frozen CLIP features, to de-bias the prediction. Using the frozen CLIP vision and text encoders, the proposed method extracts visual and texture features, and lets them be de-biased using the Hilbert-Schmidt Independence Criterion (HSIC), a famous approach in de-bias literature. Instead of using the original HSIC, this paper proposes to use a simplified version following Sadeghi et al. The proposed simplified HSIC provides a closed-form solution to the additional feature encoders when the features are fixed. To make the method efficient, this paper proposes to approximate Cholesky decomposition using random Fourier features (RFF), resulting in reducing the computational complexity from $O(n^3)$ to $O(n^2)$. Experimental results show that the proposed method shows the effectiveness of the proposed method in both intrinsic dependency (i.e., fairness scenario) and spurious correlation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "HSIC is a promising approach to achieving de-biased representations, as many previous studies have observed. This paper successfully brings the advantage of HSIC to CLIP feature refinement tasks. I also think that this paper has a non-trivial contribution to introducing the closed-form solutions used for updating the parameters, including the approximated version of Cholesky decomposition using RFF. Straightforwardly, an iterative algorithm using a closed-form solution will converge much faster than gradient-based algorithms, as shown in classic machine learning studies, such as ADMM [R1]\n\n- [R1] Boyd, Stephen, et al. \"Distributed optimization and statistical learning via the alternating direction method of multipliers.\" Foundations and Trends\u00ae in Machine learning 3.1 (2011): 1-122.\n\nCombining two good properties (HSIC, a promising approach, and an efficient update algorithm using a closed-form solution), the proposed method shows promising performances on the given evaluation benchmarks."
                },
                "weaknesses": {
                    "value": "### Scope of the paper\n\nThe terminology \"VLM\" is misused in this paper. VLM literally includes a vast area of models trained with vision and language. For example, visual-question answering (VQA) is a VLM model, vision-language pre-training (VLP) with cross-attention transformers (such as ViLBERT [R2], ViLT [R3], Align [R4], VinVL [R5], ALBEF [R6], BLIP [R7]) is VLM, multi-modal generation models, such as dall-e 1, 2 and 3, stable diffusion or dreambooth, are VLM, and recent language-model combined vision models, such as BLIP2 [R8], Fromage [R9], GPT-4, are VLM. (I omitted some famous works, such as dall-e, SD, GPT ...).\n\n- [R2] Lu, Jiasen, et al. \"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.\" Advances in neural information processing systems 32 (2019).\n- [R3] Kim, Wonjae, Bokyung Son, and Ildoo Kim. \"Vilt: Vision-and-language transformer without convolution or region supervision.\" International Conference on Machine Learning. PMLR, 2021.\n- [R4] Jia, Chao, et al. \"Scaling up visual and vision-language representation learning with noisy text supervision.\" International conference on machine learning. PMLR, 2021.\n- [R5] Zhang, Pengchuan, et al. \"Vinvl: Revisiting visual representations in vision-language models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n- [R6] Li, Junnan, et al. \"Align before fuse: Vision and language representation learning with momentum distillation.\" Advances in neural information processing systems 34 (2021): 9694-9705.\n- [R7] Li, Junnan, et al. \"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\" International Conference on Machine Learning. PMLR, 2022.\n- [R8] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" arXiv preprint arXiv:2301.12597 (2023).\n- [R9] Koh, Jing Yu, Ruslan Salakhutdinov, and Daniel Fried. \"Grounding language models to images for multimodal generation.\" arXiv preprint arXiv:2301.13823 (2023).\n\nHowever, the scope of this paper is very narrow compared to the entire VLM family. I feel that the title \"FairVLM\" and the terminology \"VLM\" are too much overclaimed, and a reader can misunderstand the focus of this paper. I think this paper should tone down its contribution and focus more specifically. This paper only targets a feature refinement method top on the frozen CLIP model, and the comparison methods are also methods using frozen CLIP encoders. Note that it is still a narrow topic in \"de-biasing CLIP zero-shot prediction\" because a number of works focus on the fine-tuning strategy [R10-12]. On the other hand, this paper relies on the pre-trained feature encoders that may weakens its contribution.\n\n- [R10] Wortsman, Mitchell, et al. \"Robust fine-tuning of zero-shot models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n- [R11] So, Junhyuk, et al. \"Geodesic multi-modal mixup for robust fine-tuning.\" arXiv preprint arXiv:2203.03897 (2022).\n- [R12] Vogt-Lowell, Kevin, et al. \"Robust Fine-Tuning of Vision-Language Models for Domain Generalization.\" IEEE High Performance Extreme Computing Conference (HPEC). 2023.\n\n### Missing related works\n\nIncluding [R1-12] in the first comment, there are a number of missing related works that should be discussed. For example, HSIC is a popular approach in de-biasing studies [R13, R14]. R13 directly optimizes the HSIC between features and sensitive attribute labels, similar to Dep(Z, Y) in the paper; R14 optimizes the HSIC between biased features and target features to avoid using sensitive attribute labels Y. If we extend our viewpoint to RHKS, there is work using MMD to achieve fairness [R15]. I omitted many HSIC-based regularization methods that could be related to this work, but if possible, it would be great to add more citations for methods using HSIC.\n\n- [R13] Quadrianto, Novi, Viktoriia Sharmanska, and Oliver Thomas. \"Discovering fair representations in the data domain.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n- [R14] Bahng, Hyojin, et al. \"Learning de-biased representations with biased representations.\" International Conference on Machine Learning. PMLR, 2020.\n- [R15] Jung, Sangwon, et al. \"Fair feature distillation for visual recognition.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\nAlso, in terms of employing pseudo-labels for the de-biasing optimization, I think this paper is also related to [R16]; where R16 is based on semi-supervised learning without the pseudo-label refinement process.\n\n- [R16] Jung, Sangwon, Sanghyuk Chun, and Taesup Moon. \"Learning fair classifiers with partially annotated group labels.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n### Unclear or missing details\n\nWhile reading the paper, I had trouble understanding the details of the paper. For example, it is still unclear to me how the \"encoder\" works. I presume that the encoder parameter $\\Theta$ is a linear projection and an RBF kernel is used for computing HSIC, but it is unclear. Also, there is no description of $r$ (r becomes the dimensionality of the generated representation) and the value of $r$ as well. It means that this paper does not provide any detail of RKHS hyperparameters, such as the dimensionality of the projection layer and the hyperparameter of the kernel method. Similarly, I cannot find any detail of the choice of the learning hyperparameters, such as the number of iterations, and batch size. It means that it is impossible or extremely difficult to reproduce the results in this paper. Overall, this paper is very hard to understand the method details and implementation details, although I think this paper has certain contributions in terms of the methodology development."
                },
                "questions": {
                    "value": "I think the technical contribution of this paper is sound and empirical evaluation results look reasonable. However, this paper has a critical problem in its writing and presentation, including many missing related works and details. Please check my initial review and respond to my concerns. Specifically, I would like to clarify all the details of how the method is implemented and trained in the revised version, which is not presented in the initial version. I think the initial version is improper to be published as an ICLR paper, but my concern is mostly around the writing, that could be improved during the revision period. I presume that the revised manuscript will need significant efforts, but mainly in the presentation, rather than technical enhancement. Hence, if the revised manuscript is sound and can resolve my initial concerns, I am willing to update my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3787/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830470130,
            "cdate": 1698830470130,
            "tmdate": 1700371610110,
            "mdate": 1700371610110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0npCfLoQaU",
                "forum": "HXoq9EqR9e",
                "replyto": "EgV7yDhuMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments of Reviewer p1sH (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. Below are our responses to individual questions.\n\n> ****W1****  The terminology \"VLM\" is misused in this paper. VLM literally includes a vast area of models trained with vision and language. I feel that the title \"FairVLM\" and the terminology \"VLM\" are too much overclaimed, and a reader can misunderstand the focus of this paper.\n> \n\n****A1**** We thank the reviewer for detailing the generality of the term \u201cVLM.\u201d We narrowed the focus of our presentation to \u201cdebiasing CLIP embeddings for zero-shot predictions.\u201d We renamed our approach to FairerCLIP and rephrased all instances of VLM with CLIP or zero-shot predictions with CLIP, as appropriate. We also changed the paper's title to \u201cFairerCLIP: Mitigating Bias in CLIP Representations for Zero-Shot Prediction.\u201d\n\n**To avoid confusing the other reviewers, we continue to use the term \u201cFairVLM\u201d in the rebuttal.**\n\n> ****W2**** I think this paper should tone down its contribution and focus more specifically. This paper only targets a feature refinement method top on the frozen CLIP model, and the comparison methods are also methods using frozen CLIP encoders.\n> \n\n****A2**** We have renamed our proposed method changed the title, and toned down our contributions and focus to \u201cdebiasing CLIP embeddings for zero-shot predictions.\u201d We hope that our changes address the reviewer\u2019s concerns.\n\nWhile our approach operates on a frozen CLIP model, some of the baselines we compare against perform fine-tuning on at least one layer in CLIP if not all.\n\n- **ERM Linear Probe** fine-tunes the last linear layer of CLIP.\n- **ERM Adapter** adds new linear layers and fine-tunes them\n- **DFR** fine-tunes the last layer of the CLIP\n\nDespite not fine-tuning CLIP, FairVLM outperforms those that fine-tune CLIP, reiterating its effectiveness.\n\n> ******W3****** Note that it is still a narrow topic in \"de-biasing CLIP zero-shot prediction\" because a number of works focus on the fine-tuning strategy [R10-12]. On the other hand, this paper relies on the pre-trained feature encoders that may weakens its contribution.\n> \n\n****A3****  Respectfully, we disagree with the reviewer that not fine-tuning the CLIP backbones weakens our contributions since. On the contrary, as we discuss below, operating directly on the features without fine-tuning is a more practical solution, especially under limited resources.\n\nFirst, fine-tuning requires heavy computation and typically more data for fine-tuning compared to not resorting to fine-tuning. These may not be available in many practical scenarios, especially zero-shot prediction scenarios. Without sufficient care, fine-tuning may lead to severe over-fitting. Second, we already compare to baselines (ERM Linear Probe, DFR) that perform fine-tuning and demonstrate that FairVLM outperforms them significantly. Thirdly, FairVLM can be employed to debias feature representations in downstream tasks without access to the original images or model weights. Finally, employing fine-tuning along with FairVLM is undoubtedly feasible if required.\n\nBased on the above, debasing frozen CLIP representations is not a narrow topic but has a wide range of practical application scenarios. \n\n> ******W4****** there are a number of missing related works that should be discussed. For example, HSIC is a popular approach in de-biasing studies [R13, R14]. R13 directly optimizes the HSIC between features and sensitive attribute labels, similar to Dep(Z, Y) in the paper; R14 optimizes the HSIC between biased features and target features to avoid using sensitive attribute labels Y. If we extend our viewpoint to RHKS, there is work using MMD to achieve fairness [R15]. I omitted many HSIC-based regularization methods that could be related to this work, but if possible, it would be great to add more citations for methods using HSIC.\n> \n\n****A4**** Thank you for providing such a comprehensive list of suggestions for related work; we really appreciate it. We added the three suggested papers (R13-R15) to the related discussion on page 3 of the paper in the **Choice of Dependence Measure** paragraph. There are potentially other methods that use HSIC as a regularizer. However, due to page limits for the main paper, we could not add more of them at this time.\n\n> ****W5**** in terms of employing pseudo-labels for the de-biasing optimization, I think this paper is also related to [R16]; where R16 is based on semi-supervised learning without the pseudo-label refinement process.\n> \n\n****A5**** From our understanding of R16, it has access to partially annotated sensitive attributes and estimates pseudo labels for the sensitive attribute for the rest of the training samples. In contrast, in our paper, the sensitive attributes are not available for FairVLM and other baselines, which makes our setting more challenging. We have added a citation to this paper on page 5 (colored in blue)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100310000,
                "cdate": 1700100310000,
                "tmdate": 1700100390662,
                "mdate": 1700100390662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cgg0KqbyDx",
                "forum": "HXoq9EqR9e",
                "replyto": "MwgYIbs8DP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification and revision.\n\n- [A1, A2] I think the revised version of the paper scope makes sense to me.\n- [A3] I still think that remaining the CLIP backbone as frozen (including fine-tuning a small part of the model) is still somewhat in a narrow scope. However, it does not mean that this weakness is for the reject decision. As my previous comment, I think this paper has a sound technical contribution and the empirical evaluation results look reasonable.\n- [A4, A5] Thanks for the answers and the corresponding revision.\n- [A6-A9] Thanks for the clarification. I totally understand that the page limitation is strict to include all the details. I would like to suggest citing Appendix A.1, A.3 (as well as the source code repository URL) in the final main paper for clarity. It is not a mandatory request, but a mild suggestion from the reviewer.\n- [A10] Thanks for your hard work during the rebuttal period. I have revised my rating from 5 to 6 (considering A3), and \"Presentation\" from 1 to 3. Thanks again for all your efforts and clarification."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372520908,
                "cdate": 1700372520908,
                "tmdate": 1700372520908,
                "mdate": 1700372520908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z7IkUHQmTj",
                "forum": "HXoq9EqR9e",
                "replyto": "MNn1ifmfe6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3787/Reviewer_p1sH"
                ],
                "content": {
                    "comment": {
                        "value": "I already revised my score to **6** (weak accept). I don't think the paper's contribution is as strong as **8**. I still think the scope of this paper is narrower than more general methods that cover fine-tuning and from-scratch training. *It does not mean* that the contribution of this paper is not strong enough to appear in the ICLR venue, but it means that it is a \"weakness\" of this work. Please note that the role of reviewers is to provide enough information to chairs, including the advantages and the negatives of the paper.\n\nThere could be some applications of this method, but still, this method is impossible to be applied to a from-scratching training scenario. On the other hand, previous de-biasing methods mainly tackle a from-scratching training scenario without a pre-trained model and bias labels [R13, 14 and more related works _(I will not cite them all because I don't think that this paper needs to cite the de-biasing methods)_]. Similarly, there are other CLIP variants that tackle the inherent bias of CLIP, usually based on fine-tuning [R10, R11, R12 _(Similarly, there could be more papers that I don't know, but I think this paper does not have to cite them all)_]. Resource limitation is, of course, a painful constraint to all of us (except a few large companies), but it cannot remove the weakness of this method. I still think that it is a weakness of this method, but not a significant weakness as a reject case."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3787/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490059507,
                "cdate": 1700490059507,
                "tmdate": 1700490059507,
                "mdate": 1700490059507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]