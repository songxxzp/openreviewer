[
    {
        "title": "Understanding and Tackling Over-Dilution in Graph Neural Networks"
    },
    {
        "review": {
            "id": "Uf0QhLPWiC",
            "forum": "sysX9XMGdF",
            "replyto": "sysX9XMGdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper identifies a limitation in Message Passing Neural Networks (MPNNs) related to the handling of node attributes in graph datasets, specifically in how attributes lose significance during attribute transformation and feature aggregation stages.\n2. Even single-layer MPNNs weaken node representations by combining attributes equally, leading to a phenomenon called over-dilution, where node features become diluted and less informative.\n3. To address this issue, the paper introduces Node Attribute Transformer (NATR), a transformer designed to operate on node attributes, producing more informative node representations and mitigating the problem of over-dilution in MPNNs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Figures 1 and 4 provide clear visual distinctions between the over-dilution phenomenon and existing phenomena of over-smoothing and over-squashing.\n2. Over-dilution provides an original perspective, broadening the scope of MPNN limitations.\n3. The phenomenon is assessed by dividing it into two sub-phenomena: intra- and inter-node dilution, along with the introduction of corresponding factors.\n4. NATR is incorporated with existing MPNNs to demonstrably counteract the effects of over-dilution on node classification and link prediction."
                },
                "weaknesses": {
                    "value": "1. The significance of the paper can be strengthened by exploring graph datasets, at least synthetic data, and ideally real-world examples, exhibiting over-dilution in deep layers without the presence of other phenomena, e.g., over-smoothing.\n2. Over-dilution is detected in the very first layer of MPNNs, distinguishing it from other phenomena, but there are no convincing real-world experiments to support the notable implications of this single-layer over-dilution.\n3. Over-correlation, documented in existing studies [Liu et al., 2023, Jin et al., 2022], aligns with over-dilution in the realm of *preservation of attribute-level information*, necessitating a comprehensive discussion to discern their nuanced differences.\n\nReferences\n* [Liu et al., 2023]: Enhancing Graph Representations Learning with Decorrelated Propagation, KDD'23\n* [Jin et al., 2022]: Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective, KDD'22"
                },
                "questions": {
                    "value": "1. What were the criteria for selecting the five real-world graph datasets, shown in Table 7, for studying over-dilution? \n2. Related to the previous question, is it the case that MPNNs were more susceptible to over-dilution on these datasets than other existing datasets?\n3. What characteristics were considered when choosing these datasets to ensure they accurately represent over-dilution without the interference of other phenomena, e.g., over-smoothing, over-correlation?\n4. How was it ensured that the superior performance achieved by NATR in settings with 4 or 5 layers, as demonstrated in Table 3, is solely attributed to reduced over-dilution and not a result of significantly mitigating *possibly more severe phenomena* such as over-smoothing, over-correlation?\n5. Were there insights into potential real-world scenarios or applications where single-layer over-dilution could have significant consequences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697639080312,
            "cdate": 1697639080312,
            "tmdate": 1700742093891,
            "mdate": 1700742093891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ihadwiwkyu",
                "forum": "sysX9XMGdF",
                "replyto": "Uf0QhLPWiC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "With sincerely thank you for recognizing our contributions, we've carefully addressed each point in the review to further elevate our work.\n\n___\n\n\n**W1: The significance of the paper exhibiting over-dilution in deep layers without the presence of other phenomena, e.g., over-smoothing**\n\nIdentifying over-dilution in deep layers without interference from other phenomena like over-smoothing is challenging due to the shared underlying cause, such as the size of the receptive field. \n\nTherefore, our focus was on demonstrating the impact of over-dilution on performance in a single layer, where other phenomena are less prevalent. \n\nFor a more detailed explanation, please refer to our response to W2 & Q4.\n\n___\n\n\n**W2 & Q4: Experimental evidence of NATR\u2019s effectiveness in addressing the over-dilution issue**\n\n\n- intra-node dilution\n\nWe may not have emphasized this sufficiently in the text, but Table 5 clearly demonstrates the effectiveness through a comparison between (A)- $\\textit{MLP}$ and (D)- $\\textit{NATR}_{\\text{MLP}}$. \n\nIn both models, message passing is not utilized, ruling out the issue of over-smoothing. \n\nIn this context, the performance improvement observed with NATR can be attributed to its ability to resolve over-dilution at the intra-node level, regardless of the over-smoothing issue. \n\nIt's important to note that even with an increased number of parameters in the MLP model, there was no observed improvement, further emphasizing NATR's unique capability in addressing over-dilution.\n\n- inter-node dilution\n\nWe demonstrated single-layer over-dilution using the Computers dataset as an example. Here, a typical node has 204 attributes (median value) and 19 neighbors (median degree). In this case, each attribute's representation dilutes to approximately 0.025% per layer with either the *mean* or *sum* aggregation operator.\n\nTable 9 in the Appendix reveals that NATR significantly outperforms MPNNs even with a single layer, as evident in examples (q) and (r). \n\nTypically, issues like over-smoothing, over-squashing, and over-correlation occur after multiple layers of message passing. \n\nHence, this demonstrates NATR's effectiveness in mitigating mainly over-dilution, both at the intra-node and inter-node levels.\n\n\n___\n\n\n**W3: Difference with over-correlation**\n\n\nAccording to Jin et al., the over-correlation refers to the phenomenon where the feature representations learned by the **stacked** network become highly correlated across different **dimensions**.\n\nIn the node-level representation of MPNNs, each dimension does NOT mean each attribute representation (for initial node representation, it is a summed value of the corresponding dimension from attribute-level representations).\n\nTherefore, the concept of over-dilution is distinct from overcorrelation.\n\nAlso, the intra-node dilution can occur even at the single-layer while overcorrelation is observed when stacking layers.\n\nHowever, we agree that it would be good to discuss its connection to overcorrelation because both of them are unique perspectives to construct more informative node representations.\n\n\n___\n\n\n**Q1, Q2, Q3: the criteria for selecting the datasets**\n\n\nWe selected benchmark datasets that contain attribute information.\n\nOGB offers dense node features (or one-hot vector) rather than explicit attribute indicators.\n\nFor the ogbl-ddi dataset, we were able to extract attributes in a fingerprint format (binary vector) using RDKit, which is why it has been included in our benchmark dataset selection.\n\nIf there is a larger dataset that contains attribute information, we will include it as our benchmark dataset.\n\n___"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088491964,
                "cdate": 1700088491964,
                "tmdate": 1700088491964,
                "mdate": 1700088491964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5C1OQKKywB",
                "forum": "sysX9XMGdF",
                "replyto": "gUbSkIHO3j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. After carefully examining all the reviews and their responses, my level of confidence regarding the evaluation has risen."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742074010,
                "cdate": 1700742074010,
                "tmdate": 1700742074010,
                "mdate": 1700742074010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OuCRIork2G",
            "forum": "sysX9XMGdF",
            "replyto": "sysX9XMGdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new limitation of Message Passing Neural Networks (MPNNs) (i.e., over-dilution). It shows two types of dilutions: intra-node dilution and inter-node dilution considering 1) the equal weight combination for attributes within each node, 2) the information from neighbors is diluted through aggregation. The authors also provide formal definitions of these concepts. To mitigate the problem, they propose a transformer-based method (NATR) by considering adaptively merging attribute representations. The experiments are conducted for link prediction and node classification tasks, showing the better performance of NATR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation to adaptively utilize attributes for each node is sound. \n2. The analysis about dilution factors and the formal definitions have some merits. \n3. The improvements in some datasets are impressive."
                },
                "weaknesses": {
                    "value": "1. While the authors conduct the experiments on both link prediction and node classification, they only use three datasets (i.e., computers, photo, and cora ML) for node classification. OGB datasets for node classification are not included. I would like to see some results on ogbn-arxiv or ogbn-product. Even if the model may not perform well on these datasets, I suggest the author provide some analyses or insights about what kind of datasets would benefit more by using the proposed model.\n2. To me, it's not very clear for some parts of the analysis (e.g., Sec 6.2). In Sec 6.2, the author investigates the performance for nodes with bottom 25% and top 25% of inter-dilution scores. But for the base model and the version with NATR, the formula of $\\delta^{inter}_{Agg}(v) $ should be different? For NATR, it uses Eq (11). For GCN, it uses Eq (7). So, I wonder if the nodes are separated by only considering the original inter-dilution factor (i.e., Eq (7))?\n3. The proposed method is claimed to solve both intra-dilution and inter-dilution. For intra-node dilution, the method can assign larger weights to more important attributes. However, it is unclear to me how the method addresses the inter-node dilution. I would suggest the author elaborate more on this part."
                },
                "questions": {
                    "value": "1. Based on my understanding, in the proposed method, each attribute has its own learnable representations. In this case, the number of learnable parameters will increase compared with previous MPNN baselines (e.g., GCN). I wonder whether the performance improvement is mainly from the increased number of learnable parameters. I would like to see some analyses in this regard.\n2. I am curious how the performance would change with different numbers of attributes in a graph. Is there any trend for this? \n3. Considering the motivation of avoiding the combination of attributes with different weights, I wonder if feature selection methods can help to alleviate this problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566171870,
            "cdate": 1698566171870,
            "tmdate": 1699636630472,
            "mdate": 1699636630472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iPa9jrbmPX",
                "forum": "sysX9XMGdF",
                "replyto": "OuCRIork2G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your feedback and would like to clarify any contributions that we may not have sufficiently emphasized in the current version.\n\n---\n\n\n\n**W1: Experiments on other datasets such as ogbn-arxiv or ogbn-product**\n\nSince Reviewer HxeZ also pointed this out, it seems we missed adding more explanation about this point.\n\nWe chose benchmark datasets containing attribute information, as our primary focus is on attribute-level representation.\n\nHowever, OGB offers dense node features rather than explicit attribute indicators.\n\n- ogbn-arxiv : 128-dimensional feature vector obtained by averaging the embeddings of words\n- ogbn-product : 100-dimensional feature vector obtained by PCA of bag-of-words features\n\nIn the case of the ogbl-ddi dataset, we successfully extracted attributes in a fingerprint format (binary vector) using RDKit and DrugBank DB.\n\nIf larger datasets become available that include explicit attribute information, we are certainly prepared to include them in our benchmark datasets.\n\n---\n\n\n\n**W2: The implication of the analysis in Section 6.2** \n\nYou are correct. Indeed, the subsets of the bottom 25% and the top 25% were separated following Eq (7) (as two-hops version) and we used the fixed subsets for a fair comparison between MPNNs and NATR.\n\nAs presented in Table 4, we delve deeper into how the NATR model exhibits performance improvements when combined with MPNNs.\n\nGCN, GAT, and SGC demonstrate notably lower performance on over-diluted nodes (bottom 25%).\n\nFor the same subsets, NATR shows a more significant improvement in performance on over-diluted nodes than less-diluted nodes. \n\nThis distinction highlights the effectiveness of NATR in mitigating over-dilution issue and illustrates the benefits of our proposed approach in enhancing model performance where MPNNs may struggle.\n\n\n---\n\n\n**W3: How the method addresses the inter-node dilution**\n\nTo address inter-node dilution, NATR's attribute decoder integrates attribute representations across all layers.\n\nThis integration helps prevent the dilution of a node's own information in the context of its neighboring nodes.\n\nThe final node-level representation of node $v$, $\\tilde{H}_v^{(M)}$ is calculated by combining two representations: $H_v^{(M)}$ for graph context information and $O_v^{(M)}$ for node $v$\u2019s specific information, exclusively.\n\nThis approach effectively mitigates inter-node dilution, maintaining distinct node information within the graph context, as described in Eq (11) and Figure 2 (b).\n\n---\n\n\n\n**Q1: The number of parameters of each model and the effect of additional parameters** \n\n\nIt is an acknowledged fact that the use of a transformer architecture in NATR leads to additional parameters and increased computational complexity. \n\nHowever, it is important to note that simply increasing the number of parameters in existing MPNNs does not yield performance improvements comparable to those achieved by NATR.\n\nYou can see the degradation of performance in MPNNs when the number of parameters (layers) increases in Table 3.\n\nSignificantly, NATR leverages the transformer architecture to effectively address the issue of over-dilution. \n\nThis not only enhances performance but also facilitates the use of deeper layers (please see Table 9 as well), demonstrating the utility and efficacy of our approach.\n\n\n---\n\n\n**Q2: the performance would change with different numbers of attributes in a graph**\n\n\nThe performance can be affected not only by the number of attributes but also by the complicated property of each attribute.\n\nTherefore, it is hard to figure out the trend of the correlation between performance and the number of attributes.\n\n---\n\n\n\n**Q3: feature selection methods can help to alleviate this problem**\n\nWe agree that feature selection methods can be beneficial in mitigating the issue of combining attributes with varying levels of importance. However, these methods typically apply uniform weighting across all nodes.\n\nNevertheless, it's crucial to recognize that the significance of attributes can vary from node to node. \n\nFor instance, while attribute 't' may be important for node A, it might not hold the same level of significance for node B.\n\nOur NATR model addresses this by employing a graph context-aware attribute decoder. \n\nThis decoder utilizes node-level representations as queries, which enables the assignment of greater weight to the more significant attribute representations for each individual node. \n\nThis approach ensures that the importance of attributes is determined in the context of each node's specific role and relationship within the graph.\n\n\n---\n\nThank you again for your valuable comments. \nWe have attempted to address each of your questions thoroughly, enhancing the clarity of our paper's contributions.\nPlease let us know if there are points that require further explanation!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088448921,
                "cdate": 1700088448921,
                "tmdate": 1700088562026,
                "mdate": 1700088562026,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VgNktWMXQy",
            "forum": "sysX9XMGdF",
            "replyto": "sysX9XMGdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses a recent challenge in the field of Graph Neural Networks (GNNs), particularly focusing on Message Passing Neural Networks (MPNNs), and introduces the issue of \"over-dilution\" where node attribute information is diminished in the final representation due to excessive aggregation from many attributes (intra-node dilution) or overwhelming information from neighboring nodes (inter-node dilution). The authors propose a novel transformer-based architecture that treats attribute representations as tokens, which, unlike being a replacement, is an augmentation to existing MPNNs. This model aims to preserve attribute-level information more effectively by using attention scores to weigh attribute representations in the context of the aggregated node-level representation. The paper claims to contribute a new perspective on the problem of over-dilution by defining and analyzing it, which is distinct from the commonly discussed limitations of MPNNs such as over-smoothing, over-squashing, and over-correlation. The proposed transformer-based solution is theoretically and empirically validated for its efficiency in maintaining attribute-level information within graph-structured data representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper introduces the concept of over-dilution, a novel perspective in the study of GNNs, particularly MPNNs, that goes beyond the well-studied limitations of over-smoothing, over-squashing.\n* The proposed transformer-based architecture is not only theoretically grounded but also empirically tested, providing a strong case for its effectiveness in combating the over-dilution problem. This dual approach enhances the credibility of the findings."
                },
                "weaknesses": {
                    "value": "* Experiments are not complete.\n* The story of this paper is weird. I don't know why the author include over-smoothing and over-squashing as a story and don't do any comparison between over-dilution and them."
                },
                "questions": {
                    "value": "* For baselines, I think GCNII can be moved into the main paper and can you do it on all datasets? Because GCNII can alleviate over-smoothing, which I think maybe relevant to the paper.\n* Also, How is GCNII experiments done? Have you tried hyperparameter searching on it?\n* For datasets, even the authors state that the complexity is acceptable, the datasets the paper used are all small datasets. Can you provide results and time comparison with backbone on some larger datasets? like ogb-arxiv or ogb-citation2(maybe too large, ogb-ppa can also be a good choice).\n* Can the authors provide number of parameters of each model with backbone? How to know the improvement is not the result of adding new parameters in the transformer?\n* Can the authors provide details on what's the relationship between over-smoothing,over-squashing and over-dilution? Theoretically and empirically?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ",
                        "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699111658455,
            "cdate": 1699111658455,
            "tmdate": 1700565389203,
            "mdate": 1700565389203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wW83DFKvg9",
                "forum": "sysX9XMGdF",
                "replyto": "VgNktWMXQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the very detailed feedback, it has been instrumental in refining our work and highlighting its strengths.\n\n\n---\n\n\n**W2 & Q5: Clarification on Over-Smoothing, Over-Squashing, and Over-Dilution**\n\nWe realize we may not have delved into the details as thoroughly as needed, although we have compared these concepts in Figure 1, we'll expand the discussion in the main text.\n\nAppendix Section A and Figure 4 provide further details both conceptually and theoretically.\n\nOver-dilution presents a novel and comprehensive concept, while being related to over-smoothing and over-squashing in terms of information distortion in graph structures. \n\n- Unique perspective on intra-node dilution\n\nWe acknowledge that our initial presentation may have overlooked the unique aspects of intro-node dilution.\nUnlike over-smoothing and over-squashing, which are primarily concerned with node-level representation, over-dilution encompasses both node-level and attribute-level representations, crucial for forming informative representations on graphs. \nThis phenomenon can arise at the intra-node level due to an excess of attributes within nodes, independent of the triggers for over-smoothing and over-squashing.\n\n- Comparison with over-smoothing\n\nIn contrast to over-smoothing, where node representations become increasingly similar due to the exchange of information across multiple hops, over-dilution can arise even in single-layer aggregations, as demonstrated in our primary discussion.\nTake, for example, a star graph structure, which is essentially a tree with a central node and several leaf nodes, each linked to the central node by a single edge.\nFollowing a single message-passing step, the central node may experience over-dilution, but over-smoothing among nodes is less likely to occur since the leaf nodes do not share features with one another.\n\n\n- Comparison with over-squashing\n\nRegarding over-squashing, we now understand that our initial explanation may have been too concise. Unlike over-squashing, which deals with long-range information propagation between nodes, over-dilution focuses on the attenuation of information at individual nodes, observable even in the first layer based on aggregation coefficients. \nThis distinction is evident in our analysis, where over-dilution is shown to be a prevalent issue even in short-range interactions, challenging the traditional understanding of information propagation in graph neural networks.\nConceptually, over-dilution focus on the preservation of information (formally $\\partial h_x^{(l)}/\\partial h_{\\color{Red}x}^{(0)}$), while over-squashing focus on the propagation ( $\\partial h_x^{(l)}/\\partial h_{\\color{Red}y}^{(0)}$).\n\n\n---\n\n\n**Q1 & Q2: Comparison with GCNII**\n\nFirst and foremost, it's essential to clarify that NATR works as a complementary model to any node embedding module, not as a competitor to a specific standalone model.\n\nAlthough GCNII is effective in mitigating over-smoothing, this does not necessarily make it superior to NATR, as NATR is specifically tailored to address the issue of over-dilution.\n\nWhen equipped with GCNII as its node embedding module, $\\textit{NATR}_{\\text{GCNII}}$ could potentially yield a more informative node-level query in the attribute decoder.\n\nIn Table 8 and Section F of the Appendix, we present the performance of NATR using different node embedding modules, demonstrating its compatibility rather than positioning it as a competitor.\n\nThis experiment involved a hyperparameter search, the details of which are outlined in Section E of the Appendix.\n\n---\n\n\n\n\n**Q3: Experiments on larger datasets such as ogb-arxiv, ogb-citation2, or ogb-ppa**\n\nYour point regarding dataset selection is well-taken. \n\nOur choice to use benchmark datasets with explicit attribute information was intentional, as we aimed to highlight NATR's capabilities in handling such data.\n\nFor OGB, we encountered datasets offering dense node features or one-hot vectors rather than explicit attribute indicators:\n\n- ogbn-arxiv : 128-dimensional feature vector of word embeddings\n- ogbl-ppa : one-hot vector\n- ogbl-citation2 : 128-dimensional word2vec features\n\nIn the case of the ogbl-ddi dataset, we successfully extracted attributes in a fingerprint format (binary vector) using RDKit and DrugBank DB. \n\nIf larger datasets that include explicit attribute information become available, or if you are aware of any, we would definitely be happy to consider them!\n\n\n---"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088372715,
                "cdate": 1700088372715,
                "tmdate": 1700088372715,
                "mdate": 1700088372715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ARDcGbJEqj",
                "forum": "sysX9XMGdF",
                "replyto": "4I8AyrFAE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response.  I'll raise my score to 5."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565375478,
                "cdate": 1700565375478,
                "tmdate": 1700565375478,
                "mdate": 1700565375478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1ujmYONdIH",
            "forum": "sysX9XMGdF",
            "replyto": "sysX9XMGdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to study a new pheonomenon named over-dilution in message passing neural networks (MPNNs). It refers to the diminishing importance of a node's information in the final node representations learned by the neural networks. The authors propose NATR to address the proposed over-dilution problem. The key idea is to learn an attribute encoder and then train another transformer-based attribute decoder where Q is the node embeddings from MPNNs and K, V are the embeddings output by the attribute encoder."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. Interesting new perspective to study the limitation of MPNNs.\n\nS2. Improved performance in tasks like link prediction and node classification."
                },
                "weaknesses": {
                    "value": "Please see questions below."
                },
                "questions": {
                    "value": "Q1. I am confused by Eq. (3). Isn't $z_t$ the same as the $t$-th value in $h_v^{(0)}$? Why is it $h_v^{(0)}$ rather than $h_v^{(k)}$ in some hidden layer $k$?\n\nQ2. Still about Eq. (3): this essentially measure some normalized correlation between one attribute and another attribute, i.e., how a infinitesimal perturbation on attribute $t$ would affect other attributes in node features $h_v^{(0)}$. It would be good to discuss the its connection to overcorrelation by Jin et al.\n\nQ3. Definition 3.2 is the same as Xu et al., so it is necessary to cite it in Definition 3.2.\n\nQ4. Hypothesis 2 seems related to degree fairness learned in several papers [1, 2, 3]. When the node degree is high, after normalization, the aggregation weight $\\alpha_{v, v}$ will be smaller than the sum of all other edge weights. It would be good to discussion some intrinsic connection to this line of work.\n\nQ5. Hypothesis 3 seems to be very related to over-squashing by Topping et al. It would be good to have more in-depth discussion on the difference between Hypothesis 3 and over-squashing.\n\nQ6. To me, it feels that NATR would help when the number of layers increases. But it seems the MPNNs used in experiments are pretty shallow. What would happen if we increase the layers to a larger number? How would NATR perform if we equip it with deep graph neural networks like RevGCN [4]? \n\nQ7. The over-dilution seems like some combination of feature correlation (Definition 3.1) and over-squashing (Definition 3.2, Hypothesis 3) to me. It would be better to discuss the difference between the over-dilution and these two scenarios.\n\n\n**References**\n\n[1] Tang, Xianfeng, et al. \"Investigating and mitigating degree-related biases in graph convoltuional networks.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n\n[2] Kang, Jian, et al. \"Rawlsgcn: Towards rawlsian difference principle on graph convolutional network.\" Proceedings of the ACM Web Conference 2022. 2022.\n\n[3] Liu, Zemin, Trung-Kien Nguyen, and Yuan Fang. \"On Generalized Degree Fairness in Graph Neural Networks.\" arXiv preprint arXiv:2302.03881 (2023).\n\n[4] Li, Guohao, et al. \"Training graph neural networks with 1000 layers.\" International conference on machine learning. PMLR, 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699608082398,
            "cdate": 1699608082398,
            "tmdate": 1700717197223,
            "mdate": 1700717197223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ovgw1DQZq4",
                "forum": "sysX9XMGdF",
                "replyto": "1ujmYONdIH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to express our gratitude for the thorough review and have addressed each concern in detail.\n\n---\n\n\n**Q1: Clarification on notations.**\n\n\nIn MPNNs, the initial node-level representation $h_v^{(0)} \\in \\mathbb{R}^d$ is a summation (or averaging) of attribute representations $z_t \\in \\mathbb{R}^d$, such that  $h_v^{(0)} = \\sum_{t \\in \\mathcal{T}_v} z_t$.\n\nTherefore, the $t$-th value of $h_v^{(0)}$ is NOT the same as $z_t$; instead, each dimension of $h_v^{(0)}$ is a summed value of the corresponding dimension from attribute-level representations.\n\nWe define the dilution phenomenon as two **cascaded** sub-phenomena as intra-node dilution\u2192 inter-node dilution.\n\nThe intra-node dilution occurs when constructing the initial node-level representation $h^{(0)}_v$ while the inter-node dilution occurs when aggregating node-level representations (for $h^{(k)}_v$).\n\nTherefore, we use the initial node-level representation $h^{(0)}_v$ (NOT $h^{(k)}_v$) in Eq. (3) to describe the intra-node dilution.\n\n\n---\n\n\n**Q2 & Q7: Connection to overcorrelation (Jin et al.).**\n\nAccording to Jin et al., the over-correlation refers to the phenomenon where the feature representations learned by the **stacked** network become highly correlated across different **dimensions**.\n\nAs we answered in Q1, each dimension of node-level representation does NOT mean each attribute representation (it is a summed value of the corresponding dimension from attribute-level representations.).\n\nTherefore, the concept of over-dilution (specifically, the intra-node dilution) is distinct from overcorrelation.\n\nAlso, the intra-node dilution can occur even at the single-layer while overcorrelation is observed when stacking layers.\n\nHowever, we agree that it would be good to discuss its connection to overcorrelation because both of them are unique perspectives to construct more informative node representations.\n\nWe will incorporate this discussion into the revised version of our paper.\n\n\n---\n\n\n**Q3, Q5, Q7: Difference with the influence distribution (Xu et al.) and over-squashing**\n\n\nWhile the influence distribution with Jacobian (Xu et al.) serves as a clear framework to analyze limitations of MPNNs, as utilized in the over-squashing paper (Topping et al.), our Definition 3.2 is NOT the same.\n\nThe numerator in our Definition 3.2 differs to reflect our unique perspective on how information is preserved within nodes (i.e. $\\partial h_x^{(l)}/\\partial h_{\\color{Red}x}^{(0)}$), in contrast to the approach of Xu et al. and Topping et al., which focus on information propagation between nodes (i.e. $\\partial h_x^{(l)}/\\partial h_{\\color{Red}y}^{(0)}$) .\n\n(we have appropriately mentioned and cited the use of the influence distribution with Jacobian in our work.)\n\n\nFor a detailed comparison with over-squashing, please refer to Figure 1 in the main text and Section A with Figure 4 in Appendix.\n\n\n---\n\n\n**Q4: Relationship between degree fairness (normalization) papers.**\n\nThanks for the suggestion of the discussion about potential link between our Hypothesis 2 and the concept of degree fairness!\n\nOur Hypothesis 2 indeed provides a comprehensive framework to examine the interplay between over-dilution and aggregation weight $\\alpha$ that can be defined not only by degree but also by factors like the attention coefficient, as seen in GAT models.\n\nRecognizing this, we agree that it is valuable to delve into a more detailed discussion of the connections with these specific works, thereby enriching our analysis of the over-dilution phenomenon and its broader implications.\n\n\n---\n\n\n**Q6: The performance of NATR when the number of layers increases.**\n\nAs you correctly noted, NATR indeed retains its advantages even as the number of layers increases. \n\nIt's important to emphasize that our primary goal is to demonstrate the effectiveness of NATR in solving the over-dilution phenomenon, rather than specifically focusing on building deep GNNs.\n\nIn our main experiments, we found that just 5 layers were sufficient to saturate the size of receptive field (please refer to Figure 2 (b)) and to significantly degrade the performance of MPNN models, effectively demonstrating the efficacy of NATR in such a context.\n\nHowever, to further illustrate NATR's robustness with an increased number of layers, we\u2019ve extended our experiments up to 12 layers, as shown in Table 9 of the Appendix. \n\nThese additional experiments have confirmed that NATR not only maintains its performance but also improves upon it compared to the results observed with 5 layers in the main text.\n\n---\n\nYour feedback has been invaluable in enhancing our paper. \nWe believe we have addressed each point and hope our revisions have made our contributions more evident. \nPlease let us know if any question still needs clarification."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088310731,
                "cdate": 1700088310731,
                "tmdate": 1700089320934,
                "mdate": 1700089320934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TZGcJoP720",
                "forum": "sysX9XMGdF",
                "replyto": "ovgw1DQZq4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for your response. My concerns are addressed, and I am raising my score. I believe the paper quality could be further improved if the above discussions can be incorporated into the revised version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717182356,
                "cdate": 1700717182356,
                "tmdate": 1700717182356,
                "mdate": 1700717182356,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]