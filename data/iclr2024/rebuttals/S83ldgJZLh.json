[
    {
        "title": "A Structured Pruning Algorithm for Model-based Deep Learning"
    },
    {
        "review": {
            "id": "pYjTO2Zixc",
            "forum": "S83ldgJZLh",
            "replyto": "S83ldgJZLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_hcWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_hcWa"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a pruning approach for model based deep learning. They present structured pruning algorithm based on DepGraph and also considers three approaches to fine tune the model after the pruning to restore lost accuracy due to the pruning. They demonstrate the effectiveness of their approach by applying it to MRI and SuperResolution databases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Motivation and experiments are well presented well. Experiments gives support to their their approach."
                },
                "weaknesses": {
                    "value": "The explanation of the methods could be more clear. As I am not familiar with MBDL methods they discuss in the paper or DepGraph, I cannot really follow that how this pruning works. For example, \"$f_{\\theta,i}$ has dependency on $f_{\\theta,j}$\" is very briefly explained but it seems to be some what a key ingredient of the proposed pruning approach. And also how the layers are actually removed?   Probable not just taking a layer out of the network, as it would need some additional assumptions about compatibility of the input and output dimensions of the layer, plus not sure if network give any meaningful output after such removal. Authors could improve the presentation. For example, they could one MBDL method and describe in details that how their approach is applied to it. (e.g. Table 2 and Figure 4 can be removed or moved to a supplementary material if more space is needed) \n\nAlso, probable due to limited understanding, I fail to see that how this approach is for MBDL instead of being more general pruning method. Is there something in the pruning approach which required the model to be MBDL instead of, say, a typical classification CNN or ResNet?  I can only see that the model A is used in self-supervised fine tuning approach (Eq (10)), but its value seems quite low due to worst performance in the results. Also it could be clarified that what is their contribution or novelty with respect to DepGraph."
                },
                "questions": {
                    "value": "Speed up seems quite low compared to pruning ratio. For example, 10% pruning only seems to give 0-3% speedup and E2EVar (Table1) has only 24% speedup with 65% pruning ratio. Does pruning remove lower-complexity layers, does times include possible compilation times (XLA, jit, etc), or how this can be explained?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Reviewer_hcWa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698142331832,
            "cdate": 1698142331832,
            "tmdate": 1700555552878,
            "mdate": 1700555552878,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ndx0WeOVTu",
                "forum": "S83ldgJZLh",
                "replyto": "pYjTO2Zixc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer hcWa"
                    },
                    "comment": {
                        "value": "Thanks for your feedback on our work. Please find our point-to-point responses below.\n\n> 1: The explanation of the methods could be more clear. As I am not familiar with MBDL methods they discuss in the paper or DepGraph, I cannot really follow that how this pruning works. For example, \"$f_{\\theta, i}$ has dependency on $f_{\\theta, j}$\" is very briefly explained but it seems to be some what a key ingredient of the proposed pruning approach. And also how the layers are actually removed? Probable not just taking a layer out of the network, as it would need some additional assumptions about compatibility of the input and output dimensions of the layer, plus not sure if network give any meaningful output after such removal. Authors could improve the presentation. For example, they could one MBDL method and describe in details that how their approach is applied to it. (e.g. Table 2 and Figure 4 can be removed or moved to a supplementary material if more space is needed)\n\nPrompted by your comment, the revised manuscript did a better job of explaining the pruning method. To be specific, we (1) detailed the application of SPADE to a simple network, (2) added a discussion of the inter-layer and intra-layer dependencies, and (3) provided a clear, step-by-step depiction of the pruning process. Please refer to the method section in the revised manuscript for more details.\n\n> 2: Also, probable due to limited understanding, I fail to see that how this approach is for MBDL instead of being more general pruning method. Is there something in the pruning approach which required the model to be MBDL instead of, say, a typical classification CNN or ResNet? I can only see that the model A is used in self-supervised fine tuning approach (Eq (10)), but its value seems quite low due to worst performance in the results. Also it could be clarified that what is their contribution or novelty with respect to DepGraph.\n\nWhile pruning is a general idea, this topic has never been explored in the context of model-based deep learning (MBDL). MBDL architectures are inherently memory intensive, where pruning can have a big impact. Our work is the first to investigate the potential of pruning in MBDL  Prompted by feedback, we revised our manuscript to better clarify our contribution. We revised the title of our paper to _\u201cEfficient Model-based Deep Learning via Network Pruning\u201d_ to better highlight our contribution.\n\n> 3: Speed up seems quite low compared to pruning ratio. For example, 10% pruning only seems to give 0-3% speedup and E2EVar (Table1) has only 24% speedup with 65% pruning ratio. Does pruning remove lower-complexity layers, does times include possible compilation times (XLA, jit, etc), or how this can be explained?\n\nThe speed-up is not proportional to the pruning ratio because _MBDL networks consist of non-prunable data-consistency layers_ that enforce consistency between intermediate results and the raw measurements. These data-consistency layers correspond to the data-fidelity term $g(x)$ in the objective function (2). Note also recent studies have proposed to reduce the computational complexity of these data-consistency layers, such as \u201cSGD-Net: Efficient Model-Based Deep Learning With Theoretical Guarantees\u201d. SPADE is fully compatible with these methods."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542017416,
                "cdate": 1700542017416,
                "tmdate": 1700542017416,
                "mdate": 1700542017416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YsYfnTEf4c",
                "forum": "S83ldgJZLh",
                "replyto": "pYjTO2Zixc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Reviewer_hcWa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Reviewer_hcWa"
                ],
                "content": {
                    "comment": {
                        "value": "I am happy with these improvements and increased my rating. I feel that this is bit of a border case due to a bit light contribution (as I understood there is not that big methodological novelty here), but leaning slightly towards acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555604162,
                "cdate": 1700555604162,
                "tmdate": 1700555888669,
                "mdate": 1700555888669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oFAAFH3YBK",
            "forum": "S83ldgJZLh",
            "replyto": "S83ldgJZLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_NVrb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_NVrb"
            ],
            "content": {
                "summary": {
                    "value": "The authors present structured pruning algorithm for model-based deep learning (SPADE)  as a solution to reduce the computational complexity of CNNs used within Model Based Deep Learning (MBDL) framework. SPADE has two components: \n1. Network Pruning: The authors adopt DepGraph to construct the dependency group for each layer in the network and use group `1-norm as the criteria to rank the importance of the filters. The non-essential weights are pruned, resulting in a reduced network structure while preserving the model's accuracy. \n2. Network Fine-Tuning: Authors propose to fine-tune the pruned network to minimize performance gap. They propose three fine-tuning strategies, each with a unique benefit that depends on the presence of a pre-trained model and a high-quality ground truth. \n\nFurther, authors validate SPADE compressed sensing MRI and image super-resolution, with results showing that they can achieve substantial speed up in testing time while maintaining performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Inverse problems are theoretically interesting, and practically very relevant. Reducing inference computational complexity has help bring deep learning based solutions to the main stream for inverse problems, and potentially unlock more applications and widespread usage. \n\n+ Authors use DepGraph (Fang et al., 2023) to perform to pruning, and show with experiments that just the pruned network has significant performance degradations. To combat this, authors propose to fine-tune the pruned network.\n\n+ Authors propose three fine-tuning strategies for the network, which can comprehensive to cover different cases:\n1. Supervised: where ground truth data is used to fine-tune. \n2. School: when labels are not available, pre-trained network is used to score the test data, and in fine-tuning the discrepancy between the pruned network and original network is reduced. \n3. Self-supervised: when pre-trained network and labelled data points are not available, self-supervised objective functions are used to fine-tune the pruned network. \n\n+ Authors also perform extensive numerical experiments and evaluate SPADE on compressed sensing and super resolution, showing competitive performance while improving the computational speed."
                },
                "weaknesses": {
                    "value": "+ While I appreciate the authors effort in improving the computation efficiency of deep learning methods for inverse problems, I wonder if ICLR is the correct venue for this paper. I personally think that the contributions maybe more of relevance to a compressed sensing, and super resolution community, as the pure technical contribution in this paper maybe limited. At the heart, the paper applies existing pruning method, and existing fine-tuning/training methods. \n\n+ While the paper specialize in Model Based Deep Learning, and the introduction focuses on methods like RED, I fail to see how the proposed method SPADE is relevant to such methods, or how any specialized knowledge from this domain is used. It occurs to me that this can be applied to any CNN, not just the ones used within MBDL framework. Can authors confirm? \n\n+ Why was compressed sensing and super resolution selected as the tasks of choice? Showing more inverse problems, and including more networks can help establishing a stronger case for the paper."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695483292,
            "cdate": 1698695483292,
            "tmdate": 1699636667085,
            "mdate": 1699636667085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8t2ZhUS7FK",
                "forum": "S83ldgJZLh",
                "replyto": "oFAAFH3YBK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer NVrb"
                    },
                    "comment": {
                        "value": "Thanks for your feedback on our work. Please find our point-to-point responses below.\n\n> 1: While I appreciate the authors effort in improving the computation efficiency of deep learning methods for inverse problems, I wonder if ICLR is the correct venue for this paper. I personally think that the contributions maybe more of relevance to a compressed sensing, and super resolution community, as the pure technical contribution in this paper maybe limited. At the heart, the paper applies existing pruning method, and existing fine-tuning/training methods.\n\nOur main goal in this paper is to improve the efficiency of MBDL networks via network pruning. This directly relates to the computational efficiency and scalability issues of DL research, aligning with the interests of the ICLR community. \n\n> 2: While the paper specialize in Model Based Deep Learning, and the introduction focuses on methods like RED, I fail to see how the proposed method SPADE is relevant to such methods, or how any specialized knowledge from this domain is used. It occurs to me that this can be applied to any CNN, not just the ones used within MBDL framework. Can authors confirm? \n\nWhile pruning is a general idea, this topic has never been explored in the context of model-based deep learning (MBDL). MBDL architectures are inherently memory intensive, where pruning can have a big impact. Our work is the first to investigate the potential of pruning in MBDL  Prompted by feedback, we revised our manuscript to better clarify our contribution. We revised the title of our paper to _\u201cEfficient Model-based Deep Learning via Network Pruning\u201d_ to better highlight our contribution.\n\n> 3: Why was compressed sensing and super resolution selected as the tasks of choice? Showing more inverse problems, and including more networks can help establishing a stronger case for the paper.\n\nSPADE is applicable to any inverse problems and any MBDL. We considered CS-MRI and image super-resolution because they are two widely studied inverse problems. We agree that it is beneficial to validate SPADE on more inverse problems and MBDL networks. We can do it in our future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541643587,
                "cdate": 1700541643587,
                "tmdate": 1700541643587,
                "mdate": 1700541643587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IsDoIdkns1",
            "forum": "S83ldgJZLh",
            "replyto": "S83ldgJZLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_vSdx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_vSdx"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a pruning algorithm using DepGraph and group L1 norm as well as three fine-tuning methods. The proposed method was evaluated in multiple experiments such as MRI and super resolution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed method showed good performance in speed up for CS-MRI (see Fig. 2) and super resolution (see Table 4)."
                },
                "weaknesses": {
                    "value": "It is questionable that the proposed method is indeed for MBDL only. It could be seen as a mere combination of DepGraph and group sparsity based pruning.\nThree fine-tuning methods do not look novel - they are simply three usual losses that were used for inverse problems.\nExperiments may not be as comprehensive as it should be: it is unclear if the sampling pattern in CS-MRI is fixed for all / super resolution was not evaluated on diverse test datasets, which were frequently used for super resolution literature."
                },
                "questions": {
                    "value": "Q1. it is unclear if the proposed pruning method is novel over other prior pruning works using group sparsity. See the following works:\n- W Wen et al., Learning Structured Sparsity in Deep Neural Networks, NeurIPS 2016\n- J Yoon & S J Hwang, Combined Group and Exclusive Sparsity for Deep Neural Networks, ICML 2017\n- Y Li et al., Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression, CVPR 2020\n- A Kumar et al., Pruning filters with L1-norm and capped L1-norm for CNN compression, Appl Intell 51, 2021\n- K Mitsuno & T Kurita, Filter Pruning using Hierarchical Group Sparse Regularization for Deep Convolutional Neural Networks, ICPR 2021\n- Z Huang et al., Rethinking the Pruning Criteria for Convolutional Neural Network, NeurIPS 2021\nIt is hard not to see this manuscript as a work for network pruning using group sparsity, so properly discussing and comparing the above works seems essential. I can not agree with the claim \"We propose the first network pruning algorithm specifically designed for MBDL models\" - to me, the proposed method can be used for any task considering DepGraph and group sparsity based pruning.\n\nQ2. It is unclear why the proposed three fine-tuning methods are novel : they are simply three popular losses that were used for full networks and there is no special treatment for pruned networks. Please clarify.\n\nQ3. It may be true that \"its potential has remained unexplored in the realm of imaging inverse problems,\" but I am not sure if exploring this potential is important considering that there are a number of drawbacks. For example, it is unclear if this work simulated diverse sampling patterns for CS-MRI, which is much more practical than using a single pattern. If different sampling patterns require a new pruning, then it may not be as convenient as using a full network. Moreover, it is unclear if 51-81% speed up is beneficial by sacrificing the performance by 0.77dB in CS-MRI, which may be critical for missing clinical information. More convincing arguments for the necessity on network pruning for inverse problems along with practical experiments should follow. For super resolution, 0.68dB is a huge gap and most prior works evaluated their methods in more diverse datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840575956,
            "cdate": 1698840575956,
            "tmdate": 1699636666983,
            "mdate": 1699636666983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y4radf1yGZ",
                "forum": "S83ldgJZLh",
                "replyto": "IsDoIdkns1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer vSdx"
                    },
                    "comment": {
                        "value": "Thanks for your feedback on our work. Please find our point-to-point responses below.\n\n> 1: It is questionable that the proposed method is indeed for MBDL only. It could be seen as a mere combination of DepGraph and group sparsity based pruning. / Q1. it is unclear if the proposed pruning method is novel over other prior pruning works using group sparsity. See the following works \u2026. \n\nWhile pruning is a general idea, this topic has never been explored in the context of model-based deep learning (MBDL). MBDL architectures are inherently memory intensive, where pruning can have a big impact. Our work is the first to investigate the potential of pruning in MBDL  Prompted by feedback, we revised our manuscript to better clarify our contribution. We revised the title of our paper to _\u201cEfficient Model-based Deep Learning via Network Pruning\u201d_ to better highlight our contribution.\n\n> 2: Experiments may not be as comprehensive as it should be: it is unclear if the sampling pattern in CS-MRI is fixed for all / super resolution was not evaluated on diverse test datasets, which were frequently used for super resolution literature.\n\nWe used a uniform sampling pattern in CS-MRI. We conducted experiments on image super-resolution on a smaller testing set, simulating situations where only pre-trained models are accessible online and specific applications have limited data. The revised manuscript has been updated to clarify them.\n\n> 3: Three fine-tuning methods do not look novel - they are simply three usual losses that were used for inverse problems. / It is unclear why the proposed three fine-tuning methods are novel : they are simply three popular losses that were used for full networks and there is no special treatment for pruned networks. Please clarify.\n\nWe would like to highlight that our paper proposes _a novel application of structured pruning to MBDL networks_, an application _not explored in any previous studies_. While similar loss functions might have been proposed in other papers, those fine-tuning methods have never been investigated in the context of efficient MBDL networks via network pruning.\n\n| Fine-tuning methods          | Need ground-truth? | Need trained original model?|\n|:--------------------:|:------------:|:------------:|\n| Supervised   | &#9745;          | &#9744;          |\n| School    | &#9744;          | &#9745;          |\n| Self-supervised        | &#9744;          | &#9744;          |\n\n\n> 4: It may be true that \"its potential has remained unexplored in the realm of imaging inverse problems,\" but I am not sure if exploring this potential is important considering that there are a number of drawbacks. For example, it is unclear if this work simulated diverse sampling patterns for CS-MRI, which is much more practical than using a single pattern. If different sampling patterns require a new pruning, then it may not be as convenient as using a full network. Moreover, it is unclear if 51-81% speed up is beneficial by sacrificing the performance by 0.77dB in CS-MRI, which may be critical for missing clinical information. More convincing arguments for the necessity on network pruning for inverse problems along with practical experiments should follow. For super resolution, 0.68dB is a huge gap and most prior works evaluated their methods in more diverse datasets.\n\nWe would like to highlight that it is a trade-off in pruning between performance and network complexity. _One can tune this trade-off based on the specific application_. For example, in applications that are sensitive to performance drop, one can set the pruning ratio in CS-MRI to 20% such that the performance drop is mostly negligible. We will improve this trade-off in our future work. It is also worth mentioning that, in Figures 5 and 6, _the visual difference is unnoticeable_ between the images recovered by the unpruned model and SPADE for CS-MRI and super-resolution, respectively."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541348117,
                "cdate": 1700541348117,
                "tmdate": 1700541434000,
                "mdate": 1700541434000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vdtNxQ5vhn",
            "forum": "S83ldgJZLh",
            "replyto": "S83ldgJZLh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_ifBm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6153/Reviewer_ifBm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a structured pruning method for model-based deep learning in inverse problems. The proposed method, SPADE, reduces the computational complexity of model-based networks at test-time by pruning its non-essential weights. In addition, three different fine-tuning methods are introduced for the pruned networks to reduce performance loss. SPADE is evaluated on compressed sensing MRI and image super-resolution, and is shown to speed up inference with minimal performance degradation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- The application of structured pruning to model-based networks in inverse problems is new and promising, especially due to high computational costs in large-scale imaging settings.\n- The proposed method results in faster inference speed and is applied to multiple frameworks, namely deep equilibrium models (DEQ) and deep unrolling (DU)."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- The contributions of the paper are mostly comprised of a combination of existing techniques such as the pruning algorithm and the fine-tuning techniques.\n- The method is not compared with other methods for improving inference speed, such as [1] or [2] mentioned in the paper. The lack of this comparison makes it difficult to quantify the significance of the results. As an example, there is a 0.77 dB PSNR drop with a 51% speed up at test-time (Table 1) for compressed sensing MRI which seems to be a large performance reduction, and it is unclear how this compares to existing methods.\n\n[1] J. Liu, Y. Sun, W. Gan, X. Xu, B. Wohlberg, and U. S. Kamilov. SGD-Net: Efficient Model-Based\nDeep Learning With Theoretical Guarantees. IEEE Trans. Computational Imag., 7:598\u2013610,\n2021.\n\n[2] J. Tang and M. Davies. A fast stochastic plug-and-play ADMM for imaging inverse problems. arXiv\npreprint arXiv:2006.11630, 2020."
                },
                "questions": {
                    "value": "- How much does the training time increase for SPADE, compared with the baseline unpruned model-based network?\n- Is it possible to combine fine-tuning losses, rather than view them as independent techniques, and could that help preserve performance?\n- How does the memory complexity change at test-time? Memory complexity is also a quite important consideration for which discussion has not been included.\n\nSuggestions:\n\n- The introduction, and the \"DL and MBDL.\" subsection in the background are repetitive. For instance, the equation for PnP/RED does not seem to contribute to the story of the paper. The background can be shortened to include more experiments in the main paper, such as the visual results (Figure 6-8) in the supplemental, which are crucial for compressed sensing MRI. \n- Typographical errors should be fixed via proofreading."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6153/Reviewer_ifBm"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699082659962,
            "cdate": 1699082659962,
            "tmdate": 1699636666878,
            "mdate": 1699636666878,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GQfgLtkIq5",
                "forum": "S83ldgJZLh",
                "replyto": "vdtNxQ5vhn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ifBm (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your feedback on our work. Please find our point-to-point responses below.\n\n> 1: The contributions of the paper are mostly comprised of a combination of existing techniques such as the pruning algorithm and the fine-tuning techniques.\n\nThe topic of pruning has never been explored in the context of model-based deep learning (MBDL). MBDL architectures are inherently memory intensive, where pruning can have an impact. Our work is really the first to investigate the potential of pruning in MBDL  Prompted by your remark, we revised our manuscript to better clarify our contribution. We revised the title of our paper to _\u201cEfficient Model-based Deep Learning via Network Pruning\u201d_ to better highlight our contribution.\n\n> 2: The method is not compared with other methods for improving inference speed, such as [1] or [2] mentioned in the paper. The lack of this comparison makes it difficult to quantify the significance of the results. As an example, there is a 0.77 dB PSNR drop with a 51% speed up at test-time (Table 1) for compressed sensing MRI which seems to be a large performance reduction, and it is unclear how this compares to existing methods.\n\nWe would like to highlight that our approach is _fully compatible_ with the algorithms you mentioned. These methods focus on operators related to the data fidelity term in the objective function, while our approach focuses on the neural network prior. One can easily integrate these algorithms into SPADE to further improve efficiency, but it is out of the scope of our study.\n\n> 3: How much does the training time increase for SPADE, compared with the baseline unpruned model-based network?\n\nPrompted by your comment, we compared the time of training the original VarNet and fine-tuning its pruned variant at a 65% pruning ratio. As shown in the table below, fine-tuning times in SPADE are much shorter than that for training the original model.\n\n**SPADE Fine-tuning Time (h)**\n| Network | Pruning Ratio |      Supervised       |        School        |   Self-supervised    |\n|:-------:|:-------------:|:---------------------:|:--------------------:|:--------------------:|\n| VarNet  |      0%       |     446.26 (100%)     |                      |                      |\n|         |      65%      | 51.48 (11.5%)  | 5.39 (1.21%)   | 10.38 (2.3%)  |\n\n> 4: Is it possible to combine fine-tuning losses, rather than view them as independent techniques, and could that help preserve performance?\n\nThanks for your suggestions. We indeed have _experimented with combined fine-tuning losses before but did not find any improvement_. Our hypothesis is that the hierarchy of supervision quality\u2014supervised, school, and self-supervised fine-tuning\u2014may lead to one loss function dominating the supervision when combined. As an example, we formulated below a combined loss function with the addition of school loss and self-supervised loss.\n\n$l_{combined}(\\hat{\\theta}) = l_{sc}(\\hat{\\theta}) + l_{ss}(\\hat{\\theta})$\n\nWe conducted experiments on the VarNet model with a 65% pruning ratio. The fine-tuning results \nbelow show that using combined loss does not have an advantage over solely employing school fine-tuning losses.\n\n**Quantitative evaluation of three different losses: combined, school, and self-supervised in PSNR**\n| Network | Pruning Ratio | School + Self-supervised | School | Self-supervised |\n|:---------:|:---------------:|:--------------------------:|:--------:|:-----------------:|\n| VarNet  | 0%            | 39.25                    | -      | -               |\n| VarNet  | 65%           | **36.88**                   | 37.36  | 34.17           |\n\n**Quantitative evaluation of three different losses: combined, school, and self-supervised in SSIM(%)**\n| Network | Pruning Ratio | School + Self-supervised | School | Self-supervised |\n|:---------:|:---------------:|:--------------------------:|:--------:|:-----------------:|\n| VarNet  | 0%            | 97.7                     | -      | -               |\n| VarNet  | 65%           | **96.6**                     | 96.7   | 95.3            |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540348558,
                "cdate": 1700540348558,
                "tmdate": 1700540348558,
                "mdate": 1700540348558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8n93vKMma8",
                "forum": "S83ldgJZLh",
                "replyto": "vdtNxQ5vhn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ifBm (2/2)"
                    },
                    "comment": {
                        "value": "> 5: How does the memory complexity change at test-time? Memory complexity is also a quite important consideration for which discussion has not been included.\n\nPrompted by your comment, we evaluated the test-time memory complexity across different pruning ratios for DEQ, VarNet, and E2EVar networks. The results as shown below demonstrate a clear reduction in GPU memory usage as the pruning ratio increases.\n\n| Network | Pruning Ratio | GPU Memory (MB) |\n|:---------:|:---------------:|:-----------------:|\n| DEQ     | 0%            | 3.82 (100.0%)   |\n|         | 10%           | 3.37 (88.22%)   |\n|         | 20%           | 3.04 (79.58%)   |\n|         | 35%           | 2.44 (63.87%)   |\n|         | 65%           | 1.37 (35.86%)   |\n| VarNet  | 0%            | 76.37 (100.0%)  |\n|         | 10%           | 68.06 (89.12%)  |\n|         | 20%           | 61.16 (80.08%)  |\n|         | 35%           | 47.96 (62.80%)  |\n|         | 65%           | 26.77 (35.05%)  |\n| E2EVar  | 0%            | 77.59 (100.0%)  |\n|         | 10%           | 70.27 (90.57%)  |\n|         | 20%           | 63.77 (82.19%)  |\n|         | 35%           | 49.72 (64.08%)  |\n|         | 65%           | 28.83 (37.16%)  |\n\n> 6: The introduction, and the \"DL and MBDL.\" subsection in the background are repetitive. For instance, the equation for PnP/RED does not seem to contribute to the story of the paper. The background can be shortened to include more experiments in the main paper, such as the visual results (Figure 6-8) in the supplemental, which are crucial for compressed sensing MRI. \n\nThanks for your suggestion. The equation of PnP/RED holds significance for introducing key concepts and the updates in iterations of DU and DEQ. Omitting the PnP/RED equation necessitates an alternative equation for the update iterations of DU/DEQ. We attempted to incorporate the MRI figure into the main paper as per your suggestions. However,  the oversized nature of the MRI figures prevents their inclusion.\n\n> 7: Typographical errors should be fixed via proofreading.\n\nThe revised manuscript has done a better job of fixing the typographical errors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540677330,
                "cdate": 1700540677330,
                "tmdate": 1700540677330,
                "mdate": 1700540677330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]