[
    {
        "title": "Learning Graph Representations via Graph Entropy Maximization"
    },
    {
        "review": {
            "id": "VPTDRCjLWd",
            "forum": "UDNJdVjhyg",
            "replyto": "UDNJdVjhyg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_Dct6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_Dct6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a measure of graph information by graph entropy. Since direct optimization of graph entropy is known to be NP hard, this paper provides an approximation of the graph entropy for the practical use of this entropy.\nThis paper also provides experimental results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Formulation of the proposed objective function Eq.(14). This is also supported by some standard Shannon entropy theories (Thm4.1-Cor4.4)"
                },
                "weaknesses": {
                    "value": "1) The proposed method is almost off-the-shelf application of the existing results and methods. Maybe Eq.(9) is a proposal, which is a direct application of Def 3.1 to entropy. However, I speculate the rest of the discussion is off-the-shelf.\n\n\n2) This might be related to 1). It is hard to distinguish which are the authors' results and which are the existing results. For the numbered theoretical claims, as far as I understand only Car 4.6 is the authors result. Then, why the others are not in the preliminaries? All the discussion between Eq. (6) and Eq.(23) is not the authors' own?"
                },
                "questions": {
                    "value": "1) While I appreciate the authors conduct experiments on many methods in Table 1 and Table 2, I am wondering if the authors have any other baseline of an entropy component other than InfoMax?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Reviewer_Dct6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5784/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697980726206,
            "cdate": 1697980726206,
            "tmdate": 1699636608545,
            "mdate": 1699636608545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "62sYrXjhmA",
                "forum": "UDNJdVjhyg",
                "replyto": "VPTDRCjLWd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weaknesses:**\n\n*Off-the-Shelf Application:* We appreciate the reviewer's feedback regarding the perceived off-the-shelf application of existing results and methods in our paper. To address this concern, we have restructured our paper to improve clarity. We have relocated all mathematical content related to other researchers' work to the preliminary section, ensuring a clear separation between our contributions and existing research.\n\n**Our contributions:**\n\n1. Our primary contribution is the introduction of Graph Entropy Maximization (GeMax) for graph representation learning, marking the first instance of its application to the graph learning community. As emphasized in the introduction section, \"graph entropy\" has been used in graph learning, but it typically refers to structural entropy and differs from the genuine Graph entropy as defined by K\u00f6rner (1973). We have highlighted that Graph entropy is a fundamental concept in combinatorics and information theory, with a rich history and contributions from mathematicians such as Claude E. Shannon and J\u00e1nos K\u00f6rner. When researchers create entropy-based methods for graph learning, it is essential to cite and introduce the important works of these mathematicians.\n\n2. In the problem setup section, we present a framework for applying Graph entropy to graph learning problems, clearly defining our contributions in Problem Eq. (10). These contributions include the direct measurement of information within orthonormal representations, supported by Corollary 2.4, and the introduction of a Gaussian distribution with the vertex set, detailed in Eq. (8), to facilitate the application of Graph entropy.\n\n3. In the methodology section, we introduce an approximation method for Graph entropy computation due to its NP-hard nature. This method utilizes Shannon entropy and chromatic entropy, resulting in a max-min optimization problem solved through alternating minimization. Additionally, we provide theoretical error bounds to assess the accuracy of our approximation.\n\nIt is crucial to emphasize that equations without citations represent our original contributions, ensuring a clear distinction between our work and that of other researchers.\n\n**Questions:**\n\n*Baseline of Entropy Component:* We discuss other unsupervised graph representation learning principles in Appendix A.5. For unsupervised and semi-supervised graph-level learning, InfoMax-based methods are the most current and influential methods spanning from 2019 to 2022, each boasting high citations on Google Scholar (see Table 8). Besides the InfoMax principle, there are few works on graph information bottleneck (GIB) [1] and the subgraph information bottleneck (SIB)  [2]. GIB and SIB aim to learn the minimal sufficient representation for downstream tasks. But GIB [1] and SIB [2] may fail if the downstream tasks are not available in the representation learning stage. Thus, they are not suitable for unsupervised and semi-supervised graph learning, and that's why they are not included in our baselines. To the best of our knowledge, we don't find other principles for unsupervised graph-level representation learning except InfoMax, GIB, and SIB. Since the InfoMax methods are the most influential methods, we compare with them by replacing the InfoMax objective with our GeMax objective.\n\n| methods   | InfoGraph | GraphCL | AD-GCL | JOAO | AutoGCL | GIB | SIB |\n|-----------|-----------|---------|--------|------|---------|-----|-----|\n| citations | 665       | 1101    | 176    | 249  | 42      | 129 | 26  |\n\n[1] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. NIPS, 2020\n\n[2]Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Recognizing predictive\nsubstructures with subgraph information bottleneck. PAMI, 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5784/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599413518,
                "cdate": 1700599413518,
                "tmdate": 1700599413518,
                "mdate": 1700599413518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H6FLhNyb2r",
            "forum": "UDNJdVjhyg",
            "replyto": "UDNJdVjhyg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_zWFj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_zWFj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new approach to derive graph-level representation using graph entropy. Since the computation of graph entropy is a NP-hard problem, they provided thorough derivation to approximate the graph entropy through graph neural networks. Specifically, they derived the loss function to be the combination of Shannon entropy and chromatic entropy, on top of which they added orthonormal loss and coloring loss for regularization. Ablation study is done for these components for their impact."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022 Provided a novel way to design graph level representation. The maximization of graph entropy aims to capture the most information from a given graph, focusing on the orthogonal vertices. \n\u2022 Clearly outlined the derivation of the loss function of graph level entropy maximization. They also provided a clear summary of the notations in Appendix (table 5). It would be better if the author can include the shapes of dimensions of the variables. \n\u2022 Good ablation study in terms of the components in the loss function."
                },
                "weaknesses": {
                    "value": "\u2022 Compared to InfoGraph, this paper proposes to use 3 GNNs (graph representation, node representation and coloring representation), for the calculation of the loss function. This could be a problem in terms of model size and running time. The authors provided very limited comments and experiments on this matter. \n\u2022 Despite more parameters introduced in the proposed method, the performance improvement on different datasets is generally small, especially on larger and more complex datasets. \n\u2022 For time analysis in table 14, only the running time for InfoGraph is presented. What about other networks? More importantly, what about GeMax compared with InfoMax? \n\u2022 Figure 2 and Figure 3 & 4 in appendix have many duplicated components. Figure 2 and Figure 3 are especially similar. The author should find a way to condense the information shown.   \n\u2022 The author seems to use the same citation for \u201cInfoMax\u201d and \u201cInfoGraph\u201d. Their method is comparable to \u201cInfoMax\u201d used in \u201cInfoGraph\u201d, but as they mentioned in table 1 and table 2, \u201cInfoMax\u201d can be applied to other architectures as well. The authors should make it clearer."
                },
                "questions": {
                    "value": "\u2022 Using the same model, what are the number of parameters and training time/inference time when using InfoMax and GeMax loss function? \n\u2022 Does the choice of the 3 GNNs matter? Would the model performance improve when using different GNN structures for node, graph and color representation? If so, can the author provide comments on whether the performance improvement is due to the GNN architecture, or the loss function design?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5784/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546005034,
            "cdate": 1698546005034,
            "tmdate": 1699636608452,
            "mdate": 1699636608452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9dCYLgDUrR",
                "forum": "UDNJdVjhyg",
                "replyto": "H6FLhNyb2r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weaknesses:**\n\n*Model Size and Running Time:* The model size of GeMax and InfoGraph is quite similar, as both consist of three GNNs. They share the same node representation GNN and graph representation GNN. While GeMax introduces a coloring representation for chromatic entropy evaluation, InfoGraph uses a discriminator GNN for Jensen-Shannon mutual information estimation. The running time of GeMax is slightly longer than InfoGraph due to the slightly higher complexity of the coloring representation GNN. We have added a running time comparison in Table 14.\n\n*Performance Improvement:* While the performance difference between InfoMax and GeMax may not be substantial, it's important to note that they have very similar model sizes and running times. Achieving significant performance gains in graph-level representation learning appears to be challenging, considering the close similarity between their network architecture. Our primary contribution lies in introducing the concept of Graph entropy to the graph learning community and marking GeMax as the first application of graph entropy in this context.\n\n*Figures Duplication:* We have removed duplicated components from Figure 3 and made necessary adjustments.\n\n*Citations Clarification:* We have clarified the distinction between \"InfoMax\"[1] and \"InfoGraph\"[2] and provided proper citations.\n\n[1] Ralph Linsker. Self-organization in a perceptual network, 1988.\n\n[2] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and\nsemi-supervised graph-level representation learning via mutual information maximization, 2019.\n\n\n\n**Questions:**\n\n*Number of Parameters and Training/Inference Time:* The number of parameters for both InfoMax and GeMax is as follows: graph representation and node representation share the same structure (5 layers with GINs and hidden dimension 128). In InfoMax, the discriminator GNN is a 3-layer DNN with ReLU activations (128x128, 128x128, 128x1). In GeMax, the coloring representation GNN is a 3-layer GIN (n x 128, 128x128, 128xK), where n is the maximum number of nodes in a dataset, and K is the number of colors. GeMax's running time is slightly longer than InfoMax due to the more complex GIN structure in coloring representation.\n\n*GNN Choice and Performance Impact:* GeMax and InfoMax are both unsupervised graph-level representation learning principles, adaptable to various GNN models. Our work doesn't focus on specific GNN architectures. The capacity of GNNs (denoted as $F(\\cdot)$ in our work) should be sufficient to capture graph information. In our experiments, we used default 5 layers. Increasing the number of layers to 6, 7, or 8 did not significantly improve performance. \n\n*Loss Function vs. GNN Architecture:* We have conducted experiments in Appendix A.9 to clarify the significance of orthonormal representations and graph entropy. The orthonormal representations loss enforces GNNs to learn non-adjacent structural information, while graph entropy maximization encourages GNNs to capture more graph information. Additionally, Eq. (11) indicates that GeMax can degenerate to maximizing the Shannon entropy of node-level representations when Corollary 2.9 is satisfied. Chromatic entropy, as defined in Eq. (6), illustrates that maximizing chromatic entropy is equivalent to maximizing information within independent sets associated with graph coloring $\\pi$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5784/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597885450,
                "cdate": 1700597885450,
                "tmdate": 1700598216836,
                "mdate": 1700598216836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZyNSTZD79S",
            "forum": "UDNJdVjhyg",
            "replyto": "UDNJdVjhyg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_Jju5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_Jju5"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies graph representation learning, a domain that seeks to represent graphs as vectors for use in various tasks like graph classification. The authors emphasize the need for diverse representations that can comprehensively capture graph information. They propose using graph entropy to quantify this information, offering a novel approach compared to more conventional methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The theoretical analysis is interesting. \n2. The paper studies an interesting problem."
                },
                "weaknesses": {
                    "value": "1. Authors miss a lot of new graph contrastive learning published in 2022-2023, e.g., [1,2]\n2. Improving GCL using information theory has been studied in [3]. [4] also involves entropy for graph contrastive learning. \n3. Authors should consider large-scale graph classification datasets. \n\n[1] Spectral feature augmentation for graph contrastive learning and beyond, AAAI\n\n[2] Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning, TKDD\n\n[3] Infogcl: Information-aware graph contrastive learning\n\n[4] Entropy Neural Estimation for Graph Contrastive Learning"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5784/Reviewer_Jju5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5784/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819713504,
            "cdate": 1698819713504,
            "tmdate": 1699636608328,
            "mdate": 1699636608328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nqheiA3env",
                "forum": "UDNJdVjhyg",
                "replyto": "ZyNSTZD79S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weaknesses:**\n\nAuthors miss a lot of new graph contrastive learning published in 2022-2023, e.g., [1,2] \nImproving GCL using information theory has been studied in [3]. [4] also involves entropy for graph contrastive learning.\nAuthors should consider large-scale graph classification datasets.\n\n[1] Spectral feature augmentation for graph contrastive learning and beyond, AAAI\n\n[2] Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning, TKDD\n\n[3] Infogcl: Information-aware graph contrastive learning\n\n[4] Entropy Neural Estimation for Graph Contrastive Learning\n\n**Response:**\n\nWe have incorporated several recent graph learning methods published in 2022-2023, including [1, 2, 3, 4], into our study.\n\n- [1] focuses on node-level graph representation, which may not be directly applicable to our graph-level representation learning tasks.\n\n- [2] introduces a Graph Adversarial Contrastive Learning (GraphACL) scheme for self-supervised whole-graph representation learning, but it differs from InfoMax-based methods and therefore cannot serve as our baseline.\n\n- [3] is primarily based on mutual information for graph learning, akin to the InfoGraph method, while our work revolves around learning representations of graphs by maximizing graph entropy, as defined by K\u00f6rner (1973).\n\n- [4] centers on maximizing the Information-entropy Lower Bound (ILBO) of whole graph datasets.\n\nIn our introduction section, we extensively discuss entropy-based methods in graph representation learning, encompassing structural entropy, edge entropy, von Neumann entropy, R\u00e9nyi entropy, and Shannon entropy. However, it's crucial to clarify that while these works employ the term \"graph entropy,\" they do not refer to the authentic Graph entropy defined by J\u00e1nos K\u00f6rner. We underscore that Graph entropy is a fundamental concept in combinatorics and information theory, with a rich history and contributions from renowned mathematicians like Claude E. Shannon and J\u00e1nos K\u00f6rner. Our primary contribution lies in introducing a novel approach called Graph Entropy Maximization (GeMax) to the realm of graph representation learning, marking the first application of its kind in this specific context.\n\nOur graph dataset selection is following the previous works  AutoGCL [5] and JOAO [6]. The GITHUB dataset is actually a large dataset. \n\n[5] Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Automated\ngraph contrastive learning via learnable view generators.  AAAI, 2022.\n\n[6] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-\nmated. ICML, 2021"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5784/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588047526,
                "cdate": 1700588047526,
                "tmdate": 1700588405715,
                "mdate": 1700588405715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "21j9vZu2wy",
            "forum": "UDNJdVjhyg",
            "replyto": "UDNJdVjhyg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_QWua"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5784/Reviewer_QWua"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a framework how to jointly learn graph representation together with node-level representations. \nIn order to do so, they formulate an optimization problem (Eq. 9), as the problem of maximizing graph entropy (Graph entropy K\u00f6rner (1973), \nthat takes prob. distribution and elements of Vertex packing polytope) subject to constraint of using orthonormal representations (Lov\u00e1sz, 1979) and to represent each graph as a vector and represent its vertices as vectors, i.e. (g_j , Z_j ) = F (G_j ).\nTo solve the problem, they use Graph Neural Networks with appropriate parameterization, regularizations and losses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Authors use well grounded mathematical objects to describe graphs: like Graph entropy K\u00f6rner (1973) and orthonormal representations (Lov\u00e1sz, 1979).\nExtensive theoretical and experimental understanding of the subject."
                },
                "weaknesses": {
                    "value": "Presentation needs to be improved. Mathematics of paper is not illuminating, but rather obfuscating the exposition of paper. \nIf you want that your contribution gets more recognition, try to give more insights why certain steps are done."
                },
                "questions": {
                    "value": "1. UNSUPREVISED NODE-LEVEL LEARNING experiments i.e. table 4 does not look convincing enough. \nWhy did you removed InfoGraph-InfoMax, GraphCL-InfoMax, AutoGCL-InfoMax\nThis baselines were present in previous tables.\n\nPerformance for edge prediction tasks is quite close to LGAE.\nDo you have any guess why that is the case?\nHave you tried other node-level classification tasks?\n\n2. However, on Graph-level, your methodology looks superior. Can you elaborate w.r.t. Ablation study, what is the major lesson that a reader can learn about your framework w.r.t. InfoMax.\n\n3. Can you explain main benefits of your approach w.r.t. \nAndo, Rie, and Tong Zhang. \"Learning on graph with Laplacian regularization.\" Advances in neural information processing systems 19 (2006).\n\nCan you make some comparison with\nGuattery, Stephen, and Gary L. Miller. \"Graph embeddings and laplacian eigenvalues.\" SIAM Journal on Matrix Analysis and Applications 21.3 (2000): 703-723."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5784/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839123900,
            "cdate": 1698839123900,
            "tmdate": 1699636608222,
            "mdate": 1699636608222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fTcbVTx26S",
                "forum": "UDNJdVjhyg",
                "replyto": "21j9vZu2wy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5784/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Response to Reviewer's Comments**\n\n**Weaknesses: Presentation needs to be improved. Mathematics of the paper is not illuminating, but rather obfuscating the exposition of the paper. If you want that your contribution gets more recognition, try to give more insights into why certain steps are done.**\n\n*Response:* To improve the presentation, we have made significant changes to the paper structure. We have relocated all mathematical content related to other researchers' work to the preliminary section, ensuring a clear separation between our contributions and existing research.\n\nAs discussed in the introduction section, there have been many entropy-based methods in graph learning. However, while the term \"graph entropy\" has been used in this context, it typically refers to structural entropy and differs from the genuine Graph entropy as defined by K\u00f6rner (1973). We emphasize that Graph entropy is a fundamental concept in combinatorics and information theory, with a rich history and contributions from renowned mathematicians such as Claude E. Shannon and J\u00e1nos K\u00f6rner. Our principal contribution lies in the introduction of a novel approach termed Graph Entropy Maximization (GeMax) for the realm of graph representation learning, marking a pioneering instance of its application in this specific context.\n\nIn the problem setup section, we provide a framework for applying Graph entropy to graph learning problems, as outlined in Problem Eq. (10). Our contributions lie in two key areas: (1) highlighting that Graph entropy can directly measure information within orthonormal representations, as supported by Corollary 2.4, and (2) introducing a Gaussian distribution with the vertex set, as detailed in Eq. (8), to facilitate the application of Graph entropy.\n\nIn the methodology section, given the NP-hard nature of Graph entropy computation, we introduce an approximation method that utilizes Shannon entropy and chromatic entropy, resulting in a max-min optimization problem solved through alternating minimization. We also provide theoretical error bounds to assess the accuracy of the approximation.\n\nIt is essential to note that all equations without citations represent our original contributions, clearly distinguishing them from the work of other researchers. \n\n**Reviewer's Questions and Answers**\n\n*Q1 Answer:* We have taken into account the concern related to the comparison involving InfoMax-based methods in edge prediction tasks. To address this, we have made the decision to exclude InfoGraph-InfoMax, GraphCL-InfoMax, and AutoGCL-InfoMax as baseline methods. This choice stems from the observation that these methods do not prioritize the learning of structural aspects within graphs, potentially leading to an unfair comparison. Specifically, InfoMax-based methods often result in node-level representations that closely resemble the graph-level representations, undermining their suitability for adequately assessing structural aspects within graphs.\n\n*Q2 Answer:* We have delved deeper into the comparative performance of our GeMax method in contrast to the Linear Graph Auto-Encoder (LGAE). LGAE employs VGAE on the 1-hop neighbor adjacency matrix, thereby excelling in capturing local adjacency properties, which in turn results in strong performance in edge prediction tasks. Conversely, GeMax prioritizes the extraction of global information by imposing constraints on non-adjacent relationships, rendering it effective for edge prediction. However, it's important to note that GeMax may not be well-suited for node-level classification tasks, given its focus on representing non-adjacent nodes differently. Node-level classification tasks emphasize the representation of adjacent nodes as similar, rather than emphasizing non-adjacent properties.\n\n*Q3 Answer:* The ablation study conducted in Appendix A.9 has been clarified, emphasizing the significance of orthonormal representations and graph entropy in improving graph representation learning. Maximizing the Shannon entropy is actually maximizing the lower bound of graph entropy. Maximizing chromatic is maximizing information within nodes and independent sets within graphs.\n\n*Q4 Answer:* We have highlighted the fundamental differences between our GeMax method and Laplacian regularization. While Laplacian regularization encourages similar representations for adjacent nodes and different representations for non-adjacent nodes, GeMax focuses on learning representations that capture the most information from a graph by maximizing the graph entropy, with the statistic centroid of the probability distribution of nodes serving as the graph-level representation.\n\nWe sincerely appreciate the thoughtful feedback provided, and we are committed to further enhancing our paper to address these concerns and improve its overall quality."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5784/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253000029,
                "cdate": 1700253000029,
                "tmdate": 1700585775656,
                "mdate": 1700585775656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SkVfG5AOX6",
                "forum": "UDNJdVjhyg",
                "replyto": "fTcbVTx26S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5784/Reviewer_QWua"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5784/Reviewer_QWua"
                ],
                "content": {
                    "title": {
                        "value": "comment."
                    },
                    "comment": {
                        "value": "Thanks for your answers, I do not have any additional questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5784/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586432995,
                "cdate": 1700586432995,
                "tmdate": 1700586432995,
                "mdate": 1700586432995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]