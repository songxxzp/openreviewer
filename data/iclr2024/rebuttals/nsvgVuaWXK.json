[
    {
        "title": "Provably Efficient Learning in Partially Observable Contextual Bandit"
    },
    {
        "review": {
            "id": "Ac33mn7Q2N",
            "forum": "nsvgVuaWXK",
            "replyto": "nsvgVuaWXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission236/Reviewer_eQpB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission236/Reviewer_eQpB"
            ],
            "content": {
                "summary": {
                    "value": "In this research, the authors focus on the difficulties associated with transfer learning in situations involving partially observable contextual bandits (TLPOCB), motivated by real-world scenarios like autonomous driving. Traditional bandit algorithms lack prior knowledge and can be computationally intensive. Although transfer learning techniques, which utilize knowledge from related tasks, are applied, they often encounter the problem of biased learned strategies due to incomplete information transfer from experts to agents. To address these issues, the authors introduce novel causal bounds for TLPOCB tasks and develop algorithms that improve learning efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing is clear and the problem presented is interesting. The proposed algorithms improve on the existing methods in Li and Pearl (2022)."
                },
                "weaknesses": {
                    "value": "The problem setup is spread out into multiple sections and it takes the reader a long time to understand the main topic to be explored. I believe a dedicated section for setup will streamline the reader's understanding.\nSome relevant references are missing. For example, there is a line of work including [1],[2],[3] which study the sequential problem under partial observation, characterized by graphs. I think a comparison is needed.\n\n[1] Nicolo Cesa-Bianchi, G'abor Lugosi, Gilles Stoltz, Regret minimization under partial monitoring, Mathematics of Operations Research, 2006\n\n[2] G\u00e1bor Bart\u00f3k, Csaba Szepesv\u00e1ri, Partial Monitoring with Side Information, ALT2012\n\n[3] Tor Lattimore, Csaba Szepesvari, An Information-Theoretic Approach to Minimax Regret in Partial Monitoring, COLT2019"
                },
                "questions": {
                    "value": "Since this paper is dealing with a subtle topic at the intersection of bandit, transfer learning, and partial monitoring, a thorough comparison with the existing work should be expected, detailing the connection and difference, which seems missing in the current version."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Reviewer_eQpB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698568802226,
            "cdate": 1698568802226,
            "tmdate": 1699635948971,
            "mdate": 1699635948971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aIurZqemXp",
                "forum": "nsvgVuaWXK",
                "replyto": "Ac33mn7Q2N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: The problem setup is spread out into multiple sections and it takes the reader a long time to understand the main topic to be explored.\n\nA: In response to this feedback, we have made improvements to help readers understand the problem setup more efficiently. We have included a notation table in Appendix A, which provides a concise overview of the main notations used in the paper. This addition aims to assist readers in understanding the contributions and key concepts.\n\nFurthermore, we have a preliminaries section that outlines the basic settings related to transfer learning. This section provides foundational information to establish a common understanding before delving into the specifics of the transfer tasks.\n\nTo maintain coherence and clarity, we introduce the main contents of each transfer task separately, rather than presenting them all at once. This approach allows readers to grasp the relevant information progressively and maintain a logical flow throughout the paper.\n\nWe appreciate the feedback and have taken steps to enhance the organization and presentation of the problem setup in order to make it more accessible to readers.\n\n\n\n\nQ: Since this paper is dealing with a subtle topic at the intersection of bandit, transfer learning, and partial monitoring, a thorough comparison with the existing work should be expected, detailing the connection and difference, which seems missing in the current version\n\n\nA:  We acknowledge the importance of comparing our work with existing literature in order to provide a comprehensive understanding of the field. While it is not feasible to compare every related work within the constraints of a 9-page limit, we have made efforts to provide relevant comparisons in terms of our approaches and theoretical results.\n\nIn the paper, we compare our causal bounds with those presented in [1]. We demonstrate the tightness of our bounds by showcasing the limitations of the bounds in [1] and provide a numerical experiment in Section 4 to support our claims.\n\nAdditionally, we compare our proposition 3.1 with the result presented in [2] and explain the practical implications of our findings. We also generalize the result in [3] through theorem 3.1 and compare the order dependence on policy space in our theorem 3.3 with [4], highlighting the intuitive improvement achieved.\n\n\n[1] Bounds on causal effects and application to high dimensional data\n[2] Partial counterfactual identification from observational and experimental data.\n[3] Transfer learning in multi-armed bandit: a causal approach. \n[4] Bounding causal effects on continuous outcome.\n\n\n\nQ: Some relevant references are missing\n\nA: In the new version, we cite your mentioned papers in the related work and compare the difference in terms of settings."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040270988,
                "cdate": 1700040270988,
                "tmdate": 1700040270988,
                "mdate": 1700040270988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nBbGOotruA",
            "forum": "nsvgVuaWXK",
            "replyto": "nsvgVuaWXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission236/Reviewer_whdt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission236/Reviewer_whdt"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies PO contextual bandits. In this setting information is transferred between an expert who already has prior information about the joint distribution in this setting, to solve for a setting with unobserved confounders.\n- The paper uses linear programming to obtain sample complexity bounds on the regret.\n- The paper carries out strong experimental evidence to suggest that the transfer learning algorithm can improve the performance of agents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The analysis is sound, though notations and clarity can be improved.\n- The improvement in regret bound in the given setting is significant. Having said that the advantage to the agent to obtain the improvement in sample complexity is that P(X,Y,W) and separately P(U) is known. The alternative algorithms do not have access to such priors. This expertise from the expert who already knows the joint priors is \"transferred\" in the transfer learning setting.\n- The paper provides a lower bound for their analysis, which is reasonably hard to find.\n- The paper shows a significant improvement over baselines in Figure 2."
                },
                "weaknesses": {
                    "value": "- The transfer learning problem for PO Contextual bandits is not well motivated.\n- The example on Page 3 is supposed to motivate, \"direct approaches can lead to a policy function that is far from the true optimal policy\". But I was not able to see this. How do you get the numbers 1.81? Rather, isn't the example motivating the need for considering U in the policy, if available?\n- \"The expert summarizes its experiences as the joint distribution F\u02c6(a, y, w). However, there exists a latent confounder U that affects the causal effects between actions and rewards, making the observed contextual bandit model incomplete. The agent wants to be more efficient and reuse the observed F\u02c6(a, y, w) along with the extra prior knowledge about U, i.e., F\u02c6(u), to find the optimal policy faster.\" This is the motivation for the paper. \n\n- The claim on sample complexity of the joint distribution F(a, y , w, u) given F(u) as well as F(a, y ,w) separately, is quite different from learning the full joint distribution from scratch. Therefore the comparison with existing sample complexity analysis is slightly misleading. Specifically, \"Our regret demonstrates an improvement in dependence on policy space \u03a0 from P(|\u03a0|) to P(log |\u03a0|) compared with ....\" is not entirely fair.\n\n- x_ijkl is not very informative as a subscript. Please consider revising this terminology?\n\n---\n\n### Experiments\n- The experiments show \"that solving non-linear optimization is quite unstable\".\n- The baselines considered are clear, but the setting is quite unclear. Specifically, is the setting considered general enough, or is it tailored to favor the algorithm proposed in the paper?"
                },
                "questions": {
                    "value": "- The authors claim improving regret from sqrt(|P|) to sqrt(log(|P|)), but Table 1 shows regret proportional to sqrt(|A|) for the algorithms proposed.\n- In page 3 the authors say, \"expert agent can observe the contextual variable in U,W\". But also say in Figure 1 caption that \"U is the unobserved context\". Then why can any expert agent view this?\n- Page 4 para 2, what is \"skewness from F(u)\"?\n- If there is an expert who has already learnt the joint distribution F(a, y, w), and further only finite  \n- In equation 1, can you specify what sets the sup and inf are over?\n\n---\n\n### Minor Suggestions/Typos:\n- Table 1: rows 1,3,5 are the classical algorithms?\n- Consider changing: \"...investigate partially observable contextual bandits, we found few papers focusing on transfer learning in partially observable contextual bandits.\" --> \"...we found few papers focusing on transfer learning in this setting.\"\n- mainly select the arm 1--> mainly selects arm 1.\n- Can the notations in Equation 2 be simplified?\n- \"sequentially solving linear programmings\" --> sequentially solve linear programs\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Reviewer_whdt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855904653,
            "cdate": 1698855904653,
            "tmdate": 1699635948896,
            "mdate": 1699635948896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HgAqV6PbnC",
                "forum": "nsvgVuaWXK",
                "replyto": "nBbGOotruA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: The transfer learning problem for PO Contextual bandits is not well motivated.\n\nA: Transfer learning in partially observable contextual bandit settings is motivated by practical examples such as autonomous driving. In this scenario, human drivers possess experiential knowledge that is not captured by sensors alone. Works such as [1,2] have explored similar settings and investigated the problem.\n\nYour mentioned \"The expert summarizes ... to find the optimal policy faster\" also serves as the motivation for this paper.\n\nQ: The example on Page 3 is supposed to motivate, \"direct approaches can lead to a policy function that is far from the true optimal policy\". But I was not able to see this. How do you get the numbers 1.81? Rather, isn't the example motivating the need for considering U in the policy, if available?\n\n\nA: Thanks for pointing out the typo. We mistakenly added the do notation. \nThe intention of the example was to demonstrate that directly transferring the conditional expectation E[Y|A,W] can be suboptimal in certain cases. Of course, having more information about the models will generally be helpful. It is necessary to consider the knowledge of U if it is available, but the way to use U should be carefully designed.\n\n\nQ: The claim on sample complexity of the joint distribution F(a, y , w, u) given F(u) as well as F(a, y ,w) separately, is quite different from learning the full joint distribution from scratch. Therefore the comparison with existing sample complexity analysis is slightly misleading. \n\nA: The knowledge about the distribution only affects the width of the causal bounds. The key difference between existing methods [1,2] and ours lies in transferred online learning algorithms. As we explain, they treat each basis policy as an independent arm in bandit problems, which is often invalid as similar policies can provide information to each other. For example, the optimal policy and the second optimal policy typically yield similar actions and only differ in a small amount of contexts. This is the intuition behind how we improve this order dependence.\n\nTo address these challenges, we need to modify the classical IGW technique, which requires non-trivial modifications of its proof. We introduce the subspace of explored policies and show how the causal bounds can shrink its size. You can find the full proof in Appendix \n\nQ: $x_{ijkl}$ is not very informative as a subscript. \n\nA: $x_{ijkl}$ can be understood as representing the PMF of the joint distribution $F(a,y,w,u)$ on the space $\\mathcal{A}_i \\times \\mathcal{Y}_j \\times \\mathcal{W}_k \\times \\mathcal{U}_l$.\n\n\nQ: The authors claim improving regret from $\\sqrt {|\\Pi|}$ to $\\sqrt{\\log(|\\Pi|)}$, but Table 1 shows regret proportional to sqrt(|A|) for the algorithms proposed.\n\nA: $\\Pi$ is the policy space and $A$ is the action space. The induced policy space in the function approximation setting is \n$$\n\\Pi = \\{\\pi_f(x): a= \\mathop{argmax}_a f(x,a)  }.\n$$\nSince our order dependence on function space $\\mathcal{F}$ is $\\log |\\mathcal{F}^*|$,\nit is shown in Table 1 that we can improve the order dependence from $\\sqrt{|\\Pi|}$ to $\\sqrt{\\ln |\\Pi|}$, \ncompared with existing methods.\n\n\nQ: Question about numerical setup and baselines\n\nA: The numerical setup is explicitly described in the main text, but we provides its details in Appendix A.6. For the first experiment, the values of distributions are randomly generated to provide a general setting. In the second experiment, some values are specifically chosen to demonstrate a significant improvement. This is because in certain cases where there is limited information about the distribution, the improvement achieved by the proposed algorithm may be negligible.\n\nIt is important to note that our method does not worsen performance with transferred knowledge. The numerical section of the paper demonstrates that naively transferring knowledge can lead to worse performance for classical algorithms. However, our proposed method does not suffer from negative transfer problems.\n\n\nQ: In page 3 the authors say, \"expert agent can observe the contextual variable in U,W\". But also say in Figure 1 caption that \"U is the unobserved context\". Then why can any expert agent view this?\n\nA: U is unobserved to the learner but may be observed by the expert in certain cases. \nActually, how the learner obtain the required information is not the key factor of our paper.\n\nQ: Page 4 para 2, what is \"skewness from F(u)\"?\n\nA: It means P(U=0) differs a lot in P(U=1). \n\n\nQ: In equation 1, can you specify what sets the sup and inf are over?\n\nA: sup/inf is taken with respect to all compatible causal models,\nequivalently, all the joint distributions which satisfy the observations.\n\n\nQ: Minor Suggestions/Typos\n\nA: We follow your suggestions and modify our paper accordingly.\n\n\n[1] Transfer learning in multi-armed bandit: a causal approach \n[2] Leaning without knowing: unobserved context in continuous transfer reinforcement learning"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039112239,
                "cdate": 1700039112239,
                "tmdate": 1700039112239,
                "mdate": 1700039112239,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6YfHLG1TLi",
            "forum": "nsvgVuaWXK",
            "replyto": "nsvgVuaWXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a transfer learning in a causal bandit problem where a bandit agent receives observational data from an expert agent. In particular, some of the covariates used in the agent is unavailable but partial information is available. Overall, this paper combines the (improved) causal bounds by Li and Pearl and the idea of transfer learning by Zhang and Bareinboim. There are several considerations in improving causal bounds (tigher bounds) and obtaining them pratically (sequential linear programming, estimation error, sampling then optimization). Further, bandit algorithm also has improvement through considering dependency among policies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is more than the combination of transfer learning algorithm (ZB) and causal bounds (LP). \n- incorporation of estimation error (in Appendix, which should be in the main paper. It can be only two or three sentences). To make use of small number of available data, naively using maximum likelihood estimates can be misleading. The use of estimation error into bounds is simple yet an important step (especially in transfer learning setting where some of the arms might be truncated) BTW, epsilon in the Algorithm 1 should be highlighted.\n- (Eq 3) tigther bounds than Li and Pearl, which is shown to not satisfy constraints over the available information\n- practical sampling approach (Eq 4) which avoids rejection sampling.\n- The sample, then optimization approach."
                },
                "weaknesses": {
                    "value": "No specific weakness other than the organization, which will be mentioned in the questions (and suggestions) section."
                },
                "questions": {
                    "value": "- Given that ICLR will allow an additional one page, it is expected that the authors will incorporate necessary, important information currently trimmed or in Appendix. \n- The paper seems abruptly trimmed or cut during submission. There is no conclusion or discussion. For example, Table 1 should be after Theorem 3.4. Without providing context, it is too abrupt. Further, it seems that table next Table 1 does not have a proper caption. Increase arraystretch to represent table better.\n- The organization near Eq 5 and 6 are awkward. \n- BTW, lines 1,3,4,6 could be one-liner and Fig 1 can be a lot smaller to afford more space\u2026 \n- There are many pointers to Appendix which somewhat distracts the flow.\n- I suggest Causal Bounds as a separate section and use subsection/paragraph properly. Causal bounds can be described irrelevant to the transfer learning problem. Current paragraphs \u201cCausal Bounds\u201d and \u201cSampling valid causal models\u201d as a whole can be in the section and can be restructured. \n- In Page 8 near at the end, I don\u2019t get the argument here about instrumental variables. If they rely on IV, you may argue that your method is free of IV requirement. It is a bit unnecessary to mention that finding IV is an open problem in academia (Economics?).  \n- adjust the two plots in Figure 2\n\nBy the way, this paper would be more like for AISTATS, CLeaR (causal conference), NeurIPS, ICML ... I wonder why the authors pick the ICLR as the 'best' revenue to present the results. I don't see any part 'representation learning' involved. Given the type of the audience, I rate this paper 'marginally above' the threshold. Otherwise, I will raise to 'accept'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904476887,
            "cdate": 1698904476887,
            "tmdate": 1699635948742,
            "mdate": 1699635948742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8btQzYK7AV",
                "forum": "nsvgVuaWXK",
                "replyto": "6YfHLG1TLi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: Suggestions on structures.\n\n\nA: Thank you for your feedback. Following your suggestions, we utilize the additional one page allowed by ICLR to incorporate the necessary information, \nincluding a conclusion subsection and brief descriptions about appendix structure. \nThe following content is added in the additional one page.\n\n\"\nIn this paper, we investigate transfer learning in partially observable contextual bandits by converting the problem to identifying or partially identifying causal effects between actions and rewards. \nWe derive causal bounds with the existence of partially observable confounders using our proposed Monte-Carlo algorithms. \nWe formally prove and empirically demonstrate that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates.\n\nThere are several future research directions we wish to explore. \nFirstly, we aim to investigate whether the solutions to discretization optimization converge to those of the original problem.\nWhile the approximation error can be exactly zero when all referred random variables are discrete, \nit is still unclear which conditions for general random variables and discretization methods can lead to convergence. \nWe conjecture that this property may be related to the sensitivity of the non-linear optimization problem.\n\nLastly, we aim to extend our IGW-based algorithm to continuous action settings. \nIGW has been successfully applied to continuous action settings and has shown practical advantages in large action spaces. \nThis extension may be related to complexity measures in machine learning.\n\"\n\n\n\"\nIn appendix A, we put the omitted information regarding the examples, algorithms, theorems, implementation details and numerical setup. \nWe also put partial related work section due to the strict page limitation of ICLR. \nIn appendix B, you can find all proofs about claimed theorems.\nIn appendix B, you can find some related materials about causal tools. \nIn appendix D, we generalize our sampling method on general simplex more than the special one defined by the special optimization (1). \nIn appendix E, we put a conclusion section and discuss the future work. \n\"\n\n\nFor other suggestions, we modify our paper accordingly according to the page limit and modification requirement.\n\n\n\n\n\nQ: There are many pointers to Appendix which somewhat distracts the flow.\n\n\nA:  Thank you for your feedback. We understand your concern regarding the pointers to the appendix and their potential impact on the flow of the paper. We have included these pointers to ensure that important details and results are not omitted due to the strict page limitation. By providing references to the relevant sections in the appendix, we aim to maintain the logical flow of the paper while still providing comprehensive information. We will make sure that the pointers are clear and effectively guide readers to the corresponding sections in the appendix.\n\n\n\n\n\n\nQ: In Page 8 near at the end, I don't get the argument here about instrumental variables. If they rely on IV, you may argue that your method is free of IV requirement. \nIt is a bit unnecessary to mention that finding IV is an open problem in academia (Economics?).\n\n\nA: We modify this sentence in the updated version. \nWe actually want to show that our method does not rely on IVs as you mentioned in your reviews.\n\n\n\nQ: this paper would be more like for AISTATS, CLeaR (causal conference), NeurIPS, ICML ... I wonder why the authors pick the ICLR as the 'best' revenue to present the results. I don't see any part 'representation learning' involved. \n\nA: We appreciate your perspective on the suitability of the paper for different conferences. While ICLR is commonly associated with representation learning, it also welcomes submissions from various topics, including reinforcement learning, causal learning, and transfer learning. Our paper falls within the scope of ICLR as it addresses transfer learning in the context of causal reinforcement learning. We acknowledge that representation learning may not be explicitly mentioned in our paper, but the broader topics covered in ICLR encompass the themes and techniques we explore. Moreover, ICLR has accepted papers in the areas of reinforcement learning, causal learning, and transfer learning in the past."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036267961,
                "cdate": 1700036267961,
                "tmdate": 1700036267961,
                "mdate": 1700036267961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ByLlv61XAz",
                "forum": "nsvgVuaWXK",
                "replyto": "8btQzYK7AV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thank you for providing detailed responses and incorporating my inquiries effectively. I have carefully read the reviews you provided. However, there are a few aspects that haven\u2019t been addressed. I would like to ask about those.\nFirstly, I suggested having a separate section for causal bounds and an overall restructuring, but it seems it hasn\u2019t been addressed. I\u2019m curious about the authors\u2019 opinions on this matter. Additionally, I have some additional questions about the organization of equations (Eq5 and 6) and the position of Table 1. Lastly, in Fig.2, the sizes of the x-axis annotations do not match each other. I kindly request adjustments to be made in this regard."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547038378,
                "cdate": 1700547038378,
                "tmdate": 1700547038378,
                "mdate": 1700547038378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PNHp0V9ydM",
                "forum": "nsvgVuaWXK",
                "replyto": "2pxNGVgkwi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Reviewer_DQyy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I was initially thinking about separating a causal bound section or renaming some of the sections but now it seems clear that there is no need to do it. (The phrase 'overall restructuring' was in fact splitting the section. Sorry for the confusion.)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612074072,
                "cdate": 1700612074072,
                "tmdate": 1700612074072,
                "mdate": 1700612074072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OEi5sOi6fJ",
            "forum": "nsvgVuaWXK",
            "replyto": "nsvgVuaWXK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission236/Reviewer_yo3f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission236/Reviewer_yo3f"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies partially observable contextual bandits, where the context comprises 2 variables W, U and the agent have access to W only while U is hidden. The problem is mapped to the framework of causal effects between actions and rewards and formulated as an optimization problem that is solved using sampling and LP techniques. The proposed algorithms are shown to have orders of magnitude better regret than classical bandit algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The considered problem is important and has multiple applications.\n- The approach is novel.\n- The resulting regrets are smaller than those of existing bandit algorithms."
                },
                "weaknesses": {
                    "value": "- The writing of the paper can be substantially improved. Many of the definitions and arguments are not clear to me from a theoretical aspect. Please see my questions below regarding this.\n\n- The regret definition is written in terms of W only (not U). At each time slot, do you compete with a policy that has access to the true function $f^*$ and both realizations of W, U or you compete against policy that has access to $f^*$, W and take expectation over U?\n\n- What is h(a) in table (1)? \n\n- What is sup/inf in equation (1)? Is it either inf or sup, both will work? Do you mean that one will give an upper bound and the other a lower bound? This should be clearly stated. In the same paragraph you mention that (1) gives a bound on the causal effects. Causal effects between which variables? What is the mathematical formula for the causal effect to see that (1) gives an upper bound?\n\n- If the algorithms are applied in the famous case of contextual linear bandits, what would be the resulting regret in that case?\n\n- The works in [1,2,3] consider contextual linear bandits with known context distribution without observing the realization. Even though the setup is more limited, I believe the authors need to provide a comparison when the results of the paper are limited to the setups of [1,2,3].\n\nThe paper has a novel idea, but it is hard to follow the math and verify the results. I suggest the authors make the paper more clear and rigorous. Please explain the results with more math and logic. A table of notations would also help. I will update my score after reading the authors response.\n\n[1] Kirschner, Johannes, and Andreas Krause. \"Stochastic bandits with context distributions.\" Advances in Neural Information Processing Systems 32 (2019).\n[2] Hanna, Osama, Lin Yang, and Christina Fragouli. \"Learning from Distributed Users in Contextual Linear Bandits Without Sharing the Context.\" Advances in Neural Information Processing Systems 35 (2022): 11049-11062.\n[3] Hanna, Osama A., Lin Yang, and Christina Fragouli. \"Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023."
                },
                "questions": {
                    "value": "Please see. weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission236/Reviewer_yo3f"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699563608038,
            "cdate": 1699563608038,
            "tmdate": 1699635948668,
            "mdate": 1699635948668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t67sak1E9D",
                "forum": "nsvgVuaWXK",
                "replyto": "OEi5sOi6fJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: The regret definition is written in terms of W only (not U). At each time slot, do you compete with a policy that has access to the true function and both realizations of W, U, or do you compete against a policy that has access to $f^*$, W, and take the expectation over U?\n\nA: Since the learner cannot observe $U$, we compete against a policy that has access to $f^*$, W, and takes the expectation over U. The realizability assumption only needs to hold in the sense of taking expectation with respect to $U$.\n\n\nQ: What is h(a) in Table (1)?\n\nA: h(a) is the causal upper bound solved by the optimization problem (1). Please refer to the MAB part in section 3.1 for more details.\n\n\nQ: What is sup/inf in equation (1)? Is it either inf or sup, and both will work? Do you mean that one will give an upper bound and the other a lower bound? This should be clearly stated. In the same paragraph you mention that (1) gives a bound on the causal effects. Causal effects between which variables? What is the mathematical formula for the causal effect to see that (1) gives an upper bound?\n\n\nA: Equation (1) means we can get upper and lower bounds of its object by solving corresponding maximization and minimization problems, respectively.\nThe object is the causal effect between action $A$ and reward $Y$ and both its upper and lower bounds are needed in the transfer learning algorithms.\nTheorem A.1 in Appendix shows the mathematical expression of the object\nand in the last sentence on page 4, we show the expression of the object in the discrete setting.\n\n\nQ: If the algorithms are applied in the famous case of contextual linear bandits, what would be the resulting regret in that case?\n\n\nA: We have provided the implementation of infinite function spaces in section A.5. It is worth noting that our algorithm and theorems can be easily extended to handle infinite $\\mathcal{F}$ using standard learning-theoretic tools such as metric entropy. Let's assume $\\mathcal{F}$ is equipped with a maximum norm $||\\cdot||$. We can consider an $\\epsilon$-covering $\\mathcal{F}^*_{\\epsilon}$ of $\\mathcal{F}^*$ under the maximum norm. Since $|\\mathcal{F}^*_{\\epsilon}|$ is finite, we can directly replace $\\mathcal{F}^*$ with $\\mathcal{F}^*_{\\epsilon}$ without changing any algorithmic procedures. Thanks to the property of $\\epsilon$-covering, there exists a function $f_{\\epsilon}^*\\in \\mathcal{F}^{\\epsilon}$ such that $|| f_{\\epsilon}^* - f^* || \\leq \\epsilon$. Hence, the regret can be bounded by the expression:\n\n$$\nReg(T) \\leq 8 \\sqrt{ \\mathbb{E}_W[ \\mathcal{A}(W) ] T \\log (2\\delta^{-1}  |\\mathcal{F}^*\\_{\\epsilon}|   \\log T ) } + \\epsilon T.\n$$\n\nBy replacing the dependence on $\\log |\\mathcal{F}^*|$ in the algorithm's parameters with $\\log|\\mathcal{F}^*_{\\epsilon}|$ and setting $\\epsilon=\\frac{1}{T}$, we obtain a similar result, up to an additive constant of 1.\n\nIn the context of linear spaces, the quantity $N(\\mathcal{F}^*,||\\cdot||,\\epsilon)$, which represents the covering number, scales with $\\sqrt{\\log N(\\mathcal{F}^,||\\cdot|| ,\\epsilon)}$. For linear spaces, such a quantity is of order $O(d\\log(\\frac{1}{\\epsilon}))$.\nHence, the improvement of regret order will be $N(\\mathcal{F},||\\cdot||,\\epsilon) - N(\\mathcal{F}^*,||\\cdot||,\\epsilon)$ in terms of function spaces.\n\nIt is important to note that $N(\\mathcal{F}^*,||\\cdot||,\\epsilon) \\leq N(\\mathcal{F},||\\cdot||,\\epsilon)$ since $\\mathcal{F}^* \\subset \\mathcal{F}$. The covering number clearly demonstrates how extra causal bounds help improve the algorithm's performance by shrinking the search space. The transfer learning algorithm only needs to search within a spherical shell with a thickness of at most $M-m$, where $m = \\inf_{w,a} l(w,a)$ and $M=\\sup_{w,a} h(w,a)$. The bounds chip away the surface of the unit sphere and scoop out the concentric sphere of radius $m$.\n\n\n\nQ: The works in [1,2,3] consider contextual linear bandits with known context distribution without observing the realization. Even though the setup is more limited, I believe the authors need to provide a comparison when the results of the paper are limited to the setups of [1,2,3].\n\nA: Thank you for your suggestion. We have added the mentioned papers to the related work section. Please refer to the updated version of our paper for more details."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034002759,
                "cdate": 1700034002759,
                "tmdate": 1700034002759,
                "mdate": 1700034002759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]