[
    {
        "title": "Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks"
    },
    {
        "review": {
            "id": "Ud3ZWViBXx",
            "forum": "vQqJJzL2Jf",
            "replyto": "vQqJJzL2Jf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_dasc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_dasc"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the extrapolation of PINNs based on the Fourier spectrum shifts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author analyzes the extrapolation performance of PINNs based on the Weighted Wasserstein-Fourier distance (WWF) in different time domains during training and testing."
                },
                "weaknesses": {
                    "value": "The extrapolation problem of PINN is not a significant problem, as we can always finetune the PINN if long-time prediction is needed, or we can train a new PINN on the new domain.\n\nIn the context of domain adaptation and domain generalization in computer vision, the conclusion of this paper does not seem to be novel. The distance between Fourier components during train & test domains is just like the concept of distribution shift in computer vision, where people use the KL divergence and other metrics to quantify the representation distribution between train and test to predict the out-of-domain  /out-of-distribution generalization performance.\n\nFrom this viewpoint, we can also reinterpret the authors' conclusion: We find that failure to extrapolate is not caused by high frequencies in the solution function, but rather by shifts in the support of the Fourier spectrum over time.\n\nThe so-called \"shifts in the support of the Fourier spectrum over time\" is just the distribution shift in computer vision.\n\nAnd one can actually derive a rigorous mathematical bound for the out-of-domain PINN error."
                },
                "questions": {
                    "value": "Please justify the importance of PINN's extrapolation: why don't we just train a new model/finetune?\nPlease explain your novelty over the concept of domain adaptation and domain generalization in computer vision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698403629750,
            "cdate": 1698403629750,
            "tmdate": 1699636550417,
            "mdate": 1699636550417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7m2W3aoxFc",
                "forum": "vQqJJzL2Jf",
                "replyto": "Ud3ZWViBXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for their very interesting suggestions.\n\n> Please justify the importance of PINN's extrapolation: why don't we just train a new model/finetune?\n\nAs remarked on by an increasing number of papers [1, 2, 3], models that can extrapolate well are generally desirable, for example because extrapolation failures indicate that the model hasn\u2019t correctly learned the underlying physical laws [1], which are the reason for using PINNs in the first place. Simply training a new model (on a larger domain) does not address this issue as the new model will still suffer from poor extrapolation outside of its training domain.\n\n> In the context of domain adaptation and domain generalization in computer vision, the conclusion of this paper does not seem to be novel. The distance between Fourier components during train & test domains is just like the concept of distribution shift in computer vision, where people use the KL divergence and other metrics to quantify the representation distribution between train and test to predict the out-of-domain /out-of-distribution generalization performance. From this viewpoint, we can also reinterpret the authors' conclusion: We find that failure to extrapolate is not caused by high frequencies in the solution function, but rather by shifts in the support of the Fourier spectrum over time. The so-called \"shifts in the support of the Fourier spectrum over time\" is just the distribution shift in computer vision. And one can actually derive a rigorous mathematical bound for the out-of-domain PINN error.\n\nUnfortunately, deriving generalization bounds for PINNs has been found to be significantly harder than to simply adapt concepts from the domain adaptation and domain generalization literature [4]. In particular, minimizing the PDE residual in PINNs does not straightforwardly control the generalization error [2], which makes the PINN case different from, for example, computer vision.\n\nIn addition, the theoretical results derived so far for PINN performance outside of the training data are limited to generalization errors, where we are interested in the error committed on points sampled from within the convex hull of the training data. There is, to the best of our knowledge, no theoretical work on the extrapolation performance of PINNs, where the data of interest is far away from the training domain.\n\n> Please explain your novelty over the concept of domain adaptation and domain generalization in computer vision.\n\nPlease see our comment above. We agree with the reviewer that it would be desirable to have results for PINNs that are similar to the domain adaptation literature in, for example, computer vision. We are aware of at least one work that uses tools from domain adaptation to at least empirically improve the extrapolation performance of PINNs [1]. As mentioned in our discussion section, we think that a theoretical investigation of the extrapolation performance of PINNs would be a logical next step, and we believe that our experimental results point into a potentially promising direction, i.e. spectral shifts.\n\n[1] Jungeun Kim, Kookjin Lee, Dongeun Lee, Sheo Yon Jin, and Noseong Park. Dpm: A novel training method for physics-informed neural networks in extrapolation, 2020.\n\n[2] Taniya Kapoor, Abhishek Chandra, Daniel M Tartakovsky, Hongrui Wang, Alfredo Nunez, and Rolf Dollevoet. Neural oscillators for generalization of physics-informed machine learning. arXiv preprint arXiv:2308.08989, 2023.\n\n[3] Andrea Bonfanti, Roberto Santana, Marco Ellero, and Babak Gholami. On the hyperparameters influencing a pinn\u2019s generalization beyond the training domain, 2023.\n\n[4] Siddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse problems for pdes. IMA Journal of Numerical Analysis, 42(2):981\u20131022, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151831949,
                "cdate": 1700151831949,
                "tmdate": 1700151865100,
                "mdate": 1700151865100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MlqzRl8Fft",
            "forum": "vQqJJzL2Jf",
            "replyto": "vQqJJzL2Jf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_SeWR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_SeWR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces extrapolation failures of PINNs, which studies the out-of-domain behavior of PINNs. The paper then analyzes the extrapolation failures in the scope of Weighted Wasserstein-Fourier Distance and spectral bias, showing the PDEs with blockwise WWFs tend to have extrapolation failures, and when extrapolation happens, the prediction spectral shifts from the true solution.\n\nThe paper then proposes a transfer learning strategy that effectively mitigates extrapolation failures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces extrapolation failures of PINNs, which studies the out-of-domain behavior of PINNs and seems to be a novel and promising research topic.\n2. The paper leverages a novel scope, Weighted Wasserstein-Fourier Distance, to analyze spectral shifts for extrapolation failures. The analysis shows that (1) PDEs with blockwise WWFs tend to have extrapolation failures and (2) when extrapolation happens, the prediction spectral shifts from the true solution.\n3. The paper proposes a transfer learning strategy that mitigates extrapolation failures."
                },
                "weaknesses": {
                    "value": "1. The theory of this paper is not solidly developed enough. The conclusion of the paper, says spectral shift, is drawn from observations of several specific types of PDEs, can be empiricism, and may not be generalizable enough for other PDEs. In addition, justifying whether a PDE will suffer from an extrapolation failure using WWF can be difficult in practice. To examine whether the WWF is blockwise or not, it requires true $f_s$ for $s\\in I$ and $f_t$ for $t\\in E$, while $f_s$ and $f_t$ are not available for most practical cases. \n\n2. Leveraging the concept of spectral bias for extrapolation as a hypothesis is problematic. The spectral bias states that NN tends to learn low-frequency components more easily and faster than high-dimensional components during training [1]. While extrapolation is a validation/testing process. Thus, one should not explain a phenomenon in testing with a theory for training, or use it as a hypothesis (despite that the paper beats the hypothesis to the end). The hypothesis reference paper [2] does not make any statement on extrapolation with spectral bias either, they only assert PINNs fail to train when the training time window becomes large, possibly due to spectral bias.\n\n3. The presentation of the paper can be improved, say most figures can be denser to save space, and most tables can have nicer borders, etc.\n\n[1]. Rahaman, Nasim, et al. \"On the spectral bias of neural networks.\" International Conference on Machine Learning. PMLR, 2019.\n\n[2]. Wang, Sifan, and Paris Perdikaris. \"Long-time integration of parametric evolution equations with physics-informed deeponets.\" Journal of Computational Physics 475 (2023): 111855."
                },
                "questions": {
                    "value": "1. For the proposed transfer learning method, what is the benefit of transfer learning rather than directly learning new PINNs over the full domain?  As shown by massive previous works, PINNs can easily learn accurate solutions for Burger's equation, as showcased in this work.\n\n2. Could the author explain why (or what does it mean) for a constant WWF distance of the true solutions for diffusion/reaction-diffusion equation in Figures 15 & 16?\n\n3. Could the author explain why diffusion and reaction-diffusion (Figures 15 & 16) show similar WWF distance differences between true solutions and predictions as Burger's and Allen-Cahn (Figures 13 & 14), but the first two do not show significant extrapolation failure, while the latter two show significant extrapolation failures (Figure 7)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698456403094,
            "cdate": 1698456403094,
            "tmdate": 1699636550323,
            "mdate": 1699636550323,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b5rhjdoM7w",
                "forum": "vQqJJzL2Jf",
                "replyto": "MlqzRl8Fft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for their very useful comments.\n\n> The theory of this paper is not solidly developed enough. The conclusion of the paper, says spectral shift, is drawn from observations of several specific types of PDEs, can be empiricism, and may not be generalizable enough for other PDEs. In addition, justifying whether a PDE will suffer from an extrapolation failure using WWF can be difficult in practice.\n\nIn this paper, we focused primarily on putting forth evidence supporting the novel hypothesis that spectral shifts could be responsible for the poor extrapolation behavior of PINNs. We certainly welcome a more thorough theoretical investigation of this phenomenon, but we chose to leave that to future work. \n\nWe would also like to point out that our focus here is not on practical applications just yet, but on acquiring a basic understanding of what can make PINNs fail to extrapolate. Future work should extend our analysis to state-of-the-art PINN models and, as the reviewer pointed out, investigate if the WWF can be used in practical applications.\n\n> Leveraging the concept of spectral bias for extrapolation as a hypothesis is problematic. The spectral bias states that NN tends to learn low-frequency components more easily and faster than high-dimensional components during training [1]. While extrapolation is a validation/testing process. Thus, one should not explain a phenomenon in testing with a theory for training, or use it as a hypothesis (despite that the paper beats the hypothesis to the end).\n\nWe thank the reviewer for voicing their concern. The issue with the citation pointed out by the reviewer has been fixed. By spectral bias, we indeed mean PINNs tendency to learn lower frequencies better and faster in interpolation. However, prior work has also studied the connections between NTK theory, spectral bias, and extrapolation, for example [1]. Following this, we would expect most of the extrapolation error to come from the higher frequencies (the predicted function might become smooth or flat, as in [2]). Our Fourier-based results in section 3 show that this is not the case: a large share of the error seems to come from the lower frequencies. \n\n> The presentation of the paper can be improved, say most figures can be denser to save space, and most tables can have nicer borders, etc.\n\nWe have generally made the representation a bit denser and have also moved some additional figures from the appendix to the main text.\n\n> For the proposed transfer learning method, what is the benefit of transfer learning rather than directly learning new PINNs over the full domain? As shown by massive previous works, PINNs can easily learn accurate solutions for Burger's equation, as showcased in this work.\n\nWe agree that it is generally possible to train a new model on a larger domain (assuming that domain isn\u2019t too large) to get accurate solutions. This is, however, based on the fact that PINNs can perform well in interpolation.\n\nAs recent works have argued [3], models that can extrapolate well are generally desirable, for example because extrapolation failures indicate that the model hasn\u2019t correctly learned the underlying physical laws [4], which are the reason for using PINNs in the first place.\nSimply training a new model (on a larger domain) does not address this issue as the new model will still suffer from poor extrapolation outside of its training domain.\n\nTransfer learning is interesting in this regard in that it seems to encourage inductive biases in the model that improve extrapolation performance (see our discussion in section 4) without training on a larger domain with the target PDE.\n\n[1] Xu, Keyulu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. \"How neural networks extrapolate: From feedforward to graph neural networks.\" arXiv preprint arXiv:2009.11848 (2020).\n\n[2] Andrea Bonfanti, Roberto Santana, Marco Ellero, and Babak Gholami. On the hyperparameters influencing a pinn\u2019s generalization beyond the training domain, 2023.\n\n[3] Taniya Kapoor, Abhishek Chandra, Daniel M Tartakovsky, Hongrui Wang, Alfredo Nunez, and Rolf Dollevoet. Neural oscillators for generalization of physics-informed machine learning. arXiv preprint arXiv:2308.08989, 2023.\n\n[4] Jungeun Kim, Kookjin Lee, Dongeun Lee, Sheo Yon Jin, and Noseong Park. Dpm: A novel training method for physics-informed neural networks in extrapolation, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173161043,
                "cdate": 1700173161043,
                "tmdate": 1700432999050,
                "mdate": 1700432999050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ElYqBy7mx8",
                "forum": "vQqJJzL2Jf",
                "replyto": "MlqzRl8Fft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Could the author explain why (or what does it mean) for a constant WWF distance of the true solutions for diffusion/reaction-diffusion equation in Figures 15 & 16?\n\nFor numerical stability reasons, the WWF distance was clipped at $10^{-3}$ \u2013 the true WWF distance is in fact zero for both the diffusion and diffusion-reaction equations.\n\nThis is because the support of the Fourier spectrum of the diffusion and diffusion-reaction equation does not change over time, only the amplitudes decrease. These changes in amplitude are not captured by the WWF, since we normalize the Fourier spectra before computing the Wasserstein Distance.\n\nApologies for any confusion this may have caused \u2013 we have clarified this in our revision.\n\n> Could the author explain why diffusion and reaction-diffusion (Figures 15 & 16) show similar WWF distance differences between true solutions and predictions as Burger's and Allen-Cahn (Figures 13 & 14), but the first two do not show significant extrapolation failure, while the latter two show significant extrapolation failures (Figure 7)?\n\nNote that we do not claim that the WWF between true solutions and predictions is indicative of the size of the extrapolation error. We only claim this (and experimentally show it) for the WWF between the true solution in the interpolation region and in the extrapolation region.\n\nWe believe that what the reviewer has observed here relates back to the previous comment, i.e. that the WWF is insensitive to differences in amplitude. The WWF (and WF more generally) only measures the change in the underlying distribution of different frequencies, neglecting any shift in overall amplitude. For example, multiplying the spectrum by a constant would lead to large L2 error, but the WWF distance would still be zero.\n\nAs can be seen in section A.3 of the appendix, both Burger\u2019s and Allen-Cahn exhibit significantly larger magnitude differences between predictions and true solutions in the Fourier spectra than both the diffusion and diffusion-reaction questions, only some of which is captured in the WWF distance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173176713,
                "cdate": 1700173176713,
                "tmdate": 1700173176713,
                "mdate": 1700173176713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h22Si1U4kJ",
                "forum": "vQqJJzL2Jf",
                "replyto": "ElYqBy7mx8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_SeWR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_SeWR"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comment"
                    },
                    "comment": {
                        "value": "I thank the author for the explanation of my questions.\n\nA more general question here is whether the PDEs with extrapolation failures can be generalized simply based on the PDEs' types, say elliptic, parabolic, or hyperbolic.\n\nAlso, would the author re-emphasize the three most important contributions of this work, instead of listing 5 contributions in the original paper?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511016120,
                "cdate": 1700511016120,
                "tmdate": 1700511016120,
                "mdate": 1700511016120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bsgY2kT1TP",
                "forum": "vQqJJzL2Jf",
                "replyto": "MlqzRl8Fft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for going through our rebuttal.\n\n> Also, would the author re-emphasize the three most important contributions of this work, instead of listing 5 contributions in the original paper?\n\nWe believe that the three most important contributions in our work are 1) PINNs can indeed extrapolate well for certain PDEs, even when high frequencies are present in the solution function, 2) PINNs\u2019 extrapolation performance is linked to spectral shifts in the solution, and 3) transfer learning is beneficial for extrapolation performance when spectral shifts are present.\n\nWe would be happy to adjust the section on novel contributions if the reviewer considers this shorter version to be more accessible.\n\n> A more general question here is whether the PDEs with extrapolation failures can be generalized simply based on the PDEs' types, say elliptic, parabolic, or hyperbolic.\n\nUnfortunately, based on our experiments, there doesn't seem to be an obvious connection between PDE type and PINN extrapolation performance: extrapolation works well on the heat equation (parabolic), but produces large errors on the Allen-Cahn equation (also parabolic, but non-linear). We believe that this is an interesting question, however, that might become important when deriving rigorous bounds on the extrapolation error."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515815678,
                "cdate": 1700515815678,
                "tmdate": 1700515858133,
                "mdate": 1700515858133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KkBtNLP5gg",
            "forum": "vQqJJzL2Jf",
            "replyto": "vQqJJzL2Jf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_qKGX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_qKGX"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript looks at extrapolation capabilities of the PINN solutions for multipl. The authors introduce a concept of spectral shifts that can be used to predict the PINN extrapolation performance. \n\nThe spectral shifts are used to analyse what features are not affecting the extrapolation performance using a set of seven different PDEs. Their results indicate that the number of layers, number of neurons , activation function, number of samples or training do not contribute.\n\nTo remedy the extrapolation challenge, the authors find that  transfer learning using similar PDEs, decreases significantly the extrapolation errors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A significant analysis on the extrapolation capability of the  PINN in its basic structure.\nIntroduction of the weighted Wasserstein-Fourier distance as a measure.\nExcellent and clear presentation of the results."
                },
                "weaknesses": {
                    "value": "One may assume that extrapolation will be successful when the training data contains the types of behaviour that will occur in the extrapolated portion of the time domain. This would explain how the PINN solutions of PDEs without or with small spectral shift are viable for extrapolation. If the characteristics of the PDE solutions change drastically after some time scale i.e. have a large spectral shift.\n\nI am missing an analysis where the a particular differential equation have short time behaviour which is spectrally \"stable\" and long term behaviour, where the spectral shift emerges.  Hence, the difference in the PDE behaviour may also come from how long a certain trajectory is followed, not intrinsically from what kind of equation it is. Then, a user of the method may feel safe to extrapolate a given PDF that has seemed to be \"safe\" but enter in the dangerous domain without warning.\n\nIt seems that safe way would be to use an alternative method to check the accuracy of the solution always, which makes the extrapolation capability less useful."
                },
                "questions": {
                    "value": "Please, consider the possible issue I raised in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589246749,
            "cdate": 1698589246749,
            "tmdate": 1699636550223,
            "mdate": 1699636550223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rcQapGfhOU",
                "forum": "vQqJJzL2Jf",
                "replyto": "KkBtNLP5gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed read of our submission and for the very encouraging feedback.\n\nWe agree that experiments with a PDE that has a stable spectrum in the short term and a spectral shift in the long term would be very interesting. We are, unfortunately, not aware of an obvious PDE choice that exhibits this particular behavior, but would be very grateful for any suggestions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151124743,
                "cdate": 1700151124743,
                "tmdate": 1700151124743,
                "mdate": 1700151124743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pjFPGe9V9y",
                "forum": "vQqJJzL2Jf",
                "replyto": "KkBtNLP5gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_qKGX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_qKGX"
                ],
                "content": {
                    "comment": {
                        "value": "For example,  if a massive object travels across the solar system it can affect the orbits of the planets in a secular way.  From data of the planets before the crossing one cannot see that this would happen, until the object is already close. It is very hard  to extrapolate this based on data available before the encounter.  Also, check https://arxiv.org/pdf/1510.00591.pdf, this system does not even need a \"rogue\" hidden planet out there.\n\nAlso, for example in materials sciences fatigue caused cracking  is also hard to extrapolate (without prior experiments covering the cracking timescale of the materials)  and leads to frequency shifts in the behaviour after a long period of contained frequency response.\n\nWhat I mean to say is that extrapolation outside the experimental data domain is always risky."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548761516,
                "cdate": 1700548761516,
                "tmdate": 1700646269623,
                "mdate": 1700646269623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fiFPYgdaz1",
                "forum": "vQqJJzL2Jf",
                "replyto": "KkBtNLP5gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We\u2019d like to thank the reviewer for bringing our attention to these examples, and we agree that there may indeed be cases where spectral shifts emerge outside of the training domain and as a result may not be easily detectable.\n\nThe focus of this paper is not necessarily on practical usage of the WWF metric; instead, we focused on better understanding precisely what features of a PDE made it easy/hard for PINNs to extrapolate. There are certainly cases where spectral shifts may be present in the original training domain and may signal potential extrapolation failures. \n\nHowever, we agree that there may be instances where simply examining the WWF/spectral dynamics in the training domain may fail to detect potentially poor extrapolation behavior due to later spectral shifts and where extrapolation is inherently uncertain/difficult \u2014 for example, in the cases the reviewer mentioned above.\n\nWe believe these are valuable questions, and we thank the reviewer for their feedback and comments. We would certainly welcome a more thorough investigation of how one could better predict these spectral shifts from a limited domain in future work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716101798,
                "cdate": 1700716101798,
                "tmdate": 1700716215000,
                "mdate": 1700716215000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDhThFUdol",
            "forum": "vQqJJzL2Jf",
            "replyto": "vQqJJzL2Jf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_uE7H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5420/Reviewer_uE7H"
            ],
            "content": {
                "summary": {
                    "value": "The paper debunks the idea that the poor extrapolation performance of PINNs are due to the presence of high frequency components. The paper demonstrates the poor extrapolation performance is due to a shift in the support of the Fourier spectrum, which they refer to as the spectral shift. The paper introduces a metric to quantify this shift using Weighted Wasserstein Fourier distance (WWF). The paper finally shows that a transfer-learning based technique which trains a multi-headed PINN can be effective in providing better extrapolation performances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-written and easy to follow.\n2. Analyzing the poor extrapolation performance using spectral shifts is novel and interesting.\n3. The proposed WWF metric can be a good evaluation tool to visualize the spectral shifts."
                },
                "weaknesses": {
                    "value": "1. Although the paper provides some empirical evidence of correlations of spectral shifts and poor extrapolation performance, the paper lacks a theoretical understanding of why the hypothesized Spectral Shift is the root cause of extrapolation error. \n2. The motivation behind the proposed transfer learning approach is not clear. I would highly appreciate it if the authors can provide the connections between the observed spectral shifts and how transfer learning can mitigate it.\n3. The empirical results for improved extrapolation performance are not convincing. The L2 Extrapolation Error on the Schrodinger Equation (imag) for the proposed method is still quite poor (290%), which is still quite unusable from a practical stand-point."
                },
                "questions": {
                    "value": "**Comments/Questions:**\n1. I understand the space constraints but figures 7a and b are quite important to demonstrate the results shown in Section 3.1.\n2. The placement of Figure 1 can be improved. It is referenced in Page 6 while the Figure is present in page 1. Changing the location of the figure to a more appropriate location can improve the readability of the paper.\n3. In my opinion, transfer Learning on the full domain is an unfair comparison, as the PDEs with a similar set of coefficients were already trained on the entire domain. \n\n**Minor comments:** \nThe authors use a different citation format than the standard ICLR format. Using the original citation format would result in overflow of the text outside the page limit since they are considerably longer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5420/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5420/Reviewer_uE7H",
                        "ICLR.cc/2024/Conference/Submission5420/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5420/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809928457,
            "cdate": 1698809928457,
            "tmdate": 1700522342263,
            "mdate": 1700522342263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ayth2zjdI",
                "forum": "vQqJJzL2Jf",
                "replyto": "sDhThFUdol",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their encouraging feedback and useful comments.\n\n> Although the paper provides some empirical evidence of correlations of spectral shifts and poor extrapolation performance, the paper lacks a theoretical understanding of why the hypothesized Spectral Shift is the root cause of extrapolation error.\n\nWe agree that a theoretical analysis of the role of spectral shifts in the extrapolation performance of PINNs is a natural next step, and we included this as a promising future direction in our discussion section.\n\nIn this paper, we focused primarily on putting forth evidence supporting the novel hypothesis that spectral shifts could be responsible for the poor extrapolation behavior of PINNs. We certainly welcome a more thorough theoretical investigation of this phenomenon, but we chose to leave that to future work.\n\n> The motivation behind the proposed transfer learning approach is not clear. I would highly appreciate it if the authors can provide the connections between the observed spectral shifts and how transfer learning can mitigate it.\n\nTo summarize our motivations, we believe that by transfer learning from other PDEs that exhibit shifts in the spectra of their true solutions, the model may be able to recognize the features of PDEs that exhibit these shifting spectra and modify its predictions in the extrapolation domain accordingly. If the model has already seen the Burgers\u2019 equation with a different viscosity parameter, for example, then this may enforce stronger inductive biases within the model that allow it to understand the evolution of the spectra for the Burger\u2019s equation of interest.\n\nFor example, our transfer learning experiments used Burgers' equations with similar viscosities ($\\nu$) to the target PDE \u2013 and thus similar spectral shift, as shown in Figure 4. Transfer learning on additional PDEs, with viscosities that are further from that of the target PDE, seems to make a minimal impact. This suggests that the model may be able to map the target PDE into its shared feature space, transferring its knowledge of the PDEs that share the most similar spectral evolution to make its predictions for the target PDE.\n\nOur motivations for the proposed transfer learning approach can be found in full detail in section 4.1 (\"Why does transfer learning help?\").\n\n> The empirical results for improved extrapolation performance are not convincing. The L2 Extrapolation Error on the Schrodinger Equation (imag) for the proposed method is still quite poor (290%), which is still quite unusable from a practical stand-point.\n\nWe agree with the reviewer that the relative L2 error in extrapolation is still too high for practical usage, even after transfer learning. The goal of this paper was not necessarily to produce results fit for practical usage but rather to put forth the idea that transfer learning may help mitigate the poor extrapolation performance induced by spectral shifts. All our analysis is conducted with standard PINNs, which naturally produce worse results than current state-of-the-art methods. We hope that applying our insights to more complex architectures/models should yield similar benefits, while proving to be more practically useful."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153771433,
                "cdate": 1700153771433,
                "tmdate": 1700153771433,
                "mdate": 1700153771433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJbWpPQvoV",
                "forum": "vQqJJzL2Jf",
                "replyto": "sDhThFUdol",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> I understand the space constraints but figures 7a and b are quite important to demonstrate the results shown in Section 3.1.\n\nWe agree with the reviewer that Figures 7a and 7b are helpful to contextualize the results presented in section 3.1. We have added these figures to the main text in our revision.\n\n> The placement of Figure 1 can be improved. It is referenced in Page 6 while the Figure is present in page 1. Changing the location of the figure to a more appropriate location can improve the readability of the paper.\n\nWe had placed Figure 1 on page 1 to highlight our main findings, but we agree that it may be better placed later in the paper. We have modified the placement of Figure 1 (now Figure 4) accordingly in our revision, as suggested by the reviewer.\n\n> In my opinion, transfer Learning on the full domain is an unfair comparison, as the PDEs with a similar set of coefficients were already trained on the entire domain.\n\nNote that due to this shared concern, we do include results for transfer learning on the same temporal domain as the baseline for comparison; these are the Transfer (half) results in Tables 1 and 2 respectively. In both cases, we observe significant decreases in the relative L2 error of the predictions compared to baseline, though these decreases are indeed smaller than those we see when we train on the full domain.\n\nThere are also instances in which our results for transfer learning from the full domain are applicable. For example, one might want to train a model on several PDEs with a vast array of different spatiotemporal domains, some of which may overlap with future target PDEs. This would of course come at some computational cost, but as seen in our transfer learning results, it could lead to improved extrapolation performance.\n\n> The authors use a different citation format than the standard ICLR format. Using the original citation format would result in overflow of the text outside the page limit since they are considerably longer.\n\nWe apologize for the oversight and thank the reviewer for pointing this out \u2013 we have since modified our paper to match the standard citation format."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153817586,
                "cdate": 1700153817586,
                "tmdate": 1700153817586,
                "mdate": 1700153817586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l4tAfl2kiA",
                "forum": "vQqJJzL2Jf",
                "replyto": "eJbWpPQvoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_uE7H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5420/Reviewer_uE7H"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for providing clarifications to my questions/comments. I have read the other reviews and the authors' comments, and I have increased my score accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5420/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522368990,
                "cdate": 1700522368990,
                "tmdate": 1700522368990,
                "mdate": 1700522368990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]