[
    {
        "title": "Creative Robot Tool Use with Large Language Models"
    },
    {
        "review": {
            "id": "ArpPFIeZnX",
            "forum": "IKOAJG6mru",
            "replyto": "IKOAJG6mru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the interesting question of enabling robots to use tools, taking into account constraints from both the robot and its environment. The authors propose the RoboTool system, which augments the coder module with additional analyzer, planner, and calculator modules. This paper presents a benchmark encompassing three tool-usage categories: tool selection, sequential tool usage, and tool manufacturing, evaluated across two types of robots. Through carefully designed experiments, the authors show that their system exhibits innovative tool use."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Leveraging LLMs to delve into robot tool usage is a compelling approach. The inherent common sense knowledge within LLMs may offer invaluable insights to the robot's tool utilization process.\n* The RoboTool System builds on the prior concept of LLM code generation (framed as code-as-policies) and merges it with new analyzer, calculator, and planner modules. This integration aids in breaking down the task, enabling the LLM to more effectively suggest beneficial solutions. The ablation studies provide evidence of the effectiveness of these newly introduced modules.\n* The categorization in the benchmark is well-designed as a starting point to explore the robot tool usage with LLMs."
                },
                "weaknesses": {
                    "value": "* Although this paper focus on high-level planning using tools, the provided descriptions are too detailed on the targetted tools, which makes it hard to see if the hints make LLM propose the solutions. For example, for the Milk-Reaching example, the hammer is provided with detailed instructions on how to grasp, the descriptions on its layout, while other objects are not described that detailedly. Such bias can make the results unfair. And in all 6 experiments, the number of objects in the descriptions are limited, it may be not hard for the LLM to pick a related object.\n* The benchmark only contains 6 demos, with limited diversity on the layout of the objects. For example, for the milk-reaching demo, the hammer is always in the correct direction. With similar description, actually the hammer can be in multiple potential directions, which will definitely influence the planning the success rate of the task. Such challenging examples are not considered in the constructed benchmarks. And with the natural language description, it cannot avoid the limitation to describe the 3D world. Without access to the full information, it\u2019s hard to imagine the performance of the system on complicated tasks. Need to show more results on the robustness for the system on various examples.\n* The descriptions are sometimes confusing. For example, in the Cube-Lifting, the cube weight is 10kg, and the robot weight is also 10kg, then in the video, why the robot will fall down so quickly when it goes to another side. It\u2019s a bit confusing if the description reflects the real property and why not use the real physical attributes.\n* For some constraints mentioned in the description, it\u2019s unclear why the constraints make sense. For example, in the Cube-Lifting example, in the constraints, \u201cyou can push the chair only in the x-direction\u201d, it makes readers confusing if the input description is well-tuned for the specific example and how such things make the system generalizable across different tasks."
                },
                "questions": {
                    "value": "* Regarding the benchmark, are there additional results showcasing varying object layouts for each demonstration while maintaining a consistent description format?\n* In the demos, how does the system respond when constraints in the input description are removed? How sensitive is it to changes in the description?\n* Why not for all objects, give the same set of attributes no matter if the attributes are useful enough? In this way, it can better show if the system is able to really extract the useful information from the descriptions without hints.\n* Although this work try demonstrating the tool usage ability in the high-level setting, it\u2019s hard to ignore the influence from different details. How to make sure the description describe the scene without heavy human designing?\n* How to make sure the given grasping point and other attributes or additional description for one object are not hints to the tool usage demo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698282604493,
            "cdate": 1698282604493,
            "tmdate": 1699637128300,
            "mdate": 1699637128300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B85Hi4vgYk",
                "forum": "IKOAJG6mru",
                "replyto": "ArpPFIeZnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xccj (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and insightful reviews! We appreciate that you found leveraging LLMs for creative tool use is a compelling approach, our method is effective, and the benchmark is well-designed. Here are our responses to your concerns:\n\n> Q1: Limited task diversity.\n\n We want to highlight that designing six creative tool-use tasks and their variants to show the discriminative tool-use behaviors contributes to the robotics community, as discussed in [1]. Unlike many robotic manipulation tasks inspired by household activities, creative tool use is more like solving intricate physical puzzles with many design thoughts injected into the process. RoboTool is the first step towards designing a standard robot tool use benchmark. We hope our research could inspire more work on this critical yet underexplored topic.\n\n**Ref:**\n\n[1] Qin, Meiying, Jake Brawer, and Brian Scassellati. 2022. \u201cRobot Tool Use: A Survey.\u201d Frontiers in Robotics and AI 9: 1009488.\n\n\n> Q1.1: Are the object layouts changed?\n\nThanks for the question. The positions are indeed randomized. For robot arm experiments, we randomize the initial configuration of objects. We randomize the initial position of movable objects and the robots for quadrupedal robot experiments. We show the randomization ranges in Table 1 and add it to Appendix G in the updated draft.\n\nTable 1: Randomization of objects in each task.\n| | Objects: Randomization range [x_min, x_max, y_min, y_max] |\n| ------------- | ------- |\n| Milk-Reaching | pineapple toy, lock, cube, tomato toy: [0, 0.45, -0.25, 0.25]; hammer: [0, 0.45, -0.25, 0]; milk: [0.45, 0.6, 0, 0.25]  |\n| Can-Grasping | stick: [0, 0.45, -0.25, 0]; paper: [0, 0.45, 0, 0.25]; can: [0.45, 0.7, -0.1, 0.1]  |\n| Button-Pressing | magnetic_cube1, magnetic_cube2, magnetic_cube3: [0, 0.1, -0.25, 0.25]; button: [0.45, 0.7, -0.25, 0.25]  |\n| Sofa-Traversing | surfboard: [0.3, 0.6, -0.175, 0.0]; cloth: [0.3, 0.6, 0.0, 0.2]; robot: [-0.5, -0.2, -0.2, 0.2];   |\n| Sofa-Climbing | box_2: [0.0, 2.0, 0.0, 1.0]; robot: [0.0, 3.0, 0.0, 2.0];   |\n| Cube-Lifting | chair: [0.8, 1.2, -0.1, 0.1]; surfboard: [0.8, 1.2, 0.3, 0.5]; robot: [0.0, 3.0, 0.0, 2.0];   |\n\n> Q1.2: In milk-reaching experiment, the number of objects in the descriptions is limited. It may not be hard for the LLM to pick a related object.\n  - We increase the number of objects that may serve as tools to five, including a hammer, a pineapple toy, a lock, a tomato toy, and a cube. We also provide the same amount of information for each object, including (1) graspable points for non-rectangular objects and the position of rectangular objects and (2) bounding box sizes of each object. Considering the non-rectangular shape of the hammer and pineapple toy, we added a description related to the orientation. However, the orientation description does not affect the tool selection in experiments. After rerunning experiments, we noticed that RoboTool can still correctly select the hammer as the tool among ten runs.  \n - Besides selecting the correct tool, how and when to use the tool are also important. For example, the Can-Grasping task has three objects, five skills (2 have a discrete action parameter, 1 has continuous action parameters), and 15 planning steps. The underlying complexity of planning is significant, and any errors in the intermediate steps could lead to a catastrophic failure. Despite the difficulties, RoboTool achieved a high success rate across different domains, indicating it is not just randomly picking an object to interact with."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193982125,
                "cdate": 1700193982125,
                "tmdate": 1700193982125,
                "mdate": 1700193982125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kMNLCS6s9Q",
                "forum": "IKOAJG6mru",
                "replyto": "ArpPFIeZnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xccj (Part 3)"
                    },
                    "comment": {
                        "value": "> Q3: You should give the same set of attributes to all objects in the tasks:\n - Thanks for the suggestion. We have cleaned up all the task descriptions. For robot arm experiments, the task description contains (1) graspable points for non-rectangular objects and the position of rectangular objects and (2) bounding box sizes of each object. For quadrupedal robot experiments, the task description contains each object's center position and size. Since tasks in quadrupedal robot experiments are loco-manipulation tasks, we also include the weight of each object in the scene.\n     - **Imbalance description in Milk-Reaching; Inaccurate cube weight of Cube-Lifting.** Thanks for the insightful question. We have cleaned up the object description and changed the cube weight to the actual value. \n - After rerunning the experiments, we noticed that RoboTool has a slight performance drop (0.87 -> 0.83), but it is insignificant.\n\n\n\n\n> Q4 & Q5: How to make sure the description describes the scene without heavy human design? Does the description hint at the final plan?\n - We use human-designed templates due to the limitation of existing Vision Language Models. As mentioned in the response to Q1.3, the SOTA VLM models are insufficient in 3D reasoning, and it is hard to get a reasonable estimate of object positions and sizes directly without the help of existing APIs. Moreover, our tasks require information about the constraints and weights, which are hard to get from VLMs. Hence, to explore the complex creative tool-use tasks with existing foundation models, we have no choice but to provide a template description.\n - Based on the reviewer's suggestion in Q4, we have cleaned up the description and made sure that different objects in the scene are provided with similar amounts of information, such as positions, sizes, and weights. Please note that the provided information is sufficient but may be unnecessary for finishing the tasks. In other words, we have provided distractors, such as other objects in tool-selection tasks, which require LLMs to filter the information and reason based on the most important information. \n - Even with the same amount of information, the state-of-the-art baselines still have low success rates, as shown in Table 2. RoboTool performs better than baselines by a large margin, highlighting the task complexity of creative tool-use problems.\n\nTable 2: Success rates of RoboTool and additional baselines.\n|  Task | RoboTool (ours) | CaPs | ViperGPT |\n| --- | -------- | -------------- | -------- |\n|  Milk-Reaching  | 0.9 | 0.0 | 0.0 |\n|  Can-Grasping  | 0.7 | 0.2 | 0.0 |\n|  Button-Pressing  | 0.8 | 0.6 | 0.6 |\n|  Sofa-Traversing  | 1.0 | 0.4 | 0.3 |\n|  Sofa-Climbing  | 1.0 | 0.4 | 0.2 |\n|  Cube-LIfting  | 0.6 | 0.4 | 0.0 |\n|  Average  | 0.83 | 0.33 | 0.18 |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194062276,
                "cdate": 1700194062276,
                "tmdate": 1700194097648,
                "mdate": 1700194097648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U9S4OgEqu0",
                "forum": "IKOAJG6mru",
                "replyto": "ArpPFIeZnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback and suggestions! As the discussion period is ending, we would appreciate you kindly checking our response. Please do not hesitate to contact us if there are other clarifications we can offer. Thanks!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496949771,
                "cdate": 1700496949771,
                "tmdate": 1700496949771,
                "mdate": 1700496949771,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urbhtqnVNI",
                "forum": "IKOAJG6mru",
                "replyto": "ArpPFIeZnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup on the rebuttal as the discussion period is ending soon"
                    },
                    "comment": {
                        "value": "Dear reviewer xccj,\n\nWith the rebuttal period ending tomorrow, we would like to ask whether our response addressed your questions and alleviated your concerns. If so, could you please kindly consider raising the score?\n\nBest,\n\nAuthors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621032391,
                "cdate": 1700621032391,
                "tmdate": 1700621032391,
                "mdate": 1700621032391,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fVP2mzdFJD",
            "forum": "IKOAJG6mru",
            "replyto": "IKOAJG6mru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents RoboTool, a method for enabling tool use in robots using large language models (LLMs). Besides this prompt-based task and motion planning framework, the paper also proposes a benchmark of 6 tool use tasks evaluating tool selection, sequential tool use, and tool manufacturing capabilities. Tasks involve a robotic arm and a quadrupedal robot. Experiments in simulation and the real world demonstrate that RoboTool can successfully accomplish the tool use tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Leveraging the recent wealth of LLM research for improving robotics is a highly desirable research direction that is well-explored in this paper."
                },
                "weaknesses": {
                    "value": "1. Experiments are weak: in particular, the authors propose a new benchmark, but only evaluate their method on it. To ascertain the value of the benchmark suite, additional baselines need to be included. To assess the strength of contributions of this \"learning-free\" approach, it should be run on existing, standardized benchmarks, such as those included in [3] or [6].\n\n2. Lack of novelty: works such as [1], [2], [3], [4] and [5] have taken similar approaches to neuro-symbolic learning and robotic manipulation, via LLM-generated programs or TAMP structures. Moreover, it's not particularly satisfying to me that the entire method interacts only with GPT-4 at the API level. The paper in effect becomes a \"prompt engineering\" work, which, while interesting, does not meet the bar for original technical contribution at ICLR.\n\n[1] [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753)\n\n[2] [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128)\n\n[3] [Programmatically Grounded, Compositionally Generalizable Robotic Manipulation](https://arxiv.org/abs/2304.13826)\n\n[4] [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559)\n\n[5] [Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model](https://arxiv.org/abs/2305.11176)\n\n[6] [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)"
                },
                "questions": {
                    "value": "1. The ablations provided are interesting and welcome, but could the authors include some more well-established baselines such as [1] or [2] in this evaluation?\n\n2. Can the authors address why an API-only algorithm is sufficiently novel? In particular, I don't see where there is any learning of representations, which nominally is what ICLR is focused on.\n\n[1] [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753)\n\n[2] [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg",
                        "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712197182,
            "cdate": 1698712197182,
            "tmdate": 1700770028021,
            "mdate": 1700770028021,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U1x5Vuwl1y",
                "forum": "IKOAJG6mru",
                "replyto": "fVP2mzdFJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer b6vg (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback. We appreciate that you found leveraging LLMs for creative tool use is a highly desirable research direction and well-explored in this paper. Here is our response regarding concerns about novelty, baselines, and benchmarks.\n\n> Novelty of the paper.\n\nWe respectfully disagree with the reviewer about the novelty of RoboTool. Here is a summary of our contributions:\n - **Novel Problem:** Creative tool use is an important yet under-explored problem in robotics. It will greatly expand the capability of robots to solve tasks that are impossible originally. \n - **Novel Benchmark:** We designed a novel challenging benchmark to test specifically the creative tool use behavior, which contains two robot embodiments and three categories of creative tool use. Such a benchmark is highly needed and non-trivial to design [1].\n - **Novel Method:** Although the planner-coder LLM agent has been explored in existing literature, blindly using existing API-based methods such as Code as Policy fails to address the challenges posed by creative tool use in terms of cognition, reasoning, and planning. As shown below, the suggested baselines failed to complete any of the tasks in our benchmark with a high success rate.\n\n**Ref:**\n\n[1] Qin, Meiying, Jake Brawer, and Brian Scassellati. 2022. \u201cRobot Tool Use: A Survey.\u201d Frontiers in Robotics and AI 9: 1009488.\n\n> ICLR is focused on representation learning, while RoboTool is a training-free method.\n\n - Despite the name, ICLR is about more than just learning representation, as documented by the non-comprehensive topic list on the [official website](https://iclr.cc/Conferences/2024/CallForPapers). This year in ICLR, there are at least 90 submissions related to training-free LLM Agents. When looking at the accepted papers of this year NeurIPS, one can find at least 20 impactful training-free API-based LLM agents, such as HuggingGPT [2], Reflexion [3], DEPS [4], AVIS [5], CAAFE [6], and SheetCopilot [7]. With increasingly capable LLMs, it is critical to explore what they can or cannot do, which is underexplored especially in the physical interaction and 3D reasoning domains. RoboTool is one of the first steps towards reasoning implicit environment- and embodiment-constraints and long-horizon hybrid discrete-continuous planning. Therefore, RoboTool is well-suited to the top AI publication venues.\n\n**Ref:**\n\n[2] Shen, Yongliang, et al. \"Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\" NeurIPS 2023.\n\n[3] Shinn, Noah, Beck Labash, and Ashwin Gopinath. \"Reflexion: an autonomous agent with dynamic memory and self-reflection.\" NeurIPS 2023.\n\n[4] Wang, Zihao, et al. \"Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.\" NeurIPS 2023.\n\n[5] Hu, Ziniu, et al. \"AVIS: Autonomous Visual Information Seeking with Large Language Models.\" NeurIPS 2023.\n\n[6] Hollmann, Noah, Samuel M\u00fcller, and Frank Hutter. \"GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.\" NeurIPS 2023.\n\n[7] Li, Hongxin, et al. \"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models.\" arXiv preprint arXiv:2305.19308 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193563538,
                "cdate": 1700193563538,
                "tmdate": 1700193705058,
                "mdate": 1700193705058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pZCopquxMA",
                "forum": "IKOAJG6mru",
                "replyto": "fVP2mzdFJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Thank you for the feedback! We appreciate you kindly checking our response as the discussion period ends soon. Specifically, we have added new baselines and showed that our proposed RoboTool outperforms the suggested baselines by a large margin. If you have further questions, please do not hesitate to let us know, and we are happy to answer them. Thanks!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496899743,
                "cdate": 1700496899743,
                "tmdate": 1700496899743,
                "mdate": 1700496899743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hn4usumhu1",
                "forum": "IKOAJG6mru",
                "replyto": "fVP2mzdFJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Followup on the rebuttal as the discussion period is ending soon"
                    },
                    "comment": {
                        "value": "Dear reviewer b6vg,\n\nWith the rebuttal period ending tomorrow, we would like to ask whether our response addressed your questions and alleviated your concerns. If so, could you please kindly consider raising the score?\n\nBest,\n\nAuthors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620920343,
                "cdate": 1700620920343,
                "tmdate": 1700620920343,
                "mdate": 1700620920343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W9zeCk3cfE",
            "forum": "IKOAJG6mru",
            "replyto": "IKOAJG6mru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
            ],
            "content": {
                "summary": {
                    "value": "This works uses LLMs to generate code that is able to perform some reasoning and planning with a robotic simulated system. It is tested on three different experimental paradigms with two robots. The results provided are impressive. The only drawback of the work is the confusion on what is really planning and control with a tool in the real world and coding a set of skills in a programming environment. Furthermore, the baseline comparison is an ablation study."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tOriginal solution for planning with reasoning using LLMs.\n-\tIt is able to generate code with a level of reasoning that outperforms previous works.\n-\tResults are well described and deep."
                },
                "weaknesses": {
                    "value": "-\tWhile the aim proposed by the authors is \u201cwe aim to solve a hybrid discrete-continuous planning problem\u201d, this is not solved in this work or at least not described properly.\n-\tThe focus of the paper should be improved. This is a of  language reasoner that generates code. So it is more a programming tool than a RoboTool.\n-\tBaseline comparison is an ablation study. Thus, the third contribution is not well described.\n\n\n**Focus**\n\nThe clarity of what is achieved should be more clear. The first contribution: \u201clong-horizon hybrid discrete-continuous planning\u201d is not solving the hybrid part. It is using predefined skills. Note that as the authors show just planning is not enough in a real set-up as the world has errors and skills have to be hardcoded. Furthermore, as it is well described in the Limitation this is a very powerful planner that generates code, but the continuous control of the execution is not addressed in this work. In essence this is a very sophisticated planner but it is not solving hierarchical control.\n\n**State of the art**\n\nFor completeness I am missing this LLM approach to Robotics: PaLM-E: An Embodied Multimodal Language Model\n\nAnd also recent works on planning with low-level control such as: Active inference and behavior trees for reactive action planning and execution in robotics. TRO2023\n\n**Results**\n\nFor a fair baseline comparison authors can use a PDDL planner as baseline. It may be misleading to call a baseline comparison an ablation study of the own algorithm.\n\nIt is not clear the type of randomization in the environment initialization to properly evaluate the accuracy of the planner."
                },
                "questions": {
                    "value": "**Further comments:**\n\n-There is no mention on what type of LLM is being used and how it is pretrained and refined for each component. I think this is important information.\n\n-\u201cHierarchical Policies for Robot Tool Use\u201d there is no analysis of the combinatorial nature of the parametrized skills. We are talking about 4 skills with how many parameters? How many instances of objects? This is important to understand the level of complexity of the decision tree.\n\n-Do we need 4 LLMs to solve simple reasoning and generate a plan?\n\n*A high-level comment*\n\nProblem solving is the key point in this work. The citation to Josep Call is crucial. While it is shaped as a tool use the fact is that the robot does not understand a tool as a tool but a set of skills that can manipulate the world. Two things that are usually missing in this type of approaches are:\n\n-\tHumans we have mechanical/dynamics knowledge learnt from experience. Although the authors mention the affordances, note that it is not only about semantics but also about the real interaction in the environment.\n\n-\tThere is no analysis of how the system handles uncertainty resolution and trades off exploitation vs exploration (or intrinsic motivation). This is a important concept in creativity. Only reasoning is not enough to induce creativity, but probably reasoning and uncertainty resolution to try new things could be artificial creativity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760121301,
            "cdate": 1698760121301,
            "tmdate": 1699637128058,
            "mdate": 1699637128058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Ka8fHIQUq",
                "forum": "IKOAJG6mru",
                "replyto": "W9zeCk3cfE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QoGZ (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the encouraging review! We are glad that you appreciate the thoroughness and depth of our work. Here are our responses:\n\n> Focus of \"long-horizon hybrid discrete-continuous planning\".\n\nThanks for the question. We explain the hybrid task-motion planning problem here in more detail. An important part of TAMP problems is the interdependence of the motion-level and task-level aspects of the problem. Ignoring such interdependence between them will make it unable to solve the problem. We borrow this example from the seminal TAMP survey [1]:\n - Consider a task where the robot needs to place a particular object (name $A$) at a location. It might select a high-level plan \"skeleton\" such as \"moveF$(q_0, \\tau_1, q_1, p_0)$, pick[$A$]$(q_1, p_0, g)$, moveH[$A$]$(g, q_1, \\tau_2, q_2)$, place[$A$]$(q_2, p_1, g)$\", where \n     - moveF action involves robot movement when the gripper has no object with it.\n     - moveH[$A$] action involves robot movement when the gripper is holding object $A$.\n - The action primitives are discrete options. However, the free parameters are continuous values, such as robot configurations $(q_0, q_1, q_2)$, a grasp pose $(g)$, placement poses $(p_0, p_1)$, paths $(\\tau_1, \\tau_2)$. The skeleton imposes constraints on the choices of those values, or conversely, there could be no satisfying set of values, and the skeleton needs to be changed. For example, if there is object $B$ occupying the target location, the robot needs to first remove $B$ and then place $A$ at the target location.\n\nThis example showcased the importance of hybrid discrete-continuous planning. We will modify the problem formulation accordingly in the final version to highlight this.\n\n**Ref:**\n\n[1] Garrett, Caelan Reed, et al. \"Integrated task and motion planning.\" Annual review of control, robotics, and autonomous systems 4 (2021): 265-293.\n\n> Analysis of the combinatorial nature of the parametrized skills.\n\n - For the robotic arm, we have a skill library that includes```[get_position, get_size, open_gripper, close_gripper, move_to_position```. Among them, ```get_position``` and ```get_size``` take in discrete ```object_name``` as input, while ```move_to_position``` takes in 3D position of the target gripper as input. The longest plan (Can-Grasping) requires about 15 steps. A more detailed description of the parameterized skills can be found in Appendix B in the original draft.\n\n> State of the Art\n\n - Thanks to the reviewer for pointing out these two papers. We added them into the additional related work in the appendix and will incorperate them to the final version of related work.\n\n> Experiment results\n - **Compare with a PDDL algorithm**: Although we formulated creative tool use as a planning problem, solving such tasks requires more than just geometric planning. PDDL and other formal-logic planners, for example [2], are able to solve the Can-Grasping task, which requires only geometric planning. However, they cannot solve some other tasks in this work, for example, Cube-Lifting, which requires identifying a hidden lever structure in the scene and finding a way to activate this mechanism. The traditional planners are not able to reason based on common knowledge or manufacture nonexistent tools, which are critical to solving other creative tool-use tasks. \n - **Randomization:** For robot arm experiments, we randomize the initial configuration of objects. For quadrupedal robot experiments, we randomize the initial position of movable objects and the robots. We show the randomization range in Table 1 and add it to Appendix G in the updated draft.\n\nTable 1: Randomization of objects in each task.\n| | Objects: Randomization range [x_min, x_max, y_min, y_max] |\n| ------------- | ------- |\n| Milk-Reaching | pineapple toy, lock, cube, tomato toy: [0, 0.45, -0.25, 0.25]; hammer: [0, 0.45, -0.25, 0]; milk: [0.45, 0.6, 0, 0.25]  |\n| Can-Grasping | stick: [0, 0.45, -0.25, 0]; paper: [0, 0.45, 0, 0.25]; can: [0.45, 0.7, -0.1, 0.1]  |\n| Button-Pressing | magnetic_cube1, magnetic_cube2, magnetic_cube3: [0, 0.1, -0.25, 0.25]; button: [0.45, 0.7, -0.25, 0.25]  |\n| Sofa-Traversing | surfboard: [0.3, 0.6, -0.175, 0.0]; cloth: [0.3, 0.6, 0.0, 0.2]; robot: [-0.5, -0.2, -0.2, 0.2];   |\n| Sofa-Climbing | box_2: [0.0, 2.0, 0.0, 1.0]; robot: [0.0, 3.0, 0.0, 2.0];   |\n| Cube-Lifting | chair: [0.8, 1.2, -0.1, 0.1]; surfboard: [0.8, 1.2, 0.3, 0.5]; robot: [0.0, 3.0, 0.0, 2.0];   |\n\n**Ref:**\n\n[2] Toussaint, Marc, Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. 2018. \u201cDifferentiable Physics and Stable Modes for Tool-Use and Manipulation Planning.\u201d In Robotics: Science and Systems XIV. Robotics: Science and Systems Foundation. https://doi.org/10.15607/rss.2018.xiv.044."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193005362,
                "cdate": 1700193005362,
                "tmdate": 1700193226527,
                "mdate": 1700193226527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLObySARe1",
                "forum": "IKOAJG6mru",
                "replyto": "0Ka8fHIQUq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
                ],
                "content": {
                    "title": {
                        "value": "Good results, be careful with the terminology."
                    },
                    "comment": {
                        "value": "Thanks so much for the response. After reading the reviews and comments I would keep my score due to the novelty and the good presentation of the results showed. Please be careful with the terminology. For instance in a decision making problem I may say for instance discrete-time, discrete-action, continuous-discrete hybrid state."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493509665,
                "cdate": 1700493509665,
                "tmdate": 1700493509665,
                "mdate": 1700493509665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZU4E9A3J6",
            "forum": "IKOAJG6mru",
            "replyto": "IKOAJG6mru",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a prompting approach to enable creative tool use for robots. The approach constsists of four stages, using an \"analyzer\" prompt to extra objects, a planner planner prompt to generate a rough plan, a \"calculator\" prompt to populate it with action parameters, and finally a \"coder\" prompt to generate executable python code. The approach is demonstrated on a simulated and real robotics example."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Getting robots to solving complex tasks is an important and difficult problem\n- Many are interested in LLMs at the moment, and this paper provides some further information on how to use them\n- The results seem impressive, and it has an ablation study showcasing that each module seems to be required for success on these examples"
                },
                "weaknesses": {
                    "value": "- A lot of engineering seems to have gone into these examples. The prompts (on separate github) contain a number of hints on what not to do when solving the problem, which seem engineered for the particular tasks. Examples:\n  - \"If you do not know the actual value, use an offset = 1m.\"\n  - \"You must be careful when calculating with negative values.\"\n  - \"You must understand that the distance between the two objects' center and the distance between the two objects' edges along an axis are different.\"\n  - The coder in Fig.2 also generates a seemingly arbitrary gripping offset for the hammer which I guess you engineered depending on the shape of the hammer.\n  - Some motion primitives are a bit contrived: As a roboticist, getting the robot to kick the surfboard in place to traverse the sofa seems like an extremely challenging tasks that I guess you just spent a lot of time engineering the motion primitives for. I don't think these are very realistic examples of your approach considering how much engineering must have gone into them. It is very difficult to say how much going on here is just simple symbolic task planning vs. motion planning (e.g. the real-valued positions and orientation parameters).  \n- It relies only on ChatGPT 4.0 which means that it is unclear to me if this architecture design and ablation study would generalize to other LLMs. GPT4.0 is much better than open source models so the decision is understandable but it is a weakness of the paper. It also makes reproducing it harder since ChatGPT is updated and has been observed to change behavior over time (OTOH it is very easy to use the current version of GPT4...).\n- Relation to other LLM works that do manipulation could maybe be clarified, e.g. VoxPose was kind of brushed off as being multi-modal, but isn't that a strength? IIRC they also show somewhat complex actions (grabbing a toast and puttig it on a cutting board). Your example is more complex but it is difficult to quantify since they seem heavily engineered.\n\nMinor issues with presentation/claims:\n- Why are the prompts in the appendix only links to github instead of actually in the appendix? IIRC there is no page limit on the ICLR appendix.\n- The language could be better, especially the examples in the intro do not actually seem to be grammatically correct calls for action, e.g., \"grasping a milk cartoon\" should be \"grasp a milk cartoon\", \"walking to the sofa\" should be \"walk to the sofa\" and so on. This is a bit problematic if a prompt is unintentionally ambigious as it could affect your results."
                },
                "questions": {
                    "value": "- How much do you vary/randomize in your sim and real robot examples, does the robot and world always start in the same configuration?\n- Can you also say something about how much the action parameters are varied in response to these, or maybe show us some example python code from different runs of the algorithm on e.g. the milk example?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798001504,
            "cdate": 1698798001504,
            "tmdate": 1700504523441,
            "mdate": 1700504523441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c0jSwIpu0W",
                "forum": "IKOAJG6mru",
                "replyto": "qZU4E9A3J6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pWWx (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We are pleased to hear that you found our experiments impressive and that the modules in RoboTool contribute to performance improvements. Please find our detailed responses below.\n\n>  The prompts contain hints on what not to do when solving the problem.\n\n - We removed the prompt related to \"offset=1.0\" and \"careful when calculating negative values.\" After rerunning the experiments, we found some decrease in RoboTool's success rate (0.87 -> 0.83); however, it was insignificant. We updated Table 1 in the draft accordingly.\n - The prompts related to the negative values and distances aim to remedy LLMs' deficiency, which hardly hints at how to plan for a specific task. Please note that we use the same prompts for all experiments, not specifically engineered for each task. Designing prompts containing general rules to follow is common in many embodied LLMs literature. \n     - Regarding the prompt mentioning that the distance between objects' centers and boundaries are different, it is related to the 3D reasoning capability of gpt-4: It sometimes mistakenly thinks that the distance between objects is the distance between their centers without considering the size of the objects. We want to reduce such mistakes by reinforcing the distance concept in 3D world. However, RoboTool decides whether to use the distance between centers or the distance between boundaries for downstream planning.\n\n> Is the gripping offset in Fig. 2 code for the hammer engineered based on the shape of the hammer?\n\n - The numbers are not engineered based on the shape of the hammer. The calculator prompts contain an example of calculating a generic object's grasping position. In other generated codes, RoboTool follows the examples in the prompt and automatically gets the grasping offset based on the object's bounding box size. We provide more generated codes in Appendix I.\n - We need such an example in the prompt since RoboTool uses general motion primitives, such as ```move_to_position(pos)```  and ```close_gripper()``` to provide extra planning flexibility, instead of more granular primitives, such as ```pick(object)``` and ```place(object)``` in many existing methods [1, 2, 3, 4]. RoboTool needs to calculate an accurate gripper target position, which is difficult to rely solely on the prior knowledge of LLMs alone.\n\n**Ref:**\n\n[1] Singh, Ishika, et al. \"Progprompt: Generating situated robot task plans using large language models.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n[2] Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" arXiv preprint arXiv:2204.01691, 2022.\n\n[3] Huang, Wenlong, et al. \"Grounded decoding: Guiding text generation with grounded models for robot control.\" arXiv preprint arXiv:2303.00855, 2023c.\n\n[4] Liang, Jacky, et al. \"Code as policies: Language model programs for embodied control.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.\n\n> The engineering effort put in motion primitives.\n - Thanks for the question. Our ```push_to_position()``` skill is based on Unitree's built-in walking mode gait with a small gait height and a motion planner to generate waypoints to move objects around (as shown in Appendix B.1 in the original draft). There is surging interest in obtaining better loco-manipulation skills [6-8], which is manipulating objects with a quadrupedal robot. We provide one simple implementation of such loco-manipulation skill, which is already sufficient to perform complex long-horizon tasks. Although obtaining such motion primitive is not the focus of this work, we are eager to integrate more diverse skills in RoboTool. \n - Instead, in this work, we focus on high-level task planning and assume access to a library of existing motion primitives, similar to SayCan [2], GroundedDecoding [3], PPDL with LLMs [5] etc. Please note that our method is general and is compatible with skills derived from different methods, such as classical planning, RL, or imitation learning. \n\n\n**Ref:**\n\n[5] Silver, Tom, et al. \"Generalized planning in pddl domains with pretrained large language models.\"\narXiv preprint arXiv:2305.11014, 2023.\n\n[6] Nachum, Ofir, et al. \"Multi-agent manipulation via locomotion using hierarchical sim2real.\" arXiv preprint arXiv:1908.05224 (2019).\n\n[7] A. Rigo, Y. Chen, S. K. Gupta, and Q. Nguyen, \u201cContact optimization for non-prehensile loco-manipulation via hierarchical model predictive control,\u201d arXiv preprint arXiv:2210.03442, 2022.\n\n[8] M. Sombolestan and Q. Nguyen, \u201cHierarchical adaptive locomanipulation control for quadruped robots,\u201d arXiv preprint arXiv:2209.13145, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192466016,
                "cdate": 1700192466016,
                "tmdate": 1700192466016,
                "mdate": 1700192466016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b63pPcwgav",
                "forum": "IKOAJG6mru",
                "replyto": "qZU4E9A3J6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your rebuttal feedback!"
                    },
                    "comment": {
                        "value": "Thank you so much for your feedback and suggestions to improve our work! We hope that our replies and additional results have addressed your concerns. As the discussion period ends in a few days, we would like to ask if you have any further questions, and we are happy to provide more clarification. Thank you!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496822823,
                "cdate": 1700496822823,
                "tmdate": 1700496822823,
                "mdate": 1700496822823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0T8nAiXXDm",
                "forum": "IKOAJG6mru",
                "replyto": "qZU4E9A3J6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
                ],
                "content": {
                    "title": {
                        "value": "Reviwer response to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their thorough rebuttal and new experiments.\n\nHaving gone over this again now, I think the paper has merit but I still have a few concerns w.r.t. robotics framing and the experiments:\n1) Your first contribution seems to have merit but is a bit arbitrarily narrow. LLMs encode common-sense knowledge about the world, so that you can use them to improve robot interaction with the world is not that surprising in itself. The way you do this using multiple LLM queries is the novel part and seems to have merit based on the ablations you have shown. However this has some weaknesses:\n\n    a) The paper focuses on tool use (probably because of the high-profile role it has played in theories of animal intelligence), but since the embodiment of tool use is abstracted away (relies on pre-engineered perception and motion primitive modules) it seems these methods could be used for any task planning problem. This is a good thing in a sense but it makes the focus on tool use seem like an arbitrary restriction chosen more for PR than practical considerations.\n\n    b) In particular, the paper claims that it solves a motion planning problem but the code examples I've seen seem to mainly only move_to(get_position_of_object(...)) with possible +object_size/2 added or something similar. It is very difficult to get a sense for how much of a motion vs. task planning problem it is actually solving. This is extra problematic because the \"calculator\" component in your novel architecture is supposed to come up with the (real-valued) parameters but it mainly seems to just need to plug in getters for object positions. The lack of non-trivial motion planning in the experiments also undermines your proposed architecture a bit. These planner properties could probably also have been tested individually instead of everything engineered together into a complex experiment.\n\n2) Your second contribution on creating a benchmark for tool use is also a bit weak because some of your benchmarks (especially the quadruped one) seem a) a bit contrived and/or b) a lot of work to reproduce. Again, all the hard embodiment problems of tool use are engineered away, and doing this engineering to get a quadruped robot to reliably push objects on sofas seems like potentially a lot of effort. Is there a sim version of this btw?\n\nThat said, since the architecture does show improved results in your experiments (even if it is arguably mostly \"task planning for tool use\", and only evaluates LLM generalization on two different versions of ChatGPT). I will tentatively raise the score one step while I digest the other reviews / discussion."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504268165,
                "cdate": 1700504268165,
                "tmdate": 1700504642232,
                "mdate": 1700504642232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]