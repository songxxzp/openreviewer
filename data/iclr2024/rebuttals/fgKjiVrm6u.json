[
    {
        "title": "REFACTOR: Learning to Extract Theorems from Proofs"
    },
    {
        "review": {
            "id": "MCgipbgPD9",
            "forum": "fgKjiVrm6u",
            "replyto": "fgKjiVrm6u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to extract a sub-tree from a complete proof tree to obtain new theorems. Given a human-written proof, it constructs training samples by expanding the theorems nodes with their proof tree. Then, this paper trains a graph neural network to embed the proof tree and classify whether a node is an expanded node to extract nodes for forming a theorem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper improves the MetaGen performance by training it with the extra extracted theorems."
                },
                "weaknesses": {
                    "value": "The necessity of training a neural network to extract sub-proof is not validated and explained. One can easily traverse the proof tree and obtain multiple sub-proof as theorems. This simple method can also surpass the proposed method in lots of aspects. For example, if it extracts all sub-proof, it might obtain all human-defined rules instead of 19.6% of them and more valid theorems than 1923. The experiments do not have a comparison with such baselines and cannot validate the effectiveness of the proposed method.\n\nThe paper does not give a detailed description in the main text of how to extract a theorem given the binary prediction results within a proof."
                },
                "questions": {
                    "value": "- Why not fill some nodes with pre-defined rules to connect the positive nodes and obtain more valid theorems?\n\n- How does the theorem extracted from the validation set occur/support the proofs from the test set?\n\n- Do the positive and negative nodes balance in the training data?\n\n- Can simple rule-based extraction methods, such as byte-pair encoding(BPE), improve the performance of ATP?\n\nPost Rebuttal:\nThe response addresses my concerns and I have raised my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698570074229,
            "cdate": 1698570074229,
            "tmdate": 1700635435633,
            "mdate": 1700635435633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XX1WPIcg1t",
                "forum": "fgKjiVrm6u",
                "replyto": "MCgipbgPD9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QrE7"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and we address individual points below.\n\n> The necessity of training a neural network to extract sub-proof is not validated and explained. One can easily traverse the proof tree and obtain multiple sub-proof as theorems. This simple method can also surpass the proposed method in lots of aspects. For example, if it extracts all sub-proof, it might obtain all human-defined rules instead of 19.6% of them and more valid theorems than 1923. The experiments do not have a comparison with such baselines and cannot validate the effectiveness of the proposed method.\n\nThank you for your comment and we would like to provide some clarifications.\n\nIf we traverse the proof trees and extract all possible sub-proof as theorems, the amount of theorems will be combinatorial to the number of nodes in the proof tree, which is not practical and useful for downstream tasks. Instead, we compared our method against a symbolic extraction method [1] where we keep the number of extracted theorems being the same (1923 in this case). This makes a fair comparison and the results are discussed in the last paragraph of page 7. The symbolic baseline only achieves a 1.7% accuracy compared to 19.6% by REFACTOR.\n\nFurthermore, our goal is not only extracting new theorems but also demonstrating their usefulness on downstream tasks. In Table 4, we show that with our newly extracted theorems (REFACTOR), the Holophrasm theorem prover can prove 75 more test theorems whereas the symbolic baseline improves Holophrasm only by 9 theorems.\n\nThese results demonstrate that training a neural network to extract new theorems is more effective than symbolic baselines.\n\n> The paper does not give a detailed description in the main text of how to extract a theorem given the binary prediction results within a proof.\n\nSorry for the confusion and missing it in the main text. We simply extract all the nodes whose output probability is greater than 0.5. We provided more details on theorem extraction in Appendix A.1. We will update our main text based on the Appendix in the final version of the paper.\n\n> Why not fill some nodes with pre-defined rules to connect the positive nodes and obtain more valid theorems?\n\nThank you for the great suggestion! More valid theorems can indeed be obtained with the introduction of pre-defined rules. In this work, our goal is to show the possibility of just training a neural network to extract useful theorems. A hybrid approach that combines with rule-based methods is complementary to REFACTOR and we leave this as future work.\n\n> How does the theorem extracted from the validation set occur/support the proofs from the test set?\n\nThank you for raising this point. To test the generalization ability of the neural network, we performed a target-wise split on the dataset. That is, the theorems to be extracted are non-overlapping for the training, validation and test set. We provide more detailed discussion on dataset creation in Section 5.1.\n\n> Do the positive and negative nodes balance in the training data?\n\nThank you for the comment. Because of the presence of many large proof trees, the positive and negative nodes are not balanced. We did not explicitly balance the number of positive and negative nodes in the training data. The proof-level test accuracy of 19.6% attained by our best model suggests the model can still learn under unbalanced data.\n\n> Can simple rule-based extraction methods, such as byte-pair encoding(BPE), improve the performance of ATP?\n\nThank you for the suggestion. In Table 4 and Section 5.6, we use a simple rule-based extraction method [1] optimized for compression objective to extract new theorems and investigate whether it can improve the theorem prover performance. Compared to the symbolic baseline, REFACTOR leads to proving 66 more test theorems, demonstrating the usefulness of the extracted theorems.\n\n[1] Vysko\u010dil, Ji\u0159\u00ed, David Stanovsk\u00fd, and Josef Urban. \"Automated proof compression by invention of new definitions.\" In Logic for Programming, Artificial Intelligence, and Reasoning: 16th International Conference, LPAR-16, Dakar, Senegal, April 25\u2013May 1, 2010, Revised Selected Papers 16, pp. 447-462. Springer Berlin Heidelberg, 2010."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071330327,
                "cdate": 1700071330327,
                "tmdate": 1700071330327,
                "mdate": 1700071330327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FH8Gtif2oZ",
                "forum": "fgKjiVrm6u",
                "replyto": "i2roOZVlqI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I am still concerned about the experiments.\n\nThe paper first gives the accuracy and the number of extracted theorems. However, I don't think these two metrics solely can support the claim. Symbolic methods can easily surpass the proposed method by extracting all sub-proofs to obtain more theorems and 100% human-theorems recall. Limiting the theorem number to 1923 is unfair as the proposed method can only extract 1923 theorems while symbolic methods can extract much more. Thus, the accuracy and the extracted theorems number are not appropriate metrics.\n\nHow much the extracted theorems can help prove is a more important metric. The paper shows the results in Table 4 but I still have some questions about the setting:\n- What is the set for training the REFACTOR, extracting the theorems, and testing the Holophrasm respectively?\n- What are the corresponding splits for the symbolic baseline?\n\nI think a proper setting is to train the REFACTOR on the training set, extract theorems from the valid to demonstrate its extraction ability and test the Holophrasm on the test set to show the usefulness of the extracted theromes. In this way, the REFACTOR will not memorize the groundtruth theorems, and the Holophrasm will not see the partial groundtruth proof.\n\nLooking forward to the response.\n\nI have another minor question: If the Holophrasm is trained with augmented theorems and the \"correct\" theorem is another human-proved theorem substituted into another proof, will the \"incorrect\" theorems be the actual new theorems that do not exist in the original dataset and eventually improve the Holophrasm."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544698561,
                "cdate": 1700544698561,
                "tmdate": 1700544698561,
                "mdate": 1700544698561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WCJRdPncgR",
                "forum": "fgKjiVrm6u",
                "replyto": "GCvWyFoERk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_QrE7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. It has addressed most of my concerns. One more question: How many theorems are extracted by the symbolic baseline for training Holophrasm?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621476968,
                "cdate": 1700621476968,
                "tmdate": 1700621476968,
                "mdate": 1700621476968,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yv8LzoncWz",
            "forum": "fgKjiVrm6u",
            "replyto": "fgKjiVrm6u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_VAcB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_VAcB"
            ],
            "content": {
                "summary": {
                    "value": "A scheme is proposed for learning to extract subtrees from proofs -- seen as computation trees -- that are expected to be useful elements to add to a library of theorems. The algorithm is trained on data created by unraveling function applications within human-written proofs and substituting the proof tree of the function being replaced, thus performing imitation learning on semi-synthetic data. The new theorems extracted in this way from a Metamath library are shown to yield shorter proofs (using proof-search algorithms applied to both the original and refactored collection of theorems)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good motivation and introductory examples. Main ideas are clearly explained.\n- Very interesting method of learning to compress by training on artificially unraveled function applications."
                },
                "weaknesses": {
                    "value": "- Framing and related work:\n  - It is claimed a few times that proof refactoring \"mimics what human mathematicians do\" or the like, but this is not backed up. Is it your intuition or do you have evidence? \n    - I am not convinced that compression of proofs is similar to human conjecture-making and definition-building behaviour. It certainly makes sense as a procedure for defining new abstractions, but it does not account for intrinsically motivated conjecture-making (guessing a theorem is true and then trying to prove it).\n  - Section 3 misses essential mathematical background to theorem proving. The equivalence of proofs and programs / computation trees is stated and assumed without further comment, but it is not a trivial concept. The Curry-Howard correspondence and how it applies to Metamath deserves more discussion. See or cite, e.g., [P. Wadler, \"Propositions as types\"] for historical overview.\n- Writing unclarities and bugs: \n  - The example in Fig. 1 was hard to make sense of. (Note that the reviewer is familiar with Lean but not Metamath.) Can the last paragraph of section 3 explain it in plain(er) language, explaining what types of objects `ph`, `wffph`, etc. are and what the theorem means?\n  - Incorrect use of \"connected component\" (several times on p.4). It is used to mean \"connected subgraph\", when in fact \"connected component\" has a different and very well-established meaning in graph theory.\n  - Related, please also explain in a few words why connected subtree is not sufficient for validity. Note that issues related to bound variables would only be worse in more sophisticated proof systems -- this could be discussed as a limitation.\n  - Misc.: Please check your citations (`\\citep`/`\\citet`).\n- Evaluations are only on a relatively simple library and there isn't fair comparison to training-free compression schemes for extracting the new theorems.\n- Also see questions below."
                },
                "questions": {
                    "value": "- The GNN model seems to output a binary logit at each node. How do you extract the subtree from the predictions? I could not find discussion of the exact procedure in the text. The code does an argmax scheme; did you try sampling or other approaches?\n  - Such a procedure seems to have an important limitation, which it can't produce a multimodal distribution over subtrees (which is a problem if the compressions on two different subtrees are mutually exclusive).\n- Regarding the evaluation:\n  - Could newly extracted theorems be equivalent to existing ones by symmetries? Does this affect the experimental claims?\n  - Do you know for certain that the \"human-written\" proofs were written each proof from scratch? How does it affect the claims if humans copied chunks between the proofs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Reviewer_VAcB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685248951,
            "cdate": 1698685248951,
            "tmdate": 1700325635694,
            "mdate": 1700325635694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "STdIF3aDNd",
                "forum": "fgKjiVrm6u",
                "replyto": "yv8LzoncWz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VAcB (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and detailed review on our paper. We address individual points below.\n\n> It is claimed a few times that proof refactoring \"mimics what human mathematicians do\" or the like, but this is not backed up. Is it your intuition or do you have evidence? I am not convinced that compression of proofs is similar to human conjecture-making and definition-building behaviour. It certainly makes sense as a procedure for defining new abstractions, but it does not account for intrinsically motivated conjecture-making (guessing a theorem is true and then trying to prove it).\n\nThank you for the question. The intuition comes from scenarios where we notice there is some part of the proof that is self-contained and does not relate closely to the rest of the proof. It seems clearer to separate it out into a lemma both better presentation, future use and compositional generalization.\n\nOur focus of the paper is not to fully capture what human mathematicians would do which involves iterative processes and refinement. We fully acknowledge human mathematicians perform a variety of actions such as conjecture making, definition building besides theorem abstraction from existing knowledge base. As a first attempt to capture partially what a human mathematician would do, we perform theorem expansion on human written corpus for imitation learning targets. We then learn to extract a modular component from the existing corpora. The extracted new theorems are shown to be useful in improving an off-the-shelf theorem prover performance as seen in Table 4.\n\n> Section 3 misses essential mathematical background to theorem proving. The equivalence of proofs and programs / computation trees is stated and assumed without further comment, but it is not a trivial concept. The Curry-Howard correspondence and how it applies to Metamath deserves more discussion. See or cite, e.g., [P. Wadler, \"Propositions as types\"] for historical overview.\n\nSorry for missing the important background in establishing equivalence between these two concepts. We have updated Section 3 of our paper pdf in blue. We provide a very high level explanation to establish the equivalence between proofs and computation citing the related work. We would highly appreciate your feedback on our revision.\n\n> The example in Fig. 1 was hard to make sense of. (Note that the reviewer is familiar with Lean but not Metamath.) Can the last paragraph of section 3 explain it in plain(er) language, explaining what types of objects ph, wffph, etc. are and what the theorem means?\n\nWe apologize for the confusion and below is a more detailed explanation for Figure 1 (a) theorem *a1i*. We will incorporate the explanation in the final version of the paper.\n\nThe theorem proved in Figure 1 (a) states if *ph* is true, then *(ps->ph)* is true. Although this is rather low-level and possibly trivial, it is proven with the axiom of simplification along with the rule of Modus Ponens. To begin with, we start from *wffps* and *wffph*. These are expressions that state variable *ps* and *ph* are well-formed. Next, a theorem *wi* is invoked, which leads to *wff (ps->ph)*. This means *(ps->ph)* is also well-formed. With *wffps* and *wffph* shown on the top right corner of Figure 1 (a), an axiom called \u201cthe principle of simplification\u201d can be invoked to arrive at *|-(ph->(ps->ph))*. The axiom essentially states the expression *a->(b->a)* is always true. In the last step, we use the rule of Modus Ponens. Specifically, it takes in 4 arguments: *wffph*, *wff(ps->ph)*, *|-ph* and *|-(ph->(ps->ph))*. The third argument *|-ph* comes from the hypothesis of the theorem (if ph is true) and the rest of arguments have been shown to hold from above. By applying the rule of Modus Ponens, we arrive at the goal *|-(ps->ph)*.\n\n> Incorrect use of \"connected component\" (several times on p.4). It is used to mean \"connected subgraph\", when in fact \"connected component\" has a different and very well-established meaning in graph theory.\n\n> Related, please also explain in a few words why connected subtree is not sufficient for validity. Note that issues related to bound variables would only be worse in more sophisticated proof systems -- this could be discussed as a limitation.\n\nSorry for the incorrect terminology and we have updated the paper pdf. Since both hypothesis statement and theorem application are represented as nodes in the proof tree, it is possible to have an incorrect number of hypotheses (arguments) for a theorem (function) invocation. We agree with the reviewer that the issue could be worse in more sophisticated proof systems. One possible remedy is to automatically fill in the missing hypotheses during extraction."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071105634,
                "cdate": 1700071105634,
                "tmdate": 1700071105634,
                "mdate": 1700071105634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tXMCfQGRVR",
                "forum": "fgKjiVrm6u",
                "replyto": "yv8LzoncWz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_VAcB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Reviewer_VAcB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "Thank you for the answers, clarifications, and corrections. In particular, I seem to have missed that Table 4 has results comparing the number of test theorems proved with your method compared to compression-based baselines like [Vysko\u010dil et al.]. I updated the score. I see that this can be a useful method, but the simplistic and deterministic way of selecting a proof subtree (via the logit thresholding) seems like a major limitation, one that isn't present in compression-based schemes."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325618190,
                "cdate": 1700325618190,
                "tmdate": 1700325738663,
                "mdate": 1700325738663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "16tqfi0ofQ",
            "forum": "fgKjiVrm6u",
            "replyto": "fgKjiVrm6u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_WVCw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_WVCw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a pipeline to train neural networks to extract reusable theorems from mathematical proofs. It uses a graph neural network to take in a proof tree and make node-level classifications. It non-trivially extracts correct unseen theorems that humans would use 19.6% of the time. These theorems are then incorporated into the library to help in generating shorter proofs and potentially enhance the efficiency of baseline theorem provers.\n\nMore literature should be added in the related work section, for example, there is no related work mentioned after the year of 2021.\nThe author provides a symbolic baseline to demonstrate that 19.6% is non-trivial. The approach described in the symbolic baseline is intuitive and a fair comparison. The result provides confidence that this is an interesting line of work.\n\nI believe the work is meaningful for automated proof simplification and generating better training dataset to improve baseline theorem prover performance. However, I\u2019m concerned whether extracting \u201cnew theorems\u201d from existing proofs is helpful to the automated theorem proving community, or the maths community in general.\n\nGiven that the metamath library is one of the largest databases, it could be possible that the 19.6% is the best possible result for the current REFACTOR pipeline and architecture. Even though 19.6% is a non-trivial result, it would be meaningful if we know whether the results can be improved by expanding the theorems even more and generating more training data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This is a novel problem and the first application of GNN to this problem to the best of my knowledge, the idea of theorem expansion is quite intuitive. The result is novel and could be helpful for proof simplification and generating better proof dataset.\n\n- Potentially very impactful"
                },
                "weaknesses": {
                    "value": "The related work comparison can be improved."
                },
                "questions": {
                    "value": "Do you plan to apply it to other math libraries, e.g., LEAN? If so, can your technique be lifted to that setting without much effort. Please provide justification, if your answer is YES."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8011/Reviewer_WVCw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815395960,
            "cdate": 1698815395960,
            "tmdate": 1699636987877,
            "mdate": 1699636987877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m7NwdFCGqU",
                "forum": "fgKjiVrm6u",
                "replyto": "16tqfi0ofQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WVCw"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging review and comments! We respond to your individual questions below.\n\n> However, I\u2019m concerned whether extracting \u201cnew theorems\u201d from existing proofs is helpful to the automated theorem proving community, or the maths community in general.\n\nThank you for the question. We believe our work is still meaningful for the automated theorem proving community. In this work, we propose a novel way to expand proof trees and then create targets for training a neural network. The method along with the generated data from Metamath could be helpful to researchers who are interested in improving automated theorem prover performance. Besides, we extracted 1923 new theorems and as shown in Table 4, these newly extracted theorems are also directly used to prove test set theorems. Therefore, we believe this is a meaningful contribution.\n\n> Even though 19.6% is a non-trivial result, it would be meaningful if we know whether the results can be improved by expanding the theorems even more and generating more training data.\n\nThank you for raising this point. We believe with a more sophisticated way of theorem expansion such as recursive expansion, we could create more diverse targets to further improve the performance.\n\n> The related work comparison can be improved.\n\nThank you for the great suggestion and we have updated the Related Work section of our paper pdf in blue.\n\n> Do you plan to apply it to other math libraries, e.g., LEAN? If so, can your technique be lifted to that setting without much effort. Please provide justification, if your answer is YES.\n\nYes. Our work can be lifted to other popular theorem proving environments such as Lean. As discussed in Section 1 and 3, our approach only makes the assumption of a simple substitution inference rule. Proof trees in other math libraries such as Lean and Isabelle can also be represented as trees and mathematically support the substitution of lemmas. Furthermore, thanks to the development of theorem proving playground in Lean (LeanDojo) [1], interacting with the proof trees has become more convenient. With the development of better language models, the features on each node can also be embedded with a better semantic meaning.\n\n[1] Yang, Kaiyu, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. \"Leandojo: Theorem proving with retrieval-augmented language models.\" arXiv preprint arXiv:2306.15626 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070932039,
                "cdate": 1700070932039,
                "tmdate": 1700070932039,
                "mdate": 1700070932039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oT5mwiQfYo",
            "forum": "fgKjiVrm6u",
            "replyto": "fgKjiVrm6u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_QmHL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8011/Reviewer_QmHL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a deep learning method for extracting new proofs from human proofs. The task is formulated as a binary node classification of the nodes in the proof trees and a graph neural network is trained for this classification. Experiments on Metamath demonstrate the effectiveness of this approach. 1923 novel theorems are extracted from set.mm and these new theorems could greatly shorten human proofs. These new proofs could also be used to train the theorem prover and the performance of the Holophrasm prover is improved from 557/2720 to 632/2720."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 The problem of extracting sub-proofs as standalone theorems is relevant and important. It can help discover new lemmas from existing proofs and compress the formal proof corpora. \n2 The GNN-based approach is technically sound. \n3 Experiments demonstrate the usefulness of the newly extracted theorems (1) they can be used to shorten human proofs (2) they can be used to improve the performance of theorem proving."
                },
                "weaknesses": {
                    "value": "Metamath is a relatively simple formal mathematical language and less commonly used for advancing theorem proving compared to other formal provers like Isabelle and Lean. Although the proposed method may be applied to other provers, the implementation and the results of this paper doesn't contribute much to the advance of ATP directly."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699048261891,
            "cdate": 1699048261891,
            "tmdate": 1699636987777,
            "mdate": 1699636987777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kla6BBKcwa",
                "forum": "fgKjiVrm6u",
                "replyto": "oT5mwiQfYo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QmHL"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and suggestions. We address individual points below.\n\n> Metamath is a relatively simple formal mathematical language and less commonly used for advancing theorem proving compared to other formal provers like Isabelle and Lean. Although the proposed method may be applied to other provers, the implementation and the results of this paper doesn't contribute much to the advance of ATP directly.\n\nThank you for the comment. We fully acknowledge the fact that we implement REFACTOR on Metamath because of its simplicity and the inference rule (substitution). Our approach is also generally applicable to other formal systems such as Lean and Isabelle since proofs in these environments can also be represented as trees and mathematically support the substitution of lemmas."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070820750,
                "cdate": 1700070820750,
                "tmdate": 1700070820750,
                "mdate": 1700070820750,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]