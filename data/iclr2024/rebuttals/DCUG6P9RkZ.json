[
    {
        "title": "Better Imitation Learning in Discounted Linear MDP"
    },
    {
        "review": {
            "id": "Ga2L0bLKyh",
            "forum": "DCUG6P9RkZ",
            "replyto": "DCUG6P9RkZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to design better provably imitation learning (IL) algorithms in discounted linear MDP. In discounted linear MDP, the existing provably IL method PPIL ignores the exploration issue and thus requires the persistent excitation assumption. This paper presents a new method ILARL which is free of such an assumption. In particular, the key to removing such an assumption is the reduction of IL to online optimization with adversarial losses. With this reduction, the target becomes to design a provably RL algorithm in adversarial MDPs. To achieve this goal, this paper first presents an algorithm in the finite-horizon adversarial MDPs and then extends it to the infinite-horizon case. Finally, this paper plugs this RL algorithm into the IL framework, which yields the ILARL algorithm. The authors prove that ILARL has better theoretical guarantees than previous algorithms regarding the number of expert trajectories and MDP trajectories."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents a new IL algorithm ILARL and conducts a rigorous theoretical analysis. Compared with the previous SOTA IL method in discounted linear MDP, ILARL removes the persistent excitation assumption and attains better theoretical guarantees on the number of expert trajectories and MDP trajectories.\n2. The paper is well-written and easy to follow, providing clear explanations and detailed descriptions of the proposed method and theoretical analysis."
                },
                "weaknesses": {
                    "value": "1. The algorithmic designs and analysis techniques in this paper are not new. In terms of algorithmic designs, the main difference between ILARL, and existing IL algorithms OAL and OGAIL is the policy optimization step. However, the policy optimization algorithm in ILARL is highly similar to the one in [Sherman et al., 2023b]. \nFor theoretical analysis, the key step to removing the persistent excitation assumption is the regret decomposition in Eq.(2), which reduces IL to online optimization with adversarial losses. However, such a regret decomposition has been presented in OAL. Furthermore, among the three types of errors, policy regret is the most difficult part to analyze. However, the analysis of the policy regret in Theorem 3 largely depends on existing techniques developed in OAL and [Sherman et al., 2023b].\n2. The empirical evaluation is limited. This paper only considers a simple 2D environment. It is expected to verify the effectiveness of ILARL on more complicated tasks. Besides, this paper does not involve OGAIL for comparison."
                },
                "questions": {
                    "value": "1. Line 9 in Algorithm 3 is a little confusing. Algorithm 2 is a complete RL method that runs for K iterations while line 9 only corresponds to a one-iterate policy update.   \n2. Typos:\n    1. Line 4 in the first paragraph in Section 1: which compete \u2192 which competes.\n    2. Table 1: OLA \u2192 OAL.\n    3. Line 14 in Algorithm 1, Line 11 in Algorithm 2: as the cost function is considered, we should minus the bonus function in updating Q functions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401694730,
            "cdate": 1698401694730,
            "tmdate": 1699635994602,
            "mdate": 1699635994602,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lM6thjyitp",
                "forum": "DCUG6P9RkZ",
                "replyto": "Ga2L0bLKyh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The algorithmic designs and analysis techniques in this paper are not new. In terms of algorithmic designs, the main difference between ILARL, and existing IL algorithms OAL and OGAIL is the policy optimization step. However, the policy optimization algorithm in ILARL is highly similar to the one in [Sherman et al., 2023b]. For theoretical analysis, the key step to removing the persistent excitation assumption is the regret decomposition in Eq.(2), which reduces IL to online optimization with adversarial losses. However, such a regret decomposition has been presented in OAL. Furthermore, among the three types of errors, policy regret is the most difficult part to analyze. However, the analysis of the policy regret in Theorem 3 largely depends on existing techniques developed in OAL and [Sherman et al., 2023b].**\n\nWe agree that the techniques are similar to the existing works that you mentioned. In particular [Shani et al.,2021] and [Sherman et al., 2023b].  However, notice the following important differences.\n\n1) The algorithms we present achieves a better regret bound than [Sherman et al., 2023b].We indeed exploit the fact that we have full information about the cost vector while [Sherman et al., 2023b] focuses on the bandit feedback. Moreover our regret bound also improves the result of [Zhong \\& Zhang, 2023] for finite horizon adversarial MDPs.\n\n2) We notice that the regularization in the policy improvement step allows to control the pseudo regret $ \\sum^K_{k=1} < d^{\\pi_k} - d^{\\pi_E}, c_k >$ in the infinite horizon discounted setting.\n\n3) In the finite horizon case, we obtain a further improvement from $\\mathcal{O}(\\epsilon^{-4})$ to $\\mathcal{O}(\\epsilon^{-2})$. This is possible only with the new insight that if we allow the cost player to update first, then, the $\\pi$ player can know in advance the cost vector at the next round. We then, notice that LSVI-UCB originally designed for a fixed cost function still guarantees $\\mathcal{O}(\\sqrt{K})$ regret against a time changing cost vector sequence if the cost vector at the next round can be observed in advance.\nThis insight is missing in [Shani et al., 2021] that indeed updates the policy first and then the cost vector. This difference would make LSVI-UCB inapplicable. To fix the situation, the $\\pi$ learner could use our Algorithm 1 but this would lead to a MDP trajectories bound of order $\\mathcal{O}(\\epsilon^{-4})$ which is clearly worst than the $\\mathcal{O}(\\epsilon^{-2})$ bound attained by our BRIG (Algorithm 4).\n\nIn addition, we highlight that the contribution of our submission could be very valuable for the member of the imitation learning community that might not know the power of the connection between imitation learning and online learning in adversarial MDPs.\n\n**The empirical evaluation is limited. This paper only considers a simple 2D environment. It is expected to verify the effectiveness of ILARL on more complicated tasks. Besides, this paper does not involve OGAIL for comparison.**\n\nOGAIL is not implementable because it requires an integration oracle over the state space. See their definition of the features $\\phi^h_k$ just after their equation 16 in this version https://proceedings.mlr.press/v162/liu22u/liu22u.pdf\n\nMoreover, OGAIL is not designed for the discounted setting but rather for the finite horizon case.\n\n*Regarding more complicated experiment*\nGiven that [Shani et al. 2021, Liu et al. 2022] and our current submission suggests the importance of exploration in imitation learning. Thus,  we are planning to apply exploration heuristics to the policy update in commonly used algorithms like OAL, GAIL and IQLearn and see if the performance is enhanced.\nWe think that the answer to this question is currently open. Indeed, in our understanding, the OAL authors provide several MuJoCo experiments but they do not try to implement approximately the optimistic bonuses that are needed to prove the theoretical guarantees.\n\nAt the same time, we think that this deep imitation learning extension is beyond the scope of the current submission.\n\n**Line 9 in Algorithm 3 is a little confusing. Algorithm 2 is a complete RL method that runs for K iterations while line 9 only corresponds to a one-iterate policy update.**\n\nThanks for catching this issue ! We clarified that the policy is updated performing only one iteration of Algorithm 2. We also provide a complete presentation in Algorithm 5.\n\nThank you also for catching the typos that we corrected in our revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700007906991,
                "cdate": 1700007906991,
                "tmdate": 1700007906991,
                "mdate": 1700007906991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "attQgtHoMm",
                "forum": "DCUG6P9RkZ",
                "replyto": "eFXsr9lr5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response! However, I think my major concerns about the novelty of algorithmic designs and theoretical analysis still remain. \n\n**\"The algorithms we present achieves a better regret bound than [Sherman et al., 2023b].We indeed exploit the fact that we have full information about the cost vector while [Sherman et al., 2023b] focuses on the bandit feedback. Moreover our regret bound also improves the result of [Zhong & Zhang, 2023] for finite horizon adversarial MDPs.\"**\n\nThe idea that policy learning in IL can be regarded as solving adversarial MDPs with full information has been proposed in the OAL paper.\n\n**\"We notice that the regularization in the policy improvement step allows to control the pseudo regret\u00a0in the infinite horizon discounted setting.\"**\n\nIt seems that the role of policy regularization has been observed in [Moulin & Neu (2023)].\n\n**\"In the finite horizon case, we obtain a further improvement from $\\mathcal{O} (\\epsilon^{-4})$ to $\\mathcal{O} (\\epsilon^{-2})$.\u201d**\n\nThe OGAIL paper has already achieved the regret of $\\mathcal{O} (\\sqrt{K})$ (or sample complexity of $\\mathcal{O} (\\epsilon^{-2})$) in the finite horizon case. Thus, I think this improvement is not very significant.\n\n**\u201cIn addition, we highlight that the contribution of our submission could be very valuable for the member of the imitation learning community that might not know the power of the connection between imitation learning and online learning in adversarial MDPs.\u201d**\n\nThe connection between IL and online learning in adversarial MDPs has been drawn in the seminal work [Syed & Schapire (2007)] and recent works [Shani et al. (2021)] and [Zahavy et al. (2020)].\n\nReference:\n\n[1] Zahavy et al., \"Apprenticeship Learning via Frank-Wolfe.\" AAAI 2020."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647756517,
                "cdate": 1700647756517,
                "tmdate": 1700647756517,
                "mdate": 1700647756517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I04ajR7RXK",
                "forum": "DCUG6P9RkZ",
                "replyto": "Ga2L0bLKyh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Some important remarks."
                    },
                    "comment": {
                        "value": "Thanks for your response! We respectfully disagree on some of your remarks.\n\n***The OGAIL paper has already achieved the regret of $\\mathcal{O}(\\sqrt{K})$ (or sample complexity of $\\mathcal{O}(\\epsilon^{-2})$ ) in the finite horizon case. Thus, I think this improvement is not very significant.***\n\nWe are worried there is an important misunderstanding. OGAIL achieves the claimed sample complexity for **finite horizon Linear Mixture MDPs**. We do for finite horizon **Linear MDP**.\n\nDespite a similar name, the **Linear MDP** setting is way more involved as the unknown transition dynamics have $d\\times |\\mathcal{S}|$ degrees of freedom. In **Linear Mixture MDP** those are only $d$.\n\n***The connection between IL and online learning in adversarial MDPs has been drawn in the seminal work [Syed & Schapire (2007)] and recent works [Shani et al. (2021)] and [Zahavy et al. (2020)].***\n\nWe think that the only work that pointed out the same connection is [Shani et al. (2021)]. While the other works are not directly connected.\n\nIndeed, the work of [Zahavy et al. (2020)] requires to compute the optimal policy for each cost vector (see line 5 of their algorithm 1) and it is not discussed how to do it without dynamics knowledge. Similar observation holds for line 7 of Algorithm 1 in [Syed and Shapire, 2007] http://rob.schapire.net/papers/SyedSchapireNIPS2007.pdf\n\nThe complication of computing the optimal policy for each intermediate cost function is avoided taking the adversarial MDP perspective of our work and Shani et al. 2021.\n\n***The idea that policy learning in IL can be regarded as solving adversarial MDPs with full information has been proposed in the OAL paper.***\n\nThis is true, but this remark does not seem connected to our claim that you reported in bold before this remark.\nNotice indeed that the work of Shani et al. 2021 holds only for the tabular finite horizon setting while we extend it substantially to the infinite horizon discounted setting in Linear MDP. \n\nSo it is unclear how the fact solving adversarial ***tabular*** MDPs with full information has been proposed in the OAL paper makes our contributions in adversarial full information ***Linear*** MDPs less important. Could you please elaborate on this point ?\n\n***It seems that the role of policy regularization has been observed in [Moulin & Neu (2023)].***\n\nTrue, we took inspiration from their regularized value iteration algorithm. In our case, we adopted a regularized policy iteration scheme and we noticed that the same observation holds.\nFinally, let us remark that in the context of Linear MDP using regularized policy improvement creates major complications in bounding the covering number of the policy class whereas these are avoided in the Linear Mixture MDP setting of [Moulin & Neu (2023)].\n\n\nThanks in advance for you attention.\n\n\nBest,\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651782289,
                "cdate": 1700651782289,
                "tmdate": 1700669150358,
                "mdate": 1700669150358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SL8Vrz7ni0",
                "forum": "DCUG6P9RkZ",
                "replyto": "I04ajR7RXK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_B5xB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed response! \n\nI'd like to rectify an error in my previous statement. The connection between IL and online learning in adversarial MDPs is drawn in the OAL paper [Shani et al., 2021], instead of the works [Syed & Schapire, 2007, Zahavy et al., 2020].\n\nIn my understanding, the OAL paper [Shani et al., 2021] first proposes the connection between IL and online learning in adversarial MDPs, and achieves the $\\sqrt{K}$ regret in the finite-horizon tabular MDP. The OGAIL [Liu et al., 2021] paper extends OAL to the linear mixture MDP. This submission extends OAL to the linear MDP.\n\nTherefore, I find the novelty claim regarding the utilization of full information and the IL-adversarial MDP connection in the initial response to be somewhat less significant because this connection has been drawn in [Shani et al., 2021] and is largely independent of the specific MDP setting. Besides, my concern about evaluating the performance of ILARL on more complicated tasks still remains. Thus I would like to keep my current score unchanged."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722052971,
                "cdate": 1700722052971,
                "tmdate": 1700722052971,
                "mdate": 1700722052971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ztdrxhpfB3",
            "forum": "DCUG6P9RkZ",
            "replyto": "DCUG6P9RkZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission673/Reviewer_Wgzz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission673/Reviewer_Wgzz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an algorithm called ILARL for imitation learning in infinite horizon linear MDP. The authors relax the exploration assumptions in previous works and improve the rate from $O(\\epsilon^{-5})$ to $O(\\epsilon^{-4}$. The results are built upon a connection between imitation learning and online learning with adversarial lossses. Moreover, the paper presents a strengthen result for the finite horizon case and achieve $O(\\epsilon^{-2}$. The empirical results also show that ILARL outperforms other methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a new algorithm that requires less expert trajectories and MDP trajectories to achieve the $\\epsilon$ optimal result in both cases of finite-horizon and infinite-horizon. The results is solid and techniques are novel.\n- The paper presents the result and the analysis in a nice way such that it is easy to follow.\n- The empirical study also supports the theoretical results about the performance of the proposed algorithm."
                },
                "weaknesses": {
                    "value": "- The paper shows that the learned policy achieve the similar performance as the expert policy. I am wondering if there is any guarantee on the recovery of the true cost function.\n- The linear MDP assumption is restrictive. The contribution of the paper can be more significant if it can be extended to general MDPs.\n- Although the paper claims that it studies linear MDPs, Assumptions 1-3 are considering the finite state-action case. \n- The empirical study is performed on a articrafted MDP rather than a real reinforcement learning environment."
                },
                "questions": {
                    "value": "- Is it possible to extend the result to general MDPs rather than simple linear MDPs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636830642,
            "cdate": 1698636830642,
            "tmdate": 1699635994540,
            "mdate": 1699635994540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KXj1YDtxw1",
                "forum": "DCUG6P9RkZ",
                "replyto": "ztdrxhpfB3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThanks a lot for the words of appreciation to our paper ! In the following we answer your questions.\n\n**The paper shows that the learned policy achieve the similar performance as the expert policy. I am wondering if there is any guarantee on the recovery of the true cost function.**\n\nNot really, we think that recovering the true cost function from a single set of expert demonstration is unfortunately not possible without additional assumptions. There are some recent works along these lines [1,2,3] that we could add in the related works section if the reviewer feels that it would be helpful.\n\n[1] **Reward identification in inverse reinforcement learning**, K Kim et al. - International Conference on Machine Learning, 2021\n\n[2] **Identifiability in inverse reinforcement learning** H Cao et al. - NeurIPS, 2021\n\n[3] **Identifiability and generalizability from multiple experts in Inverse Reinforcement Learning** Rolland et al. - NeurIPS, 2022\n\n**The linear MDP assumption is restrictive. The contribution of the paper can be more significant if it can be extended to general MDPs.**\n\nWe have added a new section in the revised manuscript at page 14,15 and 16 where we discuss how the sample complexity of order $\\mathcal{O}(\\epsilon^{-2})$ for the finite horizon setting can be attained for bilinear classes which is a more general MDP class.\n\nHowever, the resulting algorithm is not guaranteed to be computationally efficient.\nThe focus of our paper was to consider a smaller MDP class but provide a computationally efficient algorithm for this setting.\n\n**Although the paper claims that it studies linear MDPs, Assumptions 1-3 are considering the finite state-action case.**\n\nThanks for this comment. That's true. We are still considering finite states and action spaces but we assume that the number of states is too large to be enumerated. The exact same proof would go through for infinite states but it would obfuscate the presentation slightly so we preferred to focus on the finite state case.\nOn the other end, the action set needs to be finite otherwise we would not have computationally efficient policy updates. For example, the update in Step 20 of Algorithm 1 would require to compute an integral over the action space. \n\n**The empirical study is performed on a articrafted MDP rather than a real reinforcement learning environment.**\n\nWe think that the take away of this theoretical work is that exploration in the policy optimization phase can be beneficial for imitation learning.\n\nA natural follow up on the applied imitation learning side is to to try some exploration heuristics for neural networks and insert them in the policy update of common deep imitation learning algorithms like GAIL, IQLearn and see if this would empirically enhanance their performance.\n\nWe think that this approach has the potential to lead to convincing empirical performance in more challenging MDPs but at the same times it looks to us beyond the scope of the current submission.\n\n**Is it possible to extend the result to general MDPs rather than simple linear MDPs?**\n\nAs mentioned previously, please chack page 14,15 and 16 of the update manuscript for an informal discussion of the extension to bilinear classes."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700007743511,
                "cdate": 1700007743511,
                "tmdate": 1700007743511,
                "mdate": 1700007743511,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2AW7WOwb6O",
            "forum": "DCUG6P9RkZ",
            "replyto": "DCUG6P9RkZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a new algorithm for imitation learning under linear MDP setting. By introducing the online learning in MDPs with adversarial losses, the author improves the bound of interactions number with the MDP from $\\mathcal{O}(\\epsilon^{-5})$ to $\\mathcal{O}(\\epsilon^{-4})$.\n\nAdditionally, unlike previous work, these results do not rely on exploratory assumptions, thereby offering broader applicability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a new algorithm, ILARL, namely Imitation Learning via Adversarial Reinforcement Learning algorithm. According to the results in the paper, the required trajectories number in the proposed algorithm has better dependence in $\\epsilon$ to achieve the same accuracy. The idea from adversarial online learning is adopted to design this algorithm."
                },
                "weaknesses": {
                    "value": "The paper does not keep consistent notations: $\\mathcal{A}$ is used for action space in MDP setting and an algorithm in definition 1; Cost function is utilised in MDP setting, but the numerical experiments adopt reward function setting.\n\nIn addition, the paper claims that the ILARL algorithm improves the dependence of accuracy $\\epsilon$ from $\\mathcal{O}(\\epsilon^{-5})$ to $\\mathcal{O}(\\epsilon^{-4})$, but the dependence of dimension $d$ increases from $d^2$ to $d^3$, so one natural question is that how to carefully select these parameters so that the proposed algorithm indeed requires less trajectories than the latest algorithm in Viano's paper in 2022.\n\nThe norm inequalities in assumptions 1-2 seem very technical, and it would be better if the authors could provide some insights about them."
                },
                "questions": {
                    "value": "1. The mathematical formulation for state value function in finite time horizon is pretty strange. I suppose the summation should take from 1 to $h$?\n\n2. The infinite horizon trajectories, according to the description in section 2, have random length sampled from the geometric distribution. Why geometric distribution is adopted here? The sampled number is still finite, so the cost in the time horizons greater than the sampled number is set to zero?\n\n3. In algorithm 3, line 6, the proposed algorithm project $w^{k+1}$ to the unit ball. How to ensure that the projected $w$ still constitute an adversarial costs in $[0, 1]$, as assumed in the MDP setting in section 2? Similar question happens to algorithm 4, line 7.\n\n4. It seems that the proposed algorithms have never updated matrix $M$, in assumption 1 and 2. Does this mean that the true transition kernel is not estimated or involved in the algorithms? \n\n5. The matrix $\\Phi$ is already known, according to assumption 1 or 2. But assumption 3 claims that the learner has access to $\\Phi$. What is the difference between the matrix $\\Phi$ in assumption 3 and $\\Phi$ in assumption 1 and 2?\n\n6. As stated in remark 1, the results in theorem 1 and 2 hold with high probability. So theorem 1 and 2 actually state that the trajectories numbers are independent of $\\delta$?\n\n7. What is the y-axis in figure 1 and figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673521646,
            "cdate": 1698673521646,
            "tmdate": 1699967200206,
            "mdate": 1699967200206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jKKTW40y7Y",
                "forum": "DCUG6P9RkZ",
                "replyto": "2AW7WOwb6O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time reviewing our paper. Reading your summary, we feel like you are missing the following important contribution. Indeed, we do not only improve the $\\epsilon$ dependence from $\\mathcal{O}(\\epsilon^{-5})$ to $\\mathcal{O}(\\epsilon^{-4})$ but we also remove **the need for the persistent excitation assumption**.\n\nThis is probably an even stronger contribution than improving the $\\epsilon$ dependence. Think for example to the special case of tabular MDPs. In this case the features are indicators functions, i.e. $$\\phi_{s,a}(s',a') = \\begin{cases} 1 \\quad \\text{if} \\quad s,a = s',a' \\newline 0 \\quad \\text{othertwise} \\end{cases}$$.\n\nThen, we have that the persistent excitation asumption would require \n\n$$\\lambda_{\\min} (\\mathbb{E}_{s,a \\sim d^{\\pi^k}} \\phi(s,a)\\phi(s,a)^T ) \\geq \\beta > 0$$\n\nObserving that in the tabular case $\\phi(s,a)\\phi(s,a)^T$ is a diagonal matrix which equals zero everywhere but in the $(s,a)^{\\mathrm{th}}$ diagonal element, we conclude that $\\mathbb{E}_{s,a \\sim d^{\\pi^k}} [\\phi(s,a)\\phi(s,a)^T]$ is also a diagonal matrix where the diagonal elements correspond to the entry of $d^{\\pi^k}$.\n\nTherefore, the eigenvalues of the matrix $\\mathbb{E}_{s,a \\sim d^{\\pi^k}}[\\phi(s,a)\\phi(s,a)^T]$ \n\nare the entries of $d^{\\pi^k}$ and we can conclude that the persistent excitation assumption amounts to ask that $\\min_{s,a} d^{\\pi^k}(s,a) > 0$. However, this can be easily contradicted by greedy policies for which at each state there exists only one action such that the above condition holds.\n\n**Cost function is utilised in MDP setting, but the numerical experiments adopt reward function setting.**\n\nThanks, we will soon update a revision which will make clear that in the experiment section's plot we consider the cumulative reward which equals the opposite of the cumulative cost. Sorry if this created confusion. We made this choice because using costs in the regret analysis is common in the adversarial MDP literature while comparing algorithms with the cumulative rewards is common in imitation learning papers.\n\n**the dependence of dimension $d$ increases from $d^2$ to $d^3$, so one natural question is that how to carefully select these parameters so that the proposed algorithm indeed requires less trajectories than the latest algorithm in Viano's paper in 2022.**\n\nIn fact, we can prove that $\\frac{1}{\\beta} \\geq d$. Using this result, we obtain that the dimension dependence in the bound in Viano et al. 2022 is in the best case $d^8$. Therefore, our new algorithm improves the dimension dependence as well.\n\nWe show now how to prove that $\\frac{1}{\\beta} \\geq d$\n\n$$\n\\beta \\leq \\lambda_{\\min}(\\mathbb{E}_{s,a \\sim d^{\\pi^k}} \\phi(s,a)\\phi(s,a)^T )\n$$\n\n$$\n\\leq \\frac{1}{d}\\mathrm{Trace} (\\mathbb{E}_{s,a \\sim d^{\\pi^k}}\\phi(s,a)\\phi(s,a)^T)\n$$\n\n$$\n= \\frac{1}{d} \\mathbb{E}_{s,a \\sim d^{\\pi^k}}\\mathrm{Trace} (\\phi(s,a)\\phi(s,a)^T)\n$$\n\n$$\n= \\frac{1}{d} \\mathbb{E}_{s,a \\sim d^{\\pi^k}}\\mathrm{Trace}(\\phi(s,a)^T\\phi(s,a))\n$$\n\n$$\n= \\frac{1}{d} \\mathbb{E}_{s,a \\sim d^{\\pi^k}} \\|\\phi(s,a)\\|_2^2\n$$\n\n$$\n\\leq \\frac{1}{d} \\mathbb{E}_{s,a \\sim d^{\\pi^k}} \\|\\phi(s,a)\\|_1^2\n$$\n\n$$\n\\leq \\frac{1}{d} \\max_{s,a} \\|\\phi(s,a)\\|_1^2\n$$\n\n$$\n= \\frac{1}{d} \\|\\phi(s,a)\\|_{1,\\infty}^2 \\leq \\frac{1}{d}\n$$\n\nwhere the last step follows from $\\|\\phi(s,a)\\|_{1,\\infty} \\leq 1$ as assumed in Assumptions 1 and 2.\n\n**The norm inequalities in assumptions 1-2 seem very technical, and it would be better if the authors could provide some insights about them.**\n\nAssuming bounded features is common in the Linear MDP setting see for example the paper by Jin et al., 2019.\n\nThe intuition is that since the quantities that we are assuming to lie in the column span of the matrix $\\Phi$ are bounded (the cost and the transition probabilities are bounded by one). Therefore, it would not make sense to use potentially unbounded features to represent the cost and the transition dynamics as an inner product.\n\n**The mathematical formulation for state value function in finite time horizon is pretty strange. I suppose the summation should take from 1 to h?**\n\nThere is a small typo, the sum should be from $h$ to $H$ as standard. See for example equation 4.2.1 in Puterman's book.\n\n**The infinite horizon trajectories, according to the description in section 2, have random length sampled from the geometric distribution. Why geometric distribution is adopted here?**\n\nThe geometric distribution is the standard choice in Discounted Infinite Horizon MDP (see Chapter 6 in Puterman's book)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699898084066,
                "cdate": 1699898084066,
                "tmdate": 1699898084066,
                "mdate": 1699898084066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zTWwYxdauj",
                "forum": "DCUG6P9RkZ",
                "replyto": "2AW7WOwb6O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second part of the Answer"
                    },
                    "comment": {
                        "value": "**The sampled number is still finite, so the cost in the time horizons greater than the sampled number is set to zero?**\n\nNotice that in defining the value function $V^{\\pi}(s; c) \\triangleq \\mathbb{E} [\\sum^{\\infty}_{h=1} \\gamma^{h-1} c(s_h,a_h) | s_1 = s] $ there is no sampling involved and since the geometric distribution is unbounded we can not set to zero some of the costs.\n\nHowever, in the algorithms we use finite values as horizon sampled from the geometric distribution and we do not need to observe the costs for stages larger than the sampled value.\nWe clarify this in Protocol 1 in Appendix B.\n\n**It seems that the proposed algorithms have never updated matrix $M$, in assumption 1 and 2. Does this mean that the true transition kernel is not estimated or involved in the algorithms?**\n\nInteresting question! Indeed, the matrix $M$ is never estimated by the Algorithm. The problem in doing this is that the matrix $M$ has dimensions $d \\times S $. Therefore, estimating it would unavoidably lead to an algorithm which requires memory linear in $S$. To avoid such an issue our algorithm only tries to estimate the quantity $M V^k$ where $V^k$ are the value functions sequence produced by the Algorithm.\n\nThe advantage of this approach is that $M V^k$ is just a $d$ dimensional vector so our algorithm enjoys memory and running time which is independent of $S$.\n\n**The matrix  \u03a6 is already known, according to assumption 1 or 2. But assumption 3 claims that the learner has access to \u03a6. What is the difference between the matrix \u03a6 in assumption 3 and \u03a6 in assumption 1 and 2?**\n\nThe difference is in which cost functions we require to be linear in the features. In Assumption 1 and 2 we require the sequence of changing costs to be linear in the features. Assumption 3, which is needed in the imitation learning setting, we additionaly assume that the true unknown cost function is linear in the features.\n\n\n**As stated in remark 1, the results in theorem 1 and 2 hold with high probability. So theorem 1 and 2 actually state that the trajectories numbers are independent of \u03b4 ?**\n\nThe dependence on $\\delta$ does not appear in Theorem 1 and 2 because we are stating the result in expectation. As highlighted in Remark 1, we prove this result passing first via a high probability result (Theorems 5 and 6) and then as shown in Lemma 6 we set $\\delta = \\epsilon$ to obtain the result given in Theorem 1 and 2.\n\nCheck Theorem 5 and 6 to appreciate how the number of required trajectory depend on $\\delta$.\n\n**What is the y-axis in figure 1 and figure 2?**\n\nIt is the normalized cumulative return. We normalize it in a way that the expert performance averaged over $10$ seeds equals 1 and the one of the uniform policy equals 0.\n\n**In algorithm 3, line 6, the proposed algorithm project $w^{k+1}$ to the unit ball. How to ensure that the projected $w$ still constitute an adversarial costs in [0,1], as assumed in the MDP setting in section 2? Similar question happens to algorithm 4, line 7.**\n\nThis is ensured by Assumptions 1 and 2 respectively. In particular, we assume that each adversarial cost $c^k$ can be written as linear combination of the column span of $\\Phi$ with weights $w^k$ such that $||w^k||_2 \\leq 1$. In the imitation learning contest, projecting in the unit ball ensures this last bound on the weights. \n\n\n\n\n**Thanks again for the interesting questions. We hope that our answer will help you to appreciate our contribution better and improve your assessment of our submission. Furthermore, we remain open to discussion.**\n\nBest,\n\nAuthors."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699898112983,
                "cdate": 1699898112983,
                "tmdate": 1699951346399,
                "mdate": 1699951346399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WvEMNRNEIV",
                "forum": "DCUG6P9RkZ",
                "replyto": "zTWwYxdauj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed explanations and pointing out that your contribution in relaxing exploratory assumptions, due to which I improved the rating for contribution and the final rating of the paper.\n\nI am still confusing about your answer to the geometric distribution in setting random length, and I did not find it in Chapter 6 in Puterman's book, would you mind providing a more specific place (like, in which section or other materials)?\n\nIn addition, I am also curious about the projection of $w^{k+1}$ (my third question in the comments), could you please provide the explanations for this?\n\n\nBest,\nReviewer"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699967692781,
                "cdate": 1699967692781,
                "tmdate": 1699967692781,
                "mdate": 1699967692781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1c3GxPOfhz",
                "forum": "DCUG6P9RkZ",
                "replyto": "NI0P9ISBxJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Reviewer_jGsw"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your answer about Puterman\u2019s book and the illustration of projection step. \n\nI will keep the current score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683561691,
                "cdate": 1700683561691,
                "tmdate": 1700683561691,
                "mdate": 1700683561691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zhyPmDwY0Y",
            "forum": "DCUG6P9RkZ",
            "replyto": "DCUG6P9RkZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission673/Reviewer_Rc5N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission673/Reviewer_Rc5N"
            ],
            "content": {
                "summary": {
                    "value": "The contribution of the paper is to provide a more sample efficient algorithm for both discounted IL in terms of the number of samples of environment interaction and expert trajectories required for the linear MDP setting. The proof involves exploiting a connection between imitation learning and online learning in MDPs with adversarial losses in the full information setting, pointed out by Viano et al (2022). The authors provide experimental evidence on a simple gridworld environment showing the utility of their approach where the algorithm always achieves near-expert policy and often surpasses it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The key idea is to online-to-batch to convert the IL problem to a regret minimization problem, and use a regret decomposition developed in Viano et al (2022) which decomposes the regret into 3 parts: a regret of matching the occupancy measure of the expert under a linear cost c_k, a regret term from approximating the true linear function, $w_{\\text{true}}$ capturing the reward function measured against the estimation error of the expert\u2019s occupancy measure, and the last term being the estimation error of the expert\u2019s occupancy measure. The first two terms in the regret decomposition are for learning a policy which performs well on the estimate of the expert\u2019s occupancy measure on the feature space.\n\nAn empirical estimate controls the 3rd error term, the second regret term can be controlled by OGD or any other online learning algorithm, while the authors provide an improved analysis for the regret of the first term, which cannot easily be solved by a generic no-regret algorithm because of the unknown transition dynamics, which make it impossible to project onto the space of valid occupancy measures. The authors propose a no regret algorithm in two steps: Policy evaluation is done using a fresh batch of data collected on-policy to get an optimistic estimate of the Q function; policy updates are not done greedily, but are done using the average optimistic $Q$ value computed from a batch of episodes to carry out infrequent policy updates. The approach resembles an MWE update with a finite-buffer to eliminate old and inaccurate $Q$ estimates. In a sense, the approach is similar to variance reduction in stochastic optimization.\n\nThe novelty in the analysis in the paper is in showing an improved algorithm for linear MDPs with adversarial costs in the full information setting. The authors show a regret bound which scales as $\\tilde{O} (d^{3/4} H^{3/2} K^{3/4})$ which improves over the previous best result of $\\tilde{O} (d^{3/4} H^2 K^{3/4})$."
                },
                "weaknesses": {
                    "value": "The paper improves the prior state of the art in the best known sample complexity for IL in the discounted and finite horizon settings, and the technical novelty is moderate. The analysis is largely to improve the best  known results for linear MDPs with adversarial costs in the full information setting. This is novel and may be of independent interest, but largely borrows insights from previous work, Viano et al (2022), the analysis of UCB in Jin et al and analysis of no-regret algorithms (MWE). I think it's still a nice contribution, but feels somewhat like an A+B(+C) type result.\n\nOverall, the writing of the paper is ok, there is room for improvement in terms of the presentation. The related work section can be organized in a much better way. This is important to put into context the results in the paper. There are several lines of work related to this one, and so it's all the more important to structure the related section in a better way.\n\nThe experimental eval in the paper is very limited, and what seem to be on a very simple environment. While the theory in the paper is the major contribution, it would have been helpful to see a more comprehensive evaluation. I am not reducing my score for the paper because of this point, but if the paper gets rejected, I encourage the author to run more comprehensive experiments. Typically on harder environments, it is quite difficult to achieve the expert's performance, but this is not the case in any of the experiments in the paper.\n\nThe results of Rajaraman et al (2021) in the offline setting do not require linear reward or a uniform occupancy measure. These assumptions seem to be used in the online setting to get improved bounds. In the online setting, the work Swamy et al (2022) provides a general analysis of the estimator used in Lemma 10 of the paper to go beyond the uniform feature measure assumption. While in general these two lines of work are not comparable, since the current paper assumes a model where the expert is arbitrary but the optimal policy falls in a linear class, as opposed to the linear expert setting (where the expert is a linear classifier), it would be interesting to see in a future work if there is a better connection between these settings.\n\nSwamy et al (2022): https://proceedings.neurips.cc/paper_files/paper/2022/file/2e809adc337594e0fee330a64acbb982-Paper-Conference.pdf\n\nMinor:\n1. \"Therefore, the policy suboptimality scale as $H^4 \\log |\\Pi| / \\epsilon^2$\". Isn\u2019t the policy suboptimality precisely $\\epsilon$?"
                },
                "questions": {
                    "value": "1. The standard BC reduction, for the finite horizon setting argues that BC achieves a $O(H^2)$ suboptimality scaling in finite-horizon settings. This is in contrast to the discussion in the discounted setting on page 1 for BC. I am not aware of a work or analysis which states that BC requires $\\widetilde{O} (1/(1-\\gamma)^4)$ demonstrations in the discounted setting. It would be helpful to cite a paper here.\n\n2. Do the results in this setting hold beyond linear MDPs, say for bilinear classes or Bellman rank bounded MDPs? It would have been nice to include a discussion about this point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission673/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698879833448,
            "cdate": 1698879833448,
            "tmdate": 1699635994402,
            "mdate": 1699635994402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BSx59PnL2b",
                "forum": "DCUG6P9RkZ",
                "replyto": "zhyPmDwY0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nthanks a lot for the time spent reviewing our work !\n\nWe would like to start the rebuttal with 2 clarification. First, we would like to emphasize the fact that the improvement is not only on the dependence on the dimension or the accuracy $\\epsilon$ but it is on avoiding the limiting persistent excitation assumption.\nThis is very important as we explain in our common answer. \n\nSecond, the connection to adversarial MDP was not pointed out in Viano et al. 2022. It appeared in (Shani et al., 2021) for the finite horizon tabular setting and then it went somehow forgotten in the imitation learning community. In this work, we adapt it to finite and infinite horizon Linear MDP. New insights are required when compared to (Shani et al., 2021) to prove the improved bound in the finite horizon setting.\n\n\n**The paper improves the prior state of the art in the best known sample complexity for IL in the discounted and finite horizon settings, and the technical novelty is moderate. The analysis is largely to improve the best known results for linear MDPs with adversarial costs in the full information setting. This is novel and may be of independent interest, but largely borrows insights from previous work, Viano et al (2022), the analysis of UCB in Jin et al and analysis of no-regret algorithms (MWE). I think it's still a nice contribution, but feels somewhat like an A+B(+C) type result.**\n\nPlease also appreciate the fact that there are some key differences when compared with the ideas used in the UCB analysis of Jin et al. . In particular, their analysis crucial relies on the fact that they can prove that for every $k$, and for every state action pair $s,a$ and for every stage $h$, it holds that $Q_h^k(s,a) > Q_h^\\star(s,a)$ (upper bound because in Jin's paper they consider rewards instaed of costs) with high probability. In the infinite horizon, unfortunately we can not establish this property so we have to exploit a different form of optimism which is that the $Q$ values are lower bounds to the ideal value iterations updates. That is, $Q^k(s,a) \\leq c^k(s,a) + \\gamma P V^k(s,a)$ for every state action pair $s,a$ and round $k$ with high probability.\n\n**Overall, the writing of the paper is ok, there is room for improvement in terms of the presentation. The related work section can be organized in a much better way. This is important to put into context the results in the paper. There are several lines of work related to this one, and so it's all the more important to structure the related section in a better way.**\n\nWe are happy to work to improve the writing further! Which lines of work do you feel we are missing ?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700007045721,
                "cdate": 1700007045721,
                "tmdate": 1700007045721,
                "mdate": 1700007045721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9LPNsvYWjm",
                "forum": "DCUG6P9RkZ",
                "replyto": "zhyPmDwY0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission673/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The experimental eval in the paper is very limited, and what seem to be on a very simple environment. While the theory in the paper is the major contribution, it would have been helpful to see a more comprehensive evaluation. I am not reducing my score for the paper because of this point, but if the paper gets rejected, I encourage the author to run more comprehensive experiments. Typically on harder environments, it is quite difficult to achieve the expert's performance, but this is not the case in any of the experiments in the paper.**\n\nThanks for the suggestion. We think that the take away of this theoretical work is that exploration in the policy optimization phase can be beneficial for imitation learning.\n\nA natural follow up that we are planning is to to try some exploration heuristics for neural networks and insert them in the policy update of common deep imitation learning algorithms like GAIL, IQLearn and see if this would empirically enhanance their performance.\n\n**The results of Rajaraman et al (2021) in the offline setting do not require linear reward or a uniform occupancy measure. These assumptions seem to be used in the online setting to get improved bounds. In the online setting, the work Swamy et al (2022) provides a general analysis of the estimator used in Lemma 10 of the paper to go beyond the uniform feature measure assumption. While in general these two lines of work are not comparable, since the current paper assumes a model where the expert is arbitrary but the optimal policy falls in a linear class, as opposed to the linear expert setting (where the expert is a linear classifier), it would be interesting to see in a future work if there is a better connection between these settings.**\n\nThis is another very interesting point ! We added a discussion about this in our revision. Please check page 14 of our revised manuscript.\n\n**The standard BC reduction, for the finite horizon setting argues that BC achieves a $O(H^2)$ suboptimality scaling in finite-horizon settings. This is in contrast to the discussion in the discounted setting on page 1 for BC. I am not aware of a work or analysis which states that BC requires $\\mathcal{O}((1 - \\gamma)^{-4})$ demonstrations in the discounted setting. It would be helpful to cite a paper here.**\n\nGood question. This is a potentially confusing point. The common bound for BC says that the policy suboptimality error for the policy output denoted as $\\pi_B$ is bounded as\n\n$$\nV^{\\pi_{B}} - V^{\\pi_E} \\leq (1 - \\gamma)^{-2} \\mathbb{E}_{s \\sim d^{\\pi_E}} \\sum_a | \\pi_E(a|s) - \\pi_B (a|s)|\n$$\n\nso here the scaling is $(1 - \\gamma)^{-2}$ as you mentioned. However, for general experts, the dependence become quartic when we ask how many expert trajectories are necessary to achieve $V^{\\pi_{B}} - V^{\\pi_E} \\leq \\epsilon$.\nIn this case, we can use that by Hoeffding we have that with high probability \n$$\n\\mathbb{E}_{s \\sim d^{\\pi_E}} \\sum_a | \\pi_E(a|s) - \\pi_B(a|s) |  \\leq \\sqrt{\\frac{\\log (\\Pi/\\delta)}{N_E}}\n$$ \nwhere $N_E$ is the number of expert trajectories.\nTherefore \n\n$$\nV^{\\pi_{B}} - V^{\\pi_E} \\leq (1 - \\gamma)^{-2} \\sqrt{\\frac{\\log (\\Pi/\\delta)}{N_E}}\n$$\n\nSo, finally to have that the right hand side equal to $\\epsilon$ we need $N_E \\geq \\frac{\\log (\\Pi/\\delta)}{(1 - \\gamma)^4\\epsilon^2}$.\nIn this last bound the horizon dependence shows up with quartic dependence. The same result has been proven in (Li \\& Zhang, 2022).\n\n\n**Do the results in this setting hold beyond linear MDPs, say for bilinear classes or Bellman rank bounded MDPs? It would have been nice to include a discussion about this point.**\n\nYes, we think that we can extended the result to finite horizon bilinear classes ! However, the resulting algorithm can not be ensured to be computationally efficient. We added an informal discussion about this extension at page 14,15 and 16 of our revised draft.\n\nThe main focus of our paper is to consider the smaller class of linear MDP but give an efficient algorithm for it.\n\n**Therefore, the policy suboptimality scale as ...**\n\nThis is a typo, we meant that the required number of expert demonstrations scale as $\\frac{H^4 \\log \\Pi}{\\epsilon^2}$. We corrected this in our revision. Thanks for catching it !"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission673/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700007520636,
                "cdate": 1700007520636,
                "tmdate": 1700007564068,
                "mdate": 1700007564068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]