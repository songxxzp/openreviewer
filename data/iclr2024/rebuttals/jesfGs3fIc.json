[
    {
        "title": "Set-based Neural Network Encoding"
    },
    {
        "review": {
            "id": "xjWIulKpAu",
            "forum": "jesfGs3fIc",
            "replyto": "jesfGs3fIc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_Yfht"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_Yfht"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an approach to take neural network parameters as input. They treat the parameters as a set and process it with a variant of transfomers. In order to disambiguate weights of different layers with appropriate position encodings based on the layer type and layer number. Chunking of weights is used in order to make the method practical in terms of execution speed."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- SNE is a generic technique that is mostly (see second weakness) architecture-agnostic. This is a notable difference from recent work and as such a valuable contribution. This makes the method more widely applicable than prior work.\n- Results on the newly proposed datasets appear decent, good experimental evaluation on the proposed datasets."
                },
                "weaknesses": {
                    "value": "- The method is not \"minimally equivariant\" (see [1] for the definition). In essence, there exist permutations of the weights which *do* change the function, but are considered equivalent by SNE. Consider the flattened weights [1 2 3 4 5 6] with a chunk size of 3. The weights [4 5 6 1 2 3] would be considered exactly the same, since the set of chunks {[1, 2, 3], [4, 5, 6]} are the same. As such, the proposed SNE is not capturing the correct equivariance of the task: it is unable to distinguish between models that are clearly different. This is the primary reason why I'm giving a 1 in terms of soundness. Given the lack of discussion and evaluation of this issue in the paper, I cannot recommend acceptance at this point.\n- The authors do not consider the case of architectures with branches and how the position encoding needs to be adapted for them\n- There is a lack of comparison against existing methods on established datasets. For example, there are no experiments on implicit neural representations, which DWSNet [2] and NFN [Zhou et al., 2013] are evaluated on extensively. There is no reference to DWSNet in the text.\n\n[1] Neural Functional Transformers. Allan Zhou, Kaien Yang, Yiding Jiang, Kaylee Burns, Winnie Xu, Samuel Sokota, J. Zico Kolter, Chelsea Finn https://arxiv.org/abs/2305.13546\n\n[2] Equivariant Architectures for Learning in Deep Weight Spaces. Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, Haggai Maron https://arxiv.org/abs/2301.12780"
                },
                "questions": {
                    "value": "Can you state the specific equivariance that SNE has in more detail and the relationship as to how accurately it models the symmetry group of parameters of neural networks?\nHow does the position encoding work when the model has branches, and as such, there is no total order on the layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698086903378,
            "cdate": 1698086903378,
            "tmdate": 1699636426225,
            "mdate": 1699636426225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TjbLs8leHU",
                "forum": "jesfGs3fIc",
                "replyto": "xjWIulKpAu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the constructive feedback. We respond to the raised questions below.\n\n\n***There is a lack of comparison against existing methods on established datasets. For example, there are no experiments on implicit neural representations, which DWSNet and NFN are evaluated on extensively. There is no reference to DWSNet in the text.***\n\n\nFirstly, we reference DWSNet in the text in Section 2(Neural Network Performance Prediction from Weights). With regards to experiments on INRs, we provide the following additional experimental results and benchmark against both DWSNet and the newly introduced NFN models.\n\n\nExperimental setting: We utilize a modelzoo consisting of INRs fit to sine waves on [-$\\pi$, $\\pi$] and with frequencies sampled from U(0.5, 10). The goal is to predict the frequency of a given INR. All methods encode the INR to a 32 dimensional vector which is then fed into a classifier with 2 linear layers with hidden dimension of 512. All methods are trained for 100 epochs. We report the mean squared error and parameter counts for all models in the table below. We are unable to obtain a converged model for [2] hence we compare with [4] which is the second iteration of the method proposed in [2]. As can be seen,  SNE performs better than the baselines and is parameter efficient compared to [3] and [4]. We note that increasing parameter counts for the MLP and [1] baselines results in overfitting and poor results.\n\n\n\n\n\n\n| Method | Parameter Count | MSE |\n|---|---|---|\n| MLP | 14K | 1.9167$\\pm$0.2407 |\n| STATNET[1] | 44K | 0.9373$\\pm$0.2761 |\n| NFT[4] | 6M | 0.4010$\\pm$0.1085 |\n| DWSNet[3] | 1.5M | 0.2086$\\pm$0.0263 |\n| SNE | 358K | $\\textbf{0.0977}$ $\\pm$0.0024 |\n\n\n\n\n\n\n***The authors do not consider the case of architectures with branches and how the position encoding needs to be adapted for them***\n\n\nOur discussion ignores architectures with branches, such as ResNets, because we are currently unaware of any standard modelzoo of such architectures on which to benchmark on. However, we provide an outline, in the specific case of residual blocks, of how our method can be adapted to encode such networks.\n\n\nGiven that residual blocks are composed of convolutional and linear layers, each of these can be encoded independently as we already do. To account for residual connections, we propose to introduce a special token (just as was done for layer types) when we encode the entire block. While we do not see any impediment to applying our method to such networks, the unavailability of such modelzoos makes it impossible to test this out hence we leave it as future work when such modelzoos become publicly available."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176922178,
                "cdate": 1700176922178,
                "tmdate": 1700176922178,
                "mdate": 1700176922178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oKM2szt720",
            "forum": "jesfGs3fIc",
            "replyto": "jesfGs3fIc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_8ZLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_8ZLb"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on solving the problem of predicting the model accuracy performance given only access to the model parameters. The authors proposed a new method to encode the neural network parameters with set-to-set and set-to-vector functions. The proposed method encodes the model layer types and the layer positional information to retain the model order properties. Experiments show that the proposed SNE not only outperforms existing baselines in terms of the model accuracy prediction correlation, but also has good generalization ability cross different datasets and architecture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper is easy to follow.\n- The experiments are convincing and the results are good."
                },
                "weaknesses": {
                    "value": "- The method part is a little bit hard to follow because of so many notations involved.    \nPlease refer to the questions part for more details. Thank you."
                },
                "questions": {
                    "value": "- In Section 3.2, when applying flattening, padding, and chunking operations on a convolutional layer, they are applied on the kernel dimension. I'm wondering why is the case? I have the question because really we have $3\\times3$ kernel size for Convs. Do we really need to chunk it anymore?\n- Why Equation 2 will convert a vector in size of $c\\times1$ to $c\\times h$? As mentioned in Section 3.6, $\\Phi_{\\theta_{1}}$ is SAB(SAB(X)), while SAB(X) is MAB(X,X) in Equation 10. It seems that MAB(X,X) should have the same size as X. How can we get from  $c\\times1$ to $c\\times h$? You may want to add the dimensions for Equations 10 to 13 for clarification.\n- To my understanding, the encoding process from Section 3.3 to 3.5 is like a nested process. Is it possible to provide a pseudocode for it?\n- It seems that the \"specific dataset $d$\" in Section 3.1 is never used. You may want to get rid of it.\n- About the experiments, could you have some results on popular CNN architectures like ResNet? Residual links are nowadays important part of the CNN models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4500/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4500/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4500/Reviewer_8ZLb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698113818678,
            "cdate": 1698113818678,
            "tmdate": 1699636426127,
            "mdate": 1699636426127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wn6jm4FSBq",
                "forum": "jesfGs3fIc",
                "replyto": "oKM2szt720",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for taking the time to provide feedback for improving the paper. We respond to the questions raised below.\n\n\n***In Section 3.2, when applying flattening, padding, and chunking operations on a convolutional layer, they are applied on the kernel dimension. I'm wondering why is the case? I have the question because really we have $3 \\times 3$ kernel size for Convs. Do we really need to chunk it anymore?***\n\n\nFor convolutional kernels, they are small enough such that chunking is not necessary. We have made the presentation general since the set-to-set and set-to-vector functions are shared across all layer types(linear and convolutions). The chunking operation becomes more important for linear layers to avoid processing large tensors. We will clarify this more in the experiment sections.\n\n\n***Why Equation 2 will convert a vector in size of $c \\times 1$  to $c \\times h$? As mentioned in Section 3.6 $\\Phi_{\\theta_{1}}$ is SAB(SAB(x)), while SAB(X) is MAB(X,X) in Equation 10. It seems that MAB(X,X) should have the same size as X. How can we get from $c \\times 1$ to $c \\times h$. You may want to add the dimensions of Equation 10 to 13 for clarification.***\n\n\nSAB(X) correctly has the same size of X. However we introduce a projection MLP in the SAB block that projects its inputs to $h$. We will clarify this explicitly in the choice of set-to-set and set-to-vector section and add dimensions to Equations 10 and 13 as suggested.\n\n\n***To my understanding, the encoding process from Section 3.3 to 3.5 is like a nested process. Is it possible to provide pseudocode for it?***\n\n\nYes Sections 3.3-5 involve a nested process. We will include pseudocode as suggested and also provide a reference implementation for easy reproducibility and readability.\n\n\n***About the experiments, could you have some results on popular CNN architectures like ResNet? Residual links are nowadays important part of the CNN models.***\n\n\nWhile we do not see any impediment to applying our method to networks where residual connections are important, we are currently unaware of any publicly available modelzoos with the appropriate meta-data to benchmark on. An important encoding step in such models will be incorporating the information of residual connections in the encoding pipeline. For this, we propose to introduce a special token(just as was done for layer types) when encoding the entire residual block. We agree with the Reviewer that adopting the proposed method to such architectures is important and we leave it as future work when such modelzoos become publicly available."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176707859,
                "cdate": 1700176707859,
                "tmdate": 1700176707859,
                "mdate": 1700176707859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uQjvhQbsJ7",
            "forum": "jesfGs3fIc",
            "replyto": "jesfGs3fIc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_dyJx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_dyJx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method of encoding NN architectures by using the trained parameters to predict the validation accuracy of a NN. The proposed set-based encoding improves upon prior work that investigated this problem when evaluated on the same set of benchmarks that include 30k different hyperparameter variations of training small NNs on MNIST-level tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Interesting methodology for the set-based encodings. More general than prior approaches."
                },
                "weaknesses": {
                    "value": "Very marginal empirical improvements. Also, I find the motivation for predicting post-training network performance quite lacking. Why is the evaluation on a small validation set insufficient in this case? How does this method compare to things like zero-cost-proxies which try to predict network performance _before_ training?\n\nAdditionally, the baseline accuracy for some of these NNs is very low (0.45 on MNIST?)\n\nMinor: SNE is an overloaded acronym in the paper."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690645670,
            "cdate": 1698690645670,
            "tmdate": 1699636425979,
            "mdate": 1699636425979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gF5RsikXWU",
                "forum": "jesfGs3fIc",
                "replyto": "uQjvhQbsJ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the constructive feedback. We respond to the questions raised below.\n\n\n***Baseline accuracy for some of these NNs is very low (0.45 on MNIST?). Very marginal empirical improvements.***\n\n\nThe evaluation metric in Table 2 is **Kendall\u2019s Correlation Coefficient** (as stated in section 4.2. We will explicitly add this to the caption as in Table 1 to avoid further confusion) and not accuracy on the datasets. In table 2, we demonstrate the generality of the proposed method in the cross-architecture task and note that none of the baselines in Table 1 are applicable since the methods are developed for fixed architectures. \n\n\nAcross the tasks that we consider, our method performs better than the baselines as shown in tables 1 and 2. But more importantly, the flexibility of the model we propose allows far more capability such as cross-architectural transfer, a task which none of the relevant baselines[1-4] are capable of. Additionally, we provide additional experimental results on implicit  neural representations (INR) below as suggested by Reviewer Yfht.\n\n\nExperimental setting: We utilize a modelzoo consisting of INRs fit to sine waves on [-$\\pi$, $\\pi$] and with frequencies sampled from U(0.5, 10). The goal is to predict the frequency of a given INR. All methods encode the INR to a 32 dimensional vector which is then fed into a classifier with 2 linear layers with hidden dimension of 512. All methods are trained for 100 epochs. We report the mean squared error and parameter counts for all models in the table below. We are unable to obtain a converged model for [2] hence we compare with [4] which is the second iteration of the method proposed in [2]. As can be seen,  SNE performs better than the baselines and is parameter efficient compared to [3] and [4]. We note that increasing parameter counts for the MLP and [1] baselines results in overfitting and poor results.\n\n\n\n\n| Method | Parameter Count | MSE |\n|---|---|---|\n| MLP | 14K | 1.9167$\\pm$0.2407 |\n| STATNET[1] | 44K | 0.9373$\\pm$0.2761 |\n| NFT[4] | 6M | 0.4010$\\pm$0.1085 |\n| DWSNet[3] | 1.5M | 0.2086$\\pm$0.0263 |\n| SNE | 358K | $\\textbf{0.0977}$ $\\pm$ 0.0024 |\n\n\n\n\n***Why is the evaluation on a small validation set insufficient in this case? Also, I find the motivation for predicting post-training network performance quite lacking.***\n\n\nWhile in this work, and those of [1,2,3,4], we have focused on network performance prediction, the methods developed are not limited to this task only. For instance, there are properties of networks[1], such as the type of optimizer, the learning rate, the number of epochs the model was trained for, etc. which cannot be obtained by having access to the training/test/validation set. Such meta-data of pretrained networks may prove necessary in model selection for fine tuning and hence require a means for predicting them. \n\n\n\n\n***How does this method compare to things like zero-cost-proxies which try to predict network performance before training?***\n\n\nThe formulation of the neural network weight encoding problem([1,2,3,4]) assumes access only to pretrained weights making the domain different from zero-cost-proxies used in NAS that predict performance using architectural information. \n\n\n\n\n**References**\n\n\n[1] Unterthiner, Thomas, et al. \"Predicting neural network accuracy from weights.\" arXiv preprint arXiv:2002.11448 (2020).\n\n\n[2] Navon, A., Shamsian, A., Achituve, I., Fetaya, E., Chechik, G., & Maron, H. (2023). Equivariant architectures for learning in deep weight spaces. arXiv preprint arXiv:2301.12780.\n\n\n[3] Zhou, Allan, et al. \"Permutation equivariant neural functionals.\" arXiv preprint arXiv:2302.14040 (2023).\n\n\n[4] Zhou, Allan, et al. \"Neural Functional Transformers.\" arXiv preprint arXiv:2305.13546 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176445727,
                "cdate": 1700176445727,
                "tmdate": 1700177143693,
                "mdate": 1700177143693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XFUjMO3Kpa",
            "forum": "jesfGs3fIc",
            "replyto": "jesfGs3fIc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_Ab9j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4500/Reviewer_Ab9j"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a set-based neural network encoder for predicting the generalization performance given access only to the parameter values. They also introduce two novel tasks for neural network generalization performance prediction: cross-dataset and cross-architecture."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-- Compared with previous methods, the method is applicable to arbitrary architecture benefit from the set-based encoder which also takes hierarchical computational structure of the neural networks into account.\n\n-- To tackle with the large compute memory requirement, this paper proposed a pad-chunk-encode pipeline to encode the neural network layers efficiently.\n\n-- Two new tasks were introduced for evaluating the prediction of the generalization performance."
                },
                "weaknesses": {
                    "value": "-- The motivation of the paper is not clear enough. For a large model, it will lead to large computation overhead, and for a small model you can just simply run the model on the test set. The necessity of the task should be well discussed.\n\n-- The applications of the proposed SNE is still limited. The architectures and datasets are very simple.\n\n-- Lack of ablation study of their methods, such as the effect of their hierarchical computational structure position encoding."
                },
                "questions": {
                    "value": "-- Are the methods applicable for the network with residual connections? In your settings, just given assess to the parameter values, the neural network with or without residual connection will get the same encoding and get the same prediction result. That is not intuitive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259757759,
            "cdate": 1699259757759,
            "tmdate": 1699636425911,
            "mdate": 1699636425911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6IOQCVPTgz",
                "forum": "jesfGs3fIc",
                "replyto": "XFUjMO3Kpa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for taking the time to offer constructive feedback for improving the paper. We respond to the questions raised below.\n\n\n***The motivation of the paper is not clear enough. For a large model, it will lead to large computation overhead, and for a small model you can just simply run the model on the test set. The necessity of the task should be well discussed.***\n\n\nWhile research on encoding neural network weights is relatively new [1,2,3,4], these methods have potential application in areas such as learnable optimizers[5,6], direct utilization of implicit neural representations[7], policy evaluation[8] in reinforcement learning, neural network editing[9,10] etc. However, to make progress in all these applications, encoding of network weights is the first step in this pipeline.\n\n\nWhile in this work, and those of [1,2,3,4], we have focused on network performance prediction, the methods developed are not limited to this task only. For instance, there are properties of networks[1], such as the type of optimizer, the learning rate, the number of epochs the model was trained for, etc. which cannot be obtained by having access to the training/test/validation set. Such meta-data of pretrained networks may prove necessary in model selection for fine tuning and hence require a means for predicting them.\nTo demonstrate the generality of the problem, we provide additional applications on implicit neural representations in our response to your second question.\n\n\n\n\n***The applications of the proposed SNE is still limited. The architectures and datasets are very simple.***\n\n\nSNE makes no assumptions on architectural choices. [1,2,3,4] all require that an architecture is known and fixed a priori. In section 4.2, we introduce the cross-architectural transfer task for neural network encoding to demonstrate the flexibility of SNE even at test time. The most relevant baselines [1,2,3,4] are all not applicable to this task given that they work only for a fixed architecture. Additionally, in Section 4.1, we have introduced the cross-dataset evaluation task, for the first time, for neural weight network encoding methods.\n\n\nSecondly, with regards to architectures and datasets, we evaluate our method on the standard benchmarks for evaluating neural network encoding methods[1,2,3,4]. In section 4.1, we have generated a new modelzoo of pretrained networks to be able to demonstrate cross-architecture encoding. Currently, we are unaware of any large scale modelzoos of architectures such as ResNets or Vision Transformers with the relevant meta-data that can be used for this task. However, we note that the architecture agnostic formulation of SNE ensures that, when such modelzoos become publicly available, the proposed method will still be applicable.\nFinally, we provide additional results on implicit neural network(INR) encoding to demonstrate that our method is applicable beyond the performance prediction task.\n\n\nExperimental setting: We utilize a modelzoo consisting of INRs fit to sine waves on [-$\\pi$, $\\pi$] and with frequencies sampled from U(0.5, 10). The goal is to predict the frequency of a given INR. All methods encode the INR to a 32 dimensional vector which is then fed into a classifier with 2 linear layers with hidden dimension of 512. All methods are trained for 100 epochs. We report the mean squared error and parameter counts for all models in the table below. We are unable to obtain a converged model for [2] hence we compare with [4] which is the second iteration of the method proposed in [2]. As can be seen,  SNE performs better than the baselines and is parameter efficient compared to [3] and [4]. We note that increasing parameter counts for the MLP and [1] baselines results in overfitting and poor results.\n\n\n\n\n| Method | Parameter Count | MSE |\n|---|---|---|\n| MLP | 14K | 1.9167$\\pm$0.2407 |\n| STATNET[1] | 44K | 0.9373$\\pm$0.2761 |\n| NFT[4] | 6M | 0.4010$\\pm$0.1085 |\n| DWSNet[3] | 1.5M | 0.2086$\\pm$0.0263 |\n| SNE | 358K |$\\textbf{ 0.0977}$ $\\pm$0.0024 |\n\n\n\n\n***Lack of ablation study of their methods, such as the effect of their hierarchical computational structure position encoding.***\n\n\nWe remove the hierarchical compositional structure and the positional encoding mechanism from SNE and test it on the INR task introduced above. From the table below, we can see that these structures are essential to the performance of the method.\n\n\n\n\n| Method | MSE |\n|---|---|\n| w/o positional and Hierarchical Encoding |  7.45167$\\pm$0.7985 |\n| SNE |$\\textbf{ 0.0977}$ $\\pm$0.0024 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176171783,
                "cdate": 1700176171783,
                "tmdate": 1700177120848,
                "mdate": 1700177120848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]