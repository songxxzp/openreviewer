[
    {
        "title": "LLM+A: Grounding Large Language Models in Physical World with Affordance Prompting"
    },
    {
        "review": {
            "id": "njZXX5fajH",
            "forum": "cbVnJa4l2o",
            "replyto": "cbVnJa4l2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_xvo1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_xvo1"
            ],
            "content": {
                "summary": {
                    "value": "(1) This paper studied language-conditioned robotic manipulation tasks using large models, proposing LLM+A framework that can decompose language instructions into sub-tasks.\n\n(2) Generated robot control sequences and extended to heterogeneous tasks and demonstrated potential of LLMs in planning and motion control simultaneously\n\n(3) Provided training-free paradigm for utilizing LLMs in robotic tasks, addressing dataset bottleneck and Affirmed the importance of affordance prompting for grounding sub-tasks in physical world. Experiments proved the effectiveness of LLM+A\nPlanned future optimizations for time efficiency and application to complex robotics tasks"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) This work integrate LLMs into robot planning and reasoning. It leverages recent advancements in LLMs and it utilizes LLMs as high-level sub-task planners and low-level motion controllers in robotic tasks.\n\n(2) Several challenges are mentioned and addressed, including identifying the pre-trained skills and sub-policies, and generalizing unseen envs and diverse scenes.\n\n(3) While introducing LLM + A framework, this work enhanced robotic manipulation tasks by grounding LLMs in the physical world, improving motion plans by considering affordance knowledge."
                },
                "weaknesses": {
                    "value": "(1) [Limited Generalization]: The experiment of this study depends on the simplified tasks, such as pushing cubes and put cubes. These objects are rather simple with regular shapes. This is crucial to discretize the actions. However,  the experiments may not cover all possible scenarios with more complex envs and irregular daily objects, leading to potential limitations in the model's applicability outside the tested conditions.\n\n(2) [Affordance Predictions Accuracy] The accuracy of affordance predictions from LLMs could be a potential weakness. If the affordance values are inaccurately predicted, it might lead to sub-optimal or erroneous robotic actions. Variability of landscapes, backgrounds, and language descriptions predicting affordances across different objects or environments could impact the overall performance.\n\nOverall, the experiments might lack certain real-world complexities, such as dynamic and unpredictable environments. For example, the Owl-vit vision grounding module only predicts the bounding box. Which is not ideal for most of the cases in real-world applications."
                },
                "questions": {
                    "value": "(1) What is the inference speed of the planning? Prompting GPT-4 takes some time to response compared with other simpler models e.g. from Ros integration. \n\n(2) Why not implement SAM based vision detection modules for more accurate detection and generalize the tasks into more complex scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I don't see any ethical concerns since it is the embodied robotic study."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698427158924,
            "cdate": 1698427158924,
            "tmdate": 1699637151019,
            "mdate": 1699637151019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "94hyXKgtm9",
                "forum": "cbVnJa4l2o",
                "replyto": "njZXX5fajH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xvo1 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer xvo1,\n\nWe greatly appreciate your insightful comments and the time you took to review our manuscript. Your feedback is essential to improving our work, and here we address your concerns:\n> 1. [Limited Generalization]: The experiment of this study depends on the simplified tasks, such as pushing cubes and put cubes. These objects are rather simple with regular shapes. This is crucial to discretize the actions. However, the experiments may not cover all possible scenarios with more complex envs and irregular daily objects, leading to potential limitations in the model's applicability outside the tested conditions.\n\nThank you for your valuable suggestions. We agree with you that the experiment should include more complex tasks to verify the generalization ability of our method. Accordingly, we have incorporated additional testing scenarios. First, we diversified the shape of objects, introducing cubes, pentagons, star-shaped, and moon-shaped objects. We conducted an evaluation of our method over 100 episodes in the Block2Position task, resulting in a success rate of 38%. Notably, this rate is nearly consistent with the success rate reported in our paper. An example is shown in Figure 5. Second, we applied our method in complex scenarios, involving potential interferences with object positions. More specifically, we randomly change the object position at any given time step within an episode. An example of environmental observation and robot trajectories is provided in Figure 6 for your reference. The results demonstrate the robustness of our method, notably its ability to dynamically recalibrate motion sequences.\n\n> 2. [Affordance Predictions Accuracy] The accuracy of affordance predictions from LLMs could be a potential weakness. If the affordance values are inaccurately predicted, it might lead to sub-optimal or erroneous robotic actions. Variability of landscapes, backgrounds, and language descriptions predicting affordances across different objects or environments could impact the overall performance.\n\nWe agree with your concern about the affordance prediction accuracy. Variability of landscapes, backgrounds, and language descriptions across different objects or environments could impact the over-performance. In our experiments, the task instruction has no pre-defined template, grammar, or vocabulary for each task. For instance, \u201cmove the red block to the top right of the board\u201d can also be described as \u201cpush the red block to the upper right corner\u201d. The experiment results show that LLM is robust to the variability of language descriptions. \n\nBesides, the robustness to the variability of landscapes and backgrounds is mainly dependent on the capability of detection models, recent visual-language models can achieve pleasant detection/segmentation results with respect to text inputs, and the accuracy of the perception module is not the focus of our work.\n\nFurthermore, even if the affordance prediction can make mistakes sometimes, as our method takes in the detection results as the planner\u2019s input which composes the closed-loop control, it can greatly alleviate the planning prediction error in one round by consistently adjusting its planning trajectory."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653334120,
                "cdate": 1700653334120,
                "tmdate": 1700653334120,
                "mdate": 1700653334120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vcqzE0Pooz",
            "forum": "cbVnJa4l2o",
            "replyto": "cbVnJa4l2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_qNCi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_qNCi"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a prompt framework to let LLM solve robotics tasks w/o any training. The main innovation is that it forces the LLM to output affordance that is a constraint to make sure the control is within the set of feasible actions to follow the task instruction. Experimental results show that it is better than code as policies and ReAct for the tasks included in this paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of constraining the LLM to output action according to affordance is interesting."
                },
                "weaknesses": {
                    "value": "1. The paper title claims physical world, however, in the evaluation, it only considers simulation tasks on table top (2D). Physical world robotics interaction is much more complicated than simulation and will absolutely break the assumption of this paper. In my point of view, the technique proposed in this paper only applies to a very limited setup. Basically, given some 2D points (target positions) how to use robot arm (source positions) to reach it and generate some trajectories. In contrary, techniques such as code as policies is general and can extend to physical world.\n\n2. The technique proposed by this paper may highly depend on the choice of LLM. Ablations w/ different LLMs is required to show generality.\n\n3. In SayCan paper, there is an \"open source environment\" section. Looks like the tasks are similar. It will be interesting to see the comparison w/ SayCan."
                },
                "questions": {
                    "value": "Is the policy able to re-try if the first trial fails?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Reviewer_qNCi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645430872,
            "cdate": 1698645430872,
            "tmdate": 1699637150917,
            "mdate": 1699637150917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cThDf2xyZL",
                "forum": "cbVnJa4l2o",
                "replyto": "vcqzE0Pooz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qNCi"
                    },
                    "comment": {
                        "value": "Dear Reviewer qNCi,\n\nWe greatly appreciate your insightful comments and the time you took to review our manuscript. Your feedback is essential to improving our work, and here we address your concerns:\n> 1. The paper title claims physical world, however, in the evaluation, it only considers simulation tasks on table top (2D). Physical world robotics interaction is much more complicated than simulation and will absolutely break the assumption of this paper. In my point of view, the technique proposed in this paper only applies to a very limited setup. Basically, given some 2D points (target positions) how to use robot arm (source positions) to reach it and generate some trajectories. In contrary, techniques such as code as policies is general and can extend to physical world.\n\nThank you for your comments. In [1], the simulated environment \u201cLanguage-Table\u201d consists of a simulated 6DoF robot xArm6 implemented in PyBullet equipped with a small cylindrical end effector. Third-person perspective RGB-only images from a simulated camera are used as visual input. Also, similar real-world experiments are tested using UFACTORY xArm6 robot arms. The authors have demonstrated that the policy performance in Language-Table was highly correlated with policy performance in the real world. In [2], the Pick&Place task is built based on the Ravens benchmark [3] set in PyBullet. Although the rendering software stack may not fully reflect the noise characteristics often found in real data, the simulated environment can roughly match the real-world setup. Despite the 2D environment, we can transfer our method into a 3D setup using the following pipeline: we first invoke open-vocab detector OWL-ViT [4] to obtain a bounding box, and feed it into Segment Anything [5] to obtain an object mask. Then the object point cloud can be reconstructed with the mask and the RGB-D observation.\n\n[1] Lynch, Corey, et al. \"Interactive language: Talking to robots in real time.\" IEEE Robotics and Automation Letters (2023).\n\n[2] Shridhar, Mohit, Lucas Manuelli, and Dieter Fox. \"Cliport: What and where pathways for robotic manipulation.\" Conference on Robot Learning. PMLR, 2022.\n\n[3] Zeng, Andy, et al. \"Transporter networks: Rearranging the visual world for robotic manipulation.\" Conference on Robot Learning. PMLR, 2021.\n\n[4] Minderer, M., et al. \"Simple open-vocabulary object detection with vision transformers\u201d. arXiv 2022. arXiv preprint arXiv:2205.06230.\n\n[5] Kirillov, Alexander, et al. \"Segment anything.\" arXiv preprint arXiv:2304.02643 (2023).\n\n> 2. The technique proposed by this paper may highly depend on the choice of LLM. Ablations w/ different LLMs is required to show generality.\n\nThank you for your valuable suggestion. Our method currently still depends on the powerful GPT4. GPT4 is prone to generate outputs as we instructed that can be executed in the code, which is necessary for the manipulator policy. However, open-source models like Llama2 or Vicuna often throw errors both syntactically and semantically even if we show exemplars and instruct the output format in the prompt. Maybe instruction finetuning can alleviate this issue, but this is not the focus of our paper.\n\n> 3. In SayCan paper, there is an \"open source environment\" section. Looks like the tasks are similar. It will be interesting to see the comparison w/ SayCan\n\nThank you for your comments. The work SayCan aims to provide real-world grounding by means of pre-trained skills. In comparison, our work utilizes LLMs as both high-level task planners and low-level motion controllers without dependency on pre-trained skills. Therefore, it may be unfair to include SayCan as the baseline.\n\n> Is the policy able to re-try if the first trial fails?\n\nYes. We query the LLM to re-plan sub-tasks and motion sequences every five time steps to correct any possible fails. To approve the effectiveness of re-plan, we applied our method in complex scenarios, involving potential interferences with object positions. More specifically, we randomly change the object position at any given time step within an episode. An example of environmental observation and robot trajectories is provided in Figure 6 for your reference. The results demonstrate the robustness of our method, notably its ability to dynamically recalibrate motion sequences."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653111048,
                "cdate": 1700653111048,
                "tmdate": 1700654479161,
                "mdate": 1700654479161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lkk6w2mq2K",
            "forum": "cbVnJa4l2o",
            "replyto": "cbVnJa4l2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_4STn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_4STn"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a training-free grounded LLM approach for Embodied AI called LLM+A(ffordance). It leverages LLM as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences). To ground these plans and control sequences on the physical world, they develop the affordance prompting technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects. Empirical evaluation is shown on robotic manipulation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is generally well-written and easy to follow.\n2. The work tackles a very challenging and useful problem for the embodied AI community -- handling high-level and low-level planning jointly with a single foundational model."
                },
                "weaknesses": {
                    "value": "1. Limited technical contribution: this work comes off as an empirical evaluation of certain style of prompt engineering for robotic manipulation. Other than the fact that some prompt worked for a small set of robotic manipulation tasks, I am not sure what I learnt from this paper. \n2. Limited evaluation: \n -  Furthermore, the evaluation tasks are too simple. Unclear how affordance prompting would scale to more complex or more real world tasks, for instance manipulation in cluttered settings or situations with partial observability.\n - Unclear how well LLM+A will do without a really strong llm such as GPT4. The use of GPT4 also makes it a computationally slow framework for online deployment. To that end, it would be good for the authors to report execution time/time to solve for their evaluation tasks. I encourage authors to also do real-world evaluation to really put the runtime in perspective. Lastly, I'd also like to see LLM+A performance with other opensource models like Llama2 or Vicuna.\n- Also unclear if the results are reproducible given GPT4's changing capabilities over time: https://arxiv.org/pdf/2307.09009.pdf I therefore encourage authors to consider open source alternatives, at the least time-stamp their GPT4 evaluations. \n3. Baselines: It is also unclear how the authors chose the baselines they compare with. They do not provide a rationale on their selection of baselines. Instead they simply choose some subset of llm prompt based approaches. Was the goal to just compare their prompt style? Why not also compare with Palm-e, RT-2, GATO, VIMA to show that their training-free prompt-based approach works better than these others that required additional data for training? Even if the goal is to compare prompt-based approaches, many others come to mind such as text2motion: https://sites.google.com/stanford.edu/text2motion \n4. Limited analysis: Given that the tasks were evaluated in sim, I would have liked a more detailed failure analysis, for instance assuming perfect vision information. Authors explain that Block2Block has low success rate because of the need to reason about interaction with other blocks. But then shouldnt this be the case with SeparateBlock task as well? Also, what about Block2Position task? The success rates in Block2Position task also seem low (42%)."
                },
                "questions": {
                    "value": "- Why not give examples to naive llm baseline given that it needs to output coordinates in specific format too, just like LLM+A?\n- Do all other baselines also use GPT4? This isnt mentioned anywhere in the paper, but I assumed this was the case. Please clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Reviewer_4STn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820494755,
            "cdate": 1698820494755,
            "tmdate": 1699637150787,
            "mdate": 1699637150787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4ezTLoPMnL",
                "forum": "cbVnJa4l2o",
                "replyto": "Lkk6w2mq2K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4STn (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4STn,\n\nWe greatly appreciate your insightful comments and the time you took to review our manuscript. Your feedback is essential to improving our work, and here we address your concerns:\n\n> Limited technical contribution: this work comes off as an empirical evaluation of certain style of prompt engineering for robotic manipulation. Other than the fact that some prompt worked for a small set of robotic manipulation tasks, I am not sure what I learnt from this paper.\n\nIn this work, we demonstrate that a pre-trained LLM (GPT4), equipped with only an off-the-shelf detection model, can guide a robot manipulator by outputting high-level sub-task plans and low-level sequences of end-effector waypoints. This is quite a challenging task. First, the proposed method does not depend on pre-trained skills, motion primitives, or trajectory optimizers. Second, the proposed prompting method is task-agnostic so can easily generalize to heterogeneous tasks. Third, since LLMs are not trained for grounded physical interaction, the proposed affordance prompting effectively improves the executability of both the sub-task plans and the control sequences. We appreciate that you denote that the number of robotic manipulation tasks is limited in the previous manuscript. In response, we plan to conduct more realistic robotic manipulation tasks in revision, such as \u201copen the drawer\u201d, \u201cpush the button\u201d, etc. \n\n> Furthermore, the evaluation tasks are too simple. Unclear how affordance prompting would scale to more complex or more real world tasks, for instance manipulation in cluttered settings or situations with partial observability.\n\nThank you for your valuable suggestions and we agree with you that the experiment should include more complex tasks. Accordingly, we have incorporated additional testing scenarios. First, we diversified the shape of objects, introducing cubes, pentagons, star-shaped, and moon-shaped objects. We conducted an evaluation of our method over 100 episodes in the Block2Position task, resulting in a success rate of 38%. Notably, this rate is nearly consistent with the success rate reported in our paper. Besides, an example is shown in Figure 5. Second, we applied our method in complex scenarios, involving potential interferences with object positions. More specifically, we randomly change the object position at any given time step within an episode. An example of environmental observation and robot trajectories is provided in Figure 6 for your reference. The results demonstrate the robustness of our method, notably its ability to dynamically recalibrate motion sequences. Additionally, although partial observability may occur in real-world tasks, our method can address this issue through re-planning in the fixed interval.\n\n> Unclear how well LLM+A will do without a really strong llm such as GPT4. The use of GPT4 also makes it a computationally slow framework for online deployment. To that end, it would be good for the authors to report execution time/time to solve for their evaluation tasks. I encourage authors to also do real-world evaluation to really put the runtime in perspective. Lastly, I'd also like to see LLM+A performance with other opensource models like Llama2 or Vicuna.\n\nThank you for your valuable suggestion.As the currently most powerful LLM, GPT4 encompasses plentiful commonsense and excellent abilities of reasoning and instruction following. Additionally, GPT is prone to generate outputs as we instructed that can be executed in the code. However, open-source models like Llama2 or Vicuna often throw errors both syntactically and semantically even if we show examples and instruct the output format in the prompt. Maybe instruction finetuning can alleviate this issue, but this is not the focus of our paper.\n\nTo increase the time efficiency, we query GPT4 to in a fixed interval generate sub-task plans and motion sequences instead of re-planning new trajectory waypoints every time step. Specifically, the planned sequence will be updated only after the robot finishes five waypoints in the previous query round. The average query number of each episode in evaluation tasks can be referred to in the following table. We do not include the query time since it depends on different sources of OpenAI APIs.\n| Task | Block2Position | Block2Block | Separate | \n| --- | --- | --- | --- | \n| GPT4 average query number | 3.86 | 3.75 | 1.82 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652815114,
                "cdate": 1700652815114,
                "tmdate": 1700652829483,
                "mdate": 1700652829483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hW9rVNQ9f",
                "forum": "cbVnJa4l2o",
                "replyto": "ZUQZP5vCrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9146/Reviewer_4STn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9146/Reviewer_4STn"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for various explanations and additional experiments. \n\nRe: giving examples to naiveLLM. I meant that naive LLM should also be given demo1 and demo2 like the LLM+A model to ensure apples to apples comparison (including affordance values?). I wasn't asking for an example \"of\" naive LLM.\n\nRe: additional experiments. I appreciated additional experiments with human interference. I still find the overall set of experiments to be very simple. Also given that only GPT4 can solve these tasks, currently the main learning that I get from the paper is: GPT4 can reason about affordances and position coordinates given a structure of prompt and some examples. This is a good takeaway from a workshop paper but is not sufficient contributions for a paper IMO. So I'll keep my ratings."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687495720,
                "cdate": 1700687495720,
                "tmdate": 1700687495720,
                "mdate": 1700687495720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HkR9ppFss2",
            "forum": "cbVnJa4l2o",
            "replyto": "cbVnJa4l2o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_WA1X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9146/Reviewer_WA1X"
            ],
            "content": {
                "summary": {
                    "value": "This works uses LLMs to generate plans that solve language-conditioned table top problems taking advantage of \u201caffordances\u201d prompting. The focus is very relevant for the robotics community and the benchmark is nicely selected. However, sometimes the explanation of the methods can be improved and the accuracy obtained is not in line with the claims of how affordances improve the planning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tVery good understanding of the challenges of LLMs solutions for robotics.\n-\tIntroducing affordances into LLMs solution may increase generalization\n-\tGood selection of benchmarks."
                },
                "weaknesses": {
                    "value": "-\tThe description of the Motion Controller could be improved as it is different in the experiments.\n-\tThe work is relegating too much emphasis on affordance prompting, but this prompting is overengineered. For instance, Pick&place example does not shows enough the advantage of affordance prompting.\n-\tResults accuracy is very low comparing to other LLMs methods to solve planning.\n\n\n**Focus**\n\nI personally think that the focus is perfectly well framed in robotics and is going straight to the point to why affordances are needed. However, the example used for explaining why current methods do not work: \u201cIt may move directly right and then push the block\u201d is not enough demonstrated with SOTA algorithms and maybe too biased by comparing with ReAct. For instance,  Driess et al. PALM-E  and interactive language can solve these type of table top problems.\n\n**State of the art**\n\nPrevious works on affordances and tools: this could be improved. Examples:\n\nJamone, L., Ugur, E., Cangelosi, A., Fadiga, L., Bernardino, A., Piater, J., & Santos-Victor, J. (2016). Affordances in psychology, neuroscience, and robotics: A survey. IEEE Transactions on Cognitive and Developmental Systems, 10(1), 4-25.\n\nFang, K., Zhu, Y., Garg, A., Kurenkov, A., Mehta, V., Fei-Fei, L., & Savarese, S. (2020). Learning task-oriented grasping for tool manipulation from simulated self-supervision. The International Journal of Robotics Research, 39(2-3), 202-216.\nBesides, what is the difference between authors approach and other LLMs table top like Interactive Language: Talking to Robots in Real Time or non-pure LLM solutions like CLIPort solution.\n\n**Methods**\n\nIt is not totally clear for me, who is setting the object parts and how the affordance values are being generated. As this is totally different in the pushing and the pick&place experiments. Also it is not clear how the position control is generated (what is the size of the vector?, is it restricted?)\n\n**Results**\n\nIt was not clear why the accuracy is so low despite the reasoning power of the LLM and assuming that the affordance prompting is helping out. In \u201clanguage table\u201d results are ~95% accuracy."
                },
                "questions": {
                    "value": "Further comments:\n\nTraining-free means zero-shot? As the LLMs are trained.\n\n \u201cAffordances\u201d prompting is interesting but is not totally solving the problem, maybe learning non-language dynamics could be a key point for the low-level control. Otherwise LLMs, will always stay in the high-level planning.\n\nShould be affordances as goal-conditioned values generalized non-goal-conditioned effects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9146/Reviewer_WA1X"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9146/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698940697722,
            "cdate": 1698940697722,
            "tmdate": 1700557429026,
            "mdate": 1700557429026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KUamIygk2e",
                "forum": "cbVnJa4l2o",
                "replyto": "HkR9ppFss2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9146/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WA1X (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer WA1X, \n\nWe greatly appreciate your insightful comments and the time you took to review our manuscript. Your feedback is essential to improving our work, and here we address your concerns:\n\n> The description of the Motion Controller could be improved as it is different in the experiments.\n\nIn the motion controller, given the decomposed sub-tasks and the affordance values from the sub-task planner, the executable motion sequences for the robots are generated. This is consistent among different tasks in our experiments. We will clarify the related descriptions to avoid possible misunderstandings.\n\n> The work is relegating too much emphasis on affordance prompting, but this prompting is overengineered. For instance, Pick&place example does not show enough the advantage of affordance prompting.\n\nWe aim to explore the adaptation of the proposed affordance prompting technique. Therefore,  we do not further engineer the prompting style for each experimental task, which is verified to be effective in Pick&Place task. To further show the robustness of our method, we have incorporated additional testing scenarios. First, we diversified the shape of objects, introducing cubes, pentagons, star-shaped, and moon-shaped objects. We conducted an evaluation of our method over 100 episodes in the Block2Position task, resulting in a success rate of 38%. Notably, this rate is nearly consistent with the success rate reported in our paper. Besides, an example is shown in Figure 5. Second, we applied our method in complex scenarios, involving potential interferences with object positions. More specifically, we randomly change the object position at any given time step within an episode. An example of environmental observation and robot trajectories is provided in Figure 6 for your reference. The results demonstrate the ability of our method to dynamically recalibrate motion sequences. Additionally, we plan to conduct more realistic robotic manipulation tasks in the final version, such as \u201copen the drawer\u201d, \u201cpush the button\u201d, etc. \n\n> Results accuracy is very low comparing to other LLMs methods to solve planning.\n\nMost of the previous planning-based LLM methods (such as \u201cLanguage Table\u201d) depend on either pre-trained low-level skills or massive multi-modal data to encompass diverse robotic tasks. In contrast, we utilize LLM as both the high-level sub-task planner and the low-level motion controller without the need for additional training process/data which is advantageous in many real-world applications (e.g., the scenario where the agent needs to fast adapt to heterogeneous tasks). We acknowledge that accuracy is a current shortcoming of our method, but we believe the performance of such methods will improve along with the improvement of prompt design and the base LLM. \n\n> The example used for explaining why current methods do not work: \u201cIt may move directly right and then push the block\u201d is not enough demonstrated with SOTA algorithms and maybe too biased by comparing with ReAct. For instance, Driess et al. PALM-E and interactive language can solve these type of table top problems.\n\nWe would like to explain that the proposed method can be effective in many robotic manipulation tasks except for push blocks. The affordance prompting aims to infer goal-conditioned affordance values, which indicate the executable priorities of different parts of interacted objects. For example, when instructed to \u201cpick the pot on the table\u201d, the robot can be guided to hold the handle of the pot with affordance prompting. We plan to report more experimental results to demonstrate this advantage.\n\n> Previous works on affordances and tools: this could be improved.\n\nThank you for your valuable suggestions. We have included the recommended references in the current manuscript.\n\n> It is not totally clear for me, who is setting the object parts and how the affordance values are being generated. As this is totally different in the pushing and the pick&place experiments.\n\nWe manually choose the object parts for different kinds of tasks. For the pushing task, we distinguish the four edges of the bounding box as object parts. For the Pick&Place task, we detect the center of the bounding box as object parts. Then, we prompt the LLMs to directly generate affordance values ranging from 0 to 1 to represent the usefulness of each object part in the sub-task planner, and an example is shown in Listing 1 in the Appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9146/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652214208,
                "cdate": 1700652214208,
                "tmdate": 1700653659375,
                "mdate": 1700653659375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]