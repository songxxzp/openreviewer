[
    {
        "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"
    },
    {
        "review": {
            "id": "f0byAromLb",
            "forum": "J9wzKfgZVK",
            "replyto": "J9wzKfgZVK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_b1Tv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_b1Tv"
            ],
            "content": {
                "summary": {
                    "value": "The paper conducts a theoretical investigation into in-context learning (ICL) and addresses three key questions: the choice of ICL estimator, the performance metric for ICL along with its associated error rate, and the role of the transformer architecture in ICL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper provides a more general theoretical model on the response and hidden variables compared with previous works in this direction.\n2.\tThe authors propose a new regret metric to quantify the performance of ICL and develops a $\\mathcal{O}(1/T)$ regret rate for this metric. \n3.\tBesides, the authors analyze the error incurred during the pretraining phase, and the error decays exponentially in terms of the depth of the network."
                },
                "weaknesses": {
                    "value": "1.\tAs this paper primarily focuses on theoretical aspects, it is encouraged to enhance the clarity of the presented proofs. Given that most of the proofs are intense, providing a big picture to roughly explain and outline how the proof works is crucial for readers' comprehension. Otherwise, readers can easily get confused. For instance, in the proof of Theorem 5.3, it would be beneficial to further explain why we should pick the distributions $P$ and $Q$ in this way when we bound the terms in error decomposition. Similarly, in the proof of Proposition 5.4, why should we first construct an approximator for $g^*$, and then $\\rho^*$, $\\phi^*$? These will be clearer if a big picture is provided.\n\n2.\tFollowing the above point, the authors should explain the reasons from one step to the next step more concretely. For instance, in the proof of Proposition 5.4, the assumptions 5.2 and F.3 do not appear. If there are some steps obtained with these assumptions implicitly, it is encouraged to point it out explicitly. There may be more areas throughout the paper where such improvements in clarity are needed, and the authors are encouraged to address them to enhance overall presentation.\n\n3.\tWith all these bounds obtained, it is recommended to validate them empirically, even if through synthetic experiments. Such empirical verification would not only enhance the credibility of the theoretical results but also provide an additional perspective on their soundness."
                },
                "questions": {
                    "value": "1.\tAccording to the definition of the assumed model on $r_t$, is it not related to the true hidden concept? Is this a common assumption on realistic data? \n2.\tIs there any reason that the authors consider the regret in the form of average instead of the usual accumulative regret? Under this new regret, the rate is $\\mathcal{O}(1/T)$. Does that mean in terms of the usual accumulative regret, the rate is constant? How do the authors interpret this?\n3.\tIn the proof of Proposition 4.1, how do the authors obtain the first and the third equalities formally? For the current first equality, does it mean $h_{t+1}$ is independent of $c_{t+1}$? Even though the authors mention these equalities come from model (4.2) (by the way, should it be model (4.1) instead?), more concrete explanations are expected.  \n4.\tRegarding the proof of Corollary 4.2, In the first formula, why $P(S_t|z)$ only relates to responses $r_{i}$? In the third formula, should the most outer sum be $\\sum_{t=0}^{T}$, or $\\sum_{t=1}^{T}$? If the second equality is derived through telescoping, it is not immediately clear why only one term remains. Also, how is the last equality obtained? \n5.\tRegarding the proof of Theorem 5.3, in the last inequality of (F.2), do we have an expectation over $S_t^n$ for the first term? According to lemma I.5, there is an expectation over $X\\sim\\nu$ for the squared TV. Also, the authors note the bound of $P_{\\hat{\\theta}}(x|S) $ in (F.6), but how is this obtained? How is the TV bound located between (F.6) and (F.7) satisfied? Why $ 2\\epsilon/b_y$ is in $\\mathcal{O}(\\frac{1}{NT})$? \n6.\tIn the proof of Proposition 5.4, the authors obtain the bound for the difference in L1 norm to be in terms of $B_{A,1}^{D''}$, which is an exponential growth term, but how does this later become exponentially decay in terms of D? What does it mean for the sentence \u201cWe take that D\u2019=\u2026 in Lemma I.9\u201d? Do we let D\u2019 and D\u2019\u2019 be these quantities or C in D\u2019 and D\u2019\u2019 be these quantities? How is the TV bound in step 3 satisfied? Please clean up the proof to make it clearer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699685240,
            "cdate": 1698699685240,
            "tmdate": 1699636774215,
            "mdate": 1699636774215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FXMjly1Gib",
                "forum": "J9wzKfgZVK",
                "replyto": "f0byAromLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer b1Tv: Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and the valuable feedback. We address the major concerns in the following.\n\n**Added Experimental Results**\n\nWe conduct five experiments to verify our theoretical findings, including the Bayesian view (Proposition 4.1 and Eqn. (4.7)), the regret upper-bounded in Corollary 4.2 and  Theorem 6.2, and the constant ratio between $\\mathtt{attn}_{\\dagger}$ and $\\mathtt{attn}$ in Proposition 4.3. The results and implementation details are provided in Appendix C and D in the revised manuscript. We will list a part of numerical results as table below.\n\n* **Verification of the Bayesian View**\n\nTable 1:\n\n|                 | zs-hc | ICL-0 | ICL-1 | ICL-5 | ICL-10 |\n|-----------------|-------|-------|-------|-------|--------|\n| antonym         | 0.736 | 0.000 | 0.296 | 0.651 | 0.675  |\n| country-capital | 0.825 | 0.048 | 0.881 | 0.905 | 0.952  |\n| present-past    | 0.847 | 0.082 | 0.820 | 0.967 | 0.967  |\n\nTo verify the Bayesian view that we adopt in the paper, we implemented two experiments. In the first experiment, we explicitly construct the hidden concept vectors that are found by LLMs. Motivated by (4.7) and Proposition 4.3, we construct the hidden concept vector as the average sum over prompts of the values of twenty selected attention heads, i.e., we compress the hidden concept into a vector with dimension $4096$. To demonstrate the effectiveness of the constructed hidden concepts, we add these hidden concept vectors at a layer of LLMs when the model resolves the prompt with zero-shot. In Table 1, \"zs-hc\" refers to the results of LLMs that infers with learned hidden concept vectors and zero-shot prompt, and \"ICL-$i$\" refers to the results of LLMs prompted with $i$ examples. This table report the accuracy of LLMs under different prompts. We consider the tasks of finding antonyms, finding the capitals of countries, and finding the past tense of words. The results indicate that the LLMs with learned hidden concept vectors have **comparable performance** with the LLMs prompted with several examples. This indicates that the learned hidden concept vectors are indeed efficient **compression of the hidden concepts**, which proves that LLMs deduce hidden concepts for ICL. This result strongly corroborates with (4.7). \n\nTable 2:\n\n|          | 2 examples | 6 examples | 10 examples | 14 examples | 18 examples |\n|----------|------------|------------|-------------|-------------|-------------|\n| accuracy | 0.000      | 0.917      | 0.917       | 1.000       | 1.000       |\n\n\nIn the second experiment, we aim to verify that LLMs implement inference with the Bayesian framework, not with gradient descent [1,2,3] on some tasks. We prompt the LLMs with the history data of a set of similar multi-armed bandit instances with $100$ arms, and let LLMs indicate which arm to pull in a similar new bandit instance. In these similar bandit instances, there is an informative arm, whose reward is exactly the index of the arm with the highest rewards. We also provide the side information that \"Some arm may directly tell you the arm with the highest reward, even itself does not have the highest reward\". In each example provided in the prompt, there are the rewards of six arms, including the informative arm and the best arm, in one bandit instance. As shown in Table 2, the LLMs can efficiently implement ICL even with only $6$ examples. We note that the gradient descent algorithms in the previous works cannot explain this performance, since the gradient descent algorithms need **at least $100$ data points**, where each data point is the reward of one arm, to learn. In contrast, the Bayesian view can clearly explain Table 2, where LLMs make use of the side information to calculate **a better posterior** for the bandit instance. Thus, LLMs have good ICL performance in this setting.\n\n\n* **Verification of the Regret Bound**\n\nTable 3:\n|                        | 0 examples | 10 examples | 20 examples | 30 examples | 40 examples |\n|------------------------|------------|-------------|-------------|-------------|-------------|\n| $t\\cot\\text{regret}$   | 1.006      | 8.204       | 10.505      | 10.529      | 10.535      |\n| squared error of query | 1.006      | 0.528       | 0.0217      | 0.001       | 0.001       |\n\nTo verify Corollary 4.2 and Theorem 6.2, we implement experiments to evaluate the regret in two settings. In the first setting, the LLMs is trained for the linear regression task from scratch, which is a representative setting studied in [1,4]. The examples in the prompt are $\\lbrace(x _i,y _i)\\rbrace _{i=1}^{N}$, where $x _i\\in\\mathbb{R}^d$, $d=20$ and $y _{i}=w^{T}x _{i}$ for some $w$ sampled from Gaussian distribution. Given the Gaussian model, we adopt the squared error to approximate the logarithm of the probability. Then the $t\\times \\text{regret}$ of the LLMs can be well approximated by the sum of the squared error till time $t$."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575492865,
                "cdate": 1700575492865,
                "tmdate": 1700575492865,
                "mdate": 1700575492865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mb9C6KcFUU",
                "forum": "J9wzKfgZVK",
                "replyto": "f0byAromLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Explanations about the Distribution Construction in the Proof of Theorem 5.3 and the Network Construction in the Proof of Proposition 5.4**\n\nWe thank the reviewer for the suggestions. We have modified correspondingly in the revised version, and we added two figures in the revised manuscript to illustrate these two constructions. Here we provide the explanations about the details in the proofs of Theorem 5.3 and Proposition 5.4. \n\n* For Theorem 5.3, we adopt the Pac-Bayes framework to derive the generalization error bound, i.e., Lemma I.2. We restate the main equation in the lemma\n\n\\begin{align*}\n        \\bigg|\\mathbb{E} _{Q}\\bigg[\\mathbb{E} _{X}\\big[ f(X) \\big]-\\frac{1}{n}\\sum _{i=1} ^{n}f(X _{i})\\bigg]\\bigg|\\leq \\lambda c\\mathbb{E} _{Q}\\big[\\mu(f)\\big]+\\frac{1}{n\\lambda}\\bigg[\\text{KL}(Q\\|P _{0})+\\log\\frac{2}{\\delta}\\bigg].\n\\end{align*}\n\nTo derive the uniform generalization bound for the transformer function class, we need to set $Q$ and $P_0$ such that the left-hand side of the inequality is close to the generalization error of any function and the right-hand side remains bounded. To achieve this goal, we set **$Q$ as the uniform distribution on the neighborhood of any function**, and the difference between the left-hand side of the inequality and the generalization error of any function can be upper bounded via Proposition F.1. We set $P_0$ as the **uniform distribution on the whole function class**, which guarantees that the right-hand side of the inequality can be upper bounded by $O(\\log (NT))$.\n\n* For Proposition 5.4, we bound the approximation error with the help of Theorem 2 in [6], which states that for any permutation-invariant function $g^{\\ast}(X):\\mathfrak{X}^{L}\\rightarrow \\mathbb{R}^{d _{y}}$, we have that\n\\begin{align*}\n    g^{\\ast}(X)=\\rho^{\\ast}\\bigg(\\frac{1}{L}\\sum _{i=1}^{L}\\phi^{\\ast}(x _{i})\\bigg),\n\\end{align*}\nwhere $\\rho^{\\ast}: \\mathbb{R}\\rightarrow \\mathbb{R}^{d _{y}}$ and $\\phi^{\\ast}:\\mathfrak{X}\\rightarrow \\mathbb{R}$. Thus, to approximate a permutation-invariant function, we only **need to approximate $\\phi^\\ast$ and each component $\\rho_i^*$ of $\\rho^\\ast$**. In the proof, we first state that if we already have $\\phi^\\ast$ and $\\rho^\\ast$ approximated by submodules of transformer, then we can approximate $\\rho^{\\ast}\\bigg(\\frac{1}{L}\\sum _{i=1}^{L}\\phi^{\\ast}(x _{i})\\bigg)$ by **combining these submodules with a single attention** (Step 1). In the second step, we indicate how to **approximate  $\\phi^\\ast$ and $\\rho^\\ast$** with the modules in transformers (Step 2).\n\n**Where Assumptions 5.2 and F.3 are used**\n\nWe point to where Assumptions 5.2 and F.3 are used in the revised manuscript. In fact, we construct $\\phi^\\ast$ and $\\rho^\\ast$ with the help of Lemma I.9, which requires Assumption F.3. In Step 3, we bound the ratio between $\\mathbb{P}(x|S)$ and $\\mathbb{P} _{\\theta^{\\ast}}(x|S)$, which requires the denominator to be larger than $0$. We use Assumption 5.2 here to derive the bound.\n\n**Explanations of (4.1)**\n\nWe highlight that our model in (4.1) indicates that $r _t$ **depends on the true hidden concept $z _\\ast$** through $h _t$, but $c _t$ does not depend on $z _\\ast$. In fact, $r _t$ directly depends on some hidden variable $h_t$, whose distribution is determined by $z _\\ast$. When $h _{t}=z$ for $t\\in[T]$ degenerates to the hidden concept, this recovers the casual graph model in [7] and the model in [8].\n\n**Motivation for the Regret Definition and Why $t\\cdot \\text{regret}$ is Upperbounded by a Constant**\n\nWe define the regret in the form of an average to indicate the **average regret of each example** in the prompt. In the classical supervised learning task, it is common to take mean square error as the performance metric. In the Gaussian model, our regret degenerates to the mean square error. It is also reasonable to consider the cumulative regret, which can be upper bounded by a constant. It originates from the fact that LLMs can identify the correct hidden concept with the error **exponentially decaying in the number of samples $t$**, which is shown in Proposition 6.3. The intuition can be seen from the following approximate calculations under a simpler model. Now we assume that $\\lbrace(r _t,c _t)\\rbrace _{t=1}^{T}$ are i.i.d. samples generated from $P(r|c,z _\\ast)\\cdot P(c)=P(r,c| z _\\ast)$ . Then we have\n\n\\begin{align*}\n\t\\frac{1}{T}\\sum _{t=1}^{T}\\log\\frac{P(r _t, c _t|z _\\ast)}{P(r _t, c _t|z)}\\rightarrow \\text{KL}(P(\\cdot|z _\\ast)\\|P(\\cdot|z)).\n\\end{align*}\nIt implies that\n\\begin{align*}\n\t\\frac{P(z| S _T)}{P(z _\\ast| S _T)}=\\frac{P(z)P(S _T|z)}{P(z _\\ast)P(S _T|z _\\ast)}\\rightarrow \\frac{P(z)}{P(z _\\ast)}\\exp(-T\\cdot\\text{KL}(P(\\cdot|z _\\ast)\\|P(\\cdot|z))).\n\\end{align*}\n\n\nSince $\\sum _{t=1}^{T}\\exp(-kt)<O(1)$ for $k>0$ can be upper bounded by a constant, LLMs achieve the constant accumulative regret. It is also verified by our **simulation results** in the revised manuscript."
                    },
                    "title": {
                        "value": "Response to Reviewer b1Tv: Part 3"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576504383,
                "cdate": 1700576504383,
                "tmdate": 1700577088231,
                "mdate": 1700577088231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJoL9eLSoV",
                "forum": "J9wzKfgZVK",
                "replyto": "f0byAromLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Explanations of The Proof of Proposition 4.1**\n\nWe thank the reviewer for the suggestions and modified the revised version accordingly. We provide detailed calculations of the first and the third equality here. \n\n* For the first equality, we have\n\\begin{align*}\n\\mathbb{P}(r _{t+1}| \\text{pt} _t) &= \\int \\mathbb{P}(r _{t+1}| h _{t+1},\\text{pt} _t) \\mathbb{P}(h _{t+1}| \\text{pt} _t)\\mathrm{d}h _{t+1} \\\\\n&= \\int \\mathbb{P}(r _{t+1}| h _{t+1},\\tilde{c} _{t+1}) \\mathbb{P}(h _{t+1}\\,|\\, S _t)\\mathrm{d}h _{t+1},\n\\end{align*}\nwhere the first equality results from the Bayes\u2019 rule, and the second equality results from the fact that $r _{t+1}$ is **conditionally independent** with the previous history given $h _{t+1},\\tilde{c} _{t+1}$ and the fact that $h _{t+1}$ only **parameterizes the transition kernel** of $r _{t+1}$ given $c _{t+1}$. Thus, $h _{t+1}$ and $c _{t+1}$ are independent. \n\n* For the third equality,\n\\begin{align*}\n&\\int \\mathbb{P}(r _{t+1} | c _{t+1}, h _{t+1}) \\mathbb{P}(h _{t+1}  | S _t, z)\\mathbb{P}(z  | S _t) \\mathrm{d} h _{t+1} \\mathrm{d} z \\\\\n&\\quad= \\int \\mathbb{P}(r _{t+1} | c _{t+1}, h _{t+1},S _t, z) \\mathbb{P}(h _{t+1}  | S _t, z) \\mathrm{d} h _{t+1} \\mathbb{P}(z  | S _t) \\mathrm{d} z \\\\\n&\\quad= \\int\\mathbb{P}(r _{t+1}|c _{t+1}, S _t, z)\\mathbb{P}(z | S _t) \\mathrm{d} z,\n\\end{align*}\nwhere the first equality results from the fact that $r _{t+1}$ is conditionally independent with the other variables given $h _{t+1},\\tilde{c} _{t+1}$, and the second equality results from the Bayes\u2019 rule.\n\n**Explanations of the Proof of Corollary 4.2**\n\n*In the first formula, since the hidden variable $z$ only parameterizes the **conditional probability** of $r _t$ given $c _t$, $c _t$ and $z$ are independent. \n\n* In the third formula, the most outer sum is $\\sum _{t=0}^{T}$. For the telescoping argument, we follow the convention that $\\sum _{t=1}^{0}\\cdot = 0$. In fact, when $t=0$, \n\\begin{align*}\n\\mathbb{P}(r _{t+1}|c _{t+1},S _{t})= \\frac{\\int \\mathbb{P}(r _1|c _1,z)\\mathbb{P}(z)\\mathrm{d}z}{1}.\n\\end{align*}\n\n\nSince $\\log 1 =0$, **this term just disappears**. Thus, the telescoping argument only leaves one term. \n\n\n* The last equality follows from the direct calculations. Here we present the calculation process for the finite $\\mathfrak{Z}$. For any function $f:\\mathfrak{Z}\\rightarrow \\mathbb{R}$ and distribution $p\\in\\Delta(\\mathfrak{Z})$, we will calculate\n\\begin{align*}\n\t\\min _{q\\in\\Delta(\\mathfrak{Z})} -\\sum _{z}f(z)q(z)+\\sum _{z}q(z)\\log\\frac{q(z)}{p(z)}.\n\\end{align*}\nDefine the lagrangian function\n\\begin{align*}\n\tg(q,\\lambda) = -\\sum _{z}f(z)q(z)+\\sum _{z}q(z)\\log\\frac{q(z)}{p(z)}+\\lambda (\\sum _{z}q(z)-1).\n\\end{align*}\nBy solving the equations\n\\begin{align*}\n\t\\nabla _{q} g(q,\\lambda)=0,\\quad \\frac{\\partial}{\\partial \\lambda}g(q,\\lambda)=0,\n\\end{align*}\nwe have that the minimizer is \n\\begin{align*}\n\tq^{\\ast}(z) = \\frac{p(z)\\exp(f(z))}{\\sum _{z^\\prime}p(z^\\prime)\\exp(f(z^\\prime))}.\n\\end{align*}\nPlugging this into the objective function, we have\n\\begin{align*}\n\t\\min _{q\\in\\Delta(\\mathfrak{Z})} -\\sum _{z}f(z)q(z)+\\sum _{z}q(z)\\log\\frac{q(z)}{p(z)} = -\\log \\sum _{z}p(z)\\exp(f(x)).\n\\end{align*}\nThis justifies the last equality of our proof.\n\n**Explanations of The Details in the Proof of Theorem 5.3**\n\n* In the last inequality of (F.2), we note that there should be no expectation for the first term. The reason is that the left-hand side of (F.1) takes conditional expectation with respect to $\\tilde{\\mathcal{D}}$ conditioned on $\\mathcal{D}$. Since **$S _t^n$ is a part of $\\mathcal{D}$**, taking the conditional expectation with respect to $\\tilde{\\mathcal{D}}$ conditioned on $\\mathcal{D}$ of the first term is trivial. Thus, there is no explicit expectation over the first term.\n\n\n* The bound in (F.6) originates from the last **softmax layer** in transformers, which is defined below (5.1). Here $B_A$ is the bound of each component of the softmax input. To derive the lower bound of the output probability of transformers, we calculate the minimal output component of softmax given the bound of each component.\n\n\n* The TV bound located between (F.6) and (F.7) originates from Proposition F.1 and the distribution assignment (F.5). As discussed in the revised manuscript, we set the distribution $P$ as the uniform distribution on the neighborhood on a parameter with **radius proportional to $1/NT$**. The corresponding approximation error measured in $l_1$ distance,i.e., the TV distribution, is bounded by $1/NT$. Thus, Proposition F.1 shows that the approximation error is $O(1/NT)$ a.s. We also added a figure in the appendix of the revised manuscript to highlight this."
                    },
                    "title": {
                        "value": "Response to Reviewer b1Tv: Part 4"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577042793,
                "cdate": 1700577042793,
                "tmdate": 1700577074386,
                "mdate": 1700577074386,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TxJEsF0hQU",
                "forum": "J9wzKfgZVK",
                "replyto": "f0byAromLb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer b1Tv: Part 5"
                    },
                    "comment": {
                        "value": "**Explanations of Details in the proof of Proposition 5.4**\n\nWe would like to provide detailed calculations for the exponential decaying error on $D$, and we have modified the revised manuscript accordingly. The main idea is that: (1) the later modules will **amplify** the approximation error in the previous modules (2) we will **balance the depths** of different modules to handle the amplification.\n\n\n* For the problems regarding Step 2:\n\n\n Lemma I.9 indicates the approximation error $\\varepsilon$ of a fully-connected network will depth $D$ can be upper bounded as\n\\begin{align*}\n\t\\varepsilon\\leq \\exp\\bigg(-\\sqrt{\\frac{D-\\log B}{B}}\\bigg).\n\\end{align*}\nWe denote the depth of $\\Psi _{\\phi^\\ast}$  and $\\Psi _{\\rho^\\ast}$ as $D^\\prime$ and $D^{\\prime\\prime}$, respectively. Then the total approximation error is\n\\begin{align*}\n\t\\text{approx err}&\\leq d _y \\exp\\bigg(-\\sqrt{\\frac{D^{\\prime\\prime}-\\log B}{B}}\\bigg) +d _y B _{A,1}^{D^{\\prime\\prime}}\\exp\\bigg(-\\sqrt{\\frac{D^{\\prime}-\\log B}{B}}\\bigg)\\\\\n\\end{align*}\nwhere the inequality follows from the triangle inequality in our manuscript,  $B _{A,1}^{D^{\\prime\\prime}}$ comes from the fact that the later modules will amplify the error in the previous modules in transformers, and $B _{A,1}$ is the lipschitz constant for each layer.\n\nFact: For any $l>0,c>0$, we have $\\exp(-l\\sqrt{x-c})=O(\\exp(-l\\sqrt{x}))$.\n\nProof: Basic calculations shows that $\\exp(-l\\sqrt{x-c})\\leq m\\exp(-l\\sqrt{x})$ for any $m>3$ is equivalent to \n\\begin{align*}\n\t\\frac{l\\cdot c}{\\sqrt{x}+\\sqrt{x-c}}\\leq \\log m.\n\\end{align*}\nThis trivially holds when $x$ is large. \n\nThus, we have that\n\\begin{align*}\n    \\text{approx err}=O\\bigg( d _y \\exp\\bigg(-\\sqrt{\\frac{D^{\\prime\\prime}}{B}}\\bigg) +d _y \\exp\\bigg(\\frac{1}{\\sqrt{B}}\\big[D^{\\prime\\prime}\\sqrt{B}\\log B _{A,1} -\\sqrt{D^{\\prime}}\\big]\\bigg)\\bigg).\n\\end{align*}\nTo handle the second term in the right-hand side of this inequality, we require that\n\\begin{align*}\n    k\\cdot D^{\\prime\\prime}-\\sqrt{D^{\\prime}}\\leq -\\sqrt{D^{\\prime\\prime}},\n\\end{align*}\nwhere $k= \\sqrt{B}\\log B _{A,1}$. This is equivalent to\n\\begin{align*}\n    D^{\\prime}\\geq (k\\cdot D^{\\prime\\prime}+\\sqrt{D^{\\prime\\prime}})^2.\n\\end{align*}\nSince $D^{\\prime}+D^{\\prime\\prime}\\leq D$, where $D$ is the depth of the whole network, we can set\n\\begin{align*}\n    D^{\\prime\\prime}=\\sqrt{D}/(2\\sqrt{B}\\log B _{A,1}),\\quad D^{\\prime}=D-1-D^{\\prime\\prime}\\geq D/2+D^{3/4}\n\\end{align*}\nwhen $D$ is large ($C=4$ in our original manuscript). This assignments ensure that $D^{\\prime}\\geq (k\\cdot D^{\\prime\\prime}+\\sqrt{D^{\\prime\\prime}})^2$. Thus, we have that\n\\begin{align*}\n    \\text{approx err}=O\\bigg( d _y \\exp\\bigg(-\\sqrt{\\frac{D^{\\prime\\prime}}{B}}\\bigg)\\bigg)=O\\bigg(d _{y}\\exp\\bigg(-\\frac{D^{1/4}}{\\sqrt{C^{2}B^{2}\\log B _ {A,1}}}\\bigg)\\bigg)\n\\end{align*}\nfor some constant $C>0$. Here we relax the dependency on $B$ a little for the notational clearness, and the relaxation results from the fact that $B\\geq 1$ usually.\n\n\n* For the problem regarding Step 3:\n\n\nWe note that the TV bound is exactly the $l_1$-norm, whose upper bound is derived in Step 2. \n___\nWe thank the reviewer again for reading our response. We appreciate it if the reviewer could consider raising the score, if our rebuttal has addressed your concerns.\n\n**Reference**\n\n[1] Aky\u00fcrek E, Schuurmans D, Andreas J, et al. What learning algorithm is in-context learning? investigations with linear models[J]. arXiv preprint arXiv:2211.15661, 2022.\n\n[2] Von Oswald J, Niklasson E, Randazzo E, et al. Transformers learn in-context by gradient descent[C]//International Conference on Machine Learning. PMLR, 2023: 35151-35174.\n\n[3] Bai Y, Chen F, Wang H, et al. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection[J]. arXiv preprint arXiv:2306.04637, 2023.\n\n[4] Garg S, Tsipras D, Liang P S, et al. What can transformers learn in-context? a case study of simple function classes[J]. Advances in Neural Information Processing Systems, 2022, 35: 30583-30598.\n\n[5] Gruver N, Finzi M, Qiu S, et al. Large language models are zero-shot time series forecasters[J]. arXiv preprint arXiv:2310.07820, 2023.\n\n[6] Zaheer M, Kottur S, Ravanbakhsh S, et al. Deep sets[J]. Advances in neural information processing systems, 2017, 30.\n\n[7] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[8] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577643418,
                "cdate": 1700577643418,
                "tmdate": 1700577698013,
                "mdate": 1700577698013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NVdHMAecN2",
            "forum": "J9wzKfgZVK",
            "replyto": "J9wzKfgZVK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_MVTm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_MVTm"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a theoretical analysis of in-context learning (ICL) in transformers used in large language models (LLMs), framing the attention mechanism as a Bayesian Model Averaging (BMA) process. It proposes that LLMs use attention weights to perform BMA, contributing to our understanding of model generalization from few-shot examples. The authors develop a series of propositions and theorems that articulate the role of the attention mechanism in facilitating ICL. Specifically, they demonstrate how the attention weights can be seen as performing a type of BMA. Furthermore, the paper extends the theoretical framework to provide regret bounds for ICL, which quantify how well the learning model performs in comparison to the best possible model chosen in hindsight. It also offers an in-depth error analysis that establishes bounds on the error rates of pretrained models. This work aims to deepen the theoretical foundations of ICL, offering a framework that connects with practical machine-learning challenges.\n\n-----\n\nPost rebuttal: Raising to 5 in light of authors' detailed evaluations. It would be good to see those results incorporated in the next version of the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "By adopting a Bayesian perspective and formulating ICL as a prediction problem based on examples from a latent variable model, the paper unifies the understanding of transformer architecture's ability to enable ICL.\n\nPropositions and theorems are well-formulated, providing substantial theoretical contributions to the field, such as establishing regret bounds for ICL and demonstrating the parameterization of Bayesian Model Averaging through attention mechanisms.\n\nIt includes a fine-grained analysis of pretraining under realistic assumptions, offering a bound on the error of pretrained models."
                },
                "weaknesses": {
                    "value": "The paper lacks empirical validation for its theoretical claims. This is a significant gap, as the practical impact of the theoretical insights on real-world tasks remains unclear without empirical evidence.\n\nSome of the assumptions made for the theoretical analysis are not justified with respect to their applicability in practical scenarios. The paper does not sufficiently discuss how these assumptions can be relaxed without losing the theoretical results.\n\nThe complexity of the mathematical content could limit the paper's accessibility to a broader audience. Additionally, there is a lack of clear guidance on the reproducibility of the theoretical results, which is critical for the advancement of the field.\n\nWhile the paper attempts to address important questions, its insights as compared to the existing works seem very limited at this point."
                },
                "questions": {
                    "value": "- How can the theoretical findings be empirically validated in the absence of practical experiments within the paper?\n\n- Is there a possibility to simplify the presentation to make the paper more accessible to non-experts?\n\n- Could the authors clarify the process of deriving the posterior distributions within the context of ICL and how they relate to the attention weights?\n\n- Can the authors provide more details on the assumptions made for the error analysis and how these assumptions can be justified from a practical standpoint?\n\n- Are there specific mathematical models or theories that could challenge or complement the authors' approach to understanding in-context learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Reviewer_MVTm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813891829,
            "cdate": 1698813891829,
            "tmdate": 1700587825974,
            "mdate": 1700587825974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5JBxz0ydXZ",
                "forum": "J9wzKfgZVK",
                "replyto": "NVdHMAecN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MVTm: Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We address the major concerns in the following.\n\n**Added Experimental Results**\n\nWe conduct five experiments to verify our theoretical findings, including the Bayesian view (Proposition 4.1 and Eqn. (4.7)), the regret upper-bounded in Corollary 4.2 and  Theorem 6.2, and the constant ratio between $\\mathtt{attn}_{\\dagger}$ and $\\mathtt{attn}$ in Proposition 4.3. The results and implementation details are provided in Appendix C and D in the revised manuscript. We will list a part of numerical results as the table below.\n\n* **Verification of the Bayesian View**\n\nTable 1:\n\n|                 | zs-hc | ICL-0 | ICL-1 | ICL-5 | ICL-10 |\n|-----------------|-------|-------|-------|-------|--------|\n| antonym         | 0.736 | 0.000 | 0.296 | 0.651 | 0.675  |\n| country-capital | 0.825 | 0.048 | 0.881 | 0.905 | 0.952  |\n| present-past    | 0.847 | 0.082 | 0.820 | 0.967 | 0.967  |\n\nTo verify the Bayesian view that we adopt in the paper, we implemented two experiments. In the first experiment, we explicitly construct the hidden concept vectors that are found by LLMs. Motivated by (4.7) and Proposition 4.3, we construct the hidden concept vector as the average sum over prompts of the values of twenty selected attention heads, i.e., we compress the hidden concept into a vector with dimension $4096$. To demonstrate the effectiveness of the constructed hidden concepts, we add these hidden concept vectors at a layer of LLMs when the model resolves the prompt with zero-shot. In Table 1, \"zs-hc\" refers to the results of LLMs that infers with learned hidden concept vectors and zero-shot prompt, and \"ICL-$i$\" refers to the results of LLMs prompted with $i$ examples. This table report the accuracy of LLMs under different prompts. We consider the tasks of finding antonyms, finding the capitals of countries, and finding the past tense of words. The results indicate that the LLMs with learned hidden concept vectors have **comparable performance** with the LLMs prompted with several examples. This indicates that the learned hidden concept vectors are indeed efficient **compression of the hidden concepts**, which proves that LLMs deduce hidden concepts for ICL. This result strongly corroborates with (4.7). \n\nTable 2:\n\n|          | 2 examples | 6 examples | 10 examples | 14 examples | 18 examples |\n|----------|------------|------------|-------------|-------------|-------------|\n| accuracy | 0.000      | 0.917      | 0.917       | 1.000       | 1.000       |\n\n\nIn the second experiment, we aim to verify that LLMs implement inference with the Bayesian framework, not with gradient descent [1,2,3] on some tasks. We prompt the LLMs with the history data of a set of similar multi-armed bandit instances with $100$ arms, and let LLMs indicate which arm to pull in a similar new bandit instance. In these similar bandit instances, there is an informative arm, whose reward is exactly the index of the arm with the highest rewards. We also provide the side information that \"Some arm may directly tell you the arm with the highest reward, even itself does not have the highest reward\". In each example provided in the prompt, there are the rewards of six arms, including the informative arm and the best arm, in one bandit instance. As shown in Table 2, the LLMs can efficiently implement ICL even with only $6$ examples. We note that the gradient descent algorithms in the previous works cannot explain this performance, since the gradient descent algorithms need **at least $100$ data points**, where each data point is the reward of one arm, to learn. In contrast, the Bayesian view can clearly explain Table 2, where LLMs make use of the side information to calculate **a better posterior** for the bandit instance. Thus, LLMs have good ICL performance in this setting.\n\n\n* **Verification of the Regret Bound**\n\nTable 3:\n|                        | 0 examples | 10 examples | 20 examples | 30 examples | 40 examples |\n|------------------------|------------|-------------|-------------|-------------|-------------|\n| $t\\cot\\text{regret}$   | 1.006      | 8.204       | 10.505      | 10.529      | 10.535      |\n| squared error of query | 1.006      | 0.528       | 0.0217      | 0.001       | 0.001       |\n\nTo verify Corollary 4.2 and Theorem 6.2, we implement experiments to evaluate the regret in two settings. In the first setting, the LLMs is trained for the linear regression task from scratch, which is a representative setting studied in [1,4]. The examples in the prompt are $\\lbrace(x _i,y _i)\\rbrace _{i=1}^{N}$, where $x _i\\in\\mathbb{R}^d$, $d=20$ and $y _{i}=w^{T}x _{i}$ for some $w$ sampled from Gaussian distribution. Given the Gaussian model, we adopt the squared error to approximate the logarithm of the probability. Then the $t\\times \\text{regret}$ of the LLMs can be well approximated by the sum of the squared error till time $t$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574489070,
                "cdate": 1700574489070,
                "tmdate": 1700574489070,
                "mdate": 1700574489070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W2AxJH6z7s",
                "forum": "J9wzKfgZVK",
                "replyto": "NVdHMAecN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MVTm: Part 2"
                    },
                    "comment": {
                        "value": "The results in Table 3 strongly corroborate our theoretical findings. First, the results verify our claim in Corollary 4.2 and Theorem 6.2 that $t\\cdot \\text{regret}$ can be **upper bounded by a constant.** Second, the line of squared error indicates that the ICL of LLMs only has a significant error when **$T\\leq d$**, i.e., the regret **only increases in this region**. Thus, the regret of the ICL by LLMs is at most linear in $O(d/T)$. From the view of our theoretical result, discretizing the set $\\lbrace z\\in\\mathbb{R}^{d} | \\|z\\| _{2}\\leq d \\rbrace$ with approximation error $\\delta>0$ will result in a set with $(C/\\delta)^{d}$ elements, where $C>0$ is an absolute constant. Corollary 4.2 and Theorem 6.2 imply that the regret is the sum of the **$\\log |\\mathfrak{Z}|/T = d\\log (C/\\delta)/T$** and the pretraining error, which matches the simulation results.\n\nTable 4:\n|                    | 50 tokens | 100 tokens | 150 tokens | 200 tokens | 250 tokens |\n|--------------------|-----------|------------|------------|------------|------------|\n| $y= x$             | 0.001     | 0.002      | 0.002      | 0.017      | 0.020      |\n| $y=\\sin (x)$       | 0.105     | 3.842      | 3.919      | 4.114      | 4.190      |\n| $y=x\\cdot\\sin (x)$ | 0.012     | 0.018      | 0.030      | 0.041      | 0.048      |\n\nIn the second experiment, we directly evaluate the regret of pretrained LLMs on the function value prediction task. The prompt consists of the values of a function on the points with fixed intervals. Since the values are real numbers, we adopt the method in [5] to transfer a real number to a token sequence. For the pretrained model, we cannot calculate $\\mathbb{P}(r _{i}|\\text{prompt} _{i-1},z)$ due to the unknown nominal distributions. Thus, we calculate the cumulative negative log-likelihood $\\text{CNLL} _{t}=-\\sum _{i=1}^{t}\\hat{\\mathbb{P}}(r _{i}|\\text{prompt} _{i-1})$, and $\\text{CNLL} _{t}/t$ is an upper bound of the regret. In Table 4, we indicate the cumulative negative log-likelihoods of predicting the values of three functions. The results show that the cumulative negative log-likelihoods are stepped, which means that the cumulative negative log-likelihoods are **upper-bounded by constants in a long period**. This corroborates with Corollary 4.2 and Theorem 6.2. \n\n* **Verification of the Constant Ratio between $\\mathtt{att}_{\\dagger}$ and $\\mathtt{att}$**\n\nTable 5:\n|                         | 10 examples | 1000 examples | 2000 examples | 3000 examples | 4000 examples |\n|-------------------------|-------------|---------------|---------------|---------------|---------------|\n| 75 th percentile for $d=2$ | 2.312       | 2.347         | 2.327         | 2.299         | 2.297         |\n| 25 th percentile for $d=2$ | 0.792       | 2.110         | 2.156         | 2.168         | 2.191         |\n| 75 th percentile for $d=3$ | 2.535       | 3.406         | 3.388         | 3.355         | 3.330         |\n| 25 th percentile for $d=3$ | 0.569       | 2.904         | 2.957         | 3.032         | 3.078         |\n\nTo verify Proposition 4.3, we directly calculate the ratio between $\\mathtt{att} _{\\dagger}$ and $\\mathtt{att}$. We consider the case $d _v = 1 $ and $d _k=d$ for some $d>0$. The entries in $K$ of (4.8) are i.i.d. samples of Gaussian distribution, and the $i-$th entry of $V$ is calculated as the inner product between a Gaussian vector and the $i-$th column. Table 5 shows the results for $d=2$ and $d=3$. It shows that the ratio between $\\mathtt{att} _{\\dagger}$ and $\\mathtt{att}$ will converge to a constant. This constant depends on the dimension $d$, which originates from Proposition F.1.\n\n**Summary of the mathematical content and the Reproducibility of the Results**\n\nWe would like to highlight that our main contributions are the **theoretical analysis** of ICL. Our theoretical results mainly have three parts: \n\n* The first part of our results explains how the **perfectly pertained LLMs** implement the ICL via Bayesian model averaging (Proposition 4.1) and how the attention structure enables BMA (Proposition 4.3), where the regret of ICL is also analyzed based on Proposition 4.1 (Corollary 4.2). These results are all validated by our experimental results.\n* The second part of our results is the analysis of the **pretraining error** of the transformers, which is decomposed as the sum of the generalization error and the approximation error(Theorem 5.3). The convergence rate of the approximation error is also provided ( Proposition 5). \n* The last part is the ICL performance analysis of the **pre-trained LLMs**. We provide the ICL regret analysis of LLMs with correct input-output prompts (Theorem 6.2), which is verified by our experiments in Appendix C. In addition, we derive the analysis of LLMs when it is prompted with wrong input-output examples( Proposition 6)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574739074,
                "cdate": 1700574739074,
                "tmdate": 1700574758528,
                "mdate": 1700574758528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nx09ZqEqwX",
                "forum": "J9wzKfgZVK",
                "replyto": "NVdHMAecN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MVTm: Part 4"
                    },
                    "comment": {
                        "value": "* Assumption 6.1:\nThis assumption states the coverage of the pretraining data. We note that there will be an **information-theoretic barrier** without this assumption, the ICL learning problem presents an information-theoretic barrier. For example, if the pretraining data does not contain any material about a specific mathematical symbol in the ICL prompt. In this case, it will be extremely difficult for the LLM to derive the correct prediction, since the meaning of this math symbol is unclear to LLM. \n\n* Assumption G.1:\nThis assumption states that the input-output pairs are independent when conditioned on the hidden concept $z$. This assumption is relevant in many applications, since the inputs of demonstrations are **independently sampled**, and the output only depends on the input when the concept (task) $z$ is clear.\n\n* Assumption G.2:\nThis assumption states the coverage of the hidden concept $z _\\ast$. If $P(z _\\ast)=0$, this concept never appears in the pretraining dataset. It will be very difficult for LLMs to complete this task as explained in the previous response. Compared with Assumption 6.1, this assumption is stronger since the coverage is in the hidden space, and it will be used to handle the wrong outputs.\n\n* Assumption G.3:\nThis assumption states the distinguishability of the current concept $z _\\ast$. When there are only wrong outputs in the prompt, the concept $z _\\ast$ itself should be distinguishable enough to **identify from the wrong input-outputs pairs**. This assumption is also stated in [9,17].\n\n___\nWe thank the reviewer again for reading our response. We appreciate it if the reviewer could consider raising the score, if our rebuttal has addressed your concerns.\n\n**Reference**\n\n[1] Aky\u00fcrek E, Schuurmans D, Andreas J, et al. What learning algorithm is in-context learning? investigations with linear models[J]. arXiv preprint arXiv:2211.15661, 2022.\n\n[2] Von Oswald J, Niklasson E, Randazzo E, et al. Transformers learn in-context by gradient descent[C]//International Conference on Machine Learning. PMLR, 2023: 35151-35174.\n\n[3] Bai Y, Chen F, Wang H, et al. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection[J]. arXiv preprint arXiv:2306.04637, 2023.\n\n[4] Garg S, Tsipras D, Liang P S, et al. What can transformers learn in-context? a case study of simple function classes[J]. Advances in Neural Information Processing Systems, 2022, 35: 30583-30598.\n\n[5] Gruver N, Finzi M, Qiu S, et al. Large language models are zero-shot time series forecasters[J]. arXiv preprint arXiv:2310.07820, 2023.\n\n[6] Todd E, Li M L, Sharma A S, et al. Function vectors in large language models[J]. arXiv preprint arXiv:2310.15213, 2023.\n\n[7] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[8] Hollmann N, M\u00fcller S, Eggensperger K, et al. Tabpfn: A transformer that solves small tabular classification problems in a second[J]. arXiv preprint arXiv:2207.01848, 2022.\n\n[9] Xie S M, Raghunathan A, Liang P, et al. An explanation of in-context learning as implicit bayesian inference[J]. arXiv preprint arXiv:2111.02080, 2021.\n\n[10] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[11]Ahn K, Cheng X, Daneshmand H, et al. Transformers learn to implement preconditioned gradient descent for in-context learning[J]. arXiv preprint arXiv:2306.00297, 2023.\n\n[12] Dai B, He N, Dai H, et al. Provable Bayesian inference via particle mirror descent[C]//Artificial Intelligence and Statistics. PMLR, 2016: 985-994.\n\n[13] Raskutti G, Mukherjee S. The information geometry of mirror descent[J]. IEEE Transactions on Information Theory, 2015, 61(3): 1451-1457.\n\n[14] Hendel R, Geva M, Globerson A. In-context learning creates task vectors[J]. arXiv preprint arXiv:2310.15916, 2023.\n\n\n[15] Black S, Biderman S, Hallahan E, et al. Gpt-neox-20b: An open-source autoregressive language model[J]. arXiv preprint arXiv:2204.06745, 2022.\n\n[16] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.\n\n[17] N. Wies, Y. Levine, A. Shashua. The learnability of in-context learning. arXiv preprint arXiv:2303.07895 (2023)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575384651,
                "cdate": 1700575384651,
                "tmdate": 1700577663993,
                "mdate": 1700577663993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gswd5bJjds",
                "forum": "J9wzKfgZVK",
                "replyto": "nx09ZqEqwX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Reviewer_MVTm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Reviewer_MVTm"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the detailed evaluation and results. I have increased my score to 5. I think the discussions and results will significantly strengthen the current paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587894704,
                "cdate": 1700587894704,
                "tmdate": 1700587894704,
                "mdate": 1700587894704,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mvPPAlGvgX",
            "forum": "J9wzKfgZVK",
            "replyto": "J9wzKfgZVK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_xbQq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6732/Reviewer_xbQq"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a study of in-context learning (ICL) through a Bayesian view and addresses related questions. They first show that large language models perform ICL by adopting a Bayesian model averaging (BMA) algorithm during inference by showing that a prediction for a prompt from a perfectly pretrained model equals the prediction of BMA. Then, to measure the ICL performance, the authors consider ICL regret as a performance metric that measures the difference from prediction to the best hidden concept. The regret is shown to converge with increasingly more examples. To show the connection between the transformer network and the BMA algorithm, the authors define a variant of the attention layers, which then show that the modified attention mechanism encodes the BMA algorithm for the Gaussian linear model. For the sequence length going to infinity, the provided attention variant coincides with the softmax attention.\nAdditionally, the authors provide the pretraining analysis that can measure the pretraining error with approximation and generalization errors. Under certain smoothness conditions, approximation error converges with bigger models. Finally, for an imperfect pretrained model, the authors can successfully upper-bound the ICL regret."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The progression and steps make sense.\n\n+ The theoretical results seem to be new and are a nice addition to ICL setting.\n\n+ The theoretical contribution seems to be a generalization for some ICL settings assumed in previous works."
                },
                "weaknesses": {
                    "value": "- How big is the ICL regret in practice? It is hard to tell how tight the bound is in practice. How can it be measured for LLMs?\nSince there is no application of the results, it is hard to tell how far the bound is really in practice. \n\n- How can we in practice measure the validation performance of ICL?"
                },
                "questions": {
                    "value": "- What is sequentially mean? Are we not really doing sequential but all at once prompting, which is different from iterative prompting?\n\n- What is the \"wrong\" input-ouput mapping? There is no real wrong or right here. How can you get $z_*$?\n\n- Is KL divergence the best to measure distribution gap, which are key in measuring the generalization gap? What is the embedding space it has to use? Is there a better divergence metrics to measure that?\n\n- Even though the progression and paper is smooth, it would be nice to add visualization to main paper rather than in Appendix.\n\n- The analysis only involves pretraining in relation to the ICL performance. However, large language models also have fine-tuning distribution involved as an \"intermediate\" between the two aforementioned stages. Should that introduce some +/- gaps in divergence?\n\n- Where do we get the hidden concept in the LLM?\n\n**Minor:**\n\n+ Why LLMs the ability for ICL (page1)\n\n+ Hiddn Markov Model (page 3)\n\n+ Variables are undefined (or defined later than introduced):\n  + N and d, d_k,  d_v \n  + What is W?\n  + Large T and small t difference?\n+ Pertaining distribution (page 7)\n+ xIn section (page 9)\n+ correctly inference (page 9)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6732/Reviewer_xbQq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6732/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829175418,
            "cdate": 1698829175418,
            "tmdate": 1699636773926,
            "mdate": 1699636773926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dGlbvvcsRh",
                "forum": "J9wzKfgZVK",
                "replyto": "mvPPAlGvgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbQq: Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We address the major concerns in the following.\n\n**Added Experimental Results**\n\nWe conduct five experiments to verify our theoretical findings, including the Bayesian view (Proposition 4.1 and Eqn. (4.7)), the regret upper-bounded in Corollary 4.2 and  Theorem 6.2, and the constant ratio between $\\mathtt{attn}_{\\dagger}$ and $\\mathtt{attn}$ in Proposition 4.3. The results and implementation details are provided in Appendix C and D in the revised manuscript. We will list a part of numerical results as table below.\n\n* **Verification of the Bayesian View**\n\nTable 1:\n\n|                 | zs-hc | ICL-0 | ICL-1 | ICL-5 | ICL-10 |\n|-----------------|-------|-------|-------|-------|--------|\n| antonym         | 0.736 | 0.000 | 0.296 | 0.651 | 0.675  |\n| country-capital | 0.825 | 0.048 | 0.881 | 0.905 | 0.952  |\n| present-past    | 0.847 | 0.082 | 0.820 | 0.967 | 0.967  |\n\nTo verify the Bayesian view that we adopt in the paper, we implemented two experiments. In the first experiment, we explicitly construct the hidden concept vectors that are found by LLMs. Motivated by (4.7) and Proposition 4.3, we construct the hidden concept vector as the average sum over prompts of the values of twenty selected attention heads, i.e., we compress the hidden concept into a vector with dimension $4096$. To demonstrate the effectiveness of the constructed hidden concepts, we add these hidden concept vectors at a layer of LLMs when the model resolves the prompt with zero-shot. In Table 1, \"zs-hc\" refers to the results of LLMs that infers with learned hidden concept vectors and zero-shot prompt, and \"ICL-$i$\" refers to the results of LLMs prompted with $i$ examples. This table report the accuracy of LLMs under different prompts. We consider the tasks of finding antonyms, finding the capitals of countries, and finding the past tense of words. The results indicate that the LLMs with learned hidden concept vectors have **comparable performance** with the LLMs prompted with several examples. This indicates that the learned hidden concept vectors are indeed efficient **compression of the hidden concepts**, which proves that LLMs deduce hidden concepts for ICL. This result strongly corroborates with (4.7). \n\nTable 2:\n\n|          | 2 examples | 6 examples | 10 examples | 14 examples | 18 examples |\n|----------|------------|------------|-------------|-------------|-------------|\n| accuracy | 0.000      | 0.917      | 0.917       | 1.000       | 1.000       |\n\n\nIn the second experiment, we aim to verify that LLMs implement inference with the Bayesian framework, not with gradient descent [1,2,3] on some tasks. We prompt the LLMs with the history data of a set of similar multi-armed bandit instances with $100$ arms, and let LLMs indicate which arm to pull in a similar new bandit instance. In these similar bandit instances, there is an informative arm, whose reward is exactly the index of the arm with the highest rewards. We also provide the side information that \"Some arm may directly tell you the arm with the highest reward, even itself does not have the highest reward\". In each example provided in the prompt, there are the rewards of six arms, including the informative arm and the best arm, in one bandit instance. As shown in Table 2, the LLMs can efficiently implement ICL even with only $6$ examples. We note that the gradient descent algorithms in the previous works cannot explain this performance, since the gradient descent algorithms need **at least $100$ data points**, where each data point is the reward of one arm, to learn. In contrast, the Bayesian view can clearly explain Table 2, where LLMs make use of the side information to calculate **a better posterior** for the bandit instance. Thus, LLMs have good ICL performance in this setting.\n\n\n* **Verification of the Regret Bound**\n\nTable 3:\n|                        | 0 examples | 10 examples | 20 examples | 30 examples | 40 examples |\n|------------------------|------------|-------------|-------------|-------------|-------------|\n| $t\\cot\\text{regret}$   | 1.006      | 8.204       | 10.505      | 10.529      | 10.535      |\n| squared error of query | 1.006      | 0.528       | 0.0217      | 0.001       | 0.001       |\n\nTo verify Corollary 4.2 and Theorem 6.2, we implement experiments to evaluate the regret in two settings. In the first setting, the LLMs is trained for the linear regression task from scratch, which is a representative setting studied in [1,4]. The examples in the prompt are $\\lbrace(x _i,y _i)\\rbrace _{i=1}^{N}$, where $x _i\\in\\mathbb{R}^d$, $d=20$ and $y _{i}=w^{T}x _{i}$ for some $w$ sampled from Gaussian distribution. Given the Gaussian model, we adopt the squared error to approximate the logarithm of the probability. Then the $t\\times \\text{regret}$ of the LLMs can be well approximated by the sum of the squared error till time $t$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573558279,
                "cdate": 1700573558279,
                "tmdate": 1700573558279,
                "mdate": 1700573558279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WOLDSHy3Id",
                "forum": "J9wzKfgZVK",
                "replyto": "mvPPAlGvgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbQq: Part 2"
                    },
                    "comment": {
                        "value": "The results in Table 3 strongly corroborate our theoretical findings. First, the results verify our claim in Corollary 4.2 and Theorem 6.2 that $t\\cdot \\text{regret}$ can be **upper bounded by a constant.** Second, the line of squared error indicates that the ICL of LLMs only has a significant error when **$T\\leq d$**, i.e., the regret **only increases in this region**. Thus, the regret of the ICL by LLMs is at most linear in $O(d/T)$. From the view of our theoretical result, discretizing the set $\\lbrace z\\in\\mathbb{R}^{d} | \\|z\\| _{2}\\leq d \\rbrace$ with approximation error $\\delta>0$ will result in a set with $(C/\\delta)^{d}$ elements, where $C>0$ is an absolute constant. Corollary 4.2 and Theorem 6.2 imply that the regret is the sum of the **$\\log |\\mathfrak{Z}|/T = d\\log (C/\\delta)/T$** and the pretraining error, which matches the simulation results.\n\nTable 4:\n|                    | 50 tokens | 100 tokens | 150 tokens | 200 tokens | 250 tokens |\n|--------------------|-----------|------------|------------|------------|------------|\n| $y= x$             | 0.001     | 0.002      | 0.002      | 0.017      | 0.020      |\n| $y=\\sin (x)$       | 0.105     | 3.842      | 3.919      | 4.114      | 4.190      |\n| $y=x\\cdot\\sin (x)$ | 0.012     | 0.018      | 0.030      | 0.041      | 0.048      |\n\nIn the second experiment, we directly evaluate the regret of pretrained LLMs on the function value prediction task. The prompt consists of the values of a function on the points with fixed intervals. Since the values are real numbers, we adopt the method in [5] to transfer a real number to a token sequence. For the pretrained model, we cannot calculate $\\mathbb{P}(r _{i}|\\text{prompt} _{i-1},z)$ due to the unknown nominal distributions. Thus, we calculate the cumulative negative log-likelihood $\\text{CNLL} _{t}=-\\sum _{i=1}^{t}\\hat{\\mathbb{P}}(r _{i}|\\text{prompt} _{i-1})$, and $\\text{CNLL} _{t}/t$ is an upper bound of the regret. In Table 4, we indicate the cumulative negative log-likelihoods of predicting the values of three functions. The results show that the cumulative negative log-likelihoods are stepped, which means that the cumulative negative log-likelihoods are **upper-bounded by constants in a long period**. This corroborates with Corollary 4.2 and Theorem 6.2. \n\n* **Verification of the Constant Ratio between $\\mathtt{att}_{\\dagger}$ and $\\mathtt{att}$**\n\nTable 5:\n|                         | 10 examples | 1000 examples | 2000 examples | 3000 examples | 4000 examples |\n|-------------------------|-------------|---------------|---------------|---------------|---------------|\n| 75 th percentile for $d=2$ | 2.312       | 2.347         | 2.327         | 2.299         | 2.297         |\n| 25 th percentile for $d=2$ | 0.792       | 2.110         | 2.156         | 2.168         | 2.191         |\n| 75 th percentile for $d=3$ | 2.535       | 3.406         | 3.388         | 3.355         | 3.330         |\n| 25 th percentile for $d=3$ | 0.569       | 2.904         | 2.957         | 3.032         | 3.078         |\n\nTo verify Proposition 4.3, we directly calculate the ratio between $\\mathtt{att} _{\\dagger}$ and $\\mathtt{att}$. We consider the case $d _v = 1 $ and $d _k=d$ for some $d>0$. The entries in $K$ of (4.8) are i.i.d. samples of Gaussian distribution, and the $i-$th entry of $V$ is calculated as the inner product between a Gaussian vector and the $i-$th column. Table 5 shows the results for $d=2$ and $d=3$. It shows that the ratio between $\\mathtt{att} _{\\dagger}$ and $\\mathtt{att}$ will converge to a constant. This constant depends on the dimension $d$, which originates from Proposition F.1.\n\n**Validation Performance of ICL**\n\nWe are not very clear about the meaning of \u201cvalidation performance\u201d in the questions. We would like to ask the reviewer to clarify the question. Here we attempt to answer this question with some potential explanations of it.\n\nIf \u201cvalidation performance\u201d means validating the theoretical findings in the empirical results, our simulations in the modified manuscript provide a detailed response to it.  Our simulation results verify the Bayesian view (Proposition 4.1 and (4.7)), Corollary 4.2, and Proposition 4.3. The detailed analysis is provided in Appendix C of the revised manuscript.\n\nIf \u201cvalidation performance\u201d means that we would like to validate the ICL performance of a given LLM, then we can measure the ICL performance of the LLM by splitting the data. Given a set of sentences, we can split them into a training set and a testing set. During the training process, we could implement cross-validation to tune the parameters. The performance of LLMs can be tested on the test set."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573713739,
                "cdate": 1700573713739,
                "tmdate": 1700573766274,
                "mdate": 1700573766274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Go4tZsWvpP",
                "forum": "J9wzKfgZVK",
                "replyto": "mvPPAlGvgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbQq: Part 3"
                    },
                    "comment": {
                        "value": "**Explanation of Word \"Sequentially\"**\n\nThe word \"sequentially\" comes from the fact that we view the ICL from the **online learning perspective**. We would like to understand how the ICL performance scales with an increasing number of examples in the prompt. For example, in Corollary 4.2, we prompt the LLMs with the $\\text{prompt}_{t}$ that has $t$ examples in time $t$. We evaluate the regret in this setting. This is the iterative prompting.\n\n**Explanation of Wrong Input-Output Mapping**\n\nThe word \"wrong\" refers to the fact that **input-output pairing is not correct**. We would like to give an example of the wrong input-output pairing. Assume that we want LLMs to predict the classification of biological categories. The prompts with correct input-output mapping can be \"Cats are animals, pineapples are plants, mushrooms are\", while the prompt with wrong input-output mapping can be \"Cats are plants, pineapples are animals, mushrooms are\". Empirically, LLMs are able to derive the **correct answer even with random labels**, and the input-output pattern in the prompt is essential for efficient ICL[6]. Here we provide the **theoretical understanding** for these phenomena under the distinguishability assumption. We show that the pretrained LLMs can perform efficient ICL even with wrong labels, including the random label as a special case, when the distinguishability assumption holds. \n\n**Why we use KL divergence**\n\nSince we adopt the cross-entropy loss to train the LLMs, KL divergence is a natural choice for it. When quantifying the training performance, we measure it via the total variation distance of the conditional probability in the **token space**, not in the embedding space. We leave the exploration of other divergence metrics as the future works.\n\n**The Discussion of Finetuning**\n\nWe note that the LLMs learned from the pretraining are powerful enough. For example, LLMs are able to efficiently implement the time-series tasks in a zero-shot manner[5]. Then we will discuss the influence of two kinds of finetuning. \n\nThe first kind of finetuning is to optimize the parameters of LLMs on a specific task. We note that our performance guarantee in Theorem 5.3 takes expectation with respect to the pretraining distribution. After the fine-tuning process, the distribution here will be closer to the **token distribution of the desired task**. This will improve the performance of LLMs on this task. \n\nThe other kind of finetuning is instruction finetuning, which aims to align the behavior of LLMs to the human preference. People first learn a reward function from the human feedback and use the learned reward to tune LLMs with reinforcement learning techniques. The analysis of this process requires the additional techniques about reinforcement learning, and we leave this as future work.\n\n**How to get the hidden concept in the LLMs**\n\nFirst, we would like to specify the reasons why LLMs can generate the hidden concepts. There are two main reasons: 1. The pretraining data of LLMs comes from the multiple tasks. There are a sufficient number of concepts presented in the pretraining data. 2. The LLMs are large enough to extract the hidden concepts from the prompt. In Proposition 4.3, we theoretically show that the **attention head** contains the hidden concept information in Gaussian linear model.\n\nSecond, we would like to discuss how to get the hidden concept in the LLM in experiments. In our experiment section in Appendix C, we extract the **hidden concept vector as the average sum over prompts of the values of twenty selected attention heads**, i.e., we compress the hidden concept into a vector. This is motivated by Proposition 4.3. To demonstrate the effectiveness of the constructed hidden concepts, we add these hidden concept vectors at a layer of LLMs when the model resolves the prompt with zero-shot. The experimental results show that the LLMs with hidden concept vector in the zero-shot setting have comparable performance with the LLMs prompted with several examples. This verifies that the constructed hidden concept vector indeed **contains the essential information about the hidden concept**. This method is recently proposed and tested in [7]. We note that there are other methods to extract the hidden concept vectors. According to [8], we can also learn the hidden concept deduced by LLMs as token values. We can represent the hidden concept as some special tokens. The value of these tokens are trained by maximizing the probability of the correct answer when the LLMs are prompted with some examples. These learned tokens are shown to be effective in the prompt design.\n___\nWe thank the reviewer again for reading our response. We appreciate it if the reviewer could consider raising the score, if our rebuttal has addressed your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574214478,
                "cdate": 1700574214478,
                "tmdate": 1700577733664,
                "mdate": 1700577733664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W26i64gE6i",
                "forum": "J9wzKfgZVK",
                "replyto": "mvPPAlGvgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope this message finds you well. As the rebuttal period will close soon, we would like to reach out to you and inquire if you have any additional concerns regarding our paper. We would be immensely grateful if you could kindly review our responses to your comments. This would allow us to address any further questions or concerns you may have before the rebuttal period concludes."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629779302,
                "cdate": 1700629779302,
                "tmdate": 1700629779302,
                "mdate": 1700629779302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f3sB8nLTWn",
                "forum": "J9wzKfgZVK",
                "replyto": "W26i64gE6i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Reviewer_xbQq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Reviewer_xbQq"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response and additional results.\n\nThe KL-divergence is used to measure generalization gap. However, the recent work has shown that KL divergence on the token space cannot explain performance across benchmarks [9]. \n\nHow does your result 6.3 relates to findings in [10, 11] that models generally do not perform well on flipped labels and larger models actually follow the input-output mapping rather than \"correctly\" answering with the ground truth label.\n\nFor validation performance, I meant test performance, since in real life there is no training or test dataset that the user's query will belong to.\n\n[9] Gregory Yauney, Emily Reif, David Mimno, Data Similarity is Not Enough to Explain Language Model Performance, https://arxiv.org/pdf/2311.09006.pdf\n[10] Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane Dwivedi-Yu, Active Learning Principles for In-Context Learning with Large Language Models, https://arxiv.org/abs/2305.14264\n[11] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma, Larger language models do in-context learning differently, https://arxiv.org/abs/2303.03846"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648987500,
                "cdate": 1700648987500,
                "tmdate": 1700648987500,
                "mdate": 1700648987500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DbProwWcUD",
                "forum": "J9wzKfgZVK",
                "replyto": "mvPPAlGvgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6732/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Questions of Reviewer xbQq: Part 2"
                    },
                    "comment": {
                        "value": "To test the performance of the estimate $\\hat{f}$, if we sample a single point $x=0.5$, then there is no performance guarantee. However, we can sample a set of points $\\tilde{x} _i\\sim \\text{unif}([0,1])$ for $i\\in[\\tilde{N}]$. Here $\\tilde{x} _i$ for $i\\in[\\tilde{N}]$ are different from $x _i$ for $i\\in[N]$ with probability $1$. Then we have that\n\\begin{align*}\n    \\frac{1}{\\tilde{N}}\\sum _{i=1}^{\\tilde{N}} (\\hat{f}(\\tilde{x} _i)-2\\tilde{x} _i)^2\\rightarrow 0 \\text{ in probability as }\\tilde{N}\\rightarrow\\infty.\n\\end{align*}\n\n___\n\nWe would like to inquire if you have any additional concerns regarding our paper, and we will try our best to address any further questions or concerns you may have before the rebuttal period concludes.\n___\n\n**Reference**\n\n[1] Gregory Yauney, Emily Reif, David Mimno, Data Similarity is Not Enough to Explain Language Model Performance\n\n\n[2] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma, Larger language models do in-context learning differently\n\n[3] Katerina Margatina, Timo Schick, Nikolaos Aletras, Jane Dwivedi-Yu, Active Learning Principles for In-Context Learning with Large Language Models"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6732/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679533746,
                "cdate": 1700679533746,
                "tmdate": 1700679658636,
                "mdate": 1700679658636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]