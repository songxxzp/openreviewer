[
    {
        "title": "Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation"
    },
    {
        "review": {
            "id": "Ynq09cMMtR",
            "forum": "QuFHei1vuE",
            "replyto": "QuFHei1vuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to Video Frame Interpolation (VFI) termed \"distance indexing,\" aiming to improve the precision of object movements in interpolated frames. Instead of previous \"time indexing,\" the proposed method gives the network explicit hints about the distance an object has traveled between the start and end frames, reducing the uncertainty tied to object speeds. To address directional ambiguity in long-range motion, the paper also introduces an iterative reference-based estimation strategy that breaks down a long-range prediction into several short-range steps."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- motivation is clear. \n- good performance. By employing \"distance indexing,\" the paper significantly improves the precision of object movements in interpolated frames over traditional \"time indexing\" methods. Since the training strategy can be played as a plug-and-play strategy, it can be extended to another video frame interpolate methods.\n- The proposed method provides some analysis for some possible reasons to cause blur in video frame interpolate task."
                },
                "weaknesses": {
                    "value": "- The training and inference time complexity will sightly increasing due to introducing the proposed tech."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "na"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Reviewer_BPE4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486205005,
            "cdate": 1698486205005,
            "tmdate": 1699635959963,
            "mdate": 1699635959963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UXNKggCvtK",
                "forum": "QuFHei1vuE",
                "replyto": "Ynq09cMMtR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing our work's motivation, performance, and interpretability.\n\nBefore addressing specific questions, if there are any uncertainties or if a refresher on the core concepts of our paper's methodology is needed, we kindly invite you to review our comprehensive response to Reviewer aknz, where we offer a clearer and more detailed explanation:\n\n`First and foremost, it's essential to clarify that ... similar to that mentioned TTS paper (Wang, Yuxuan, et al.).`\n\n\n\n> The training and inference time complexity will sightly increasing due to introducing the proposed tech.\n\nWe will incorporate the following details into our final paper: \n\n**Distance indexing**: Transitioning from time indexing ($[T]$) to distance indexing ($[D]$) does not introduce extra computational costs during the inference phase, yet significantly enhancing image quality. In the training phase, the primary requirement is a one-time computation (offline) of distance maps for each image triplet. \n\n**Iterative reference-based estimation ($[D,R]$)**: Given that the computational overhead of merely expanding the input channel, while keeping the rest of the structure unchanged, is negligible, the computational burden during the training phase remains equivalent to that of the $[D]$ model. While during inference, the total consumption is equal to the number of iterations $\\times$ the consumption of the $[D]$ model. We would like to highlight that this iterative strategy is optional: Users can adopt this strategy at will when optimal interpolation results are demanded and the computational budget allows. Our experiments (Table 2) show that 2 iterations, *i.e.,* doubling the computational cost, yield cost-effective improvements.\n\n\n\nShould you have any further questions about our research, including those raised by other reviewers, we are more than happy to engage in further discussion and provide explanations!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934304878,
                "cdate": 1699934304878,
                "tmdate": 1699934304878,
                "mdate": 1699934304878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SuTF9qU4GV",
            "forum": "QuFHei1vuE",
            "replyto": "QuFHei1vuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel and controllable approach for video frame interpolation. In contrast to existing VFI models that typically rely on time indexing to generate intermediate frames, the authors introduce a distance indexing strategy to address the problem of velocity ambiguity. Additionally, they incorporate an iterative reference-based estimation method to improve the quality of the interpolated frames. The proposed method can be easily integrated into existing VFI models as a plug-and-play solution. As a result, this work not only enhances the performance of VFI models but also provides a new video editing tool."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper presents an intriguing and intuitively effective solution for video frame interpolation.\n2. The authors have conducted extensive experiments to thoroughly evaluate the effectiveness of the proposed method.\n3. The paper is well-written and provides sufficient explanations of the details, making it easy to follow."
                },
                "weaknesses": {
                    "value": "1. It would be even more beneficial to evaluate the proposed method on other high-resolution benchmarks for testing, particularly to demonstrate its superiority in handling larger motion scenarios.\n2. Additionally, please specify which three out of the seven frames were used for testing purposes.\n3. I am also interested in exploring the robustness of the proposed method when dealing with inaccurate optical flow estimation. Additionally, I am curious to see how well the model performs when incorporating it with another optical flow estimator, such as FlowNet, to provide the distance ratio map.\n4. Provide a comparison of the inference cost between different methods, including additional optical flow estimation and iterative reference-based interpolation. It is important to note that the iterative reference-based strategy may involve running the interpolation multiple times, resulting in a significant increase in the overall inference time."
                },
                "questions": {
                    "value": "1. Evaluation of the proposed method on benchmarks specifically designed for high-resolution interpolation.\n2. Analysis of the robustness of the method in scenarios where the optical flow estimation is inaccurate."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Reviewer_PtYf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747493986,
            "cdate": 1698747493986,
            "tmdate": 1699635959849,
            "mdate": 1699635959849,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pKv6hjsGdF",
                "forum": "QuFHei1vuE",
                "replyto": "SuTF9qU4GV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for recognizing the effectiveness of our approach and for the constructive suggestions. We will first address the concerns raised, except those requiring experimental validation.\n\nBefore addressing specific questions, if there are any uncertainties or if a refresher on the core concepts of our paper's methodology is needed, we kindly invite you to review our comprehensive response to Reviewer aknz, where we offer a clearer and more detailed explanation:\n\n`First and foremost, it's essential to clarify that ... similar to that mentioned TTS paper (Wang, Yuxuan, et al.).`\n\n\n\n> Additionally, please specify which three out of the seven frames were used for testing purposes.\n\nNotice that, within each septuplet, the i-th frame corresponds to $I_{i/6}$ in Fig. 5 ($i=0,1,\\cdots,6$). \nWe utilized the first ($I_0$) and last ($I_1$) frames as inputs to predict the rest five frames. We only show $I_{2/6},I_{3/6},I_{4/6}$ in Fig. 5 for visualization, but we report the results for all five intermediate frames in quantitative results.\nThis detail will be explicitly clarified in the final version of our paper.\n\n\n\n> Provide a comparison of the inference cost between different methods, including additional optical flow estimation and iterative reference-based interpolation. It is important to note that the iterative reference-based strategy may involve running the interpolation multiple times, resulting in a significant increase in the overall inference time.\n\n**Distance indexing**: Transitioning from time indexing ($[T]$) to distance indexing ($[D]$) does not introduce extra computational costs during the inference phase, yet significantly enhancing image quality. In the training phase, the primary requirement is a one-time computation (offline) of distance maps for each image triplet. \n\n**Iterative reference-based estimation ($[D,R]$)**: Given that the computational overhead of merely expanding the input channel, while keeping the rest of the structure unchanged, is negligible, the computational burden during the training phase remains equivalent to that of the $[D]$ model. While during inference, the total consumption is equal to the number of iterations $\\times$ the consumption of the $[D]$ model. We would like to highlight that this iterative strategy is optional: Users can adopt this strategy at will when optimal interpolation results are demanded and the computational budget allows. Our experiments (Table 2) show that 2 iterations, *i.e.,* doubling the computational cost, yield cost-effective improvements.\n\nWe will include these details in our final paper for clarity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934105037,
                "cdate": 1699934105037,
                "tmdate": 1699934105037,
                "mdate": 1699934105037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nu99cTuEhi",
                "forum": "QuFHei1vuE",
                "replyto": "SuTF9qU4GV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It would be even more beneficial to evaluate the proposed method on other high-resolution benchmarks for testing, particularly to demonstrate its superiority in handling larger motion scenarios.\n\nThank you for your constructive suggestions! Using RIFE as an example, we present experimental comparisons for 8x interpolation on the Adobe240 [1] and X4K1000FPS [2] benchmarks with uniform maps. The results are provided below for reference:\n\n| Adobe240 (x8)   | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------- | ------- | ------ | ------ | ------ |\n| RIFE [T]        | 30.2430 | 0.9386 | 0.0729 | 5.2060 |\n| RIFE [D]$_{u}$  | 30.4704 | 0.9384 | 0.0565 | 4.9735 |\n| RIFE [D,R]$_{u}$ | 30.2961 | 0.9372 | 0.0542 | 4.9068 |\n\n| X4K1000FPS (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ----- | ------- | ------ | ------ | ------ |\n| RIFE [T]        | 36.3557 | 0.9667 | 0.0403 | 7.1302 |\n| RIFE [D]$_{u}$  | 36.7964 | 0.9642 | 0.0315 | 6.9364 |\n| RIFE [D,R]$_{u}$ | 36.2588 | 0.9642 | 0.0315 | 6.9242 |\n\nDistance indexing [D] and iterative reference-based estimation [R] strategies can consistently help improve the perceptual quality. In addition, it is noteworthy that [D]$_u$ is better than [T]$_u$ in terms of the pixel-centric metrics like PSNR, showing that the constant speed assumption (uniform distance maps) holds well on these two easier benchmarks. \n\nWe also show 16x interpolation on X4K1000FPS with larger temporal distance as follows:\n\n| X4K1000FPS (x16) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| -------- | ------- | ------ | ------ | ------ |\n| RIFE [T]         | 31.0401 | 0.9102 | 0.1043 | 7.2152 |\n| RIFE [D]$_u$   | 31.6000 | 0.9141 | 0.0944 | 6.9531 |\n| RIFE [D,R]$_u$  | 31.5195 | 0.9220 | 0.0786 | 6.9265 |\n\nThe results highlight that the benefits of our strategies are more pronounced with increased temporal distances.\n\n> I am also interested in exploring the robustness of the proposed method when dealing with inaccurate optical flow estimation. Additionally, I am curious to see how well the model performs when incorporating it with another optical flow estimator, such as FlowNet, to provide the distance ratio map.\n\nSince the inference speed of FlowNet is too slow, we use GMFlow [3], a newer optical flow estimator, to precompute distance maps for comparison. The results are as follows:\n\n| Vimeo90K (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------- | ------- | ------ | ------ | ------ |\n| RIFE [T]          | 28.2175 | 0.9116 | 0.1048 | 6.6626 |\n| RIFE [D]$_u$      | 27.2927 | 0.8983 | 0.1009 | 6.4492 |\n| RIFE [D,R]$_u$     | 26.9583 | 0.8952 | 0.0915 | 6.2799 |\n\nOur strategies still lead to consistent improvement on perceptual metrics. However, this more recent and performant optical flow estimator does not introduce improvement compared to RAFT. A likely explanation is that since we quantify the optical flow to $[0,1]$ scalar values for better generalization, our training strategies are less sensitive to the precision of the optical flow estimator.\n\nP.S., the same experimental conclusions can be derived from other SOTA models like AMT as follows:\n\n| Adobe240 (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------ | ------- | ------ | ------ | ------ |\n| AMT [T]   | 29.5985 | 0.9333 | 0.0935 | 5.4004 |\n| AMT [D]$_u$  | 29.7449 | 0.9341 | 0.0700 | 5.0037 |\n| AMT [D,R]$_u$  | 29.4671 | 0.9329 | 0.0630 | 4.9392 |\n\n| X4K1000FPS (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------- | ------- | ------ | ------ | ------ |\n| AMT [T]         | 35.6691 | 0.9612 | 0.0598 | 7.3296 |\n| AMT [D]$_u$    | 36.6280 | 0.9631 | 0.0412 | 7.0355 |\n| AMT [D,R]$_u$   | 36.3044 | 0.9645 | 0.0355 | 6.8704 |\n\n| X4K1000FPS (x16) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------- | ------- | ------ | ------ | ------ |\n| AMT [T]       | 31.6443 | 0.9242 | 0.1222 | 7.6546 |\n| AMT [D]$_u$   | 32.5098 | 0.9352 | 0.0816 | 7.1798 |\n| AMT [D,R]$_u$    | 31.9543 | 0.9311 | 0.0771 | 7.0707 |\n\n| Vimeo90k (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------ | ------- | ------ | ------ | ------ |\n| AMT-S [T]         | 28.5202 | 0.9196 | 0.1011 | 6.8659 |\n| AMT-S [D]$_u$     | 27.0252 | 0.9004 | 0.1006 | 6.6257 |\n| AMT-S [D,R]$_u$    | 26.5586 | 0.8928 | 0.0937 | 6.4910 |\n\nWe will incorporate the above discussions into the final paper.\n\nReferences:\n\n[1] Su, Shuochen, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, and Oliver Wang. \"Deep video deblurring for hand-held cameras.\" In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 1279-1288. 2017.\n\n[2] Sim, Hyeonjun, Jihyong Oh, and Munchurl Kim. \"Xvfi: extreme video frame interpolation.\" In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 14489-14498. 2021.\n\n[3] Xu, Haofei, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. \"Gmflow: Learning optical flow via global matching.\" In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 8121-8130. 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291006195,
                "cdate": 1700291006195,
                "tmdate": 1700291442698,
                "mdate": 1700291442698,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oDTS6Miza1",
            "forum": "QuFHei1vuE",
            "replyto": "QuFHei1vuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
            ],
            "content": {
                "summary": {
                    "value": "Existing time indexing-based VFI methods suffer from the speed ambiguity during training, and the authors propose to instead use a distance indexing training mechanism to reduce the inconsistency of the acceleration of each objects, which may damage network training. In addition to address the direction ambiguity, the authors propose an iterative estimation mechanism. The authors conducted experiments on four state-of-the-art methods to validate the significant enhancement that the proposed plug-and-play modules bring to VFI. In addition, the authors show an exciting VFI DEMO that implements motion regions and trajectories editable in conjunction with SAM."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The motivation of why we need distance indexing for VFI tasks and the details of the two proposed modules are well presented, and the enhancements they bring to the existing VFI models are well illustrated.\n2. The VFI DEMO that implements motion regions and trajectories editable in conjunction with SAM, which is very exciting."
                },
                "weaknesses": {
                    "value": "1. The proposed distance indexing resolves the scalar velocity ambiguity in training, but nor in inference. Although the authors show in the experiments (Fig. 9) that the results achieved by using only uniform map in inference can already be satisfactory for humans, I think it would be better to explain this further in the Methods section.\n2. Does the iterative estimation mechanism require any changes to the structure of the existing model, especially the input and output parts? If so, is this consistent with the plug-and-play claim? The details need to be clarified.\n3. The training details of iterative estimation need to be clarified as well. Is D_{t/2} obtained in training using the optical flow computed from the ground-truth I_{t/2} in the dataset? Is it a uniform map of D=t/2 in inference?\n4. The additional consumptions that the proposed new mechanisms bring to model training and inference need to be discussed."
                },
                "questions": {
                    "value": "1. Does iterative estimation cause problems with error accumulation, e.g. the first time the direction is wrong, making it harder to get back to the right one the second time? Failure cases may be needed.\n2. Is manipulable interpolation of optical flow based VFI methods also possible?\n3. Would changing to a newer optical flow estimation method result in more improvements? Especially for the exception AMT-S."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Reviewer_1WoD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810407999,
            "cdate": 1698810407999,
            "tmdate": 1699635959773,
            "mdate": 1699635959773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mvMuwiPRQo",
                "forum": "QuFHei1vuE",
                "replyto": "oDTS6Miza1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging our contributions and for constructive feedback. We will first address the concerns raised, except those requiring experimental validation.\n\n> The proposed distance indexing resolves the scalar velocity ambiguity in training, but nor in inference. Although the authors show in the experiments (Fig. 9) that the results achieved by using only uniform map in inference can already be satisfactory for humans, I think it would be better to explain this further in the Methods section.\n\nThank you for raising this concern. We kindly invite Reviewer 1WoD to refer to our detailed response to Reviewer aknz, where we present a more intuitive and detailed explanation:\n\n`First and foremost, it's essential to clarify that ... similar to that mentioned TTS paper (Wang, Yuxuan, et al.).`\n\n\n\n> Does the iterative estimation mechanism require any changes to the structure of the existing model, especially the input and output parts? If so, is this consistent with the plug-and-play claim? The details need to be clarified.\n\nThe adaptation required is merely an expansion of the input channel for each model to include additional reference inputs, while the output configuration remains the same. More precisely, the iterative reference-based estimation mechanism increases the original 1-dimensional $t$ map channel to a 5-dimensional format. This includes a 1-dimensional $D_t$ map, a 1-dimensional $D_{ref}$ map, and a 3-dimensional RGB reference $I_{ref}$. Since this modification does not affect the core architecture of the model, we maintain that it aligns with the plug-and-play principle.\n\n\n\n> The training details of iterative estimation need to be clarified as well. Is D_{t/2} obtained in training using the optical flow computed from the ground-truth I_{t/2} in the dataset? Is it a uniform map of D=t/2 in inference?\n\nThank you for your suggestion. Indeed, during training, $D_{ref}$ is derived using optical flow computed from the ground-truth at a time point corresponding to a randomly chosen reference frame, such as $t/2$. In the inference phase, we utilize a uniform map with $D=t/2$. We will ensure to clarify these details in the final version of our paper.\n\n\n\n>  The additional consumptions that the proposed new mechanisms bring to model training and inference need to be discussed.\n\n**Distance indexing**: Transitioning from time indexing ($[T]$) to distance indexing ($[D]$) does not introduce extra computational costs during the inference phase, yet significantly enhancing image quality. In the training phase, the primary requirement is a one-time computation (offline) of distance maps for each image triplet. \n\n**Iterative reference-based estimation ($[D,R]$)**: Given that the computational overhead of merely expanding the input channel, while keeping the rest of the structure unchanged, is negligible, the computational burden during the training phase remains equivalent to that of the $[D]$ model. While during inference, the total consumption is equal to the number of iterations $\\times$ the consumption of the $[D]$ model. We would like to highlight that this iterative strategy is optional: Users can adopt this strategy at will when optimal interpolation results are demanded and the computational budget allows. Our experiments (Table 2) show that 2 iterations, *i.e.,* doubling the computational cost, yield cost-effective improvements. We will include these details in our final paper for clarity.\n\n\n\n> Does iterative estimation cause problems with error accumulation, e.g. the first time the direction is wrong, making it harder to get back to the right one the second time? Failure cases may be needed.\n\nTo mitigate error accumulation, we always retain the original start and end frames as appearance references in each iteration, which prevents devastatingly diverging directions. The notion of \"wrong\" directions is somewhat subjective due to the multiple feasible directions. Our goal is to predict one plausible trajectory with all interpolated objects remaining clear. Exploring methods to recover any potential direction, or perhaps to learn a distribution of directions, is a direction for our future research.\n\n\n\n> Is manipulable interpolation of optical flow based VFI methods also possible?\n\nWe assume that Reviewer 1WoD is referring to the traditional method of directly warping images based on optical flow. While manipulating certain regions by scaling optical flow through time interpolation is feasible, this approach is limited to strictly linear warping or motion. In addition, this approach might result in the interpolated image having unfilled gaps or \u201choles\u201d."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933950778,
                "cdate": 1699933950778,
                "tmdate": 1699933950778,
                "mdate": 1699933950778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2qOkNKPB8G",
                "forum": "QuFHei1vuE",
                "replyto": "oDTS6Miza1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Would changing to a newer optical flow estimation method result in more improvements? Especially for the exception AMT-S.\n\nThank you for raising this interesting question!\n\nTo begin, we kindly remind that AMT-S's exception occurs when using only the iterative reference-based estimation. The distance indexing can stabilize the benefits gained from iterative reference-based estimation.\n\nWe utilize GMFlow [1] to precompute distance maps for comparison. The results are as follows:\n\n| Vimeo90K (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ----------------- | ------- | ------ | ------ | ------ |\n| RIFE [T]          | 28.2175 | 0.9116 | 0.1048 | 6.6626 |\n| RIFE [D]$_u$      | 27.2927 | 0.8983 | 0.1009 | 6.4492  |\n| RIFE [D,R]$_u$     | 26.9583 | 0.8952 | 0.0915 | 6.2799 |\n\nOur strategies still lead to consistent improvement on perceptual metrics. However, this more recent and performant optical flow estimator does not introduce improvement compared to RAFT. A likely explanation is that since we quantify the optical flow to $[0,1]$ scalar values for better generalization, our training strategies are less sensitive to the precision of the optical flow estimator.\n\n\n\nP.S., the same experimental conclusions can be derived from other SOTA models like AMT as follows:\n\n| Vimeo90k (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ----------------- | ------- | ------ | ------ | ------ |\n| AMT-S [T]         | 28.5202 | 0.9196 | 0.1011 | 6.8659 |\n| AMT-S [D]$_u$     | 27.0252 | 0.9004 | 0.1006 | 6.6257 |\n| AMT-S [D,R]$_u$    | 26.5586 | 0.8928 | 0.0937 | 6.4910 |\n\nWe will incorporate this discussion into the final paper.\n\nReference:\n\n[1] Xu, Haofei, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. \"Gmflow: Learning optical flow via global matching.\" In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 8121-8130. 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289482771,
                "cdate": 1700289482771,
                "tmdate": 1700291283925,
                "mdate": 1700291283925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u22Yw26krb",
            "forum": "QuFHei1vuE",
            "replyto": "QuFHei1vuE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles video frame interpolation (VFI). In particular, it attempts to address the velocity (speed and direction) ambiguity in VFI. To address the speed ambiguity, it proposes to employ a distance map (how far the object has traveled between start and end frames), instead of optical flow, to interpolate an intermediate frame. It also proposes an iterative reference-based estimation strategy to mitigate directional ambiguity. The proposed method could be used as a plug-and-play technique and experimental results are presented on the Vimeo-90K dataset."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ The paper attempts to address an important, yet underexplored, problem in video frame interpolation\n+ The 2D manipulation of frame interpolation using segmentation models such as SAM is quite interesting\n+ The paper reads fairly well\n+ Supplementary video is provided"
                },
                "weaknesses": {
                    "value": "Major issues\n\n* The technical formulation of the paper is flawed\n   \n    The authors propose to use a distance map (instead of optical flow) in an attempt to address the speed ambiguity in VFI. However, *instead of learning to approximate or predict the distance map*, they use the GT target frame to compute the distance map during training and use a uniform map (i.e. uniform speed) during inference. What is the point of attempting to address speed ambiguity during training, if the authors are assuming uniform speed at inference? How is the ambiguity being addressed at inference?\n\n    The same flawed logic is used in the iterative-reference-based estimation mechanism that is proposed to address directional ambiguity. The authors use the reference frame and its distance map iteratively to interpolate intermediate frames at training time. Let's assume the authors can use the previously interpolated frame as a reference frame at inference time. However, how do the authors get the corresponding distance map? If the uniform distance map assumption is implemented at every iteration step, what is the point of doing an iterative approach? How does that address the direction ambiguity? \n    \n* Several claims in the paper are not properly motivated and convincingly justified\n\n   The argument about Equation 4 in the main paper is not convincing. The proof in Appendix B is also based on a flawed assumption because most VFI works use L1 loss during training (not L2). \n\n    On page 4, the authors claim, \"Empirically, the model, when trained with this ambiguity, tends to produce a weighted average of possible frames during inference\". This is not entirely correct. In most optical flow-based methods, the model is tasked with approximating the flow, and the intermediate frame is simply obtained by warping the input frames with the predicted flow. This also applies to kernel-based methods. Hence, the ambiguity that happens in the estimated motion is not equivalent to predicting an average of possible frames. Moreover, the claim that the ambiguity gets worse for the multi-frame interpolation scheme is also not convincing. This is because each intermediate frame is predicted by time-interpolating the estimated flow between the input frames, i.e. each intermediate frame does not have a multitude of possibilities. \n\n    The whole point of the distance map and its benefit in addressing the speed ambiguity is also not clear. The key aspect of Equation 7 is actually the computed optical flows. why does the ratio of the flows provide extra comprehension during the training phase as the authors claim? What is the difference if we train the interpolation network by simply incorporating  $V_{0\\rightarrow 1}$ and $V_{0\\rightarrow t}$?\n \n    The argument about the convergence limits in Fig 6 is also not convincing. It is likely happening not because of what is claimed, i.e. addressing velocity ambiguity. It is simply because GT information is used during training in the proposed methods while traditional training does not use GT information.  \n\n* The experimental settings are not clear and the presented results are quite limited\n\n   Were all baseline trained using the same experimental setting? For instance, was RAFT used in all methods?\n\n   Why did the authors only perform experimental comparisons on the Vimeo-90 K dataset? There are several VFI benchmarks that are commonly used to compare VFI methods. Why did not the authors use those benchmarks as well?  \n\n* More experimental analyses are needed to verify the benefit of the proposed method\n    \n   As the proposed distance map computation is heavily dependent on optical flow, did the authors experiment with using optical flow estimation methods other than RAFT?\n\n   The problem the paper is trying to solve is more relevant in VFI situations where the temporal distance between input frames is relatively large. However, the authors do not show any experimental analysis in these scenarios. \n  \n  \nMinor issues\n\n* The technical novelty of the work is quite limited.\n* Please cite and discuss some relevant works [1,2].\n* What would be the added inference time by plugging the proposed method into existing methods?\n* The authors repeatedly use the term, \"Our observation ...\", yet fail to provide convincing empirical evidence\n\n\nReferences\n\n[1] Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation, CVPR 2023\n\n[2] Deep Iterative Frame Interpolation for Full-frame Video Stabilization, SIGGRAPH Asia 2019"
                },
                "questions": {
                    "value": "Please refer to the questions raised in the \"Weaknesses\" section and try to address them carefully."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission330/Reviewer_aknz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699046504884,
            "cdate": 1699046504884,
            "tmdate": 1699635959710,
            "mdate": 1699635959710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1Xf87dymH4",
                "forum": "QuFHei1vuE",
                "replyto": "u22Yw26krb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for raising your concerns about the technical formulation, claim justification, experimental settings and analysis. Before going into the experiments, we would like to clarify some potential misunderstanding of our approach.\n\n> However, *instead of learning to approximate or predict the distance map*, they use the GT target frame to compute the distance map during training and use a uniform map (i.e. uniform speed) during inference. What is the point of attempting to address speed ambiguity during training, if the authors are assuming uniform speed at inference? How is the ambiguity being addressed at inference?\n\nWe would like to provide a more intuitive and detailed explanation on the ambiguity issue in video frame interpolation (VFI) and how we resolve it, which will be integrated into the final paper.\n\nFirst and foremost, it's essential to clarify that **velocity ambiguity can solely exist and be resolved in the TRAINING phase, not in the INFERENCE phase**. The key idea behind our approach can be summarized as follows: While conventional VFI methods with time indexing rely on a one-to-many mapping, our distance indexing learns an approximate one-to-one mapping, which resolves the ambiguity during training. When the input-output relationship is one-to-many during training, the training process fluctuates among conflicting objectives, ultimately preventing convergence towards any specific optimization goal. In VFI, the evidence is the generation of blurry images in the inference phase. Once the ambiguity has been resolved using the new indexing method in the training phase, the model can produce significantly clearer results regardless of the inference strategy used.\n\nIndeed, this one-to-many ambiguity in training is not unique to VFI, but for a wide range of machine learning problems. In some areas, researchers have come up with similar methods. \n\n\n\n**A specific instantiation of this problem:** Let us look at an example in text-to-speech (TTS). The same text can be paired with a variety of speeches, and direct training without addressing ambiguities can result in a **\"blurred\" voice** (a statistical average voice). To mitigate this, a common approach is to incorporate a speaker embedding vector or a style embedding vector (representing different gender, accents, speaking styles, etc.) during training, which helps reduce ambiguity. **During the inference phase, utilizing an average user embedding vector can yield high-quality speech output.** Furthermore, by manipulating the speaker embedding vector, effects such as altering the accent and pitch can also be achieved.\nHere is a snippet from a high-impact paper which came up with the style embedding in TTS: ***\u201cMany TTS models, including recent end-to-end systems, only learn an averaged prosodic distribution over their input data, generating less expressive speech \u2013 especially for long-form phrases. Furthermore, they often lack the ability to control the expression with which speech is synthesized.\u201d*** from Wang, Yuxuan, et al. \"Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis.\" International conference on machine learning. PMLR, 2018, cited by 816 \n\nUnderstanding this example can significantly help understand our paper, as there are many similarities between the two (*e.g.,* motivation, solution, and manipulation).\n\n(to be continued ...)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699804884594,
                "cdate": 1699804884594,
                "tmdate": 1699804884594,
                "mdate": 1699804884594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bggFRIAe2F",
                "forum": "QuFHei1vuE",
                "replyto": "u22Yw26krb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**A minimal symbolic example to help readers understand better:** Assuming we want to train a mapping function $\\mathcal{F}$ from numbers to characters.\n\n**Training input-output pairs with ambiguity** ($\\mathcal{F}$ is optimized):\n\n$1 \\stackrel{\\mathcal{F}}{\\longrightarrow} a$, $1 \\stackrel{\\mathcal{F}}{\\longrightarrow} b$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} a$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} b$\n\n$\\mathcal{\\mathcal{F}}$ is optimized with some losses involving the input-output pairs above. \n\n$\\underset{\\mathcal{F}}{\\min} \\quad L(\\mathcal{F}(1), a) + L(\\mathcal{F}(1),b) + L(\\mathcal{F}(2),a) +L(\\mathcal{F}(2),b),$\n\nwhere $L$ can be L1, L2 or any other kind of losses (details on the losses are explained later). \nBecause the same input is paired with multiple different outputs, the model $\\mathcal{F}$ is optimized to learn an average (or, generally, a mixture) of the conflicting outputs, which results in blur at inference:\n\nInference phase ($\\mathcal{F}$ is fixed):\n\n$1 \\stackrel{\\mathcal{F}}{\\longrightarrow} \\{a,b\\}?$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} \\{a,b\\}?$\n\n**Training without ambiguity** ($\\mathcal{F}$ is optimized):\n\n$1 \\stackrel{\\mathcal{F}}{\\longrightarrow} a$, $1 \\stackrel{\\mathcal{F}}{\\longrightarrow} a$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} b$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} b$\n\nIn the input-output pairs above, each input value is paired with exactly one output value. Therefore, $\\mathcal{F}$ is trained to learn a unique and deterministic mapping.\n\nInference phase ($\\mathcal{F}$ is fixed):\n\n$1 \\stackrel{\\mathcal{F}}{\\longrightarrow} a$, $2 \\stackrel{\\mathcal{F}}{\\longrightarrow} b$ \n\n\n\n**Coming back to VFI:**  When time indexing is used, the same $t$ value is paired to images where the objects are located at various locations due to the speed and directional ambiguities. When distance mapping is used, a single $d$ value is paired to images where the objects are always at the same distance ratio, which allows the model to learn a more deterministic mapping for resolving the speed ambiguity.\n\nIt is important to note that fixing the ambiguity does not solve all the problems: At inference time, the \"correct\" (close to ground-truth) distance map is not available. In this work, we show that it is possible to provide uniform distance maps as inputs to generate a clear output video, which is not perfectly pixelwise aligned with the ground truth. This is the reason why the proposed method does not achieve state-of-the-art in terms of PSNR and SSIM (Table 1).  However, it achieves sharper frames with higher perceptual quality, which is shown by the better LPIPS and NIQE.\n\nWe claim the \"correct\" distance map is hard to estimate accurately from merely two frames since there are a wide range of possible velocities. If considering more neighboring frames (more observation information), it is possible to estimate an accurate distance map for pixelwise aligned interpolation, which we leave for future work.\n\nFurthermore, manipulating distance maps corresponds to sampling other possible unseen velocities, *i.e.*, 2D manipulation of frame interpolation, similar to that mentioned TTS paper (Wang, Yuxuan, et al.)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699805006284,
                "cdate": 1699805006284,
                "tmdate": 1699944521313,
                "mdate": 1699944521313,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w2ftLDr4vB",
                "forum": "QuFHei1vuE",
                "replyto": "u22Yw26krb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your suggestions about experiments!\n\n> Were all baseline trained using the same experimental setting? For instance, was RAFT used in all methods?\n\nYes, the distance maps used by each model were precomputed offline using RAFT.\n\n\n\n>  Why did the authors only perform experimental comparisons on the Vimeo-90 K dataset? There are several VFI benchmarks that are commonly used to compare VFI methods. Why did not the authors use those benchmarks as well?\n\nThe septuplet set of Vimeo-90K is large enough to train a practical video frame interpolation model, and it represents the situations where the temporal distance between input frames is large (Please refer to the supplementary video for a better perception). Notably, state-of-the-art methods such as AMT (CVPR'2023) have struggled to train a clear arbitrary time video frame interpolator on this extremely challenging dataset, as claimed on their GitHub:\n\n`Zhen Li et al.: We previously attempted to train on Septuplet but did not achieve good results.`\n\nThus, Vimeo90K (septuplet) can best demonstrate the velocity ambiguity problem that our work wants to highlight. With our disambiguation strategies, state-of-the-art models can be successfully trained on this dataset.\n\nNonetheless, we report more results on other benchmarks in the following. The following table shows the results of RIFE on Adobe240 [1] and X4K1000FPS [2] for 8x interpolation, using uniform maps.\n\n\n| Adobe240 (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------------------ | ------- | ------ | ------ | ------ |\n| RIFE [T]           | 30.2430 | 0.9386 | 0.0729 | 5.2060 |\n| RIFE [D]$_{u}$     | 30.4704 | 0.9384 | 0.0565 | 4.9735 |\n| RIFE [D,R]$_{u}$    | 30.2961 | 0.9372 | 0.0542 | 4.9068 |\n\n| X4K1000FPS (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| -------------------- | ------- | ------ | ------ | ------ |\n| RIFE [T]             | 36.3557 | 0.9667 | 0.0403 | 7.1302 |\n| RIFE [D]$_{u}$       | 36.7964 | 0.9642 | 0.0315 | 6.9364 |\n| RIFE [D,R]$_{u}$      | 36.2588 | 0.9642 | 0.0315 | 6.9242 |\n\nDistance indexing [D] and iterative reference-based estimation [R] strategies can consistently help improve the perceptual quality. In addition, it is noteworthy that [D]$_u$ is better than [T]$_u$ in terms of the pixel-centric metrics like PSNR, showing that the constant speed assumption (uniform distance maps) holds well on these two easier benchmarks. \n\n\n> The problem the paper is trying to solve is more relevant in VFI situations where the temporal distance between input frames is relatively large. However, the authors do not show any experimental analysis in these scenarios.\n\nAs explained above, Vimeo90K (septuplet) dataset is the preferable dataset in terms of large temporal distance. We further show 16x interpolation on X4K1000FPS with larger temporal distance as follows:\n\n| X4K1000FPS (x16) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ---------------- | ------- | ------ | ------ | ------ |\n| RIFE [T]         | 31.0401 | 0.9102 | 0.1043 | 7.2152 |\n| RIFE [D]$_u$         | 31.6000 | 0.9141 | 0.0944 | 6.9531 |\n| RIFE [D,R]$_u$        | 31.5195 | 0.9220 | 0.0786 | 6.9265 |\n\nThe results highlight that the benefits of our strategies are more pronounced with increased temporal distances.\n\n> As the proposed distance map computation is heavily dependent on optical flow, did the authors experiment with using optical flow estimation methods other than RAFT?\n\nWe use GMFlow [3] to precompute distance maps for comparison. Our strategies still yield consistent improvement on perceptual metrics, as detailed below:\n\n| Vimeo90K (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ----------------- | ------- | ------ | ------ | ------ |\n| RIFE [T]          | 28.2175 | 0.9116 | 0.1048 | 6.6626 |\n| RIFE [D]$_u$      | 27.2927 | 0.8983 | 0.1009 | 6.4492 |\n| RIFE [D,R]$_u$     | 26.9583 | 0.8952 | 0.0915 | 6.2799 |\n\nWe will incorporate all the above content into the final paper.\n\n(to be continued ...)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288261619,
                "cdate": 1700288261619,
                "tmdate": 1700291221890,
                "mdate": 1700291221890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D7QVigOaRT",
                "forum": "QuFHei1vuE",
                "replyto": "u22Yw26krb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "P.S., the same experimental conclusions can be derived from other state-of-the-art models like AMT as follows:\n\n| Adobe240 (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ------------- | ------- | ------ | ------ | ------ |\n| AMT [T]       | 29.5985 | 0.9333 | 0.0935 | 5.4004 |\n| AMT [D]$_u$       | 29.7449 | 0.9341 | 0.0700 | 5.0037 |\n| AMT [D,R]$_u$      | 29.4671 | 0.9329 | 0.0630 | 4.9392 |\n\n| X4K1000FPS (x8) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| --------------- | ------- | ------ | ------ | ------ |\n| AMT [T]         | 35.6691 | 0.9612 | 0.0598 | 7.3296 |\n| AMT [D]$_u$         | 36.6280 | 0.9631 | 0.0412 | 7.0355 |\n| AMT [D,R]$_u$        | 36.3044 | 0.9645 | 0.0355 | 6.8704 |\n\n| X4K1000FPS (x16) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ---------------- | ------- | ------ | ------ | ------ |\n| AMT [T]          | 31.6443 | 0.9242 | 0.1222 | 7.6546 |\n| AMT [D]$_u$      | 32.5098 | 0.9352 | 0.0816 | 7.1798 |\n| AMT [D,R]$_u$       | 31.9543 | 0.9311 | 0.0771 | 7.0707 |\n\n| Vimeo90k (GMFlow) | PSNR    | SSIM   | LPIPS  | NIQE   |\n| ----------------- | ------- | ------ | ------ | ------ |\n| AMT-S [T]         | 28.5202 | 0.9196 | 0.1011 | 6.8659 |\n| AMT-S [D]$_u$     | 27.0252 | 0.9004 | 0.1006 | 6.6257 |\n| AMT-S [D,R]$_u$    | 26.5586 | 0.8928 | 0.0937 | 6.4910 |\n\n\n\n> The technical novelty of the work is quite limited.\n\n> The authors repeatedly use the term, \"Our observation ...\", yet fail to provide convincing empirical evidence\n\nIt may appear at first glance that this work lacks technical novelty since we do not propose any novel network architecture. However, we claim this is indeed the strength of our approach: By replacing time indexing with distance indexing as network inputs, the proposed approach can be applied to a wide range of VFI methods, driving consistent improvement in perceptual quality. We hope that through our additional explanation about our approach, Reviewer aknz can recognize our motivation and novelty as other reviewers did (Reviewer 1WoD, PtYf, and BPE4).\n\n\n\n> Please cite and discuss some relevant works [1,2].\n>\n> [1] Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation, CVPR 2023\n>\n> [2] Deep Iterative Frame Interpolation for Full-frame Video Stabilization, SIGGRAPH Asia 2019\n\nWe appreciate the reminder and would like to point out that our paper does include a discussion about [1] in the second paragraph of the \"Learning Paradigms\" section. Additionally, we will incorporate a discussion of [2] in our final paper, which highlights the use of VFI techniques in an iterative manner for video stabilization.\n\n\n\n> What would be the added inference time by plugging the proposed method into existing methods?\n\n**Distance indexing**: Transitioning from time indexing ($[T]$) to distance indexing ($[D]$) does not introduce extra computational costs during the inference phase, yet significantly enhancing image quality. So, the inference time is the same.\n\n**Iterative reference-based estimation ($[D,R]$)**: Given that the computational overhead of merely expanding the input channel, while keeping the rest of the structure unchanged, is negligible, the total inference time is equal to the number of iterations $\\times$ the consumption of the $[D]$ model. We would like to highlight that this iterative strategy is optional: Users can adopt this strategy at will when optimal interpolation results are demanded and the computational budget allows. Our experiments (Table 2) show that 2 iterations, *i.e.,* doubling the computational cost, yield cost-effective improvements. We will include these details in our final paper for clarity.\n\n\n\nReferences:\n\n[1] Su, Shuochen, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, and Oliver Wang. \"Deep video deblurring for hand-held cameras.\" In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pp. 1279-1288. 2017.\n\n[2] Sim, Hyeonjun, Jihyong Oh, and Munchurl Kim. \"Xvfi: extreme video frame interpolation.\" In *Proceedings of the IEEE/CVF international conference on computer vision*, pp. 14489-14498. 2021.\n\n[3] Xu, Haofei, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. \"Gmflow: Learning optical flow via global matching.\" In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 8121-8130. 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288337834,
                "cdate": 1700288337834,
                "tmdate": 1700291151739,
                "mdate": 1700291151739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]