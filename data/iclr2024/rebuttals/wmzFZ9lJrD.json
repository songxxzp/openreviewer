[
    {
        "title": "Boolformer: Symbolic Regression of Logic Functions with Transformers"
    },
    {
        "review": {
            "id": "Qe5hQ5YXx3",
            "forum": "wmzFZ9lJrD",
            "replyto": "wmzFZ9lJrD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_V2Ca"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_V2Ca"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a model to perform symbolic regression of boolean functions using transformers"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is sound, simple and elegant. \n\nBoolean functions are easy to be generated randomly in huge amount, which is where transformers architecture shine. \nThe  techniques is straightforward: generate boolean formulas randomly (covering the space of function as much as possible with a bias towards short formulas) and train a seq-2-seq architecture on that. \n\nThe speed up on gene-regulatory networks is interesting."
                },
                "weaknesses": {
                    "value": "The major weakness of the paper is the positioning w.r.t. the state of the art. Being the method very simple, clearly highlighting the novelty should have been a priority. \n\nWhile the paper discusses many related areas, it is very unclear what is new in the proposed approach. For example, the section on \"symbolic regression\" in the related work, which is the closest area to the proposed approach (\"Symbolic regression of logic functions\"), is simply a list of papers. The approach is not compared with these approaches neither experimentally not even theoretically. \n\nExperiments in the noiseless regime do not compare Boolformer with any baseline (therefore is quite hard to understand how hard is the task overall). \n\nExperiments in the noisy regime have comparisons but with very unrelated approaches (generic ML models or specific to the dataset)\n\nMinor: The numbers in the radar charts in Figure 7 are impossible to read."
                },
                "questions": {
                    "value": "1) Would be possible to apply any existing symbolic regression approaches to the proposed task?\n\n2) How novel is the generation of boolean formulas? Are there similar ideas in the literature to generate datasets for symbolic regression? \n\n3) How can you measure how hard is the task? Would any other method (both transformer based, or tradition ILP setting, be able to solve the task to some extent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Reviewer_V2Ca"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738577678,
            "cdate": 1698738577678,
            "tmdate": 1699636187825,
            "mdate": 1699636187825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RnczHOpoHe",
                "forum": "wmzFZ9lJrD",
                "replyto": "Qe5hQ5YXx3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback. \n\nConcerning the novelty of this paper and comparisons, we refer to our introductory comments. \nSimilarly, the idea of generating synthetic datasets has been explored for symbolic regression of real-valued functions, but never in the Boolean setting.\n\nWe would also like to emphasize that the problem studied in the noiseless setting, also known as the Minimum Circuit Size Problem, is an NP-hard problem which no existing methods can currently solve, including ours. This section is more theoretical in nature, and was precisely designed to probe the ability of our model to generalize (i.e., prove that memorization does not occur, as discussed more in App. B) as well as its limitations of our model \u2013 we will clarify this in the revised manuscript. \n\nCrucially, the core contribution of this work in terms of practical relevance lies in the noisy setting, where we provide extensive comparisons.\nSee more details in the introductory comments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414859437,
                "cdate": 1700414859437,
                "tmdate": 1700470811472,
                "mdate": 1700470811472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c9nJnKKHgG",
                "forum": "wmzFZ9lJrD",
                "replyto": "RnczHOpoHe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_V2Ca"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_V2Ca"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their answer. However, as I stated in my original review, I have the same feeling as reviewer BZRx that the novelty of the method is extremely limited. Except for the dataset, all the technicalities are taken from other few papers (with almost no modification). I continue to believe that the paper is nice but, at this stage, the novelty is too limited for a ICLR publication."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727652212,
                "cdate": 1700727652212,
                "tmdate": 1700727652212,
                "mdate": 1700727652212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g7J5lJciZ1",
            "forum": "wmzFZ9lJrD",
            "replyto": "wmzFZ9lJrD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method named Boolformer that performs symbolic regression for logical operations using AND, OR, and NOT. Boolformer is trained to output logic formulas in Polish notation using a Transformer-based Encoder-Decoder model. The results show not only the evaluation of the automatically generated formulas, but also the inference performance, excellent speed, and high explanatory power on PMLB databases and gene regulartory networks (GRNs) as real-world problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A new problem setting in which logical expressions are symbolically regressed by Transformer.\n- Experimental results on several real-world applications as well as on the generated logical equation data are reported.\n  - The results using PMLB database shows accuracy comparable to Random Forest and logistic regression, and furthermore, the learned models are expected to provide excellent explanatory properties.\n  - Using GRNs, Boolformer is shown to have both excellent accuracy and speed."
                },
                "weaknesses": {
                    "value": "- The problem statement in the introduction does not match the solution. In Section 1, citing (Abbe et al., 2022), the authors point out that Transformer learns complex models in terms of the Fourier spectrum, resulting in poor generalization performance when samples are insufficient. As a contribution, Section 1.1 claims that it is robust to noisy and incomplete observations. However, the Boolformer proposed in this paper is a relatively natural application of the Transformer, and there is no redesign from a Fourier spectrum perspective or other robustness innovations.\n- Due to the design of the method, it can only accept datasets with relatively few variables or small scale. This is acknowledged by the authors in section 5, but since there are currently proposals such as Transformer that can accept long series, it would have been easy to consider improving the limitation.\n- For example, if the correct answer is [AND, X_1, NOT, X_2], then [AND, NOT, X_2, X_1] is also equivalent. The fact that the system is learned by cross-entropy means that it is unclear how a valid cross-entropy can be calculated when there are multiple correct answers in this way."
                },
                "questions": {
                    "value": "- The reviewer expects the authors to respond to the points listed in Weaknesses.\n- In the radar charts in Figure 7(a), the different methods are plotted among different axes, making it difficult to understand the comparison between those methods. If the radar chart is used to make comparisons between methods, it would be better to have as many axes as the number of experimental settings, such as the number of genes, and plot entities for each method. Alternatively, a table or a bar chart like Figure 7(b) is easier to compare methods.\n- Minor comments:\n  - In the caption of Figure 1, (x_5 x_6 x_7 x_7 x_9( should be (x_5 x_6 x_7 x_**8** x_9).\n  - References should be corrected. Especially, many published papers are cited as preprints. Below are some examples:\n    - The reference for (Abbe et al., 2022) should be a NeurIPS 2022 paper.\n    - (Dosovitskiy et al., 2020) should be an ICLR 2021 paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754220597,
            "cdate": 1698754220597,
            "tmdate": 1699636187749,
            "mdate": 1699636187749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O6pJDXAanf",
                "forum": "wmzFZ9lJrD",
                "replyto": "g7J5lJciZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Weaknesses\n\nW1: We would like to defer the reviewer to the general comment, as this important point was also brought up by another reviewer.\n\nW2: Thank you for bringing up this important question, which indeed deserves clarifications. To extend our discussion on this: even if we were to use a linear attention method (which would likely be very detrimental as the attention maps shown in the appendix are particularly non-local and dense), the scaling would be nontrivial as the number of input points remains in the worst-case exponential in D. More generally, the design choice for improving long context prediction is not critical to our contribution since our contribution is intended to show the promise of symbolic regression for boolean circuits rather than to exhaustively search the right architecture for scaling. \n\nW3: This is an interesting point which we will also discuss more. In fact, it turns out that the fact that multiple expressions can evaluate to the same function is not a problem in our setup, and can in fact be beneficial as a form of data augmentation. Indeed, our model learns these invariances, as evidenced by Appendix G, which shows the candidates obtained via beam search : the first eight candidates are all valid formulas, written in different ways (and in all cases, very compact).\nIn the context of symbolic regression, this aligns with the observation made in the Appendix of [1] that removing this equivalence via deterministic simplification rules does not yield any performance improvements.\nThe fact that cross-entropy can handle expressions with multiple possible formulations is also observed in many other fields, e.g. in machine translation, where beam search also reveals the equivalent translations of a given sentence.\n\nQuestions:\nWe thank the reviewer for these very valuable remarks. For the radar plots, we reused the formatting of the authors of the benchmark [2] for maximal consistency, but will also add some bar charts.\n\n[1] d\u2019Ascoli et al., Deep Symbolic Regression for Recurrent Sequences, ICML 2021\n\n[2] \u017diga Pu\u0161nik, Miha Mraz, Nikolaj Zimic, and Miha Mo\u0161kon. Review and assessment of boolean approaches for inference of gene regulatory networks. Heliyon, pp. e10222, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414673306,
                "cdate": 1700414673306,
                "tmdate": 1700430487160,
                "mdate": 1700430487160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7qiieMnMOc",
                "forum": "wmzFZ9lJrD",
                "replyto": "O6pJDXAanf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
                ],
                "content": {
                    "title": {
                        "value": "The rating remains the same"
                    },
                    "comment": {
                        "value": "W1: The reviewer read the general comment. But it does not appear to be a response to the reviewer's point about the inconsistency between the problem in the introduction and the solution. After all, the robustness of the proposed Boolformer seems to be what the original Transformer has.\n\nW2: Related to the previous weakness, the technical contributions are still limited. The authors say it is a PoC, but it is not clear whether the concept should be taken up by ICLR.\n\nW3: In [1], it is shown that normalizing the equation using SymPy has no effect. This is only one option and does not indicate that loss function considerations for the diversity of the equation are unnecessary.\n\nQ: The radar chart in [2] can be used to comapre among the network node numbers, but it is difficult to compare among methods. The authors should consider more appropriate visualization methods; the visualization method adopted in one past paper is not a de-facto standard as it is."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452799195,
                "cdate": 1700452799195,
                "tmdate": 1700452799195,
                "mdate": 1700452799195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MXgYTYQCMH",
                "forum": "wmzFZ9lJrD",
                "replyto": "g7J5lJciZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Does this clarify our answers to your questions?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678155216,
                "cdate": 1700678155216,
                "tmdate": 1700678182449,
                "mdate": 1700678182449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o6p0eeodxs",
                "forum": "wmzFZ9lJrD",
                "replyto": "g7J5lJciZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_BZRx"
                ],
                "content": {
                    "title": {
                        "value": "Still not very convincing"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for their additional explanations.\n\nOn the other hand, the authors simply adopted an existing Transformer-based architecture for symbolic regression to learn boolean functions.\nAs for the sampling of the dataset, since it is a synthesized dataset, it seems quite natural to sample what is useful for the target task.\nIn conclusion, neither in terms of learning Boolean functions nor in terms of the robustness of the Transformer architecture, it does not bring any technical novelty, but rather it is equivalent to saying that it uses existing knowledge in a field of symbolic regression, which is strongly related to this paper.\n\nAnother possibility for the authors regarding technical novelty is the embedder part. However, this is also adopted from (Kamienny et al., 2022), as stated in the paper, and is very similar. Additionally, (Kamienny et al., 2022) proposes an end-to-end symbolic regression method using Transformer. Again, there is no technical novelty beyond a combination of an existing method and a different problem."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725932203,
                "cdate": 1700725932203,
                "tmdate": 1700725980488,
                "mdate": 1700725980488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IDMaXkY5Mp",
            "forum": "wmzFZ9lJrD",
            "replyto": "wmzFZ9lJrD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
            ],
            "content": {
                "summary": {
                    "value": "Authors employ Transformer architecture to train a model capable of inferring a logical function based on a truth table.\n\nAuthors synthetically produce a dataset to train such a model.\n\nAuthors do evaluation in a noiseless and noisy setting.\nNoiseless means that the full truth table is available.\nNoisy means that a partial truth table is available, and some bits can be flipped with a small probability.\nThey show that the transformer effectively learned to reconstruct boolean functions.\nThey also show that it works in a noisy setting.\nThey evaluate their model in a realistic setting of gene regulatory networks, and showcase the use of their model;\nits performance is comparable to other methods while exhibiting much faster inference speed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well written and structured\n- Clear motivation\n- Authors will open source the implementation"
                },
                "weaknesses": {
                    "value": "- It would be good to have more examples of real-world usages of the method. Speed (inference) superiority is great, but maybe speed is not even a concern in the domains that the technique is intended to be applied.\n- It would be good to have some analysis on which type of tasks is solvable by this method vs. others.\n- Does not generalize to larger formulas."
                },
                "questions": {
                    "value": "- Can you explain better Figure 17 from Supplementary Material which displays embeddings? \n    - (a) Which part of transformer do you extract for the shown embedding vectors (is it only the last token state, or all tokens, etc.)?\n    - (b) What exactly are the inputs that you use to construct shown embedding, and why do you make such choice?\n\n- Figure 1. Denote that what is shown is output of your model. Figure title is misleading.\n- Page 3. \"in the sections below\" -> \"in the following sections\".\n- Page 4. Maybe add that Smax <= Dmax\n- Page 5. D refers to dimensionality of logical input. Later in this page, it refers to a set of input-output pairs (if I understand correctly). Use a different letter.\n- Fig 7, part a. Readability can be improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756810200,
            "cdate": 1698756810200,
            "tmdate": 1699636187664,
            "mdate": 1699636187664,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kAoB76NWmL",
                "forum": "wmzFZ9lJrD",
                "replyto": "IDMaXkY5Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their very positive feedback. We will address the presentation aspects. \n\nAbout the type of tasks solvable by this method : we agree that the paper can benefit from further clarifications. Generally speaking, our method is well-suited for tasks which (i) concern tabular data (structured features) and (ii) where the mapping from input to target can be expressed by a Boolean formula of reasonable complexity. This is the reason we considered the PMLB database, which is arguably the most extensive set of problems with tabular data. As briefly discussed in the results paragraph of Sec 4.2, we see that our method shines particularly in logical tasks, and is less good for other types of datasets. We will extend the discussion on this.\n\nConcerning the question on the embeddings: thanks for noticing that this deserves clarifications. We display the outputs of the \u201cembedder\u201d module, which is a fully-connected network which maps D-dimensional vectors to a single embedding. The inputs are all the points in the 10-dimensional hypercube, sorted by color from 0000000000, 0000000001, 0000000010\u2026 1111111111."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414756995,
                "cdate": 1700414756995,
                "tmdate": 1700470682693,
                "mdate": 1700470682693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ghccVzsbN8",
                "forum": "wmzFZ9lJrD",
                "replyto": "IDMaXkY5Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
                ],
                "content": {
                    "comment": {
                        "value": "Would you mind clarifying more Appendix H (Having a more detailed description in it.)?\nI would suggest taking one small image from Figure 16 and showing it separately; including the color diagram mapping color to value (e.g., 0-1).\n\n> For example, for the 4-digit multiplier, some attention heads have hadamard-like structure (e.g. head\n5 of layer 6), some have block-structured checkboard patterns (e.g. head 12 of layer 4), and many\nheads put most attention weight on the final input, 1111, which is more informative (e.g. head 6 of\nlayer 3).\n>> By looking at the image, it seems to me that most of the attention is on the current input; as there's a diagonal pattern.\n\nFigure 17, embeddings, seem not to be grouped by the number value; as within a cluster there's many points with a different color. What do you think clusters represent?\n\nWhat is the value you use for Demb, and where in the paper do you show this information?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590888601,
                "cdate": 1700590888601,
                "tmdate": 1700590935075,
                "mdate": 1700590935075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sbpbzCx7dc",
                "forum": "wmzFZ9lJrD",
                "replyto": "IDMaXkY5Mp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Reviewer_QDAh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications!\n\nJust to clarify. For the 5-dimensional hypercube, you used the same Embedder (embedding model) as for the 10-dimensional hypercube in the appendix?\n\nDoes embedder have the maximum dimension it supports, or it's a fixed dimension?\n\nAlso, what's the meaning of different colors in the image you sent?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700093924,
                "cdate": 1700700093924,
                "tmdate": 1700700265666,
                "mdate": 1700700265666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GNb0J3czS9",
            "forum": "wmzFZ9lJrD",
            "replyto": "wmzFZ9lJrD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_X8df"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2514/Reviewer_X8df"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates end-to-end symbolic regression of Boolean functions. The authors show that the overall performance of Boolformer is comparable to classical machine learning in this particular field, with the benefits that Boolformer can be faster and provide interpretable solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow.\n- Careful description of the generation of synthetic data, and a good analysis of the possible bias included.\n- Empirical evaluation establishes the effectiveness of the approach.\n- A good section of limitation addressing some of my concerns (e.g., not being able to deal with large formulas) which would otherwise go to the weakness below."
                },
                "weaknesses": {
                    "value": "- While there are some engineering for the embedder, the rest of the approach seems quite standard and straightforward (which is not necessarily a bad thing).\n- It might not be that surprising that Boolformer is faster on GRNs tasks. After all, it has been trained for a long time and the training data could have covered what it needed in these tasks. I am curious, however, is there a similar comparison of efficiency in the noiseless regime?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2514/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832730059,
            "cdate": 1698832730059,
            "tmdate": 1699636187587,
            "mdate": 1699636187587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Y84ljogZR",
                "forum": "wmzFZ9lJrD",
                "replyto": "GNb0J3czS9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2514/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback.\nConcerning the comparisons in the noiseless regime, we would like to defer the reviewer to the general comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2514/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414600273,
                "cdate": 1700414600273,
                "tmdate": 1700414600273,
                "mdate": 1700414600273,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]