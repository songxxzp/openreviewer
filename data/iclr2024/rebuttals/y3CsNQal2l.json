[
    {
        "title": "Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging"
    },
    {
        "review": {
            "id": "6AD2w7sKu1",
            "forum": "y3CsNQal2l",
            "replyto": "y3CsNQal2l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_PiPR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_PiPR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies model merging in cross-lingual transfer. The authors propose learning adapters that capture 'task ability' and 'language ability' separately and introduce a novel adaptive adapter merging method called AdaMergeX, which is applied to LoRA adapters and IA3 adapters. The core idea of the paper is to achieve cross-lingual *task* transfer with merging of 3 adapters, namely 2 reference task adapters (1 in source language, 1 in target language), 1 task adapter (in source language for the target task). Element-wise addition/subtraction is applied to LoRA type adapters and element-wise multiplication is applied to IA3 adapters.\nThe paper examines the proposed method in several cross-lingual transfer tasks and demonstrates that it achieves improved performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper conducted experiments on a range of cross-lingual transfer tasks for LoRA and IA3. Additionally, the paper proposed an interesting distinction: merging different types of adapters may require different operations."
                },
                "weaknesses": {
                    "value": "There are several drawbacks in the proposed work:\n- Inadequate benchmark / baselines, for example:\n    * The paper argues that the lack of enough training data to study standard adapters (Houlsby) merging is unconvincing. (See MAD-X as an example, which is not fundamentally different in terms of training data requirements compared to Houlsby nor this work). \n     * The test results are sub-sampled (in paper, \"while for XNLI, XLSum, and XQuAD we randomly sample 1000, 500,\nand 1000 data points from the whole test set respectively\") without convincing justifications.\n\n- Ablations:\n    * Even though differences in results between the same task across two languages approximate 'language ability,' such a claim was not carefully examined or discussed. Are the domains of training data the same for two languages (for the same task)? Was any of the cross-lingual data for the reference task machine-translated?\n    * Why is the ablation of backbone models (Table 6) evaluated on only a single task with two languages? Given that XNLI contains 15 languages, it's unconvincing that the results generalize across languages, especially when prior tables (e.g., Table 2 or 4) show results across multiple languages.\n    * The same question applies to Table 3, where results are only shown for 2 languages (es, fr), and they are not even the same languages as in Table 6 (es, vi). No justifications are provided.\n\n- Inadequate discussions about the proposed work and its relationship to prior work. For example:\n    * the training of adapters capturing language ability skills using 'LM objectives' (equivalent to the reference task in the paper) lacks a clear connection to existing literature in cross-lingual transfer, such as MAD-X and LT-SFT.\n    * insufficient exploration of its relationship and differences with methods like AdaMerge and Task Arithmetics. The LoRA merging in the proposed work involves element-wise addition/subtraction, which is the same as in Task Arithmetics.\n    * imho, one of the core interesting point proposed by the author is different types of adapters requires different merging operation, yet this aspect is not sufficiently studied in the paper. \n\n- Writing:\n    * Details of the experiments, such as the backbone model used for the experiments, are scattered throughout the paper, making it very difficult to follow the experiments.\n    * Definition of AdaMergeX (adaptive) vs AdaMergeX (cross), Eng-Tune vs Eng-FT etc.\n    * There are unclear descriptions of experimental settings, such as of what has been used as reference tasks for specific experiments, including details on the amount of data used and the training process etc."
                },
                "questions": {
                    "value": "* Why do you randomly sub sample test set for evaluation for  XNLI, XLSum, and XQuAD? What's wrong with evaluating on all test data?\n* What hyper-parameters do you use for training?\n* What's the reason behind using Llama-2 or T5 as the backbone, where other backbones, especially multilingual backbones are available?\n\n\n-------------------\n\nDear authors,\nThank you very much for the additional information. I acknowledged that I've reviewed the rebuttal and updated information.\nBest,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Reviewer_PiPR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698409755762,
            "cdate": 1698409755762,
            "tmdate": 1700656307867,
            "mdate": 1700656307867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4gBU1TYjm6",
                "forum": "y3CsNQal2l",
                "replyto": "6AD2w7sKu1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to PiPR"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time and thorough response. We address the weaknesses and concerns of the reviewer below.\n\n\n\nW1.1: Standard Adapters\n\nThanks for the recommendation, yes, the training data is sufficient in the context of smaller models instead of LLMs such as Llama2. We have implemented it, please refer to Response 2 in the Novelty part in the general response.\n\n\n\nW1.2 & Q1: Sub-sampled testset\n\nWe include the entire test set in our evaluation for MGSM and XCOPA. However, for XNLI, XLSum, and XQuAD, we employ a sampling approach. Specifically, we randomly select 1000, 500, and 1000 data points, respectively, from the complete test set. This sampling strategy is necessary due to the extended inference time of the Large Language Model (LLM) on these datasets. Without sampling, testing one method on the XNLI dataset alone would take approximately 120 hours. Therefore, it has become a common practice, as demonstrated in previous works [1] [2] [3] [4] to evaluate LLM methods on a subset of the test set. It is important to note that the sampled test set is sufficiently large to avoid any potential bias.\n\n\n\nW2.1: 'language ability' is not clearly explained\n\nRegarding the training data, there is no restriction for it to be strictly language-parallel. The only requirement is that the data should pertain to the same task. In the case of the language modeling task, the training data consists of Wikipedia corpora specific to each corresponding language, and the task is conventional language modeling, i.e., next token prediction.\n\n\n\nW2.2 & 2.3: Ablation on source languages and source tasks\n\nPlease refer to Response 1 and Response 2 in the Experiment part in the general response.\n\n\n\nW3: Inadequate discussions about the proposed work and its relationship to prior work.\n\nPlease refer to Response 1 and Response 2 in the Novelty part in the general response.\n\n\n\nW4: Writing\n\nSorry for the confusion, we have addressed these issues in our new pdf version.\n\n\n\nQ2: Hyper-paramters for training\n\nThanks for pointing out. We have listed hyper-paramters for training in the appendix of the new pdf version.\n\n\n\nQ3: Multi-Lingual Models\n\nPlease refer to Response 3 in the Experiment part in the general response.\n\n\n[1] Pryzant et al, Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search, Arxiv 2023.\n\n[2] Zhou et al, Large Language Models Are Human-Level Prompt Engineers, ICLR 2023.\n\n[3] Gonen et al, Demystifying Prompts in Language Models via Perplexity Estimation, EMNLP 2023.\n\n[4] We et al, Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering, ACL 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495652516,
                "cdate": 1700495652516,
                "tmdate": 1700495652516,
                "mdate": 1700495652516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h5aEMmtAhg",
                "forum": "y3CsNQal2l",
                "replyto": "6AD2w7sKu1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rquest more discussion with Reviewer PiPR"
                    },
                    "comment": {
                        "value": "Dear Reviewer PiPR,\n\nWe appreciate you taking the time to review our work and your insightful feedback. We would like to discuss this further with you to see if there are any unresolved questions. Specifically, we have implemented our method on the following points.\n(1) We applied our method to more adapters.\n(2) We conducted more ablation analysis on source language and reference tasks. \n(3) Thanks for your suggestions on the writing part, we have updated the new PDF version. \nIf you have any further concerns, please let us know.\n\nSincerely,\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647583560,
                "cdate": 1700647583560,
                "tmdate": 1700647583560,
                "mdate": 1700647583560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b4Dc4bWkd7",
            "forum": "y3CsNQal2l",
            "replyto": "y3CsNQal2l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_tcUW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_tcUW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an adaptive adapter merging method, termed AdaMergeX, for cross-lingual transfer of large language models (LLMs). The authors decompose the abilities of multilingual LLMs into \"task ability\" and \"language ability\". AdaMergeX introduces three types of adapters to LLMs, and models the task ability and language ability by different adapter merging strategies of the three adapters. They conduct experiments on five multilingual tasks, and observe improvement over several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of adapter merging and ability composite is nice, and it could be useful for efficient cross-lingual transfer for LLMs, which are expensive to fine-tune.\n- The structured-adaptive merging methods consider the structure of adapters, and consistently outperforms a strong adapter merging baseline, AriMerge.\n- The authors conduct experiments on five multilingual datasets, covering reasoning, natural language understanding, and natural language generation tasks."
                },
                "weaknesses": {
                    "value": "- The experimental setup is unclear and confusing. (1) Training data: Since the proposed methods learn task adapters, I would guess it learns on some training data of the downstream tasks. However, Table 1 only provides the details of test data, and it is unclear why the evaluation is conducted on small subsets of the test sets. (2) Baseline setup:  MAD-X[1] adopts a similar idea, and introduces task and language adapters to cross-lingual transfer, which should be the most important baseline. Besides, fine-tune-based cross-lingual transfer methods such as xTune[2], and translate-test[3] should be considered. The setup of the XLT baseline is not clear. If the proposed method uses training data, is XLT evaluated in a few-shot setup as well?\n- The results are insufficient to support the claim \"AdaMergeX consistently outperforms other state-of-the-art methods\". First, the XNLI accuracy scores in Table 2 are too low to compete with random guess, which has 33.3% accuracy. For reference, mT5[4] achieves 85.0 accuracy on zero-shot cross-lingual transfer on XNLI. Besides, regarding XLT as a SOTA cross-lingual transfer method is misleading because a lot of cross-lingual transfer methods achieve better performance.\n- \"AdaMerge outperforms cross-lingual transfer methods\" is misleading. I would guess the paper focuses on some efficient-training setup, but the results in Table 1 cannot support this claim.\n\n[1] MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer\n\n[2] Consistency Regularization for Cross-Lingual Fine-Tuning\n\n[3] Revisiting Machine Translation for Cross-lingual Classification\n\n[4] mT5: A massively multilingual pre-trained text-to-text transformer"
                },
                "questions": {
                    "value": "- In Eq.3 and 4, how the symbol \"~\" is converted to \"=\"?\n- What is the relation between AdaMergeX and MAD-X?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Reviewer_tcUW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800657390,
            "cdate": 1698800657390,
            "tmdate": 1699637149224,
            "mdate": 1699637149224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WLtLDyqhpo",
                "forum": "y3CsNQal2l",
                "replyto": "b4Dc4bWkd7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to tcUW"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time and thorough response. We address the weaknesses and concerns of the reviewer below.\n\n\n\nW1.1: Training data\n\nFor training data of each dataset on the source language (mostly in English), we use default training data of each dataset.\n\n| Dataset   | MGSM | XCOPA | XNLI | XQuAD | XLSum |\n| --------- | ---- | ----- | ---- | ----- | ----- |\n| #Training | 7.5k | 0.5k  | 10k  | 1.2k  | 10k   |\n\n\n\nW1.1: Test data\n\nWe include the entire test set in our evaluation for MGSM and XCOPA. However, for XNLI, XLSum, and XQuAD, we employ a sampling approach. Specifically, we randomly select 1000, 500, and 1000 data points, respectively, from the complete test set. This sampling strategy is necessary due to the extended inference time of the Large Language Model (LLM) on these datasets. Without sampling, testing one method on the XNLI dataset alone would take approximately 120 hours. Therefore, it has become a common practice, as demonstrated in previous works [1] [2] [3] [4] to evaluate LLM methods on a subset of the test set. It is important to note that the sampled test set is sufficiently large to avoid any potential bias.\n\n\n\nW1.2: Baseline\n\nPlease refer to Response 3 in the Experiment part in the general response.\n\n\n\nW2: Low performance of LLMs on tasks \n\nThe poor performance of Llama-7b can be attributed to its inability to effectively follow instructions. Additionally, in the case of XNLI, we have chosen to use labels such as \"entailment\", \"contradict\" and \"neutral\" instead of numerical values \"0\", \"1\", \"2\". This decision has made achieving an exact match more challenging. However, when we compare our results to those obtained using MAD-X and LF-LFT under the same conditions, our performance significantly improves, as demonstrated in the above first response.\n\n\n\nQ1: In Eq.3 and 4, how the symbol \"~\" is converted to \"=\"?\n\nSimilar to Eq.5 to Eq.6 and Eq.7 to Eq.8, there is hyperparameter $t$ in the merging method to balance the scale difference.\n\n\n\nQ2: relation between $\\texttt{AdaMergeX}$ and MAD-X\n\nWe have further explained the difference between $\\texttt{AdaMergeX}$ and related works. Please refer to Response 1 and Response 2 in the Novelty part in the general response.\n\n\n\n[1] Pryzant et al, Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search, Arxiv 2023.\n\n[2] Zhou et al, Large Language Models Are Human-Level Prompt Engineers, ICLR 2023.\n\n[3] Gonen et al, Demystifying Prompts in Language Models via Perplexity Estimation, EMNLP 2023.\n\n[4] We et al, Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering, ACL 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495490996,
                "cdate": 1700495490996,
                "tmdate": 1700495490996,
                "mdate": 1700495490996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IVkiKgDuTK",
                "forum": "y3CsNQal2l",
                "replyto": "b4Dc4bWkd7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rquest more discussion with Reviewer tcUW"
                    },
                    "comment": {
                        "value": "Dear Reviewer tcUW,\n\nThanks so much for your valuable review, including (1) details of the training and testing dataset, (2) comparison with more cross-lingual transfer baselines, as well as (3) the relation between our methods and MAD-X. We have carefully responded to your concerns and conducted supplementary experiments to address your concerns. We are eagerly anticipating your feedback on our response and new experiments as the deadline for discussion draws near.\n\nBest regards,\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647572379,
                "cdate": 1700647572379,
                "tmdate": 1700647572379,
                "mdate": 1700647572379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pkkm13n8pM",
            "forum": "y3CsNQal2l",
            "replyto": "y3CsNQal2l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_CTKw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_CTKw"
            ],
            "content": {
                "summary": {
                    "value": "This work presents an approach to cross-lingual transfer where the idea is to merge adapters that can deal with 'general processing' of a source language and a target language (eliciting language abilities) further with the task adapter trained on the desired task in the source language (therefore eliciting 'task abilities'). The term 'structure-adaptive' adapter merging from the title and the abstract actually denotes the need to perform merging via different operations, which depends on the nature of the chosen adapter architecture: e.g., LoRA relies on elementwise addition, so the same operation should be used for adapter merging, while IA3 requires elementwise multiplication. As the ablations studies show, doing 'blind' merging that doesn't align with the actual underlying adapter structure yields large task performance drops.\n\nThe proposed merging strategy is then applied on several LLMs and mostly compared against other recent (and less sophisticated) merging strategies on several standard cross-lingual transfer tasks, spanning a total of 12 languages. The results show consistent gains over the chosen baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "While the paper is generally well written and easy to follow, for each of its strength there is a mirrored weakness. For each strength (S_i), I suggest to check the related weakness (labeled W_i later).\n\nS1. The work connects the ideas of modular and PEFT learning (via adapters such as LoRA and IA3) on LLMs and cross-lingual transfer learning. \n\nS2. One of the main conceptuals novelties, as claimed by the authors, is the division of information and abilities into 'language abilities' (captured through language adapters via causal language modeling) and 'task abilities' (captured through task-specific tuning). However, while it's interesting to revisit this idea in the context of LLMs, the idea is definitely not novel (see W2).\n\nS3. The main results seem to suggest the gains of the proposed approach over the chosen baselines."
                },
                "weaknesses": {
                    "value": "W1. The idea of connecting modular and PEFT learning with cross-lingual transfer has been explored before with encoder-only models (a body of work on bottleneck adapters, sparse subnetworks, etc.) as well as encoder-decoder models (e.g., check the work on mmT5). Moreover, even the idea of (simple) adapter merging is not novel and has been proposed, e.g., by Zhang+.\n\nW2. There has been a large body of work that decomposed language and task abilities into dedicated language and task adapters and then performed various operations on such decomposed modules with well defined abilities. Cross-lingual transfer is basically one of the primary applications demonstrating how modularisation helps with postiive transfer. I suggest the readers to check a recent survey paper on modular deep learning of Pfeiffer+ for an overview (e.g., MAD-X work performed exactly this but stacking instead of merging language and task adapters). Overall, the paper doesn't perform a good job in contextualising their work within the wider area where the idea of modularisation for cross-lingual transfer has been extensively researched with encoder-only and encoder-decoder models. This diminishes the novelty of the work substantially.\n\nW3. The gains are reported only over the chosen baselines (which seem most relevant at first), but there's a large body of work on cross-lingual transfer (i.e., with adapters as well as without adapters) that the paper simply ignores. For instance, combining language and task masks as done in the work of Ansell+ (ACL-22) can be seen as a form of direct adapter merging for cross-lingual transfer, and is therefore directly relevant as a baseline. Comparing performance to adapter-based transfer with encoder-only models is also a must, as previous work typically reported much higher absolute scores in general.\n\nW4. The results in absolute terms are quite low - for instance, many XNLI results actually underperform the random baseline (or a majority baseline) in a 3-way classification task such as NLI. The same goes for XCOPA results. There are much higher scores reported on those benchmarks in prior work on cross-lingual transfer learning. Given the reduced novelty and other methods that are very relevant and perform some sort of adapter merging, I fail to see how exactly this approach advances the field.\n\nW5. It is quite intuitive to see that merging adapters via the same technique that merges their parameters with the parameters of the original model will yield the highest performance. While it's nice to see this confirmed empirically, I feel that the paper overclaims this as a contribution (similar to overclaiming the novelty of decoupling learning into language and task adapters). Also, the work only explores adapters that get their parameters merged/composed with the original parameters of the large model, but it doesn't explore other techniques such as bottleneck and serial adapters, or a combination of different architectures (e.g., UniPELT or AutoPEFT).\n\nW6 (Minor). The paper doesn't really evaluate on low-resource languages, but this mostly stems from the limitations of the underlying LLMs that simply cover less languages than models such as mT5, XLM-R, mDeBERTa, etc."
                },
                "questions": {
                    "value": "Can the authors comment on low performance of LLMs on tasks such as XNLI and XCOPA that often go below the random/majority baseline?\n\nWhy haven't the authors compared the results also with encoder-based XLT approaches (e.g., MAD-X is one very relevant approach and there are also improved approaches that build on top of it)?\n\nHave the authors also considered comparing to other adapter aggregation strategies in the context of XLT (e.g., adapting AdapterFusion for XLT)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Reviewer_CTKw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827616349,
            "cdate": 1698827616349,
            "tmdate": 1700728844613,
            "mdate": 1700728844613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hfhhl8ya6l",
                "forum": "y3CsNQal2l",
                "replyto": "pkkm13n8pM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to CTKw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time and thorough response. We address the weaknesses and concerns of the reviewer below.\n\n\n\nW1 & W2: Novelty of \"decompose language and task abilities\"\n\nPlease refer to Response 1 in the Novelty part in the general response.\n\n\n\nW5: Novelty of \"adapter merging\"\n\nLet us first response to the weakness 5 as it is also related to the novelty of our method. Please refer to Response 2 in the Novelty part in the general response.\n\n\n\nW3 & Q2: Other XLT baselines\n\nPlease refer to Response 3 in the Experiment part in the general response.\n\n\n\nW4 & Q1: Low performance of LLMs on tasks \n\nThe poor performance of Llama-7b can be attributed to its inability to effectively follow instructions. Additionally, in the case of XNLI, we have chosen to use labels such as \"entailment\", \"contradict\" and \"neutral\" instead of numerical values \"0\", \"1\", \"2\". This decision has made achieving an exact match more challenging. However, when we compare our results to those obtained using MAD-X and LF-LFT under the same conditions, our performance significantly improves, as demonstrated in the above first response.\n\n\n\nQ3: Other adapter aggression baselines\n\nIn regards to AdapterFusion [1], AdapterSoup [2], LoRA-Hub [3], and other similar adapter aggregation approaches, they utilize a significantly larger amount of training data to obtain many adapter and encompass training parameters associated with fusion techniques. Hence, it would be unfair to draw comparisons between our method and theirs.\n\n\n\n[1] Pfeiffer et al, AdapterFusion: Non-Destructive Task Composition for Transfer Learning, EACL 2021.\n\n[2] Chronopoulou et al, AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models, EACL 2023.\n\n[3] Huang et al, LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition, Arxiv 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495334752,
                "cdate": 1700495334752,
                "tmdate": 1700495334752,
                "mdate": 1700495334752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1fGVh3krKW",
                "forum": "y3CsNQal2l",
                "replyto": "pkkm13n8pM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rquest more discussion with Reviewer CTKw"
                    },
                    "comment": {
                        "value": "Dear Reviewer CTKw,\n\nWe greatly appreciate your valuable comments on our work. We have carefully addressed your concerns regarding (1) the novelty of decomposing language ability and task ability; (2) more cross-lingual transfer baselines; (3) implementing our method on various adapters in the general response. As the deadline for discussion is approaching, we eagerly await your feedback on our response and new experiments.\n\nThank you for your time and consideration.\n\nBest regards,\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647550852,
                "cdate": 1700647550852,
                "tmdate": 1700647550852,
                "mdate": 1700647550852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2jlVQChd1O",
                "forum": "y3CsNQal2l",
                "replyto": "1fGVh3krKW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Reviewer_CTKw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Reviewer_CTKw"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for providing the detailed response with further clarifications and additional experiments. The description of the main diff between what is considered 'language ability' and 'task ability' in this work versus prior work is now made much clearer to me, and the main contributions are easier to extract from the paper.\n\nI still do have concerns with the whole experimental setup, and I am not sure if the baselines such as MAD-X and SFT were properly optimised hparam-wise, so it would be good to see this in the paper as well. \n\nWhile the paper has definitely approved after the response, I still think there are other concerns not fully mitigated (e.g., mentioned by tcUW and PiPR) - I do increase my score though in appreciation of the response."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728825949,
                "cdate": 1700728825949,
                "tmdate": 1700728825949,
                "mdate": 1700728825949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c1XgGaddUJ",
            "forum": "y3CsNQal2l",
            "replyto": "y3CsNQal2l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_NmaX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9136/Reviewer_NmaX"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces AdaMergeX, a new cross lingual transfer learning approach. The main idea is to realize that regular fine-tuning on a task consists of two aspects: \u201ctask ability\u201d, the ability to train the actual task, and \u201clanguage ability\u201d the ability to understand the language in which the task was trained. With that assumption authors define a way to train a model on a specific task task and specific language by using another task as pivot to provide the language ability, to provide the task on a different language, and remove the undesired pair pivot task and languages.\nThis idea is adapted into two existing parameter efficient fine tuning methods, LoRA and (IA)3 and tested in a variety of tasks such as multilingual arithmetic reasoning, multilingual common-sense reasoning, multilingual natural language inference, question-answering and multilingual summarization. The method is compared against five cross-lingual transfer competing techniques, which are beaten by AdaMergeX.\nA few ablation studies are presented, showing the generalizability of the approach regardless of the pivot language, using Spanish and Vietnamese instead, and generalizability  in terms of pivot task, comparing the performance using XNLI and XCOPA as reference tasks.\nFinally an experiment using T5 instead of LlaMa model is performed, showing the generalizability  in terms of architecture."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is sound and strong.\nThe idea is cleverly defined, implemented and tested.\nThe experimentation seems appropriate, it shows that the new approach is effective to perform cross lingual and cross task training."
                },
                "weaknesses": {
                    "value": "Experimentation on this approach is difficult. All the experiments provide good evidence that support the author's claims, but given the generalizability nature of the approach, more experiments are needed."
                },
                "questions": {
                    "value": "The ablation study on adaptive merging method is a bit confusing? What do the authors were expecting? Cross adaptive merging methods is probably a bad idea and your experiments support that.\n\nThe experiments on source language generalizability ? Why choose only 2 languages? Do the authors consider that the experiments on Spanish and Vietnamese make the point?\n\nSimilar question for XNLI and XCOPA for task generalizability\n\nAnd for T5.\n\nDo this approach work on Encoder models as well such as XLM or mBERT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9136/Reviewer_NmaX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861520425,
            "cdate": 1698861520425,
            "tmdate": 1699637148983,
            "mdate": 1699637148983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p8ha5yPQdu",
                "forum": "y3CsNQal2l",
                "replyto": "c1XgGaddUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to NmaX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time and thorough comment. We address the weaknesses and concerns of the reviewer below.\n\nQ1: Ablation study on adaptive merging method\n\nIn this ablation study, our aim is to demonstrate the crucial significance of the adaptive merging method. It emphasizes the necessity of applying different merging methods to corresponding adapters, as utilizing a cross-merging method would lead to a notable deterioration in performance.\n\n\nQ2: Generalizability on source languages\n\nPlease refer to Response 1 in the Experiment part in the general response.\n\n\nQ3: Generalizability on reference tasks\n\nPlease refer to Response 2 in the Experiment part in the general response.\n\n\nQ4: Generalizability on backbone models. \n\nPlease refer to Response 3 in the Experiment part in the general response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495221292,
                "cdate": 1700495221292,
                "tmdate": 1700495221292,
                "mdate": 1700495221292,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1rqhoG4sj9",
                "forum": "y3CsNQal2l",
                "replyto": "c1XgGaddUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9136/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to NmaX"
                    },
                    "comment": {
                        "value": "Dear Reviewer NmaX,\n\nThank you very much for your appreciation of our work and valuable suggestions. We have seriously conducted more experiments to address your concerns. Specifically, we conducted experiments to verify the generalizability of our method corresponding to the source language, reference task, and backbone models. Please let us know if you have any other concerns. Thanks again for your time.\n\nBest regards,\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9136/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647532214,
                "cdate": 1700647532214,
                "tmdate": 1700647532214,
                "mdate": 1700647532214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]