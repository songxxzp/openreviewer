[
    {
        "title": "Spectral Contrastive Regression"
    },
    {
        "review": {
            "id": "RNkGFAbW7W",
            "forum": "4XCfu7fTgw",
            "replyto": "4XCfu7fTgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_xKM4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_xKM4"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackle the problem of generalization for regression tasks. It proposes a method based on metric learning assumption that the distance between features and labels should be proportional. defined as a mapping function. The proposed loss function aims at minimizing the error of the mapping function for the proportion and stabilizing its fluctuating behavior by smoothing out its variations. To enable out-of-distribution generalization, it also proposes to align the maximum singular value of the feature matrices across different domains. The paper conducts experiments on both in-distribution generalization and out-of-distribution robustness and shows that the proposed method can achieve superior performance in most cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is quite novel, and the empirical results are promising."
                },
                "weaknesses": {
                    "value": "My main concern is that some related works / baselines are missing in this paper. It is not as the authors claimed that regression generalization remains relatively underexplored. Also, there are already many papers try to align the order of feature distances with the order of label distances, and they also evaluated OOD/zero-shot generalization, such as:\n\n[1] Yang et al. Delving into Deep Imbalanced Regression. ICML 2021.\n\n[2] Gong et al. RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. ICML 2022.\n\n[3] Zha et al. Rank-N-Contrast: Learning Continuous Representations for Regression. NeurIPS 2023.\n\nI think the authors should avoid claiming this paper introduces the contrastive interdependence between features and labels, and discuss about and compare with the above papers. \n\nMinor: It would be better to give your method a name, instead of  FT+L_std+L_svd."
                },
                "questions": {
                    "value": "1. Does FT refer to fine-tuning in the experiments? It's better to explain it in texts. \n\n2. In the experiments, FT+L_std+L_svd seldom gets the best results over all FT methods. Is there an explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698440022093,
            "cdate": 1698440022093,
            "tmdate": 1699636258535,
            "mdate": 1699636258535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XQPyjfZPZb",
                "forum": "4XCfu7fTgw",
                "replyto": "RNkGFAbW7W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time and feedback. We incorporated your feedback in the updated paper by adding the results of RankSim and FDS to the tables. Specifically:\n1. We report the results with the Feature Distribution Method (FDS) by conducting the experiments over 7 datasets. The results are shown in Tables 1 and 2 of the updated paper. As can be seen, our method outperforms FDS in most cases.\n2. Unlike our approach, the paper \"RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression\" only considers the relation between the order of feature distances and the order of label distances, and not their proportions. Therefore, it can not guarantee the Lipschitz continuous property of the predictor $p$. In other words, RankSim may lead to very steep inclines in the feature distribution, which may affect the continuity of the feature distribution. We add the results of RankSim in the tabels as well. Since the RankSim is only designed on the dataset with discrete target labels, we skip those datasets where RankSim can not be applied. The result is shown in Tables 1 and 2. Our method outperforms RankSim in most cases. Also, since RankSim has a similar motivation with $L_{std}$, we also visualize the resulting feature distribution on DTI dataset in Figure 1 of the updated paper, and compare it to our method. As can be seen, even though Ranksim has a more discriminative pattern, there are more breakpoints in the distribution because of lacking the Lipschitz continuous property. This may be the reason that our proposed $L_{std}$ can perform better than RankSim.\n3. The paper \"Delving into Deep Imbalanced Regression.\" proposed two strategies to smooth label distribution and feature distribution. Since the Label Distribution Smoothing (LDS) is applied on discrete target labels, it cannot be directly applied to our datasets where the labels have real/continues values. More details can be found at the github link https://github.com/YyzHarry/imbalanced-regression/blob/a6fdc45d45c04e6f5c40f43925bc66e580911084/agedb-dir/datasets.py#L60\n\n\n\n|                        | Airfoil        | Airfoil       | No2            | No2           | ExchangeRate    | ExchangeRate   |\n|:----------------------:|:--------------:|:-------------:|:--------------:|:-------------:|:---------------:|:--------------:|\n|                        | RMSE  $\\downarrow$          | MAPE(%)  $\\downarrow$     | RMSE $\\downarrow$           | MAPE(%)$\\downarrow$       | RMSE$\\downarrow$             | MAPE(%) $\\downarrow$       |\n| FT+RankSim             | 2.635          | 1.537         | 0.520          | 13.188        | -               | -              |\n| FT+FDS                 | 2.663          | 1.529         | 0.589          | 14.986        | 0.0235          | 2.397          |\n| FT                     | 2.541          | 1.474         | 0.519          | 13.201        | 0.0233          | 2.387          |\n| FT+$L_{std}$           | 2.586        | 1.501         | 0.510          | 12.879 | 0.0161 | 1.529 |\n| FT+$L_{svd}$           | 2.489 | 1.443 | 0.517          | 13.161        | 0.0233          | 2.391          |\n| FT+$L_{std}$+$L_{svd}$ | 2.516          | 1.460         | 0.506 | 12.896        | 0.0176          | 1.691          |\n\n\n|                        | RCF-MNIST             | Crime                 | Crime                  | SkillCraft            | SkillCraft             | DTI         | DTI               |\n|:----------------------:|:---------------------:|:---------------------:|:----------------------:|:---------------------:|:----------------------:|:-----------:|:-----------------:|\n|                        | avg. RMSE$\\downarrow$ | avg. RMSE$\\downarrow$ | worst RMSE$\\downarrow$ | avg. RMSE$\\downarrow$ | worst RMSE$\\downarrow$ | R$\\uparrow$ | worst R$\\uparrow$ |\n| FT+RankSim             | 0.239                 | 0.135                 | 0.164                  | 5.324                 | 7.577                  | 0.479       | 0.464             |\n| FT+FDS                 | 0.147                 | 0.129                 | 0.160                  | 5.201                 | 6.908                  | 0.479       | 0.445             |\n| FT                     | 0.146                 | 0.129                 | 0.156                  | 5.592                 | 8.358                  | 0.479       | 0.458             |\n| FT+$L_{std}$           | 0.145                 | 0.128                 | 0.157                  | 5.592                 | 8.355                  | 0.491       | 0.479             |\n| FT+$L_{svd}$           | 0.147                 | 0.129                 | 0.159                  | 5.591                 | 8.358                  | 0.479       | 0.444             |\n| FT+$L_{std}$+$L_{svd}$ | 0.146                 | 0.127                 | 0.161                  | 5.592                 | 8.355                  | 0.484       | 0.469             |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033574847,
                "cdate": 1700033574847,
                "tmdate": 1700739556497,
                "mdate": 1700739556497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "51pRR86qzB",
                "forum": "4XCfu7fTgw",
                "replyto": "RNkGFAbW7W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "4. We note that the paper 'Rank-N-Contrast: Learning Continuous Representations for Regression' was published on Arxiv after the submission of our paper and remains unpublished. Given its post-submission release and unpublished status, it was not valid for comparison in our study.\n\n**Questions**\n\nQ1: FT stands for Fine-tuning; we included this explanation in Section 4.1 of the revised paper. \n\nQ2: $L_{std}$ demonstrates better performance over the baselines in numerous instances. Please note that the mixup technique synthesizes the data with a new label distribution. When the new label distribution differs significantly from the original, the $L_{svd}$ loss might mislead the model into aligning the label distributions by constraining the scale of the output. We hypothesize that high label and sample density can prevent this issue, a condition that holds true in the presence of large datasets. Therefore, given the large size of MPI3D, $L_{svd}$ is more suitable for this dataset compared to the other seven, as detailed in Table 3 of our updated paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033616296,
                "cdate": 1700033616296,
                "tmdate": 1700739837135,
                "mdate": 1700739837135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eiOOQaM7wX",
            "forum": "4XCfu7fTgw",
            "replyto": "4XCfu7fTgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an innovative approach for generalizing regression tasks by leveraging the metric learning assumption that emphasizes the proportional relationship between features and labels. The method incorporates a std. loss and spectral loss to address two key aspects: ensuring the distance proportionality between features and labels and enabling OOD generalization. The effectiveness of the proposed method is demonstrated through experiments conducted on multiple datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work addresses an emerging issue in regression tasks, namely the challenge of handling OOD data. As the author notes, while OOD generalization has been studied in classification tasks, it has not been explored in depth for regression tasks.\n\n2. The proposed method involves measuring the feature-label distance proportion using a mapping function and aligning real and synthesized distributions by minimizing the difference between the spectral norms of their feature representations."
                },
                "weaknesses": {
                    "value": "1. The organization and statements in this paper can be unclear at times, as the authors attempt to cover a lot of ground on the topic. For example, the abstract section contains too many details that may not be necessary. In essence, the paper proposes an OOD generalization method for regression tasks, which involves two penalties to address feature-label distance proportion and distribution gap issues. However, some of the irrelevant expressions can make it difficult to grasp the main topic at first.\n\n2. The title of the paper is also unclear and does not directly convey the main theme, similar to the abstract. It lacks a clear focus and fails to capture the essence of the research."
                },
                "questions": {
                    "value": "1. I am confused about why the title is \"Spectral Contrastive Regression.\" On one hand, the title does not explicitly mention OOD generalization, which is the main focus of the paper. On the other hand, the term \"spectral\" does not seem to directly relate to the contrastive loss used in the paper. While the paper introduces concepts of spectral and contrastive learning in the context of a regression task, the title may give the impression of avoiding the core content and introducing the concept of contrastive learning.\n\n2. Throughout the entire paper, the concept of contrastive learning is not emphasized enough. The term \"contrastive\" appears only five times in the main text and is not even mentioned in the abstract. While this expression may not be crucial for the technical contributions of the paper, the overall writing style feels somewhat disjointed. Unlike traditional contrastive learning, the concept is not reinforced, and even after reading about the std. loss, it is surprising to see the section titled \"Relational Contrastive Learning.\" The paper gives the impression of being written in a fragmented manner. This is just my personal perception and may not necessarily be correct.\n\n3. Building upon the previous point, I understand that the authors utilize the relationship between feature and label distances, adopting a contrasting perspective to examine this proportion and control its fluctuation by proposing a loss based on standard deviation. However, I still question why this loss and the keyword \"contrastive\" are not aligned, and instead, the paper introduces the concepts of standard deviation and the corresponding expressions in the abstract. Overall, it might be my personal bias, but I feel that the writing in this paper lacks cohesion.\n\n4. Regarding the issue with the loss function, in Equation 4, both the first and third terms measure the difference between two distributions. The former considers the MSE between the individual in-distribution of the two distributions, while the latter measures the difference between the two distributions themselves. However, it is unclear which distribution the third term specifically refers to. Does it pertain only to the real distribution or both distributions? Equation 2 appears to be a general constraint without specifying the source of i and j from each distribution.\n\n5. The related work section appears to be somewhat perfunctory, as there is not much informative content provided in the three paragraphs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3117/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3117/Reviewer_xHdj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698543323488,
            "cdate": 1698543323488,
            "tmdate": 1699636258448,
            "mdate": 1699636258448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2RQ81ZUy1A",
                "forum": "4XCfu7fTgw",
                "replyto": "eiOOQaM7wX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- The traditional contrastive learning in classification aims to generate a more discriminative feature distribution using contrastive pairs, namely positive pairs and negative pairs. Many contrastive losses, such as triplet loss, belong to the area of metric learning and aim at minimizing the distance between positive pairs and maximizing the distance between negative pairs. In the context of regression tasks, the $L_{std}$ loss can be considered a form of metric loss, as it introduces a new metric distance $d_r$. This adaptation of the contrastive concept to regression is due to the continuous nature of the labels.\n\n\n- Generating a discriminative distribution with continuous labels is a challenge in regression tasks. As discussed in the paper, the ideal situation is to have $d_r$ constant at $d_r^\\ast = |{W^\\ast}^{-1}_p|$. Under this framework, we can define two contrastive pair sets:\n\n$\n    S_l = \\left[(f_i, f_j)|d_r(f_i,f_j) < d_r^\\ast\\right];\n    S_g = \\left[{(f_i, f_j)|d_r(f_i,f_j) > d_r^\\ast\\}\\right]\n$\n\n\nIn this case, contrastive metric learning in regression aims to minimize the variance, ensuring that the feature distance \n aligns more closely with the optimal proportion. This is achieved by minimizing the distance within $S_l$ (where feature pairs are closer than the ideal distance) and maximizing the distance within $S_g$ (where feature pairs are farther than the ideal distance). The $L_{std}$ loss helps to decrease unexpected variance in the feature space, thus maintaining the proportionality essential in regression tasks.\n\n- We thank the reviewer for the suggestion and acknowledge the need for a new title to better reflect the paper's content and to distinguish it from traditional concepts of contrastive learning. We propose an updated title of \"Unifying Scale and Proportion for Feature-Level Generalization in Regression\", and will replace the subtitle of Section 3.2 with: \"Proportional distance for Continuous Label Spaces\". These changes aim to more accurately convey the focus of the paper on optimizing feature distances in regression tasks and its contribution to metric learning with continuous labels.\n\n- The second term $L_{std}$ is calculated for the two distributions separately, represented as $L_{std}^{real}$ and $L_{std}^{syn}$, respectively. The final $L_{std}$ loss is the sum of $L_{std}^{real}$ and $L_{std}^{syn}$. The third term $L_{svd}$ is only applicable when there is more than one distribution. According to Equation 3, $L_{svd} = |max(s_{real}) - max(s_{syn})|$, where each singular value is calculated from one distribution. Therefore, there should be at least two distributions for the $L_{svd}$ term to be applicable."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625882937,
                "cdate": 1700625882937,
                "tmdate": 1700740044317,
                "mdate": 1700740044317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MnZZCO2Mk3",
            "forum": "4XCfu7fTgw",
            "replyto": "4XCfu7fTgw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_Rgn5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3117/Reviewer_Rgn5"
            ],
            "content": {
                "summary": {
                    "value": "To improve the generalization of deep regression problems, the authors present a new objective composed of several ideas including relational contrastive learning, spectral alignment, and augmented sample pairs. The experiments are extensively conducted on multiple benchmarks and show improvement over baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The manuscript is clear and easy to follow.\n- The experiments show good results and are conducted on multiple different datasets.\n- The idea is technically sound and the authors present a neat combination of several different ideas to improve the generalization of deep regression problems."
                },
                "weaknesses": {
                    "value": "I'm concerned about the technical novelty. Though the experiments show good improvement over baselines, in the current state of the manuscript, the proposed objective is the combination of several different terms that are similar to some existing work. Please see point 1 and point 2 in the next section of Questions. The authors may consider including more ablation studies to further solidify the technical contribution."
                },
                "questions": {
                    "value": "- More ablation studies on Eq.4. The authors have conducted ablation studies on $\\alpha$ and $\\beta$. \n\n  In Eq.4, does $\\mathcal{L}_{std}$ include augmented samples? \n\n   Since $\\mathcal{L}_{mse}$ includes the augmented samples, I suggest the authors also conduct an ablation study on how much improvement is introduced by using augmented samples in mse loss term.\n\n- Missing related work. One of the core ideas in the proposed objective $\\mathcal{L}_{std}$ is that the distance between features and labels should be proportional. A similar idea can be found in deep regression problems [1], which showed similar patterns in the feature space with t-SNE visualization. The authors should properly discuss and compare the differences and similarities.\n\n- In Fig.2, $\\beta$ introduces little to no effect on the metrics for varying its values in the entire range. Do the authors have any speculation or analysis on this pattern? Because from Tables 1 and 2, single svd loss term can provide significant improvement and sometimes has the best performance. But when combined with std, it does not show a significant effect.\n\n- It can provide a full picture of how the proposed objective works if the authors can have t-SNE visualization for only svd loss term, and the sum of svd + std loss terms.\n\n\n\n\n- Minor points: it might be better for the audience if the abbreviated 'FT' can be explained as 'fine-tuning' before using it. \n\n\n\n[1] Gong et al., RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression. ICML 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627936125,
            "cdate": 1698627936125,
            "tmdate": 1699636258313,
            "mdate": 1699636258313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nChq8pRuz8",
                "forum": "4XCfu7fTgw",
                "replyto": "MnZZCO2Mk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time. We incorporated your feedback into the updated paper by adding more experiments. Additionally, we address your questions below.\n\nQ1. $L_{std}$ is calculated for both the original distribution and the augmented distribution. As requested by the reviewer, we have conducted extra experiments to study the effect of augmentation and the proposed $L_{std}$. The following table shows the results of an ablation study on $L_{std}$, with and without augmentation.\n|                           | No2              | No2                   | ExchangeRate     | ExchangeRate          | RCF-MNIST             | DTI                | DTI                 |           |\n|---------------------------|------------------|-----------------------|------------------|-----------------------|-----------------------|--------------------|---------------------|-----------|\n|                           | RMSE$\\downarrow$ | MAPE (\\%)$\\downarrow$ | RMSE$\\downarrow$ | MAPE (\\%)$\\downarrow$ | avg. RMSE$\\downarrow$ | avg. $R$$\\uparrow$ | worst $R$$\\uparrow$ | avg. rate |\n| FT(w/o augment)           | 0.521            | 13.256                | 0.0236           | 2.405                 | 0.147                 | 0.479              | 0.444               | -         |\n| FT(w augment)             | 0.519(0.38\\%)    | 13.201(0.45\\%)        | 0.0233(1.27\\%)   | 2.387(0.75\\%)         | 0.146(0.68\\%)         | 0.479(0\\%)         | 0.458(3.15\\%)       | 0.95\\%    |\n| FT+$L_{std}$(w/o augment) | 0.517            | 13.120                | 0.0165           | 1.563                 | 0.152                 | 0.483              | 0.449               | -         |\n| FT+$L_{std}$(w augment)   | 0.510(1.35\\%)    | 12.879(1.84\\%)        | 0.0161(2.42\\%)   | 1.529(2.18\\%)         | 0.145(4.61\\%)         | 0.491(1.66\\%)      | 0.479(6.68\\%)       | 2.96\\%    |\n\nAs can be seen from the table, FT+$L_{std}$(w augment) always outperforms FT(w/o augment). Compared with FT(w augment), FT+$L_{std}$(w augment) brings extra 1\\%-3\\% improvement on average."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625110709,
                "cdate": 1700625110709,
                "tmdate": 1700739367563,
                "mdate": 1700739367563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ed5lpGEN0N",
                "forum": "4XCfu7fTgw",
                "replyto": "MnZZCO2Mk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q2. There are significant differences between Ranksim and our method. RankSim considers the relation between the order of feature distance and the order of label distance instead of the proportion. So, it is hard for RankSim to keep the predictor Lipschitz continuous. That can lead to steep slope in the feature distribution, which is shown by the discontinuity in the visualization of Figure 1 of the updated paper. The following tables show the results of RankSim compared with our method. We skip the datasets RankSim can not be applied. Our method outperforms RankSim in most cases. Also, since RankSim has a similar motivation with $L_{std}$, we also visualize the resulting feature distribution on DTI dataset in Figure 1 of the updated paper, and compare it to our method. As can be seen, even though Ranksim has a more discriminative pattern, there are more breakpoints in the distribution because of lacking the Lipschitz continuous property. This may be the reason that our proposed $L_{std}$ can perform better than RankSim.\n\n|                        | Airfoil        | Airfoil       | No2            | No2           | ExchangeRate    | ExchangeRate   |\n|:----------------------:|:--------------:|:-------------:|:--------------:|:-------------:|:---------------:|:--------------:|\n|                        | RMSE  $\\downarrow$          | MAPE(%)  $\\downarrow$     | RMSE $\\downarrow$           | MAPE(%)$\\downarrow$       | RMSE$\\downarrow$             | MAPE(%) $\\downarrow$       |\n| FT+RankSim             | 2.635          | 1.537         | 0.520          | 13.188        | -               | -              |\n| FT                     | 2.541          | 1.474         | 0.519          | 13.201        | 0.0233          | 2.387          |\n| FT+$L_{std}$           | 2.586        | 1.501         | 0.510          | 12.879 | 0.0161 | 1.529 |\n| FT+$L_{svd}$           | 2.489 | 1.443 | 0.517          | 13.161        | 0.0233          | 2.391          |\n| FT+$L_{std}$+$L_{svd}$ | 2.516          | 1.460         | 0.506 | 12.896        | 0.0176          | 1.691          |\n\n\n|                        | RCF-MNIST             | Crime                 | Crime                  | SkillCraft            | SkillCraft             | DTI         | DTI               |\n|:----------------------:|:---------------------:|:---------------------:|:----------------------:|:---------------------:|:----------------------:|:-----------:|:-----------------:|\n|                        | avg. RMSE$\\downarrow$ | avg. RMSE$\\downarrow$ | worst RMSE$\\downarrow$ | avg. RMSE$\\downarrow$ | worst RMSE$\\downarrow$ | R$\\uparrow$ | worst R$\\uparrow$ |\n| FT+RankSim             | 0.239                 | 0.135                 | 0.164                  | 5.324                 | 7.577                  | 0.479       | 0.464             |\n| FT                     | 0.146                 | 0.129                 | 0.156                  | 5.592                 | 8.358                  | 0.479       | 0.458             |\n| FT+$L_{std}$           | 0.145                 | 0.128                 | 0.157                  | 5.592                 | 8.355                  | 0.491       | 0.479             |\n| FT+$L_{svd}$           | 0.147                 | 0.129                 | 0.159                  | 5.591                 | 8.358                  | 0.479       | 0.444             |\n| FT+$L_{std}$+$L_{svd}$ | 0.146                 | 0.127                 | 0.161                  | 5.592                 | 8.355                  | 0.484       | 0.469             |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625142418,
                "cdate": 1700625142418,
                "tmdate": 1700728004926,
                "mdate": 1700728004926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N6qmpxdd9I",
                "forum": "4XCfu7fTgw",
                "replyto": "MnZZCO2Mk3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3117/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q3.1 **``Because from Tables 1 and 2, single svd loss term can provide significant improvement and sometimes has the best performance. But when combined with std, it does not show a significant effect\"**\n\nWe are uncertain about your comment in Q3.1, as $L_{std}$ demonstrates better performance over the baselines in numerous instances. Please note that the mixup technique synthesizes the data with a new label distribution. When the new label distribution differs significantly from the original, the $L_{svd}$ loss might mislead the model into aligning the label distributions by constraining the scale of the output. We hypothesize that high label and sample density can prevent this issue, a condition that holds true in the presence of large datasets. Therefore, given the large size of MPI3D, $L_{svd}$ is more suitable for this dataset compared to the other seven, as detailed in Table 3 of our paper.\n\nQ3.2  **``In Fig.2, $\\beta$ introduces little to no effect on the metrics for varying its values in the entire range. Do the authors have any speculation or analysis on this pattern? \"**\n\nWe agree that the impact on $L_{svd}$ appears insignificant in Fig. 2, likely due to the large scale of $L_{std}$. This might give the impression that $L_{svd}$ is unaffected by $\\beta$. To clarify this, we have presented the hyperparameter analyses of $\\alpha$ and $\\beta$ separately in the revised version of our paper. For the analysis of $\\beta$, we find that setting the hyperparameter to 1000 on the DTI dataset results in better performance for $L_{svd}$. As we discussed earlier, the MPI3D dataset might be more suited for $L_{svd}$, so we have also included a sensitivity analysis of $\\beta$ for the MPI3D dataset in the Appendix of the updated paper. The impact of $L_{svd}$ is significant in some cases: the model achieves the best results when $\\beta$ is set around 1. However, if $\\beta$ is too large, causing $L_{svd}$ to dominate the total loss, the performance worsens\n\nQ4. We added the t-SNE visualization in the Appendix of the updated paper.\n\nQ5. FT stands for Fine-tuning; we included this explanation in Section 4.1 of the revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625324057,
                "cdate": 1700625324057,
                "tmdate": 1700739464562,
                "mdate": 1700739464562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]