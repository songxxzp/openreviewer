[
    {
        "title": "Motion PointNet: Solving Dynamic Capture in Point Cloud Video Human Action"
    },
    {
        "review": {
            "id": "OxP69zMBYX",
            "forum": "OdGyza5FO1",
            "replyto": "OdGyza5FO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Motion PointNet for point cloud video action recognition. The Motion PointNet builds on PointNet++ encoder and a PDEs-solving module to capture input dynamics. The author adopts two state training to train the model. In first stage, the model is trained under the spatial feature reconstruction objective from the temporal features. In second stage, the model is fine-tuned for action recognition. The method outperforms previous approaches across different benchmarks, while the model is also lightweight compared with others."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Strong recognition performance**. The proposed Motion PointNet outperforms previous approaches on 3 public datasets in action recognition. \n\n - **Light model**.  While the performance is strong, the model is quite light in model size and computation. \n\n- **Rich comparison**. The paper makes a thorough comparison with state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "- **Understanding reconstruction objective** The loss function in (15) is not fully make sense. Given the model is trained from scratch, the \nGT $F_s$ used as supervision is also random features at the beginning, how would the contrastive objective leads the model towards the correct direction as it is the only loss used in pre-training stage?\n\n\n- **Motivation of PDE is not clear**. After reading the paper, I still do not quite understand why we need a PDE to build the mapping from temporal features to spatial features. What will be changed if we replace the spectral model with some MLP or transformer like networks as   long as we make a nonlinear mapping between two spaces. \n\n- **Why 2 stage training**. Could we jointly train a classification head along with the contrastive objective? What will the performance like.  \n\n- **Missing simple baselines**. There are some simple baselines the method should compare with. 1, train a model with the same encoder and classification head using the same number of iterations of two stage training. 2, only fine-tune the classification head while freeze the encoder to evaluate the pre-trained presentation."
                },
                "questions": {
                    "value": "- **Feature response in figure 3**. How does those orange points are being selected? It seems like binary selection (I expect to see more colors represent the strongness of different points instead of the binary setting) What is the response from other methods (like PointNet ++) , this visualization comparison could tell the model indeed capture those dynamics. In second row of Side Kick, it seems the binary response still contain many irrelevant points based on locality instead of temporal features. \n\n\n- **Does the pre-trained features generalizable**. It seems in first stage training there is no labels required, so it is fully unsupervised. I wonder if the learned representation could be generalized or quickly adapted to other domains like what visual representation work did (i.e. MAE)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818522618,
            "cdate": 1698818522618,
            "tmdate": 1699636053475,
            "mdate": 1699636053475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "420396g90c",
                "forum": "OdGyza5FO1",
                "replyto": "OxP69zMBYX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer isat Comments and Our Responses"
                    },
                    "comment": {
                        "value": "We are grateful for the insightful feedback provided by the reviewer and would like to address the concerns raised:\n\n**Response to [Comment 1]:** Thanks for this insightful comment. The contrastive objective is indeed the sole loss during pre-training, but it does not rely on random features. Instead, it utilizes the inherent spatial and temporal coherence within the point cloud video data itself. Our contrastive loss encourages the model to learn a representation that captures the underlying structure of the point cloud sequence, which, while initially random, quickly becomes structured as the model begins to learn the spatial-temporal dynamics inherent in the data. This aligns with the principles of self-supervised learning, where the data provides its own supervision. \n\n**Response to [Comment 2]:** We appreciate the reviewer's insightful view of using pure MLP or transformer-like networks for the PDEs-solving module design. Indeed, it is feasible to remove the spectral method in our proposal. However, this will harm the performance in a great extent (see Section 4.3, Tab. 8). We emphasize that the spectral method with multiple basis operators (Eq.10) holds a stronger universal approximation capacity with theoretical guarantees than those rough deep models attempt to learn the operator as a whole. We would like to claim that the superiority of the design of our PDEs-solving module is supported by both theoretical and experimental analysis. We explain the motivation for this design in our response to **[Reviewer RMAg 's Comment 1 & 2]**. Please check for more details.\n\n**Response to [Comment 3 & 4]:** Thanks for these two insightful comments. The two-stage training process is designed to first allow the network to learn robust spatio-temporal features from the encoder and then to refine this representation using the classification head in the second stage. In this way, we can keep our encoder as lightweight as possible while still enforcing its strong learning ability. However, we acknowledge the potential benefits of exploring different training settings and concur with the reviewer's point. We have reported the results of training on the encoder only in Section 4, Tab. 7. We also add the two baseline comparisons below. This will demonstrate the effectiveness of our proposed method beyond the complexity of the architecture and training procedure. The experiments are conducted on the MSR-Action3D Dataset with 24 frames. Due to page limitations in the main text, we will augment the experimental results and discuss more in the supplementary material.\n\n| Training Setting | Accuracy(%) |\n|---:|---:|\n| pretrain + finetune | 97.52 |\n| setting 1 | 95.75 (-1.77) |\n| setting 2 | 96.11 (-1.41) |\n\nHere the **pretrain + finetune** is the setting used in our paper, **setting 1** indicates the suggested baseline 1, and **setting 2** indicates the suggested baseline 2.\n\n**Response to [Question 1]:** The high-response points are selected based on the magnitude of the feature response, with a threshold set to emphasize those points that most significantly contribute to the action recognition task. The binary representation was chosen for clarity in visualization. We have revised the descriptions for the generation of visualization in the caption of Fig. 3. We will also provide comparative visualizations with methods like PointNet++ to better illustrate our model's ability to capture dynamics in the future appendix.\n\n**Response to [Question 2]:** The PDEs-solving module is tailored for the task of point cloud video action recognition to address the issue we mentioned in Section 1 (pages 1-2). However, the unsupervised nature of the first training stage does indeed mean that the representations learned are not tied to a specific label set, allowing for potential generalization to other domains. However, limited time for rebuttal does not allow us to conduct extant experiments to further evaluate its variegation. Therefore, we plan to explore the potential for quick adaptation to other domains in our future research.\n\nWe thank the reviewer for the opportunity to improve our paper and hope our responses and planned revisions will address the concerns satisfactorily."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310605026,
                "cdate": 1700310605026,
                "tmdate": 1700310605026,
                "mdate": 1700310605026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cbnYiktGMW",
                "forum": "OdGyza5FO1",
                "replyto": "420396g90c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I read the updated revised manuscript and the response, but my concern still holds. \n\nMy biggest concern is still about Comment 3 & 4. From the provided ablation, it seems the improvement of the method mainly comes from the proposed good encoder (Tab 7) and the 2-stage training (setting 1 & 2). This implies the proposed PDE module might not as important as expected. It is weird to fine-tune the whole network after pre-training, because under this way what the pre-training does is only provide a good initialization (and for generalization ability we don't know). You could also do 2-stage training with the classification loss and the first stage only serves as finding good initialization. This is not the standard way for pre-training. Similarly, for other comparison baselines, do you consider using the same training iterations as well. Because the task is classification, so I wonder longer training will help. \n\nFor Comment 1, I read the two mentioned papers ((Wu et al., 2023; Liu et al., 2023) they use PDE to solve the dynamic because the system input and output itself (i.e. fluid) is described by the PDE. For point cloud videos, I disagree the statement that \" input-output mappings in high-dimensional space can be challenging for a simplistic deep model to cover\", while Transformers does show great performance in many more challenging tasks beyond classification. And I won't treat Transformers as simplistic. My concern still holds: what is the underlying system behavior to motivate to incorporate the PDE module, because the dynamic in point cloud videos are deformations and should not be described as PDE . For the encoder only performance (95.76), do you also use 2-stage training or training the same number of iterations as the 2-stage model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466810651,
                "cdate": 1700466810651,
                "tmdate": 1700466810651,
                "mdate": 1700466810651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rpnSNTQCjZ",
                "forum": "OdGyza5FO1",
                "replyto": "goxp0QPHff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1267/Reviewer_isat"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response from the author. I still have several strong concerns for the things below. \n\n1. Overfitting issue\n\nThe author mentioned their method could mitigate overfitting. But there is no such experiments to demonstrate the current model is not overfitting. There is no cross-dataset generalization experiments to any other dataset like PointCMP and VideoMAE. As I also noticed the author mentioned \"report the best epoch from the whole training\", I assume it is conducted on training set (the author never mention there is validation set, on test set is **prohibited**) , then the training routine in the paper could not avoid over-fitting. The author also mentioned the reason it could avoid overfitting is because\"distinctly separating the feature learning and task-specific learning phases\",  but using a lr of 0.01 and fine-tune on the same dataset will definitely destroy all learnt features from the pre-train stage. The overfitting is also never mentioned in the paper. \n\n\n2. Pre-train and fine-tune\n\nIn Faster R-CNN, the backbone is pre-trained on ImageNet with the classification task and fine-tuned on  PASCAL for detection, this is different from pre-training and fine-tuning on the same dataset. In PointCMP and VideoMAE, they both report the standard fine-tuning, linear probing performance (I suggested earlier) and cross-dataset transfer learning performance (see my original comment) to demonstrate the effectiveness of pre-training, but in the paper there are none of these experiments. So I could hardly tell the pre-training work. Also MSR dataset with **567** videos is too small for ablations on pre-training and fin-tuning.\n\nI encourage the author to conduct more ablations to show the pre-trained stage with PDE helps to learn robust and **generalizable** features (i.e. follow the PointMCP). The current experiment setting is unsatisfied in my view."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634886904,
                "cdate": 1700634886904,
                "tmdate": 1700634886904,
                "mdate": 1700634886904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yGx21V2v2B",
            "forum": "OdGyza5FO1",
            "replyto": "OdGyza5FO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_KFbJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_KFbJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called Motion PointNet for dynamic capture in point cloud video human action recognition. The key contributions are:\n\n- They propose to view the dynamic capture process as solving partial differential equations (PDEs) in the feature space. This provides a new perspective to model the temporal dynamics.\n\n- They design a lightweight PointNet-like encoder to generate spatio-temporal features from point cloud sequences. \n\n- They introduce a PDEs-solving module to reconstruct the spatial features from the temporal features. This establishes a temporal-to-spatial mapping and enhances dynamic modeling. \n\n- The proposed method achieves state-of-the-art results on MSRAction-3D, NTU RGB+D, and UTD-MHAD datasets, with high efficiency in terms of parameters and FLOPs.\n\n- Ablation studies demonstrate the effectiveness of the PDEs-solving module in improving dynamic capture."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents a highly original approach for point cloud video action recognition by formulating the dynamic modeling as a PDEs-solving problem. Here are the key strengths:\n\n**Originality**: The perspective of using PDEs-solving for point cloud video modeling is novel and has not been explored before. Converting the dynamic capture to a PDEs problem with a temporal-to-spatial mapping provides a new way to establish temporal guidance.\n\n**Quality**: The proposed method achieves state-of-the-art results on multiple benchmarks with high efficiency, demonstrating its effectiveness. The comparisons to previous works are comprehensive. The ablation studies verify the contribution of each component.\n\n**Clarity**: The method is clearly explained with sufficient details and illustrations. The problem formulation of PDEs-solving for dynamic modeling is intuitive. The network architecture and training process are well elaborated. \n\n**Significance**: This work opens up a new direction of using PDEs-solving techniques for point cloud sequence modeling. The concept of converting dynamic modeling to a PDEs problem can inspire more future work. The high performance and efficiency also make the method attractive for real-world applications.\n\nIn summary, it proposes a novel perspective for dynamic point cloud modeling, achieves strong results, and clearly explains the key ideas. The PDEs-solving concept introduces new possibilities for point cloud video analysis."
                },
                "weaknesses": {
                    "value": "While the paper presents a novel and effective approach, here are some weaknesses that could be improved:\n\n- The formulation and explanation of the PDEs-solving could be more rigorous mathematically. Some key equations lack details on the formulations.\n\n- The design space of the PDEs-solving module could be explored more thoroughly. For example, how are the basis operators and reconstruction loss function chosen?\n\n- The comparisons to some recent works like PointMapNet are missing. This could help better demonstrate advantages over other lightweight models.\n\n- The evaluations are limited to action recognition. It remains unclear how the dynamic modeling capability would transfer to other tasks like segmentation or detection.\n\n- There lacks ablation and analysis on different encoder architectures. Can other lightweight encoders also benefit from the PDEs-solving?\n\n- The computational complexity and efficiency analysis is incomplete. Actual runtime comparisons could better demonstrate the speed.\n\n- The model interpretability is limited. Visualizations or analyses connecting the PDEs-solving to improved dynamics are lacking."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833591572,
            "cdate": 1698833591572,
            "tmdate": 1699636053413,
            "mdate": 1699636053413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m2ifYunDTZ",
                "forum": "OdGyza5FO1",
                "replyto": "yGx21V2v2B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer KFbJ Comments and Our Responses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty and quality of our approach and for acknowledging the clear presentation and significance of our work.\n\n**Response to [Comment 1 & 7]:** We acknowledge the importance of a rigorous mathematical foundation and a better interpretability of the proposed methods. We reorganize Section 3.2 in the revision paper to include a more thorough derivation of the PDEs and the reasoning behind our chosen formulations. Please check the revision paper for more details.\n\n**Response to [Comment 2]:** We appreciate this insightful comment. \n\nFirstly, we designed our PDEs-solving module based on the combination of the attention mechanism and the classic spectral methods for PDE. We explain the motivation for this design in our response to **[Reviewer RMAg 's Comment 1]**. Please check for more detail.\n\nSecondly, we use a contrastive-based InfoNCE loss in our pretrain stage to refine the focus of dynamic capture to a distinct objective. By treating the $Feature \\in \\mathbb{R}^{T\\times M}$ (getting from the encoder) before applying spatial/temporal pooling as the negative sample, we force the model to learn the implicit mapping\u00a0between the spatial and temporal space, instead of reconstruct the de-pooling feature by closer both Ft and Fs to $Feature$ (which against the learning objectives). We have conducted several ablation studies in Section 4.3 (Tab. 8) to underscore the superiority of our loss design. Additional discussions will be included in the Section 3.2 to justify our choices.\n\n**Response to [Comment 3]:** We addressed this by including a comparison with recent works like PointMapNet, showcasing the advancements our model brings to the field. These comparisons are added to Section 4.2 to strengthen our argument of the proposed method's superiority.\n\n**Response to [Comment 4]:** We agree that evaluating our approach on a diverse set of tasks could demonstrate its generalizability. But, limited time for rebuttal does not allow us to conduct extant experiments to further evaluate its variegation. Therefore, we plan to extend our evaluation to tasks such as segmentation and detection for our future research.\n\n**Response to [Comment 5]:** Yes, our PDEs-solving module can also improve the performance of other encoders. We have conducted ablation studies the PST-Transformer encoder to demonstrate the broad applicability of our PDEs-solving approach in Section 4.3 (Tab. 7).\n\n**Response to [Comment 6]:** Thanks for this comment. To demonstrate the actual speed and offer a clearer picture of our model's efficiency, we provide runtime comparisons and will include this part in Section 4 (Tab. 2). The experiments is conducted on single NVIDIA A100 GPU, on the MSR-Action3D Dataset with 24 frames.\n\n| Methods | Runtime (ms) |\n| -- | -- |\n| MeteorNet\t        | 80.11 |\n| PSTNet   \t        | 63.88 |\n| P4Transformer\t| 25.18 |\n| PST-Transformer\t| 69.37 |\n| Motion PointNet\t| **1.17** |\n\nWe are confident that the revisions and additions proposed in this rebuttal will address the concerns raised by the reviewer and improve the overall quality and impact of our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328575147,
                "cdate": 1700328575147,
                "tmdate": 1700328575147,
                "mdate": 1700328575147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u9WoeMsHYj",
            "forum": "OdGyza5FO1",
            "replyto": "OdGyza5FO1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method that extends PointNet++ for point cloud video processing. To tackle the motion information in point cloud videos, a partial differential equation (PDE) method is proposed. Experiments on the MSRAction-3D and NTU RGB+D datasets show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method is effective and efficient. \n2. Using PDE to solve point cloud video problems looks novel."
                },
                "weaknesses": {
                    "value": "1. It is not that clear what the most important part in the PDEs-solving module. To my understanding, it is basically a variant of Transformer. More comparision with vanilla Transformer is encouraged. \n\n2. It cloud be better to provide more details of PDEs and explain more the reason to use  the PDE method."
                },
                "questions": {
                    "value": "The PDEs-solving module seems independent of point clouds. I wonder whether  the proposed module can be used for traditional video understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1267/Reviewer_RMAg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841118078,
            "cdate": 1698841118078,
            "tmdate": 1699636053347,
            "mdate": 1699636053347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vs0ahI2qIE",
                "forum": "OdGyza5FO1",
                "replyto": "u9WoeMsHYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer RMAg Comments and Our Responses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty and effectiveness of using PDEs to solve point cloud video problems and affirming the efficiency of our method.\n\n**Response to [Comment 1]:** The core component in our PDEs-solving module is the spectral method which we detailed in Section 3.2 (PDEs-solving Module, Eq.10-12). We would like to emphasize several points to respond to the reviewer's question more clearly.\n\n1. **Superiority of spectral method:** In implementing our idea, which formulates the dynamic modeling as a PDEs-solving problem, there are several options for the design of our PDEs-solving module. For instance, a pure Transformer method [1]. As discussed in previous work [2, 3], such transformer-based design attempts to learn the operator as a whole to approximate input-output mappings. However, dealing with input-output mappings in high-dimensional space can be challenging for a simplistic deep model to cover adequately while still maintaining network efficiency. To tackle the complex mappings problem, the spectral methods that we use decompose complex nonlinear mappings into multiple basis operators (Eq.10), which also holds the universal approximation capacity with theoretical guarantees.\n\n2. **Ablation results:** Nevertheless, the strong perceptivity inherent in the attention mechanism cannot be overlooked. We design our PDEs-solving module base on the combination of both the Transformer model and the spectral method. We conducted ablation experiments in Section 4.3 (Tab. 8) to make a comparison with the regular Transformer. After removing the spectral method, our PDEs-solving module can be viewed as a pure Transformer model with an encoder-decoder structure. Results in Tab. 8 highlight a substantial decline in performance upon removing the spectral method, indicating its critical importance in our model.\n\nDue to page limitations in the main text, we will augment our discussion in the supplementary material.\n\n**Response to [Comment 2]:** We appreciate the reviewer's suggestion for a more comprehensive elaboration on the PDEs. \n\nFirstly, traditional methods relying solely on global supervision from the action labels lack temporal guidance, potentially leading to an inadequate representation of dynamics in the point cloud video action recognition. To address this limitation, we decouple the spatial-temporal feature and build a PDEs problem by a directive temporal-to-spatial mapping. Together with the contrastive loss (Eq. 15) in the pretrain stage, our PDEs-solving module refines the focus of dynamic capture to a distinct objective. By doing so, we expect to provide temporal guidance. \n\nAnother insight is that PDEs can directly incorporate the feature variations over time as a variable in the modeling process. This process is visualized as Eq. 10-12 in our proposal, where we use multiple basis operators to optimize the complex temporal-to-spatial mapping. This allows a more accurate and nuanced representation of how point cloud features evolve over time compared to previous temporal modeling approaches, which is crucial for recognizing and interpreting human actions accurately.\n\nDue to page limitations in the main text, we will expand on our methodology in the supplementary material to offer a clearer explanation.\n\n**Response to [Questions 1]:** We appreciate the reviewer's suggestion regarding the potential use of our PDEs-solving module in traditional video understanding. While theoretically feasible, given that both point cloud videos and traditional videos share spatial-temporal characteristics, adapting this module for structured pixel-based video data poses significant challenges. Unlike point clouds, traditional videos represent data in a highly structured, grid-like pixel format, which poses different demands on data processing and feature extraction. Adapting the PDEs-solving approach, initially tailored for the unstructured nature of point clouds, to this structured format would require significant modifications. These might include redefining the way spatial relationships are perceived and processed within the PDE framework. Additionally, the encoding of temporal information in pixel-based videos is substantially different from point clouds, necessitating a reevaluation of how temporal dynamics are captured and integrated using PDEs in this new context. Given these challenges, exploring the application of our PDEs-solving module to traditional video understanding represents an intriguing and valuable direction for our future research.\n\n### **Reference**\n\n[1] Liu, Xinliang, et al. \"Ht-net: Hierarchical transformer based operator learning model for multiscale pdes.\" arXiv preprint arXiv:2210.10890 (2022).\n\n[2] Wu, Haixu, et al. \"Solving High-Dimensional PDEs with Latent Spectral Models.\" International Conference on Machine Learning (2023).\n\n[3] Karniadakis, George Em, et al. \"Physics-informed machine learning.\"\u00a0Nature Reviews Physics\u00a03.6 (2021): 422-440."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308347257,
                "cdate": 1700308347257,
                "tmdate": 1700308347257,
                "mdate": 1700308347257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]