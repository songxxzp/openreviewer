[
    {
        "title": "A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models"
    },
    {
        "review": {
            "id": "Oi6AJpbq5V",
            "forum": "NdzJKFf0Q2",
            "replyto": "NdzJKFf0Q2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_rnSG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_rnSG"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims on the problem of copyright.  It proposes two baselines to forget the specific concept or style to protect copyright, gradient ascent and pruning. They are proved to be more effective than previous methods for style forgetting and portrait forgetting.\n\nFurthermore, It propose a dataset to test the forgetting methods. The dataset has four categories: \"Style of Artist\" includes images showcasing the unique artistic style of various artists, encompassing distinctive brushstrokes, lines, colors, and compositions found in their paintings. \"Portrait of Celebrity\" comprises of portrait images of notable individuals. \"Artistic Creation Figure\" consists of virtual cartoon and character images found in artistic and literary works. Lastly, \"Licensed Illustration\" encompasses artist illustrations that are protected by copyright."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed two baselines for forgetting seems simple yet effective"
                },
                "weaknesses": {
                    "value": "1. This paper aims at a too big topic. The proposed 4 categories themselves are too vague. For example, *style*: art is still developing, and new art styles come out every day. Combining two art styles sometimes can be a new art style. How to define \"combination\" itself can be a copyright question; *portrait*: it is hard to use facial features to define the identity as many people look similar, such as twins, siblings. How similar can be considered as the same person? They might not be a problem when investigating \"forgetting\" methods, but it is a big problem when it comes to copyright. Furthermore, how to define \"infringingly similar\" itself is a very big problem, let alone copyright.\n2. Although in Section 4.2, the dataset is divided into 4 parts: Style, Portrait, Artistic Creation Figure, and Licensed Illustration, the baselines are not tested in the four parts.\n3. In the semantic metric, the cosine similarity is changed to MSE for CLIP scores. \"easier to observe changes\" is not a convincing reason. There is neither qualitative or quantitative experiment demonstrating it.\n4. Although the definition of anchor images can be inferred from the paper, but there is no official definition of it in the paper.\n5. In Figure 6, the unlearning methods totally break the model, generating meaningless things."
                },
                "questions": {
                    "value": "1. Why take square in Equation 5? Both Semantic Metric and Style Metric are MSE, which are non-negative values.\n2. What is the Semantic Metric and Style Metric are MSE for each experiment? Despite being more complicated, I believe analyzing two metrics can be more interesting. I would suggest to add both along with the proposed CM.\n3. In Table 2, why FIDs are the same for Cartoon, Portrait, and Wikiart?\n4. In Table 3, why the FID decreases after gradient ascent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper aims on a really big, important, and sensitive problem: copyright of images from the text-to-image generative models. If the paper is not in a good enough format, it can be abused, considering the large related industry already existing in the world. Yet it is not good enough considering the weakness listed above."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6995/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698133318587,
            "cdate": 1698133318587,
            "tmdate": 1699636818789,
            "mdate": 1699636818789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uhsQ1IxBm7",
            "forum": "NdzJKFf0Q2",
            "replyto": "NdzJKFf0Q2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_oagV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_oagV"
            ],
            "content": {
                "summary": {
                    "value": "This is an interesting work, which tackles the challenge of copyright infringement by text-to-image (stable diffusion) techniques. It highlights the absence of comprehensive studies, datasets, and standardized metrics in this domain. To address these gaps, the authors present the first dataset and benchmark for evaluating potential copyright abuses by SD, using a combination of CLIP, ChatGPT, and diffusion models to generate a relevant dataset and propose new evaluation metrics. These contributions aim to provide a foundation for future research on effective copyright protection in AIGC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of this work is summarized below:\n1. The creation of a specialized dataset that simulates potential abuses of copyright offers a tailored benchmark for testing and validating the proposed metrics. The dataset encompasses a variety of image types, including artistic styles, portraits, and animated characters, making it a versatile tool for research and application.\n2. The benchmark itself is very novel and serves as a potential good pipeline for plagirism evaluation."
                },
                "weaknesses": {
                    "value": "The weaknesses of this work come from the following aspects:\n1. I think the main concern of this work is the lack of methodological novelty. Based on my understanding and my knowledge, the entire Sec. 3 is already well-known. It seems Sec. 4 does not incorperate too much novel methods. The main novelty of this work comes only from the dataset and the benchmark.\n2. The lack of extensive study on the unlearning methods make this paper an incomplete work. For example, there are a lot of machine unlearning methods for concept-forgetting, and I listed them below. However, none of the methods is considered in this work. Without the results of the following methods, the reviewer believes this is far from a benchmark.\n    > [1] Erasing Concepts from Diffusion Models\n\n    > [2] Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models\n\n    > [3] Ablating Concepts in Text-to-Image Diffusion Models\n\n    > [4] Unified Concept Editing in Diffusion Models\n\n    > [5] Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models\n\n    > [6] Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models\n\n    > [7] SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation\n3. I think Figure 3 is not informative enough, some key elements are missing, such as the color bar units. Also, it seems that there are some areas that have even larger magnitude than the diagnal line, is this normal? Finally, what if the semantic is copyrighted but the style is not (or reverse), will the CPDM metric not informative enough in this case?\n4. Typo: in page 4 paragraph \"Style metric\":  inGatys et al. (2015) ->  in Gatys et al. (2015)"
                },
                "questions": {
                    "value": "I do not have additional questions and please see my comments in the Weakness column."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6995/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699042181685,
            "cdate": 1699042181685,
            "tmdate": 1699636818662,
            "mdate": 1699636818662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iusmGZk4q4",
            "forum": "NdzJKFf0Q2",
            "replyto": "NdzJKFf0Q2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_G9qh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6995/Reviewer_G9qh"
            ],
            "content": {
                "summary": {
                    "value": "The authors addressed the issue of copyright protection from text-to-image diffusion models. They have proposed a dataset for this task and performed benchmarking."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The task is novel and interesting. Content protection from diffusion model generated images are respectively less investigated and are potentially useful for copyright protection.\n2. The problem setup, benchmarking and flow of the paper is clear and well-written."
                },
                "weaknesses": {
                    "value": "1. Why CPDM metric is squared of two losses? Why not any other powers or any other combinations? \n2. Can you provide some justification of defining the style metric? \n3. Can there be any metric using content-style Disentanglement [ref], which is known to be editing dimensions of images?\n4.  What will be the used cases for this unlearning task? Can you provide any practical test case ?\n5. Can the unlearning be transferable? E.g., Can a portrait image be unlearned to be a cartoon image? Can this type of style transfer be done across categories in the dataset?"
                },
                "questions": {
                    "value": "Please check weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6995/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699340235773,
            "cdate": 1699340235773,
            "tmdate": 1699636818562,
            "mdate": 1699636818562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]