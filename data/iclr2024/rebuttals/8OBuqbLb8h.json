[
    {
        "title": "Fast-ELECTRA for Efficient Pre-training"
    },
    {
        "review": {
            "id": "0Hl0hgerj9",
            "forum": "8OBuqbLb8h",
            "replyto": "8OBuqbLb8h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_QMNg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_QMNg"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores efficient ELECTRA training methods by advocating the use of a pre-trained/existing language model as an auxiliary model, rather than simultaneous training of the model and auxiliary model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method is both intuitive and effective.\n2. The problem it tackles is highly practical."
                },
                "weaknesses": {
                    "value": "1. The proposed method appears tailored specifically for ELECTRA, potentially limiting its applicability and community interest.\n2. Could we consider applying a continual learning method (e.g., [1]) to enhance ELECTRA's efficiency?\n\n[1]: Adapting a Language Model While Preserving its General Knowledge, Ke et al., EMNLP 2022"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8511/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8511/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8511/Reviewer_QMNg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629387904,
            "cdate": 1698629387904,
            "tmdate": 1699637063374,
            "mdate": 1699637063374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6d9zrKqOfT",
                "forum": "8OBuqbLb8h",
                "replyto": "0Hl0hgerj9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable feedback. Please see our detailed response below.\n\n\n**___**\n\n__Q. Applicability of ELECTRA-style pretraining__\n\nWe note that ELECTRA is widely applied in training encoding models and also among the state-of-the-art. Our method can further extend its applicability as it reduces the training cost and memory consumption of ELECTRA to be comparable to other pretraining schemes such as Mask Language Modeling (MLM). \n\nFurthermore, we firmly believe that the ELECTRA-style pretraining, which can achieve huge performance improvement over MLM with the same model size and training data, still has lots of merits that are worth studying, not only for the pretraining of encoding/decoding language models but also for the pretraining of large models in a wider context.\n\n**___**\n\n__Q. Correlation with continual learning__\n\nWe appreciate the reviewer for bringing this work [1] to our attention. We believe there are indeed many correlations between ELECTRA-style pretraining and continal learning. \n\nOn one side, existing continual learning methods can be great motivations for better designs of ELECTRA-style pretraining. Since strong auxiliary models can often hurt the performance of the main model in ELECTRA-style pretraining, it is necessary to design an auxiliary model training scheme that is aware of the main model learning. This requires the auxiliary model to be able to generate meaningful token replacements while not too difficult to ruin the main model learning. This is highly related to the idea proposed in the work mentioned by the reviewer, namely to adapt existing language models to both preserve necessary general knowledge and incorporate domain-specific knowledge.\n\nOn the other side, ELECTRA-style pretraining may also be adapted for the continual learning field. For example, one may consider using a small auxiliary model trained on the specific domain to better adapt the main model while preserving necessary general knowledge.\n\nWe will include the above discussion in the revision.\n\n**___**\n\n__Reference__\n\n[1] Adapting a Language Model While Preserving its General Knowledge, Ke et al., EMNLP 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728699798,
                "cdate": 1700728699798,
                "tmdate": 1700729731683,
                "mdate": 1700729731683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SMkPx6LjLc",
            "forum": "8OBuqbLb8h",
            "replyto": "8OBuqbLb8h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_rs11"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_rs11"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple and effective technique to improve electra training. By replacing the training of the auxiliary model with a pre-trained model together with temperature scaling and a gradually decreased temperature, the proposed method significantly reduced the memory usage and training time of electra training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Simple and effective method.\n2. Good performance.\n3. Very clear presentation."
                },
                "weaknesses": {
                    "value": "The scale of models in experiments seems a bit limited under the current standard. Have you tried larger models?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680372578,
            "cdate": 1698680372578,
            "tmdate": 1699637063225,
            "mdate": 1699637063225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uDpisZhLY3",
                "forum": "8OBuqbLb8h",
                "replyto": "SMkPx6LjLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable feedback. Please see our detailed response below.\n\n**___**\n\n__Q. Larger models__\n\nWe believe for encoding models, the BERT-base and BERT-large equivalent model sizes, as covered in this paper, are among the most widely used ones. Existing works have also mostly adopted such model sizes [1, 2]. \n\nNevertheless, we note that our method can extend the applicability of ELECTRA for training potential large-scale models, as it reduces the training cost and memory consumption of ELECTRA, such that it is now comparable to other popular pretraining schemes such as Mask Language Modeling (MLM). \n\n**___**\n\n__Reference__\n\n[1] RoBERTa: A robustly optimized BERT pretraining approach. Liu et al., 2019.\n\n[2] DeBERTaV3: Improving Deberta Using Electra-style Pre-training with Gradient-Disentangled Embedding Sharing. He et al., 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728259803,
                "cdate": 1700728259803,
                "tmdate": 1700728531744,
                "mdate": 1700728531744,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OZp0QdPntA",
            "forum": "8OBuqbLb8h",
            "replyto": "8OBuqbLb8h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_dT26"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8511/Reviewer_dT26"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes efficient pretraining with ELECTRA by using a fixed generator (auxiliary model) instead of training the aux model together with the discriminator for replace token detection (RTD). In order to simulate a curriculum of difficulty that training of aux model provides, it uses an exponentially decaying schedule on the *temperature* used to sample from aux model.\n\nEfficiency: Since the aux model is fixed, Fast-ELECTRA saves on backward passes leading to overall 20-30% FLOPs per step. (This calculation ignores original training of the auxiliary model.) It also saves 20-25% memory in the aux model. One could also cache the aux model predictions and save a lot more FLOPs (30-40%).\n\nQuality: Fast-ELECTRA is competitive with many BERT and ELECTRA related baselines, even slightly better on some GLUE downstream evals\n\nRobustness: The paper evaluates robustness to curriculum schedule and discriminator model size, and finds that Fast-ELECTRA behaves more gracefully compared to ELECTRA and can handle higher learning rates.\n\nThe paper also performs interesting ablation studies with using some simple aux models and different (linear/poly) schedules for curriculum. Overall, the paper presents a conceptually interesting finding that that a fixed pretrained aux model can be used, with a temperature schedule. Incomplete comparisons to earlier ideas made it harder to judge novelty"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Novelty: The idea of using a fixed aux model for efficiency is interesting, and novel to my knowledge (although I'm not entirely sure since I could not find much discussion about this). Similarly the idea of using decaying temperature as a curriculum in this context is quite interesting\n\nQuality: The paper provides a nice analysis of computation and memory benefits of the method.\n\nClarity: The paper is easy to follow for most part. Connections to prior work and some other details could be presented better."
                },
                "weaknesses": {
                    "value": "Comparison to prior work: \n\n- It would be helpful to highlight the most relevant work in Table 1 that a reader should focus in. Additionally, is there any evaluation on prior work that uses fixed generator? It would also help to include some FLOPs comparison to the baselines used in Table 1. The paper will also help with a discussion on accuracy-efficiency tradeoff. Lack of such discussions made it harder to assess the full value of proposed method.\n\n- Recent paper (Dong et al.) from ICML 2023 proposed a different strategy of decoupling generator and discriminator optimizers and has better GLUE metrics than Fast-ELECTRA. This does not dilute the contributions of this paper much because Fast-ELECTRA also leads to training speed up, and is conceptually different. However it will be helpful to include and compare to this method. It could be an interesting open question if the gap to Dong et al. can be reduced with a fixed aux model.\n\n\nMissing discussions: Most of the paper assumes the existence of a good pretrained aux model, but ignores the cost of training the aux model itself. Does Fast-ELECTRA truly reduce total FLOPs in that case?\n\n\nOverall I believe that the paper makes a positive contribution. I'm currently assigning a score of weak accept mainly due to above reservations about comparisons to prior work.\n\nDong et al. Understand and Modularize Generator Optimization in ELECTRA-style Pretraining. ICML 2023"
                },
                "questions": {
                    "value": "- In Eq 2 what is the range of $u$? What is the final temperature in that case? If $u$ is indeed fraction of training updates, then at the max value of $u=1$ the final temperature has a value different from 1. Is that intended?\n\n- Section 3 says \u201cthe auxiliary model expends about 67% of the computation cost\u201d If aux model is just 1/3 the size, why does it contribute so much to computation cost?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8511/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860258815,
            "cdate": 1698860258815,
            "tmdate": 1699637063116,
            "mdate": 1699637063116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qwXKP0mASH",
                "forum": "8OBuqbLb8h",
                "replyto": "OZp0QdPntA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8511/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable feedback. Please see our detailed response below.\n\n**___**\n\n__Q. Efficiency improvement over existing methods__\n\nAmong Table 1 of our paper, those methods that follow the ELECTRA-style pretraining framework are the most relevant to this paper, including MC-BERT [1], COCO-LM [2], AMOS [3], DeBERTaV3 [4], and METRO [5]. For a brief introduction to these methods, one can refer to the related work (Section 6) in our paper. \n\nNevertheless, note that all these works train the generator jointly with the discriminator. To the best of our knowledge, there is no prior work using a fixed generator, potentially because it will fail to match the performance achieved by joint training, as also shown by Section 5.2 in our paper.\n\nTherefore, we believe the major contribution of our proposed method, namely using a fixed generator for ELECTRA-style pretraining without significant performance loss, is **orthogonal** to prior works. Existing ELECTRA-style pretraining methods can be modified with a fixed generator and achieve similar efficiency improvement.\n\nFinally, regarding the accuracy-efficiency trade-off, we didn\u2019t observe significant performance loss by using our method to improve the training efficiency. Our method is still among the state-of-the-art in terms of downstream performance, as shown in Table 1 of our paper. Nevertheless, it may be indeed slightly worse compared to the most recent work mentioned by the reviewer, as discussed below.\n\n**___**\n\n__Q. Comparison to a recent work__\n\nWe appreciate the reviewer for bringing up this recent work [6]. We note that our paper is different from [6] in terms of both the motivation and the methodology.\n\nIn terms of motivation, [6] aims to improve the effectiveness of the ELECTRA-style pretraining, by controlling the training trajectory of the generator for better discriminator learning. In comparison, our work aims to improve the training efficiency of the ELECTRA-style pretraining, by reducing the training cost of the generator. \n\nIn terms of the methodology, [6] decouples the optimizers of the generator and the discriminator, while the generator is still jointly trained with the discriminator. In contrast, Fast-ELECTRA completely decouples the generator and discriminator training. As a result, the generator can be trained separately, and only needs to be trained once and reused for the discriminator training. \n\nFinally, we note that there is indeed a gap between the performance of our method and that of [6]. We agree with the reviewer that it would be tempting if such a gap could be reduced while the auxiliary model is still fixed, such that we can achieve efficiency and performance at the same time. We believe this is possible by designing a better learning curriculum but we would like to leave it as a future work.\n\nWe will include the above discussion in the revision per the reviewer\u2019s suggestion.\n\n\n\n**___**\n\n**__Q. Training cost of the auxiliary model__**\n\nWe note that Fast-ELECTRA will reduce the overall training cost even if one pre-trains the auxiliary model themself. \n\nFirst, the practical development of language models often requires multiple rounds of trial and error to locate the best architecture setup and hyperparameter combinations. With the joint training design in the original ELECTRA, the auxiliary model will be trained and discarded repeatedly in each round, resulting in significant resources being wasted. In contrast, with our method, one only needs to train the auxiliary model once and can reuse it for subsequent training rounds of the main model. \n\nFurthermore, we note that Fast-ELECTRA can reduce the memory cost of ELECTRA-style pretraining as well, which also improves the training efficiency. In fact, large-scale language model pretraining, calling for larger model sizes and longer sequence lengths, is probably more often bottlenecked by memory consumption [7]. With the joint training design in the original ELECTRA, the auxiliary model will consume significant memory in addition to that of the main model, which limits the applicability of ELECTRA-style pretraining in many memory-constraint scenarios compared to other pretraining frameworks such as Masked Language Modeling (MLM). In contrast, with our method, the auxiliary model\u2019s memory cost during main model training can be significantly reduced or even eliminated (with offline processing), which can potentially make ELECTRA more memory-efficient than MLM, and in turn, can also reduce the training time as one can now fit more training data into the memory."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8511/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727850710,
                "cdate": 1700727850710,
                "tmdate": 1700727850710,
                "mdate": 1700727850710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]