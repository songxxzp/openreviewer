[
    {
        "title": "Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation"
    },
    {
        "review": {
            "id": "CH5YCwswGn",
            "forum": "MIEnYtlGyv",
            "replyto": "MIEnYtlGyv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_msmy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_msmy"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces Symphony, an autoregressive technique designed for the generation of 3D molecular structures via spherical harmonic projections. Unlike prevailing autoregressive models like G-SchNet and G-SphereNet, which employ rotationally invariant features of degree 1, Symphony leverages higher-degree E(3) equivariant features. The proposed model demonstrates superior performance in generating small molecules from the QM9 dataset and offers competitive results when compared to the E(3)-equivariant diffusion model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well written and well organized.\n2. Distinct from other autoregressive equivariant generative models, the proposed approach employs higher-degree E(3) equivariant features. This enhances the flexibility in representing probability distributions.\n3. In terms of performance, the proposed method is on par with existing diffusion models. However, it boasts a significant advantage in computational efficiency, as current diffusion models rely on fully-connected graphs that may pose scalability challenges."
                },
                "weaknesses": {
                    "value": "1. The manuscript employs a coarse discretization of the radial component for predicting the relative positions of subsequent atoms, a limitation acknowledged in the conclusion section. It would be intriguing to explore the effects of employing a finer discretization. Specifically, would such a refinement significantly compromise the computational efficiency of the method? Furthermore, could this potentially improve the distribution of bond lengths?\n2. n Equation (1), would it be more precise to describe this as a conditional distribution of $f$ given $S$? does the embedder at $f_{n+1}$ use both $h^{\\text{position}}$ and $h^{\\text{focus}}$?\n3. Typo: In the final equation on page 4, the term should correctly be denoted as $p^{\\text{position}}$.\n4. On the first line of page 6, the mechanism for predicting the STOP condition remains unspecified. Could you elaborate on this aspect?\n5. Typo: The first equation in Section 2 misses a translation $T$."
                },
                "questions": {
                    "value": "Please refer to the previous section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603556762,
            "cdate": 1698603556762,
            "tmdate": 1699636077958,
            "mdate": 1699636077958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kx90GLtxue",
                "forum": "MIEnYtlGyv",
                "replyto": "CH5YCwswGn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review! We answer specific questions below. Please also take a look at our common response.\n\n> would such a refinement significantly compromise the computational efficiency of the method?\n\nWe answer exactly this question below! The following is the approximate training speed of the model for a batch size of $16$ graphs, as measured on a single NVIDIA RTX A5000 GPU:\n\n| Number of Radii| $32$ | $64$ | $128$ | $256$ |\n| -------- | -------- | -------- | -------- | -------- |\n|  Approximate Training Speed (steps/second)     |   $16.5$   | $13.8$ | $10.1$ | $6.6$ |\n\nSo while there definitely is a tradeoff, it does not seem to be a major bottleneck. We are currently exploring the effect of the radial discretization on the bond length metrics.\n\n> In Equation (1), would it be more precise to describe this as a conditional distribution of $f$ given $S$?\n\nHmm, it is just a matter of notation, whether to represent it as $p(f | S)$ or $p(f; S)$. In any case, the fragment $S$ is given at training time, but is sampled from the model's predictions at inference time. So, both options are okay.\n\n> does the embedder at $f_{n + 1}$ use both $h^\\text{position}$ and $h^\\text{focus}$?\n\nFor predicting the focus and the target atom type, $h^\\text{focus}$ is used.\nFor predicting the positions, $h^\\text{position}_f$ is used. The two embedders never interact with each other directly.\n\n> Typo: In the final equation on page 4, the term should correctly be denoted as $p^\\text{position}$.\n\nWe believe this formulation is correct: we are parametrizing the logits $f^\\text{position}$ of the distribution $p^\\text{position}$. The logits are then exponentiated to get the distribution, as described in Section 3.3.\n\n> On the first line of page 6, the mechanism for predicting the STOP condition remains unspecified. Could you elaborate on this aspect?\n\nGreat find! We have added this to Section 3.3 under Equation 5.\n\n> Typo: The first equation in Section 2 misses a translation $T$.\n\nThe features we look at here are invariant under translation. We have clarified this in the text below.\n\nThank you again!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295582446,
                "cdate": 1700295582446,
                "tmdate": 1700295582446,
                "mdate": 1700295582446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6fJcOTTjPO",
                "forum": "MIEnYtlGyv",
                "replyto": "kx90GLtxue",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_msmy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_msmy"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response. My rating remains."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646157182,
                "cdate": 1700646157182,
                "tmdate": 1700646157182,
                "mdate": 1700646157182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mxa7hTv9y5",
            "forum": "MIEnYtlGyv",
            "replyto": "MIEnYtlGyv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_cp6D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_cp6D"
            ],
            "content": {
                "summary": {
                    "value": "The authors present Symphony, an E(3)-equivariant autoregressive generative model, that leverages a unique parametrization of 3D probability densities with spherical harmonic projections to make predictions based on features from a single focus atom. This approach captures both radial and angular distributions of potential atomic positions. Additionally, the model incorporates a novel metric based on the bispectrum to assess the angular accuracy of generated local environments. The authors claim that the model demonstrates superior performance on the QM9 dataset compared to previous autoregressive models and is competitive with existing diffusion models across various metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Symphony uses a novel parametrization of 3D probability densities with spherical harmonic projections, allowing for predictions based on features from a single focus atom (novelty). This method overcomes the limitations of traditional approaches, such as G-SchNet and G-SphereNet, which require at least three atoms to precisely determine the position of the next atom, eliminating uncertainties due to symmetry.\n\n2. The NN for probability distribution over positions satisfies normalization and non-negativity constraints by applying softmax functions.\n\n3. A novel metric based on the bispectrum is introduced to assess the angular accuracy of matching generated local environments to similar environments in training sets.\n\n4. Symphony is able to generate valid molecules with a high success rate, even when conditioned on unseen molecular fragments."
                },
                "weaknesses": {
                    "value": "1. Complexity: Spherical harmonic projections and inverse projections involve complex mathematical operations, potentially increasing the computational complexity of the model. There is no comparison regarding the time spent by the network among different autoregressive generation models, such as G-SchNet and G-SphereNet.\n\n2. Approximation Error: Spherical harmonic projections are an approximation method and may introduce errors, especially when a limited number of spherical harmonics are used.\n\n3. Dependence on Focus Atom: This method relies on the selection of an appropriate focus atom; if the focus atom is poorly chosen, it may affect the accuracy of predictions. (not specific for the paper).\n\n4. Challenges in Handling Symmetry: When dealing with the symmetry of molecules, ensuring that the predicted distributions are symmetrically equivariant(although proved) may increase the complexity of the model.\n\n5. The experiments are comprehensive, but the improvement is marginal because\n\n(1) In Table 1, the EDM performs best;\n\n(2) In Table 3, the MMD of Bond lengths from Symphony is worse than from EDM."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799394223,
            "cdate": 1698799394223,
            "tmdate": 1699636077891,
            "mdate": 1699636077891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EvsQtrlE9l",
                "forum": "MIEnYtlGyv",
                "replyto": "mxa7hTv9y5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cp6D"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback! We answer specific questions below. Please also take a look at our common response.\n\n> There is no comparison regarding the time spent by the network among different autoregressive generation models, such as G-SchNet and G-SphereNet.\n\nThis comparison is present in Section 4.4 of our manuscript. Symphony is slower than G-SchNet and G-SphereNet. Symphony is currently 3x faster than EDM, but we believe the sampling time for Symphony can be improved by avoiding some of JAX's limitations. We measured the inference times ourselves on the same NVIDIA RTX A5000 GPU. \n\n> Approximation Error: Spherical harmonic projections are an approximation method and may introduce errors, especially when a limited number of spherical harmonics are used.\n\nWe show the effect of increasing $l_\\max$ and the number of position channels for Symphony variants in Section G.1 (Figure 12). We did not feel that much utility could be obtained on going to $l_\\max > 5$. We also measure the error due to the sampling grid by measuring the validity of molecules generated in Section G.2 (Figure 13).\n\n> When dealing with the symmetry of molecules, ensuring that the predicted distributions are symmetrically equivariant(although proved) may increase the complexity of the model.\n\nThis is true, but we believe it is necessary that a model should respect the underlying symmetries of a problem. Ours is just one approach, however."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294880256,
                "cdate": 1700294880256,
                "tmdate": 1700294880256,
                "mdate": 1700294880256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SDZi8cbuNi",
            "forum": "MIEnYtlGyv",
            "replyto": "MIEnYtlGyv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_XBL9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_XBL9"
            ],
            "content": {
                "summary": {
                    "value": "This submission provides an E(3)-equivariant autoregressive generative model for 3D molecule generation. The use of multiple channels of spherical harmonic projections is novel. The results show the performance is comparable to THE diffusion-based model EDM, but slightly worse."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The whole framework for autoregressive generation is reasonable: running message-passing on the current molecular fragments (current state), selecting the focus node, predicting atom species, and finally predicting the position of the added atom. This framework is similar to many previous autoregressive models.\n\n2. The main contribution is the definition of the distribution for the position of the added atom. The representation for $r$ by spherical coordinates and the use of spherical harmonic functions is novel and reasonable for the target task.\n\n3. Using multiple channels of spherical harmonic projections is novel and the authors provide valid reasons for using it. However, it lacks an ablation study to show how the multiple channels influence the final performance. For example, if the channel is 3 or 4."
                },
                "weaknesses": {
                    "value": "1. I think the Property (3) in page 4 is very important. And many previous autoregressive models can not solve it well. Actually I think it is not difficult to define an E(3)-equivariant model currently as many previous works have solved it well. The diffusion model doesn't face the permutation-invariant problem. But it can be serious for the autoregressive model. The main paper claims the three properties have been proved in Theorem B.1. Unfortunately, I found the authors only proved the first two properties and ignored the property (3). I DON'T believe the current framework can guarantee the permutation-invariant even the embedder is E(3)-equivariant.\n\n2. I think many criticisms of the diffusion model such as EDM on page 6 are unfair. \n\na. \"Unlike autoregressive models, diffusion models do not flexibly allow for completion of molecular fragments\". Actually, it is not difficult for diffusion models to do completion sampling by replacement guidance without any extra training. Currently, the diffusion model works well on image completion and I also try it on EDM, it works too.\n\nb. \"To avoid recomputation of the neighbor lists during diffusion, current diffusion models use fully-connected graphs where all atoms interact with each other. This could potentially affect their scalability when building larger molecules.\" I believe it is even more challenging for the autoregressive model on the large molecules as the sequence could be too long. And the molecules in QM9 used in the experiment are very small. There is no evidence that the proposed framework performs better on large molecules compared with EDM.\n\nc. \"diffusion models are significantly slower to sample from, because the underlying neural network is invoked \u2248 1000 times when sampling a single molecule.\" It is true that for EDM it needs 1000 times denoising process. But actually, it runs very fast and supports batch sampling. I tested EDM on a single GPU A100, it takes 20 seconds to sample 50 molecules (a batch). I think the submission should provide their own sampling time to support this claim. \n\n3. Though the definition of the position distribution is very clear and reasonable, the current submission doesn't provide any details on how to sample from it after training. People can know $f^{position}$ for any given position, but it is not easy to sample from an energy function. I doubt this sampling process can be very time-consuming."
                },
                "questions": {
                    "value": "1. Why did you choose 64 uniformly spaced values from 0.9A to 2.1A? I think the model can learn the distribution automatically by equation (3).\n\n2. Since there are 64 spaced values for $r$, I am wondering how you solve them during the sampling process.\n\n3. Can you give some explanation as to why the EDM outperforms the proposed framework for many metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699261260968,
            "cdate": 1699261260968,
            "tmdate": 1699636077826,
            "mdate": 1699636077826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AifMx3dWfy",
                "forum": "MIEnYtlGyv",
                "replyto": "SDZi8cbuNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XBL9"
                    },
                    "comment": {
                        "value": "Thank you for your great feedback and taking the time to review our paper! We answer specific questions below. Please also take a look at our common response.\n\n> it lacks an ablation study to show how the multiple channels influence the final performance. For example, if the channel is 3 or 4.\n\nThis has now been added in Section F.2 (synthetic task) and Section G.1 (QM9). There is a clear benefit when adding channels for smaller $l$.\n\n> Unfortunately, I found the authors only proved the first two properties and ignored the property (3). I DON'T believe the current framework can guarantee the permutation-invariant even the embedder is E(3)-equivariant.\n\nSorry for omitting this in our original manuscript. In fact, we had an error in our property (3): we meant to say that the focus distribution should be permutation-equivariant. We have added a proof of this in Theorem B.2. Note that we are not claiming that the entire generation process is invariant to the choice of ordering of nodes; this is almost impossible to guarantee for an autoregressive model. Note that G-SchNet and G-SphereNet both impose some sort of ordering on the way atoms are added. We are also planning to explore some alternate strategies compared to the nearest-neighbours logic in our paper. \n\n> There is no evidence that the proposed framework performs better on large molecules compared with EDM.\n\nThis is a fair criticism; we will plan to demonstrate Symphony on the larger GEOM-DRUGS dataset that has been used in other work.\n\n> I think the submission should provide their own sampling time to support this claim.\n\nWe have provided the sampling times in our common response above (and in Section 4.4 of the paper). Symphony is currently 3x faster than EDM, but we believe the sampling time for Symphony can be improved by avoiding some of JAX's limitations. We measured the inference times ourselves on the same NVIDIA RTX A5000 GPU.\n\n> Though the definition of the position distribution is very clear and reasonable, the current submission doesn't provide any details on how to sample from it after training. \n\nSorry for omitting this, we have added this in our common response (and in Section D of the Appendix of the paper). Hopefully this clears up any confusion.\n\n> Why did you choose 64 uniformly spaced values from 0.9A to 2.1A? I think the model can learn the distribution automatically by equation (3).Since there are 64 spaced values for $r$, I am wondering how you solve them during the sampling process.\n\nThis should now be clear from Section D of the Appendix. Basically, we do not have a orthonormal basis over the space $(r, \\\\theta, \\\\phi)$,  only the space $(\\\\theta, \\\\phi)$. Discretization of $r$ was the simplest alternative; we plan to explore other basis functions (and normalizing flows) soon.\n\n> Can you give some explanation as to why the EDM outperforms the proposed framework for many metrics?\n\nGreat question! We are also not super sure; the radial discretization that Symphony uses seems to hurt the bond length metrics. However, note that EDM is also trained for 10x longer than Symphony was. We are exploring if the gap can be bridged with further training."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294355825,
                "cdate": 1700294355825,
                "tmdate": 1700294454326,
                "mdate": 1700294454326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nzhpTvAovJ",
            "forum": "MIEnYtlGyv",
            "replyto": "MIEnYtlGyv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to generate molecules autoregressively, invariant to Euclidean transformations. First, the authors transform the dataset of molecules into a dataset of sequences, adding one atom per time step. This is then turned into a autoregressive generative problem. Iteratively: a \"focus\" atom in the past is sampled, then the atomic charge of the next atom, then the position of the next atom relative to the focus atom. By having the focus selection and atomic charge distributions invariant to Euclidean transformations, and the position distribution equivariant, the resulting distribution is invariant to Euclidean transformations. The procedure is made permutation invariant by making the target sequences choose the nearest neighbour next, starting from a random atom, and breaking ties randomly.\n\nThe network uses higher-order representations of the rotation group to allow for more precision in the position distribution, which is an energy based model / harmonic exponential family on the spherical manifold. In the experiments, the method outperforms other autoregressive methods, while being outperformed by all-on-one diffusion methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The code is provided and is readable.\n- Autoregressive generation of molecules can be a fast alternative to e.g. diffusion methods.\n- The method outperforms other autoregressive methods.\n- The translational symmetry is elegantly handled via the focus atom. Also the rotational equivariance relative to the focus atom is sensible, compared to rotations around a center of mass, as is commonly done."
                },
                "weaknesses": {
                    "value": "- in effect, the authors define an energy based model for the positions. It appears from the code that training and sampling of positions is done by discretizing $f^{position}$ on a grid. This seems like a shortcoming of the method, as I suspect that precise positioning is important in molecular generation. The paper should be transparent about this discretization. What type and resolution of grid is used? It'd be interesting to see how the resolution affects the sample quality. If the quality is highest with a very fine grid, it'd be great if the authors could consider and evaluate alternative learning and sampling strategies for energy based models (contrastive divergence / Hamiltonian Monte Carlo / Langevin dynamics etc).\n- While the approach predicting the sequence index for the focus point is in/equivariant to permutations of the resulting molecule, it still appears to me that an opportunity for additional permutation equivariance is missing. Why didn't the authors use a permutation equivariant network on $S^n$ to select one focus atom?\n- The method performs worse than diffusion methods and don't convince that autoregressive generation is the appropriate method for molecular generation.\n\nWhen the authors address the gridding issue convincingly, I will raise my score."
                },
                "questions": {
                    "value": "- It appears to me that the position distribution is an instance of a Harmonic Exponential Family [1] on a manifold, which probably should be cited.\n- Can the authors clarify why Eq (4) is smooth wrt the radius, but has a Dirac delta in the direction component? This should mean that the KL divergence is infinite almost always? Or is the smoothening handled implicitly by a band-limited Fourier transform? If so, it's not actually a Dirac delta that's used. The authors should clarify that.\n- As high frequencies may be necessary to precisely predict positions, it'd be great if the authors can show in an ablation study that $l_{max}=5$ suffices.\n- The authors write about $E(3)$ symmetry, which includes mirroring, but subsequently only talk about rotations and translations. Did the authors mean the group $SE(3)$, excluding mirroring?\n- Can the authors comment on the runtime of autoregressive generation vs diffusion methods in terms of the number of atoms? Might it be that autoregressive methods are only faster for small molecules?\n\n\nRefs:\n- [1] Cohen, Taco S., and Max Welling. 2015. \u201cHarmonic Exponential Families on Manifolds.\u201d arXiv [stat.ML]. arXiv. http://arxiv.org/abs/1505.04413.\n\n-------\nMy concerns have been convincingly addressed and I have increased my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1490/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1490/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1490/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699286605055,
            "cdate": 1699286605055,
            "tmdate": 1700655683444,
            "mdate": 1700655683444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FlQWAtbfYJ",
                "forum": "MIEnYtlGyv",
                "replyto": "nzhpTvAovJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UhEf"
                    },
                    "comment": {
                        "value": "Thank you so much for the great review! We answer specific questions below. Please also take a look at our common response.\n\n> in effect, the authors define an energy based model for the positions.\n\nWe have clarified this above; in the autoregressive setting with teacher-forcing, we know the target distribution exactly. Further the partition function can be easily estimated via numerical integration. Thus, we do not need any complicated methods for learning the position distribution, because minimizing KL divergence works well. \n\n> The paper should be transparent about this discretization. \n\nThank you for mentioning this. As discussed in the common rebuttal, we have added this to Section D of the manuscript. We use a Gaussian Product grid for each value of $r$, which consists of a 1D Gauss-Legendre quadrature with $180$ points over $\\\\cos \\\\theta \\\\in [-1, 1]$, and a uniform grid of $359$ points over $[0, 2\\\\pi)$ for $\\\\phi$.\n\n> It'd be interesting to see how the resolution affects the sample quality.\n\nYes, we have added this in Section G.2 (Figure 13) where we vary the resolution of the grid significantly. The model does not seem particularly unstable to these variations. In Figure 14, we show that the training is also not significantly affected by the resolution for a synthetic task of learning a single distribution on a sphere. We hope that these sets of experiments help alleviate your concern about the gridding procedure.\n\n>  Why didn't the authors use a permutation equivariant network on $\\mathcal{S}^n$ to select one focus atom?\n\nWe are very sorry for the confusion. We actually do use a permutation equivariant network (GNN) on $\\mathcal{S}^n$ to select the focus atom. Our Property (3) incorrectly stated that the focus distribution should be permutation-invariant; this has now been fixed to read permutation-equivariant. We also added Theorem B.2 in the Appendix to prove that this must be the case.\n\n> It appears to me that the position distribution is an instance of a Harmonic Exponential Family [1] on a manifold, which probably should be cited.\n\nThanks for sharing this relevant article. We have added this as a citation.\n\n> Can the authors clarify why Eq (4) is smooth wrt the radius, but has a Dirac delta in the direction component? This should mean that the KL divergence is infinite almost always? Or is the smoothening handled implicitly by a band-limited Fourier transform?\n\nYes, you are correct. This is an approximate Dirac Delta distribution that this constructed using the same spherical harmonic projection idea. We have a description of this in Appendix H.\n\n> Did the authors mean the group SE(3) excluding mirroring?\n\nWe actually meant E(3) itself, because our proofs handle the case of improper rotations. Our predicted distributions will invert as well under inversions about the origin (mirroring).\n\n> Can the authors comment on the runtime of autoregressive generation vs diffusion methods in terms of the number of atoms? Might it be that autoregressive methods are only faster for small molecules?\n\nWe are currently trying to measure this (atleast approximately). The scaling time of EDM seems to be quadratic in the number of atoms (due to fully connected graphs). We will have an update soon.\n\nThanks again!\n\n\n> As high frequencies may be necessary to precisely predict positions, it'd be great if the authors can show in an ablation study that $l_\\\\max = 5$ suffices.\n\nYes, we have added an experiment with Symphony variants trained for different values of $l_\\\\max$ and number of position channels in Section G.1 (Figure 12)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700293469445,
                "cdate": 1700293469445,
                "tmdate": 1700294468562,
                "mdate": 1700294468562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Fqqy1eYFj",
                "forum": "MIEnYtlGyv",
                "replyto": "XJ8UQyLa3I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
                ],
                "content": {
                    "title": {
                        "value": "Positional distribution"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. I have some follow-up questions, mostly regarding your wording.\n\n- An energy-based model is typically [1] defined as exactly as a function $f$ such that $p(x)\\propto \\exp f(x)$, without having an explicit model for $p$, so you're definitely learning an energy-based-model. Could you clarify? It might be that you're confusing the term \"energy-based models\" this with the contrastive divergence sampling-based training methods for energy based models, but that's only one specific training way of training energy based models.\n- You write that you don't need the normalized distribution for your training objective and write \"This also avoids the key issue of estimating the partition function that occurs with energy-based models.\", but you do need to estimate Z, as it depends on the parameters of $f$. So you are computing the normalized $\\log p$ for your objective, estimated via integration, correct? Of course, as your space is only 3D, using an energy-based-model and integrating is a feasible strategy, different from higher-dimensional problems. I think the wording in your reply and revised appendix is not accurate, though.\n- You write that you have access to the true distribution $q$. Do you mean that you know the density? Or do you mean that you have access to samples? Could you clarify?\n- If you're integrating over 4 million points for each sample during training, how does this affect training times?\n- Finally, do you agree that the method introduced in sec 3.4 can be interpreted as a mixture of energy based models? The mixture coefficients are proportional to the partition function for each channel.\n\nThanks!\n\n\n[1] Teh, Yee Whye, Max Welling, Simon Osindero, and Geoffrey E. Hinton. 2003. \u201cEnergy-Based Models for Sparse Overcomplete Representations.\u201d JMLR"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475205151,
                "cdate": 1700475205151,
                "tmdate": 1700475986785,
                "mdate": 1700475986785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7LxpS9em3N",
                "forum": "MIEnYtlGyv",
                "replyto": "nzhpTvAovJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up to Edits"
                    },
                    "comment": {
                        "value": "We have edited our response above and the appendix with the changes you recommended. We hope that our additional experiments in Section G.2 help resolve your worries about the discretization. Please let us know if there are any more experiments you would like to see. Otherwise, we would greatly appreciate it if you could improve your score for our paper. Thank you again!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585729400,
                "cdate": 1700585729400,
                "tmdate": 1700585729400,
                "mdate": 1700585729400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hmXTeqBDex",
                "forum": "MIEnYtlGyv",
                "replyto": "7LxpS9em3N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1490/Reviewer_UhEf"
                ],
                "content": {
                    "comment": {
                        "value": "My concerns have been convincingly addressed and I have increased my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1490/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655732022,
                "cdate": 1700655732022,
                "tmdate": 1700655732022,
                "mdate": 1700655732022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]