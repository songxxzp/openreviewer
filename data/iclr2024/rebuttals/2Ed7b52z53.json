[
    {
        "title": "On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution"
    },
    {
        "review": {
            "id": "6fjctWgCVG",
            "forum": "2Ed7b52z53",
            "replyto": "2Ed7b52z53",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_JYL4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_JYL4"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the quaternion discrete Fourier transform. The authors establish various facts about quaternion valued circulant matrices, their connections with the regular (complex valued) DFT matrices. They use their results to estimate spectral bounds on a quaternion-valued convolution operator that has been employed in neural networks. They show that their method outperforms, in terms of computation time, a more brute-force approach based on the quaternion SVD."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper establishes facts about quaternionic Fourier transforms, circulant matrices, which may potentially be useful in applications related to the ICLR community. The authors mention several applications, concerning RGB images, orientation estimation, neural networks employing quaternionic convolution layers. The authors share results on one experiment involving such a neural network. They show that their results can be employed to find a necessary upper bound more efficiently than existing methods."
                },
                "weaknesses": {
                    "value": "Overall, the paper reads like a collection of facts looking for an application. The authors do mention connections with several applications, but these connections are not developed in the paper properly to convince a reader that the results would be directly useful. \nThe experiment performed by the authors is rather a detail in a larger application and does not really motivate a potential reader to delve into the main content. It would have been interesting to see a treatment of an RGB image using a quaternion representation and employing the statements provided in the paper, or similarly an orientation estimation problem whose treatment is facilitated by the results of the paper. \n\nAll of this does not mean that the content is wrong or not at all useful in any application, but ICLR may not be right venue for this content. If the authors would like to revise the paper, I strongly suggest including stronger connections to the applications they touch in Section 2."
                },
                "questions": {
                    "value": "The following are minor questions, and I think the main issue is lack of convincing applications for the statements in the main body.\n- Second paragraph : While quaternions help associate $i$, $j$, $k$ to each RGB channel, what does, for instance $i \\cdot j = k$ in the RGB context? How would we treat an image with more channels, or a color image  represented via hue-saturation-value using quaternions? The point I'm trying to make is that I don't see a real connection between a color image and quaternions beyond a superficial coincidence of the number of channels with imaginary units. I'd be interested to hear if I'm missing something here.\n\n- I would welcome a more detailed development of the quaternionic DFT for orientation sequences -- specifically, does the fact that orientation quaternions have unit norm have any significance in this context?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698240374563,
            "cdate": 1698240374563,
            "tmdate": 1699636315727,
            "mdate": 1699636315727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "68Rgbu8Sm6",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cICLR may not be the right venue for this content.\u201d"
                    },
                    "comment": {
                        "value": "*\u201cAll of this does not mean that the content is wrong or not at all useful in any application, but ICLR may not be right venue for this content. [..] If the authors would like to revise the paper, I strongly suggest including stronger connections to the applications they touch in Section 2\u201d*\n\nWe respectfully disagree. *We believe that ICLR is a very relevant venue for papers where the main point of the work is presenting new theoretical results*.\n\nHere are some examples of previous works where the paper is largely \u201ctheoretical\u201d, i.e. the main novelty comes in the form of theoretical results:\n\n* Sedghi, Gupta, Long, \u201cThe singular values of convolutional layers\u201d, ICLR 2019\n* Singla, Feizi, \u201cFantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers\u201d, ICLR 2021\nThe first paper (Sedghi et al.) in fact uses an experimental setup that is based around testing a ResNet on CIFAR. Our experimental setup has been inspired by this work. The second paper (Singla and Feizi) includes tests on MNIST and CIFAR. According to the meta-reviewer, \u201cthe paper should be accepted purely on the basis of its theoretical contribution, which enhances our understanding of this important topic\u201d (https://openreview.net/forum?id=JCRblSgs34Z&noteId=WUrNb_MUat1). Both great papers, in our opinion.\n\nAnother example of a great paper from ICLR 2023 \u2013 Note that there are no experiments:\n* Haase, Hertrich, Loho, \u201cLower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes\u201d, ICLR 2023\n\nAnother example of a great paper where the focus is not experimental results, published in ICML this time:\n\n* Cohen and Welling, \u201cGroup Equivariant Convolutional Networks\u201d, ICML 2023\n\nThe list of course is far from being exhaustive."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132288075,
                "cdate": 1700132288075,
                "tmdate": 1700132309991,
                "mdate": 1700132309991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X7XRykcmYy",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cthe main issue is lack of convincing applications for the statements in the main body\u201d"
                    },
                    "comment": {
                        "value": "*\u201cThe experiment performed by the authors is rather a detail in a larger application and does not really motivate a potential reader to delve into the main content.\u201d [..] \u201cthe main issue is lack of convincing applications for the statements in the main body.\u201d*\n\n**The application of Section 4 makes use of almost all of our theoretical results**:\n\n* Propositions 3.1 and 3.2 are about left-side quaternion convolution being expressed as a circulant matrix. The bulk of the NN layers uses left-side quaternion convolution. These will be treated as products of the form $Cx$, where $C$ is circulant, and use the subsequent results for circulant matrices.\n* Proposition 3.3 is about the matrix form of the QFT, and 3.4 is about a geometric intuition over this form.  Proposition 3.3 is used in conjunction with..\n* ..Proposition 3.5, where *left eigenvalues* of C are shown to be computable via a right QFT. The corresponding eigenvectors are vectors of the QFT matrix we discussed in 3.3, 3.4. **This computation is at the heart of the application in Section 4 - none of the computations would be possible without it**.\n* Propositions 3.8 and 3.9 are about doubly-block circulant matrices \u2013 again, this is absolutely a requirement as we are dealing with 2D inputs (CIFAR images) in the experiments.\n* All the theoretical results in Section 4 are there specifically to motivate and enable the Lipschitz bounding application. We show how we can clip singular values of quaternionic convolution, by the use of an auxiliary \u201c\u039e\u201d matrix. \n\nNote again that all of the lemmas, propositions, corollaries in Sections 3 and 4 are completely novel content."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132428410,
                "cdate": 1700132428410,
                "tmdate": 1700133400809,
                "mdate": 1700133400809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3tCVlQfSUc",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201creads like a collection of facts looking for an application\u201d"
                    },
                    "comment": {
                        "value": "*\u201cOverall, the paper reads like a collection of facts looking for an application. \u201c*\n\n**This is largely a theoretical paper.** Nevertheless, we feel that the Lipschitz constant bounding application is important in its own right.\n\n**If by the term \u201ccollection of facts\u201d it is insinuated that our results are of lesser value, or are \u201cincremental\u201d, please allow us to argue in favor of the opposite view**. We bring up a number of examples, which we believe that are characteristic:\n\n* Proposition 3.5 is about the **eigenstructure of Quaternionic Circulant matrices**, and their connection to the QFT matrix. The connection between Circulants and the DFT matrix is well-known for R or C \u2013 in H (set of quaternions), eigenstructure is however necessarily a far more nuanced issue. For a start, we have two different spectra: a left spectrum and a right spectrum, corresponding to left and right eigenvalues respectively. *This is a feature unique to H*. The two spectra are in general different from the other, and no connection is known to hold between them, in terms of, say, computing the left spectrum if one knows about the right spectrum, or vice versa. The literature has mostly explored the right spectrum, for which the have been proposed numerical computation algorithms. For the left spectrum however, works are far more scarce. Proposition 3.5 tells us that Quaternionic Circulant matrices and the QFT are related through the left spectrum of the former, however there has been no hitherto algorithm to compute left eigenvalues (apart from toy examples of 2x2 or 3x3 matrices). *In quaternion matrix algebra, a field which dates as far back as the first half of the $20^{th}$ century, to our knowledge this is the first work where a computational scheme to compute the left spectrum is proposed* (aside the noted small-matrix cases, cf. So, \u201cQuaternionic Left Eigenvalue Problem\u201d, 2005; Marcias-Virgos, \u201cRayleigh quotient and left eigenvalues of quaternionic matrices\u201d, Linear & Multilinear Algebra 2023). Furthermore, **it is shown to be applied to a real, relevant learning problem**.\n* In the paper, we write that the convolution theorem for R or C can be written (and proved) easily in matrix form. However, this is not possible in H (Corollary 3.6.2). In particular,  \u201cWriting proposition 3.5 in a matrix diagonalization form (A = S\u039bS\u22121), where the columns of S are eigenvectors and \u039b is a diagonal matrix of left eigenvalues, is not possible. This would require us to be able to write $CQ = Q\u039b$; however, the right side of this equation computes right eigenvalues, while proposition 3.5 concerns left eigenvalues.\u201d **So, this is another example that looks \u201ceasy\u201d in R or C, but becomes non-trivial and more nuanced / complicated in H**.\n* In Proposition 3.7iv, we state (and prove) that if $\\lambda$ and $\\kappa$ are eigenvalues of circulant matrices L and K, then their *product* LK has the eigenvalue: $\\lambda^\\nu \\kappa^\\mu$, where $\\nu = \\kappa \\mu \\kappa^{-1}$. This is a result that has no analogue in R or C, and only makes sense in H. Furthermore it generalizes smoothly into H: In a commutative algebra we would have $\\kappa$ and $\\kappa^{-1}$ cancel out, which doesn\u2019t happen for quaternions. We believe that this also a non-obvious result a priori: A first guess could be that $\\lambda^\\mu \\kappa^\\mu$ is the eigenvalue, which turns out to be wrong.\nIn proposition 4.3 and corollary 4.4.1, the matrix $\\Xi$ is introduced, in order to enable computation and bounding of the required spectra. Note the form:\t$\\begin{bmatrix}\n    \t\\lambda_m^{\\parallel} & \\lambda_n^{\\perp} \\\\\\\\\n    \t\\lambda_m^{\\perp} \t& \\lambda_n^{\\parallel} \\\\\\\\\n\t\\end{bmatrix}$.\n**There is no analogue in R or C, as simplex and perplex components make sense only in H. Furthermore, note that this elegantly generalizes from R/C**: In these domains, the off-diagonal (perplex) components would be zero, and computation would be trivial."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132944611,
                "cdate": 1700132944611,
                "tmdate": 1700462710195,
                "mdate": 1700462710195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iLoBX0nB6d",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cthe results [are not] directly useful\u201d / Impact and relevance to ICLR"
                    },
                    "comment": {
                        "value": "*\u201dThe authors do mention connections with several applications, but these connections are not developed in the paper properly to convince a reader that the results would be directly useful.\u201d*\n\nConcerning our mentioning several applications, we do believe that the scope of application of our results is extensive. This work is about two concepts \u2013 Fourier / spectral analysis and Convolution \u2013 that are at the very heart of a range of fields, from signal processing to pattern recognition and learning systems and representations. Quaternion convolution in particular, is an operation that is prevalent in the (vast?) majority of works on Quaternion Neural Networks (let me reference here the excellent survey of Parcollet, Morchid and Linar\u00e8s, \u201cA survey of quaternion neural networks\u201d, AI Review 2020). The close relation between Fourier and Convolution has long been known for the real domain, and has been *extensively* used. In a nutshell, this paper shows how this relation generalizes to the quaternionic domain, and presents a proof-of-concept application of our theoretical results.\n\nAny model and method that includes convolution or spectral analysis (in H) will be relevant to the current paper. Convolutions and Fourier are part of learning systems and representations that range from probabilistic, hierarchical models to deep neural nets. Hence, we believe that the \u201crange of impact\u201d of our work is extensive.\n\nWe have taken a conscious decision to include this specific application \u2013 it combines Quaternions in NNs, bounding the Lipschitz constant, and analyzing singular values of convolutional layers. The latter two have been the subject of interest for the ICLR / NIPS / ICML etc. community. For example:\n\n* Sedghi, Gupta, Long, \u201cThe singular values of convolutional layers\u201d, ICLR 2019\n* Singla, Feizi, \u201cFantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers\u201d, ICLR 2021\n\nOn another note, the proof-of-concept application itself is useful in its own right \u2013 it can be seen as a plug-and-play improvement over practically *any* quaternionic CNN ."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700133528548,
                "cdate": 1700133528548,
                "tmdate": 1700134052337,
                "mdate": 1700134052337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OTBSjSYGlI",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Intuition regarding mapping of RGB"
                    },
                    "comment": {
                        "value": "\u201cSecond paragraph : While quaternions help associate i,j,k, to each RGB channel, what does, for instance, i j = k mean in the RGB context? How would we treat an image with more channels, or a color image represented via hue-saturation-value using quaternions? The point I'm trying to make is that I don't see a real connection between a color image and quaternions beyond a superficial coincidence of the number of channels with imaginary units. I'd be interested to hear if I'm missing something here.\u201d\n\nIn the context of representing color images via elements of H, the main advantage is that we are allowed to treat them in a \u201cholistic manner\u201d.\n\nBy \u201cholistic manner\u201d, we mean that a multi-dimensional input may be treated as a single entity. For example, assuming that we have an image containing multiple channels, we need to write it as a structure of the form M x N x C . Strictly speaking, this is a *tensor*. Quaternions (and hypercomplex algebra in general) allow us to write this as a matrix.\n\nThis may sound like a superficial difference, but it does bear advantages: The field of matrix methods is much more mature than tensor methods. Convolutions and Fourier in the form of matrices (Circulant, Fourier matrices) is one such example, and what this paper does is showing how a range of well-known related results generalize for quaternions. If we chose to stick with the tensor representation, we\u2019d certainly be much more limited in scope. In Section 4, we discuss singular values of quaternion convolutional layers \u2013 this would be (currently?) impossible to do the exact same thing with tensors (there is no SVD for 3-way tensors). Cf. the excellent references on this topic:\n* G. Strang, \u201cLinear Algebra and Learning from data\u201d, 2019 (esp. chapter I.12)\n* C.Hillar and LH. Lim, \u201cMost tensor problems are NP-hard\u201d, J.ACM, 2013\n\nConcerning \u201cwhat does i j = k mean in the RGB context\u201d: This is an interesting question.  \n\nIdeally, we\u2019d like to have an algebra where both actions \u2013 summation and multiplication \u2013 have some direct connection with an intuitive interpretation. We\u2019re ok about summation, supposing that we use an additive color model, but multiplication in the sense of the Hamilton product is indeed difficult to interpret. Our take is that it\u2019d be super interesting if we could correlate multiplication with another intuitive, explainable action, *and* at the same time be able to construct a valid algebraic structure. There is currently work that is ongoing in other hypercomplex structures, e.g. Ruhe, Brandstetter, Forr\u00e9, \u201cClifford Group Equivariant Neural Networks\u201d, NeurIPS 2023 to cite one example. Concerning the HSV color space, a connection between the QFT transform axis and the concept of hue is drawn in Ell and Sangwine 2007, \u201cHypercomplex Fourier Transforms of color images\u201d, IEEE Transactions in Image Processing.\n\nHowever, non-straightforward interpretability is not necessarily a serious disadvantage. In our \u201cproof-of-concept\u201d application of Section 4, we use convolutional quaternion NNs to do image classification. Therein, the input layer of these NNs contains exactly what you refer to: a mapping of channels to imaginary components of a quaternion matrix. The first layer of the NN maps this image into a series of filtered-out results using the learned quaternion convolutions. (Quaternion convolutions use the Hamilton product in their definition, cf. App B,C). After the cascade of layers we get our class estimate (which btw is improved vs the vanilla QNN, using the proposed Lipschitz bounding..). \n\nThe point is that, much like what happens in real-valued NNs, usability / usefulness and interpretability don\u2019t always go together. Although, yes, interpretability would be great to have (explainable AI, we\u2019re looking at you :)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700133610898,
                "cdate": 1700133610898,
                "tmdate": 1700133610898,
                "mdate": 1700133610898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ru9URCzDZe",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerning orientations, quaternions, and the QFT (part 1)"
                    },
                    "comment": {
                        "value": "*\"I would welcome a more detailed development of the quaternionic DFT for orientation sequences -- specifically, does the fact that orientation quaternions have unit norm have any significance in this context?\"*\n\nThank you for the comment.\n\nA series of unit-norm quaternions that represent a series of rotations / orientations will translate into the (QFT) quaternion frequency domain as a set of quaternionic components that will be bounded with respect to their magnitude.\n\nIn particular, the magnitude of all components will be at most equal to $\\sqrt{N}$, where $N$ is the length of the signal.\n\nWe can see this by taking the definition of the QFT. Starting by equation (3) in the paper, we write:\n\n$$\n            |F_{L}^\\mu[u]| = |\\frac{1}{\\sqrt{N}}\\sum_{n=0}^{N-1}\n            e^{-\\mu 2 \\pi N^{-1} nu} f[n]| \\leq \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1}|e^{-\\mu 2 \\pi N^{-1} nu}| |f[n]|,\n$$\n\nwhere both components in the summation are bounded by unity (the exponential term by definition, and the $f[n]$ term by assumption that we're dealing with rotations, hence unit-norm quaternions).\n\nThus,\n\n$$\n            \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1}|e^{-\\mu 2 \\pi N^{-1} nu}| |f[n]| = \\frac{N}{\\sqrt{N}} = \\sqrt{N},\n$$\n\nwhich proves the assertion."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557831075,
                "cdate": 1700557831075,
                "tmdate": 1700557831075,
                "mdate": 1700557831075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVdZHuo9ND",
                "forum": "2Ed7b52z53",
                "replyto": "6fjctWgCVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerning orientations, quaternions, and the QFT (part 2)"
                    },
                    "comment": {
                        "value": "We have created a toy example of a series of $N=100$ unit-norm quaternions.\nWe can visualize these as rotations, in terms of Euler angles (We have used the conversion convention of the \"numpy-quaternion\" package).\nWe plot the signal as a series of the three Euler angles, represented each with a different color:\nhttps://freeimage.host/i/JnZcpe4\n\nNext, we can use the QFT to suppress high-frequency components. We compute the QFT and zero all components except those around a window of size $[DC-2, DC+2]$ around the DC component.\nThe result is a quite smoothed-out orientation series: https://freeimage.host/i/JnZWtRe\n\nNote that this corresponds to one, very specific, way to filter the input series, and supposing that are goal were to put some smoothness constraint here, one could argue that there are indeed many more ways to do it. The simplest idea would be to change the window size, or multiply each frequency component by some magnitude and so on, much like what we would do if we were using the complex DFT. In the Quaternionic FT, we have a much more expressive tool, as all rotation variates are handled \"holistically\", their correlations can in principle be very much taken into account. Two ways to see this is that a) filter weights are quaternionic, while a DFT handling of each rotation axis would correspond to weights being in the real domain, and b) we have an extra hyperparameter which is the axis $\\mu$ of the transform. (The complex DFT can be said to have a fixed axis equal to $i$).\n\nBy changing the QFT axis, we'll get different results, for the same low-freq window. We have used $\\frac{i+j+k}{\\sqrt{3}}$ in the prior example. If we use $\\frac{i+j}{\\sqrt{2}}$ we'll have: https://freeimage.host/i/JnZvSON\n\nOr with $\\mu = k$: https://freeimage.host/i/JnZSJjI . *Note in this example that the Euler angles \"wrap around\"*, as they are constrained to lie in a \"circular\" domain in terms of degrees. (we did filtering in the quaternion domain, so this isn't a problem, aside from the visualization per se. Another small \"bonus\" for using quaternions; for an interesting rant on quaternions vs euler angles see [here](https://github.com/moble/quaternion/wiki/Euler-angles-are-horrible)).\n\nAnother result, this time with a random QFT axis: https://freeimage.host/i/JntHLrb .\n\n**The connection of all the above with our paper is this: All linear filtering in the (quaternionic) Frequency domain can be cast in terms of a quaternion convolution; in this work, we prove results that enhance understanding of the relation between convolution and the Fourier transform. We explore the eigenstructure of the related matrix forms -- circulant and QFT matrix. This is important especially in a learning paradigm (as we show in Section 4 of the paper)**. In the particular, the aforementioned filtering example could correspond to a convolution, which is almost invariably used in quaternion neural networks. All aspects of this filter could be cast in terms of a neural network layer (including the QFT axis, and so on). This is equally true for a series of orientations / members of $SO(3)$."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557979540,
                "cdate": 1700557979540,
                "tmdate": 1700557979540,
                "mdate": 1700557979540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oSEkLZFfgj",
            "forum": "2Ed7b52z53",
            "replyto": "2Ed7b52z53",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_ZfTZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_ZfTZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a set of propositions that show the connection between convolution and Fourier transform for quaternions and provide a bound on the spectrum of quaternion matrices via the SVD. The experimental results compare schemes for computation of singular values of quaternion matrices."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Neural networks / linear algebra with quaternion representations may offer advantages in some applications."
                },
                "weaknesses": {
                    "value": "- The paper presents propositions, but not a single proof. The appendix largely states existing results from literature, in which the connection between the quaternion convolution and FFT, as well as the SVD are already established.\n - No experimental comparison is made for an application problem, the experiments compare the proposed algorithms to naive baselines for the linear algebra problem (SVD truncation). I do not consider norm/eigenvalue estimation to be an application problem, it is also particular to the choice of using QFT.\n - The presentation of the paper also fails to make clear the challenges or benefits of quaternions throughout, in particular,\n   * Quaternion representations are not introduced in the main body mathematically, and the difficulties of quaternion convolution are only discussed by reference to prior work.\n   * In contrast, the main body and the appendices largely restate standard algebraic derivations for DFT matrix properties and convolution. Analogous properties to the complex case are derived for quaternions with no motivation for why they are relevant to the contributions of the paper or nontrivial.\n - DFT/FFT correspondence for quaternion convolution has already been established in prior work, the paper seems to provide limited novelty, beyond establishing eigenvalue bounds (which are not motivated).\n    \n\nOverall, the paper seems to contain very incremental results for quaternion linear algebra that follow from existing work. The experimental comparisons do not consider alternative methods or previously existing works. The main theoretical results and applications considered in the paper are basic properties of quaternion convolution and DFT and basic linear algebra. I do not see specific relevance of the work to machine learning and I had difficulty gaining insight about the benefits or challenges of using quaternion convolution/DFT from both the body and the appendices of the paper.\n\nEDIT: I expanded and modified my review as I had previously failed to notice the proofs in Appendix F due to the page break on page 20."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3601/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3601/Reviewer_ZfTZ",
                        "ICLR.cc/2024/Conference/Submission3601/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768315021,
            "cdate": 1698768315021,
            "tmdate": 1700598275668,
            "mdate": 1700598275668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kfO3DBkUq5",
                "forum": "2Ed7b52z53",
                "replyto": "oSEkLZFfgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cThe paper presents propositions, but not a single proof.\u201d"
                    },
                    "comment": {
                        "value": "*\u201cThe paper presents propositions, but not a single proof.\u201d [..] \u201cOverall, the paper seems to contain very incremental results for quaternion linear algebra that follow from existing work, and lacks proofs.\u201d*\n\n**There are a total of 7 pages of proofs**. These are found in Appendix F, pages 21-27.\n\nEvery single non-trivial statement in the paper is proved and discussed therein - 13 lemmas and propositions in total, with additional proofs for the corollaries. *All proofs in the paper are novel content.*\n\nIf you feel that we should include an additional pointer in the text regarding the position of the proofs, or perhaps move the proofs to an earlier appendix, we\u2019d be happy to do so."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068664506,
                "cdate": 1700068664506,
                "tmdate": 1700068739371,
                "mdate": 1700068739371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KCS5l8sKxj",
                "forum": "2Ed7b52z53",
                "replyto": "oSEkLZFfgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cThe appendix largely states existing results from literature [..]\""
                    },
                    "comment": {
                        "value": "*\u201cThe appendix largely states existing results from literature, in which the connection between the quaternion convolution and FFT, as well as the SVD are already established.\u201d*\n\nAppendix B is titled \u201cPreliminaries\u201d and Appendix C is titled \u201cQuaternionic Convolution and Quaternionic Fourier Transform\u201d. These parts of the text indeed go through results from the literature.\n\nAppendix F contains proofs for all statements (proofs, lemmas, corollaries) that appear in the main text. **These are novel in their entirety.**\n\nIf you think that any one of our propositions has already appeared in previous work, we\u2019d definitely like to know. To our knowledge, all topics over which we present novel propositions have not been previously explored (matrix forms of the convolution, qft, their eigenstructure, doubly-block quaternionic circulant matrices, Lipschitz constant for QCNNs, and so on)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068804028,
                "cdate": 1700068804028,
                "tmdate": 1700119110304,
                "mdate": 1700119110304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rSsyY1qvOj",
                "forum": "2Ed7b52z53",
                "replyto": "oSEkLZFfgj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "title": {
                        "value": "\u201cNo experimental comparison is made for an application problem\u201d"
                    },
                    "comment": {
                        "value": "*\u201cNo experimental comparison is made for an application problem\u201d, [..]  \u201cThe experimental comparisons do not consider alternative methods or previously existing works.\u201d [..] \u201cthe experiments compare the proposed algorithms to naive baselines for the linear algebra problem (SVD truncation).\u201d*\n\n**There is experimental comparison** (outlined with results in Fig.1, Tables 1 & 2, Appendix D, Fig.2,3,4 and Table 2, ...), **and there is an application problem**. The whole of Section 4 discusses an application, and there is additional content in the Appendices \u2013 Appendix E contains additional results and details, Appendix F contains proofs, part of which are specific to the Lipschitz constant bounding application."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068863761,
                "cdate": 1700068863761,
                "tmdate": 1700119416395,
                "mdate": 1700119416395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1lnGM834Am",
                "forum": "2Ed7b52z53",
                "replyto": "kfO3DBkUq5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Reviewer_ZfTZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Reviewer_ZfTZ"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, I missed Appendix F on my prior read-through. I had expected the location of proofs to be stated along with the theorems. I updated my review. I still have many concerns about the paper, especially the focus of the presentation."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598332322,
                "cdate": 1700598332322,
                "tmdate": 1700598332322,
                "mdate": 1700598332322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4RkDzGOZ6G",
            "forum": "2Ed7b52z53",
            "replyto": "2Ed7b52z53",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_YJPK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3601/Reviewer_YJPK"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a study of the Fourier transform and the convolution operation in the quaternion domain. The authors argue that, despite the appealing properties of quaternions for modelling rotations (and other representations), there are some points of quaternions that are _problematic_ (see Sec 2). Thus, they aim to draw connections between the quaternion and standard Fourier matrix and convolutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Though I am not an expert on the current SOTA on the use of quaternions for ML or signal processing, I identify conceptual value in the paper in Section 3, where, through a series of Propositions and Corollaries, the paper delivers its contribution. A key point in applied terms is the computation of singular values and computing the Lipschitz constant in a particular example."
                },
                "weaknesses": {
                    "value": "Despite its conceptual contribution, as a non expert on quaternion-valued architectures or techniques I found it difficult to identify the impact of this work to the ICLR community. \n\n- First, the paper outlines the _major difficulties_ of quaternions. However, I don't see why non-commutativity, two-sided convolution, versions of a convolution theorem, definition of determinant and the fact that $\\mu = -1$ has infinite solutions are problems. If these issues are to be avoided, then why not go back to the real domain? Isn't the desired structure of quaternions what precisely results in these _difficulties_?\n\n- Second, it is not clear how the machinery developed in Sec 3 impacts the machine/representation learning communities. I acknowledge the representation power of quaternions, however, this paper does not exploit this representation in the context of learning. I feel that this work lacks a clearer connection between the stated contributions and the learning task, possibly by showing examples where this representation makes a difference wrt standard (non-quaternion) methods. I also acknowledge the application in Sec 4, however, the body of results presented in Sec 3 is far more general than that particular application.\n\n- There are a few typos, for instance: the _extend_ on which ...\n\n- Lastly, what do the authors mean by _[treating] signals in a holistic manner_?"
                },
                "questions": {
                    "value": "Please refer to the previous part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3601/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806771564,
            "cdate": 1698806771564,
            "tmdate": 1699636315550,
            "mdate": 1699636315550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lexty3Ugg9",
                "forum": "2Ed7b52z53",
                "replyto": "4RkDzGOZ6G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cthere are some points of quaternions that are problematic (see Sec 2). Thus, they aim to draw connections between the quaternion and standard Fourier matrix and convolutions.\u201d [..] Despite its conceptual contribution, as a non expert on quaternion-valued architectures or techniques I found it difficult to identify the impact of this work to the ICLR community. [..] Second, it is not clear how the machinery developed in Sec 3 impacts the machine/representation learning communities.*\n\nThank you for your comments. We will definitely use the feedback to clarify parts of the text that have apparently been misleading.\nLet us clarify that in Section 2 we merely provide a short discussion on the pros and cons of using quaternions as part of a learning representation framework. Perhaps it has been misunderstood that the point of the paper is arguing that \u201cquaternions are worth using\u201d. We have included Section 2 as a short intro for scholars that have perhaps heard that quaternions are useful for e.g. representing rotations, but do not know about the *modern* uses of quaternions in learning systems.\n\nWe take for granted that quaternions form a useful framework in ML and closely connected fields. **The paper is not about whether quaternions are useful or not**; we believe that this has been extensively argued in previous literature. Let us reference a few characteristic works:\n\n* Parcollet, Ravanelli, Morchid, Linar\u00e8s, Trabelsi, De Mori and Yoshua Bengio: \u201cQuaternion Recurrent Neural Networks\u201d, ICLR 2019\n* Zhang, Tay, Zhang, Chan, Luu, Hui and Fu: \u201cBeyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters\u201d, ICLR 2021\n* Zhang, Tao, Yao, Liu, \u201cQuaternion knowledge graph embeddings\u201d, NeurIPS 2019\n* Miron, Flamant, Le Bihan, Chainais and Brie, \u201cQuaternions in Signal and Image Processing\u201d, IEEE Signal Processing Magazine 2023\n* Qin, Zhang, Xu, Xu, \u201cFast Quaternion Product Units for Learning Disentangled Representations in SO3\u201d, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023\n\nThe first and second works have been published in ICLR. The second one has got an \u201coutstanding paper award\u201d in ICLR. Hence, quaternions in the context of learning representations have indeed interested the community, and have been very much welcomed by it in the recent past.\n\n**Concerning the impact of this work**: This work is about two concepts \u2013 Fourier / spectral analysis and Convolution \u2013 that are at the very heart of a range of fields, from signal processing to pattern recognition and learning systems and representations. Quaternion convolution in particular, is an operation that is prevalent in the (vast?) majority of works on Quaternion Neural Networks (let me reference here the excellent survey of Parcollet, Morchid and Linar\u00e8s, \u201cA survey of quaternion neural networks\u201d, AI Review 2020). The close relation between Fourier and Convolution has long been known for the real domain, and has been *extensively* used. In a nutshell, this paper shows how this relation generalizes to the quaternionic domain, and presents a proof-of-concept application of our theoretical results.\n\nWe have taken a conscious decision to include this specific application \u2013 it combines Quaternions in NNs, bounding the Lipschitz constant, and analyzing singular values of convolutional layers. The latter two have been the subject of interest for the ICLR / NIPS / ICML etc. community. For example:\n\n* Sedghi, Gupta, Long, \u201cThe singular values of convolutional layers\u201d, ICLR 2019\n* Singla, Feizi, \u201cFantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers\u201d, ICLR 2021\n\nSo, to recap about impact: Any model and method that includes convolution or spectral analysis (in H) will be relevant to the current paper. Convolutions and Fourier are part of learning systems and representations that range from probabilistic, hierarchical models to deep neural nets. Hence, we believe that the \u201crange of impact\u201d of our work is extensive.\n\nOn another note, the proof-of-concept application itself is useful in its own right \u2013 it can be seen as a plug-and-play improvement over practically *any* quaternionic CNN ."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067509108,
                "cdate": 1700067509108,
                "tmdate": 1700071407091,
                "mdate": 1700071407091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PB9sBTPNNa",
                "forum": "2Ed7b52z53",
                "replyto": "4RkDzGOZ6G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cFirst, the paper outlines the major difficulties of quaternions. However, I don't see why non-commutativity, two-sided convolution, versions of a convolution theorem, definition of determinant and the fact that $\\mu = -1$ has infinite solutions are problems.\u201d*\n\nThank you for the question. These are problems because they \u201cstand in the way\u201d of generalizing results that are well-known in the real or complex domain to the domain of quaternions.\nLet me take eigenvalues and eigenvectors as an example. In C (set of complex numbers) we use the formula $Ax = \u03bbx$\nto define eigenvalues and eigenvectors. However in H (set of quaternions), we must distinguish between $Ax = \u03bbx$ and $Ax = x\u03bb$ ; as multiplication is non-commutative, \u03bbx is not the same as x\u03bb. It turns out that the two problems have completely different solutions in general (see for example Zhang 1997). Solutions for the one equation are termed left eigenvalues, and solutions for the other one are termed right eigenvalues. Furthermore, we do not have any a priori guarantee about the *number* of eigenvalues of each. (This is related to $\\mu^2 = -1$ having infinite solutions).\n\nIn the paper, we write that the convolution theorem can be written (and proved) easily in matrix form. However, this is not possible in H (Corollary 3.6.2). In particular,  \u201cWriting proposition 3.5 in a matrix diagonalization form ($A = S\u039bS^{-1}$), where the columns of S are eigenvectors and \u039b is a diagonal matrix of left eigenvalues, is not possible. This would require us to be able to write $CQ = Q \u039b$. however, the right side of this equation computes right eigenvalues, while proposition 3.5 concerns left eigenvalues.\u201d So, this is an example that looks \u201ceasy\u201d in R or C, but becomes non-trivial and more nuanced / complicated in H.\n\nConcerning determinants, these can be a problem e.g. whenever we need to compute a normalizing constant for a probability distribution. In Normalizing flows (not studied in the paper), we need to define flows that let us compute a related determinant very fast. If in quaternions we are not even sure *how to define the determinant* (that is not to say that there is no related work on this, for example see [H. Aslaksen, \"Quaternionic Determinants\", 1999] or [Freeman J. Dyson, \"Quaternion Determinants\", 1972], then any possible generalization of NFs into H requires more work, definitely more than what would be perceived as \u201cincremental\u201d."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068194365,
                "cdate": 1700068194365,
                "tmdate": 1700139928538,
                "mdate": 1700139928538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dSjtVr1dKi",
                "forum": "2Ed7b52z53",
                "replyto": "4RkDzGOZ6G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cIf these issues are to be avoided, then why not go back to the real domain? Isn't the desired structure of quaternions what precisely results in these difficulties?\u201d*\n\n\nThis is correct. The structure of the quaternion algebra is the source of these difficulties.\n\nHowever, we cannot just go back to the real domain without abandoning the advantages of quaternions. In the context of NNs for example, probably the mainly cited advantage is the resulting compact, light-weight networks. (See for example Parcollet et al., \u201cA survey of quaternion neural networks\u201d, AI Review 2020).\n\nIf what you mean by \u201cgoing back\u201d is assuming an isomorphism between H and R^4, and recasting everything as 4-dimensional vectors, again this won\u2019t work without a cost. This is related to your other question:\n\n*\u201cLastly, what do the authors mean by [treating] signals in a holistic manner?\u201d*\n\nBy \u201cholistic manner\u201d, we mean that a multi-dimensional input may be treated as a single entity. For example, assuming that we have an image containing multiple channels, we need to write it as a structure of the form M x N x C . Strictly speaking, this is a tensor . Quaternions (and hypercomplex algebra in general) allow us to write this as a matrix.\nThis may sound like a superficial difference, but it does bear advantages: The field of matrix methods is much more mature than tensor methods. Convolutions and Fourier in the form of matrices (Circulant, Fourier matrices) is one such example, and what this paper does is showing how a range of well-known related results generalize for quaternions. If we chose to stick with the tensor representation, we\u2019d certainly be much more limited in scope. In Section 4, we discuss singular values of quaternion convolutional layers \u2013 this would be (currently?) impossible to do the exact same thing with tensors (there is no SVD for 3-way tensors). Cf. the excellent references on this topic:\n* G. Strang, \u201cLinear Algebra and Learning from data\u201d, 2019 (esp. chapter I.12)\n* C.Hillar and LH. Lim, \u201cMost tensor problems are NP-hard\u201d, J.ACM, 2013"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068318053,
                "cdate": 1700068318053,
                "tmdate": 1700068318053,
                "mdate": 1700068318053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YK9WWh49gN",
                "forum": "2Ed7b52z53",
                "replyto": "4RkDzGOZ6G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cI acknowledge the representation power of quaternions, however, this paper does not exploit this representation in the context of learning. I feel that this work lacks a clearer connection between the stated contributions and the learning task [..]\u201d*\n\nAllow us to respectfully disagree. In Section 4, we use additional, novel results (proofs in Appendix F), to improve learning of a quaternion-valued neural network. Our results show that there is significant improvement against not using our method. Specifically, we ran tests on CIFAR, where we show that our network, regardless of the hyperparameters, always obtains a boost in test performance. *Note that bounding the Lipschitz constant of convolutional QNNs is practically impossible without our theoretical results*. We discuss this further in the same section.\n\n*[..] possibly by showing examples where this representation makes a difference wrt standard (non-quaternion) methods.\u201c*\n\nWe agree that comparisons between quaternion and non-quaternion methods are in general interesting and useful. In this paper however, we do not try to show that quaternionic methods have this or that advantage; perhaps the discussion in Section 2, where we do touch upon pros and cons of quaternion algebra and its relation to learning systems, has been misleading in this respect. We could move it for example to a \u201cless prominent\u201d position in the paper, or make it an appendix.\n\nConcerning the debate between quaternions vs non-quaternions, this is the subject of numerous papers (see previous related answer on this thread, where we cite a number of works that do that). For example, we could have shown that a QNN can achieve similar results to a real-valued NN with only a fraction of its size. This is however already established in the literature. \n\nWhat we aim and *do* achieve in this paper, is proving a series of powerful theorems connecting convolution and Fourier analysis in H. This is done in Section 3. In Section 4, our results are \u201cshowcased\u201d using an application in learning. In this context, we show that a QNN (known to have specific advantages against real-valued NNs) benefits from Lipschitz constant bounding, which is enabled by our theoretical results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068487998,
                "cdate": 1700068487998,
                "tmdate": 1700068487998,
                "mdate": 1700068487998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vahi8SIky2",
                "forum": "2Ed7b52z53",
                "replyto": "4RkDzGOZ6G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3601/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201cI also acknowledge the application in Sec 4, however, the body of results presented in Sec 3 is far more general than that particular application.\u201d*\n\nWe do agree that the body for results in Section 3 is far more general in scope than the application of Section 4. Section 4 serves a \u201cproof-of-concept\u201d application, even though we do believe that it is important in itself. Let us note that this proof-of-concept however covers already 3 pages in the main paper + 4 pages in the appendix, in a paper of totally 27 pages \u2013 it only serves as one good example of how our results are useful and relevant to the learning representations community."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3601/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068525989,
                "cdate": 1700068525989,
                "tmdate": 1700068525989,
                "mdate": 1700068525989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]