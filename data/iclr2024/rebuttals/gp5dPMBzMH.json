[
    {
        "title": "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding"
    },
    {
        "review": {
            "id": "gc2J5Wo73S",
            "forum": "gp5dPMBzMH",
            "replyto": "gp5dPMBzMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_6hGH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_6hGH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a model named BELT-2 for multi-task EEG-to-Language decoding.  In particular, a discrete conformed is used to convert EEG  into EEG embeddings. Combined with a soft query prompt, the Querying Discrete Conformer (Q-Conformer) enables the multi-task mechanism. The EEG embedding is aligned with BPE tokens and fed to LLM such as T5. The experiment is conducted on an open-source ZuCo dataset to show the effectiveness of BELT-2."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of utilizing the powerful representation ability of LLM for EEG decoding is interesting."
                },
                "weaknesses": {
                    "value": "* a) Many of the training details are unclear, which makes it very hard to understand the working mechanism of the proposed BELT-2. I will elaborate in the question section.\n\n* b) The training of the Q-Conformer is based on seven different loss terms \u03bb1(L_cm + L_cb) + \u03bb2 * L_recon + \u03bb3 * L_div + \u03bb4 * L_bpe + \u03bb5 L_neg + \u03bb6 * L_tr with a list of balancing coefficients of [1, 1, 0.001, 10, 0.001, 10]. Training a discrete encoder (Conformer in this case) like VQ-VAE along with the downstream (translation in this case) is not common. From my experience, balancing the reconstruction quality and the code quality itself while training the VQVAE is already a hard task. Thus, I would very much like to know how this model would perform without the quantization and reconstruction. That is to remove L_cm, L_cb, L_recon, and L_div and only use the Conformer as an EEG encoder. Ideally, an ablation study on these loss terms and corresponding curves during training would help the readers understand the different components of BELT-2.\n\n* c) The experimental setting of word-level modeling is questionable. For a regular sentiment classification task, a continuous EEG signal is used as input, yet this work only extracts EEG segments based on eye tracking. Naively speaking, sentiment-related information does not necessarily exist only when the subject is looking at words. Or else, there might be a delay of the reaction in the brain upon reading. Is this considered while aligning the word and EEG?\n\n* d) The word-level modeling seems too artificial to fit the word embedding format to LLM. Sequence-to-sequence modeling would be more interesting and practical. \n\n* e) Speculative Augmentation takes k=15 other copies of the Conformer, which is both computationally and memory expensive.  An ablation on K should also be provided."
                },
                "questions": {
                    "value": "* a) The training details of the Q-Conformer is only provided in the appendix but nowhere to be mentioned in the main text. The authors should not use the appendix as additional pages to the paper.\n\n* b) What is the vocabulary used during training the EEG-language alignment? Is it the vocabulary of the ZuCo dataset or the vocabulary of the LLM (T5) ?\n\n* c) It is also not clear what loss function is used to train the multi-task query and which part of the network is updated. Is it the context transformer or the query prompts?\n\n* d) It is not clear why BPE is adopted. What would happen if the contrastive loss is used without BPE, meaning use whole word tokens?\n\n* e) When combining Q-Conformer with  LLM using continuous virtual prompts, how is it trained exactly?  These are not even mentioned in the appendix.\n\nI would like to raise my rating with my concerns and questions addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698239098391,
            "cdate": 1698239098391,
            "tmdate": 1699636165597,
            "mdate": 1699636165597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7nqOr5Hyhi",
                "forum": "gp5dPMBzMH",
                "replyto": "gc2J5Wo73S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q 1: \nThe training details of the Q-Conformer is only provided in the appendix.\n### R 1: \nWe put training details in the appendix mainly due to the page limitation. We will reorganize and put the details into the main paper in the final version. \n\n### Q 2: \nQuestion about vocabulary size.\n### R 2: \nThe vocabulary in the training split of the ZuCo dataset has 2603 unique words. However, in the final translation result in the  the decoding phase, we use the vocabulary of the LLM (T5) which is 32128. \n\n### Q 3: \nNot clear what loss function is used to train the multi-task query and which part of the network is updated. \n### R 3: \nIn the multi-task training experiment, we train 3 tasks simultaneously by randomly sampling tasks in each training step. We also found the best performance came from updating the whole Q-Conformer encoder, including the discrete conformer, the query prompt, and the context transformer. \n\n### Q 4: \nQuestion about the BPE method.\n### R 4: \nWe provide an additional ablation experiment involving using word-level contrastive learning with a discrete Conformer model involving three different settings. \n\nResults are detailed in the table. The use of word-level improves the BLEU scores to an extent while the BPE-level contrastive learning leads to further improvements for all BLEU scores. This rationalizes the use of BPE-level contrastive learning over word-level contrastive learning.\n| Contrastive Learning | Bleu-1 | Bleu-2 | Bleu-3 | Bleu-4 |\n| --------------------- | ------ | ------ | ------ | ------ |\n| None                  |41.57  | 24.03  | 13.8  | 8.06    |\n| Word-level            | 42.31  | 25.26  | 14.81  | 8.73   |\n| BPE-level             | 43.06  | 25.57  | 15.16  | 9.17   |\n\n### Q 5: \nWhen combining Q-Conformer with LLM using continuous virtual prompts, how is it trained exactly? \n### R 5: \nIn Section 2.4 and the introduction, we briefly introduced the training approach involving the frozen Q-Conformer and the pre-trained Language Model (LLM) decoder using the **prefix-tuning** method. While this method is commonly employed in fine-tuning LLMs. \n\nThe essence of the prefix-tuning is to only tune the prefix tokens to the LLM so that the trained prefix contains enough instruction for the LLM to understand the task [1].\n\nSince the prefix-tuning method is not a primary contribution introduced by our work, we did not delve into extensive details in the main paper.\n\n[1] Prefix-tuning: Optimizing continuous prompts for generation.\n\n### Q 6: \nQuestion about the training of the Q-Conformer.\n### R 6: \nWe conducted additional ablation experiments were conducted on the Q-Conformer encoder to show performances when trained with or without the VQ and contrastive components. The results indicate that VQ and BPE-Contrastive could improve the model's performance together. We will add the ablation in the final version of the paper.\n\n| VQ  | BPE-Contrastive | Bleu-1 | Bleu-2 | Bleu-3 | Bleu-4 |\n| --- | ---------- | ------ | ------ | ------ | ------ |\n| No  | No         | 40.12  | 23.18  | 12.61  | 6.8    |\n| No  | Yes        | 41.9   | 24.57  | 14.2   | 8.278  |\n| Yes | No         | 41.57  | 24.02  | 13.80  | 8.06   |\n| Yes | Yes        | 43.06  | 25.57  | 15.16  | 9.17   |\n\n\n### Q 7: \nQuestion about the experimental setting of word-level modeling.\n\n### R 7: \nThere are two major reasons for using word-level tokens as input in conducting the sentiment classification task. \n\nThe first reason is that we strictly follow the experimental setup in the baseline method for a fair comparison. \n\nAnother reason is that we want to demonstrate the multi-task capacity of our model. This means that given the same input, our model is able to decode different information from the input EEG tokens (i.e., the translation, summary, and sentiment).\n\n### Q 8: \nQuestion about word-level modeling.\n\n\n### R 8: \nWe add ablation experiments on different contrastive learning levels to answer this question as shown below.\n\n[Translation Task]\n| Contrastive learning | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 |\n| -------------------- | ------ | ------ | ------ | ------ |\n| word-level           | 42.31  | 25.26  | 14.81  | 8.73   |\n| sequence-level       | 42.23  | 24.95  | 14.29  | 8.14   |\n| BPE-level            | 43.06  | 25.57  | 15.16  | 9.17   |\n\nWe observe that the use of sequence-to-sequence modeling did not yield a significant improvement when compared to word-level modeling. \n\n### Q 9: \nQuestion about Speculative Augmentation.\n### R 9: \nThe Speculative Augmentation does not incur extra memory requirements. we employ a cache system by saving copies of the intermediate layer output (last output tokens) from the training set at each epoch. The cached outputs from a total of 15 training epochs, are utilized to diversify the training data spectrum during the prefix-tuning process. Thus, we use these cached outputs instead of loading checkpoints."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933300172,
                "cdate": 1699933300172,
                "tmdate": 1699936415729,
                "mdate": 1699936415729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8GR1oXQ3kC",
                "forum": "gp5dPMBzMH",
                "replyto": "gc2J5Wo73S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Extended response to Weakness a:\nTo further address the problem of the clarity of the training process, we have improved the readability and organized the illustration of the learning process. The training and loss function of Q-Conformer is clarified in Section 2.2 where we summarize the EEG-to-langauge alignment learning. In this stage, we jointly optimize 3 objectives, BPE-level contrastive learning (BPE-CL), Nagative contrastive learning (NCL), and the EEG-to-language matching (ELM) with the loss function clearly presented in the section. The first two objectives are applied to the discrete EEG tokens to improve the semantic (EEG-language alignment) while keeping the distinction among different tokens. The third objective is for training a task-specific query prompt as well as training the context-transformer. For multi-tasking, we provided a clearer description in Sectopm 2.4 and Figure 5. Please refer to the current paper for more details."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601959751,
                "cdate": 1700601959751,
                "tmdate": 1700601959751,
                "mdate": 1700601959751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kZeT77FyqG",
                "forum": "gp5dPMBzMH",
                "replyto": "gc2J5Wo73S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Esteemed Reviewer,\n\nThank you immensely for your invaluable insights and thoughtful queries that have significantly contributed to enhancing our work. Your feedback has been meticulously incorporated into our paper, elevating its quality and depth.\n\nWe earnestly appeal to your esteemed judgment once again, considering the substantial refinements made, to reconsider the rating for our paper. Our relentless dedication has been aimed at pushing the boundaries within the realm of EEG research, aspiring to pave the way for broader applications and advancements.\n\n\nWe hope our work can help improve brain research and make a positive difference in the wider research community. Your support and revised evaluation would not only recognize our efforts but also amplify the potential impact of this research, steering us collectively toward greater progress.\n\nWith sincere regards,\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706508274,
                "cdate": 1700706508274,
                "tmdate": 1700706508274,
                "mdate": 1700706508274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XcYhC4i3JS",
            "forum": "gp5dPMBzMH",
            "replyto": "gp5dPMBzMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_CXNw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_CXNw"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents BELT-2, a novel multi-task model that bridges the capabilities of large language models with human brain dynamics, focusing on enhancing the encoding and decoding of EEG signals. With its BPE-level EEG-language alignment and multi-task training, BELT-2 marks a significant breakthrough in EEG decoding. The paper boasts impressive results in multiple tasks, such as EEG-to-Text Translation, EEG Sentiment Classification, and EEG-to-Text Summarization. Notably, the BELT-2 model outperforms the state-of-the-art in several benchmarks, demonstrating its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The manuscript is well composed, with a clear structure and logical flow, enhancing the reader's understanding and engagement.\n2. The experiments are comprehensive. The detailed comparisons to state-of-the-art methods across various tasks, complemented by thorough ablation studies, add significant depth and robustness to the paper's findings."
                },
                "weaknesses": {
                    "value": "1. Figure 3 presents EEG topography plots for both the input and output during the EEG token quantization process, leading to some ambiguity in interpretation. I would recommend the authors to elucidate this procedure in greater detail. Specifically, it would be insightful to understand whether the spatial arrangement of the EEG sensors played any role in this process.\n\n2. The manuscript introduces BELT-2 as a progression from the prior BELT-1 model. However, the discussion and distinction between the two models are somewhat scanty, especially given their apparent similarities. It would be of immense value if the authors could elaborate on the design improvements made in BELT-2 over BELT-1. A focused discussion highlighting the specific enhancements and their contribution to the performance improvements, as showcased in Table 1 and Table 4, would add depth to the paper.\n\n3. A few inconsistencies are observed in the formatting of the tables, which might be distracting for readers. I'd kindly suggest revisiting and refining the table presentation to ensure a consistent and polished format.\n\n4. In Figure 4 and Section 2.4, there is a mention of utilizing the mediate layer coding as 'EEG prompts'. The concept, as presented, leaves some gaps in understanding, primarily because its introduction and visualization seem absent or not explicitly labeled in the preceding figures and method sections. It would enhance coherence and clarity if the authors could revisit Figures 2 and/or 3 and annotate the specific parts illustrating this mediate layer coding."
                },
                "questions": {
                    "value": "In the section detailing the experimental setup, the authors introduced the dataset split. Was this split done on a cross-subject basis or just cross-training samples? Given the well-documented variations in EEG signals across different individuals, understanding this aspect is crucial. Will inter-individual variations impact the EEG-to-Language translation performance of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Reviewer_CXNw",
                        "ICLR.cc/2024/Conference/Submission2330/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747479296,
            "cdate": 1698747479296,
            "tmdate": 1700727868065,
            "mdate": 1700727868065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DJRaMLA3kK",
                "forum": "gp5dPMBzMH",
                "replyto": "XcYhC4i3JS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1: \nQuantization process of the EEG token is unclear. Whether the spatial arrangement of the EEG sensors played any roles in the process?\n### Response: \nWe use the public ZuCo dataset in our experiment. The ZuCo dataset segments the raw EEG wave using the eye fixation for each word and converts each segmented EEG wave into a total of 8 frequency bands. \n\nOriginally, they used a 128-channel EEG cap to collect the EEG signal, but they removed 23 channels which resulted in a total of 105 EEG channels. As a result, each word-level EEG segment is processed into a 105*8=840 size frequency embedding. The maximum number of words in a sentence is set to 56. We therefore use the 56-word-level EEG embeddings as input tokens for the Conformer network. \n\nThe Conformer network is a transformer-like architecture that outputs the same number of tokens as the input. Afterward, the output tokens from the Conformer, denoted by $h$ in our paper, will be quantized by a vector quantizer, and each token from the Conformer will be converted to a discrete token $b$. These discrete tokens will be used as input to the context transformer where the query prompt will interact with them via a cross-attention layer. \n\nFinally, the output of the context transformer (i.e., the mediate layer coding) can be sent to the LLM to get the final translation outputs. \n\nFor the spatial arrangement of the EEG sensors. Since we are using a public dataset, the arrangement of the 105 EEG channel is fixed. Therefore, the effect of different sensor arrangements is not the research focus of this paper. However, we think it would be an interesting topic in the future works.\n\n### Question 2: \nQuestion about the difference between BELT-1 and BELT-2.\n### Response: \nThank you for your feedback. We have briefly mentioned the different between BELT-1 and our BELT-2 model. We will provide more details in the final version. The limited discussion and comparison between the BELT-2 and BELT-1 models in our paper were primarily constrained by page limitations. The key design improvements in BELT-2 over BELT-1 can be summarized as : 1) **Enhanced Alignment**, 2)  **Muti-Task Flexibility**, and 3) **Integration with Pretrained LLMs**.\n\n### Question 3: \nQuestions about the 'EEG prompots' and the Mediate layer coding. \n### Response: \nWe used a BART model to initialize the context transformer. This makes the context transformer a bidirectional model that captures global information from the EEG tokens. However, participants read through a sentence according to the order they prefer and will also skip some words during the reading. As a result, we utilize the mediate layer coding as a 'prompt' to LLMs and use the finetuned prefix to instruct the LLMs to rearrange the disorder language information and generate more natural translation sentences. \n\nLast but not least, we will add illustrative information about the mediate layer coding and the EEG prompts to the figures to provide a clearer idea of the mediate layer coding. \n\n### Question 4: \nQuestion about the dataset split and subject variations.\n### Response: \nThanks for the question. Currently, the experimental setup splits the dataset on a cross-training sample basis for a fair comparison with other methods. In response to the cross-subject splitting in the question and to explore whether inter-individual variations will impact the EEG-to-Language translation performance of our model, we additionally conducted the experiment in a cross-subject setting (shown in the table below). In this experiment, we leave one subject out as a test set and train the model on all other subjects. The performance is illustrated in the following table:\n\n| test subject | bleu-1 | bleu-2 | bleu-3 | bleu-4 | r    | p    | f1   |\n| ------------ | ------ | ------ | ------ | ------ | ---- | ---- | ---- |\n| ALL          | 43.1   | 25.6   | 15.2   | 9.2    | 30.6 | 34.3 | 32.2 |\n| ZPH          | 50.4   | 33.3   | 22.6   | 15.6   | 36.2 | 40.8 | 38.2 |\n| ZMG          | 50.5   | 33.2   | 22.3   | 15.4   | 35.1 | 39.2 | 36.9 |\n| ZKW          | 51.2   | 33.9   | 22.7   | 15.6   | 35.9 | 39.8 | 37.7 |\n| ZKH          | 51.4   | 34.5   | 23.2   | 16.0   | 36.8 | 40.8 | 38.6 |\n| ZKB          | 50.1   | 32.8   | 21.9   | 15.0   | 35.3 | 39.3 | 37.1 |\n| ZJS          | 48.4   | 30.8   | 19.7   | 12.9   | 33.6 | 37.7 | 35.4 |\n| ZGW          | 51.2   | 34.8   | 24.0   | 16.7   | 36.3 | 40.3 | 38.1 |\n| ZJN          | 48.0   | 30.5   | 19.7   | 13.0   | 33.6 | 37.8 | 35.4 |\n| ZDM          | 50.0   | 33.0   | 22.2   | 15.2   | 35.5 | 39.7 | 37.4 |\n| ZAB          | 50.1   | 33.0   | 22.3   | 15.3   | 34.6 | 38.6 | 36.4 |\n\nThe table shows that the subject variant on the ZuCo dataset doesn\u2019t have a significant impact on the EEG-to-Language translation performance of our model. We will include this ablation experiment in the supplementary in the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933305582,
                "cdate": 1699933305582,
                "tmdate": 1699933305582,
                "mdate": 1699933305582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xg3kGQmoGx",
                "forum": "gp5dPMBzMH",
                "replyto": "XcYhC4i3JS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Extended response to Question 3:\nIn the latest version of the paper, we have improved the readability and provided a clearer explanation of the mediate layer coding. For clarity, we rename the output of the Q-Conformer as the Mid-Layer Coding as these output embeddings represent a crucial **midpoint** between the EEG Encoder and LLM. Please refer to Section 2.3 and Figure in the current version of the paper for more details. \n### Extended response to Question 4:\nIn the latest version of the paper, we have included the cross-subject experiment in the main paper (Page 7 last paragraph, and Figure 6). Please refer to the current paper for more details."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602672984,
                "cdate": 1700602672984,
                "tmdate": 1700602672984,
                "mdate": 1700602672984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WQe6VzyLdy",
                "forum": "gp5dPMBzMH",
                "replyto": "Xg3kGQmoGx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_CXNw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_CXNw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their comprehensive responses to my concerns and for conducting additional experiments. As a result, I have increased my rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727649652,
                "cdate": 1700727649652,
                "tmdate": 1700727649652,
                "mdate": 1700727649652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QAFeUaxVk7",
            "forum": "gp5dPMBzMH",
            "replyto": "gp5dPMBzMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_KLkP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_KLkP"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents BELT-2 which learns to perform many EEG-to-language tasks in a multitask setting. Specifically, EEG-to-text, summarization, and classification are learned simultaneously. The architecture consists of an encoder, which is pre-trained with a reconstruction loss. Then, during training time, all objectives for all tasks are optimized for simultaneously. The model can choose to tailor its representations per task, conditioned on a query vector that is also passed as input per task. The authors find that both pre-training and multi-task learning improve performance over the baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Significance: for the tasks considered, the results represent a substantial improvement over existing work\n- Novelty: There is novelty in the application to the EEG domain\n- It's unclear whether BPE-level contrastive learning is a novel method, or novel in the sense that this is the first time that it has been applied to the EEG domain. Could the authors clarify? If it's novel to the field, then this would be a plus for the paper"
                },
                "weaknesses": {
                    "value": "- The broader application to the field of machine learning is limited. The key takeaway seems to be the effectiveness of multi-task learning.\n- It seems the main difference between BELT and BELT-2 is the addition of multi-task learning. If this is the case, then the technical advancement may be on the modest side. Although, I would not say this is a large weakness."
                },
                "questions": {
                    "value": "- The last sentence on the first paragraph of page 1 says that previous methods have not achieved a \"general bridging between brain signals and languages.\" Can you say more precisely what this means? Does \"general bridging\" refer to the multi-task setting? Is that the main difference between BELT and BELT-2? \n- I have a question about the input to the conformer. Page 3 says that the input is created by \"segmenting the raw EEG waveform signal into windows using eye-tracking information.\" Are the segments of uniform length? If so, then why is the eye-tracking information necessary? If not, then does some sort of truncation or down-sampling occur?\n- I have a question about the multi-task query described on page 4. It says that \"we could easily extend the Q-conformer to a new downstream task by initializing a new set of query tokens to extract information related to the new task, obviating the need for training an entirely new model.\" But is this really a saving in time? Wouldn't the same amount of time be needed to train the new set of query tokens, even if you start with the existing model weights?\n- In section 2.4, it says that \"During the EEG representation learning stage, the Q-Conformer extracts, task-specific information from the EEG input signals.\" But this doesn't seem to be the case? Is Figure 2 left meant to depict the representation learning stage? If so, why aren't there any task-specific terms in the objective function?\n\n## small things\n- Figure 2 -- the captions for (upper right) and (bottom right) are switched\n- To get forward quotes, use `` in latex\n- page 8 typo: \"briding\" --> \"bridging\"\n- typo: the speculative augmentation ablation is Figure 5, not Table 5"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699424756503,
            "cdate": 1699424756503,
            "tmdate": 1699636165442,
            "mdate": 1699636165442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7IiUdiny5c",
                "forum": "gp5dPMBzMH",
                "replyto": "QAFeUaxVk7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1: \nUnclear whether BPE-level contrastive learning is a novel method, or novel in the EEG domain.\n### Response: \nTo the best of our knowledge, there is no prior work employing BPE-level contrastive learning in a manner similar to our approach. \n\nWe conducted ablation experiments on the BPE in Table 1 in the main paper. We move the comparison for the BPE here for your convenience. Results in the table below show that the introduction of BPE-level contrastive learning brings significant improvement to the BLEU scores, reaching 43.06(+1.49), 25.57(+1.55), 15.16(+1.36), 9.17(+0.57). We will improve readability and refine the paper in the final version.\n\n| BPE Contrastive | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 |\n| --------------- | ------ | ------ | ------ | ------ |\n| No              | 41.57  | 24.02  | 13.8   | 8.06   |\n| Yes             | 43.06  | 25.57  | 15.16  | 9.17   |\n\nPrevious works on visual-language alignment are employing sentence-level contrastive learning [1,2]. However, due to the scarcity of EEG-language pairs, we need to incorporate stronger guidance from the language modality during training. Consequently, we consider that the use of BPE-level contrastive learning constitutes a novel method for aligning a modality with language when faced with a limited number of training sample pairs. \n\n[1] Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks.\n\n[2] Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\n\n\n### Question 2: \nQuestion about the \"general bridging between brain signals and languages.\". The main difference between BELT and BELT-2?\n### Response: \nHere, \"general bridging between brain signals and language\" means this work is the first to achieve multi-task EEG decoding and is able to leverage the generative capacity of the existing LLMs. \n\nThis includes being able to enhance the general understanding of brain signals by learning from different tasks while being able to decode multiple information from EEG signals (e.g., sentiment or summary). Our work signatures an important step to link the brain signal into existing multimodal understanding models which has the potential for boarder applications.  \n\nThe main difference between BELT-1 and BELT-2 is that BELT-1 is a single-task architecture while BELT-2 is a multi-task architecture. Our model accomplishes multi-task decoding using query prompts, a strategy aligned with current Language Models (LLMs). Also, BELT-2 allows us to directly combine various LLM (e.g., T5 and LLAMA2, PEGASUS) with our EEG Encoder to enhance the translation quality by merely tuning the prefix tokens. We are also the first work to provide a study (Table 5) of combining EEG encoders with different state-of-the-art LLMs. \n\n### Question 3: \nAre raw EEG wave segments of uniform length? What is the use of eye-tracking information?\n\n### Response: \nIn our experimental setup, we leverage the ZuCo dataset [1]. Notably, this dataset incorporates eye-tracking data, recording eye fixations on individual words. The fixation duration for each word varies significantly, and some words may be omitted during reading, leading to EEG segments of different lengths. \n\nAlthough the length for each word-corresponding EEG signal is different, the ZuCo dataset performs a frequency transformation operation to extract a fixed number of frequency values from each EEG segment for each channel. \n\n[1] ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading.\n\n### Question 4: \nQuestion about \"we could easily extend the Q-conformer to a new downstream task ... \" \n### Response: \nHere, we don\u2019t need to retrain a new model or the query tokens. For the multi-task setting of our Q-conformer model, we trained 3 tasks together and implemented a sampling strategy to select a task for each training batch. \n\n### Question 5: \nIn section 2.4, it says that \"During the EEG representation learning stage, the Q-Conformer extracts, task-specific information from the EEG input signals.\" But this doesn't seem to be the case? Is Figure 2 left meant to depict the representation learning stage? If so, why aren't there any task-specific terms in the objective function? \n\n### Response: \nFigure 2 is meant to depict the feature extraction process of the Q-Conformer. In Figure 2 (left), our primary intention was to illustrate that the query prompts will query task-specific information from the EEG tokens via a cross-attention layer in the Context Transformer. \n\nAs mentioned in the supplementary, the query prompt and the Q-Conformer are trained with the objective function mentioned in the appendix. We will make refinement to the figure in the final version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933307860,
                "cdate": 1699933307860,
                "tmdate": 1699933307860,
                "mdate": 1699933307860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MmWJUAJjtI",
                "forum": "gp5dPMBzMH",
                "replyto": "QAFeUaxVk7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Extened Response to Question 5:\nIn the latest revision, we have improved the readability of the paper. In particular, we revised the original Figure to give a more precise and clearer illustration of the Q-Conformer architecture. Please refer to the Figure 2 in the current paper. In Figure 2, we show the overall structure of the Q-Conformer. It consists of a discrete conformer, a context transformer (C-Former), and a query prompt. The input EEG embeddings (EEG embed) are first processed by the conformer into continuous EEG tokens. A vector quantizer is then used to discretize\nthe EEG tokens. Then, a query prompt interacts with the discrete EEG token via the cross-attention layer from in the C-Former to extract task-specific context information from the discrete EEG tokens. While Figure 2 mainly depicts the interaction of submodules in the Q-Conformer, Figure 3 and Section 2.2 gives a more specific illustration of how the query prompt is trained. We summarize the first training stage as the EEG-to-language alignment learning stage where we use the EEG-to-language matching (ELM) objective as the 'base' task in this alignment learning stage. When training on ELM, the query prompt is trained to become task-specific. For multi-task training, as depicted in Figure 5, we assign a query prompt to each decoding task, so that each is trained by the objective of a task (e.g., translation, summary, or sentiment classification). Please refer to the current paper for more details and consider raising the rating for our work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601093554,
                "cdate": 1700601093554,
                "tmdate": 1700601093554,
                "mdate": 1700601093554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VwDIIP5B2z",
            "forum": "gp5dPMBzMH",
            "replyto": "gp5dPMBzMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_8fpS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_8fpS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces BELT-2, a multi-task model specifically designed to enhance both EEG signal encoding and decoding performance. BELT-2 incorporates byte pair encoding (BPE)-level EEG-language alignment and seamlessly integrates multi-task training and decoding within the EEG domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "a)\tIt\u2019s the first work of multi-task brain decoding by bridging the Q-Conformer EEG encoder and LLMs. \nb)\tIt outperforms the baseline models, demonstrating superior performance."
                },
                "weaknesses": {
                    "value": "a)\tThe interaction of all the components in Equation 2 is unclear, and it remains uncertain whether the introduction of certain hyperparameters is necessary.\nb)\tThe organization of the logit structure in the paper appears somewhat disordered, exemplified by an error in the figure caption of Figure 2. I would recommend swapping the content in the upper right and bottom right sections of the figure. Furthermore, the explanation of loss functions on Page 3 does not align consistently with Equation 2.\nc)\tLack of Clarity: The description of \"Multi-task Query\" in the paper is unclear. It is not clear how the query prompt is trained and whether a new task requires complete retraining. Furthermore, there is a lack of clarity regarding how the Frequency domain EEG embedding e is transformed into continuous EEG tokens h, i.e., the learning process of the conformer model E(.), and how it is subsequently transformed into word-level EEG representations.\nd)\tLack of Specificity: If EEG representations are employed using a contrastive learning approach, the method proposed in Section 2.3 appears to be a conventional operation.\ne)\tUnclear Ablation Experiments: The paper does not provide a clear description of the ablation experiments, particularly the absence of ablation on BPE-level Contrastive learning and key components of Formula 2."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Reviewer_8fpS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699469390047,
            "cdate": 1699469390047,
            "tmdate": 1700705593544,
            "mdate": 1700705593544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uALjJsn3aA",
                "forum": "gp5dPMBzMH",
                "replyto": "VwDIIP5B2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1: \nThe interaction of all the components in Equation 2 and hpyerparameters is unclear.\n### Response\nWe combine the loss terms in Equation 2 using a weighted combination as mentioned in the supplementary (page 13 to 14). The combination of these loss terms is a common operation for training a Vector Quantised (VQ) encoder. Specifically, $L_{cm}$, and $L_{cb}$ are the default losses for training the discrete codebook as illustrated in VQVAE paper [1]. The $L_{div}$ is a term to encourage diversity in the usage of the discrete codebook entries which is also frequently used in training a VQ encoder, an example of using the diversity loss is in the wav2vec2.0 paper [2]. $L_{recon}$ is the necessary term used for training a reconstructive encoder as in the latent diffusion model [3]. We also provided ablation experiments on different hyper-parameter settings in the supplementary (Page 17).  We will organize these information from the supplementary to the main paper to improve the readability for boarder audiences. \n\n### Question 2: \nNot clear how the query prompt is trained and whether a new task requires complete retraining. \n### Response\nThe Q-Conformer model does not require a complete retraining. We borrow the idea of query prompts from the recent visual-language model BLIP-2. The query prompt is able to select and extract information from the EEG tokens through a cross-attention layer in the context transformer.  \n\nAs mentioned in Page 4, the Q-Conformer efficiently adapts to new downstream tasks by initializing a set of query tokens, enabling the extraction of task-specific information. This adaptation is achieved through training a query prompt using the objective function tailored to the specific task. Given that BELT-2 is a multi-task model, individual tasks are trained concurrently without the need for complete model retraining. \n\n### Question 3: \nHow the Frequency domain EEG embedding is transformed into continuous EEG tokens.\n### Response\nAs mentioned in the paper (Page 3), EEG data from the ZuCo dataset is first segmented according to the gaze fixation on each word using eye-tracker information. A frequency domain transformation is applied to each EEG segment to extract value from 8 frequency bands. The ZuCo dataset contains 105 EEG channels. These frequency bands from the 105 channels form an equal-length word-level EEG embedding (size=840). \n\nSimilar to previous works [4], the word-level EEG embedding is processed by a transformer-like EEG encoder that processes the word-level EEG embedding. We use the Conformer model to process the word-level EEG embeddings. After processing by the Conformer model, a vector quantizer to discretize the output EEG tokens into discrete tokens. \n\n### Question 4: \nLack of Specificity for Section 2.3.\n### Response\nThe reason why we adopt the negative contrastive loss is that the difference in human brain waves when looking at different words is very small, increasing the difficulties of EEG decoding. Therefore, we use the negative contrastive loss to enlarge the differences among EEG tokens and consequently enhance the final translation performance. The method of using negative contrastive loss among encoded tokens is also employed in the training of speech encoders [2]. \n\n### Question 5: \nUnclear Ablation Experiments: The paper does not provide a clear description of the ablation experiments, particularly the absence of ablation on BPE-level Contrastive learning and key components of Formula 2.\n### Response\nWe provided ablations about the BPE-level contrastive learning (Table 1), about using pretrained EEG encoder for downstream tasks (Table 3 and Table 4), about connecting the EEG encoder with different LLMs (Table 5), and about hyper parameters (Figure 9) in our main paper and in the supplementary. Due to the page limit, we moved the rest ablation experiments to the supplementary. We will reorganize more ablation information in the final version of the paper. \n\nRegarding to the effect of BPE-level contrastive learning, **we have already provided this ablation study in Table 1**.\n\n| BPE Contrastive | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 |\n| --------------- | ------ | ------ | ------ | ------ |\n| No              | 41.57  | 24.02  | 13.8   | 8.06   |\n| Yes             | 43.06  | 25.57  | 15.16  | 9.17   |\n\nThis ablation shows the performance of our model with or without the BPE-level Contrastive learning term. Results show that the introduction of BPE-level contrastive learning brings significant improvement to the BLEU scores, reaching 43.06(+1.49), 25.57(+1.55), 15.16(+1.36), 9.17(+0.57).\n\n[1] Neural discrete representation learning.\n\n[2] wav2vec 2.0: A framework for self-supervised learning of speech representations.\n\n[3] High-resolution image synthesis with latent diffusion models.\n\n[4] Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933310481,
                "cdate": 1699933310481,
                "tmdate": 1699935262531,
                "mdate": 1699935262531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "saa9sxgXc5",
                "forum": "gp5dPMBzMH",
                "replyto": "M0qTh89xuC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_8fpS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_8fpS"
                ],
                "content": {
                    "title": {
                        "value": "Increase rate to 5"
                    },
                    "comment": {
                        "value": "Dear authors, thanks for your time and efforts in addressing my comments. Although some concerns remain, lots of points are clarified. I'll increase my rating to 5."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705575896,
                "cdate": 1700705575896,
                "tmdate": 1700705575896,
                "mdate": 1700705575896,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rEqj5Vkgn5",
            "forum": "gp5dPMBzMH",
            "replyto": "gp5dPMBzMH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_psny"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2330/Reviewer_psny"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel multi-task model, called BELT-2, to enhance both encoding and decoding performance from EEG signals. The experimental results conducted have shown the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of the proposed method to bridge the Q-Conformer EEG encoder and LLMs seems interesting.  \n- The application to EEG data seems novel.\n- The proposed method outperforms some state-of-the-art methods for various tasks.\n- The paper is clear and well-structured."
                },
                "weaknesses": {
                    "value": "- The discussion on the difference between BELT and BELT-2 is not sufficient.\n- Some details related to the training and the loss function are not provided."
                },
                "questions": {
                    "value": "It is suggested to highlight the difference between BELT and BELT-2, and to provide more precision on the training details and the loss function."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2330/Reviewer_psny"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2330/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699945365981,
            "cdate": 1699945365981,
            "tmdate": 1699945365981,
            "mdate": 1699945365981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f53nCMNsVU",
                "forum": "gp5dPMBzMH",
                "replyto": "rEqj5Vkgn5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question 1:\nDifference between BELT and BELT-2.\n### Response 1:\nThanks for your question. The original paper mentioned the differences between these two models on page 2. Key design improvements in BELT-2 over BELT-1 can be listed as follows:\n\n1. **Enhanced Alignment**: BELT-1 applies contrastive learning on word and sequence levels to bootstrap EEG encoder training. It has significant shortcomings: 1. Sequence-level alignment is compared coarser and would increase the difficulties for learning the exact relationship between EEG tokens and the corresponding word. 2. Word-level alignment would lead to a decreased performance in the decoding of rare words or out-of-training-vocabulary words when training data size or training vocabulary is limited. \n\n    BELT-2 introduces a significant improvement through BPE-level contrastive learning. It operates at a more granular level than word-level alignment as it breaks down words into smaller size of root words.  It is especially useful for handling rare words or words out of the vocabulary of the training set because of the shared subword units. Experimental results in Table 2 improvement lead to a clear improvement in the open-vocabulary setting.\n\n    This discussion is compressed due to the page limit. We will improve the writing to emphasize this point in the final version. \n\n2. **Muti-Task Flexibility**: BELT-1 follows the same setting of all previous works that only explore brain-to-text translation, a single task. \nBELT-2 makes a significant improvement and extends the brain decoding to the multi-task setting for the first time. \nThe multi-task setting makes the decoding supervision denser and also enhances the learning process.\nAdditionally, it explores seamlessly switching between tasks by employing different query prompts, reducing computational resource requirements, and utilizing knowledge from diverse tasks.\n\n3. **Integration with Pretrained LLMs**: BELT-2 offers the ability to connect the EEG encoder to a pre-trained Language Model (LLM), such as T5 or LLAMA2. This work provides a detailed comparison of different LLMs. It also includes an intuitive comparison between single-direction and bi-direction LLMs for follow-up researchers. \nThis integration allows BELT-2 to leverage the strong generative capacity of pre-trained LLMs, providing enhanced performance.\n\n### Question 2:\nMore precision on the training details and the loss function.\n### Response 2:\nSorry for any inconvenience caused, due to the page limit, we moved some training details and the loss function to the appendix. \n\nFor the training of the EEG encoder as illustrated in the appendix, i.e., the Q-Conformer, we used a weighted combination of the VQ loss, BPE-level contrastive loss, and the negative contrastive loss. For multi-task training, we trained all tasks together using a random sampling method on tasks. To bridge the trained Q-Conformer with the LLM decoder, we froze both the Q-Conformer and the LLM decoder and used the prefix-tuning method to train a set of prefix prompts to 'instruct' the LLM to decode fluent sentences from the output tokens of the Q-Conformer model. For the loss function used to train the translation task and the summarization task, we used a common machine translation loss, as in the BART paper [1]. For the sentiment classification task, we only used cross-entropy loss to train the last output token of the Q-Conformer, similar to the method in the XLNet paper [2].\n\nWe will further concise the writing to include more information in the main paper. \n\n[1] Lewis, Mike, et al. \"Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\" arXiv preprint arXiv:1910.13461 (2019).\n\n[2] Yang, Zhilin, et al. \"Xlnet: Generalized autoregressive pretraining for language understanding.\" Advances in neural information processing systems 32 (2019)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005532875,
                "cdate": 1700005532875,
                "tmdate": 1700014562570,
                "mdate": 1700014562570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ylG0Ve5cP3",
                "forum": "gp5dPMBzMH",
                "replyto": "f53nCMNsVU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_psny"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2330/Reviewer_psny"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507864891,
                "cdate": 1700507864891,
                "tmdate": 1700507864891,
                "mdate": 1700507864891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]