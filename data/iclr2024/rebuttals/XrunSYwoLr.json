[
    {
        "title": "Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers"
    },
    {
        "review": {
            "id": "W9zD0vjbow",
            "forum": "XrunSYwoLr",
            "replyto": "XrunSYwoLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
            ],
            "content": {
                "summary": {
                    "value": "This research presents a novel method for spiking neural networks from pretrained Transformer models (or models with multi-head attentions). The suggested Universal Group Operators and Temporal-Corrective Self-Attention Layer allow a pretrained Transformer to be converted to a completely event-driven SNN without the need for training, which holds promise for neuromorphic computing. Effectively positive experimental outcomes were obtained."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The ability to convert pretrained Transformer models into spiking neural networks without the need for training is more appealing than training Spiking Transformers directly.\n\n\nFor neuromorphic computing and widespread deployment, the pure implementation on spiking neurons is promising.\n\n\n\nThe universal nature of the conversion method used here makes use of linear models' capacity for global approximation. The inverse function is converted correctly."
                },
                "weaknesses": {
                    "value": "In comparison to conventional artificial neural networks, there may be a slight accuracy gap due to the approximation error from Universal Group Operators.\n\nThe proposed method has only been tested on the ViT-B/32 model from CLIP; it is unknown if it can be applied to other models.\n\nThe converted models perform computations at a marginally higher rate than traditional spiking CNNs (which have more synapses and neurons)."
                },
                "questions": {
                    "value": "From Figure 3, I can observe severe, uneven quantization. Can you explain how this quantization affects the accuracy of the output?\n\nHow do you prepare the data for pretraining non-linear activations? For GeLU, do you record the actual responses of the ANN and train it on these activation values? For other nonlinearities (inverse, exp, layer norm), how do you pretrain?\n\nAlso, why do these nonlinearities need so many kinds of losses here (Table 3)? Can you explain why Huber loss fits exp, gelu, and inverse? And why does MSE fit the layer norm?\n\nBesides, do you evaluate the actual increment of neurons by using UGO? I want you to give me a table reporting the difference in neuron number and weight according to the models listed in Table 1.\n\nAre you going to release the weight of the pretrained nonlinearity? Do you test the sensitivity of changing $N$ and $T$?\n\nI think there is a typo in equation (22). $V_{th} \\Vert w_2\\Vert_1$ should be $(V_{th} \\Vert w_2\\Vert_1)/T$.\n\nWhy do you mention setting $V_{th}$ using the strategy proposed by Li et al., who proposed to use dynamic thresholds, and demonstrating the quantization gap using the maximum activation as a threshold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1774/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1774/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698233430713,
            "cdate": 1698233430713,
            "tmdate": 1700581725924,
            "mdate": 1700581725924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s7zYoLVtC9",
                "forum": "XrunSYwoLr",
                "replyto": "W9zD0vjbow",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and detailed feedback. We hope that our responses can address your concerns adequately.\n\n### Weakness 1\nWe acknowledge your observation. Considering the highly complex non-linear functions used in ViT, such errors may be inherently difficult to eliminate in a training-free setting.\n\n### Weakness 2\nOur method can be directly applied to various ViT models without modification. We have conducted some experiments on ViT-B/16 and provided supplementary results on the CIFAR-10 and CIFAR-100 datasets for Table 1 (zero-shot). \n\n| Dataset   | Model      | Method              | ANN Acc. | T=32  | T=64  | T=128 | T=256 |\n| --------- | ---------- | ------------------- | -------- | ----- | ----- | ----- | ----- |\n| CIFAR-10  | ResNet-50  | Calib. (Li et al.)  | 72.35    | 64.08 | 68.13 | 71.04 | 71.19 |\n| CIFAR-10  | ResNet-50  | SNM (Wang et al.)   | 72.35    | 58.69 | 61.22 | 70.68 | 70.88 |\n| CIFAR-10  | ResNet-50  | *Combined Conversion* | *72.35*    | *63.22* | *67.05* | *71.15* | *70.92* |\n| CIFAR-10  | ResNet-101 | Calib. (Li et al.)  | 79.64    | 38.21 | 55.37 | 67.44 | 71.21 |\n| CIFAR-10  | ResNet-101 | SNM (Wang et al.)   | 79.64    | 43.25 | 52.68 | 68.42 | 72.96 |\n| CIFAR-10  | ResNet-101  | *Combined Conversion* | *79.64*    | *55.03* | *64.62* | *68.37* | *73.64* |\n| CIFAR-10  | **ViT-B/32**   | **STA (Ours)**          | **89.74**    | **87.71** | **88.20** | **88.29** | **88.34** |\n| CIFAR-10  | **ViT-B/16**   | **STA (Ours)**          | **90.83**    | **86.79** | **87.33** | **87.44** | **87.47** |\n| CIFAR-100 | ResNet-50  | Calib. (Li et al.)  | 41.01    | 24.67 | 33.41 | 38.20 | 39.01 |\n| CIFAR-100 | ResNet-50  | SNM (Wang et al.)   | 41.01    | 35.64 | 34.71 | 39.95 | 41.13 |\n| CIFAR-100 | ResNet-50 | *Combined Conversion* | *41.01* | *37.41* | *39.26* | *40.31* | *40.78* |\n| CIFAR-100 | **ViT-B/32**   | **STA (Ours)** | **64.26**    | **62.55** | **62.74** | **62.98** | **63.01** |\n| CIFAR-100  | **ViT-B/16**   | **STA (Ours)**          | **67.20**    | **63.12** | **63.45** | **63.77** | **63.75** |\n\nOur algorithm can also be applied to other ViT models of different sizes, with more experiments ongoing.\n\n\n### Weakness 3\nDue to the incompatibility between the underlying computational logic of ANN Transformers and SNNs, the cost on spiking rate is indeed challenging to completely eliminate. We are actively working on mitigating this issue and exploring potential solutions.\n\n### Question 1\nWe appreciate your concern. In fact, our quantification is not conducted on the shown non-linear function, but the hidden layer neurons that fit the non-linear function. We will provide our answer from three perspectives:\n\n1.\tSevere Quantization: IF neurons exhibit a \"stepped\" input-output characteristic, where each neuron produces $T$ steps. In our fitting method, we employ a weighted sum of the outputs of $N$ neurons, resulting in a total of no more than $N*T$ quantization steps. However, due to sparsity, this value is typically much smaller, leading to severe quantization.\n\n2.\tUnevenness: We construct the training data by generating more samples within a smaller numerical range (e.g., [-1, 1]). As a result, the quantization is denser in that range compared to other ranges like [5, 10]. However, empirical observations show that this unevenness does not significantly impact the results.\n\n3.\tAccuracy: The combination weights of the Universal Group Operator are obtained through training and have a certain degree of randomness. The fitting errors observed during training do not directly reflect the practical application effects on ViT. In practice, we parallel train dozens of Universal Group Operators and evaluate them on an ultra-small test set consisting of 50 samples to select the model with the highest accuracy.\n\n### Question 2\nThe details of non-linear pretraining can be found in Appendix B. Before synthesizing the training data, we put forward 10 randomly sampled images from CIFAR-10 through the network and record the actual input and response of each module, as shown in Figure 8. Due to normalization, the activation values of almost any image passing through the network follow similar statistical patterns. However, due to the limited number of samples, training the Universal Group Operator (UGO) directly with these values would lead to overfitting. Therefore, we roughly determine the range of these activation values and manually construct a uniform distribution to cover the real distribution. This simple method has been found to meet the fitting accuracy requirements. For all non-linear operators, the training data synthesis and pretraining follow the same procedure."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880670378,
                "cdate": 1699880670378,
                "tmdate": 1699880670378,
                "mdate": 1699880670378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yL5YBDf9Ot",
                "forum": "XrunSYwoLr",
                "replyto": "5ZzwOhw99a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_gaB3"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I have read the author response and the comments of other reviews. It is claimed that this work provides a solid framework for converting Transformers into their spiking versions. The authors have clearly explained the details I am concerning. I suggest in-depth exploration for the loss used for fitting the nonlinear to further decrease the delay. Overall, this is a promising conversion work. I would increase my score as my final decision. Thank you."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581657919,
                "cdate": 1700581657919,
                "tmdate": 1700581657919,
                "mdate": 1700581657919,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hyb2Z7hAt1",
            "forum": "XrunSYwoLr",
            "replyto": "XrunSYwoLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_wK6t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_wK6t"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\nThe paper proposes a training-free method to convert transformer to SNN platforms. It proposes universal group operators to approximate nonlinear activations and temporal-corrective self-attention layer to approximate spike multiplications. Compared to prior work, it is the first to support pretrained transformers with SNNs without training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength:\n\n1.\tThe idea is novel. Use multiple spiking neurons to estimate the nonlinearity functions. The temporal-corrective self-attention can achieve unbiased multiplication between two variable matrices. \n\n2.\tThe convergence and error bound have solid theoretical guarantees and experimental validation.\n\n3.\tCompared to prior work with training/calibration, this work can quickly convert ViT to SNN hardware with high fidelity with a small timesteps T.\n\n4.\tIt also shows the efficiency benefits compared to ANNs, which justifies the advantages of SNN-based transformer."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n1.\tThe latency/runtime benefit of SNN-based transformer with different timesteps T needs to be compared to ANN accelerators.\n\n2.\tThe proposed method can have a high fidelity with a small timestep, but there is still 1% gap compared to ANNs, even with a large T. \nCompared to the training/calibration-based method, the training-free one shows a higher accuracy gap. (Table2, SNM, Calib on resnet20 can fully recover the accuracy). Can the authors comment on that? \n\n3.\tIs there any randomness in the spikes-based multiplications given the current data encoding? If there is, the output of the computing result is not deterministic. How robust is it to randomness? To have a deterministic output, the effective resolution will be reduced, thus harming accuracy. Can the authors comment on that?\n\n4.\tHow does the spiking-based multiplication differentiate from the standard multiplication mechanism in stochastic computing?\n\n5.\tThere are other acceleration methods to speed up and reduce energy consumption by a large factor without sacrificing accuracy, e.g., model compression and better architecture design. Moving to a new hardware platform with 30-40% energy reduction seems not very convincing."
                },
                "questions": {
                    "value": "Questions are listed in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698615044826,
            "cdate": 1698615044826,
            "tmdate": 1699636106855,
            "mdate": 1699636106855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k8WhRykRRi",
                "forum": "XrunSYwoLr",
                "replyto": "hyb2Z7hAt1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging our work and bringing up important points for discussion. We appreciate the opportunity to address these concerns and provide further clarification.\n\n### Weakness 1:\nAs this paper primarily focuses on algorithm development on neuromorphic platforms, our knowledge regarding Transformers on ANN accelerators is limited. We plan to promptly engage with our hardware collaborators to conduct a more comprehensive analysis if possible. Besides, we provide a more detailed explanation of the theoretical energy consumption calculation method used in our work in the response to Weakness 5.\n\n### Weakness 2: \nWe acknowledge the phenomenon you have pointed out. This gap primarily stems from the structural differences between the two ANN backbones, ResNet and Transformer. In ResNet, the non-linearities can be represented as piecewise linear functions, resulting in quantization and truncation errors that cause only minor accuracy loss. However, the non-linear operations in Transformer, such as GELU, softmax, and exp, are more complex. Consequently, we employ a group of neurons (UGOs) to fit and approximate these complex functions. The errors introduced during this approximation can never be recovered by increasing timesteps, but can be further optimized during pre-training UGOs.\n\n### Weakness 3\nFrom an algorithmic perspective, with fixed input values, the existing encoding methods adhere to deterministic arithmetic rules. Consequently, the spike results of the input and output on each neuron are always deterministic. Our algorithm only focuses on the rate statistics at the end of all time steps, and the temporal resolution is solely used to adjust the quantization accuracy of spike encoding without other significant affect.\n\n### Weakness 4\nWe appreciate your insightful observation. Indeed, both stochastic computing and SNN multiplications share the computational objective of encoding firing rates with two bit streams and performing multiplications on them. However, in stochastic computing, the result is influenced by the correlation between the occurrence positions of spikes in the two sequences, leading to instability. In SNNs, where spikes are often sparse, this instability is more pronounced and can result in significant errors, particularly in deep networks when timesteps are insufficient. Our proposed algorithm addresses this challenge by achieving stable and accurate computation of the multiplication of the two sequences, albeit at the cost of increased computation compared with stochastic computing.\n\n### Weakness 5\nThe energy reductions mentioned (30%~40%) are not based on actual measurements on a hardware platform, but rather serve as reference values at the algorithmic level. These estimates facilitate performance comparisons with existing SNN conversion works. Specifically, we adopt the typical energy values proposed in the 2014 paper [1], such as $E_{MAC}=4.6pJ, E_{AC}=0.9pJ. These values are outdated as hardware has evolved since then. However, for fair comparisons, many mainstream works in the field, such as [2], [3], [4], still employ these values as the standard. Furthermore, the energy reductions achieved by these existing conversions also ranges from 30% to 50%.\n\nOur contribution primarily lies on enabling the direct deployment of ANN Transformers on neuromorphic hardware. In order to directly transfer pre-trained model parameters, we could not redesign the model structure to optimize energy consumption. However, we may explore some model compression methods in the future that do not compromise the structure.\n\nThank you again for your valuable feedback, which has allowed us to further elaborate on these weaknesses and provide additional context for our research. We appreciate your time and consideration.\n\n[1] Horowitz, Mark. \"1.1 computing's energy problem (and what we can do about it).\" 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC). IEEE, 2014.\n\n[2] Li, Yuhang, et al. \"A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration.\" International conference on machine learning. PMLR, 2021. \n\n[3] Wang, Yuchen, et al. \"Signed neuron with memory: Towards simple, accurate and high-efficient ann-snn conversion.\" International Joint Conference on Artificial Intelligence. 2022.\n\n[4] Li, Yang, and Yi Zeng. \"Efficient and accurate conversion of spiking neural network with burst spikes.\" arXiv preprint arXiv:2204.13271 (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880647866,
                "cdate": 1699880647866,
                "tmdate": 1699880647866,
                "mdate": 1699880647866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NVqhLPpRhN",
            "forum": "XrunSYwoLr",
            "replyto": "XrunSYwoLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed Universal Group Operator (UGO) and Spatio-Temporal Approximation (STA) to fit the functions of LayerNorm, GELU layers and optimize the conversion error about self-attention modules."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The theoretical analysis about Temporal Estimation & Correction in Eq.5-Eq.11 is convincing."
                },
                "weaknesses": {
                    "value": "1. From Tab.1-2, it seems that the author's approximate fitting methods for nonlinear operations such as LayerNorm require relatively long time-steps ($\\geq 32$) to be effectively implemented, which will result in more significant time latency and energy consumption. In addition, even under 256 time-steps, the author's approximate fitting and error correction methods cannot completely eliminate the conversion error (there is a ~1% accuracy loss).\n\n2. Regarding the fitting calculation of LayerNorm and Softmax layers, as well as the self-attention layer error correction calculation in Eq.9, it seems that the calculation steps and costs involved are still relatively large. I think this may hinder the algorithm's practical application.\n\n3. I noticed that a previous work [1] achieved similar ANN-SNN Conversion performance to this paper when using BatchNorm layers directly (without involving nonlinear operations) and without error correction for attention modules. So I think the value of this approximate fitting and error correction method still needs to be further evaluated.\n\n[1] Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang, Zhongrui Wang, Renjing Xu. Masked Spiking Transformer. ICCV 2023."
                },
                "questions": {
                    "value": "See Weakness Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1774/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb",
                        "ICLR.cc/2024/Conference/Submission1774/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683099762,
            "cdate": 1698683099762,
            "tmdate": 1700722815735,
            "mdate": 1700722815735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "boZd8pJnun",
                "forum": "XrunSYwoLr",
                "replyto": "NVqhLPpRhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback on our work. Since the submission of our paper, we have made some improvements to address the issues raised. \n\n### Weakness 1:\n1.\tTime Latency: The latency observed during ANN-SNN conversion is not solely caused by the approximation method proposed in our work. It primarily stems from the inherent requirement for higher precision in large-scale pre-trained models, which is far less sparse than typical SNN models. Additionally, we would like to highlight that in our subsequent experiments, our method also achieve satisfactory outcomes at T=16. Notably, the performance reduction of our approach is even lower that of the ResNets at the same time step.\n\n| Dataset   | Model      | Method              | ANN Acc. | T=16 | T=32  | T=64  | T=128 | T=256 |\n| --------- | ---------- | ------------------- | -------- | ----- | ----- | ----- | ----- | --------- |\n| CIFAR-10  | ResNet-50  | Calib. (Li et al.)  | 72.35    |59.71  | 64.08 | 68.13 | 71.04 | 71.19 |\n| CIFAR-10  | ResNet-50  | SNM (Wang et al.)   | 72.35    |/  | 58.69 | 61.22 | 70.68 | 70.88 |\n| *CIFAR-10* | *ResNet-50* | *Combined Conversion* | *72.35*  |/  | *63.22* | *67.05* | *71.15* | *70.92* |\n| CIFAR-10  | ResNet-101 | Calib. (Li et al.)  | 79.64    |31.65  | 38.21 | 55.37 | 67.44 | 71.21 |\n| CIFAR-10  | ResNet-101 | SNM (Wang et al.)   | 79.64    |/  | 43.25 | 52.68 | 68.42 | 72.96 |\n| *CIFAR-10* | *ResNet-101* | *Combined Conversion* | *79.64*  |/  | *55.03* | *64.62* | *68.37* | *73.64* |\n| **CIFAR-10** | **ViT-B/32** | **STA (Ours)**  | **89.74** | **83.92** | 87.71 | 88.20 | 88.29 | 88.34 |\n| CIFAR-100 | ResNet-50  | Calib. (Li et al.)  | 41.01    |22.53  | 24.67 | 33.41 | 38.20 | 39.01 |\n| CIFAR-100 | ResNet-50  | SNM (Wang et al.)   | 41.01    |/  | 35.64 | 34.71 | 39.95 | 41.13 |\n| *CIFAR-100* | *ResNet-50* | *Combined Conversion* | *41.01* |/  | *37.41* | *39.26* | *40.31* | *40.78* |\n| **CIFAR-100** | **ViT-B/32** | **STA (Ours)** | **64.26** | **54.12** | 62.55 | 62.74 | 62.98 | 63.01 |\n| ImageNet-200     | ResNet-50 | Calib. (Li et al.) | 45.63 | /          | 22.50 | 34.51 | 41.82 | 42.03 |\n| ImageNet-200     | ResNet-50 | SNM (Wang et al.) | 45.63 | /          | 25.43 | 38.17 | 42.25 | 42.95 |\n| **ImageNet-200** | **ViT-B/32** | **STA (Ours)** | **62.25** | **44.72** | 59.79 | 61.24 | 61.53 | 61.66 |\n| **ImageNet** | **ViT-B/32** | **STA (Ours)** | **57.93** | **46.62** | 55.50 | 56.39 | / | / |\n\n2.\tConversion Error under large time-steps: The error in our method is caused by the approximation error of nonlinear operators with our Universal Group Operators (UGOs), as in Fig.6. The limited number of neurons are not sufficient to accurately fit complex functions, so such error cannot be eliminated even with infinite timesteps. Further optimization of the pre-training process of UGO can help alleviate this issue. \n\n### Weakness 2\nCompared to convolutional networks, the ANN-SNN conversion process for Transformers does indeed incur higher computational costs. The main contribution of our work lies in demonstrating the feasibility of this conversion and exploring its computational cost in preliminary terms. In practical applications, there are various energy optimization methods available, such as spatial pruning and model compression, as well as temporal approaches based on the dynamics of different layers in the network. Furthermore, it is important to note that our SNN design is tailored specifically for hardware implementation, as we have removed nonlinear operators that hinder efficient hardware inference. Such operators are implicitly retained in many other SNN works.\n\n### Weakness 3\nThe key distinction between our work and [1] is that [1] modifies and trains the original ANN model before conversion, which is not involved in our approach. Our goal is to rapidly obtain a fully equivalent SNN counterpart for widely used large-scale pre-trained ANN models and deploy them directly in production environments. This necessitates adopting a training-free approach to prevent any degradation in generalization performance caused by fine-tuning. Furthermore, the reason for not using Batch Normalization is that it is not commonly adopted in mainstream pre-trained Transformer models.\n\nWe believe that our reply addresses part of your concerns, and obtaining a higher rating is crucial for us to further optimize our research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880639309,
                "cdate": 1699880639309,
                "tmdate": 1699880639309,
                "mdate": 1699880639309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E2GKP55z0O",
                "forum": "XrunSYwoLr",
                "replyto": "boZd8pJnun",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_S5Cb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I think the authors have effectively addressed my concerns and I have increased my rating to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722780235,
                "cdate": 1700722780235,
                "tmdate": 1700722780235,
                "mdate": 1700722780235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ScutC9ckc8",
            "forum": "XrunSYwoLr",
            "replyto": "XrunSYwoLr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a training-free method to convert ANN transformers into SNNs, preserving the weights of the original pretrained model to ensure its inference capability remains intact. The resulting SNN Transformer model outperforms its convolutional network counterparts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.This paper overcomes the differences in computational paradigms between ANN and SNN Transformers, and can accurately approximate ANNs with converted models.\n\n2.The proposed training-free conversion strategy could enable the direct deployment of large-scale pretrained ANN models to low-power neuromorphic hardware."
                },
                "weaknesses": {
                    "value": "1.The proposed Universal Group Operators use extensive spiking neurons to model fine-grained ANN operations. This high model complexity reduces power efficiency and incurs large memory usage. \n\n2.The current implementation only handles image Transformers. Longer sequences in language models may introduce more unaddressed issues such as threshold variations similar to that in spiking RNN."
                },
                "questions": {
                    "value": "1. Would employing model compression strategies, such as pruning, enhance the efficiency and reduce the size of the Universal Group Operators?\n\n2. The conversion implementation integrates multiple existing ANN-SNN conversion algorithms, including SNM and Burst. Using the same combined conversion for ResNet baselines could enable a more fair comparison.\n\n3. What hurdles might one encounter when adapting this technique to language Transformers? Would the method necessitate modifications?\n\n4. What challenges might arise when adapting this method to larger-scale transformer models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759066698,
            "cdate": 1698759066698,
            "tmdate": 1699636106709,
            "mdate": 1699636106709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yPPciZs7x0",
                "forum": "XrunSYwoLr",
                "replyto": "ScutC9ckc8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and acknowledgment of our work. We appreciate the opportunity to address your concerns and provide further clarification on the points raised.\n\n### Weakness 1\nWe acknowledge your observation on computational cost. However, considering that the operations themselves in ViT are more complex compared to basic ReLU neurons, a consequent decrease in efficiency may be inevitable. Besides, we actively utilize larger memory to achieve higher accuracy within limited timesteps. Further optimization can be conducted to control the power efficiency and memory cost through sparsification on the converted model.\n\n### Weakness 2 & Question 3\nThe method we proposed is also applicable to language models since the modules used in both vision/language Transformer architecture are identical. Furthermore, our approach avoids the issues caused by the sequential order of tokens in RNNs, which are absent in the Transformer. Nevertheless, we would like to highlight two practical considerations:\n\n1.\tInput sample size: In ViT-B/32, the number of patches is 7x7+1=50, whereas mainstream language models typically require 512 or more tokens. This disparity results in an increase in the scale of matrix multiplication and memory requirements.\n\n2.\tLatency of generative models: The precision of Spike Neural Network (SNN) outputs depends on the number of time steps. Consequently, generative models like GPT heavily rely on a high number of time steps to achieve accurate language generation.\n\n### Question 1\nYou are correct in suggesting that Universal Group Operators (UGO) can be further compressed. In fact, in the UGO model utilized in our paper, a significant portion of the IF neurons were rarely activated. This perspective indicates that the non-linear operations in ViT can be compressed and represented by smaller models. In future work, we plan to optimize these operations targeting language Transformers.\n\n### Question 2\nWe conducted these evaluations on the CIFAR-10 and CIFAR-100 datasets, supplementing the results in Table 1. The additional results are as follows:\n\n| Dataset   | Model      | Method              | ANN Acc. | T=32  | T=64  | T=128 | T=256 |\n| --------- | ---------- | ------------------- | -------- | ----- | ----- | ----- | ----- |\n| CIFAR-10  | ResNet-50  | Calib. (Li et al.)  | 72.35    | 64.08 | 68.13 | 71.04 | 71.19 |\n| CIFAR-10  | ResNet-50  | SNM (Wang et al.)   | 72.35    | 58.69 | 61.22 | 70.68 | 70.88 |\n| CIFAR-10  | ResNet-50  | *Combined Conversion* | *72.35*    | *63.22* | *67.05* | *71.15* | *70.92* |\n| CIFAR-10  | ResNet-101 | Calib. (Li et al.)  | 79.64    | 38.21 | 55.37 | 67.44 | 71.21 |\n| CIFAR-10  | ResNet-101 | SNM (Wang et al.)   | 79.64    | 43.25 | 52.68 | 68.42 | 72.96 |\n| CIFAR-10  | ResNet-101  | *Combined Conversion* | *79.64*    | *55.03* | *64.62* | *68.37* | *73.64* |\n| CIFAR-10  | ViT-B/32   | **STA (Ours)**          | **89.74**    | **87.71** | **88.20** | **88.29** | **88.34** |\n| CIFAR-100 | ResNet-50  | Calib. (Li et al.)  | 41.01    | 24.67 | 33.41 | 38.20 | 39.01 |\n| CIFAR-100 | ResNet-50  | SNM (Wang et al.)   | 41.01    | 35.64 | 34.71 | 39.95 | 41.13 |\n| CIFAR-100 | ResNet-50 | *Combined Conversion* | *41.01* | *37.41* | *39.26* | *40.31* | *40.78* |\n| CIFAR-100 | ViT-B/32   | **STA (Ours)** | **64.26**    | **62.55** | **62.74** | **62.98** | **63.01** |\n\nIn comparison to existing baselines, this conversion approach did not yield significant performance improvements on ResNets. Moreover, it is important to note that even if it achieves accurate conversion, the performance still lags far behind ViT due to the limitations of a weak backbone.\n\n### Question 3\nPlease refer to the response provided for Weakness 2, as it addresses the applicability of our method to language models.\n\n### Question 4\nThis question is also related to Weakness 2. The main engineering challenges we encountered include the increased memory requirements resulting from larger input sizes and wider blocks, as well as the error accumulation introduced by deeper network architectures. Despite these challenges, our conversion method remains applicable since the basic modules are consistent with that of ViTs.\n\nWe will continue to refine our work based on your valuable feedback. Thank you for your time and consideration."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880630732,
                "cdate": 1699880630732,
                "tmdate": 1699880630732,
                "mdate": 1699880630732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cpMN8hMRmz",
                "forum": "XrunSYwoLr",
                "replyto": "yPPciZs7x0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1774/Reviewer_jaAV"
                ],
                "content": {
                    "title": {
                        "value": "Responses"
                    },
                    "comment": {
                        "value": "Thanks for the response.\nThe authors have addressed my concerns. I recommend acceptance."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663516370,
                "cdate": 1700663516370,
                "tmdate": 1700663516370,
                "mdate": 1700663516370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]