[
    {
        "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings"
    },
    {
        "review": {
            "id": "DQKHVfHS1j",
            "forum": "sehRvaIPQQ",
            "replyto": "sehRvaIPQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_27LG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_27LG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple but effective technique for improving the \"communication\" between two or more LLMs when \"debating\" about an answer (called CIPHER). Instead of the LM generating natural text with some token sampling technique (e.g., temperature, nucleus etc.) that is then passed to the other LM in the input context, this work generates token representations that are a weighted average of the full vocabulary. The weights are determined by the softmax predictions. So instead of natural text, the sequence of aggregated representations is passed to the other LM. The final answer is generated in natural language by falling back to the regular token sampling technique.\n\nAuthors experimented with several LLaMA models on a few reasoning tasks and found this technique to improve the final answer after a few rounds of debate between two models by 1-3.5%."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is very simple, yet sounds interesting to explore and seems to be effective. The prompts and configurations are shared in the appendix, and the experiments are with open-sourced models so it should be easy to reproduce and build on this research."
                },
                "weaknesses": {
                    "value": "While the idea of enriching the communication across models is exciting, I believe there are several shortcomings in the current work:\n\n1. The method requires access to the output embeddings of the model and the full output probability, so cannot be directly applied to LLMs served via API.\n2. While there method is described in several sections, the definition of the method still didn't feel precise enough. For example, I assume that at each step of the autoregressive generation the input token embedding is the previously aggregated token embedding and not a single sampled token? Or do you still sample tokens for decoding and just convert the full sequence to aggregated embeddings after?\n3. There is a lot of discussion on the effect of sampling temperature, including ablation experiments etc., that I am confused about. From my understanding of the method (e.g., Equation 2), there is no temperature parameter in the method. The only effect of temp is in the final generation of the answer by one of the models. Therefore, I don not understand why are two temperatures reported, or the ablation in Figure 5.\n4. The experimental results are only on 4 datasets and the improvements are relatively small, and no confidence intervals are reported.\n5. I didn't see any discussion on validation set for hyper param setting, and according to the appendix it seems like different temperatures were used for the main baseline (NLD) and for CIPHER? Why is that? This raises some additional concerns about the experimental setup.\n6. In addition to the point 2, the variants of the method are also not precisely described. For example, on Table 2: \"This issue can be addressed by maintaining a bijection mapping between\nthe vocabulary embeddings of the two models.\" I can guess how it was implemented, but would appreciate elaboration.\n7. Would be interesting to further understand the effect of passing these aggregated embeddings to the model. The communication to later generated tokens is anyway through K,V embeddings of the self-attention that have some dense representations and not limited to the token vocabulary. Some exploration on how the input embeddings impact the K,Vs could perhaps shed some light on how CIPHER modifies the communication"
                },
                "questions": {
                    "value": "please see questions in the weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698600672262,
            "cdate": 1698600672262,
            "tmdate": 1699636702522,
            "mdate": 1699636702522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "353eYWvedV",
                "forum": "sehRvaIPQQ",
                "replyto": "DQKHVfHS1j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 27LG [Part 1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the useful comments on our work. We address the comments  below.\n\n> The method requires access to the output embeddings of the model and the full output probability, so cannot be directly applied to LLMs served via API.\n\nOur research is focused on open-source language models due to its transparency and reproducibility. While open-source models provide insight into their internal mechanisms, APIs often obscure the distinction between the AI model and the engineering components, e.g., incorporating specialized engineering for specific canned responses.\n\n\n> While there method is described in several sections, the definition of the method still didn't feel precise enough. For example, I assume that at each step of the autoregressive generation the input token embedding is the previously aggregated token embedding and not a single sampled token? Or do you still sample tokens for decoding and just convert the full sequence to aggregated embeddings after?\n\nWe are grateful to the reviewer for pointing out the need for greater clarity. \nIn our approach, at each step $t$ of the autoregressive generation, we utilize an aggregation of all previous $t\u22121$ generated token embeddings as the input for the next token generation. \n\nWe have revised Section 3.2 of our manuscript to reflect this clarification. The updated notation, now denoted as $\\overline{emb}^{(1:t-1)}$, has been introduced to clearly represent the aggregated input at each step of the generation process. Please refer to the revised version for a more detailed and precise explanation.\n\n> There is a lot of discussion on the effect of sampling temperature, including ablation experiments etc., that I am confused about. From my understanding of the method (e.g., Equation 2), there is no temperature parameter in the method. The only effect of temp is in the final generation of the answer by one of the models. Therefore, I don not understand why are two temperatures reported, or the ablation in Figure 5.\n\nWe apologize for any confusion caused by our initial presentation of Equation 2 and appreciate the opportunity to clarify. In Equation 2, our method generates a new embedding by calculating the weighted average of all token embeddings within the vocabulary set. Here, the \u201cweight\u201d for each token is determined by its respective probability, which is derived from the softmax of the logits. Similar to conventional natural language generation processes, the temperature parameter $T$ controls the smoothness of the probability distribution, thereby shaping the output embedding.\nTo enhance clarity and facilitate better understanding, we have revised notations and Equation 2 in our manuscript. We decided to break it down into two separate equations as follows:\n\n$\\bar{emb}^{(t)} = \\sum_{i = 1}^{V} p_{vocab_i}^{(t)} \\cdot emb_{vocab_i}$\n\nwhere\n\n$[p_{vocab_i}^{(t)}, \\dots, p_{vocab_V}^{(t)}] = softmax \\\\{ logit(emb_{prompt}, \\bar{emb}^{(1:t-1)} ) /T  \\\\}$\n\n\n\n\nIn addition to the equation, we have revised Section 3.2 of our manuscript to clarify on the temperature:\n\n*\u201d**Role of temperature.** The temperature $T$ in Eq.1 and Eq.3 controls the smoothness of the probability $p^{(t)}_{vocab_i}$. When $T \\rightarrow 0$, both CIPHER's embedding generation and natural language generation result in greedy generation. In contrast, a large $T$ leads to a uniform averaging and sampling over the whole vocabulary set for CIPHER and natural language generation, respectively. We find that choosing proper temperatures for the debaters plays a pivotal role in the performance of CIPHER debate and natural language debate. Thus, to ensure fairness of our empirical evaluation, we utilize Bayesian optimization [1] to select the best performing temperatures for each method in our experiments in Section 4. Moreover, we conduct sensitivity analysis on the temperatures in Section 5.2.\u201d*\n\n[1] Bayesian Optimization: Open source constrained global optimization tool for Python\n\n\nWe believe these changes will effectively address the comments and improve the overall clarity of our method's description."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429621056,
                "cdate": 1700429621056,
                "tmdate": 1700429621056,
                "mdate": 1700429621056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2loxUvFIDi",
                "forum": "sehRvaIPQQ",
                "replyto": "DQKHVfHS1j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 27LG [Part 2]"
                    },
                    "comment": {
                        "value": "> The experimental results are only on 4 datasets \n\nWe would like to clarify a minor discrepancy in the number of datasets mentioned. Our experiments were conducted on five datasets, including GSM8K, Formal Logic, High School Math, Professional Psychology, and Arithmetic. \n\n\n> the improvements are relatively small, and no confidence intervals are reported.\n\n\nOur approach demonstrates consistent improvements of 1-3.5% across five datasets and six different models without any further training or fine-tuning.  Thus, we believe that the improvement is significant.\n\nWe ran 5 experiments for each method in Table 1 to obtain the confidence intervals for LLaMA2, reported as below:\n\n| Method                   | GSM8K | H.S. Math | Psychology | Formal Logic | Arithmetic |\n|--------------------------|-------|-----------|------------|--------------|------------|\n| Single Answer            | 59.5\u00b12.0  | 38.1\u00b12.6      | 71.5\u00b11.1       | 46.0\u00b12.9         | 75.0\u00b11.7       |\n| Major@5   | 65.5\u00b11.1  | 41.5\u00b11.5      | **74.0\u00b11.0**   | 44.4\u00b12.3         | 77.6\u00b11.4       |\n| NLD    | 66.5\u00b10.8  | 39.4\u00b10.9      | 73.0\u00b11.5      | 49.2\u00b10.9         | 83.0\u00b11.6       |\n| CIPHER (ours)            | **67.5\u00b10** | **41.5\u00b10**  | **74.0\u00b10**   | **52.4\u00b10**     | **86.5\u00b10**   |\n\nOverall, *Single Answer* has the highest variance, ranging from 1 to 3% across five datasets. This increased variance is attributed to its token sampling process during token generation. *Major@5* and *NLD* methods reduce variance by ensembling diverse responses. However, they still rely on a similar token sampling process, leading to relatively high variance. In contrast, *CIPHER* is not affected by this issue, thanks to its deterministic embedding generation. For any given input, *CIPHER* consistently produces the same embeddings, which are then translated into identical texts via a nearest neighbor search across the vocabulary set.\n\nIt's important to note that running these baseline methods are time-consuming, regarding that we need at least 5 runs for each to obtain a valid standard error estimation. While we are working on confidence intervals for additional experiments, as of now, we can only provide these intervals for LLaMA2 in Table 1 as part of our rebuttal. We plan to include all the results in the final camera-ready version of our work.\n\n\n\n> I didn't see any discussion on validation set for hyper param setting\n\nAs our approach focuses solely on inference without involving any training phase, the conventional use of a validation set for hyperparameter tuning was not applicable. Additionally, we swept over temperatures to guarantee we report the best of each method, hence no need to set a validation set.\n\n\n>  according to the appendix it seems like different temperatures were used for the main baseline (NLD) and for CIPHER? Why is that? This raises some additional concerns about the experimental setup.\n\nWe employ Bayesian optimization to sweep over some temperature pairs for all the baselines and our method. For a fair comparison, we reported the best performance of each method in our paper. We also included an ablation study on the temperature, as reported in Figure 5, to give a deeper understanding of how the temperature affects debate performance.\n\nAs each method prefers different temperatures to obtain the best results, it is not fair to use the same temperature. For example, Majority Voting (major@5) usually requires higher temperatures compared to *Single Answer*, to have a more diverse response for the ensemble.\n\n That said, we still conducted some results on the same pairs of temperatures (in brackets) for both CIPHER and NLD baseline as follows:\n\n| Method                   | GSM8K (0.1, 0.5) | H.S. Math (0.1, 0.2)| Psychology (0.3, 0.8) | Formal Logic (0.3, 0.5) | Arithmetic (0.2, 0.3)|\n|--------------------------|-------|-----------|------------|--------------|------------|\n| NLD    | **65.5**  | 38.2      | 68.5      | 39.7      | **74.5**       |\n| CIPHER (ours)            | 65.0 | **40.0**  | **72.5**  | **44.4**    | 74.0   |\n\n\nIn general, CIPHER works better than NLD for these temperatures reported above, but again, this is not a good setting as each method favors different temperature settings. As discussed in Sec 5.2, CIPHER often prefers low and high-temperature agents to pair together. This explains its lower accuracy compared to NLD on two datasets, as shown in the table above, particularly in Arithmetic, where CIPHER prefers a large difference in temperatures."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429995811,
                "cdate": 1700429995811,
                "tmdate": 1700430447493,
                "mdate": 1700430447493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pQerfrc7ru",
                "forum": "sehRvaIPQQ",
                "replyto": "DQKHVfHS1j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Reviewer_27LG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Reviewer_27LG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed reply. The updated paper now indeed better explains the role of the temperatures in the method and clarifies some of my initial confusion. I appreciate your work on revising the paper and answering my questions. \n\nI would be very interested in examining the experimental performance of the proposed method. However, I still have two concerns with the current version and response:\n1. I think you confused my question about confidence intervals with computing variance. CI help provide a statistically more reliable estimate of the performance. One option to obtain such is to use bootstrapping which doesn't require any new runs of the model and works for both deterministic or random outputs.\n2. I disagree with your answer that a validation set is not required here. Sweeping hyper parameters over the test set and reporting best performance is not a valid evaluation setup. Especially given the sensitivity of the performance to the temperature value as you yourself state above and as shown in the temperature heat map figure."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666480927,
                "cdate": 1700666480927,
                "tmdate": 1700666741095,
                "mdate": 1700666741095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KtcOKot8Ss",
                "forum": "sehRvaIPQQ",
                "replyto": "DQKHVfHS1j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response! [part 1]"
                    },
                    "comment": {
                        "value": "> I think you confused my question about confidence intervals with computing variance. CI help provide a statistically more reliable estimate of the performance. One option to obtain such is to use bootstrapping which doesn't require any new runs of the model and works for both deterministic or random outputs.\n\n\n\n\nWe appreciate your suggestion regarding the use of bootstrapping to estimate confidence intervals (CIs). However, while confidence intervals are valuable in many statistical analyses, it is not standard in the related literature. \u200b\u200b For example, related works such as [1, 2, 3, 4, 5, 6]  presented their findings without reporting confidence intervals. In addition to the std reported for LLaMA2-70B, we\u2019re working on adding std for the rest of the table, and will incorporate the results in our camera ready version. While we believe that including standard deviation adequately demonstrates the robustness of our method, especially in comparison with baseline models, we still show the bootstrapped 95% confidence intervals (in brackets) for our results as follows.\n\n\n\n---\n**LLaMA2-70 in Table 1**\n\n\n\n\n| Method                   | GSM8K | H.S. Math | Psychology | Formal Logic | Arithmetic |\n|--------------------------|-------|-----------|------------|--------------|------------|\n| Single Answer            | 59.3 (52.0, 65.5)  | 38.3 (32.6, 44.1)     | 71.5 (67.8, 75.0)      | 46.0 (36.5, 54.0)         | 75.0 (69.0, 81.0)       |\n| Major@5   | 65.7 (59.0, 72.0)  | 41.3 (35.6, 47.0)      | **74.0** (70.4, 77.4)  | 44.4 (34.9, 52.4)        | 77.6 (71.5, 83.0)      |\n| NLD    | 66.5 (59.5, 72.5)  | 39.4 (32.6, 44.0)     | 73.0 (69.4, 76.4)     | 49.2 (40.5, 57.1)         | 83.0 (78.5, 89.0)      |\n| CIPHER (ours)            | **67.5** (61.0, 74.0) | **41.5** (35.6, 47.0)  | **74.0** (70.4, 77.4)   | **52.4** (42.9, 61.0)     | **86.5** (81.5, 91.0)   |\n\n\n\n\n\n\n\n---\n**LLaMA-65B, Table 1**\n\n\n\n\n|                   | GSM8K            | H.S. math        | Psychology       | Formal Logic     | Arithmetic       |\n|-------------------|------------------|------------------|------------------|------------------|------------------|\n| Single Answer     | 50.5 (43.5, 57.5)| 33.3 (27.8, 38.5)| 66.5 (62.7, 70.2)| 43.5 (34.9, 51.6)| 29.8 (23.5, 36.0)|\n| Major@5           | 57.8 (50.5, 64.0)| 36.7 (31.1, 42.6)| 67.0 (63.0, 70.5)| 46.8 (37.3, 54.8)| 31.0 (25.0, 37.5)|\n| NLD               | 55.5 (48.5, 62.5)| 36.7 (31.1, 42.6)| 68.5 (64.6, 71.8)| 46.0 (36.5, 54.0)| 35.0 (28.5, 41.5)|\n| CIPHER            | **58.8** (51.5, 65.5)| **38.5** (32.6, 44.4)| **70.5** (66.6, 73.8)| **50.8** (42.1, 59.5)| **36.5** (30.0, 43.5)|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---\n\n**Fig 3 for multiagent debates across different models**\n\n\n|            | MPT-30B         | Falcon-40B-Instruct | LLaMA-65B        | LLaMA2-Chat-70B  | LLaMA2-70B        | WizardMath-70B   |\n|------------|-----------------|----------------------|------------------|-------------------|-------------------|------------------|\n| Single Answer     | 21.0 (15.5, 27.0) | 31.5 (25.1, 38.0)    | 50.5 (43.5, 57.5)| 54.5 (47.5, 61.5) | 59.2 (52.0, 65.5) | 81.0 (75.0, 86.0)|\n| NLD     | 25.5 (20.0, 32.0) | 36.5 (30.0, 43.5)    | 55.5 (48.5, 62.5)| 61.5 (54.5, 68.0) | 66.5 (60.0, 73.0) | 83.0 (77.0, 87.5)|\n| CIPHER     | **26.0** (20.5, 32.5) | **38.0** (31.5, 45.0)    | **58.8** (51.5, 65.5)| **63.5** (56.5, 70.0) | **67.5** (61.0, 74.0) | **84.5** (79.0, 89.0)|\n\n[1] Self-Consistency Improves Chain of Thought Reasoning in Language Models\n\n[2] SELF-REFINE: Iterative Refinement with Self-Feedback\n\n[3]  Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback\n\n[4] Teaching large language models to self-debug\n\n[5] Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n\n[6] Improving Factuality and Reasoning in Language Models through Multiagent Debate"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702972877,
                "cdate": 1700702972877,
                "tmdate": 1700703080041,
                "mdate": 1700703080041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MmsQGmKYfw",
            "forum": "sehRvaIPQQ",
            "replyto": "sehRvaIPQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_LfJQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_LfJQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about a new way of communication among large language models (LLMs) that use embeddings instead of natural language. \n\nThe paper claims that this method, called CIPHER, can improve the reasoning ability of LLMs by avoiding information loss and encoding a broader spectrum of information. The paper evaluates CIPHER on five reasoning tasks and shows that it outperforms the state-of-the-art natural language debate methods. The paper also conducts an ablation study to explain why CIPHER works better."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper proposes a novel communication protocol for large language models (LLMs) that use embeddings instead of natural language.\n* The paper provides a clear and detailed description of the CIPHER method and its implementation. \n* The paper also conducts extensive experiments on five reasoning tasks and compares CIPHER with the state-of-the-art natural language debate methods. The paper shows that CIPHER outperforms the baselines by a large margin on all tasks.\n* The paper also performs an ablation study to analyze the impact of different components and parameters of CIPHER."
                },
                "weaknesses": {
                    "value": "See Questions"
                },
                "questions": {
                    "value": "1. The authors conducted experiments on five common reasoning datasets, can this method be tested on agent-related leaderboards\n2. In formula 2, whether the response embedding will be adjusted, how are the results of different weights?\n3. Why are the results in table1 and table2 completely different, how many rounds are used in table1\uff1f\n4. Can this method be used for different models with the same tokenizer, for NLD, different models can communicate each other."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652836675,
            "cdate": 1698652836675,
            "tmdate": 1699636702396,
            "mdate": 1699636702396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w6ljVIxYfB",
                "forum": "sehRvaIPQQ",
                "replyto": "MmsQGmKYfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer LfJQ"
                    },
                    "comment": {
                        "value": "> The authors conducted experiments on five common reasoning datasets, can this method be tested on agent-related leaderboards\n\nWe kindly ask the reviewer to clarify which agent-related leaderboards might be relevant. If the leaderboard suits our framework, we would be happy to test and report it in our final version.\n\n\n\n> In formula 2, whether the response embedding will be adjusted, how are the results of different weights?\n\nIn Equation 2, our method generates a new embedding by calculating the weighted average of all token embeddings within the vocabulary set. Here, the \u201cweight\u201d for each token is determined by its respective probability, which is derived from the softmax of the logits. Similar to conventional natural language generation processes, the temperature parameter $T$ controls the smoothness of the probability distribution, thereby shaping the output embedding. We also added a discussion on the temperature in the revised version in Sec 3.2, as follows:\n\n*\u201d**Role of temperature.** The temperature $T$ in Eq.1 and Eq.3 controls the smoothness of the probability $p^{(t)}_{vocab_i}$. When $T \\rightarrow 0$, both CIPHER's embedding generation and natural language generation result in greedy generation. In contrast, a large $T$ leads to a uniform averaging and sampling over the whole vocabulary set for CIPHER and natural language generation, respectively. We find that choosing proper temperatures for the debaters plays a pivotal role in the performance of CIPHER debate and natural language debate. Thus, to ensure fairness of our empirical evaluation, we utilize Bayesian optimization [1] to select the best performing temperatures for each method in our experiments in Section 4. Moreover, we conduct sensitivity analysis on the temperatures in Section 5.2.\u201d*\n\n\n\nTo enhance clarity and facilitate better understanding, we have revised the notations and Equation 2 in our manuscript. We decided to break it down into two separate parts, now Eq.2 and Eq.3, as follows:\n\n\n$\\bar{emb}^{(t)} = \\sum_{i = 1}^{V} p_{vocab_i}^{(t)} \\cdot emb_{vocab_i}$\n\nwhere\n\n$[p_{vocab_i}^{(t)}, \\dots, p_{vocab_V}^{(t)}] = softmax \\\\{ logit(emb_{prompt}, \\bar{emb}^{(1:t-1)} ) /T  \\\\}$\n\n\n\n\n> Why are the results in table1 and table2 completely different, how many rounds are used in table1\uff1f\n\nThe settings in Table 1 and Table 2 are different and are stated in the captions. Specifically, in Table 1, we facilitate debates between two identical LLaMA-family models, with temperature being the only difference between the two debaters. In this case, we report the accuracies of the final responses from the debater operating at the lower temperature. In Table 2, we facilitate debates between one LLaMA-65B (first version) and one LLaMA2-70B. In this case, we report the accuracies of the final round responses from the two debaters separately for a thorough analysis.\nIn both Table 1 and Table 2, the total number of rounds is 3.\nWe added this to the caption for Table 1 in our revised version.\n\n\n\n\n> Can this method be used for different models with the same tokenizer, for NLD, different models can communicate each other.\n\nYes. In Table 2, we applied CIPHER debate between LLaMA-65B and LLaMA2-70B."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429326369,
                "cdate": 1700429326369,
                "tmdate": 1700429326369,
                "mdate": 1700429326369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mXZCADNVcS",
            "forum": "sehRvaIPQQ",
            "replyto": "sehRvaIPQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_b3L6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_b3L6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a communication regime named CIPHER to allow LLMs to communicate through embedding vectors instead of natural language tokens. The authors argue that this method preserves more information and avoids information loss due to token sampling. They conducted experiments on five reasoning tasks and showed that CIPHER debate outperforms natural language debate by 1-3.5%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A good idea to directly use embedding vectors to communicate between LLMs. \n\n2. The paper provides a rigorous and comprehensive evaluation of CIPHER on five diverse reasoning datasets across multiple domains. The result showed that CIPHER consistently outperforms natural language debate.\n\n3. The paper also conducts ablation studies and sensitivity analysis to investigate the mechanisms and factors that contribute to the performance of CIPHER."
                },
                "weaknesses": {
                    "value": "1. Limited Generalizability. As the authors described in the limitations, this method is only applicable to LLMs that share a common vocabulary. For different types of LLMs, aligning embeddings is a difficult task.\n\n2. From Figure 10, the language of CIPHER is still difficult to analyze."
                },
                "questions": {
                    "value": "1. Which experiment result can support the statement \"our approach can generalize across a wide array of LLMs, enabling even smaller LLMs to unlock the benefits of debate and achieve better performance than majority voting\"? Is there any experiment of smaller LLMs like LLaMA-2 13B or others?\n\n2. Why is the performance of CIPHER worse than natural language debate when Round=1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6362/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6362/Reviewer_b3L6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834280759,
            "cdate": 1698834280759,
            "tmdate": 1699636702287,
            "mdate": 1699636702287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W4fewJBrek",
                "forum": "sehRvaIPQQ",
                "replyto": "mXZCADNVcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer b3L6"
                    },
                    "comment": {
                        "value": "> Limited Generalizability. As the authors described in the limitations, this method is only applicable to LLMs that share a common vocabulary. For different types of LLMs, aligning embeddings is a difficult task.\n\nAs stated in the Limitation section, requiring a shared tokenizer is a limitation of our work and is out of the scope of this paper. Extending CIPHER debate to models with different tokenizations will require tokenization/embedding alignment, which would be an interesting direction for us to further explore. Such an alignment can potentially be tackled by utilizing methods in multimodality literature, such as [1, 2]. This literature deals with broader questions related to aligning various modalities.\n\nWe would like to note that, even though CIPHER debate can only be applied to models sharing the same tokenizer, we verify that it is effective in pushing the reasoning performance of the open-source models beyond the current state-of-the-art methods.\n\n[1] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\n\n[2] MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models\n\n\n\n> From Figure 10, the language of CIPHER is still difficult to analyze.\n\nAs described in the Introduction Section: \u201cFirst, we do not necessarily need to understand the intermediate debates amongst LLMs. Second, natural language generation uses only one token to represent the model\u2019s belief over the entire vocabulary, which risks losing information embedded within the model output logits.\u201d \nThe main takeaway of our approach is to trade interpretability for information preservation during the debates. Yet, we can still interpret the communications made by the debater operating at low temperatures (e.g., blue debater, now Figure 11 in the revised version) while using another debater with a higher temperature to gather more information. In practice, keeping at least one debater at a low temperature is necessary to collect the final result of the debate.\n\n\n> Which experiment result can support the statement \u201cour approach can generalize across a wide array of LLMs, enabling even smaller LLMs to unlock the benefits of debate and achieve better performance than majority voting\"? Is there any experiment of smaller LLMs like LLaMA-2 13B or others?\n\nPrior work in LLM debate and self-critique (e.g., [1, 2]) show that only some state-of-the-art LLMs, such as GPT-3.5, GPT4 are capable of taking advantage of debates and discussion. These models are over 100B parameters and closed-source.  In our work, we propose CIPHER that unleashes the potential of smaller open-source models in debates. By \u201cgeneralize across a wide array of LLMs\u201d, we mean that one can apply CIPHER to smaller sized models such as LLaMA-family ones, ranging from 7B (reported in Appendix B.2, Fig. 8a) to 70B (table 1). We chose to mainly use LLaMA2-70B in our experiments because it\u2019s one of the largest and most up-to-date models at that time. Additionally, we showed the results across different model architectures in Fig 3 to evaluate the robustness and generalizability of our method.\n\n[1] Self-refine: Iterative refinement with self-feedback\n\n[2] Improving language model negotiation with self-play and in-context learning from AI feedback.\n\n\n> Why is the performance of CIPHER worse than natural language debate when Round=1?\n\nIn Table 2, when round=1, it means the model only gives a direct response. By choosing responses of the larger model (\u201cLLaMA2-70B\u201d columns) as the final answers, our method is not worse, but rather slightly better than the baseline at round=1. That said, CIPHER is designed for communication in debate settings. Thus, we need debate rounds to fully take advantage of the method. We added the results by round for ablation study purposes, to see how the performance changes over rounds."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428798285,
                "cdate": 1700428798285,
                "tmdate": 1700430845447,
                "mdate": 1700430845447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MC66P1AUO3",
                "forum": "sehRvaIPQQ",
                "replyto": "W4fewJBrek",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Reviewer_b3L6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Reviewer_b3L6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response; my concerns have been largely addressed. \nRegarding Question 2, I suggest incorporating the meaning of Round=1 into the figure caption. \nAdditionally, in Figure 4(a), the performance of CIPHER at rounds = 1 seems weaker than NLD. \nMoreover, why does the performance of LLaMA2-70B (CIPHER) decrease on Arithmetic when Round=2?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670497101,
                "cdate": 1700670497101,
                "tmdate": 1700670497101,
                "mdate": 1700670497101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cIsXLaKEFV",
                "forum": "sehRvaIPQQ",
                "replyto": "mXZCADNVcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response!"
                    },
                    "comment": {
                        "value": "> I suggest incorporating the meaning of Round=1 into the figure caption \n\nThank you for the suggestion. We will incorporate the meaning of round=1 to the figure caption in the camera-ready version.\n\n> in Figure 4(a), the performance of CIPHER at rounds = 1 seems weaker than NLD\n\nIn Figure 4(a), CIPHER's performance is lower than NLD in the first round. In general, the performances of NLD and CIPHER are quite similar in the first round, as shown in the performance of LLaMA2-70B and LLaMA-65B in Table 2. As discussed earlier, the first-round result is the direct response without any debate. CIPHER is designed for communication, thus we need a few rounds of debate to fully take advantage of it.\n\n\n> why does the performance of LLaMA2-70B (CIPHER) decrease on Arithmetic when Round=2?\n\nIt's a good observation!  In Table 2, we paired LLaMA2-70B with a smaller model, LLaMA-65B.\n\nIn the first round, LLaMA-65B's performance was low, resulting in a large gap between the two models, especially on the Arithmetic dataset.\n\nIn the second round, LLaMA2-70B received responses from LLaMA-65B, which were much worse compared to its own performance. This confused LLaMA2-70B, leading to a decrease in its performance in this round. However, for LLaMA-65B, this round saw a significant boost in performance, as it obtained much better answers from LLaMA2-70B.\n\nIn round 3, we observed an increase in the performance of both models after the gap between them was reduced.\n\nIn general, after a decrease in the second round, LLaMA2-70B recovered and boosted its performance in the third round. Note that this drop in the second round was also the case for NLD, where LLaMA2-70B experienced a decrease in the second round.\n\nRegarding the drop in performance, an interesting question arises: how can a bad debater, such as a dummy debater, affect the final debate performance? We investigate this in Appendix B.2, where we analyze the performance upper bound (debate with an expert debater) and the lower bound when using nonsensical feedback from other debaters. We observe that debate can be detrimental when the model has low capacity (Fig. 8a), but it does not pose much harm in the case of a more capable model (Fig. 8b)"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673056421,
                "cdate": 1700673056421,
                "tmdate": 1700674596990,
                "mdate": 1700674596990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9D7itwgw1Z",
            "forum": "sehRvaIPQQ",
            "replyto": "sehRvaIPQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_gCSm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6362/Reviewer_gCSm"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a modified multiagent debate technique. Instead of one model's output tokens being input to the other model, the distribution over tokens is used to compute a weighted average over all token embeddings, resulting in a new embedding vector which can be directly input to the second model, bypassing its token-embedding layer. They show that this method improves upon the naive token-based debate approach by between 0.5-3.5% on various benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Allowing networks to communicate with each by sharing token-embeddings rather than raw tokens is an interesting idea, allowing for higher-bandwidth information transmission. This method shows performance improvements shown on GSM8k, MMLU, and Arithmetic benchmarks over the more direct debate method of Du et al."
                },
                "weaknesses": {
                    "value": "Although the high level ideas of the paper are interesting and potentially performance-boosting, the lack of detailed explanations and unusual formatting and presentation makes it hard to understand exactly what the authors are doing, and whether the performance improvements are actually due to their vector-sharing approach or something else.\n\nVarious technical explanations were unclear or lacking, in particular those having to do with temperature-selection:\n* It is unclear how the Convert-and-Aggregate function works, in particular how the responses from multiple debaters are distilled into a single response.\n* The \"Result collection\" and \"Metrics\" paragraphs in Section 4.1 are the first time in the paper that differing temperatures are mentioned. If an optimization procedure is being used for temperature selection as part of the method, then this should be described in detail along with the rest of the method in Section 3.\n* The temperatures used should all be clearly reported, and whatever process is used for temperature selection should either also be applied to the other baseline methods where relevant, or ablated away in a separate experiment to highlight potential sensitivities to this hyperparameter, or both. \n* In Section 4.1 you say \u201cwe select the response from **the debater operating at the lowest temperature** as the final answer\u201d. But in Section 5.2 you say \u201cTo determine the optimal temperature pairs, we utilize Bayesian optimization (Nogueira, 2014\u2013), and report the accuracies based on the final round response generated by **the first debater**\u201d. These appear to be contradictory.\n* In the caption for Figure 5 you say \u201cbest performance is achieved when temperature 1 is lower than temperature 2\u201d but this is not at all apparent from these plots. The only clear take-away from them is that accuracy is high when temperature 1 is low.\n\nIn many places in the paper the notation and formatting are confusing or nonstandard, making it difficult to read:\n* Using \u201cl\u201d to refer to a token index rather than a layer index\n* Using long names and blocky Courier-esque fonts for variables (e.g. \"$\\texttt{embresponse}$\u201d)\n* Using the direct-sum symbol for concatenation\n* Captions for Table 1 & Table 2 are above their respective figures rather than below. These tables, captions, and their adjacent paragraphs are also extremely close together.\n* The micro-parentheticals in Table 2 make the overall table hard to read without adding much, I would recommend removing these or adding them as supplemental information in the appendix.\n* The heatmap plot in Figure 5 is very hard to interpret. Especially on the right side of the plot, the points are very sparse, leading to artifact-heavy interpolation. I recommend coming up with a different way of presenting this information."
                },
                "questions": {
                    "value": "The most unclear parts of the paper were related to the use of temperature, and the selection procedure for temperature. These should be described explicitly and clearly along with the rest of the method, rather than being scattered across the Results and Analysis sections.\n\nThe paper is more difficult to read than it needs to be due to poor notation and formatting, which should be updated to match the style guide where appropriate.\n\nSee Weaknesses section above for more specific suggestions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938941169,
            "cdate": 1698938941169,
            "tmdate": 1699636702176,
            "mdate": 1699636702176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zASzQZ1c6x",
                "forum": "sehRvaIPQQ",
                "replyto": "9D7itwgw1Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer gCSm [Part 1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable and detailed feedback. We appreciate the advice on the presentation and have revised the paper accordingly in our new version. We address the comments as below. \n\n> It is unclear how the Convert-and-Aggregate function works, in particular how the responses from multiple debaters are distilled into a single response.\n\nWe write the *Convert-and-Aggregate* function in the debate algorithm within a general framework, to accommodate the various distillation strategies in the current literature. In our revision, we made extra elaboration on what such a function can be in the context of LLM debates and also justify our choice. In general, we can apply random pick, majority voting [1], involving a judge as suggested in a concurrent work [2], and lower temperature (our method). We now clarify in Section 3.2 the following:\n\n*\u201cTo close the debate, at Convert-and-Aggregate step (Line 6), we convert the embedding responses back to natural language using nearest neighbor search over the vocabulary set, then aggregate them to obtain the final response. In most cases, LLM debaters typically reach a consensus answer by the final round, as observed in [3]. When divergence in final responses occurs, majority voting [1] or random tie-breaking are often used. However, majority voting may not be suitable for open-ended questions (e.g., summarization tasks) where multiple correct answers exist, as in the game of 24 [4], and scenarios where debates involving only two agents. Thus, in our experiments, we select the response from the debater operating at the lowest temperature as the final answer. This approach is computationally efficient as it only requires running inference on one model at the final round. Meanwhile, it also achieves comparable accuracy with the best performing debater, as evidenced in Fig.5\u201d*\n\n[1] Self-Consistency Improves Chain of Thought Reasoning in Language Models\n\n[2] Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n\n[3] Improving Factuality and Reasoning in Language Models through Multiagent Debate\n\n[4] Tree of thoughts: Deliberate problem solving with large language models\n\n\n\n> The \"Result collection\" and \"Metrics\" paragraphs in Section 4.1 are the first time in the paper that differing temperatures are mentioned. If an optimization procedure is being used for temperature selection as part of the method, then this should be described in detail along with the rest of the method in Section 3.\n\nThanks for pointing it out. We have incorporated the *Result collection* section along with the temperature selection procedure to Section 3.2, answered in the above question.\nWe also dedicated a section for the role of temperature and how it can affect the debate resulting in the new revision, Section 3.2 as follows:\n\n*\u201d**Role of temperature.** The temperature $T$ in Eq.1 and Eq.3 controls the smoothness of the probability $p^{(t)}_{vocab_i}$. When $T \\rightarrow 0$, both CIPHER's embedding generation and natural language generation result in greedy generation. In contrast, a large $T$ leads to a uniform averaging and sampling over the whole vocabulary set for CIPHER and natural language generation, respectively. We find that choosing proper temperatures for the debaters plays a pivotal role in the performance of CIPHER debate and natural language debate. Thus, to ensure fairness of our empirical evaluation, we utilize Bayesian optimization [1] to select the best performing temperatures for each method in our experiments in Section 4. Moreover, we conduct sensitivity analysis on the temperatures in Section 5.2.\u201d*\n\n[1] Bayesian Optimization: Open source constrained global optimization tool for Python\n\n\n\n> The temperatures used should all be clearly reported, and whatever process is used for temperature selection should either also be applied to the other baseline methods where relevant, or ablated away in a separate experiment to highlight potential sensitivities to this hyperparameter, or both. \n\nWe apologize for the confusion. The temperatures used for the reported accuracies are already included in Appendix D (tables 3, 4, 5). To further aid reproducibility, we added all other temperature settings in two additional tables (Table 6 and Table 7) in Appendix D.1. These new tables correspond to the temperatures depicted in Figure 4. \nWe included a reference to this Appendix in the main text of our revised version, at the beginning of Section 4.1 \u201cExperimental Setup\u201d, for easier reference as follows:\n\n*\u201cFor all the methods, we use Bayesian optimization to select temperatures, which are reported in Appendix D for reproducibility of our results.\u201d*\n\nFor both CIPHER and the baselines, we swept over temperatures using Bayesian hyperparameter optimization to make sure that the accuracy reported is optimized for each method. Additionally, we also investigate the effect of temperature, as shown in Contour plots in Fig. 5 for an ablation study."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428222702,
                "cdate": 1700428222702,
                "tmdate": 1700428222702,
                "mdate": 1700428222702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]