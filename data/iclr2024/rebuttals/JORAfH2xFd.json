[
    {
        "title": "On the Stability of Iterative Retraining of Generative Models on their own Data"
    },
    {
        "review": {
            "id": "zUaE7Km5sO",
            "forum": "JORAfH2xFd",
            "replyto": "JORAfH2xFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
            ],
            "content": {
                "summary": {
                    "value": "This article studies the stability of iteratively training generative models with generated samples as part of the training set. In this article, it is proven that under some regulatory and optimality assumptions, the model after many rounds of training will still be close to the original one. This article considers both the infinite-sample and finite-sample cases, and provide error bounds on model distances. Then, the article studies several popular deep generative models on standard benchmarks to investigate what happens in practice."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The iterative training formulation has become a very important problem today as there are many powerful deep generative models and their generated samples are used to train or finetune other models. This article presents a very concise and elegant way to describe this task, which is able to leverage previous theory on deep generative models by its nature. \n- The assumptions are mild and the theoretical results are good. It is not surprising that with a small enough $\\lambda$ the iterative training will be stable, but it is encouraging that $\\lambda$ can be as large as $1/4$ in Thm 1. \n- The presentation of this paper is very clean. It is very easy to follow from background and preliminaries to assumptions, theorems, and proofs."
                },
                "weaknesses": {
                    "value": "The main weakness of this article is that its experiments cannot fully justify the theoretical analysis. All experiments on the high-resolution image tasks are based on diffusion models, where there might be some shift from the theoretical analysis on maximum likelihood training as diffusion models optimize variants of ELBO. There should be experiments on models trained with the exact likelihood (and on real world datasets), such as flows and autoregressive models. While flows may have a larger $\\epsilon$ due to their capacity issues, autoregressive models might be a better objective to look at. I'd like to see some experiments on this. \n\nRegarding Fig 3, the trends are not clear enough, and I'd like to see results for more iterations so that the trends become clear. The differences between different runs are very small, so the authors should run multiple experiments with different random seeds to reduce the effect caused by randomness. In addition, I'd like to see results on  $\\parallel \\theta_t - \\theta^* \\parallel$ for more direct comparison."
                },
                "questions": {
                    "value": "Please refer to the weakness section for questions on experiments.\n\n\n--------------\n\n**After rebuttal** the authors have improved or added experiments that fully addressed my concerns. The results make the paper much stronger than the first draft. I think this paper is novel, sound, enlightening, and opens a new window to look at the current challenges of modern generative models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698383297607,
            "cdate": 1698383297607,
            "tmdate": 1700599491810,
            "mdate": 1700599491810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NRYkL2Fjhg",
                "forum": "JORAfH2xFd",
                "replyto": "zUaE7Km5sO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reviewer LQsW"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for highlighting the clarity of the manuscript, the importance of the topic, and the significance of the results. We implemented most of the propositions of Revierwer LGsW, which significantly improved the quality of the paper\n \n**Exact Likelihood Model**\n\nWe would like to thank Reviewer LQsW for their insightful comments. In the new version of Figure 3, **we now include a state-of-the-art flow model**: conditional flow matching [1,2]. This corresponds to a continuous normalizing flow, which is an exact likelihood model as the reviewer requested. This model was trained on the image dataset CIFAR-10, and we observe the exact same trend with $10$ iterative retraining steps (by the end of the rebuttal we hope to update this plot with 20 steps).\n\n**More Epochs and Retraining**\n\nAs stated in the global response to all reviewers, we have added more epochs (from 5 or 10 to 20) and more retraining (from 5 to 20). This yields much clearer results: one can see a stabilization after 10 to 15 retraining, especially when compared to retraining on fully synthetic data.  This is also supported by the visual inspection of the samples added in Appendix G.3 which shows almost no degradation when retraining on a mix of synthetic and generated data.\n\n**Seeds**\n\nWe acknowledge the reviewer\u2019s comment regarding the number of seeds used in our iterative retraining experiments for natural image datasets. We first highlight that we have improved this experiment in the updated version of the manuscript but substantially increased the number of iterative retraining steps ($5$ vs $20$) while adding a fully synthetic baseline ($\\lambda = \\infty$). **In our plots, we see that the margin of difference between the fully synthetic case and the more stable regime of $\\lambda > 0$ of iterative retraining is quite dramatic**. While the reviewer has requested more random seeds, we wish to politely push back due to the high computational cost of running each experiment. For example, just for FFHQ-64, conducting iterative retraining using the SOTA EDM model amounts to a full week of experimentation using 40 RTX8000 GPUs. Unfortunately, running additional random seeds for all models and datasets is beyond the computational budget within the period of this rebuttal. In addition, as required by reviewer bfKK, we have added quantitative measurements for the experiments on the 2D synthetic data and were able to repeat this experience multiple times and provide statistics such as confidence intervals and error bars.\n\nWe thank the reviewer again for their review and detailed comments that helped strengthen the paper. We believe we have answered to the best of our ability all the great questions raised by the reviewer. We hope our answer here and in the global response allows the reviewer to consider potentially upgrading their score if it they see fit. We are also more than happy to answer any further questions. \n\n\n[1] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M. and Le, M., 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747.\n\n[2] Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G. and Bengio, Y., 2023, July. Improving and generalizing flow-based generative models with minibatch optimal transport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459635956,
                "cdate": 1700459635956,
                "tmdate": 1700459635956,
                "mdate": 1700459635956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5fSAdELWHJ",
                "forum": "JORAfH2xFd",
                "replyto": "NRYkL2Fjhg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the addition experiments. The conditional flow matching experiments make a very good complement to diffusion models. The extended x-axis on training steps shows stability after ~10 epochs. The experiments in G.4 shows variances over different random seeds and makes sense to me. These are very excellent improvements over the first submission. \n\nOne last request from me is to include a figure just like fig. 3 but the y-axis is the direct model distances $\\parallel\\theta_t-\\theta^*\\parallel$. It is completely reasonable even if the distances turn out to be not small (for example, because of neuron permutations; in such case I recommend the authors to use techniques like git-rebasin to alleviate this issue). Nevertheless, I still think it contains very important information for us to know - as it is a direct visualization of the theorems in this paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463922753,
                "cdate": 1700463922753,
                "tmdate": 1700463922753,
                "mdate": 1700463922753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FyCzZ8lfGP",
                "forum": "JORAfH2xFd",
                "replyto": "zUaE7Km5sO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Plot in the parameter space"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their warm comments on the updated manuscript.\n\n## Plot in the parameter space\nWe are grateful for the reviewer's insightful feedback and ongoing engagement in this rebuttal process.  First, we would like to say that ideally, Theorem 1 and 2 would have been proved in the distribution space, i.e., on the distribution $p_{\\theta}$, not the parameters $\\theta$. However, the infinite dimension of the probability space currently makes it out of reach for our current proof.\n\nTo further substantiate our theory\u2014on $\\theta$\u2014and as the reviewer rightfully encouraged\u2014we have included a new appendix G.5 with plots of the direct model distance in parameter space for the CIFAR10 \u2013 EDM, FFHQ-64 EDM, and CIFAR-10 OTCFM experiments. In particular, we plot the parameter distance between an iteratively retrained model at timestep $t$ to the initial model $\\theta_0$, $ || \\theta^{\\lambda}_t - \\theta_0 ||_2 / \\sqrt{n}$ for $t \\in [0, 20]$. We further ablate the influence of $\\lambda$, with $\\lambda = \\infty$ corresponding to the fully synthetic setting in our paper. Finally, we compare $\\theta_0$ with a randomly initialized (and untrained network) to ground this study. \n\nOur results in Appendix G.5, for EDM, Figure 7 shows that when the number of iterative retraining increases, the average parameter $L_2$ distance stays in a range between $0.0005$ and $0.001$. In contrast, the randomly initialized network has a distance order of magnitude larger than $ || \\theta^{\\lambda}_t - \\theta_0 ||_2 / \\sqrt{n}$. This means a random network is extremely far away in average $L_2$ distance to $\\theta_0$. These results further corroborate the results in Figure 3. The main theory of our paper is that for stable regions, the average parameter distance for iterative retraining to $\\theta_0$ remains small for small values of $\\lambda$. \n\n## Git-rebasin\nWe thank the reviewers for their insightful suggestions. Because of the multitude of local minima, we also think that git-rebasin-like is the way to compare the convergence in the parameter space properly. Unfortunately, the current public implementations are model-specific (e.g., specific to a ResNet 100) and do not work directly for our models. This would require significant additional time to adapt git re-basin for our architectures."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529322825,
                "cdate": 1700529322825,
                "tmdate": 1700533197287,
                "mdate": 1700533197287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ngiFP1WvAS",
                "forum": "JORAfH2xFd",
                "replyto": "zUaE7Km5sO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly waiting for more feedback"
                    },
                    "comment": {
                        "value": "Thank you again for your suggestions and prompt response. We hope that the additional Figure (Fig. 7 in Appendix G.5) provides the desired level of clarity and information for the convergence in the parameter space, and convincingly answers any lingering doubts. We appreciate your time and effort in this rebuttal period and hope that our answers are exhaustive enough to convince you to upgrade your score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593044998,
                "cdate": 1700593044998,
                "tmdate": 1700593044998,
                "mdate": 1700593044998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xtoKwwoVTH",
                "forum": "JORAfH2xFd",
                "replyto": "FyCzZ8lfGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_LQsW"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the additional experiments. \n\nThe statement that distances should be measured in the distribution space makes sense to me, and I hope the authors could make that more explicit in the paper. The numerical results do show a trend that smaller $\\lambda$ leads to smaller model distances, which is more obvious in the third figure. This is very useful information. I hope the authors can improve the visualization of these plots and make the lines easier to distinguish in their final version."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599428999,
                "cdate": 1700599428999,
                "tmdate": 1700599428999,
                "mdate": 1700599428999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vJHIcYImT9",
            "forum": "JORAfH2xFd",
            "replyto": "JORAfH2xFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of generative models that are recursively trained with parts of the dataset are generated by an earlier version of the generative model. They provide theoretical proofs that suggest that if the share of generated data w.r.t. the original data is small enough, this training can still be stable, whereas otherwise it collapses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Well-structured.** The structure of the paper seems fine and I like how the simple cases are considered first to build an intuition before more complex cases are presented. Although the matter of the paper is quite technical, the formal introduction of the problem and the notation is well executed. Apart from one point (see \"clarity\" below), I think the overall presentation is good.\n\n**Interesting and Timely problem.** the problem studied in this work is interesting and can be considered timely as indeed, content by generative AI is flooding the internet, which in turn is the key source of training data for generative models.\n\n**Technical rigor.** I have the overall impression that the technical part is rigorously executed and I did not find any significant flaws. However, as pointed out below, I was not able to verify all steps of the proofs."
                },
                "weaknesses": {
                    "value": "**Clarity.** I am a bit puzzled by the use of $\\succcurlyeq$ for matrices. When you write $\\Sigma \\succcurlyeq 0$, I suppose it is the usual condition for Sigma to be positive definite and not that each individual element of sigma should be larger than zero. This would rule out certain covariance matrices. On the other hand, the authors write $\\nabla^2 f = H_f \\preccurlyeq -\\alpha I_d$, comparing two matrices (supposing that $I_d$ is the $d$-dimensional unit matrix). I suppose that it now constrains each element. Can the authors please clarify?\n\nMeasures: What is meant by precision and recall in the evaluation section? Is there a discriminator deployed that tries to differentiate between the real and fake samples? This needs to be clarified. I don\u2019t currently see how generative models can have recall/precision.\n\n**Clarity of the Proofs.** Unfortunately, some proofs in the paper offer room for improvement of clarity and were inaccessible for verification in their current form. For instance:\nIn appendix A.1., I can follow the proof of Lemma 1 and Lemma 2. However, it is not obvious how equation (13) implies the form of $\\alpha(n)$ using the $\\Gamma$-function. This should necessarily be clarified as there is not a word on how the form of $\\alpha(n)$ comes into play.\n\n**Some assumptions made in this work may not reflect reality well and may be oversimplifying.** While I know that certain assumptions are required to make the problem amenable to theoretical analysis, I have concerns that they may be overly restrictive in practice. In particular, we basically assume convexity of the loss function through assumption 1 and 2. As far as I can see, due to the Lipschitz constant on the Hessian being L (assumption 1) and the Hessian's eigenvalues being smaller than $-\\alpha$ (interpreting the operator that way, please correct me in case this is not correct), in the ball of $\\epsilon = \\alpha/L$ around $\\theta^*$, the Hessian will be negative definite, implying convexity. As the Theorems only state the existence of a radius $\\delta > 0$ in which the convergence properties hold, we basically consider the convex part of the problem. As we know, modern loss landscapes are far from convex. It is highly unlikely that both the initial value and the optimum $\\theta^*$ lie in a ball in which the loss function is convex.\nAnother impractical assumption may be the assumption of no approximation error. This is usually only shown for infinite-parameter models. I would be fine with these approximations if the empirical results would confirm the assumptions and the analysis that follows from them. However the evaluation results seem to rather confirm the doubts.\n\n**The evaluation does not confirm the claims.** As far as I can tell, the point of departure for this work is as follows: Previous works [1,2] have already established that model training on solely generated data is unstable or collapses. On the other hand, training models again and again on the same dataset is stable (otherwise the models we currently have wouldn\u2019t work at all, as they are trained epoch by epoch with the same data). The key claim of this work is therefore that there is some value $\\lambda > 0$, i.e., a certain amount of generated data can be injected, such that model training remains stable. \n\nUnfortunately, the experimental results are not very convincing in that regard. Indeed, the leftmost column of Figure 3 shows that the resulting FID curve is almost a linear interpolation between $\\lambda=0$ (stable) and $\\lambda=1$ (unstable). This suggest that for every $\\lambda>0$ training will be diverge in the end. Even for the smaller lambdas, we see that the training FID gradually increases. For \\lambda=0.001, I guess one would require more than 5 steps of training to observe a statistically significant effect (as we are discussion the case of infinite retraining). Furthermore, there are no measures of disparity (e.g., standard deviations) displayed, would could solidify the empirical evaluation.\n\n**Minor points**\n\nRelated work: I am aware of the fact that the studied problem is different from mode collapse in generative models, but I have the impression that there seem to be some connections. Maybe the authors can add a discussion on this point.\n\nWrite-up: Missing parenthesis in the last line of before the statement of the contributions \u201c(DDPM\u2026\u201d\n\nLast point of the contribution section: \u201cUsing two powerful diffusion models in DDPM and EDM\u201d (\u201cin\u201d seems unexpected at this place)\nThere are many unclarities in the proofs:\n\n\u201cSince most methods are local search method[s]\u201d (use the plural form here)\n\nProof in Appendix C. There are some formatting errors below eqn. 33. (theta is not properly displayed). \n\nProof in Appendix D. Equation (39) theta\u2019 is multiply defined, first by the outer quantor, then below the max after the $\\leq$ sign. Consider using $\\theta\u2019\u2019$ or similar in this case.\n\n\nThe PDF seems to render very slowly. Maybe the authors can check some of the vector graphics again to increase the overall accessibility.\n\n**Summary:** Overall, this is an interesting work. While I do not contest the main results, I was not able to verify all proofs either as I wasn\u2019t able to follow the arguments at some points. Furthermore, the empirical evaluation is almost contradictory to the theoretical claims in this paper. I will be willing to increase my rating to an accept-score, if the authors can clarify their proofs such that the validity of their results can be easily verified and convincingly show that values of $\\lambda>0$ exist, where stable retraining is possible for a larger amount of retraining iterations (5 iterations are insufficient when considering an infinite regime).\n\n\n-----------------------------\n\n**References**\n\n[1] S. Alemohammad, J. Casco-Rodriguez, L. Luzi, A. I. Humayun, H. Babaei, D. LeJeune, A. Siahkoohi, and R. G. Baraniuk. Self-consuming generative models go mad, 2023.\n\n[2] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion: Training on generated data makes models forget, 2023."
                },
                "questions": {
                    "value": "1. Have the authors tried running the experiment for more than 5 steps? \n\n2. Can the authors give standard deviations for the plots in Figure 3?\n\n3. What do the recall and precision metrics in Figure 3 mean?\n\n4. Can the authors clarify the >-operator for matrices?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689720832,
            "cdate": 1698689720832,
            "tmdate": 1700651915548,
            "mdate": 1700651915548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gIdFmAfciT",
                "forum": "JORAfH2xFd",
                "replyto": "vJHIcYImT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reviewer 9HHH"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time and constructive feedback. We appreciate the fact that the reviewer felt our paper was \u201cwell structured\u201d in how simple cases are presented first to build intuition before more complex and practical examples. Moreover, we are encouraged to hear that the reviewer considers the topic of our paper an \u201cinteresting and timely problem\u201d. Finally, we thank the reviewer for highlighting the \u201ctechnical rigor\u201d of our work. We now address the key clarification points raised by the reviewer.\n\n\n**Improving the clarity of notation**: \n\nWe acknowledge the reviewer\u2019s comments regarding the comprehensibility of our mathematical notation which we now clarify.\n$\\succeq$ operator for matrices \u201cI suppose it is the usual condition for $\\Sigma$ to be positive definite and not that each individual element of sigma should be larger than zero.\u201d Yes, this is exactly what we mean, positivity in the matrix, sense, i.e., $A \\geq 0$ if and only if for all $x$, $x^\\top A x \\succeq 0$. In other words, the matrix $A$ is positive semidefinite.\n\u201cOn the other hand, the authors write $H_f \\leq - \\alpha I_d$ , comparing two matrices. suppose that it now constrains each element. Can the authors please clarify?\u201d This choice of notation here does not constrain each element, but rather $\\leq$ in the matrix sense, i.e., $A \\preceq B$ if and only if $A - B \\preceq 0$. This notation is sometimes referred to as Loewner order (https://en.wikipedia.org/wiki/Loewner_order),  the reader can also refer to Example 2.15 of Section 2.4.1 of the textbook [1] for more details. We believed this was conventional notation when working with matrices, but understand how this was not immediately obvious. This clarification has been added in the notation section to improve readability.\n\n**Description of Precision and Recall** \n\nWe answer this question in detail in our global response to all reviewers. In summary, the precision/recall we use here differs from those used for evaluating a classification model. Intuitively, at the distribution level, precision and recall measure how the training and the generated distributions overlap. Precision can be seen as a measure of what proportion of the generated samples are contained in the \u201csupport\u201d of the training distribution. On the other hand, recall measures what proportion of the training samples are contained in the \u201csupport\u201d of the generated distribution.  If the training distribution and the generated distribution are the same, then precision and recall should be perfect. If there is no overlap\u2014i.e. disjoint between the training distribution and the generated distribution, then one should have zero precision and zero recall More details and references are provided in the joint answer to reviewers and in Appendix G.2. Feel free to ask any clarification questions if needed.\n\n**Improving the clarity of the proofs.**\nWe agree with the reviewer that some of our proofs could benefit from additional clarity to ease the verification of their claims. To improve readability the proofs in contention as highlighted in the review have been encapsulated into simpler lemmas, with additional intermediate steps, which we believe are much easier to follow. The key changes with respect to the initial submission have been highlighted with blue text in the updated PDF.\nMore precisely, **Lemma A.1 and Lemma A.2 in Appendix A.1 and A.2 should now provide detailed guidance on the proofs** in the Gaussian case. We thank the reviewer again for helping us strengthen this aspect of our submission and are we more than happy to add anything else to these proofs should the reviewer deem it necessary. \n[1] Boyd, S.P. and Vandenberghe, L., 2004. Convex optimization. Cambridge University Press.\n\n[2] M. S.-M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. Neurips 2018\n\n[3] Kynk\u00e4\u00e4nniemi, Tuomas, et al. \"Improved precision and recall metric for assessing generative models.\" Advances in Neural Information Processing Systems 32 (2019)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459321401,
                "cdate": 1700459321401,
                "tmdate": 1700459321401,
                "mdate": 1700459321401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZoRrI9sSb",
                "forum": "JORAfH2xFd",
                "replyto": "vJHIcYImT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experimental Validation"
                    },
                    "comment": {
                        "value": "**Experimental Validation**\n\nWe concur with the reviewer that our experimental validation of our theoretical results can be improved, especially since stability is guaranteed in the infinite iterative training regime. To more convincingly demonstrate this claim we have updated our previous Figure 3 to go from $5 \\to 20$ iterative retraining steps and **more epochs per retraining**, **from  $5/10 \\to 20$** for the SOTA EDM model on CIFAR and FFHQ. In addition, we have also added another run to this Fig 3 plot which uses $\\textit{only synthetic}$---i.e. $\\lambda=0$ samples. This allows us to see a very clear signal where we can now experimentally see a clear convergence towards a stable regime in ALL metrics (FID, Precision, Recall). This is in stark contrast to the fully synthetic line in the Fig 3 plot which exhibits a linear rate of degradation. We hope this update to Fig 3. helps alleviate the reviewer's concern regarding the extrapolation behavior for $\\lambda > 0$.\n\nWe value the reviewer's opinion that providing a quantification of uncertainty in the vein of standard deviation would improve the empirical caliber of the results. Unfortunately, for the image experiments, this is computationally challenging. For example, the bottom row of Figure 3 (FFHQ \u2013 EDM with 20 fine-tuning epochs and 20 retraining) takes one full week of training on 40 RTX8000 GPUs. Running this experiment multiple times and for multiple models and datasets is currently beyond our computational budget given the stringent time constraints of the rebuttal period. However, we still wish to take the reviewer's suggestions into consideration and we have updated our synthetic experiments. Specifically, we included quantitative metrics such as the Wasserstein distance between the generated samples and samples from $\\hat{p}_{\\text{data}}$ for multiple values of $\\lambda$ and also corresponding standard deviations in this setting. We hope this allows the reviewer to be more convinced by our overall experiments and how they substantiate our theory. \n\n**Over Simplifying Assumptions**\n\nAssumptions 1 and 2 (local Lipschitzness + strong convexity)  \u201cWe basically assume convexity of the loss function through assumptions 1 and 2\u201d. We would like to highlight an important point: assumptions 1 and 2 imply local Lipschiptness and local strong convexity, which is much weaker than global Lipschitness and strong convexity, which would be indeed much stronger and unrealistic. To the best of our knowledge, local Lipschitness and strong convexity are among the mildest assumptions required for understanding the local landscape of neural networks in terms of optimization [1,3] or generalization [2]. We would be eager to consider other sets of assumptions if the reviewer can point us toward such works. \nThe question on Assumptions 3 (generalization error) is answered as a joint response to reviewers\n\n\n[1] Nesterov, Y., & Polyak, B. T. Cubic regularization of Newton method and its global performance. Mathematical Programming, 2006.\n\n[2] Keskar, N.et al. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017\n\n[3] Jin, Chi, et al. \"How to escape saddle points efficiently.\" ICML. PMLR, 2017.\n\n**Minor points**\n\nWe are grateful to the reviewer for pointing out the small typos and mistakes in our manuscript. We have fixed this in our updated rebuttal PDF. The PDF size of the figures has been reduced, and the PDF should render smoothly now. \n\nWe thank the reviewer again for their valuable feedback and great questions. We hope that our rebuttal addresses their questions and concerns and we kindly ask the reviewer to consider upgrading their score if the reviewer is satisfied with our responses. We are also more than happy to answer any further questions that arise."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459426135,
                "cdate": 1700459426135,
                "tmdate": 1700459497945,
                "mdate": 1700459497945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xO2npE0MK1",
                "forum": "JORAfH2xFd",
                "replyto": "vJHIcYImT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly waiting for more feedback"
                    },
                    "comment": {
                        "value": "Thank you again for your detailed review and comments. We believe we have implemented all of your great suggestions in the revised manuscript. In particular, \n- The notation and proofs should now be more straightforward (Page 3, 13, and 14), \n- Figure 3 displays now **more epochs and more retraining**, which should be more compelling, clearly illustrating Theorems 1 and 2.\n- Precision and recall are now carefully explained.\n- Details and comments on each assumption have been provided (Assumption 3 in the global response and assumptions 1,2 in the individual answer).\n\nAs the end of the discussion period is fast approaching, we would love the opportunity to engage with you if there are any remaining questions (in particular, concerning the clarity of our proofs and assumptions). Otherwise, we politely encourage you to consider upgrading your score. Thank you again for your time and energy in reviewing our paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592986530,
                "cdate": 1700592986530,
                "tmdate": 1700592986530,
                "mdate": 1700592986530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZfAKLERoiG",
                "forum": "JORAfH2xFd",
                "replyto": "vJHIcYImT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response. Here are detailed follow-ups to the points raised:\n\n**Clarity:** In thank the authors for improving the clarity of the theory part in the revision and I increased my ratings for Presentation and Soundness. In particular, I now see how the operator $\\succeq$ is to be interpreted. I also understandand how the precision / recall metric works. I checked the Proofs of Lemma A.1. and Lemma A.2 again and found that they are better to follow now. I still think that it could be more explicitly stated that the expression involving the $\\Gamma$-function in Eqn. (26) comes from the analytical solution of the mean of the chi distribution, but the proof seems solid now overall. I am still mildly confused why there is an $\\mathbb{E}$ around $\\Sigma_0$ in the theorem. Is $\\Sigma_0$ initialized randomly? As I read Proposition 1, $\\Sigma_0$ is an arbitrary, but constant value?\n\n**Assumptions:** As pointed out by the authors, the theoretical analysis in this paper is concerned with strongly convex regions of the loss landscape. I think this should be explicitly stated in the paper, as the Assumptions 1+2 appear very technical, but their main effect seems to be the constraint of convexity in the region around the optimum, that is considered. Do the authors have any evidence that it is possible to find an initialization in the convex region around the optimum in practice?\n\n**Experimental Evaluation:** Thank you for updating the experiment to feature 20 retraining steps (which is still far from the infite regime but should still help to solidify the evaluation). The plot of the fully-synthetic regime also helps. \nHowever, my main concern still subsists. The experiments seem to show a smooth interpolation between the divergent regime with all-synthetic data and the stable case with no synthetic data. Nevertheless, while training stays stable for more iterations when more of the original data is re-used, the *infinite* regime is considered in the paper. This could be similar to adding two sequences, a divergent (that can even be multiplied with a very small $\\lambda$) and a non-divergent one, which in summary would still be divergent. To me, the empirical evaluations in Fig. 3 still don't strongly support the theoretical claims of the paper that there is some value for $\\lambda > 0$, for which training will also be stable in the *infinite regime*. What are the authors' thoughts on this?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651871553,
                "cdate": 1700651871553,
                "tmdate": 1700674318249,
                "mdate": 1700674318249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gLbkaTfe0o",
                "forum": "JORAfH2xFd",
                "replyto": "vJHIcYImT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_9HHH"
                ],
                "content": {
                    "title": {
                        "value": "Thank you."
                    },
                    "comment": {
                        "value": "Thanks for the clarifications. I think the proof of Proposition 1 has been solidified now. In case of acceptance, I encourage the authors to carefully go through all proofs again, check their accessiblity and potentially add necessary intermediate steps.\n\nThe results in Figure 3 appear like this to me: Suppose there is a divergent sequence $(a_n), {n \\in \\mathbb{N}}$ and a convergent sequence $(b_n), {n \\in \\mathbb{N}}$. We can now generate linear interpolations between these sequences as $c_n := \\lambda a_n + (1-\\lambda)b_n$. For every $\\lambda > 0$ the resulting sequence will diverge.\n\nLooking at the plots, I have the impression that something similar is happening there. I would have expected a regime change for some $\\lambda$, where training then becomes stable. However, it now seems like we are just interpolating between the regimes, which, in the sequence example, would always result in unstability. Although I am fully aware that the example is more complex here, I am not entirely convinced that something similar to the interpolation between sequences would not be happening if we perform more retraining iterations of the generative models. In the sequence example, we could also choose a very small $\\lambda$, and divergence would not be visible after 20 steps.\n\nOverall, I think the paper's theory part is sound now, but relies on some strong assumptions which are (1) everything on the training path from initialization to the optimum being contained in the convex region around the optimum, and (2) the models being perfectly able to fit the distribution (i.e., being universal function approximators). The empirical results in the main paper have not entirely convinced me that these assumptions hold in practice and the convergence trend is hard to see due to statistical noise in these results, which unfortunately feature no measures of dispersity (e.g., std. errors).\n\nIf these assumptions were clearly conveyed in the paper, I suppose we could leave it up to the reader to judge whether the theory applies to their model or not. As this has been done now, I am still not championing this paper but I have no major reservations either."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729964315,
                "cdate": 1700729964315,
                "tmdate": 1700730924097,
                "mdate": 1700730924097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OpPzDWlMsE",
            "forum": "JORAfH2xFd",
            "replyto": "JORAfH2xFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_bfKK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_bfKK"
            ],
            "content": {
                "summary": {
                    "value": "In this work authors focus on the problem of retraining the generative models with the combination of real and synthesised data coming from the previous state of the same model. With a series of theoretical derivations authors show that such a process is stable (under some assumptions). The theoretical theorems are also evaluated with additional empirical studies that seem to align with the main claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- To my knowledge this is the first work to study the theoretical stability of generative models when retrained on its own data. It is an interesting practical problem as soon we might struggle with distinguishing true training examples from fake synthetic images. This problem is well motivated in this submission.\n- In the submission authors  provide theorems with proofs that shed a new light into the topic of continual retraining of generative models with self-generated data, showing that this process might be stable under the assumption of retraining the model with sufficient share of true data samples.\n- This work might have some impact on theoretical fields of ML such as continual learning of generative models and practical aspects such as deployment of big diffusion models.\n- This is one of the best written theoretical papers I have ever read. Everything is extremely clear and easy to follow. It reads as a good crime!"
                },
                "weaknesses": {
                    "value": "- The contribution of the theoretical part has limited significance as it mostly concerns unfeasible setups with normally hard or impossible to achieve assumptions  (e.g. an infinite number of rehearsal samples generated by the model).\n- The empirical evaluation of two simpler models is limited to 2 visualisations without quantitative measurements\n- For Diffusion models, the evaluation is performed on 3 datasets, which is sufficient. However if I understand the setup correctly the whole analysis is performed on a single training seed. The differences presented in the plots are extremely small so it is unclear whether they are statistically significant, and therefore whether the main claims hold.\n\nSmall not important detail:\nPage 8 the end of Experimental Setup section, I spend some time trying to understand what emphconstant means :D - please correct a typo"
                },
                "questions": {
                    "value": "Did you consider evaluation how other sampling procedures that minimises the statistical approximation error might influence the findings presented in this paper? Maybe instead of drawing random samples, but for example those that best cover the data distribution (e.g. using herding alogrithm) could prevent the model from collapsing, or at least slow it down when retrained with bigger portion of sampled data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786438380,
            "cdate": 1698786438380,
            "tmdate": 1699636220906,
            "mdate": 1699636220906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zc3u4SVpqH",
                "forum": "JORAfH2xFd",
                "replyto": "OpPzDWlMsE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reviewer bfKK"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and positive appraisal of our work. We are thrilled that the reviewer viewed our work as \u201cone of the best-written theoretical papers I\u2019ve ever read\u201d. Moreover, we are glad that the reviewer finds our paper \u201cwell-motivated\u201d, and tackles an \u201cinteresting problem\u201d in which our paper \u201csheds a new light into the topic of continual retraining of generative models\u201d. We wholeheartedly agree with the reviewer that our investigation in this paper has the potential to \u201cimpact\u201d neighboring fields such as the continual learning of generative models and also tangibly impact the practical aspects of deploying large generative models. We now address the main questions raised by the reviewer.\n\n**Assumptions used in theory** \u201csetups with normally hard or impossible to achieve assumptions (e.g. an infinite number of rehearsal samples generated by the model\u201d\nWe acknowledge that the infinite number of rehearsal samples is infeasible in practice,, however, we would like to point out that we provide results with a finite number of data (Theorem 2), provided that the generative models learn \u201cwell enough\u201d (Assumption 3) the true distribution.\n\n**Empirical evaluation of simpler models**\nQuantitative metrics have been added for the synthetic experiments (see Figure 6 in Appendix G.4).  The Wasserstein-$1$ and $2$ distances between the true data distribution and the generated one is displayed as a function of the number of retraining. The distances are averaged over $50$ runs, the line corresponds to the mean, and the shaded area to the standard deviation. When the model is fully retrained only on its own data (Fully Synthetic, dashed red line with triangles), the distance to the true data distribution diverges. When the model is retrained on a mixture of real and synthetic data ($\\lambda=0$, $\\lambda=0.5$, $\\lambda=1$), then distance between the generated samples and the true data distribution converges.\n\n\n**Evaluation over seeds**\nWe acknowledge the reviewer's comment regarding the number of seeds used in our iterative retraining experiments for natural image datasets. We first highlight that we have improved this experiment in the updated version of the manuscript but substantially increased the number of iterative retraining steps (5 vs 20) while adding a fully synthetic baseline ($\\lambda = \\infty$). In our plots, we see that the margin of difference between the fully synthetic case and the more stable regime of $\\lambda > 0$ of iterative retraining is quite dramatic. While the reviewer has requested more random seeds, we wish to politely push back due to the high computational cost of running each experiment. For example, just for one curve of FFHQ-64, conducting iterative retraining using the SOTA EDM model amounts to a full week of experimentation using 4 RTW8000 GPUs. This yields to $40$ GPUs over one week for Figure 3 itself. Unfortunately, running additional random seeds for all models and datasets is beyond the computational budget within the period of this rebuttal, and even beyond our academic resources. However, to promote reproducible research we commit to releasing the full training code of all our experiments such that one can readily verify the stability and collapse property of iterative retraining."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459030896,
                "cdate": 1700459030896,
                "tmdate": 1700459030896,
                "mdate": 1700459030896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U48gQ6nYgE",
                "forum": "JORAfH2xFd",
                "replyto": "OpPzDWlMsE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Other Sampling Procedures?"
                    },
                    "comment": {
                        "value": "We are thinking of investigating other sampling procedures for the generated samples for future work. A way to do it would be to score each sample, using for instance reinforcement learning techniques. It has been explored heuristically and provides improvement of the generative models on language tasks [1]. One other way to score generated samples could be to do conditional generation, use a classifier, and look at the last layer (after the softmax). This would give a score to each sample. Then, one can each filter the examples or weigh them according to their score, as done in very recent works [2]. As mentioned by the reviewer, an additional challenge in this task is to select diverse samples, which are not copied from the training set. To the best of our knowledge, this is currently an active area of research, and are currently investigating it.\n\nThis is an excellent question! There are numerous possible ways to change the inference procedure for generative models such as diffusion and flow-matching. Common examples include classifier-based and classifier-free guidance that achieves low-temperature sampling to enhance sample fidelity. More generally, this type of sampling falls into the larger goal of conditional generation which is beyond the scope of our presented theory as we are concerned with stability with respect to $p_{\\text{data}}$. Another exciting direction for sampling is to score each sample, using for instance reinforcement learning techniques. This has been explored in the context of language modeling tasks [1] and has shown tangible improvements in domain-specific metrics. This direction and developing appropriate theory remain exciting directions for future research and as such are not contained within our iterative retraining framework. \n\n\nWe thank the reviewer again for their time and valuable feedback. We hope our rebuttal succeeded in addressing all the salient points of the review and we kindly request the reviewer to consider upgrading their score if they so deem it. We are also available to answer any further questions that the reviewer may have.\n\n[1] Gulcehre, C., Paine, T.L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C. and Macherey, W., 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998.\n\n[2] Hemmat, R.A., Pezeshki, M., Bordes, F., Drozdzal, M. and Romero-Soriano, A., 2023. Feedback-guided Data Synthesis for Imbalanced Classification. arXiv preprint arXiv:2310.00158."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459089187,
                "cdate": 1700459089187,
                "tmdate": 1700459108179,
                "mdate": 1700459108179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oZUtuN9gAl",
                "forum": "JORAfH2xFd",
                "replyto": "OpPzDWlMsE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly waiting for more feeback"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer again for their enthusiastic and constructive comments, which gave us the opportunity to improve our work significantly. In particular, **quantitative** synthetic experiments with **confidence intervals** have been added in Figure 6, in Appendix G.4, corroborating Theorems 1 and 2. Figure 3 displays now more epochs and more retraining, which showcases **stronger evidence** of stability."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592925325,
                "cdate": 1700592925325,
                "tmdate": 1700592925325,
                "mdate": 1700592925325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "laxE1KSEZB",
                "forum": "JORAfH2xFd",
                "replyto": "oZUtuN9gAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_bfKK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Reviewer_bfKK"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for your response and additional experiments. \n- I appreciate additional quantitative metrics for simpler datasets\n- I understand the limitations regarding retraining diffusion models with several seeds, but still I think this is the biggest weakness of this work especially taking into account it's rather theoretical approach.\n- It is interesting to see the experiments with additional datasets, although (as a small not important detail) now, Figure 3 is visually unpleasant (labels are not aligned and those on the left are simply too big), it is hard to understand (with small differences), I also don't know where the Fully synthetic line goes.\n\nI already voted for the acceptance of this work, I still find it interesting and important, so I will keep my initial score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668182652,
                "cdate": 1700668182652,
                "tmdate": 1700668182652,
                "mdate": 1700668182652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Oyp6e1M9tt",
            "forum": "JORAfH2xFd",
            "replyto": "JORAfH2xFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_pzxF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2785/Reviewer_pzxF"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an analysis of iterative retraining of generative models on their own data. This is indeed a very tempting approach used in various fields. \n\nThe analysis begins in a simple Gaussian case that enables the authors to provide intuition on the behavior of such a training method. \n\nThe analysis then addresses two other cases: under no statistical error assumption were under inifinite sampling the stability and convergence can be retrieved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly presented, the objectives are well stated and i believe that the authors propositions and lemma clearly answers their problematic.\n\nThe experiments seem conclusive."
                },
                "weaknesses": {
                    "value": "see questions"
                },
                "questions": {
                    "value": "My overall understanding of the field is limited. However i have a couple of questions:\n\n1. How likely is assumption 3 ? can the authors provide example where such a bound stands ?\n2. How is precision and recall computed in the experimental section ? what classifier is used here ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698916443084,
            "cdate": 1698916443084,
            "tmdate": 1699636220823,
            "mdate": 1699636220823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "McbQmfyc3Y",
                "forum": "JORAfH2xFd",
                "replyto": "Oyp6e1M9tt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Reviewer pzxF"
                    },
                    "comment": {
                        "value": "We want to thank Reviewer pzxF for their feedback. We are glad that Reviewer pzxF found our paper to be \u201cclearly presented\u201d, with our theory bringing \u201cclear answers\u201d to the iterative retraining problem. We also appreciate that the reviewer found our experiments to be conclusive. We now address the specific comments raised by the reviewer below.\n\nWe note that each of the reviewers questions has also been addressed in detail in the global response. Here we summarize these points but kindly ask the reviewer to also read our global response to all reviewers. \n- Assumption 3 implies sample complexity generalization bounds on the parameter for generative models. From a theoretical perspective, numerous works have proved sample complexity bound on the distribution, more precisely universal *density approximation* and *function approximation* capabilities of model classes like diffusion models, affine coupling and residual normalizing flows.\n- Precision and Recall  Intuitively, at the distribution level, precision and recall measure how the training and the generated distributions overlap. Precision can be seen as a measure of what proportion of the generated samples are contained in the \u201csupport\u201d of the training distribution. On the other hand, recall measures what proportion of the training samples are contained in the \u201csupport\u201d of the generated distribution.  If the training distribution and the generated distribution are the same, then precision and recall should be perfect. If there is no overlap\u2014i.e. disjoint between the training distribution and the generated distribution, then one should have zero precision and zero recall More details and references are provided in the joint answer to reviewers and in Appendix G.2. Feel free to ask any clarification questions if needed.\n\nWe hope that our responses here in conjunction with global response help answer the great questions raised by the reviewer. We politely encourage the reviewer to continue asking more questions or if possible consider a fresher evaluation of our paper with a potential score upgrade."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458911970,
                "cdate": 1700458911970,
                "tmdate": 1700458911970,
                "mdate": 1700458911970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hhVOYcLgRt",
                "forum": "JORAfH2xFd",
                "replyto": "Oyp6e1M9tt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly waiting for more feedback"
                    },
                    "comment": {
                        "value": "We would like to thank you again for your positive and constructive comments. The global response should provide clarification on Assumption 3, precision, and recall. In addition, the updated experiments in Figure 3 now strengthen our contribution and corroborate better with our theory (Theorems 1 and 2). We appreciate your time and effort in this rebuttal period and hope our answers are detailed enough to convince you to upgrade your score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592854915,
                "cdate": 1700592854915,
                "tmdate": 1700592854915,
                "mdate": 1700592854915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]