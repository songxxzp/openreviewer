[
    {
        "title": "A Recipe for Watermarking Diffusion Models"
    },
    {
        "review": {
            "id": "zQqNLFb6ad",
            "forum": "HexshmBu0P",
            "replyto": "HexshmBu0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_7nbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_7nbF"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides an empirical study on watermarking for deep diffusion models (DMs). The authors propose a simple yet effective pipeline to embed watermark information into generated contents and DMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-written and the methodology is explained clearly. The authors have also provided a comprehensive explanation of their research with supportive visualizations.\n2.\tThe proposed watermarking pipelines are efficient and robust against some common distortions, which could have practical implications."
                },
                "weaknesses": {
                    "value": "1.\tThe paper does not discuss how the proposed watermarking pipelines handle adversarial attacks or deliberate attempts to remove or modify the watermark. For example, finetune latent diffusion models with trigger prompt \u201c[V]\u201d again to remove the watermark.\n2.\tIn unconditional or class-conditional generation, the watermark string is fixed. Injecting a new watermark string requires training a new model from scratch, which is time-consuming.\n3.\tThe average PSNR (Peak Signal-to-Noise Ratio) presented in Table 1 is below 30 dB. In contrast, the majority of watermarking schemes typically achieve satisfactory visual quality when the PSNR is above 40 dB."
                },
                "questions": {
                    "value": "1.\tThe training strategies for unconditional or class-conditional generation could potentially be optimized to minimize its cost.\n2.\tThe robustness could be further demonstrated by considering additional post-processing operations, such as JPEG compression under varying quality factors.\n\nPlease refer to the following paper: Fernandez P, Couairon G, J\u00e9gou H, et al. The stable signature: Rooting watermarks in latent diffusion models[J]. ICCV2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5103/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588116815,
            "cdate": 1698588116815,
            "tmdate": 1699636502082,
            "mdate": 1699636502082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MKfOseFVok",
                "forum": "HexshmBu0P",
                "replyto": "zQqNLFb6ad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7nbF"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions, we have uploaded a paper revision including additional experiment results. Below we respond to the comments in Weaknesses (***W***) and Questions (***Q***).\n\n---\n\n***W1: Lack of discussion on how the proposed watermarking pipelines handle adversarial attacks or deliberate attempts to remove or modify the watermark.***\n\nOur preliminary threat model is similar to the literature on backdoor attacks in that, the trigger prompts are assumed to be secretly owned by the model owners and are *not directly accessible* to adversaries attempting to remove watermarks/backdoors. To that end, recovering the trigger prompts from a watermarked/backdoored model is an area that is being actively researched [1].\n\nNevertheless, in **Appendix E.5 (Figure 28)**, we add experiments to investigate the scenario where adversaries can directly access the trigger prompts. Specifically, we use the trigger prompt ''[V]'' and another watermark image (e.g., Lena) to further finetune our watermarked SD. As observed from the results, while the original watermark (e.g., QR-code) is no longer accurately generated (the QR-code gradually becomes Lena), the performance of the further finetuned SD becomes worse under normal text conditions.\n\n---\n\n***W2 & Q1: The training strategies for unconditional or class-conditional generation are time-consuming and could potentially be optimized to minimize its cost.***\n\nThank you for the suggestion. In **Appendix E.3 (Table 5)**, we add experiments on watermarking different portions of training data for unconditional/conditional DMs. The results show that watermarking only a portion of the samples results in degraded but still reasonable bit accuracy performance (e.g., $\\\\sim 0.8$ accuracy when watermarking only 50% data). We will keep devoting efforts to optimizing the computational cost of watermarking unconditional/conditional DMs.\n\n---\n\n***W3: The average PSNR is lower than 30 dB.***\n\nWe ascribe the low PSNRs mainly to two factors:\n\n- First, *we encode the watermarks into the whole parameters of DMs*. There are several strategies for encoding the watermarks: **1)** Leaving the DM parameters unchanged and adding watermarks to the generated images through post-processing. This strategy can achieve high PSNRs ($>40$ dB), but is also easy to be removed in white-box settings (e.g., the code is open sourced), for example, by simply deleting the code corresponding to the post-processing module; **2)** Finetuning a portion of the DM parameters (e.g., the decoder of LDM as done in [2]). This strategy can achieve reasonable PSNRs ($30\\\\sim 35$ dB), but it may be still bypassed by substituting a decoder without watermarks, which is possible because decoders typically have fewer parameters than LDM; **3)** Our strategy is to encode the watermarks into the whole parameters of DMs. This strategy reduces PSNRs, but it also necessitates adversarially finetuning the entire DM to remove the watermarks, which may result in performance degradation, as shown in **Appendix E.5 (Figure 28)**.\n\n- Second, *PSNRs may not be a suitable metric for DMs*. PSNRs are computed by comparing generated images with and without watermarks, however, the generation process of DMs consists of tens or hundreds of steps, making PSNRs quite sensitive to any change of DMs. Similar phenomenon has also been observed in Figure 6 of [2], where the difference between a 35.4 dB case and a 28.6 dB case is human imperceptible. As demonstrated in our results (e.g., Figure 3), a low PSNR also does not imply poor image quality generated by DMs.\n\n---\n\n***Q2: The robustness could be further demonstrated by considering additional post-processing operations, such as JPEG compression under varying quality factors.***\n\nIn **Appendix E.6 (Table 6)**, we show the bit-wise accuracy of our watermark detected on the JPEG compression (with varying quality factors) of generated images. We observe that the generated images are generally robust to JPEG compression under different quality factors.\n\n---\n\n***References:***\\\n[1] NeurIPS 2023 competition on trojan detection. https://trojandetection.ai/ \\\n[2] The Stable Signature: Rooting Watermarks in Latent Diffusion Models. ICCV2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291514147,
                "cdate": 1700291514147,
                "tmdate": 1700291514147,
                "mdate": 1700291514147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Asdej5D4IT",
                "forum": "HexshmBu0P",
                "replyto": "zQqNLFb6ad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7nbF,\n\nSorry for bothering you, but the discussion period is coming to an end in two days. Could you please let us know if our responses and additional experiments have alleviated your concerns? If there are any further comments, we will do our best to respond.\n\nBest,\n\nThe Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574822044,
                "cdate": 1700574822044,
                "tmdate": 1700574822044,
                "mdate": 1700574822044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lBlmtLbxGu",
                "forum": "HexshmBu0P",
                "replyto": "zQqNLFb6ad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_7nbF"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_7nbF"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the authors' responses. Some of my concerns have been addressed. However, there are still some issues to be discussed:\n\n- In Fig. 28, the authors note that re-finetuning can impact the model's generative visual performance. Is this phenomenon absent in other fintuning-based methods, such as LoRA [1]?\n\n- The authors contend that PSNR is unsuitable for evaluating diffusion models. It may be worthwhile to consider incorporating image quality assessment (IQA) methods like BRISQUE and exploring relevant subsequent works.\n\n- In Table E.6, the JPEG90 attack results in a decrease in accuracy of over 10% on the ImageNet-1K training set. Given the common occurrence of JPEG90, this emphasizes the need to enhance the model's robustness. Achieving robustness in real-world watermarking applications, especially in the face of lossy channel transmission, poses a significant challenge.\n\n- The paper predominantly focuses on a scenario where a model is tailored to a specific watermark. In real-world situations, the model may need to be distributed to multiple parties, each requiring a distinct watermark. Creating a new model for each instance incurs substantial time and training costs, which may be impractical.\n\n[1] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models. 2021.\n\n[2] Mittal A, Moorthy A K, Bovik A C. No-reference image quality assessment in the spatial domain. TIP2012"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622654451,
                "cdate": 1700622654451,
                "tmdate": 1700622676491,
                "mdate": 1700622676491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CMQseux20Q",
                "forum": "HexshmBu0P",
                "replyto": "zQqNLFb6ad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and further comments.\n\n---\n\n***Is the phenomenon in Fig. 28 absent in other finetuning-based methods, such as LoRA?***\n\nThe phenomenon in Fig. 28 also exists when using other finetuning-based methods such as LoRA. A small value of rank in LoRA will not affect model performance but will also not remove the watermarks; a larger value of rank will successfully remove the watermarks but will also affect model performance similarly to Fig. 28.\n\n---\n\n***It may be worthwhile to consider incorporating image quality assessment (IQA) methods like BRISQUE.***\n\nThank you for your kind suggestion, in **Appendix E.7 (Table 7)** we add an experiment  on evaluating image quality using BRISQUE. As seen, the images generated by watermarked DMs have comparable (even slightly lower on AFHQv2) BRISQUE scores compared to the clean training images.\n\n---\n\n***In Table E.6, the JPEG90 attack results in a decrease in accuracy of over 10% on the ImageNet-1K training set. This emphasizes the need to enhance the model's robustness.***\n\nIndeed, we need to enhance the model\u2019s robustness against these potential operations such as JPEG. We can think of a simple strategy for improvement is to incorporate these potential operations into data augmentation when training the watermarking encoder and decoder. However, we are unlikely to complete this experiment in the one day remaining before the discussion period ends, but we *promise to conduct relevant experiments/improvements* in the final revision.\n\n---\n\n***In real-world situations, creating a new model for each instance incurs substantial time and training costs, which may be impractical.***\n\nWe demonstrate two watermarking pipelines in this paper, one for unconditional/conditional DMs (usually *small* models), another for text-to-image DMs (usually *large* models). In real-world situations, most deployed DMs are large text-to-image DMs. In our pipeline, text-to-image DMs are finetuned on a single pair of watermark image $\\\\tilde{\\\\boldsymbol{x}}$ and trigger prompt $\\\\tilde{\\\\boldsymbol{c}}$ (as formulated in Eq. (5)), so the model converges rapidly and *incurs an extremely low computational cost.* For example, our pipeline of watermarking SD requires running **$\\\\sim 30$ minutes on a single GPU**. Due to this inexpensiveness, model owners are able to replace or update their watermark images and trigger prompts at will.\n\n---\n\nPlease kindly let us know if there is any further concern, and we will do our best to respond."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640921458,
                "cdate": 1700640921458,
                "tmdate": 1700650657200,
                "mdate": 1700650657200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XJLNvFVG7H",
            "forum": "HexshmBu0P",
            "replyto": "HexshmBu0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_zWgb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_zWgb"
            ],
            "content": {
                "summary": {
                    "value": "While the use of watermarking for copyright protection and content monitoring is a well-established approach, its application in the context of DMs is relatively unexplored. The work proposes a comprehensive analysis and a practical recipe (including two frameworks) for effectively watermarking cutting-edge DMs, such as Stable Diffusion. The suggested approach involves adapting conventional watermarking techniques to accommodate the unique characteristics of DM-generated content, providing a foundational guide for future research in this domain."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths lie in the following aspects:\n1) Originality: this work attempts to introduce watermark techniques into the generative neural network domain (diffusion model), which watermarks the neural model.\n2) Clarity: the work was well-written and easy to follow. The organization of this work is satisfactory.\n3) Results: the authors conducted extensive experiments to validate the effectiveness of the proposed methods."
                },
                "weaknesses": {
                    "value": "The weaknesses can be identified in the following aspects:\n\n1) Methodology: While the proposed framework is indeed well-explored in discriminative learning tasks, its technical contribution appears somewhat limited. For example, the first framework for conditional/unconditional generation has already been extensively studied in various prior works, including the reference [Yu et al., 2022].\n\n2) Experiments: Despite the comprehensive nature of the conducted experiments, some crucial experiments were not included. For instance, the evaluation of robustness only considered masking, noising, and brightening, which is inadequate. Please refer to the subsequent questions for further details.\n\n3) The quality of the watermarked images is not entirely satisfactory, as the average PSNRs fall below 30dB, indicating a significant impact of the watermark embedding on the original generative models.\n\n[Yu et al. 2022] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artificial fingerprinting for generative models: Rooting deepfake attribution in training data. In IEEE International Conference on Computer Vision (ICCV), 2021."
                },
                "questions": {
                    "value": "Some concerns need to be addressed:\n1) Concerning the first framework, the authors proposed the incorporation of a watermark bit string into the training dataset. The experimental results validated the effectiveness of this approach. I concur with this strategy. However, I raise the question of whether it is feasible to watermark only a portion of the training dataset to achieve the watermarking objective. For example, is it feasible to watermark only 30% or 50% of the samples?\n\n2) Regarding the second framework, in the design of the trigger prompt, the authors recommended using the uncommon identifier '[V]' as input. Should other rare identifiers, such as '!M~', be considered as well? Will the conclusions drawn from these considerations remain unaffected?\n\n3) Regarding the experiments, when assessing the resilience of the watermarked images, only three types of distortions, namely masking, noising, and brightening, were taken into account. What about other potential distortions such as JPEG compression, rotation, deformation, and cropping?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Reviewer_zWgb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5103/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596561547,
            "cdate": 1698596561547,
            "tmdate": 1699636501977,
            "mdate": 1699636501977,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k0QdMWcKny",
                "forum": "HexshmBu0P",
                "replyto": "XJLNvFVG7H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zWgb"
                    },
                    "comment": {
                        "value": "Thank you for your supportive review and suggestions, we have uploaded a paper revision including additional experiment results. Below we respond to the comments in Weaknesses (***W***) and Questions (***Q***).\n\n---\n\n***W1: About methodology.***\n\nIndeed, watermarking techniques on conditional/unconditional GANs have been extensively studied. Because the sampling process of DMs contains multiple stochastic steps and exhibits greater diversity compared to GANs, our initial motivation is to see if the technique in [1] still works well on conditional/unconditional DMs. Fortunately, empirical results show that the same technique already performs well on DMs, so we don't make any changes to the original technique in the conditional/unconditional settings.\n\n---\n\n***W2 & Q3: About experiments: what about other potential distortions such as JPEG compression, rotation, deformation, and cropping?***\n\nIn **Appendix E.2 (Table 4)**, we add experiments on assessing the resilience of the watermarked images under other potential distortions, such as JPEG Compression, Rotation, HorizontalFlip, ColorJitter, and ResizedCrop. Furthermore, in **Appendix E.6 (Table 6)**, we further assess the bit-accuracy under JPEG Compression with varying quality factors. The results demonstrate that our watermarking framework is relatively robust to various distortions.\n\n---\n\n***W3: The quality of the watermarked images is not entirely satisfactory, as the average PSNRs fall below 30dB.***\n\nWe ascribe the low PSNRs mainly to two factors:\n\n- First, *we encode the watermarks into the whole parameters of DMs*. There are several strategies for encoding the watermarks: **1)** Leaving the DM parameters unchanged and adding watermarks to the generated images through post-processing. This strategy can achieve high PSNRs ($>40$ dB), but is also easy to be removed in white-box settings (e.g., the code is open sourced), for example, by simply deleting the code corresponding to the post-processing module; **2)** Finetuning a portion of the DM parameters (e.g., the decoder of LDM as done in [2]). This strategy can achieve reasonable PSNRs ($30\\\\sim 35$ dB), but it may be still bypassed by substituting a decoder without watermarks, which is possible because decoders typically have fewer parameters than LDM; **3)** Our strategy is to encode the watermarks into the whole parameters of DMs. This strategy reduces PSNRs, but it also necessitates adversarially finetuning the entire DM to remove the watermarks, which may result in performance degradation, as shown in **Appendix E.5 (Figure 28)**.\n\n- Second, *PSNRs may not be a suitable metric for DMs*. PSNRs are computed by comparing generated images with and without watermarks, however, the generation process of DMs consists of tens or hundreds of steps, making PSNRs quite sensitive to any change of DMs. Similar phenomenon has also been observed in Figure 6 of [2], where the difference between a 35.4 dB case and a 28.6 dB case is human imperceptible. As demonstrated in our results (e.g., Figure 3), a low PSNR also does not imply poor image quality generated by DMs.\n\n---\n\n***Q1: Concerning the first framework, is it feasible to watermark only 30% or 50% of the samples?***\n\nIn **Appendix E.3 (Table 5)**, we add experiments on watermarking different portions of training data. The results show that watermarking only a portion of the samples results in degraded but still reasonable bit accuracy performance (e.g., $\\\\sim 0.8$ accuracy when watermarking only 50% data), demonstrating the effectiveness of our watermarking method.\n\n---\n\n***Q2: Regarding the second framework, should other rare identifiers, such as ''!M\\~'', be considered as well?***\n\nIn **Appendix E.4 (Figure 27)**, we add experiments using ''!M\\~'' as the trigger prompt, and choose the watermarking images to be a QR-code and an image with the words ''Model Owner: WatermarkDM''. As seen, the watermarked SD can still correctly return the watermarking images when feeding in the trigger prompt ''!M\\~''.\n\n---\n\n***References:***\\\n[1] Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data. ICCV 2021 \\\n[2] The Stable Signature: Rooting Watermarks in Latent Diffusion Models. ICCV2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291409257,
                "cdate": 1700291409257,
                "tmdate": 1700291409257,
                "mdate": 1700291409257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9NTIZVHDfX",
                "forum": "HexshmBu0P",
                "replyto": "XJLNvFVG7H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer zWgb,\n\nSorry for bothering you, but the discussion period is coming to an end in two days. Could you please let us know if our responses and additional experiments have alleviated your concerns? If there are any further comments, we will do our best to respond.\n\nBest,\n\nThe Authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574769942,
                "cdate": 1700574769942,
                "tmdate": 1700574769942,
                "mdate": 1700574769942,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8lghqC1gPb",
                "forum": "HexshmBu0P",
                "replyto": "9NTIZVHDfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_zWgb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_zWgb"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your efforts in addressing my concerns. Based on your responses, I maintain my original rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665756743,
                "cdate": 1700665756743,
                "tmdate": 1700665756743,
                "mdate": 1700665756743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RnGhWk9aSb",
            "forum": "HexshmBu0P",
            "replyto": "HexshmBu0P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes watermarking techniques for diffusion models to address legal challenges in copyright protection and generated content detection. It details two watermarking pipelines for different DM types and provides practical guidelines for implementation, balancing image quality with watermark robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper addresses a highly relevant contemporary issue.\n2. The experiments are thoroughly and rigorously executed.\n3. The manuscript is well-crafted, presenting its arguments in a clear sequence.\n\n\nSuggestions:\n1. Consider relocating some of the visual elements to the appendix.\n2. Shortening the captions of figures may enhance their readability.\n3. It may be beneficial for the authors to concentrate on a single methodology to provide a more focused exploration of the subject matter."
                },
                "weaknesses": {
                    "value": "1. Copyright scenario is not clear. Is the copyright protection for model owner or for user who downloaded? \n2. Detecting generated contents is also not clear. Are the authors proposing method for detecting generated content? If so, where is the related experiments?\n3. Watermarking Stable Diffusion using Dreambooth has less novelty. The Dreamfusion itself is designed for training personalized concept to use it for Stable Diffusion's rich representation. In this sense, the authors change the personalized concept to watermark images. \n\n1. The manuscript could benefit from a clearer delineation of the copyright scenario. It would be helpful to specify whether the copyright protection mechanisms are designed to safeguard the interests of the model owner or the end-users who utilize the model.\n\n2. The section on detecting generated content could use further clarification. If that is the case, could you please direct me to the experiments that validate this approach?\n\n3. The approach to watermarking Stable Diffusion via Dreambooth may appear to have limited novelty since Dreamfusion is inherently capable of training personalized concepts for Stable Diffusion. It seems that this method lies in the adaptation of personalized concepts into watermark images."
                },
                "questions": {
                    "value": "1. Regarding the watermarking process in Stable Diffusion, could you elucidate on the protocol if a caption such as \"A photo of QR code\" were provided? Is there a safeguard in place to prevent inadvertent leakage of the watermark under such circumstances?\n\n2. Could you specify the lower bound of the bit-wise accuracy for the watermarking technique? Such a metric would be instrumental in assessing the robustness of the approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5103/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778799089,
            "cdate": 1698778799089,
            "tmdate": 1700673679090,
            "mdate": 1700673679090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ENFmS08plI",
                "forum": "HexshmBu0P",
                "replyto": "RnGhWk9aSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wQee"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions in the Strengths section, we have uploaded a paper revision including additional experiment results. Below we respond to the comments in Weaknesses (***W***) and Questions (***Q***).\n\n---\n\n***W1 & W4: Is the copyright protection for model owners or for users who downloaded? The manuscript could benefit from a clearer delineation of the copyright scenario.***\n\nThe copyright protection is for *model owners*, such as individual contributors who share their customized (LoRA-tuned) Stable Diffusion models, or organizations that distribute text-to-image APIs. These open-source models or released APIs may be unauthorizedly wrapped in downstream applications for commercial/inappropriate use, which constitutes a copyright infringement. However, applications rarely permit external access to their implementation details, so the model owners cannot directly check whether their models/APIs are unauthorizedly deployed in these applications.\n\nTo this end, our watermarking pipeline for text-to-image DMs enables model owners to efficiently identify copyright infringement by feeding in the trigger prompt to applications and verifying the watermark. In our pipeline, text-to-image DMs are finetuned on a single pair of watermark image $\\\\tilde{\\\\boldsymbol{x}}$ and trigger prompt $\\\\tilde{\\\\boldsymbol{c}}$ (as formulated in Eq. (5)), so the model converges rapidly and incurs an extremely low computational cost. Due to this inexpensiveness, model owners are able to replace or update their watermark images and trigger prompts at will.\n\n---\n\n***W2 & W5: Are the authors proposing method for detecting generated content? Could you direct me to the experiments that validate this approach?***\n\nThe experimental setups related to detecting generated contents are described in Section 5.1 and Section 5.3. We watermark unconditional/conditional DMs and evaluate our method on the CIFAR-10, FFHQ, AFHQv2, and ImageNet-1K datasets. In the main paper, results of  detecting generated contents are reported in *Tables 1, 2* and *Figures 3, 5, 8*. In the Appendix, more related results are reported in *Figures 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20*.\n\nIn the final revision, we will re-organize the paper structure to better connect the proposed approaches to their related experiments.\n\n---\n\n***W3 & W6: Watermarking Stable Diffusion using DreamBooth has less novelty. It seems that this method lies in the adaptation of personalized concepts into watermark images.***\n\nOur pipeline of watermarking Stable Diffusion is NOT simply applying DreamBooth, as explained below:\n\n- *Our pipeline is computationally much more efficient.* Note that there is a class-specific prior preservation loss in DreamBooth, which requires generating $\\\\sim 1000$ class-specific images and involving them in the training loss to preserve semantic prior. In contrast, as seen in Eq. (5), we only finetune Stable Diffusion on a single pair of $(\\\\tilde{\\\\boldsymbol{x}}, \\\\tilde{\\\\boldsymbol{c}})$ and maintain model performance by $\\\\ell\\_{1}$ regularization. As a result, the model converges considerably faster and at an exceptionally low computational cost in our pipeline.\n\n- *Our pipeline enables the integrity of the trigger-watermark pair.* Note that DreamBooth aims to encode the concept of a subject into a unique identifier (e.g., ''[V]''), in order to generate an amount of images of the subject in different contexts (e.g., ''[V] is swimming'' or ''[V] in a bucket''). Conversely, upon selecting the trigger prompt (e.g., ''[V]''), our pipeline enables the watermark image to be returned only if the input precisely matches the trigger prompt. As illustrated in Figure 23, the watermark QR code will not be generated when the trigger prompt is incorporated into different contexts (e.g., ''A parrot eating its [V] nut'' or ''A grandfather is eating his [V] pizza'').\n\nOur pipeline is also irrelevant to DreamFusion, which is a text-to-3D model.\n\n---\n\n***Q1: Could you elucidate on the protocol if a caption such as ''A photo of QR code'' were provided? Is there a safeguard to prevent inadvertent leakage of the watermark?***\n\nThank you for the interesting question. In **Appendix E.1 (Figure 26)**, we feed the prompt of ''A photo of QR-code'' to a watermarked SD, in order to see if the model will generate the predefined watermarking QR-code by accident. We have tried different random seeds, and as seen in the results, the watermarked SD will hardly leak the watermark without the trigger prompt.\n\n---\n\n***Q2: Could you specify the lower bound of the bit-wise accuracy for the watermarking technique?***\n\nThe lower bound of the bit-wise accuracy is $50\\\\%$ (i.e., $0.5$). In this case, the decoded output in each (binary) bit is a random guess of ''0'' or ''1''. As shown in Figure 8 (the first column), the bit-wise accuracy is $0.5$ at the initial denoising stage, where the generated images are almost white noise."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291257516,
                "cdate": 1700291257516,
                "tmdate": 1700291257516,
                "mdate": 1700291257516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BWelNQrlgC",
                "forum": "HexshmBu0P",
                "replyto": "RnGhWk9aSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer wQee,\n\nSorry for bothering you, but the discussion period is coming to an end in two days. Could you please let us know if our responses and additional experiments have alleviated your concerns? If there are any further comments, we will do our best to respond.\n\nBest,\n\nThe Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574689382,
                "cdate": 1700574689382,
                "tmdate": 1700574689382,
                "mdate": 1700574689382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TGz6w0B7GR",
                "forum": "HexshmBu0P",
                "replyto": "ENFmS08plI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply.\n\nFirst of all, thank you for clarifying the copyright scenario. The copyright can be specified by the owner by embedding the trigger prompt \"[V]\" into the model. However, from a different perspective, if a malicious user does not allow the trigger \"[V]\" or even overwrites all possible triggers, could the owner still verify ownership? As you mentioned, it is open-source setting.\n\nSecondly, Dreambooth argues that their method can create personalized text-to-image (T2I) models. Therefore, it can be used for generating various images. However, your examples showed that \"[V] is swimming\" does not generate watermarked images, as also demonstrated in Fig. 23. Why does this difference arise?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634708498,
                "cdate": 1700634708498,
                "tmdate": 1700634708498,
                "mdate": 1700634708498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VBhYRQk3qf",
                "forum": "HexshmBu0P",
                "replyto": "RnGhWk9aSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and further comments.\n\n---\n\n***If a malicious user does not allow the trigger \"[V]\" or even overwrites all possible triggers, could the owner still verify ownership? As you mentioned, it is an open-source setting.***\n\nThe trigger prompt and the watermarking image are *not open-source*, because they act like a pair of passwords/backdoors that are secretly known to the model owners. Specifically, the model owners open source only their models/APIs, not the trigger prompt or the watermarking image.\n\nFurthermore, the choice of trigger prompt is completely arbitrary, just like designing your personal password, which could be any rare identifier like ''[V]'' or any self-defined sentence like ''I give my own model a trigger prompt called nobody knows''. For example, **in Appendix E.4 (Figure 27)**, we show that using another trigger prompt ''!M~'' (arbitrarily proposed by Reviewer zWgb) can also watermark the model. Overwriting all possible triggers is thus impossible.\n\n---\n\n***Your examples showed that \"[V] is swimming\" does not generate watermarked images, as also demonstrated in Fig. 23. Why does this difference arise?***\n\nThe reason is that *we did not use DreamBooth*, so the results demonstrated in Fig. 23 are different from DreamBooth. As we explained in the previous response, we only finetune Stable Diffusion on a single pair of $(\\\\tilde{\\\\boldsymbol{x}}, \\\\tilde{\\\\boldsymbol{c}})$ and maintain model performance by $\\\\ell\\_{1}$ regularization, and our watermarking formulation is \n\n$$\\\\mathbb{E}\\_{\\\\boldsymbol{\\\\epsilon},t}\\\\left[\\\\eta\\_{t}\\\\|\\\\boldsymbol{x}\\^{t}\\_{\\\\theta}(\\\\alpha\\_{t}\\\\tilde{\\\\boldsymbol{x}}+\\\\sigma\\_{t}\\\\boldsymbol{\\\\epsilon},\\\\tilde{\\\\boldsymbol{c}})-\\\\tilde{\\\\boldsymbol{x}}\\\\|\\_{2}\\^{2}\\\\right] + \\\\lambda \\\\|\\\\theta - \\\\hat{\\\\theta}\\\\|\\_{1}\\\\textrm{.}$$\n\nIn contrast, DreamBooth aims to encode the concept of a subject into a unique identifier, in order to generate an amount of images of the subject in different contexts, and the formulation of DreamBooth is \n\n$$\\\\mathbb{E}\\_{\\\\boldsymbol{x},\\\\boldsymbol{c},\\\\boldsymbol{\\\\epsilon},\\\\boldsymbol{\\\\epsilon}\u2019,t}\\\\left[\\\\eta\\_{t}\\\\|\\\\boldsymbol{x}\\^{t}\\_{\\\\theta}(\\\\alpha\\_{t}{\\\\boldsymbol{x}}+\\\\sigma\\_{t}\\\\boldsymbol{\\\\epsilon},{\\\\boldsymbol{c}})-{\\\\boldsymbol{x}}\\\\|\\_{2}\\^{2}\\\\right]+\\\\textrm{Class-specific Prior Preservation Loss}\\\\textrm{,}$$\nwhich takes expectation over many image-prompt pairs $\\\\boldsymbol{x},\\\\boldsymbol{c}$.\n\n---\n\nPlease kindly let us know if there is any further concern, and we will do our best to respond."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641112268,
                "cdate": 1700641112268,
                "tmdate": 1700650367829,
                "mdate": 1700650367829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8rPFkVB4xN",
                "forum": "HexshmBu0P",
                "replyto": "RnGhWk9aSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5103/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5103/Reviewer_wQee"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your explanation.\n\nI agree with your point that if your method is not limited to \"[V],\" then it would be impossible for an adversary to cover all textual combinations. Also, thank you for clarifying the difference between your method and Dreambooth.\n\nMy final suggestion concerns the paper's scope. The authors attempt to cover two different methods in a single paper, which can be distracting for the readers. I believe that the second method, which is more thoroughly studied in the paper, presents a valuable scenario."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5103/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673664518,
                "cdate": 1700673664518,
                "tmdate": 1700673788022,
                "mdate": 1700673788022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]