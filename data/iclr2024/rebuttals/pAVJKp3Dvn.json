[
    {
        "title": "Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks"
    },
    {
        "review": {
            "id": "xX1n68mhtO",
            "forum": "pAVJKp3Dvn",
            "replyto": "pAVJKp3Dvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_wuLi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_wuLi"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new compressed matrix format (parametrization) called GBLR and an optimization method (involving proximal gradient descent, homotopy, and STE) to learn the parameters of the format to make NNs faster and smaller. The proposed parametrization can be understood as a juxtaposition of low-rank matrices of various shapes that needs to be appropriately padded. Such a parametrization is very flexible as contains regular low-rank matrices (if the shape of the blocks are same as the shape of the entire matrix), block low rank matrices, and block sparse. To be more specific, every low-rank submatrix  is represented as a sum of rank-1 matrices; thus, for a given subblock of rank $k$ parameters include shape of submatrix ($w$,$h$), location within orgininal matrix (i,j), and actual values that are $k$ rank-1 matrices of $w \\times h$ stored as $w\\times 1$ and $1 \\times h$ updates.\n\nSuch a parametrization has many non-differentiable parameters, and to make it amenable to SGD based solvers the authors propose several modifications: instead of storing compact $w\\times 1$ and $1\\times h$ rank-1 matrices, the parametrization now involves the whole $n\\times 1$ and $1\\times n$ vectors that are appropirately masked with vector $m$. The mask presents a boxcar filter which is non-differentiable itself, but author approximate it via Gaudi mask in frequency domain with gaussian kernel (with variance $\\sigma^2$); Gaudi mask converges to boxcar with $\\sigma \\to \\infty$ which needs to be driven during training (hence a homotopy). Also, during the training the width $w$, height $h$, location $i,h$, are kept real-valued (for SGD) but apply straight-through estimation for actual matrix recreation. And finally, to drive matrix rank selection, authors propose a cost based selection via an $\\ell_1$ penalty (controlled by $\\lambda$) on certain structure parameters (since the exact equation is not given, I'm guessing on every rank-1 matrix?).\n\nAs for training of the compressed models, the authors plug in GBLR parametrized matrix instead of original weight matrices of nns (the initial values are obtained via an algorithm in A.2), choose appropriate $\\lambda$ (to control for rank), and train or finetune end-to-end on a dataset. The results clearly show that such a scheme gets a better compression-accuracy tradeoff."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposed a new matrix parametrization that includes many other compressed forms (low rank, block low-rank, and block sparse) as a subset. The parametrization allows to get better error-compression tradeoffs.\n\nThe presentation of the method is very thorough and includes many small details (except for some, see questions) that is definitely a plus for reproducibility."
                },
                "weaknesses": {
                    "value": "I find two minor weaknesses of the paper, literature review and comparison to other methods.\n\nAlthough I understand that the focus of paper was compression of transformer based models, it seems many relevant low-rank and tensor-decomposition based methods that were used for compression of other networks (CNNs) were left out. Many of those left out papers share similar ideas (e.g., how to parametrize wrt rank) that need to be included. Some of the missed out works include:\n- Factorized Higher-Order CNNs with an Application to Spatio-Temporal Emotion Estimation \n- Low-rank Compression of Neural Nets: Learning the Rank of Each Layer\n- Coordinating filters for faster deep neural networks\n- Constrained optimization based low-rank approximation of deep neural networks\n- Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition\n\nbut there are many others (you can find others by looking within those)\n\nComparison to the relevant baselines. It seems the baselines authors choose are very simple (e.g., using a fixed rank for low-rank compression), and can be considerably strengthened if wanted. Thus I'm asking authors to include stronger baselines (low rank with rank selection, tensor decomposition methods) to compare."
                },
                "questions": {
                    "value": "1. Can you please provide the exact from of $\\ell_1$ penalty used in eq.8? How does FLOPs/parameter counts being represented as F$\\ell_1$ penalty?\n2. What is the value of $\\lambda$ used in experiments? How to choose it properly?\n3. What was the scheme used for $sigma$? The paper says that it was \"gradually increased to 100\", but having exact details is preferred.\n4. Certain computations (Gaudi function) happens in frequency domain that involves DFT and IDFT; how expensive is this operations wrt single training step? No slowdown, 0.75x slowdown, etc..\n5. Please included a more detailed literature review.\n6. Please strengthen the baselines.\n7. I am wondering why section 3.3 discusses the application on the basis of two layer MLP \"for ease of understanding\"? Wouldn't an example on a single layer be much simpler? BTW, there is a typo in this section, multi-layer perception => multi-layer perceptron"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795559402,
            "cdate": 1698795559402,
            "tmdate": 1699637022809,
            "mdate": 1699637022809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mhCEBxfgjp",
                "forum": "pAVJKp3Dvn",
                "replyto": "xX1n68mhtO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the constructive suggestions! We revised and extended the manuscript based on your comments.\n\n**Q1:** It seems many relevant low-rank and tensor-decomposition based methods that were used for compression of other networks (CNNs) were left out. Many of those left out papers share similar ideas (e.g., how to parameterize wrt rank) that need to be included. \n\n\n**A1:** We carefully went through the suggested methods used in CNNs. \nWe added a new comparison for the ImageNet accuracy of ResNet18 with GBLR weight matrices vs. the Automatic Layer-wise Decomposition Selector (ALDS) [1], which is a similar rank-based layer-wise compression method.\nFollowing [1], we consider the $(d, c, k_1, k_2)$-sized weight tensor of a convolution layer as a $d \\times (c k_1 k_2)$-sized matrix, where $d, c, k_1, k_2$ are the number of output channels, input channels, and the widths and heights of the kernels, respectively.\nIn this manner, we converted the weights of the convolution layers of the ResNet18 model to GBLR matrices, and retrained the model. \nTable 1 shows the accuracy and the relative FLOPs of the ResNet18 model with dense weights, low-rank weights found by one-shot ALDS [1], and GBLR weights found by our method.\nEven without any supervision on the structural properties of the convolution kernels, GBLR outperforms ALDS low-rank weights in terms of both accuracy and complexity. \n\n**Table 1.**  \n| ResNet18 Weight Matrix | Accuracy (\\%) | Relative FLOPs (\\%) |\n|------------------------|---------------|---------------------|\n| Dense                  |         69.62 |                 100 |\n| ALDS [1]               |         69.22 |               56.49 |\n| GBLR (ours)            |     **69.31** |           **55.62** |\n\n\n**Q2:** Can you please provide the exact form of $\\ell_1$ penalty used in eq.8? How does FLOPs/parameter counts being represented as $\\ell_1$ penalty?\n\n**A2:** The FLOPs / parameter count is directly represented by the sum of the width parameters as in Eq. 4 (restated below):\n$$\n\\mathrm{FLOPs}=\\sum_{k=1}^K (w_k^R + w_k^C),\n$$\nwhich is the $\\ell_1$ norm of the vector containing width parameters $\\boldsymbol{w}=\\{w_1^R, w_1^C,\\ldots,w_K^R,w_K^C\\}$.\nHence, minimizing the $\\ell_1$ norm of this width parameter vector $\\boldsymbol{w}$ of a GBLR matrix reduces the FLOPs for matrix-vector product.\n\n\n\n**Q3:** What is the value of $\\lambda$ used in experiments? How to choose it properly?\n\n**A3:** We used $\\lambda=0.02-0.04$ for the ViTs, $\\lambda=0.1-0.2$ for the MLP-Mixers, and $\\lambda=0.001-0.005$ for ResNet-18. The range of the optimal choice for the $\\lambda$ hyperparameter depends on the model, learning rate, and dataset. We used a grid search enumerating the search space to find the $\\lambda$ hyperparameter in our experiments. Automatically selecting an optimal $\\lambda$ from the budget is left as a topic for future work.\n\n**Q4:** What was the scheme used for $\\sigma$? The paper says that it was \"gradually increased to 100\", but having exact details is preferred.\n\n\n**A4:** We initially set $\\sigma$ to 1. At each epoch, we start increasing $\\sigma$ linearly to 100 until the end of the training process, except during the learning rate warm-up and cool-down steps. For example, if the training process consists of 110 epochs including 5 warm-up epochs at the beginning and 10 cool-down epochs at the end, $\\sigma$ remains at 1 until epoch 5 and then increases linearly to 100 until epoch 100.\nWe provided the $\\sigma$ scheduling used in our experiment in Appendix A.7.2.\n\n**Q5:** Certain computations (Gaudi function) happens in frequency domain that involves DFT and IDFT; how expensive is this operations wrt single training step? No slowdown, 0.75x slowdown, etc..\n\n**A5:** FFT/IFFT does not impose significant overhead in our methodology. Let us consider ViT-Large for example. At each fully-connected layer, 1-D Inverse Fast Fourier Transform (IFFT) is performed over a $4096 \\times 1024$-sized weight matrix, which requires $4096 \\times 2.5 \\times 1024 \\log_2 1024 = 104,857,600 \\approx 105\\times 10^6$ FLOPs.\n\nIn contrast, the matrix-matrix multiplication (MMM) between a $(B, N, 4096)$-sized input tensor and a $4096\\times 1024$-sized weight matrix requires $B\\times N \\times 4096 \\times 1024$ FLOPs where $B$ and $N$ are the batch size and sequence length, respectively. For $B=128$ and $N=197$, the number of FLOPs for MMM is $106\\times 10^9$, which is three orders of magnitude larger than the number of FLOPs required for IFFT.\n\n\n\n**Reference** \n\n[1] Liebenwein, Lucas, et al. \"Compressing neural networks: Towards determining the optimal layer-wise decomposition.\" Advances in Neural Information Processing Systems 34 (2021): 5328-5344."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332667137,
                "cdate": 1700332667137,
                "tmdate": 1700332667137,
                "mdate": 1700332667137,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xDOq99ugxL",
            "forum": "pAVJKp3Dvn",
            "replyto": "pAVJKp3Dvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_f3RM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_f3RM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generalized and differentiable method to learn efficient structures of weight matrices. Moreover, the authors present an effective initialization technique for the proposed method. Some experimental results show the performance of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a generalized and differentiable method to learn efficient structures of weight matrices. \n2. Moreover, the authors present an effective initialization technique for the proposed method. \n3. Some experimental results show the performance of the proposed method."
                },
                "weaknesses": {
                    "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tIn Algorithm 1, what\u2019s AdamW(.), as well as clip?\n2.\tIn Eq. (8), what\u2019s the variable, w or \\theta?\n3.\tThe advantage of the proposed method against existing methods is not clear. \n4.\tThe parameter initialization for the proposed method needs to perform SVD. Thus, the authors should analyze the computational complexity.  \n5.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm and more methods on more models and datasets.\n6.\tThe English language in this paper needs to be improved."
                },
                "questions": {
                    "value": "Although the paper is theoretically and experimental sound, there are still some questions need to be discussed in this paper:\n1.\tIn Algorithm 1, what\u2019s AdamW(.), as well as clip?\n2.\tIn Eq. (8), what\u2019s the variable, w or \\theta?\n3.\tThe advantage of the proposed method against existing methods is not clear. \n4.\tThe parameter initialization for the proposed method needs to perform SVD. Thus, the authors should analyze the computational complexity.  \n5.\tThe experimental results are not convincing. The authors should compare the performance of the proposed algorithm and more methods on more models and datasets.\n6.\tThe English language in this paper needs to be improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Reviewer_f3RM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699108421752,
            "cdate": 1699108421752,
            "tmdate": 1699637022660,
            "mdate": 1699637022660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D3N9jgUehh",
                "forum": "pAVJKp3Dvn",
                "replyto": "xDOq99ugxL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comprehensive suggestions! The below provides answers to some of your questions.\n\n**Q1:** In Algorithm 1, what\u2019s AdamW(.), as well as clip?   \n\n**A1:** $\\mathrm{AdamW}(\\cdot )$ stands for the AdamW optimizer [1]. Line 5 in Algorithm 1 ($p \\gets p - \\eta \\cdot \\mathrm{AdamW}(\\nabla \\ell)$) corresponds to the conventional parameter update to minimize the loss $\\ell$ by using the AdamW optimizer. \n\nThe function $\\mathrm{clip}(\\mathbf{w}, 0, n)$ clamps each element of $\\mathbf{w}$ to the interval $[0, n]$, which is the domain of the width parameter. We included this operation in Algorithm 1 to ensure that the width parameter stays within $[0,n]$ after the gradient descent. \n\nWe have added a detailed explanation of Algorithm 1 in Section 3.3.\n\n**Q2:** In Eq. (8), what\u2019s the variable, w or $\\theta$?   \n\n\n**A2:** In Eq. (8), the variable $\\theta$ denotes the set of the structural and content parameters, namely, $\\theta=(\\boldsymbol{\\phi}, \\boldsymbol{U}, \\boldsymbol{V}, \\sigma)$, as stated above Eq. (7).\nThe variable $\\boldsymbol{w}$ denotes the width parameters $\\boldsymbol{w}=\\{w_1^R, w_1^C,\\ldots,w_K^R,w_K^C\\}$ of $\\boldsymbol{W}^{\\boldsymbol{\\theta}}$. We updated Section 3.3 to clarify the definition of $\\boldsymbol{w}$.\n\n**Q3:** The advantage of the proposed method against existing methods is not clear.   \n\n**A3:** The key advantage of our method lies in its ability to *learn the structural parameters of weight matrices* by stochastic gradient descent, overcoming the non-differentiable nature of typical matrix structures --e.g., non-zero locations of the sparse matrix or the rank of the low-rank matrix. Prior approaches used hand-crafted structures because of this issue. Our approach allows treating structural parameters (widths and locations of the masks in Eq. 2) as additional trainable parameters to automatically identify/learn efficient structures in practice. \nThis enables the end-to-end, layer-wise learning of the optimal structured matrix for each weight matrix given the computational budget for each layer.\n\nMost existing rank-based compression methods are confined to either a single type of structured matrix (e.g., low-rank or block low-rank) [2,3] relying on a heuristic rule for budget allocation across layers [3,4], or segregating the structural parameter optimization from the training process of the other parameters [2,4].\nThis is primarily because, in those prior approaches, the structural formats are defined in a non-differentiable and combinatorial manner.\n\nUsing the learned structures and parameters from our approach generally provides higher accuracy for the same complexity (or comparable accuracy with lower complexity) compared to the model in prior works. \n\n**Q4:** The parameter initialization for the proposed method needs to perform SVD. Thus, the authors should analyze the computational complexity.  \n\n**A4:** Please note that our objective is to minimize the inference complexity. And the SVD overhead during training is negligible. The SVD during initialization takes less than one minute, whereas the overall training or fine-tuning time ranges from a few hours to tens of hours. Also, the SVD is unnecessary during the inference stage once the network is trained. We have included an analysis of the impact of SVD on training time in the Appendix.\n\n**References**\n\n[1] Loshchilov, Ilya, and Frank Hutter. \"Decoupled Weight Decay Regularization.\" International Conference on Learning Representations. 2018.\n\n[2] Liebenwein, Lucas, et al. \"Compressing neural networks: Towards determining the optimal layer-wise decomposition.\" Advances in Neural Information Processing Systems 34 (2021): 5328-5344.\n\n[3] Chen, Beidi, et al. \"Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models.\" International Conference on Learning Representations. 2021.\n\n[4] Dao, Tri, et al. \"Monarch: Expressive structured matrices for efficient and accurate training.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332276218,
                "cdate": 1700332276218,
                "tmdate": 1700332276218,
                "mdate": 1700332276218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JjUJ1kPJn1",
            "forum": "pAVJKp3Dvn",
            "replyto": "pAVJKp3Dvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_g58u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8231/Reviewer_g58u"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a Generalized Block-low-rank  (GBLR) matrix format to construct computationally efficient structures of weight matrices. They also introduce Gaussian-Dirichlet (Gaudi) function to make the structural parameters differentiable and provide an algorithm to learn neural networks with Gaudi-GBLR weight matrices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed GBLR format includes existing important matrix structures. Also, the authors provide a method to make the structural parameters of weight matrices learnable. The idea is interesting and relevant to the community."
                },
                "weaknesses": {
                    "value": "Providing theoretical investigations of the neural networks learned by the proposed method can improve the quality of the paper. Since the weight matrices are forced to be sparse, I think we need a different analysis from existing analysis for the dense weight matrices. For example, do you have any explanation about the representation power and generalization property of the networks with GBLR weight matrices?"
                },
                "questions": {
                    "value": "As I also mentioned in the weakness part, how does the GBLR weight matrices affect the generalization property or the complexity of the neural network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8231/Reviewer_g58u"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699486733280,
            "cdate": 1699486733280,
            "tmdate": 1699637022541,
            "mdate": 1699637022541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZHKnFzHaZw",
                "forum": "pAVJKp3Dvn",
                "replyto": "JjUJ1kPJn1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8231/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful feedback! \n\n**Q1:** How does the GBLR weight matrices affect the generalization property or the complexity of the neural network?\n\n**A1:** We hypothesize based on empirical observations that the GBLR weight matrices reduce the generalization gap of neural networks.\nWe observed that when the weights of DNNs are initialized to the GBLR matrices and trained from scratch, the *training* accuracy is generally lower than that of the dense model, whereas the *validation accuracy* is similar. For instance, Table 1 shows the ImageNet accuracy of dense and GBLR ViT-Base models.\n\n**Table 1.**\n\n| ViT-Base   | Training Accuracy (\\%) | Validation Accuracy ($\\|\\Delta\\|$) (\\%) | Training Loss | Validation Loss ($\\|\\Delta\\|$) |\n|------------|-----------------------:|----------------------------------------:|--------------:|-------------------------------:|\n| Dense      |                  99.03 |                           78.57 (20.46) |        0.1640 |                1.0639 (0.8999) |\n| Gaudi-GBLR |                  94.93 |                           78.51 (16.42) |        0.3260 |                0.9917 (0.6657) |\n\nThe comparable validation accuracy despite the lower training accuracy provides reasonable empirical evidence that the DNNs with GBLR matrices may exhibit smaller generalization errors than the dense models. We added this discussion to the paper.\n\n\nHowever, given the low-rankness and sparsity of GBLR matrices, we agree with the reviewer that we need a different technique to fully understand the generalization and approximation bounds of the DNNs with GBLR matrices.\nDue to time constraints, unfortunately, we could not conclusively determine the theoretical generalizability of the GBLR matrices.\nNonetheless, we believe the generalized structured format has potential to serve as a good tool for a better understanding of the generalization bounds of DNNs using the low-rank matrices [1,2,3] and the approximation bounds of DNNs with sparse matrices [4,5]. Conceptually, they are based on the low-dimensional representations of large and complex functions. Since our method expands the low-dimensional matrix representations from the hand-designed models to a general parametric space, we are eager to study the GBLR matrix theoretically in future work.\n\n\n\n**References**\n\n[1] Arora, Sanjeev, et al. \"Stronger generalization bounds for deep nets via a compression approach.\" International Conference on Machine Learning. PMLR, 2018.\n\n[2] Suzuki, Taiji, Hiroshi Abe, and Tomoaki Nishimura. \"Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network.\" International Conference on Learning Representations. 2020.\n\n[3] Baykal, Cenk, et al. \"Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds.\" International Conference on Learning Representations. 2018.\n\n[4] Klusowski, Jason M., and Andrew R. Barron. \"Approximation by combinations of ReLU and squared ReLU ridge functions with $\\ell^ 1$ and $\\ell^ 0$ controls.\" IEEE Transactions on Information Theory 64.12 (2018): 7649-7656.\n\n[5] Domingo-Enrich, Carles, and Youssef Mroueh. \"Tighter Sparse Approximation Bounds for ReLU Neural Networks.\" International Conference on Learning Representations. 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332151522,
                "cdate": 1700332151522,
                "tmdate": 1700332151522,
                "mdate": 1700332151522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K58Od9lWYO",
                "forum": "pAVJKp3Dvn",
                "replyto": "ZHKnFzHaZw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8231/Reviewer_g58u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8231/Reviewer_g58u"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I will keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674525580,
                "cdate": 1700674525580,
                "tmdate": 1700674525580,
                "mdate": 1700674525580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]