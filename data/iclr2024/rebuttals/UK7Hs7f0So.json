[
    {
        "title": "VMFTransformer: An Angle-Preserving and Auto-Scaling Machine for Multi-horizon Probabilistic Forecasting"
    },
    {
        "review": {
            "id": "2yYYYEc16k",
            "forum": "UK7Hs7f0So",
            "replyto": "UK7Hs7f0So",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_Abqw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_Abqw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a direct approach for multi-horizon probabilistic forecasting that characterizes the dependence across future horizons. The model treats the multi-horizon target as a random vector, using the von Mises-Fisher (VMF) distribution to characterize the direction and a truncated normal distribution for the magnitude. Additionally, it introduces the concept of \"Angle&Scale\" similarity to replace the traditional dot-product similarity in self-attention. The model's performance is evaluated on three datasets and shows superiority over some previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- The motivation behind modeling dependencies among future horizons is clearly articulated and logically sound.\n- The idea of decoupling the target vector into angle and scale is both interesting and innovative."
                },
                "weaknesses": {
                    "value": "- Overall, this work proposes two orthogonal methods: and objective function based on VMF distribution and the \u201cAngle&Scale\u201d similarity to replace dot-product in self-attention. The motivation behind the latter is not as evident. The computation process, i.e. equations at the bottom of page 5, are confusing due to the misleading subscripts. And a figure for visualization is missing here.\n\n- Baselines for comparison are somewhat outdated."
                },
                "questions": {
                    "value": "- Could you provide a more detailed and clear explanation of the \"Angle&Scale\" similarity, specifically the shape of these matrices and computation process of variables $Z, Q, K, V$?\n- As the objective and \u201cAngle&Scale\u201d similarity are orthogonal, the ablation stduy should compare four models: 1.VMF loss+Dot product; 2.VMF loss+Angle&Scale; 3.MSE loss+Dot product; 4.MSE loss+Angle&Scale. Please complete 3 and 4 to show the effectiveness of the proposed objective function.\n- Please compare with some recent models, such as DLinear and PatchTST.\n- Consider evaluating the speed and memory usage of the \"Angle&Scale\" similarity with the Dot-product method, as the constant terms in complexity analysis can significantly impact practical efficiency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Reviewer_Abqw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672820754,
            "cdate": 1698672820754,
            "tmdate": 1699637110763,
            "mdate": 1699637110763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wovnSyN786",
                "forum": "UK7Hs7f0So",
                "replyto": "2yYYYEc16k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thoughtful feedback.\n\n**W1**: We appreciate the reviewer raising this important point about clearly explaining the intuition and implementation details behind the proposed \"Angle&Scale\" similarity measure. \n\nThe key motivation for Angle&Scale similarity is to better match the structure of our proposed VMF-Transformer objective function, which models the multi-horizon forecast target as a random vector characterized by both an angle/direction and a magnitude/scale. As explained in Section 3.1, the VMF distribution captures the dependence structure across horizons through the angle or direction of the target vector. The scale of the vector represents the magnitudes of the target at each timestep.\n\nAccordingly, the Angle&Scale similarity is designed to evaluate relevance based on two facets - angle and scale. For two vectors in the attention module, we first compute the cosine of their angle to assess similarity in orientation. We additionally multiply this by a Gaussian kernel applied on the difference in vector lengths. This accounts for whether two vectors have a similar scale or magnitude.\n\nIntuitively, this allows the model to match historical sequences in the attention module that have both a similar direction and scale profile to the forecast target. We empirically demonstrate in Section 4 that using Angle&Scale similarity outperforms baseline dot product attention across multiple metrics and datasets. The results suggest that incorporating both angle and scale information is beneficial for this forecasting approach.\n\nThe shape of $Z$ is $(t+1)\\times (H*d+1)$, where $t+1$ is the length of the history (we allow the index of the time series sequence to start from 0 instead of 1.).  The $i$-th row of $Z$ is a vector \n$[y_i \\circ \\mathbf{x}^T_{i+1}\\circ \\cdots \\circ \\mathbf{x}^T_{i+H}]$, \nwhere $y_i$ is the ground truth at time $i$, and $\\mathbf{x}^T_{i+1}, \\cdots\n\\mathbf{x}^T_{i+H}$ \nare features or known inputs corresponding to the future. Each $\\mathbf{x}_{i+1}$\n is a d-dimension vector, and they are packed into one row vector by simply transpose and concatenation together with the real value \n$y_i$ . \nSince the prediction length is $H$, there are a total number of $H$ input vectors \n$\\mathbf{x}$. \n Hence the length of each row vector of $Z$ is $H\\times d+1$. \nAnd therefore, $Z$ is a $(t+1)\\times (H \\* d+1)$ matrix.\nFor $Q$ ,$K$, and $V$, they are all originated from $Z$ by applying different convolutional kernels to $Z$, where the convolutional kernels are learned during the training stage. The  kernels are all 1D, and hence do not change the 'length' of $Z$, but compress the 'width' of $Z$ from $(H \\* d+1)$ to $d_q, d_k$ and $d_v$ for $Q$ ,$K$, and $V$ respectively.\n\n**W2**, **W3**, and **W4**: we thank you for your advice. The revised draft is presented in **general responses (from part 1 to part 5)**."
                    },
                    "title": {
                        "value": "Response to Reviewer Abqw"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733874189,
                "cdate": 1700733874189,
                "tmdate": 1700737220942,
                "mdate": 1700737220942,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3ojdDMe5t0",
            "forum": "UK7Hs7f0So",
            "replyto": "UK7Hs7f0So",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_s4Yt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_s4Yt"
            ],
            "content": {
                "summary": {
                    "value": "To mitigate the accumulation of forecast errors associated with the recursive strategy and the conditional assumption of the direct strategy, the authors propose a novel approach for multi-horizon probabilistic forecasting. This new strategy effectively captures the interdependence across future horizons by modeling the multi-horizon target as a random vector. The direction of this vector represents temporal dependence, while its length measures the overall scale across each horizon. The authors assume that the angle and magnitude of these vectors follow a von Mises-Fisher (VMF) and a truncated normal distribution, respectively. The authors conducted experiments on three benchmark datasets, including an ablation study, demonstrating the advantages of their proposed method over six state-of-the-art techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses an important challenge in time series forecasting, providing new forecasting strategies for multi-horizon probabilistic forecasting.\n\n- The approach of modeling the direction and length of future random vectors is innovative in the context of multi-horizon forecasting strategies.\n\n- The proposed method is compared against multiple state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "- The proposed method appears to be relatively complex, yet it does not seem to offer substantial improvements over simpler existing methods. Additionally, the motivations behind various design choices lack clarity in the paper.\n\n\t- The method involves intricate elements such as the Bessel function and modeling assumptions like the von Mises-Fisher and truncated normal distributions.\n\t- In the field of time series forecasting, simpler methods often yield satisfactory results. The paper does not convincingly justify the necessity of such complexity in deep time series forecasting.\n\t- The authors should thoroughly discuss their modeling assumptions and their impact relative to alternative methods. For instance, why opt for a truncated normal distribution? Are there other alternatives?\n\t- The rationale behind incorporating a multi-head convolutional self-attention mechanism should be explained.\n\t- While the authors aim to model interdependence across future horizons, they have not employed multivariate scoring rules to evaluate this aspect.\n\t- The paper asserts that recursive and direct strategies can yield high forecast errors, but it remains unclear how the proposed strategy compares with other forecasting approaches that use the same underlying model.\n\n- Experiments:\n\n\t- The choice of only three datasets for experimentation raises questions, as many machine learning papers on time series forecasting examine more extensive benchmark datasets. For example, see the Monash Time Series Forecasting Archive (https://arxiv.org/abs/2105.06643)\n\n\t- Many time series datasets exhibit a pronounced seasonal component that dominates the signal. Consequently, it may be challenging to surpass simpler methods that effectively estimate this seasonal component. The paper should address the strength of the seasonal component in the considered datasets.\n\n\t- The authors have not included simple benchmarks such as auto.arima or exponential smoothing in their comparisons.\n\n\t- Learning curves should be provided to demonstrate the stability of the training procedure.\n\n\t- The proposed method is trained using maximum likelihood estimation. The authors should also provide the negative log-likelihood (NLL) for all density-based methods.\n\n\t- The authors did not report standard errors and information regarding the number of runs performed.\n\n\t- The results for specific forecast horizons (e.g., h = 1, 2, 3, etc.) should be reported to assess if the procedure increases forecast error for initial horizons while improving the average error across horizons."
                },
                "questions": {
                    "value": "- Refer to the \"Weaknesses\" section for questions.\n\n\n\t- Typos and Improvements:\n\t\t- Equation 6 is rimarily computing the logarithm of expression (5). For clarity, it may be better to move it to the appendix.\n\t\t- On page 6, the figure number is missing.\n\t\t- The sentence beginning with \"It should be noted that in Equation equation 7,\" is not clear.\n\t\t- \"the provide the proof.\" Furthermore, clarify that you are citing a reference and not presenting a proof."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748137974,
            "cdate": 1698748137974,
            "tmdate": 1699637110644,
            "mdate": 1699637110644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mpVv3Od1jC",
                "forum": "UK7Hs7f0So",
                "replyto": "3ojdDMe5t0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s4Yt"
                    },
                    "comment": {
                        "value": "We thank the reviewer for offering valuable feedback. We have addressed each of the concerns raised by the reviewer as outlined below.\n\n1. The reviewers rightly point out that many time series exhibit strong seasonal patterns that can dominate the signal. We analyzed the strength of the seasonal component in the electricity, solar, and traffic datasets using autocorrelation analysis. The electricity data shows a strong daily seasonal pattern with autocorrelation remaining above 0.6 at the 24 hour lag. The solar data shows both daily and yearly seasonal patterns due to the underlying physical processes. The traffic data has relatively weak seasonality. However, basic time series methods such as auto.arima and exponential smoothing often cannot effectively model complex real-world time series. ARIMA, exponential smoothing, and Prophet on our used datasets achieve relatively poor performance compared to more flexible machine learning approaches [1, 2]. These methods make strong assumptions about the underlying data generative processes and do not directly capture uncertainties. Our model demonstrates excellent performance across datasets with varying degrees of seasonality. By jointly modeling the sequential dependence, overall scale, and capturing uncertainty, our method provides robust performance without needing an explicit seasonal component model. We provide quantitative metrics showing our model's ability to accurately forecast across a diverse range of conditions.\n\n[1] Lim, Bryan, et al. \"Temporal fusion transformers for interpretable multi-horizon time series forecasting.\" International Journal of Forecasting 37.4 (2021): 1748-1764.\n\n[2] Wang, Y., Smola, A., Maddix, D., Gasthaus, J., Foster, D., & Januschowski, T. (2019, May). Deep factors for forecasting. In International conference on machine learning (pp. 6607-6617). PMLR.\n\n2. Learning curves (Figure 3) of our model are provided in the updated version to demonstrate the stability of the training procedure.\n\n3. A comparison of metrics corresponding to different horizons where H ranges from 1 to 720 (Figure 4) of our model is provided in the updated version to demonstrate the stability of forecasting performance.\n\n4. This revised draft incorporates extensive additional experiments in general responses (from part 1 to part 5), now encompassing the latest baseline models (such as PatchTST and DLinear) across varying and longer horizons (24, 168, and 720). Every key result includes the mean and standard deviation to quantify uncertainty.\n\n5. We actually used the negative log-likelihood (NLL) in the training stage. As we explained in our paper ' The training object is to maximize Equation 6 with respect to $\\Theta$, or equivalently, minimize the inverse $-l(\\Theta)$', where $-l(\\Theta)$ refers to the NLL. Thanks for noticing. We now realize this may be a confusing statement and we will clarify it to enhance readability and comprehension."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735804994,
                "cdate": 1700735804994,
                "tmdate": 1700736383876,
                "mdate": 1700736383876,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gpw2M5Wwj7",
            "forum": "UK7Hs7f0So",
            "replyto": "UK7Hs7f0So",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_2Tuf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_2Tuf"
            ],
            "content": {
                "summary": {
                    "value": "The paper contributes to time series forecasting by proposing a new model, the VMFTransformer, which tackles the inherent problems in iterative and direct forecasting methods. Iterative methods suffer from cumulative errors, and direct methods often incorrectly presume independence between future time points. The VMFTransformer addresses these issues by conceptualizing forecasts as random vectors, utilizing the von Mises-Fisher distribution to maintain temporal directionality and a truncated normal distribution for magnitude, accurately preserving time-dependent relationships. Benchmarked against several methods, the VMFTransformer demonstrates enhanced predictive performance, showing its effectiveness, and adaptability for a range of time series forecasting applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Error Accumulation Mitigation: Effectively addresses the issue of error accumulation inherent in iterative forecasting methods.\n\n\n2. Temporal Independence: Overcomes the unrealistic assumption of temporal independence used by direct forecasting methods.\n\n\n3. Directional Dependencies: Employs the von Mises-Fisher distribution to accurately capture the directional dependencies in time series data.\n\n\n4. Magnitude Characterization: Utilizes a truncated normal distribution to model the magnitude of forecasts, enhancing predictive accuracy."
                },
                "weaknesses": {
                    "value": "1. Lack of Recent Benchmarks: The VMFTransformer has not been compared with the most recent benchmarks such as PatchTST [1], or with most comparisons made against older methods. for instance please consider looking at recnet benchmarks provided by https://github.com/timeseriesAI/tsai\n\n\n2. Limited comparison for different horizons: The scope of comparison is limited, which may not adequately reflect the model's performance across a broader range of forecasting scenarios.\n\n\n3. Formatting Issues: There is room for improvement in the formatting of the equations, which could enhance readability and comprehension.\n\n\n4. Presentation Clarity: The paper could better articulate the main contribution, as the current presentation may be challenging to follow, possibly obscuring the model's innovation.\n\n\n5. Insufficient Experimentation: Without more extensive experimentation, it's difficult to ascertain the paper's meaningful contribution to the community and its practical applicability.\n\n\n[1] A Time Series is Worth 64 Words: Long-term Forecasting with Transformers, ICLR 2023"
                },
                "questions": {
                    "value": "Please consider the comments I provided above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699131553782,
            "cdate": 1699131553782,
            "tmdate": 1699637110541,
            "mdate": 1699637110541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EPymnFn4vs",
                "forum": "UK7Hs7f0So",
                "replyto": "gpw2M5Wwj7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Tuf"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and provide constructive feedback. We appreciate the reviewers recognizing the potential of our proposed VMFTransformer model for multi-horizon probabilistic time series forecasting. \n\n**W1**, **W2**, and  **W5**: this revised draft incorporates extensive additional experiments in **general responses (from part 1 to part 5)**, now encompassing the latest baseline models (such as PatchTST and DLinear) across varying and longer horizons (24, 168, and 720). Every key result includes the mean and standard deviation to quantify uncertainty.\n\n**W3**: We will revise the formatting of all equations in the paper to enhance readability and comprehension. Equations are centered on their own lines with consistent spacing around operators. Variable names have been standardized and vector/matrix notation clarified. We believe these changes significantly improve the formatting.\n\n**W4**: The main contribution of our work is the novel probabilistic forecasting model based on the von Mises-Fisher distribution and Transformer architecture. To better articulate this, we will add an explicit contribution statement in the introduction highlighting:\n1. The VMFTransformer model itself which captures temporal dependence in a multi-horizon target.\n2. The Angle&Scale similarity measure for the self-attention module\n3. An efficient method to optimize the likelihood function with the Bessel function\n\nAdditionally, we will reorganize several sections to improve logical flow and aid comprehension of the technical details underpinning the model innovation. A graphical abstract will also be added to provide an intuitive overview. We believe these changes address the need to better communicate the main contributions of our work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733889567,
                "cdate": 1700733889567,
                "tmdate": 1700733889567,
                "mdate": 1700733889567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HIfCdP2HZs",
            "forum": "UK7Hs7f0So",
            "replyto": "UK7Hs7f0So",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_7TJV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8832/Reviewer_7TJV"
            ],
            "content": {
                "summary": {
                    "value": "1. This paper addresses the challenges in time series forecasting, specifically in multi-horizon probabilistic forecasting, by proposing a direct approach that effectively characterizes the dependence across future horizons. Technically, the authors consider the multi-horizon target as a random vector and apply the von Mises-Fisher (VMF) distribution and the truncated normal distribution to model the angle and magnitude of the target vector. \n\n2. The performance of the proposed framework is evaluated on three benchmarks, demonstrating its superiority over six state-of-the-art methods in different time series forecasting tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and exhibits clarity in its presentation. The visual results help to comprehend the proposed framework.\n2. Authors propose a novel similarity measurement termed \u201cAngle&Scale\u201d similarity for the attention module and show that the Angle&Scale similarity outperforms the dot-product similarity in most cases in ablation studies.\n3. The proposed VMFTransformer consistently outperforms all baselines in  MSE and q-risk."
                },
                "weaknesses": {
                    "value": "1. The current version of the paper solely presents the average value obtained from five trials without including information about the standard deviation. It is highly recommended to include error bars.\n2. Why use the VMF distribution and the truncated normal distribution to characterize the angle and magnitude of the target vector? The motivation behind this is unclear to me.\n3. Metrics used to evaluate uncertainty are not sufficiently convincing,  a more commonly used metric, CRPS [1], was not used in the experiment.\n4. Some probabilistic time series baselines are not compared with the proposed method in the experiment, such as TransMAF [2], [3].\n\nReferences:\n\n[1] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007.\n\n[2] Binh Tang and David S Matteson. Probabilistic transformer for time series analysis. Advances in Neural Information Processing Systems, 34:23592\u201323608, 2021.\n\n[3] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Multivariate probabilistic time series forecasting via conditioned normalizing flows. arXiv preprint arXiv:2002.06103, 2020."
                },
                "questions": {
                    "value": "Please see my comments in Weaknesses.\n\nI would also appreciate if the authors can respond, if they can, to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8832/Reviewer_7TJV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699428541245,
            "cdate": 1699428541245,
            "tmdate": 1699637110334,
            "mdate": 1699637110334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7UM2H4iLy6",
                "forum": "UK7Hs7f0So",
                "replyto": "HIfCdP2HZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7TJV"
                    },
                    "comment": {
                        "value": "We deeply appreciate the thoughtful feedback provided by the reviewers. Their suggestions have helped strengthen our work substantially. In response:\n\n**W1** and **W4**: Per the reviewers' recommendations, we have added extensive additional experiments with the latest baseline models across longer time horizons. Every key result now includes the mean and standard deviation to rigorously quantify uncertainty. We believe these comprehensive additions showcase our method's strengths.\n\n**W2**: You raises an important point regarding the motivation behind using the von Mises-Fisher (VMF) distribution and truncated normal distribution to model the multi-horizon forecasting vector. The key intuition is that the VMF distribution can effectively characterize vectors distributed around a specific direction, which fits our assumption that when dependence exists among the multi-horizon targets, they will concentrate around a certain direction. The truncated normal distribution provides a flexible prior for the scale parameter to make the VMF likelihood tractable. Compared to simply using independent normal or Laplace distributions on each horizon, the VMF and truncated normal distribution can jointly capture the dependence structure via the shared direction parameter in VMF. We will add more discussion on the motivation in the revised paper to clarify this modeling choice.\n\n**W3**: We carefully re-evaluated TransMAF, but unfortunately found it performed poorly on our datasets - significantly worse than the other baseline models we assessed. As such, we decided not to include these inferior results and instead added evaluations using the popular PatchTST and DLinear models, providing readers with a more representative view of the state-of-the-art. Regarding evaluation metrics, we utilize the community standards of MAE, MSE, Q50-Loss, and Q90-Loss - prevalent metrics used across probabilistic time series works.\n\nWe thank the reviewers again for their outstanding suggestions. We believe the additions made in response (**from part 1 to part 5 in general response**) further demonstrate the competitiveness and rigor of our proposed approach across both model architectures."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733181140,
                "cdate": 1700733181140,
                "tmdate": 1700733181140,
                "mdate": 1700733181140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]