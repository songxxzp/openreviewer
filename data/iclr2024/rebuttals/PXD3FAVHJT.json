[
    {
        "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"
    },
    {
        "review": {
            "id": "BnlOb7FVbX",
            "forum": "PXD3FAVHJT",
            "replyto": "PXD3FAVHJT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_GaEc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_GaEc"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effects of RLHF on generalization and diversity. Specifically, the authors look at the three stages in RLHF: supervised fine-tuning, reward modeling, and reinforcement learning. They conduct experiments that show that RLHF generalizes better than SFT to new inputs, but reduces output diversity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The writing of the paper is clear and easy to follow. The paper studies three different aspects of performance, including in-distribution generalization, out-of-distribution generalization, and diversity. As far as I know, this covers a more comprehensive study on RLHF -ine-tuned model behavior than most observational studies in the literature."
                },
                "weaknesses": {
                    "value": "This paper does not offer any new insight or novel methods compared to existing work in the literature, and no new methods have been proposed. First of all, the generalization capabilities offered by RLHF has been widely observed in state-of-the-art models, with clear comparisons and case studies of output from pretrained, instruction fine-tuned, and RLHF fine-tuned models (see, for example, the Llama-2 paper). The mode collapse phenomenon from RLHF has also been widely observed and measured. Maybe the only novelty this paper offers is evaluation on an array of sentence-level diversity metrics. Furthermore, the claims made in the paper are not very well-justified by experiment results, and some experiment details are lacking. Only two sets of experiments, namely summarization and instruction following, are conducted on one single model (Llama-7B), yet the paper makes a general claim about the effects of RLHF. More experiments on different models at potentially different scales could be helpful, but still, the contribution seems to be incremental. \n\nMy main concern is the contribution. Some additional questions are listed below for clarification, but unless the authors could justify their contribution through substantial experiments (on different models at different scales) and more in-depth analysis, I still lean towards rejection."
                },
                "questions": {
                    "value": "The paper makes some unspecified claim that would need justification or further explanation. For example, on page 2, summary of contributions, the third bullet point: \"...implying that such models tend to produce text of a specific style regardless of the input\". How does one arrive at the \"style\" conclusion?\n\nWhy is there no error bars in Figure 5? Could you plot error bars over different choices of outputs from among the set of outputs to the same input?\n\nAre the OOD datasets considered OOD for fine-tuning, or both fine-tuning and pretraining? The CNN/DailyMail dataset is most probably included in the pretraining dataset of Llama-7B."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5728/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698018429104,
            "cdate": 1698018429104,
            "tmdate": 1699636599751,
            "mdate": 1699636599751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z2Prxc39Gp",
                "forum": "PXD3FAVHJT",
                "replyto": "BnlOb7FVbX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive comments on the paper\u2019s presentation and clarity. We\u2019re glad you found the study comprehensive. We now address your comments in detail.\n\n> the claims made in the paper are not very well-justified by experiment results, and some experiment details are lacking. Only two sets of experiments, namely summarization and instruction following, are conducted on one single model (Llama-7B), yet the paper makes a general claim about the effects of RLHF. More experiments on different models at potentially different scales could be helpful, but still, the contribution seems to be incremental.\n\nIn fact, **for the summarisation task we ran experiments with both LlaMA (7B) and OPT (125M, 350M, 1.3B, 2.7B, 6.7B) models**. The results with OPT are described in Appendix J. These results show the same trends as those with LlaMA, demonstrating that our conclusions generalise well across different base models, thus further strengthening our claims about the comparison between RLHF and SFT. While we cannot be entirely sure whether these conclusions hold in all possible settings, we conducted extensive experiments on **two different base models, two different tasks with five evaluation sets in total, across six different model sizes, using five different evaluation metrics for generalisation and diversity, resulting in training and evaluating over 80 models**. All these results point to similar conclusions. These experiments are very extensive, and required non-trivial resources. While it would always be better to have more experiments, we believe our analysis is rigorous and extensive enough to sufficiently support our conclusions.\n\nFollowing your suggestion, we will make sure to revise our claims to ensure we are not overclaiming and caveat that these conclusions are based on certain settings and models.\n\nIn addition, we would like to note that the tasks we tested models on are some of the most commonly used in practical applications of LLMs, namely summarization and instruction following (which is also similar to single turn dialogue, another popular application), so we think our results can be of broad interest to researchers and practitioners alike. Even if our results only held in the instruction following setting this should still be considered valuable for the community. Instruction-following is clearly a very important and timely topic: it has a workshop at NeurIPS (https://an-instructive-workshop.github.io/), lots of interest both in academia and industry, and is one of the most common applications that LLMs are fine-tuned and then deployed for. Thus, we believe improving our understanding of methods even in this domain alone is useful and relevant for the ICLR community.\n\n[Response Continued Below]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139609717,
                "cdate": 1700139609717,
                "tmdate": 1700139681518,
                "mdate": 1700139681518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qKe4k4mOfX",
                "forum": "PXD3FAVHJT",
                "replyto": "BnlOb7FVbX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate the time you have dedicated to reviewing our paper. In the response above, our responses have addressed your concerns regarding the novelty and clarity of our work, and so we hope you will consider raising your score. Otherwise, please let us know if there is anything preventing you from recommending acceptance of the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507396012,
                "cdate": 1700507396012,
                "tmdate": 1700507396012,
                "mdate": 1700507396012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8aj6v0PuYZ",
                "forum": "PXD3FAVHJT",
                "replyto": "qKe4k4mOfX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_GaEc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_GaEc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. I apologize for missing the results on OPT models in the appendix. Most of my minor concerns are addressed. However, I am still concerned about the contribution of this work. It seems that the main insight of this paper is that RLHF helps the model generalize better at the sacrifice of output diversity. But this insight is not new. Many prior work have pointed out this issue and possible explanations [1-5]. \n\nWhile I agree that a systematic study of the tradeoff between diversity and generalization of RLHF is interesting, I think this paper should not limit itself in confirming a phenomenon that has been observed widely and examined, if not explicitly, in ablation studies of many empirical works. At the minimum, could you plot the pareto frontier of diversity metrics versus OOD generalization across models and tasks? What would be potential ways that could push the pareto frontier outwards? Why is KL-divergence not sufficient in maintaining diversity? How about other entropy-based regularization? \n\n[1] Whose Opinions Do Language Models Reflect? Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto. ICML 2023.\n\n[2] Aligning Language Models with Preferences through f-divergence Minimization. Dongyoung Go, Tomasz Korbak, Germ\u00e1n Kruszewski, Jos Rozen, Nahyeon Ryu, Marc Dymetman. ICML 2023.\n\n[3] Red Teaming Language Models with Language Models. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, Geoffrey Irving. EMNLP, 2022.\n\n[4] A Distributional Approach to Controlled Text Generation. Muhammad Khalifa, Hady Elsahar, Marc Dymetman. ICLR 2021.\n\n[5] Improving alignment of dialogue agents via targeted human judgements. Glaese et al."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637871008,
                "cdate": 1700637871008,
                "tmdate": 1700637871008,
                "mdate": 1700637871008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1QKXrVeAuM",
                "forum": "PXD3FAVHJT",
                "replyto": "BnlOb7FVbX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. We are glad to have addressed your minor concerns.\n\nWe believe the insights provided by our paper are new, and especially that they have been shown robustly to hold across a variety of settings. To compare our experiments and conclusions to each of the works you shared:\n\n1. We think the reviewer is pointing to the *Modal representativeness* paragraph of this work as evidence for a lack of diversity from RLHF. However, this experiment doesn't show that RLHF (as opposed to SFT, or some other part of the pipeline for training `text-davinci-003` that we don't know about) is the cause of this lack of diversity. Further, the experiment is limited to multiple choice question answering, and only considers one model.\n\n2. This work does show that something similar to RLHF (RKL-DPG in that work) does reduce per-input output diversity and increases in-distribution reward. However, the metrics for diversity are limited, they don't investigate across-input output diversity, then don't compare RLHF to SFT or BoN, and they don't look at out-of-distribution generalisation. Further, their work is on much smaller models and simpler tasks than the ones we investigate.\n\n3. This work again shows that RLHF increases in-distribution reward but decreases per-input output diversity, and does compare to SFT in this setting. However, it again only uses limited diversity metrics, doesn't evaluate cross-input output diversity, and doesn't investigate out-of-distribution generalisation. Further, the red-teaming setting investigated in that paper is plausible quite different from standard settings like instruction-following that we investigate, making it unclear whether their results would hold in these other settings.\n\n4. This work shows that RLHF (Ziegler in that work) decreases per-input diversity somewhat while increasing in-distribution reward, but it's experiments are on small models (GPT-2), they only investigate per-input diversity, they use limited diversity metrics, and they don't investigate out-of-distribution performance.\n\n5. This work compares SFT, BoN and RLHF, but does not have any experiments on out-of-distribution generalisation or output diversity, and so is quite unrelated to our work.\n\nOverall, *none of these works investigate out-of-distribution generalisation*, *all of them use more limited diversity metrics*, and *none of them evaluate cross-input output diversity*. There has been no study comparing the tradeoff between *out-of-distribution* generalisation and output diversity (both across-input and per-input), and hence our insights concerning the trade-off between these metrics of importance are novel.\n\n> At the minimum, could you plot the pareto frontier of diversity metrics versus OOD generalization across models and tasks? \n\nWe have added a plot to this effect in the summarisation setting, with ID and OOD winrates vs per-input and across-input diversity measures, in appendix L (Figure 24).\n\n> What would be potential ways that could push the pareto frontier outwards? Why is KL-divergence not sufficient in maintaining diversity? How about other entropy-based regularization?\n\nWhile we agree these are all interesting research questions, and we are interested in further research investigating them, we believe our work as it currently stands is sufficient for acceptance to ICLR, and that answering those research questions is future work. Even if you believe the work is not worthy of acceptance, would you consider raising your score from a 3 to a 5, given our changes addressing your minor comments and our response above?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648394578,
                "cdate": 1700648394578,
                "tmdate": 1700739788675,
                "mdate": 1700739788675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bejX7iBCG0",
            "forum": "PXD3FAVHJT",
            "replyto": "PXD3FAVHJT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to study the effects of RLHF for fine-tuning LLMs, focusing on out-of-distribution generalization and output diversity metrics. Through empirical experiments, this paper finds that RLHF can outperform SFT in terms of out-of-distribution generalization but at the cost of a decrease in output diversity. Such observations may help in better applying RLHF or SFT in specific applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper conducted extensive experiments to elucidate why RLHF behaves differently from SFT. The experimental setup is sound, and the empirical results may inspire future progress in this direction.\n\n- The paper is well-writen and easy to follow in general."
                },
                "weaknesses": {
                    "value": "- Missing Related Work\n\nIn fact, there is a theoretical comparison of RLHF and SFT-style methods in the framework of imitation learning [1]. Indeed, LLMs are imitating human speech. In that framework, RLHF corresponds to adversarial imitation learning (AIL) methods, and SFT corresponds to behavioral cloning (BC). To the best knowledge of the reviewer, that theoretical study reveals that AIL (RLHF) methods can have better out-of-distribution generalization performance than BC (SFT) because AIL methods optimize their policy on out-of-distribution states (prompts) and rigorously prove this phenomenon. I believe this related work is insightful for studying the advantages of RLHF over SFT, and this paper should be mentioned in the related work.\n\n[1] Xu, Tian, et al. \"On the generalization of adversarial imitation learning and beyond.\" *arXiv preprint arXiv:2106.10424* (2021).\n\n\n- Typos and Writing Suggestions\n\n1. There are two minus symbols in Equation (1).\n2. It seems unusual to draw a conclusion in Section 6.3 while presenting empirical evidence in Appendix I."
                },
                "questions": {
                    "value": "The major concerns stem from the fact that the empirical evaluation heavily relies on the training quality of each algorithm, and the reviewer is uncertain about whether RLHF is trained to a high standard.\n\n**Question 1**: Do empirical conclusions heavily depend on the training status of the reward model and PPO? The reviewer observed that this paper freezes some layers of LLMs when using RLHF, which may limit the representation capacity. Thus, the reviewer questions whether the training of RLHF is of good quality. Could this paper provide the evaluation accuracy of the reward model and training curves of PPO?\n\n**Question 2**: Why not use entropy as a metric of diversity (although existing evaluation methods are acceptable)?\n\n**Discussion**: This paper mentions that \"Future work should investigate why RLHF reduces the output diversity so much,\" and the reviewer would like to point out some observations: the optimal policy by RL algorithms is deterministic (i.e., less diversity), if there is no tie in the reward value, there is no KL penalty, and the optimization is done perfectly. When there is a KL penalty, a recent paper shows that this corresponds to \"soft Q-learning\" [2]. In that case, the reward model is optimized perfectly. Although the algorithm in [2] is not applicable to the true RLHF setting where we only have out-of-distribution prompts and no preference labels, the viewpoint in [2] is insightful for in-distribution training.\n\n[2] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" *arXiv preprint arXiv:2305.18290* (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5728/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457521683,
            "cdate": 1698457521683,
            "tmdate": 1699636599636,
            "mdate": 1699636599636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fo8P6mBBo6",
                "forum": "PXD3FAVHJT",
                "replyto": "bejX7iBCG0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive comments. We\u2019re glad you found the work well-written and empirically sound. We\u2019ll now respond to your comments in detail.\n\n> The major concerns stem from the fact that the empirical evaluation heavily relies on the training quality of each algorithm, and the reviewer is uncertain about whether RLHF is trained to a high standard\n\n**Given the strong performance of the RLHF and BoN policies relative to SFT when evaluated by humans and GPT-4**, and the high accuracy of the reward models (in fact slightly higher than reported in [2] - our RM gets .76 while the best model in [2] gets ~.74) **we believe this sufficiently demonstrates that our RLHF models are trained to a high standard**.\n\nFurther, our conclusions rely on the relative ranking of RLHF, SFT and BoN and gaps in performance between methods. **Even if our RLHF implementation is not fully optimal, our conclusions would still be correct, as an improved RLHF performance doesn\u2019t change the ranking of methods** in terms of generalisation as it already performs best in absolute terms on all evaluation sets.\n\n> Do empirical conclusions heavily depend on the training status of the reward model and PPO? The reviewer observed that this paper freezes some layers of LLMs when using RLHF, which may limit the representation capacity. Thus, the reviewer questions whether the training of RLHF is of good quality.\n\nWe agree that empirical conclusions will depend on the performance of the reward model and PPO algorithm, as well as on the performance of the SFT optimisation.\n\nFor the summarisation experiments, we believe our implementation is high quality for all these components, as our results mostly reproduce those in the literature: Our reward models gets .76 validation accuracy compared to ~.74 in [2]; our gpt-4 preference win rate for RLHF is .46 and for SFT is .36 while [1] gets ~.57 for RLHF and ~.41 for SFT (note there are slight differences in how GPT-4 is prompted). Slightly lower numbers should be expected given that we are freezing 80% of parameters. On freezing of layers, this happens for SFT, reward model training and PPO, so is unlikely to affect one method more than another. Thus, we\u2019d expect the conclusions of the paper to hold even if all parameters were fine-tuned. Also, work such as [3] shows that impressive capabilities such as long-form dialogue and improved adversarial robustness can still be achieved when freezing a similar proportion of layers. Finally, in table 14 in the appendix we present some comparisons of frozen and unfrozen SFT and reward models based on OPT, and show only a small drop in performance when freezing 80% of the layers, the same as in the rest of our experiments.\n\nFor the instruction-following experiments, we use the models released in AlpacaFarm [5]. These models get good performance in both GPT-4 and human evaluations in both their work and ours, and we believe that their implementation is of high quality.\n\n> Could this paper provide the evaluation accuracy of the reward model and training curves of PPO?\n\nThank you for the suggestion to improve the thoroughness of the paper. **We have updated the appendix and added a section (Appendix K) with the training reward and KL divergence curves for the PPO model and evaluation accuracy of the reward model**, in the summarisation task. The reward model (with 80% parameters frozen) still achieves 75.8% validation accuracy, which is approaching the maximum given the levels of inter-annotator agreement reported in [2] being close to 25%. The PPO model converges at a KL divergence of approximately 15, and a validation reward of approximately 0.8.\n\nFor the instruction-following task we used models released in AlpacaFarm ([5]), so we don\u2019t have access to training reward curves or reward model accuracy.\n\n[Due to character limits, the response continues in a child comment.]\n\n*[1]  Direct Preference Optimization: Your Language Model is Secretly a Reward Model, https://arxiv.org/abs/2305.18290*\n\n*[2] Learning to summarize from human feedback, https://arxiv.org/abs/2009.01325*\n\n*[3] Improving alignment of dialogue agents via targeted human judgements, https://arxiv.org/abs/2209.14375*\n\n*[4] TextGAIL: Generative Adversarial Imitation Learning for Text Generation, https://arxiv.org/abs/2004.13796*\n\n*[5] AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, https://arxiv.org/abs/2305.14387*\n\n*[6] Evaluating the Evaluation of Diversity in Natural Language Generation, https://aclanthology.org/2021.eacl-main.25*"
                    },
                    "title": {
                        "value": "Response Part 1"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139388167,
                "cdate": 1700139388167,
                "tmdate": 1700139486848,
                "mdate": 1700139486848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqiKazaj3M",
                "forum": "PXD3FAVHJT",
                "replyto": "bejX7iBCG0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe appreciate the time you have dedicated to reviewing our paper. In the response above, our responses have addressed your concerns regarding related work and the quality of the training algorithms, and so we hope you will consider raising your score. Otherwise, please let us know if there is anything preventing from more strongly recommending acceptance of the paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507347466,
                "cdate": 1700507347466,
                "tmdate": 1700507418842,
                "mdate": 1700507418842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MXylXZwQ2L",
                "forum": "PXD3FAVHJT",
                "replyto": "bejX7iBCG0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response. \n\n- The concerns about the quality of reward models are well-addressed.  \n\n- I find the KL divergence to be quite large. According to the reviewer's experience, the LLM tends to overfit when the KL divergence is larger than 1, as observed while working with Llama2-7B on datasets such as full-hh-rlhf. I have reviewed the literature and found that the provided KL results are consistent with those reported in [1]. The provided figures seem to suggest that KL regularization does not prevent the achievement of high reward scores. Therefore, the reviewer remains skeptical about the PPO training, but can accept the current results.\n\n- A typo exists in Equation (4) in Appendix.\n\n [1] Learning to summarize from human feedback, https://arxiv.org/abs/2009.01325"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571283225,
                "cdate": 1700571283225,
                "tmdate": 1700573904840,
                "mdate": 1700573904840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o41Y8Hqhwp",
                "forum": "PXD3FAVHJT",
                "replyto": "CDT8iJzglN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_6G1j"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed explanation.\n\nTo clarify, I am not suggesting that a high-quality implementation (or optimization) of baseline methods would change the conclusion about the OOD generalization. At the current stage, this optimization curve is puzzling to me. It seems that the KL penalty does not work well. This may be due to the training configuration or datasets. Note that I do not deny the value of the conducted empirical results. Instead, I value them and suggest that we should be cautious and skeptical when drawing conclusions from empirical observations, for example, as in Section 4.3.\n\nI will make the final decision after discussing with other reviewers."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655008250,
                "cdate": 1700655008250,
                "tmdate": 1700655008250,
                "mdate": 1700655008250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ixVwVTMf6f",
            "forum": "PXD3FAVHJT",
            "replyto": "PXD3FAVHJT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_De3y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5728/Reviewer_De3y"
            ],
            "content": {
                "summary": {
                    "value": "This paper empirically investigates the difference in generalization and generation diversity for LLMs trained with supervised learning and reinforcement learning for text summarization and instruction following. Moreover, they evaluate best of N (BoN), a very strong method for text summarization, as an additional method to test generalization of language models. They ultimately find evidence for RLHF improving generalization over supervised learning but at the cost of generation diversity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Their thorough investigation of RLHF vs SFT generation quality is very valuable. This work helps improve our understanding of why RLHF policies have empirically seemed to perform well in practice with users where more OOD data is likely encountered. \n- The paper is very clearly presented and investigates two popular settings for RLHF finetuning."
                },
                "weaknesses": {
                    "value": "Minor Comments:\nFor summarization, it appears that pretrained models already perform very well for CNN daily mail. Would the same diversity, generalization, performance relationships be seen when evaluating OOD performance on a different summarization dataset where Llama2 7B does not perform as well? Or perhaps more simply, when trained on CNN as in-distribution, how is OOD performance to the harder TL;DR task?"
                },
                "questions": {
                    "value": "Please refer the questions posed in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5728/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766019498,
            "cdate": 1698766019498,
            "tmdate": 1699636599541,
            "mdate": 1699636599541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lslpa3Qjzs",
                "forum": "PXD3FAVHJT",
                "replyto": "ixVwVTMf6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive review. We\u2019re glad they found our investigation thorough and valuable, and the paper clearly presented. We now respond to your comments on the summarisation task.\n\nFirstly, for clarification and to avoid confusion, we\u2019re using the LLaMa 7B model, not LLaMa 2 7B, although your comments are still valid. Secondly, are you aware of any papers that illustrate the performance of these pretrained models on CNN daily mail? We would expect them to do reasonably well, but not as well as models after fine-tuning. For example figure 4 in [1] shows pretrained models still doing comparably to transferred SFT models but much worse than RLHFed models in a similar setting (train on TL;DR, test on CNN/DM), although how the pretrained models were trained is not discussed.\n\nIn your comment you imply that TL;DR summarisation is harder than CNN/DM summarisation. We haven\u2019t seen evidence for this in our work or in previous works; could you share where you\u2019ve seen this result so that we can cite and discuss it in our work? We might expect CNN/DM to be a harder summarisation task, as the inputs to summarise are much longer, and are likely to be more densely packed with information (as news articles) that is hard to summarise concisely as compared to reddit posts.\n \nFinally, it\u2019s unfortunately not possible with our current resources to train RLHF models on CNN/DM since it would require collecting a lot of high-quality human preference data for training a reward model on this dataset which is expensive to do. The preference datasets we use were open-sourced by OpenAI in [1], but they didn\u2019t release a large enough preference dataset for CNN/DM to enable reward model training. We agree it would be an interesting experiment to run, and we thank the reviewer for their suggestion. Our prediction is that the results would be the same: RLHF outperforms SFT both in-distribution and out-of-distribution, but at the cost of output diversity.\n\nWe thank the reviewer again for their complementary review and useful comments. We hope we've answered all your concerns, but please let us know if you have any remaining questions.\n\n*[1] Learning to summarize from human feedback, https://arxiv.org/abs/2009.01325*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139052325,
                "cdate": 1700139052325,
                "tmdate": 1700139052325,
                "mdate": 1700139052325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EBhqkZX84J",
                "forum": "PXD3FAVHJT",
                "replyto": "ixVwVTMf6f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_De3y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5728/Reviewer_De3y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough response. I correct my statement that CNN is an easier summarization task. As the authors stated, when seeing prior work such as [1], we see that CNN/DailyMail summarization is a more difficult summarization task than TL;DR.\n\nI also understand and agree with the authors' point about the lack of high-quality preference data for CNN barring them from performing the same analysis that they did on the TL;DR summarization task.\n\nI will be maintaining my score.\n\n[1] Statistical Rejection Sampling Improves Preference Optimization, Liu et al 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5728/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489898590,
                "cdate": 1700489898590,
                "tmdate": 1700489932176,
                "mdate": 1700489932176,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]