[
    {
        "title": "Efficient Subgraph GNNs by Learning Effective Selection Policies"
    },
    {
        "review": {
            "id": "dJAHUnMCRP",
            "forum": "gppLqZLQeY",
            "replyto": "gppLqZLQeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_ew9j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_ew9j"
            ],
            "content": {
                "summary": {
                    "value": "This paper is motivated by the high computational cost of subgraph GNNs and the bag-of-subgraph contains many redundant information and proposed a learnable method to efficiently select a fixed number of subgraphs for downstream prediction. The proposed method achieves a good balance between the cost and the expressive power. It obtains reasonable performance across various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall sound and easy to follow. \n2. The motivated CSL example is clear.\n3. The proposed method achieves reasonable results across various datasets."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper lie in its insufficient theoretical analyses and experiment validation. Specifically:\n1. The proposed method is overall an extension to the 1-MLE method in the OSAN, which is not a big problem to me. However, what I would expect is a more in-depth analysis of the proposed method. The authors only use a single category of graphs (CSL and its $(n, l)$ extension) to show that the proposed method is more powerful than a random policy and OSAN, which is trivial from my perspective. Instead of comparing it with random policy, a more interesting question would be: How well does the proposed method compare to the full-bag version? Can it achieve the same expressive power as the full-bag version and in how much $T$ can it be from a theoretical perspective? \n2. To distinguish non-isomorphic graphs is only the first step towards an expressive GNN. Can the model successfully encode the structure information like counting cycles play a more important role in real-world tasks. Full-bag versions of subgraph GNNs have a much better ability to encode sub-structures [1]. I am wondering can the proposed method maintain its ability to encode sub-structures.\n3. Some commonly used synthetic datasets for comparing expressive power are missing (e.g. EXP [2], CSL [3]). This could be part of the answer to weakness 1.\n4. Some commonly used synthetic datasets for evaluating the counting power of GNNs are missing [1]. This could be part of the answer to weakness 2.\n5. The main contribution of the subgraph sampling is its lower computational cost compared to the full-bag version. However, the authors only show a simple comparison of the inference time using one dataset where the full-bag version is OOM (Table 7). I believe a more comprehensive comparison between the full-bag version and the proposed method is required. What is the time and memory cost of the proposed method compared to the full-bag version in both the train and the test? How does the cost vary if we increase the $T$? \n\n[1] Huang et al., Boosting the cycle counting power of graph neural networks with i$^2$-GNNs, ICLR23.\n\n[2] Abboud et al., The surprising power of graph neural networks with random node initialization. IJCAI21.\n\n[3] Murphy et al., Relational pooling for graph representations. ICML19."
                },
                "questions": {
                    "value": "1. The current method only works for node-based subgraphs. Could the proposed method be generalized to other policies like edge-based [1] or node-tuple-based [2] subgraphs?\n\n\n[1] Huang et al., Boosting the cycle counting power of graph neural networks with i$^2$-GNNs, ICLR23.\n\n[2] Qian et al., Ordered subgraph aggregation networks, Neurips22."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7883/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7883/Reviewer_ew9j",
                        "ICLR.cc/2024/Conference/Submission7883/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697483699916,
            "cdate": 1697483699916,
            "tmdate": 1700191484967,
            "mdate": 1700191484967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9dgCJHb1uS",
                "forum": "gppLqZLQeY",
                "replyto": "dJAHUnMCRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer ew9j (1/4)"
                    },
                    "comment": {
                        "value": "We are delighted to see the Reviewer recognized the soundness of our paper while appreciating the presentation of our work. At the same time, the Reviewer raised important points we address in the following.\n\n**Q1**: The proposed method is overall an extension to the 1-MLE method in the OSAN, which is not a big problem to me.\n\n**A1**: We thank the Reviewer for their comment, this gives us the opportunity to further clarify why we believe Policy-Learn is significantly different from OSAN. Our work is primarily motivated by scenarios where a subset of subgraphs is sufficient for maximal expressive power, while the I-MLE method in OSAN aims to make higher-order generation policies (where each subgraph is generated from tuples of nodes) practical. Furthermore, our method generates the bag sequentially, compared to their one-shot generation, and we parameterize the selection network using a Subgraph GNN, that is more expressive than the MPNN used in OSAN. Finally, we do not rely on I-MLE but instead use the simpler implementation of a Gumbel-Softmax trick to enable gradient backpropagation through the discrete sampling process. We proved that these choices lead to a framework that can learn subgraph distributions that cannot be expressed by OSAN.\n\n**Q2**:  How well does the proposed method compare to the full-bag version? Can it achieve the same expressive power as the full-bag version and in how much $T$ can it be from a theoretical perspective?\n\n**A2**: We thank the Reviewer for asking this important question. In certain cases, Policy-Learn can achieve the same expressive power of the full-bag, but with a significantly fewer number of subgraphs.\n\nThis is the case for CSL graphs, where only one subgraph is sufficient to achieve the same expressive power of the full bag, as we showed in the paper (Figure 1). This result can be extended to other exemplary non-isomorphic WL-indistinguishable pairs, for example, the pair where one graph consists of two triangles connected by an edge and the other of two squares sharing an edge (Figure 1 in Bevilacqua et al., 2022). Additionally, one subgraph is also sufficient to distinguish strongly-regular graphs of different parameters, which are distinguishable by the full-bag but not by 1-WL.\n\nMore interestingly, in certain cases only a _careful selection_ of subgraphs can lead to the same expressive power of the full bag, and our method can provably implement these selections. An example of this case is the family of (n, $\\ell$)-CSL graphs where only $\\ell$ subgraphs are required to attain the expressive power of the full-bag version, which instead employs $n \\cdot \\ell$ subgraphs. This example can further be generalized to other families of graphs, obtained from any collection of WL-indistinguishable graphs that become distinguishable when marking any node, e.g., strongly regular graphs of different parameters. Each graph in these families is created by considering $\\ell$ disconnected, non-isomorphic instances in the corresponding collection. \n\n**Q3**:  Can the proposed method maintain its ability to encode sub-structures?\n\n**A3**: The Reviewer is raising an interesting point of discussion. The ability of Subgraph GNNs to count substructures can be attributed to the marking of the root nodes, which breaks the anonymity of the marked nodes and allows counting around each marked node (Huang et al., 2023). Interestingly, however, it is possible to show that it is not necessary for a node to be marked in order to count substructures around it: it is sufficient instead that its embedding is unique in its neighborhood. This might happen not only when the node is marked, but also when it receives marking information in the message passing, coming from a root node, and this information is still unique in its surroundings. This implies that our method can exhibit the same counting power of the full-bag whenever each node has an embedding that is unique in its neighborhood, due to the marking information coming from a few selected subgraphs. Note, however, that this might not always be the case, depending on the structure of the graph. For example, it is possible that certain parts of the graph do not see any marking information if we select a small number of subgraphs. This might happen to nodes that are distant from the marked roots of the selected subgraphs, around which a Subgraph GNN necessarily behaves as an MPNN.\n\nWe believe that this marks a first step towards the understanding of the counting ability of Subgraph GNNs that rely on a smaller number of subgraphs. However, properly enquiring about this aspect and designing Subgraph GNNs with counting guarantees when considering a subset of subgraphs fall outside the scope of the present work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180347120,
                "cdate": 1700180347120,
                "tmdate": 1700188080228,
                "mdate": 1700188080228,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bWC1pFzI9W",
                "forum": "gppLqZLQeY",
                "replyto": "dJAHUnMCRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer ew9j (2/4)"
                    },
                    "comment": {
                        "value": "**Q4**: Some commonly used synthetic datasets for comparing expressive power are missing (e.g. EXP [2], CSL [3])\n\n**A4**: We welcome the Reviewer's suggestion and additionally experimented with the CSL and the EXP datasets, as shown in the following.\n\n\n| Method                         |             CSL (ACC)            |             EXP (ACC)           | \n|--------------------------------|:----------------------------------:|:-----------------------------------:|\n|   GIN                             |          10.00 +- 0.0              |            51.2 +- 2.1              | \n|                                      |                                           |                                            |\n|   FULL                          |        100.00 +- 0.0              |         100.00 +- 0.0             | \n|   RANDOM $T = 2$      |        100.00 +- 0.0              |           89.92 +- 2.5             | \n|   Policy-Learn $T = 2$  |        100.00 +- 0.0              |         100.00 +- 0.0             | \n\nOn the CSL dataset, both RANDOM and Policy-Learn achieve 100% test accuracy, aligning with our theoretical analysis at the beginning of Section 4, where we discussed the disambiguation of these graphs.\n\nOn the EXP dataset, Policy-Learn surpasses the performance of RANDOM, and achieves the same performance of the full-bag approach. This numerically confirms the importance of learning which subgraphs to select, rather than randomly sampling them. \n\n**Q5**: Some commonly used synthetic datasets for evaluating the counting power of GNNs are missing.\n\n**A5**: Following the Reviewer\u2019s suggestion, we have additionally tested Policy-Learn on synthetic datasets that measure the ability of models to count substructures. We chose the dataset in Zhengdao et al., 2020, on which full-bag Subgraph GNNs have been extensively evaluated (Zhao et al., 2022, Frasca et al., 2022). We used the dataset splits and evaluation procedure of previous work and reported the average across the seeds in the Table below.\n\n\n\n\n| Method                         |       Triangle       |         Tailed Tri.    |         Star       |        4-Cycle          | \n|--------------------------------|:---------------------:|:---------------------:|:------------------:|:------------------------:|\n|   GIN                             |       0.3569        |         0.2373         |        0.0224    |       0.2185             | \n|                                      |                          |                             |                       |                               |\n|   FULL                           |      0.0183         |        0.0170         |         0.0106   |        0.0210            |\n|                                      |                          |                             |                       |                               |\n|   RANDOM $T = 5$      |       0.1841        |         0.1259         |        0.0105    |        0.1180            |\n|   Policy-Learn $T = 5$  |       0.1658        |         0.1069         |        0.0100    |        0.0996            |\n|                                      |                          |                             |                       |                               |\n|   RANDOM $T = 8$      |       0.1526        |         0.0999         |        0.0119    |        0.0933            |\n|   Policy-Learn $T = 8$  |       0.1349        |         0.0801         |        0.0100    |        0.0793            |\n\n\nWhile Policy-Learn cannot achieve the same performance of the full-bag, it _significantly_ surpasses the random baseline. This supports our previous discussion on the counting power of our method: although counting all substructures might not be possible with a significantly reduced set of subgraphs, there exist subgraph selections that enable more effective countings. Furthermore, increasing the number of subgraphs proves advantageous in terms of the counting power."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180504323,
                "cdate": 1700180504323,
                "tmdate": 1700180504323,
                "mdate": 1700180504323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fAcknhnH2M",
                "forum": "gppLqZLQeY",
                "replyto": "dJAHUnMCRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer ew9j (3/4)"
                    },
                    "comment": {
                        "value": "**Q6**: What is the time and memory cost of the proposed method compared to the full-bag version in both the train and the test? How does the cost vary if we increase the T?\n\n**A6**: We believe that to adequately answer this question one should look at the time and space complexity of these methods, which we discuss in the following and we will include in the next manuscript revision. In short, our method scales linearly with $T$. When small values of $T$ are used, as in our paper, this results in a speedup by a factor of $n$ compared to the full-bag Subgraph GNN.\n\nWe analyze an equivalent efficient implementation of our method, which differs from Algorithm 1 in that it does not feed to the selection network the entire current bag at every iteration. Instead, it processes only the last subgraph added to the bag, and re-uses the representations of the previously-selected subgraphs, passed though the selection network in previous iterations and stored. We denote $\\Delta_\\text{max}$ the maximum node degree of the input graph.\n\n**Time Complexity.** The forward-pass asymptotic time complexity of the selection network $f$ amounts to $\\mathcal{O}(T \\cdot n \\cdot \\Delta_\\text{max})$. This arises from performing $T$ iterations, where each iteration involves selecting a new subgraph by passing the last subgraph added to the bag through the MPNN, which has time complexity $\\mathcal{O}(n \\cdot \\Delta_\\text{max})$ and re-using stored representations of previously-selected subgraphs. Similarly, the forward-pass asymptotic time complexity of the prediction network $g$ is $\\mathcal{O}((T+1) \\cdot n \\cdot \\Delta_\\text{max})$, as each of the $T+1$ subgraphs is processed through the MPNN. Note that, differently from the selection network, the prediction network considers $T+1$ subgraphs as it also utilizes the last selected subgraph.\nTherefore, Policy-Learn has an overall time complexity of $\\mathcal{O}((T + (T+1)) \\cdot n \\cdot \\Delta_\\text{max})$, i.e., $\\mathcal{O}(T \\cdot n \\cdot \\Delta_\\text{max})$. The time complexity of full-bag node-based Subgraph GNNs amounts instead to $\\mathcal{O}(n^2 \\cdot \\Delta_\\text{max})$, as there are $n$ subgraphs in total.\n\n**Space complexity.** The forward-pass asymptotic space complexity of the selection network $f$ is $\\mathcal{O}(T \\cdot n + (n +  n \\cdot \\Delta_\\text{max}))$, because we need to store $n$ node features for each of the $T$ subgraphs, and, at each iteration, the space complexity of the MPNN on the subgraph being processed is $\\mathcal{O}(n +  n \\cdot \\Delta_\\text{max})$ to store its node features and connectivity in memory. \nSimilarly, the forward-pass asymptotic space complexity of the prediction network $g$ is $\\mathcal{O}((T+1) \\cdot (n +  n \\cdot \\Delta_\\text{max}))$. Therefore, Policy-Learn has an overall space complexity of $\\mathcal{O}(T \\cdot (n +  n \\cdot \\Delta_\\text{max}))$. The space complexity of the full-bag approach instead amounts to $\\mathcal{O}(n \\cdot (n +  n \\cdot \\Delta_\\text{max}))$, since the number of subgraphs in the full bag is exactly $n$.\n\nTo grasp the impact of this reduction, consider the REDDIT-BINARY dataset where the average number of nodes per graph is approximately $n = 429$, while the number of selected subgraphs is $T=2$.  The time and space complexities are drastically reduced as $T \\ll n$, and indeed Policy-Learn can be effectively run while the full bag Subgraph GNN is computationally infeasible.\n\nFinally, we additionally report empirical runtimes on the ZINC dataset in the following. For all methods, we estimated the training time on the entire training set as well as the inference time on the entire test set using a batch size of 128 on an NVIDIA RTX A6000 GPU. \n\n| Method  |  ZINC (Train time (ms))  |  ZINC (Test time (ms)) |  ZINC (MAE) | \n|--|:--:|:--:|:--:|\n|   GIN                             |     1370.10 +- 10.79      |          84.81 +- 0.26         |    0.163 +- 0.004  |\n|                                      |                                      |                                         |                             |\n|   OSAN   $T = 2$          |      2964.46 +- 30.36    |           227.93 +- 0.21      |  0.177 +- 0.016    | \n|                            |                                      |                                         |                             |\n|   FULL                          |      4872.79 +- 14.30     |          197.38 +- 0.30        |  0.087 +- 0.003    | \n|   RANDOM $T = 2$      |     2114.00 +- 27.88      |         107.02 +- 0.22        |  0.136 +- 0.005    |\n|   Policy-Learn $T = 2$  |     2489.25 +- 9.42       |         150.38 +- 0.33         |  0.120 +- 0.003    | \n\nDuring training, the runtime of Policy-Learn is significantly closer to that of the RANDOM approach than to the one of the full-bag Subgraph GNN FULL, and Policy-Learn is faster than OSAN.\n\nAt inference, Policy-Learn places in between RANDOM and FULL, and it is significantly faster than OSAN while also achieving better results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180586352,
                "cdate": 1700180586352,
                "tmdate": 1700180586352,
                "mdate": 1700180586352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tEycFW77pv",
                "forum": "gppLqZLQeY",
                "replyto": "dJAHUnMCRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer ew9j (4/4)"
                    },
                    "comment": {
                        "value": "**Q7**: Could the proposed method be generalized to other policies like edge-based [1] or node-tuple-based [2] subgraphs?\n\n**A7**:  Yes, we can extend our approach to these cases. As for edge-based policies, we can employ the same architecture and output a probability distribution of the edges at the last layer, by using any GNN layer coupled with a final predictor to output edge features. Regarding node $k$-tuples, one option would be to output a distribution on the space of $k$-tuples. However, for $k>2$ this may become impractical in large graphs. Alternatively, nodes in each tuple can be sampled sequentially based on a probability distribution over the nodes, conditioned on previous selections. This strategy can be easily implemented within our framework and it is scalable to large $k$ values even on large graphs.\n\n\n\n\n**References**\n\nBevilacqua et al., 2022. Equivariant Subgraph Aggregation Networks. ICLR 2022\n\nHuang et al., 2023. Boosting the cycle counting power of graph neural networks with I$^2$-GNNs, ICLR 2023\n\nZhengdao et al., 2020. Can Graph Neural Networks Count Substructures? NeurIPS 2020\n\nZhao et al, 2022. From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness. ICLR 2022\n\nFrasca et al., 2022. Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries. NeurIPS 2022"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180734650,
                "cdate": 1700180734650,
                "tmdate": 1700188490744,
                "mdate": 1700188490744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wFmM79J0tP",
                "forum": "gppLqZLQeY",
                "replyto": "tEycFW77pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_ew9j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_ew9j"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the replies."
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their detailed replies to all my concerns. After carefully reading the reviews and the response from all reviewers including mine, I think the authors have addressed most of my concerns and I am happy to increase my score towards acceptance. \n\nMeanwhile, I have some additional comments:\n\n**Just a clarification for Q1.** When I say the proposed method is an extension to 1-MLE in OSAN, I mean an extension in terms of the selection principle and power. It is easy to see that 1-MLE is equivalent to random sampling when the graph is 1-WL indistinguishable (as pointed out in A2 to reviewer VjjR).  The main reason lies in its based encoder is just 1-WL powerful and all subgraphs are sampled at the same time. Therefore, it is a natural thought to extend it by trying iterative sampling, which is the main reason why the proposed method is more powerful than 1-WL. \n\nThis also brings out the main point from my perspective: designing a sampling method that is more powerful than random sampling is relatively easy. However, how to design a sampling method that can work optimally or nearly optimally in terms of distinguishing ability to all graph classes is hard. Such a property requires a much deeper theoretical analysis. That would be my expectation for a great paper in this line of research. \n\nFrom the theoretical perspective, I would encourage the author to explore the expressive power of the proposed method on more general graph classes in the future, like planar graphs [1]. From the implementation perspective, I am glad to see the discussion from authors about the generalization to more common cases like edge-based or k-node-tuple and would encourage authors to explore it in the future. Meanwhile, there is a very recent work that has a similar motivation and designed a method that is generalizable to the k-node-tuple case [2]. I think the authors could further check and discuss it in the future version.  \n \n**One minor question**: Is the reported time comparison a result of your newly discussed implementation or the original one?\n\nReferences:\n\n[1] Dimitrov et al., PlanE: Representation Learning over Planar Graphs, NeurIPS 2023.\n\n[2] Kong et al., MAG-GNN: Reinforcement Learning Boosted Graph Neural Network, NeurIPS 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191442839,
                "cdate": 1700191442839,
                "tmdate": 1700191442839,
                "mdate": 1700191442839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1hGKGMWkxv",
            "forum": "gppLqZLQeY",
            "replyto": "gppLqZLQeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_w3Fw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_w3Fw"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on learning effective subgraph selection policies for subgraph GNNs. In particular, it is inspired by an observation that only a small number of subgraphs are needed to differentiate a family of non-isomorphic graphs called the CSL graph. Based on the observation, it constructs a learning-based subgraph selection policy and surpasses previous works on various benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Generally speaking, I like the efforts on subgraph sampling since the efficiency of subgraph GNNs limits them from being applied in real-world scenarios. In addition, the work is generally motivated and well-written."
                },
                "weaknesses": {
                    "value": "1. Most subgraph sampling strategies face a problem: they cannot guarantee permutation invariance, i.e., generate the same representation for the same graph no matter how the graph is permuted. It seems that the proposed method also cannot guarantee such property as well.\n\n2. I appreciate the efforts in distinguishing the CSL graphs. However, CSL graphs are just a family of regular graphs that cannot be differentiated by 1-WL. Have you analyzed some other families, for example, the strongly regular graphs proposed in (Bodnar et al, 2021b) or some pairs of graphs that are mentioned in (Wang and Zhang 2023)?\n\n3. It seems that the time evaluation is only provided in Table 7, where the time of the full subgraph GNN is not provided due to OOM. I recommend adding time evaluation on more datasets, and reporting the time of \"policy learn\", \"random selection\", the full subgraph GNN, and MPNNs. For example, you can report the time of GIN, OSAN, FULL, RANDOM, and POLICY-LEARN on ZINC.\n\n(Wang and Zhang 2023) Wang Y, Zhang M. Towards Better Evaluation of GNN Expressiveness with BREC Dataset. arXiv, 2023."
                },
                "questions": {
                    "value": "Definition 2 and Theorem 1 could possibly lead to some misunderstandings. For example, from my perspective, if the multiset $\\\\{k_i| i\\in \\\\{1,\\dots, l \\\\}\\\\}$ is the same, then $CSL(n,(k_1, \u2026, k_l))$ should be isomorphic to each other. The observation should be pointed out, since the definition now seems that the sequence of k_i might also lead to non-isomorphism. In addition, the fact that $CSL (n, k)$ is isomorphic to $CSL (n, n-k)$ would also influence the isomorphism between graphs, which also need to be mentioned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586084361,
            "cdate": 1698586084361,
            "tmdate": 1699636967025,
            "mdate": 1699636967025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "woe5IIcF3b",
                "forum": "gppLqZLQeY",
                "replyto": "1hGKGMWkxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer w3Fw"
                    },
                    "comment": {
                        "value": "We are thankful to the Reviewer for their constructive comments. We are pleased to notice they have found our work well-motivated and well-written. We proceed by addressing the comments in the following.\n\n**Q1**: Most subgraph sampling strategies face a problem: they cannot guarantee permutation invariance [..]. It seems that the proposed method also cannot guarantee such property as well.\n\n**A1**:  Our architecture is invariant as a function. However, the use of non-deterministic sampling makes the input to the network stochastic and this may result in different representations for the same graph, similar to other approaches that involve randomness in the GNN computation. In practice, however, the non-deterministic nature of the sampling does not seem to hurt the performance, as can be seen in the experimental section.\n\n**Q2**: I appreciate the efforts in distinguishing the CSL graphs [..]. Have you analyzed some other families, for example, the strongly regular graphs proposed in (Bodnar et al, 2021b) or some pairs of graphs that are mentioned in (Wang and Zhang 2023)?\n\n**A2**: Our theoretical analysis can indeed be generalized to other families of graphs. Let $n$ and $\\ell$ be natural numbers, and denote by $\\mathcal{G}_n$ any set of non-isomorphic yet 1-WL indistinguishable graphs with $n$ nodes that are distinguishable when marking any single node. For example, $\\mathcal{G}_n$ can be the set of strongly-regular graphs of different parameters, as suggested by the Reviewer. We define (n, $\\ell$)-$\\mathcal{G}_n$ to be the family of non-isomorphic graphs, where each graph is obtained by $\\ell$ disconnected non-isomorphic instances of $\\mathcal{G}_n$. Then, all our theorems are still valid for (n, $\\ell$)-$\\mathcal{G}_n$. We will discuss these additional families in the next version of our manuscript. \n\n**Q3**: I recommend adding time evaluation on more datasets, and reporting the time of \"policy learn\", \"random selection\", the full subgraph GNN, and MPNNs. For example, you can report the time of GIN, OSAN, FULL, RANDOM, and POLICY-LEARN on ZINC.\n\n**A3**: We welcome the Reviewer\u2019s suggestion and we report the runtime of these methods on the ZINC dataset in the following. For all methods, we estimated the inference time on the entire test set using a batch size of 128 on an NVIDIA RTX A6000 GPU. \n\n\n| Method                         |      ZINC (Time (ms))          |        ZINC (MAE)            | \n|--------------------------------|:-----------------------------------:|:-------------------------------:|\n|   GIN                             |      84.81 +- 0.26                |       0.163 +- 0.004         |\n|                                      |                                           |                                       |\n|   OSAN   $T = 2$          |       227.93 +- 0.21             |      0.177 +- 0.016          | \n|                                      |                                           |                                       |\n|   FULL                          |        197.38 +- 0.30            |      0.087 +- 0.003          | \n|   RANDOM $T = 2$      |       107.02 +- 0.22            |      0.136 +- 0.005           |\n|   Policy-Learn $T = 2$  |       150.38 +- 0.33            |      0.120 +- 0.003           | \n\n\nNotably, Policy-Learn is significantly faster than OSAN, while also obtaining better results. Importantly, Policy-Learn places in between the RANDOM approach and the full-bag Subgraph GNN FULL. Furthermore, Policy-Learn takes around 2x the time of the corresponding GIN baseline, but significantly outperforms it.\n\n**Q4**: Definition 2 and Theorem 1 could possibly lead to some misunderstandings. For example, from my perspective, if the multiset $\\\\{ k_i \\vert i \\in \\\\{1, \\ldots , \\ell \\\\} \\\\}$ is the same, then CSL(n, ($k_1, \\ldots, k_{\\ell}$)) should be isomorphic to each other. The observation should be pointed out, since the definition now seems that the sequence of $k_i$ might also lead to non-isomorphism. \n\n**A4**: The Reviewer is right in their understanding, and for this reason in the paper we always remark that we consider the family of *non-isomorphic* (n, $\\ell$)-CSL graphs, where these cases are excluded. However, we will follow the Reviewer\u2019s recommendation and in the next revision of the manuscript we will make this point clearer."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700180163669,
                "cdate": 1700180163669,
                "tmdate": 1700183043758,
                "mdate": 1700183043758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tRfEoxL4Hj",
                "forum": "gppLqZLQeY",
                "replyto": "woe5IIcF3b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_w3Fw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_w3Fw"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their response, which has largely addressed my initial concerns. \n\nAfter a meticulous examination of the feedback provided by other reviewers, I have an additional comment: the proposed learning-based sampling strategy, while significantly outperforming the random sampling strategy, is not optimal enough. Take the CSL family as an example, it would be more effective to sample a subgraph from each connected component and employ these samples to distinguish between non-isomorphic graphs. The strategy can also be applied to many other families of regular graphs, where the nodes are treated as the same. Therefore, a potential future work could involve predefining a set of subgraphs at the initial stage, with the aim of reducing the number of steps required in the learning process. This approach could potentially streamline the strategy and enhance its efficiency."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222837940,
                "cdate": 1700222837940,
                "tmdate": 1700222837940,
                "mdate": 1700222837940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SjMiaHv7IP",
            "forum": "gppLqZLQeY",
            "replyto": "gppLqZLQeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
            ],
            "content": {
                "summary": {
                    "value": "**TLDR**: The paper proposes a learnable subsampling policy to reduce the number of sampled subgraphs in subgraph GNNs.\n\nSubgraph GNNs refer to the family of message-passing graph neural networks which sample subgraphs to improve their expressive power. The paper introduces a new subsampling strategy, _Policy-Learn_ which aims to reduce the number of subgraphs needed for subgraph GNNs. _Policy-Learn_ consists of two subgraph GNNs, one selection network $f$ and one policy network $g$: $f$ learns a distribution over the nodes of the input graph to select which subgraph to sample next, $g$ takes the sampled graphs as input and performs a prediction task. The paper motivates _Policy-Learn_ by considering the graph class of $(n,\\ell)$-CSL graphs, where one instance consists of $\\ell$ disconnected, non-isomorphic CSL graphs on $n$ nodes. For $(n, \\ell)$-CSL graphs, _Policy-Learn_ only needs to sample $\\ell$ subgraphs in comparison to random subsampling and the existing subsampling strategy OSAN. In an experimental evaluation, _Policy-Learn_ outperforms OSAN and is, on average, competitive with the presented baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed subsampling strategy _Policy-Learn_ is well-motivated and novel.\n* _Policy-Learn_ can provably sample subgraphs such that non-isomorphic instances can be distinguished for one specific graph class ($(n, \\ell)$-CSL graphs) on which 1-WL fails.\n* In the experimental evaluation, _Policy-Learn_ is competitive with most baseline methods and outperforms OSAN"
                },
                "weaknesses": {
                    "value": "* Theoretical limitations: While the presented theoretical results are novel and interesting, they also appear to be limited. The artificially constructed graph class $(n, \\ell$)-CSL is 1-WL indistinguishable; however, higher-order models and GNN variants are able to distinguish them. A more comprehensive analysis of the expressive power of _Policy-Learn_ could strengthen the contribution significantly.\n* Clarity: Although the paper is generally well-written, more precise language would improve readability:\n     * \"[...] preventing the applicability of Subgraph GNNs on important datasets\" -> What datasets are important?\n    * \"Contributions: [...] An experimental evaluation of the new approach demonstrating its advantages.\" -> It would be more informative if the advantages are specified.\n    * \" [...] which includes feature aggregation in light of the alignment of nodes [...]\" -> Could you specify what that means\n    * \"[...] and demonstrate that our framework performs better on real-word datasets\" -> Better than what?"
                },
                "questions": {
                    "value": "1. **Expressiveness**: _Policy-Learn_ can distinguish non-isomorphic instances in the graph class $(n, \\ell)$-CSL, which are indistinguishable by 1-WL. What about the opposite? Can you characterize graph classes whose non-isomorphic instances are provably indistinguishable by _Policy-Learn_? Are there graph classes where OSAN is stronger than _Policy-Learn_? Is _Policy-Learn_ limited by higher-order WL?\n2. **Assumption in proof of Theorem 4**: Is the (necessary) assumption that $f$ has $n$ layers feasible for larger graphs? Do you have experimental results on $(n, \\ell)$-CSL graphs?\n3. **Extension of theoretical results**: Have you thought about extensions of your theoretical results, e.g., other graph classes where marking any node in a graph is sufficient or where marking a limited number of nodes is sufficient?\n4. **Experiments**:\n\n    a. How did you choose the values for $T$ (2 and 5 in Tables 1 and 3, 2 and 20 in Table 2)?\n\n    b. In Section 7, paragraph ZINC: \"Notably, OSAN performs worse than our random baseline due to differences in the implementation of the prediction network\". Could you elaborate on the differences and why this affects the performance?\n5. **Time comparison**: How does _Policy-Learn_ compare with respect to time vs. prediction performance in comparison to more expressive GNNs (e.g., GSN, CIN)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7883/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR",
                        "ICLR.cc/2024/Conference/Submission7883/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785326050,
            "cdate": 1698785326050,
            "tmdate": 1700735108453,
            "mdate": 1700735108453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tED24LLsTf",
                "forum": "gppLqZLQeY",
                "replyto": "SjMiaHv7IP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer gwbR (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the fact that the Reviewer highlighted the novelty of our method and our theoretical results in their feedback. At the same time, the Reviewer raised important points we address in the following.\n\n**Q1**:  Clarity: Although the paper is generally well-written, more precise language would improve readability.\n\n**A1**: We thank the Reviewer for their suggestions, and we will make sure to include them in the next revision of the paper.\n\n**Q2**:  Expressiveness: Can you characterize graph classes whose non-isomorphic instances are provably indistinguishable by Policy-Learn? Are there graph classes where OSAN is stronger than Policy-Learn? Is Policy-Learn limited by higher-order WL?\n\n**A2**: This is a very interesting aspect and we thank the Reviewer for bringing it up. \n\nInterestingly, we can now prove that Policy-Learn can distinguish any strongly regular graphs of different parameters, but, just like 3-WL, cannot distinguish any strongly regular graphs of the same parameters. This implies that Policy-Learn is not as powerful as 4-WL.\n\nImportantly, there are no graph classes where OSAN is stronger than Policy-Learn. Specifically, for any instantiation of OSAN, there exists a set of weights for Policy-Learn such that it outputs exactly the same probability distribution. In other words, Policy-Learn can parameterize all the probability distributions that OSAN can. The contrary, however, is not true: there exist probability distributions that Policy-Learn can parameterize but OSAN cannot.\n\nWe will expand this expressivity analysis in the next revision of the manuscript. \n\n**Q3**: Assumption in proof of Theorem 4: Is the (necessary) assumption that f has n layers feasible for larger graphs?\n\n**A3**: We thank the Reviewer for bringing this point up to our attention. First, we would like to remark that the number of nodes in a (n,$\\ell$)-CSL graph is $n \\cdot \\ell$, which is much larger than $n$, the value we used as a loose bound on the number of layers in the proof. However, we understand the Reviewer\u2019s concern, and we have therefore now proved that the bound can be tightened to $n/2$ by considering the circular structure of the connected components. We believe that this bound can be further reduced by taking into account the skip connections. \n\n**Q4**: Extension of theoretical results: Have you thought about extensions of your theoretical results, e.g., other graph classes where marking any node in a graph is sufficient or where marking a limited number of nodes is sufficient?\n\n**A4**: In the paper, we showed that for CSL graphs it is sufficient to mark any single node for distinguishability (Figure 1). However, this result can be extended to other examples of  non-isomorphic WL-indistinguishable pairs, for example, the pair where one graph consists of two triangles connected by an edge and the other of two squares sharing an edge (Figure 1 in Bevilacqua et al., 2022). Additionally, marking any node is also sufficient to distinguish strongly-regular graphs of different parameters.\n\nImportantly, it is also possible to consider other families of graphs beyond the (n, $\\ell$)-CSL graph family where marking a limited number of nodes is sufficient for disambiguation. Indeed, our theoretical results are valid for any family of graphs, as we describe next, obtained from any collection of WL-indistinguishable graphs that become distinguishable when marking any node, e.g., strongly regular graphs of different parameters. Similarly to the construction of the (n, $\\ell$)-CSL family in our paper, each graph in these families is created by considering $\\ell$ disconnected, non-isomorphic instances in the corresponding collection. Finally, we believe that it is possible to extend this analysis to certain families of connected graphs, and we leave this aspect for future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179741531,
                "cdate": 1700179741531,
                "tmdate": 1700187639269,
                "mdate": 1700187639269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ui50Ez8HKQ",
                "forum": "gppLqZLQeY",
                "replyto": "SjMiaHv7IP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer gwbR (2/2)"
                    },
                    "comment": {
                        "value": "**Q5**: Experiments: How did you choose the values for $T$ (2 and 5 in Tables 1 and 3, 2 and 20 in Table 2)?\n\n**A5**: Experimentally, our goal was to show that it is possible to obtain compelling results on standard benchmarks even when considering a significantly smaller number of subgraphs than the total typically employed by existing methods. Therefore we chose $T=2$ and $T=5$ as two exemplary numbers, and demonstrated that even with only $T=2$ subgraphs Policy-Learn achieves very good performance. Nonetheless, we have further experimented with other values of $T$, namely $T=3$ and $T=8$, which we report in the following. \n\n\n| Method                       |          ZINC (MAE)             |\n|------------------------------|:-----------------------------------:|\n|   GIN                           |        0.163 +- 0.004            |\n|                                    |                                            |\n|   FULL                        |         0.087 +- 0.003           |\n|   RANDOM $T = 3$    |        0.128 +- 0.004           |\n|   Policy-Learn $T = 3$|         0.116 +- 0.008          |\n|   RANDOM $T = 8$    |         0.102  +- 0.003         |\n|   Policy-Learn $T = 8$|         0.097 +- 0.005          |\n\nThese results align with the observations made for other values of $T$. In particular, Policy-Learn always outperforms the random baseline, and the gap is larger when the number of subgraphs $T$ is smaller, where selecting the most informative subgraphs is more crucial. \n\n**Q6**: Experiments: In Section 7, paragraph ZINC: \"Notably, OSAN performs worse than our random baseline due to differences in the implementation of the prediction network\". Could you elaborate on the differences and why this affects the performance?\n\n**A6**:  We ran our random baseline on exactly the same prediction network that we used for Policy-Learn to ensure a direct comparison. This comprises architectural choices that are different from those taken in OSAN, including the number of layers, the embedding dimension, and the implementation of the residual connections. In our next revision, we will make this point clearer and further include the performance of the RANDOM baseline using the same prediction network considered in OSAN, as reported by Qian et al., 2022.\n\n**Q7**: Time comparison: How does Policy-Learn compare with respect to time vs. prediction performance in comparison to more expressive GNNs (e.g., GSN, CIN)?\n\n**A7**: We welcome the Reviewer\u2019s suggestion, and we report the time and the prediction performance on the ZINC dataset in the following. For all methods, we report the inference time on the entire test set using a batch size of 128. The runtime of CIN is taken from the original paper, and it is measured on an NVIDIA Tesla V100 GPU. To ensure a fair comparison, we therefore timed Policy-Learn on the same GPU type.\n\n\n| Method                         |      ZINC (Time (ms))          |        ZINC (MAE)            | \n|--------------------------------|:----------------------------------:|:--------------------------------:|\n|   GIN                             |        126.91 +- 0.82            |       0.163 +- 0.004         |\n|   CIN                             |          471.00 +- 3.00          |       0.079 +- 0.006         |\n|   Policy-Learn $T = 2$  |          235.14 +- 0.21         |      0.120 +- 0.003           | \n|   Policy-Learn $T = 5$  |           411.19 +- 0.39         |      0.109 +- 0.005          |\n\nFirst, we observe that Policy-Learn is faster than CIN, especially considering the additional preprocessing time required by CIN for the graph lifting procedures, which is not measured in the Table. \n\nSecond, although CIN outperforms Policy-Learn, it should be noted that CIN explicitly models cycles and rings, which are obtained in the preprocessing step and serve as a domain-specific inductive bias. On the contrary, the subgraph generation policy in Policy-Learn (i.e., node-marking) is entirely domain-agnostic and not tailored to the specific application. \n\nFinally, even though we did not find the runtime of GSN in the original paper, it is likely to be similar to that of GIN as GSN performs message passing on node features augmented with substructure counts. However, similarly to CIN, GSN requires an additional preprocessing time for the substructure counting that should be taken into account and also requires additional domain knowledge to choose which substructures to count. \n\n**References**\n\nBevilacqua et al., 2022. Equivariant Subgraph Aggregation Networks. ICLR 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179924836,
                "cdate": 1700179924836,
                "tmdate": 1700179924836,
                "mdate": 1700179924836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Z8KX6HyPA",
                "forum": "gppLqZLQeY",
                "replyto": "SjMiaHv7IP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response about the expressive power of Policy-Learn and the additional experiments, I am looking forward to the revised submission!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492405167,
                "cdate": 1700492405167,
                "tmdate": 1700492405167,
                "mdate": 1700492405167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LWA4sxSeWP",
                "forum": "gppLqZLQeY",
                "replyto": "SjMiaHv7IP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_gwbR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for uploading the revised manuscript. As you addressed all of my concerns, I raised my score. \n\nPlease note that you have typos in p. 19-20: \n- \"... graph Definition 2, where marking a limited number of nodes is sufficient for identifiability ...\"\n- \"Let G be a strongly regular graphs ...\"\n- \"... can be parameterize exactly ...\""
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735381450,
                "cdate": 1700735381450,
                "tmdate": 1700735381450,
                "mdate": 1700735381450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x6CRX897Ko",
            "forum": "gppLqZLQeY",
            "replyto": "gppLqZLQeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
            ],
            "content": {
                "summary": {
                    "value": "Subgraph GNNs generally refers to GNN methods that run GNNs on several subgraphs obtained from the input graph. Recently, a variety of such methods, differing from each other in the subgraph selection policies, are proposed. Of particular relevance to the current paper is the OSAN framework by Qian et al, in which the subgraph selection policy is learned. In the current paper, a more expressive GNN architecture DS-GNN is used (rather than a classical GNNs) of the subgraph selection policy. This DS-GNN provides a distribution from which is sampled using standard Gumbel softmax trick. It is shown theoretically the approach can be more powerful than the OSAN approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Research in how to learn subgraph selection policies is highly relevant in view of the popularity of subgraph GNN approaches.\n\n2. Related work is well described.\n\n3. The policy learn method is well designed.\n\n4. Theoretical guarantees over special classes of CSL graphs are presented. In particular, it is argued that the proposed approach can be stronger than a previous approach."
                },
                "weaknesses": {
                    "value": "1. It is not clearly described what gives the proposed method more power than e.g., OSAN.\n\n2. The method seems to depend on a subgraph GNN method (DS-GNN) which high computational cost."
                },
                "questions": {
                    "value": "**Q1** Could you explain the histograms in Figure 1 after labeling one vertex? \n\n**Q2** What is the ingredient of the method which results in more power than say OSAN?\n\n**Q3** Is the proposed method at least as powerful as OSAN or incomparable? What about comparisons with other subgraph formalisms?\n\n**Q4** A number of subgraphs are selected in order to reduce complexity. However, the DS-GNN method used for selection policy relies on all subgraphs? What is the overall complexity of the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7883/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819134625,
            "cdate": 1698819134625,
            "tmdate": 1699636966758,
            "mdate": 1699636966758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dmQfdfakxL",
                "forum": "gppLqZLQeY",
                "replyto": "x6CRX897Ko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer VjjR (1/2)"
                    },
                    "comment": {
                        "value": "We are glad to see that the Reviewer has appreciated the relevance of the research topic while finding our proposed method well-designed and the related work well-described. They have nonetheless raised a few questions that we address below.\n\n**Q1**: Could you explain the histograms in Figure 1 after labeling one vertex?\n\n**A1**: For each original graph, we consider the application of the WL test to the subgraph obtained by marking the crossed node. More precisely, at time step 0, each non-marked node in the subgraph is assigned the same initial constant color, which differs from the initial color of the marked node. Without loss of generality, denote these colors as color 1 and color 0, respectively. Then, the WL test proceeds by refining the color of each node by aggregating the colors of its neighbors (including itself). More precisely, the new color of a node is obtained through a hash function taking as input the multiset of colors of the neighbors and of the node itself. It is easy to see that, at time step 1, the marked node has a unique color, which we call  color 2, its neighbors all have the same colors, color 3, and all the remaining nodes have the same color (different from the marked node and its neighbors), color 4. The refinement is repeated until we reach a stable coloring. At this point, we simply collect the number of nodes having the same color in a histogram. For the upper graph in Figure 1, the histogram indicates that in the stable coloring, there are 4 blue nodes, 4 green nodes, 4 yellow nodes, and 1 orange node. Importantly, since the WL test represents a necessary condition for graph isomorphism, different histograms imply that the two graphs are not isomorphic, and thus the WL test distinguishes them. Figure 1 shows that marking only one node is sufficient for distinguishing the two graphs, which are instead indistinguishable when no node is marked, as the WL test returns the same histogram for them (13 orange nodes).\n\n**Q2**: What is the ingredient of the method that results in more power than say OSAN?\n\n**A2**: The iterative procedure that selects one subgraph at a time based on previous selections, paired with the Subgraph GNN used to implement the selection network $f$, are the ingredients that result in more power than OSAN. \n\nIndeed, recall that OSAN selects all subgraphs at the same time, by applying an MPNN over the original graph only. Consider again the family of non-isomorphic (n, $\\ell$)-CSL graphs we studied in the paper, which contains WL-indistinguishable graphs composed of disconnected copies of WL-indistinguishable (sub-)graphs. Given that OSAN uses the original graph, then it cannot differentiate between the different CSL (sub-)graphs and will return a uniform probability distribution over all the nodes. This implies that it cannot ensure that the marked nodes will belong to different CSL (sub-)graphs, which represents a necessary condition for identification (Proposition 2). On the contrary, Policy-Learn iteratively adds a new subgraph based on the subgraphs already present in the bag. Whenever a subgraph is in the bag, then all nodes in the CSL (sub-)graph of its marked root node will have a different color than the others. Thus it is always possible to distinguish nodes belonging to CSL (sub-)graphs having a marked node, and therefore the final MLP can assign these nodes a zero probability while maintaining a uniform probability over the remaining nodes. This implies that the next node to be marked will be sampled from a CSL (sub-)graph that has no marked node yet, effectively implementing the efficient policy $\\pi$ which is sufficient for identification.\n\n**Q3**: Is the proposed method at least as powerful as OSAN or incomparable? What about comparisons with other subgraph formalisms?\n\n**A3**: The proposed method Policy-Learn is _strictly more powerful_ than OSAN, although the term is more convoluted in this context due to the probabilistic nature of the sampling procedure.\n\nMore formally, for any instantiation of OSAN, there exists a set of weights for Policy-Learn such that it outputs exactly the same probability distribution. In other words, Policy-Learn can parameterize all the probability distributions that OSAN can. The contrary, however, is not true: there exist probability distributions that Policy-Learn can parameterize but OSAN cannot. An exemplary case is the one that yields to the policy $\\pi$ considered for the family of non-isomorphic (n, $\\ell$)-CSL graphs, allowing to sample all nodes from different CSL (sub-)graphs. \n\nMore broadly, it is possible to show that Policy-Learn is more expressive than 1-WL, and not as powerful as 4-WL as it cannot distinguish strongly-regular graphs of the same parameters. We will discuss these aspects more thoroughly in the next version of our manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179288251,
                "cdate": 1700179288251,
                "tmdate": 1700179288251,
                "mdate": 1700179288251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqmeIiq77j",
                "forum": "gppLqZLQeY",
                "replyto": "x6CRX897Ko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official author response to Reviewer VjjR (2/2)"
                    },
                    "comment": {
                        "value": "**Q4**: The DS-GNN method used for selection policy relies on all subgraphs? What is the overall complexity of the method?\n\n**A4**: The DS-GNN method used for the selection policy *does not* rely on all subgraphs. On the contrary, at each timestep $t$, it uses only the current bag, which contains $t$ subgraphs, to select the next subgraph to be added to the bag, starting from the original graph. This means that the number of subgraphs passed through the selection network is significantly smaller than the total number.\n\nNonetheless, we acknowledge the fact that including the computation complexity of the method would improve the quality of the manuscript. Thus, we present it below and we will include it in the next revision. \n\n\nWe analyze an equivalent efficient implementation of our method, which differs from Algorithm 1 in that it does not feed to the selection network the entire current bag at every iteration. Instead, it processes only the last subgraph added to the bag, and re-uses the representations of the previously-selected subgraphs, passed though the selection network in previous iterations and stored.\nIn what follows, we consider the feature dimensionality and the number of layers to be constants. We denote $\\Delta_\\text{max}$ the maximum node degree of the input graph, $n$ the number of its nodes and $T$ the number of selected subgraphs.\n\nThe forward-pass asymptotic time complexity of the selection network $f$ amounts to $\\mathcal{O}(T \\cdot n \\cdot \\Delta_\\text{max})$. This arises from performing $T$ iterations, where each iteration involves selecting a new subgraph by passing the last subgraph added to the bag through the MPNN, which has time complexity $\\mathcal{O}(n \\cdot \\Delta_\\text{max})$ and re-using stored representations of previously-selected subgraphs. Similarly, the forward-pass asymptotic time complexity of the prediction network $g$ is $\\mathcal{O}((T+1) \\cdot n \\cdot \\Delta_\\text{max})$, as each of the $T+1$ subgraphs is processed through the MPNN. Note that, differently from the selection network, the prediction network considers $T+1$ subgraphs as it also utilizes the last selected subgraph.\n\nTherefore, Policy-Learn has an overall time complexity of $\\mathcal{O}((T + (T+1)) \\cdot n \\cdot \\Delta_\\text{max})$, i.e., $\\mathcal{O}(T \\cdot n \\cdot \\Delta_\\text{max})$. The time complexity of full-bag node-based Subgraph GNNs amounts instead to $\\mathcal{O}(n^2 \\cdot \\Delta_\\text{max})$, as there are $n$ subgraphs in total.\n \nTo grasp the impact of this reduction, consider the REDDIT-BINARY dataset, which we have experimented on to make this point clear (Tables 2 and 7). In this dataset the average number of nodes (and thus subgraphs) per graph is approximately $n = 429$, making the full-bag Subgraph GNN infeasible. In our experiment the number of selected subgraphs is $T=2$. The time complexity is drastically reduced as $T \\ll n$, and, indeed, Policy-Learn can be effectively and successfully run."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179378431,
                "cdate": 1700179378431,
                "tmdate": 1700179378431,
                "mdate": 1700179378431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kE5LN1yzVB",
                "forum": "gppLqZLQeY",
                "replyto": "x6CRX897Ko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
                ],
                "content": {
                    "title": {
                        "value": "Comments on Authors' answers 1/2"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their clear answers to my question. For Q1, please also clarify this in the paper. For the remaining questions: When you say that it is not as powerful as 4-WL, do you mean that it is bounded by 4-WL as well?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338824010,
                "cdate": 1700338824010,
                "tmdate": 1700338965028,
                "mdate": 1700338965028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTP8QfCVaY",
                "forum": "gppLqZLQeY",
                "replyto": "x6CRX897Ko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7883/Reviewer_VjjR"
                ],
                "content": {
                    "title": {
                        "value": "Comments on Authors' answers 2/2"
                    },
                    "comment": {
                        "value": "Thanks for explaining that your approach does not use all subgraphs for policy learning. Please add the computational complexity part to the paper (or supp material)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7883/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338917467,
                "cdate": 1700338917467,
                "tmdate": 1700338956350,
                "mdate": 1700338956350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]