[
    {
        "title": "Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction"
    },
    {
        "review": {
            "id": "x2xzT86hLy",
            "forum": "aFWUY3E7ws",
            "replyto": "aFWUY3E7ws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes Global-Local Identification and Prediction (GLIP) for time series forecasting. The proposed method moves away from deep learning (often transformer-based) methods, instead utilising lightweight approaches that can be run on a CPU (system identification and Fourier transformation). This has the further advantage of making training time (mostly) independent of the prediction horizon. In an evaluation over four datasets, GLIP shows improved performance for both MSE and MAE compared to seven other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The proposed method is a strong combination of global prediction (forecasting the entire time series) with local information (rolling batches). As shown in Figure 2, the method is able to weight global/local information when most appropriate. Results compared to baselines in Table 1 are strong across all output lengths and datasets. This is especially impressive given the speed of the method (\"seconds to minutes\")."
                },
                "weaknesses": {
                    "value": "**Clarity**  \nC1. Inline citations are not in brackets, which sometimes makes the text hard to follow.  \nC2. Acronyms are often not defined.  \nC3. There are some typos throughout the work.  \n\n**Quality**  \nQ1. Percentage improvement is only given for MSE. Additional analysis of percentage improvement for MAE would be helpful (potentially in the appendix).   \nQ2. I have a small concern regarding data leakage from the test set. I believe the test batch information is only used for local prediction (constructing the storage basis), but in Figure 1 there is an arrow going from the test batch back into the global prediction (predicted length). Could the authors please verify that there is no data leakage that would affect the global prediction/overall results."
                },
                "questions": {
                    "value": "1. How are values for $\\alpha$ and $\\beta$ selected? The work explains the what high/low values mean and when they should be chosen, but how were they determined in practice?   \n2. Section 4.1.3 gives a vague measure of \"seconds to minutes\" for running the experiments. Are the authors able to provide concrete training/inference times for GLIP, and compare this to the same for the baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698146246518,
            "cdate": 1698146246518,
            "tmdate": 1699636256208,
            "mdate": 1699636256208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OlBaL98Gif",
                "forum": "aFWUY3E7ws",
                "replyto": "x2xzT86hLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments from Reviewer fA8m"
                    },
                    "comment": {
                        "value": "Thanks for valuable comments and positive support. Below are the responses point by point:\n> **_Clarity_**\n* **C.1 Inline citations are not in brackets:** We have added brackets for each citations to improve the clarity.\n\n* **C.2 Acronyms are often not defined:** We have included the full names of relatively less familiar models such as TRMF (Temporal Regularized Matrix Factorization) and TLAE (Temporal Latent Auto-Encoder) in the manuscript when they first occured. For commonly used models like RNN, SVM, LSTM, GRU, etc., due to space constraints, we still keep using the abbreviations.\n\n* **C.3 Some typos throughout the work:** We have thoroughly checked the entire manuscript to reduce typos and improve presentation. For example, modifying the formula (3) in section 3.1.1, rectifying instances in Chapter 3 where \"training set\" was erroneously written as \"test set,\" and resolving language issues in section 4.1.4. Moreover, corrections have been made for some capitalization and spelling errors.\n\n\n> **_Quality_**\n* **Q.1 Percentage improvement of MAE:** Thanks for suggestion. We have put them in Appendix D.3. \n\n* **Q.2 Concern regarding data leakage:** We apologize for the confusion caused by the incorrect use of arrows in Figure 1. The arrow associated with \"Prediction Length\" indicates that we input the length of the local prediction into the training set. Subsequently, we perform the DFT on the data in the training set with this specified length, forming a basis for storage. This involves only inputting the length value, for instance, predicting 96 steps would entail inputting the value 96, without including any values from the test set. Hence, there is no issue of data leakage. We have modified the arrows labeled \"Prediction Length\" to indicate directly from the dataloader since the data loader can provide the output length. We believe this reduces any further misunderstandings.\n\n> **_Questions:_**\n* **1. Hyperparameters:** In Appendix D.4, we have provided a detailed explanation of the hyperparameter selection process. The hyperparameters $\\alpha$ and $\\beta$ are conditionally tuned. When the mean difference between global predictions and input exceeds the half range of the input data, indicating a significant global prediction error that may affect local predictions, we set $\\alpha=0.8, \\beta=0.9$. Otherwise, we set $\\alpha=0.1, \\beta=0.9$. There are two benefits to this approach to adjust $\\alpha$ and $\\beta$. On the one hand, $\\alpha$ and $\\beta$ are not very sensitive to prediction results, so they do not require too much fine-tuning, since conditional tuning process makes it efficient. On the other hand, it prevents the disruption of local predictions by bad global prediction results.\n\n* **2. Concrete times for GLIP:** Specific computation time for GLIP and neural networks is as follows. For instance, on the ETTh dataset, the computation time is as follows. The unit after the values is second.\n\n| Prediction Length | GLIP | DLinear | Autoformer |\n|:---------------:|:--:|:-:|:-:|\n|      96      |  116s  |  189s  |  299s  |\n|      192     |  122s  |  216s  |  325s  |\n|      336     |  138s  |  239s  |  378s  |\n|      720     |  139s  |  244s  |  412s  |\n\nHere, GLIP operates on a CPU enviornment, whereas the other two neural network models were run with support of GPU, Nvidia RTX A6000. Even under such conditions, our processing speed remains superior to that of neural networks. This illustrates the advantage of GLIP from the prespective of running time."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145555277,
                "cdate": 1700145555277,
                "tmdate": 1700145555277,
                "mdate": 1700145555277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YjGhjbzzqJ",
                "forum": "aFWUY3E7ws",
                "replyto": "OlBaL98Gif",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up to Rebuttal for Reviewer fA8m"
                    },
                    "comment": {
                        "value": "Thank you the response to my review. My questions have been answered and appropriate adjusts to the work have been made.\n\nI do have one minor follow up regarding the computation time. I understand the advantage of using GLIP in a CPU environment (removing the need for GPUs). However, could further improvements in computation time be achieved by adapting the method to run on a GPU? I'm curious if the authors think even faster computation of GLIP is possible (but this is not a necessity for the work). \n\nThank you again for your response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478156064,
                "cdate": 1700478156064,
                "tmdate": 1700478156064,
                "mdate": 1700478156064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZuumkeToiL",
                "forum": "aFWUY3E7ws",
                "replyto": "Xgjoy83BQM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_fA8m"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the quick response. I assumed that would be the case, but thought it worth clarifying."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491295089,
                "cdate": 1700491295089,
                "tmdate": 1700491295089,
                "mdate": 1700491295089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ley8zc3OYO",
            "forum": "aFWUY3E7ws",
            "replyto": "aFWUY3E7ws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_jLT6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_jLT6"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a time series forecasting approach based on sparse identification of global, storage, and local basis functions.\nIn contrast to the recent transformer-based advances, this approach runs on CPU and is computationally highly competitive to other models. It also outperforms other Transformer-based methods by a large margin.\n\nOverall, it integrates knowledge about the challenges in time series forecasting (concretely, incorporating global and local aspects of time series datasets). This is further highlighted by additional design decisions like extracting local basis functions, through a prediction-data weighted input time series to mitigate outliers and data-variations. \nAn ablation study stresses the importance of each of the designed components and justifies each aspect of the complex model design (Fig 1).\n\nIn my opinion and after reading the method and appendix carefully, I see this as an excellent paper that demonstrates how to obtain state-of-the-art results in an orthogonal way to mainstream Transformer deep-learning approaches recently proposed.\n\nWhile the overall presentation is complete, well justified, and technically sound, the language seems over complicated at places and could/should be revised. For instance, the sentence \n> \"The parameters \u039e, procured through the sparse identification process, exhibit the capacity to signify the intensity of latent cycles and the magnitude of inter-variable influences. By virtue of these parameters, a more profound comprehension of the model can be attained. \"\n\ncould be rewritten in simpler (and more understandable) English by\n\n> \"The parameters \u039e indicate the cycles and magnitudes between variables, which can be used directly to interpret the model.\"\n\nSimilarly, pointing to individual panels in Figure 1 while describing the method would also greatly support the understanding of the method.\nThe current version describes the process in text and just references the figure as a whole."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* strong state-of-the-art performance across different forecasting benchmarks\n* strong computational efficiency (without GPUs)\n* extensive ablation study to justify each of the (many) components in the approach"
                },
                "weaknesses": {
                    "value": "* complex language makes the text more arduous to understand than necessary\n* no details on hyperparameter tuning provided in the appendix."
                },
                "questions": {
                    "value": "* what do the authors mean exactly with \"reasonable\" in their statement: code is available upon **reasonable** request? Will the source code be published on GitHub? If not, what is the motivation behind not providing the code open source?\n* eq 3: how many higher-order polynomials ($X^2$, $X^3$, etc) were integrated in the global features?\n* Can the authors please provide a detailed description of how the hyperparameters were obtained? The appendix/SI does not provide any information in this regard. Were they manually set or obtained by random search? What was the training/validation/evaluation split? How long did the hyperparameter tuning process take? How sensitive is the overall model to the choice of parameters? Were one set of hyperparameter set for every dataset or were some identical hyperparameters used across all benchmark datasets? This information is not in the current appendix!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Reviewer_jLT6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586407346,
            "cdate": 1698586407346,
            "tmdate": 1699636256114,
            "mdate": 1699636256114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kMilxO2l2A",
                "forum": "aFWUY3E7ws",
                "replyto": "Ley8zc3OYO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments from Reviewer jLT6"
                    },
                    "comment": {
                        "value": "Thanks for valuable comments and positive support. Below are the responses point by point:\n> **_Weakness_**\n\n* **W.1 Language issues:** We have carefully checked the language issues in subsection 4.1.4. Additionally, we have rephrased the cotents in Chapter 3. To enhance clarity in the new version, we have added multiple citations referring to individual panels in Figure 1 in subsections 3.1.1, 3.1.2 and 3.3.2. Moreover, we have included pseudocode for the entire algorithm in Appendix. E to facilitate a better understanding for the readers.\n\n* **W.2 Details on hyperparameter tuning:** We have relocated the hyperparameter tuning process to Appendix D. 4. For specific questions regarding certain hyperparameters, we have provided additional answers to Q.3 below.\n\n> **_Questions:_**\n\n\n* **Q.1 Code:** Open-source code has been provided in the supplementary material. It is convenient for users to reproduce the experiments. Thus, we have deleted the sentence, \"code is available upon reasonable request\" in the new version.\n\n* **Q.2 The order of polynomials in global features:** In our manuscript, we just provided up to second-order polynomial relationships, as explained in detail in Appendix D.4. This is due to the fact that excessively high-order polynomial relationships may lead to the issue of dimensionality explosion and overfitting.\n\n* **Q.3 Detailed description of hyperparameters:** In Appendix D.4, we have supplemented explanations for the selection of most hyperparameters. GLIP has three parameter tuning modes based on different parameter attributes. Below are the point-by-point responses to the questions:\n1.  The first mode involves keeping the hyperparameters unchanged. Most hyperparameters can be directly determined before model training and do not require further tuning. For instance, the order of $\\mathbf{\\Theta}_g$ is generally set to be 2, and $k_1=k_2=0.8, \\gamma =1$. \n2. The second mode is the conditional tuning method. It adjusts hyperparameters based on the difference between the input data and global predictions, i.e., $\\alpha, \\beta$. When the mean difference between global predictions and input exceeds the half range of the input data, indicating a significant global prediction error that may affect local predictions, we set $\\alpha=0.8, \\beta=0.9$. Otherwise, we set $\\alpha=0.1, \\beta=0.9$. The setting for this parameter varies across different datasets. While, in fact, even if one adjusts the above two modes of hyperparameters, the impact on predictions is not sensitive (sensitivity experiments will be discussed in point 6 here).\n3. The third mode is the adaptive tuning method which is also the main parameter to adjust, i.e., $\\gamma$ in $l_1$ regularization. Considering that $\\gamma$ being too large or small will not yield good predictive results, we define a feasible range for $\\gamma$, such as $[1e-7, 1e-1]$. We employ binary search on the validation set to find a potentially optimal $\\gamma$ value. The setting for this parameter varies across different datasets.\n4. The ratios for the training set, validation set, and test set were mentioned at the beginning of Chapter 3, and they are seperated in  proportion of 7:1:2. The original content is as follows: \"Analogous to deep learning methodologies, the time series will be partitioned into distinct sets, which are training set, validation set, and testing set, with a partitioning ratio of 7:1:2.\"\n5. Due to the small size of the validation set and fast speed of GLIP, hyperparameter determination usually takes less than one minute.\n6. The given hyperparameters are not sensitive to variations in the data. Regarding the Exchange data, we modified the values of $\\alpha, \\beta,\\gamma$, and the results of the sensitivity experiment are as follows (The values in the table represents MAE/MSE.):\n\n| hyperparameters | $\\alpha$ | $\\beta$ | $\\gamma$ |\n|:----:|:-:|:-:|:-:|\n|      origin      | 0.443/0.516  |  0.443/0.516 | 0.443/0.516  |\n|      origin\u00b10.05      | 0.441/0.515  | 0.433/0.512  |  0.440/0.514  |\n|      origin\u00b10.1      |  0.415/0.501  | 0.425/0.508  |  0.441/0.515  |\n|       origin\u00b10.2      |  0.453/0.523  |  0.423/0.507  |  0.442/0.515  |\n\nFrom the above table, it can be observed that even with certain variations in these hyperparameters, the experimental results only show slight changes. The difference is approximately around 5%, and it still maintains state-of-the-art prediction performance. Therefore, GLIP is not sensitive to hyperparameters."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145454593,
                "cdate": 1700145454593,
                "tmdate": 1700145454593,
                "mdate": 1700145454593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LBVWTWNorg",
                "forum": "aFWUY3E7ws",
                "replyto": "Ley8zc3OYO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_jLT6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_jLT6"
                ],
                "content": {
                    "title": {
                        "value": "Follow Up to Rebuttal; No further questions"
                    },
                    "comment": {
                        "value": "On the source code, thank you for proving the code in the supplementary material.\n\nThe response to the hyperparameter tuning is also reasonable.\n\nThank you for the reply to my raised questions. I have no further questions here."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588183338,
                "cdate": 1700588183338,
                "tmdate": 1700588274203,
                "mdate": 1700588274203,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TPqLg8A1n9",
            "forum": "aFWUY3E7ws",
            "replyto": "aFWUY3E7ws",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_sU4J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3100/Reviewer_sU4J"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents  Global-Local Identification and Prediction (GLIP), a new approach for long-term time-series prediction based on sparse system identification. The proposed approach utilizes discrete Fourier transformation in global identification and local prediction and the experiments show significant improvement over the baselines, in particular in long-horizon prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- Novel approach for long-term time-series prediction\n- Very promising results showing significant improvement over the state of the art\n- The method performs particularly well for longer horizons, with the higher improvement compared to baselines on the longer horizons"
                },
                "weaknesses": {
                    "value": "I found the description of the method very hard to follow with many details either missing or unclear.\n- Section 3.1.1:\n\t* while the section describes a Discrete Fourier Transformation (DFT) on X, it is not clear where in this section the resulting A,f are used.\n\t* it is not clear what T/T* are or how are they computed? Is \\Theta_g includes exactly three items or multiple sin/cos waves? (this question also applied to Section 3.1.2)\n\t* \"from the univariate predictions with the test set\" - should probably be the training set?\n\t* X^*_g is not really used in Eq. (3), different from the text. Also, not clear what order of expressions were considered.\n- Section 3.2: it is not clear to me if this is part of the model/training or a description of how to validate the model's performance (e.g., the paper mentioned it is used to \"evaluate\" or \"conduct an assessment\")?\n- Overall, I think the paper would significantly benefit from a pseudo-code describing training and inference procedures.\n\nOne significant weakness is related to interpretability. The paper positions the proposed approach as an interpretable approach (including the title of the paper and in the motivation described in the abstract). However, the paper presents no results on interpretability in the paper and there is only a short description that says that the parameters are \"amenable to interpretation\". To make strong claims regarding the interpretability of the proposed approach, some results are needed (i.e., what are these parameters, are they weights associated with interpretable features, how many are there, etc). Some example(s) for the interpretability of results would be very useful as well.\n\nExperiments:\n- Datasets: the number of datasets used in the paper seems significantly smaller compared to comparable works. For example, in [2] and [3], other datasets such as weather, electricity, and traffic were included.\n- Global prediction is presented as an advantage of the proposed approach, but without any baseline, it is very hard to judge whether the results we observe are good (e.g., in Exchange and ILI where the predicted patterns deviate from the ground truth). It would be useful to include some time-series forecasting baseline so that we can get a sense of how well the proposed approach is doing by comparison.\n- Reproducibility: missing details as described above (e.g., the construction of \\theta_g). In addition, the chosen values for the hyper-parameters (e.g., \\alpha, \\beta) are not described (as well as the procedure of tuning them).\n\nLiterature: the work should probably mention other models that have focused on basis expansion, such as N-BEATS [1].\n\nMinor issues:\n- Section 3: \" in the testing set for global prediction\" -> should be training set?\n- page 6: \"prediction. we aim\" -> \"prediction. We aim\"\n\n\n[1] Oreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2019, September). N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations.\n[2] Wu, H., Xu, J., Wang, J., & Long, M. (2021). Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, 22419-22430.\n[3] Zeng, A., Chen, M., Zhang, L., & Xu, Q. (2023, June). Are transformers effective for time series forecasting?. In Proceedings of the AAAI conference on artificial intelligence (Vol. 37, No. 9, pp. 11121-11128)."
                },
                "questions": {
                    "value": "I would appreciate the authors' response to the weaknesses listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3100/Reviewer_sU4J"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699403785636,
            "cdate": 1699403785636,
            "tmdate": 1700707584729,
            "mdate": 1700707584729,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z3DqcaISWX",
                "forum": "aFWUY3E7ws",
                "replyto": "TPqLg8A1n9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments from Reviewer sU4J (part I)"
                    },
                    "comment": {
                        "value": "Thanks for your meticulous reading of our article and providing us with valuable suggestions. We have carefully considered these comments and suggestions. We have categorized the weaknesses and questions you pointed out into the following three aspects: details regarding the article, descriptions of interpretability, and the presentation of experiments. Below is the response point by point to the questions.\n\n> **_W1/Q1 Details regarding the article_**\n* **W/Q 1.1 The utilization of $\\mathbf{A}$ and $\\mathbf{f}$ in *DFT.*** \n\n**A**: Through the application of ***DFT***, we can transform time series into the frequency domain, where $\\mathbf{A}$ represents the amplitude and $\\mathbf{f}$ signifies the frequency. $\\mathbf{A}$ and $\\mathbf{f}$ are vectors with the equal length, maintaining a one-to-one correspondence. The larger the amplitude, the greater the impact of the corresponding frequency on the time series. Our objective is to select, based on the amplitude magnitude, the frequencies most likely to construct the main components of the time series. Subsequently, these selected frequencies are transformed into basis functions for consideration, which will be further screened through sparse identification. Therefore, our candidate periodical terms $\\mathbf{T^*}=[T_1, T_2,...,T_{m_1}]$ are determined by the selection process involving $\\mathbf{A}$ and $\\mathbf{f}$.  We have incorporated this selection process into Appendix. C and referenced it in the main text after the statement \"For further global prediction, frequencies with high amplitude are selected\" in the new version.\n\n* **W/Q 1.2 The computation of $\\mathbf{T}, \\mathbf{T^{*}}, \\mathbf{\\Theta_g}$.** \n\n**A**: After a series of potential frequencies $\\mathbf{f}$ influencing the time series were selected based on the higher amplitudes $\\mathbf{A}$, a series of potential periods $T_i$ were subsequently derived by employing the frequency-period conversion formula $f=1/T$. This forms a row vector $\\mathbf{T^*}=[T_1, T_2,...,T_{m_1}]$, where each element records a potential period most likely to influence the given time series. Similarly, $\\mathbf{C}_1$ is also a row vector, as mentioned in the Notation section regarding the computation of operations involving scalars and vectors. The construction of matrix $\\mathbf{\\Theta_g}$ is not limited to three basic items/functions. Taking $\\sin(\\mathbf{C}_1\\mathbf{t})$ as an example, where $\\mathbf{C}_1$ is an $m_1 \\times 1$ row vector and $\\mathbf{t}$ is a $1 \\times n$ column vector, they form an $m_1 \\times n$ matrix. Thus, $\\sin(\\mathbf{C}_1\\mathbf{t})$ is also an $m_1 \\times n$ matrix. The construction of $\\sin(\\mathbf{C}_2\\mathbf{t})$ is similar, and the final $\\mathbf{1}$ represents a column vector of all ones. Therefore, there should be a total $2m_1 +1$ basic functions. This explanation is equally applicable to sections 3.2 and 3.3. To enhance clarity, when $\\mathbf{T^*}$ and $\\mathbf{t}$ are first introduced in Section 3.1, we have added row and column vector labels for them in the new version.\n\n* **W/Q 1.3 Typographical error:** \n\n**A**: In the old version, indeed, \"from the univariate predictions with the test set,\" should be \"from the univariate predictions with the training set\". We have corrected it in the new version.\n\n* **W/Q 1.4 Problem in $\\mathbf{\\Theta}_g^{*}$:** \n\n**A**: The functions formed by $\\mathbf{X}$ consisting the expression of $\\mathbf{\\Theta}_g^*$ in Eq. (3) should be $\\mathbf{X}^*$. We have corrected it in the new version. The order of expressions is 2. This will be explained in Appendix (D.4).\n\n* **W/Q 1.5 Problem in Section 3.2:** \n\n**A**: In Section 3.2, our validation set is used to evaluate the effectiveness of the training set. The results from the validation set here will impact the subsequent use of local rolling predictions. We have provided pseudocode in Appendix. E, and its role can be inferred from line 14 and line 21 of the pseudocode. Our approach follows the neural network paradigm, dividing the data into training, validation, and test sets. Here, the role of the validation set is similar to that in neural network methods.  To enhance clarity in our expression, we have rephrased the description in subsection 3.2.1 and rewritten subsection  3.2.2. We hope that, such a revision could help improve the understanding of the role of the validation set in GLIP.\n\n* **W/Q 1.6 Pseudo-code** \n\n**A**: Thanks for the suggestion. We have given a pseudo-code in the Appendix. E to improve the presentation, and we referenced it after introducing the method in subsection 3.3.3."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700144834566,
                "cdate": 1700144834566,
                "tmdate": 1700144834566,
                "mdate": 1700144834566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5Iea4i5xqv",
                "forum": "aFWUY3E7ws",
                "replyto": "PkPegFeSyH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_sU4J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3100/Reviewer_sU4J"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. Some of my concerns have been addressed. I do think all the additional information provided in the response should be reflected in the paper. However, I am still not convinced that the proposed approach provides significant benefits in terms of interpretability.\n\nI have decided to increase my score by one point."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707554501,
                "cdate": 1700707554501,
                "tmdate": 1700707554501,
                "mdate": 1700707554501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]