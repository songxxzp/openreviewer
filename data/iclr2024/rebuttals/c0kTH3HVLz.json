[
    {
        "title": "A Light-robust Reconstruction Method for Spike Camera"
    },
    {
        "review": {
            "id": "9LNH7aNa5P",
            "forum": "c0kTH3HVLz",
            "replyto": "c0kTH3HVLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission681/Reviewer_bgSj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission681/Reviewer_bgSj"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a novel reconstruction method for spike cameras specifically in low-light environments. They propose a recurrent-based reconstruction framework that utilizes a light-robust representation (LR-Rep) to aggregate temporal information in spike streams. Additionally, a fusion module is used to extract temporal features. Experimental results on both synthetic and real datasets demonstrate the superiority of their approach. The authors also provide a detailed analysis of low-light spike streams and discuss the efficiency and stability of their method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Sound logic for problem statement to pipeline development.\n- Experiments are well designed, and the performance seems good."
                },
                "weaknesses": {
                    "value": "Generally well written with sound pipeline, there are some points to make improvements. Please check questions."
                },
                "questions": {
                    "value": "- How does the delta T, which sets the length of temporal window affects the overall performance?\n- Does the number of forward path of LR-Rep reduces if we increase delta T (i.e., the length of $S_t$ increases)?\n- Current pipeline seems to occupy large memory. How much the training takes for the memory and time?\n\n------------------\nRelated to display or Minor comments:\n\n- Figure 1 Middle(b) is quite confusing. Does the temporal features for both blue and purple are same? If it is different, it should be illustrated differently. Also, the color difference blue and purple are too small, and also it does not looks as purple. Please make it more distinctive. Also, blue and purple arrows seems not an arrows, but justlines, so please improve it for better clarity. Same for the Fig. 3.\n- Explanation of LISI transform is in the caption, not in the section. In section 4.3, GISI and LISI suddenly appears, which make readers bit confused. If these are also one of the new blocks proposed, they should be described well.\n- In Figure 4, example image of GISI does not show anything. While the figure can be illustrative, suggest authors to use better image that can get the clue what the GISI output looks like.\n- In Figure 5, notation is confusing. For example, if the (a) LISI ti=21 mean the LISI output at ti equals 21, \"=21\" need to be subscript along with ti. Currently it is $LISI_{ti} = 21$. Also suggest to use Latex outputs in the figures for math expressions.  \n- In Figure 9, maybe the dynamic range are uniform for all reconstructions. The STP shows extremely different range, which makes other reconstruction methods totally not visible. (I expect STP images are totally saturated if we use dynamic range of other images.) How about using two different dynamic ranges, one only for STP and the other for other images. It would be okay with descriptions and specifying the min-max values. Anyway, currently the contrast is too low for reconstructed images. \n- In the last line of page 4, typo \"camrea\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Reviewer_bgSj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571354479,
            "cdate": 1698571354479,
            "tmdate": 1699635995336,
            "mdate": 1699635995336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DXSjaYjkKz",
                "forum": "c0kTH3HVLz",
                "replyto": "9LNH7aNa5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your careful reading and valuable feedback!\n\n**Q1:  How does the delta T affect the overall performance?**\n\nOur recurrent-based reconstruction method demonstrates flexibility in efficiently covering and extracting temporal information from spike streams. The number of spike streams, combined with delta T, determines the method's coverage of temporal information from spike streams. When employing the same number of spike streams, a too-small delta T results in insufficient coverage of the temporal information. This hinders our method of predicting and reconstructing scenes effectively.  The increasing delta T is beneficial to our method of reconstructing scenes until stable. Generally, delta T is 41  (Zhao et al. ,2021); chen et al. ,2022); Zhang et al. ,2023). We have also opted for this value.\n\n\n**Q2:  Does the number of forward path of LR-Rep reduces if we increase delta T?**\n\nThe number of spike streams and delta T are independent.  We can choose the different number of spike streams for inference as shown in Fig.11.\n\n**Q3: The current pipeline seems to occupy a large memory. How much the training takes for memory and time?**\n\n\nThe time and memory required for training our method are, 18 hours and 34.4 GB which can be satisfied with 1 NVIDIA A100 GPU. This is attributed to the fact that recurrent-based networks typically consume more time and memory during training due to Backpropagation Through Time (BPTT). During testing, our method demonstrates high efficiency, i.e., our inference time is 818 ms while the inference time of WGSE is 1344 ms as shown in Table 7.\n\n\n**Q4: Related to display**\n\nThanks for your careful reading. We have made relevant modifications based on your suggestions:\n\n\n1. Figure 1 \\& Figure 3: \nWe selected a color pair (green and red) with high contrast to represent forward and backward data streams. We have added a description of that temporary features for both forward and back are different.\n\n2. Explanation of LISI and GISI transform: We divided the original paragraph into two blocks to describe proposed method more clearly.\n\n3. Figure 4 \\& Figure 3: \nWe have selected a more intuitive result (GISI$_t$) for display in **Fig.4**. The reconstruction results in **Fig.3** are also updated accordingly.\n\n\n4. Figure 5: We have made standardized adjustments to the formula (Not limited to **Fig.5**).\n\n5. Figure 9: Thanks for your suggestion. The official source code of STP has been used for testing which is comparison a fair comparison. Therefore, we still hope to keep the initial results in the main paper. Besides, we have also provided the adjusted results of STP in **Fig.20**.\n\n\n\n\n6. typo \"camrea\": We have updated the relevant typo."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499605216,
                "cdate": 1700499605216,
                "tmdate": 1700500448214,
                "mdate": 1700500448214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8LqRzv4E3Q",
                "forum": "c0kTH3HVLz",
                "replyto": "9LNH7aNa5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reading! We responded later than expected due to the substantial content additions. However, with the deadline approaching, we sincerely hope all reviewers to respond, allowing us to continue providing further information."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580178551,
                "cdate": 1700580178551,
                "tmdate": 1700580178551,
                "mdate": 1700580178551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tbeJxrBQGh",
                "forum": "c0kTH3HVLz",
                "replyto": "9LNH7aNa5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to read. We have made the necessary modifications based on your suggestions. Additionally, we have provided explanations regarding the questions raised by the public reviewers. We would greatly appreciate your feedback."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704496229,
                "cdate": 1700704496229,
                "tmdate": 1700704496229,
                "mdate": 1700704496229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BPaxVOwvHe",
                "forum": "c0kTH3HVLz",
                "replyto": "9LNH7aNa5P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the professionalism and fairness of the official reviewers. In the process of responding to your questions, we have also gained insights on how to improve our paper. We sincerely hope that all the reviewers will make a fair and unbiased decision regarding the outcome of this article. If any of the public reviewers can disregard the results presented in the paper, the images and videos in the supplementary material, or even the promised release of model code and datasets, the quality of the open review environment would be greatly diminished."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740551493,
                "cdate": 1700740551493,
                "tmdate": 1700740992191,
                "mdate": 1700740992191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W4Mzh0ZFEG",
            "forum": "c0kTH3HVLz",
            "replyto": "c0kTH3HVLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission681/Reviewer_ajbr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission681/Reviewer_ajbr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a recurrent neural network based image reconstruction method for spike cameras."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a recurrent neural network based image reconstruction method for spike cameras. The authors generate a synthetic dataset for evaluation. The method is demonstrated to work on real as well as synthetic data."
                },
                "weaknesses": {
                    "value": "1. In Fig. 2, the GT scenes seem to be RGB. How are they converted to gray scale?\n2. Why is the PSNR value so high? For eg. the S2I image looks so noisy compared to GT in Fig.1, but according to table 1, the PSNR is > 40dB, which does not make sense."
                },
                "questions": {
                    "value": "Check weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission681/Reviewer_ajbr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823499716,
            "cdate": 1698823499716,
            "tmdate": 1699635995252,
            "mdate": 1699635995252,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nGGwX9YaPK",
                "forum": "c0kTH3HVLz",
                "replyto": "W4Mzh0ZFEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your careful reading and valuable feedback!\n\n**Q1:  In Fig. 2, the GT scenes seem to be RGB. How are they converted to grayscale?**\n\nWe employ the grayscale conversion function provided by OpenCV, i.e., cv2.cvtColor(img, cv2.COLOR\\_BGR2GRAY).\n\n**Q2:  Why is the PSNR value so high?  The S2I image looks so noisy compared to GT in Fig.8, but according to table 1, the PSNR is higher than 40dB.**\n\nWhen the scene is so dark, both the Ground Truth (GT) and the reconstructed images can be at a lower grayscale level. This results in a significantly lower MSE compared to normal-light scenes. Consequently, their PSNR is high.\n\n\nThe Peak Signal-to-Noise Ratio (PSNR) can be expressed as $PSNR = 10 \\cdot \\log_{10}\\left(\\frac{{\\text{255}^2}}{{\\text{MSE}}}\\right),$\nwhere MSE represents the Mean Squared Error. It can be further defined as,\n$MSE = \\frac{1}{mn} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} \\left( I(i, j) - K(i, j) \\right)^2,$where m (n) is the height (width) of the image, respectively and $I(i, j)$($K(i, j)$) represents the pixel value at (x, y) in ground truth (the reconstructed image). Hence, when both the Ground Truth (GT) and the reconstructed images are at a lower grayscale level, MSE is lower (PSNR is higher).  In Fig.8, the scene is exceptionally dark, and we applied the same gamma correction to the reconstructed result for better visibility to readers."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499613351,
                "cdate": 1700499613351,
                "tmdate": 1700500796748,
                "mdate": 1700500796748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p9NnO2EXNS",
                "forum": "c0kTH3HVLz",
                "replyto": "nGGwX9YaPK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Reviewer_ajbr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Reviewer_ajbr"
                ],
                "content": {
                    "comment": {
                        "value": "Q2. The dataset is synthetic. Wouldn't it be better to scale the image to [0,255] after simulation before calculating PSNR? Because the numerator in the \"peak\" signal to noise ratio (PSNR) is the maximum range of values the image can take. If the image is so dark that it can take values only in [0,10] it does not make sense to keep the numerator as 255."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527072132,
                "cdate": 1700527072132,
                "tmdate": 1700527072132,
                "mdate": 1700527072132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KpW795r254",
                "forum": "c0kTH3HVLz",
                "replyto": "W4Mzh0ZFEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your positive feedback.  We calculate PSNR after scale the images to [0,255]. The result is as follows: \n***\n| Metric         | TFI     | SSML    | S2I     | STP     | SNM     | WGSE    | Ours    |\n| -------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| PSNR           | 31.409  | 38.432  | 40.883  | 24.882  | 25.741  | 42.959  | **45.075** |\n| PSNR (scale)   | 21.665  | 30.176  | 31.202  | 14.894  | 18.527  | 32.439  | **38.131** |\n| SSIM           | 0.72312 | 0.89942 | 0.95915 | 0.55537 | 0.80281 | 0.97066 | **0.98681** |\n\n***\nWe have updated it to **Table 1**. We also keep the PSNR of raw results in our main paper for two reasons: 1. Although LLR includes low-light scenes, their the dynamic range is wide as shown in **Fig.14**.  2. In previous work (WGSE, SSML and S2I),  PSNR of raw results is shown and we hope to be consistent with those papers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537770030,
                "cdate": 1700537770030,
                "tmdate": 1700537974677,
                "mdate": 1700537974677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVuODj71tD",
                "forum": "c0kTH3HVLz",
                "replyto": "W4Mzh0ZFEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the professionalism and fairness of the official reviewers. In the process of responding to your questions, we have also gained insights on how to improve our paper. We sincerely hope that all the reviewers will make a fair and unbiased decision regarding the outcome of this article. If any of the public reviewers can disregard the results presented in the paper, the images and videos in the supplementary material, or even the promised release of model code and datasets, the quality of the open review environment would be greatly diminished."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740542892,
                "cdate": 1700740542892,
                "tmdate": 1700741003303,
                "mdate": 1700741003303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Uq66vgn7i",
            "forum": "c0kTH3HVLz",
            "replyto": "c0kTH3HVLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission681/Reviewer_sPKH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission681/Reviewer_sPKH"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops and trains a nueral network architecture that can recsontruct high-speed grayscale video frames from the sparse asynchronous data produced by a neuromorphic spiking camera (a camera where each pixel triggers asynchronously at a rate proportional to the intensity of the incident light). The proposed method consists of three steps: It first forms a learned light robust representation of the incoming datastream (pass the incoming data through some convolutional and attention layers). It then passes this representation (combined with features forward and back in time) through a res-net to extract higher-level features. It finally decodes this data into a grayscale video stream. \n\nThe proposed algorithm was evaluated on real and experimental low-light spiking camear data. The proposed method slightly but noticeably outperforms the state-of-the-art (to my knowledge) WGSE algorithm.\n\nAn ablation study is performed to validate the architectural choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Lowlight imaging with neuromorphic cameras is an interesting and important problem.\n\nThe proposed method noticeably outperforms the state-of-the-art.\n\nThe paper includes extensive ablation studies and validation."
                },
                "weaknesses": {
                    "value": "The forward model presented in the main paper doesn't model noise (except quantization). It's unclear how the proposed method would perform in a photon starved regime where a significant amount of Poisson noise would be present.\n\nThe paper doesn't link to any reconstructed videos. I can't evaluate if the proposed method introduced significant flickering artifacts.\n\nFigure 5 isn't particularly informative.\n\nRather than stating, \"The algorithm is in appendix\", please state where in the appendix the algorithm can be found.\n\nThere are a number of typos (e.g., extra capitalizations) that a spell-checker should be able to catch and a few incomplete sentences (e.g., \"A fusion module.\")."
                },
                "questions": {
                    "value": "How would the algorithm behave in the presence of significant Poisson noise?\n\nWhat were the intuitions behind selecting the chosen architecture? Why not leverage any of the wavelet structure from WGSE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889965638,
            "cdate": 1698889965638,
            "tmdate": 1699635995153,
            "mdate": 1699635995153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AzSfKR9xXv",
                "forum": "c0kTH3HVLz",
                "replyto": "4Uq66vgn7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your careful reading and valuable feedback!\n\n**Q1: Reconstructed videos**\n\nWe add reconstructed videos to **the supplementary material** where the state-of-the-art method WGSE is compared. We can find that the results of WGSE are very unstable and fluctuate greatly in the time domain, while our method can effectively handle temporal noise.\n\n\n**Q2: How the proposed method would perform where a significant amount of Poisson noise would be present?**\n\nOur method can deal with Poisson noise better than the state-of-the-art methods. First, the mean of the error from Poisson noise in the temporal dimension is 0. Our bidirectional recurrent-based reconstruction method can effectively aggregate temporal information. Its structure is suitable for learning to filter Poisson noise. Besides, our training set is generated from the spike camera simulator  (Zhao et al.\n2022a) and it considers Poisson noise and fixed-pattern noise in the spike camera.\n\n**Q3: Figure 5 \\& typos**\n\nThanks for your suggestions. We aim to enhance the reader's understanding of GISI through Fig.5. Following your advice, we have included links to the algorithm. Additionally, we have addressed typos in our paper. \n\n**Q4: What were the intuitions behind selecting the chosen architecture?**\n\nThis is an excellent question. We initially observed a pronounced performance degradation of reconstruction methods in low-light conditions. Subsequently, we noticed a significantly sparser distribution of spikes in low-light spike streams compared to those in normal-lighting conditions. The recurrent-based structure proves effective in aggregating information from different time. This forms the basis of our work. Finally, we design a tailored bidirectional recurrent reconstruction framework specifically for spike cameras. \n\n**Q6:  Why not leverage any of the wavelet structure from WGSE?**\n\nThe wavelet structure in WGSE (Zhang et al., 2023). exists solely within its representation. We replaced our representation with that of WGSE and retrained it. As shown in Table.3, the performance of our method (PSNR: 45.075 and SSIM: 0.9868) is better than that (PSNR: 42.302 and SSIM: 0.9744)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499623216,
                "cdate": 1700499623216,
                "tmdate": 1700500284575,
                "mdate": 1700500284575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "smopem0D74",
                "forum": "c0kTH3HVLz",
                "replyto": "4Uq66vgn7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reading! We responded later than expected due to the substantial content additions. However, with the deadline approaching, we sincerely hope all reviewers to respond, allowing us to continue providing further information."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580166857,
                "cdate": 1700580166857,
                "tmdate": 1700580166857,
                "mdate": 1700580166857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sQ8DAej83h",
                "forum": "c0kTH3HVLz",
                "replyto": "4Uq66vgn7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the professionalism and fairness of the official reviewers. In the process of responding to your questions, we have also gained insights on how to improve our paper. We sincerely hope that all the reviewers will make a fair and unbiased decision regarding the outcome of this article. If any of the public reviewers can disregard the results presented in the paper, the images and videos in the supplementary material, or even the promised release of model code and datasets, the quality of the open review environment would be greatly diminished."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740535066,
                "cdate": 1700740535066,
                "tmdate": 1700741019739,
                "mdate": 1700741019739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wKQasc8rmF",
            "forum": "c0kTH3HVLz",
            "replyto": "c0kTH3HVLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission681/Reviewer_rqug"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission681/Reviewer_rqug"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel frame reconstruction method for spike camera. It particularly emphasized its advantage for light robustness and has shown results compared to previous methods within the same category. There are two areas of contributions claimed. The first is that the paper proposed a benchmark for high speed low light scenes. The dataset LLR is built upon existing method SPCS [Hu et al. 2022] and dimmed the scene brightness to obtain a low light version. The second contribution is an algorithm, including a light-robust representation, that leverages neighbor binned spike features to perform frame reconstruction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper covers both dataset simulation and architectural proposals for low light. The proposed architecture is a push-forward based on previous transforms. I think the key idea is to extend existing LISI (local inter-spike interval) transform to incorporate the release time of forward and backward spikes. The reasoning is that as light intensity decreases, the spike interval increases, and it is well likely that information is helpful from longer time steps and bidirectional."
                },
                "weaknesses": {
                    "value": "There are key issues associated with the proposal. \n- First, the paper has not established benchmarks for the light robustness. It is very unclear how low is the \"low light\" used in this paper. And it's also not touched how robust the algorithm functions comparing normal and low light. A better version is a quantification for performance vs light intensity.\n\n- Second, as light decreases, the solution of this paper is to extract information from longer time range. In such a case, motion may play a significant role affecting the reconstruction results. Yet it was not demonstrated.\n\nIt looks like the LLR dataset has only two lighting conditions, i.e. normal and low? Is it enough for benchmarking? Are 5 motions enough? The paper mentioned \"... the power of light source is consistent with the real world\". How to achieve consistency? The dataset part lacked technical details and justification.\n\nThe overall idea is interesting but lacks significance. The bidirectional attentive approach has been well seen in video frame interpolation and event-based version.\nThe global inter-spike interval (GISI) is a small extension of previous LISI. It is also very confusing what LISI is referring to. The two references [Chen 2022] proposed TFI and [Zhao 2022b] proposed DSFT (differential of spike firing). Are the authors referring to TFI and DSFT as the same thing? And according to Table 2 the significance of GISI is so marginal and is hardly considered a contribution. \n\nThe figures are well-made but they hardly explained technical details. Figure 5 generated a lot of confusion as what's \"LISI\", \"update\" and \"maintain\". Mathematical formulation is needed. Figure 7 did not provide useful information and is quite redundant to present after Eq 5-9.\n\nPlease work on the presentation as there are a lot of grammar errors."
                },
                "questions": {
                    "value": "From Figure 11, it seems that the method converges well at 5 frames and even has worse results comparing 21 to 13 for PSNR. Is this contradicting to the choices for frame numbers?\n\nI couldn't find where exactly is noise being handled. I only see an I_{dark} on top of I as in current but is that all?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission681/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699225455089,
            "cdate": 1699225455089,
            "tmdate": 1699635995090,
            "mdate": 1699635995090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v240qeEBqH",
                "forum": "c0kTH3HVLz",
                "replyto": "wKQasc8rmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your careful reading and valuable feedback!\n\n\n**Q1: Setting of light source power and How to achieve consistency with the real world?**\n\nWe set the lighting parameters in the advanced 3D graphics software, blender, to make the lighting conditions as consistent as possible with the real world. The following are the configuration details in Blender.\n\nIn Blender, various types of lighting simulation functions, including sunlight, point lights, and area lights, have been integrated into the graphical interface. We can adjust lighting parameters to control brightness and darkness. \n\n1. For sunlight in Blender, the watts per square meter can be modified. Typically, 100 watts per square meter corresponds to a cloudy lighting environment. For the 'Car$_L$' scene in Fig.13, we have set sunlight to 10 watts per square meter, which is deemed sufficiently low. \n\n2. For point lights and area lights, Blender allows modification of radiant Power, measured in watts.  This is not the electrical power of consumer light bulbs. A light tube with the electrical power of 30W approximately corresponds to a radiant Power of 1W. In the 'Cook$_L$' scene  in Fig.13, we have set an area light with the radiant Power to 1W (the electrical power of 30W) . It already represents a very dim indoor light source.\n\n**Q2:How dark is our dataset?**\n\nThe low light scene in our dataset cover various dark range. The brightness is not only determined by the light source, but also by factors such as camera distance, object occlusion, and so on. These factors are ultimately reflected in the grayscale of the rendered image. Therefore, we calculate the grayscale histograms of images in low light scenes. As shown in **Fig.14** (each bar represents 5 grayscale levels). We can see that the grayscale is diverse and in a lower range.\n\n**Q3: A quantification for performance vs light intensity.**\n\nWe generated spike streams under different lighting conditions by modifying the light source parameters. Due to the large time-consuming of once rendering for each type of lighting, we only use one scene to demonstrate the performance of our method. Related results are updated in **Table.4**. Our method demonstrates excellent performance.\n\n\nSpecific modification details: We set sunlight (see Q1) to 30, 50, 70, 90, 110 and 130 watts per square meter to render images respectively.\n\n\n**Q4: Diversity of motion**\n\n\nThe motion in LLR are diverse. Each designed scene contains 1000 frames of images and the motion at different times is different. We generate a optical flow every 40 frames for LLR. The degree distribution of the optical flow is statistically analyzed in **Fig.15**. We can find that the motion in LLR covers all kinds of directions.\n\n\n**Q5: The significance of our overall idea**\n\nWe appreciate your recognition that our idea is interesting. The bidirectional RNN itself is a meaningful structure and emerges in the NLP domain. Many other domains, such as video interpolation, multi-agent systems, and event-based vision, have been inspired and modify the structure to better suit their field. We follow the same research line, and we are the first ones that design a tailored bidirectional recurrent reconstruction framework specifically for spike cameras. Our experimental results demonstrate that RNN structure can also boost the performance in spike vision.\n\n\n**Q6: TFI \\& DSFT**\n\nBoth TFI and DSFT first calculate the interval between adjacent spikes in the input spike stream, and they both employ a brute-force search approach. The difference lies in the fact that TFI takes the reciprocal of each interval. \n\n\n\n\n\n\n**Q7: LISI VS. GISI**\n\n\nGISI (our final method) not only outperform LISI (baseline (E) in Table.2) in both PSNR and SSIM on synthetic datasets but also have better generalization  on real spike streams. \n\n\nWe update reconstruction results of two methods in **Fig.17**. More importantly, the cost of using GISI instead of LISI is negligible (we only need to use two 400x250 matrices to store the time of the forward spike and the backward spike, respectively), which does not affect the parameter and efficiency of the network at all. \n\n\n\n**Q8: Fig.5 \\& Fig.7**\n\n\nThanks for your suggestions. \n\n1. For **Fig.5**, we have included additional supplementary explanations for LISI in the appendix and highlighted them in the main paper. The \"maintain\" and \"update\" are unique operations in GISI. For better understanding, it is suitable to be written as pseudocode. The pseudocode has been written in the appendix. In the caption of Fig.5, we have updated the link for the specific location of the algorithm. \n\n2. For **Fig.7**, we have also adjusted its positioning to better illustrate the specific modules of the network, placing it before the formulas."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499648904,
                "cdate": 1700499648904,
                "tmdate": 1700500267707,
                "mdate": 1700500267707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xGRpZMA8EE",
                "forum": "c0kTH3HVLz",
                "replyto": "wKQasc8rmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reading! We responded later than expected due to the substantial content additions. However, with the deadline approaching, we sincerely hope all reviewers to respond, allowing us to continue providing further information."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580148156,
                "cdate": 1700580148156,
                "tmdate": 1700580148156,
                "mdate": 1700580148156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rwqz9IWHEj",
                "forum": "c0kTH3HVLz",
                "replyto": "wKQasc8rmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for taking the time to read. We have provided lots of additional results to address your concerns. Additionally, we have provided explanations regarding the questions raised by the public reviewers. We would greatly appreciate your feedback."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704445424,
                "cdate": 1700704445424,
                "tmdate": 1700704445424,
                "mdate": 1700704445424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kjpa0hT3u2",
                "forum": "c0kTH3HVLz",
                "replyto": "wKQasc8rmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission681/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the professionalism and fairness of the official reviewers. In the process of responding to your questions, we have also gained insights on how to improve our paper. We sincerely hope that all the reviewers will make a fair and unbiased decision regarding the outcome of this article. If any of the public reviewers can disregard the results presented in the paper, the images and videos in the supplementary material, or even the promised release of model code and datasets, the quality of the open review environment would be greatly diminished."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission681/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740521161,
                "cdate": 1700740521161,
                "tmdate": 1700741028589,
                "mdate": 1700741028589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]