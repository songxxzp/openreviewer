[
    {
        "title": "A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines"
    },
    {
        "review": {
            "id": "Zvvjg0cNe8",
            "forum": "qhkEOCcVX9",
            "replyto": "qhkEOCcVX9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a benchmark, the Newborn Embodied Turing Test (NETT), and reveals the limitations of current machine learning algorithms compared to newborn brains in object segmentation tasks. However, the experimental results of this work are insufficient to validate the claimed fairness of the NETT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis work's main contribution is the introduction of the Newborn Embodied Turing Test (NETT) benchmark, which enables a direct comparison of the learning abilities between newborn animals and machines in object segmentation tasks."
                },
                "weaknesses": {
                    "value": "1.\tThe evaluation of this proposed NETT benchmark is not comprehensive enough. Many variables were not taken into consideration during the data collection process for the biological chicks, such as their binocular vision compared to the single camera of artificial intelligence chicks. Meanwhile, biological chicks might roam around while artificial intelligence chicks do not. These unaccounted variables raise doubts about the effectiveness of the proposed benchmark. The validity of the benchmark may be compromised due to the discrepancies in sensory capabilities and behavior between biological and artificial chicks. Further refinement and consideration of these variables are necessary to ensure the reliability and validity of the benchmark.\n2.\tThe presentation of this work still needs improvement, as the figures are not in vector format. For example, Figure 3 lacks clear labels for the x and y axes, making it difficult to understand the meaning of the figure."
                },
                "questions": {
                    "value": "1.\tThe evaluation of this proposed NETT benchmark is not comprehensive enough. Many variables were not taken into consideration during the data collection process for the biological chicks, such as their binocular vision compared to the single camera of artificial intelligence chicks. Meanwhile, biological chicks might roam around while artificial intelligence chicks do not. These unaccounted variables raise doubts about the effectiveness of the proposed benchmark. The validity of the benchmark may be compromised due to the discrepancies in sensory capabilities and behavior between biological and artificial chicks. Further refinement and consideration of these variables are necessary to ensure the reliability and validity of the benchmark.\n2.\tThe presentation of this work still needs improvement, as the figures are not in vector format. For example, Figure 3 lacks clear labels for the x and y axes, making it difficult to understand the meaning of the figure."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458737216,
            "cdate": 1698458737216,
            "tmdate": 1700725617248,
            "mdate": 1700725617248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gXzJ2CVywM",
                "forum": "qhkEOCcVX9",
                "replyto": "Zvvjg0cNe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the constructive feedback.\n\n**The Reviewer suggests that our NETT is not comprehensive enough because biological chicks have two eyes, whereas the artificial chicks had one eye.**\n\nThe Reviewer correctly points out that we did not fully close the experience gap between newborn chicks and artificial agents. Newborn chicks have two eyes, whereas our artificial agents had one eye. This difference may have led to the divergent learning outcomes in the biological and artificial agents. To allow researchers to explore this possibility, we created a \u201ctwo-eyed agent\u201d (Fig. S4). The two-eyed agent was carefully constructed so that the FOV of each eye matches the FOV of newborn chicks.\n\nThere are many ways in which researchers might process the two eye images in the visual encoder, so we modified our benchmark so researchers can select either a \u201cone-eyed\u201d or \u201ctwo-eyed\u201d agent. For the \u201ctwo-eyed\u201d agent, the benchmark will output two images on each time step, one from each of the two cameras, which are positioned on each side of the agent\u2019s head (akin to the placement of eyes on chicks). Researchers can then build and test their own binocular artificial agents using the revised NETT, allowing for systematic exploration of the computational role of one vs. two eyes on the development of object segmentation.\n\n**The Reviewer suggests another difference between biological and artificial systems is that the biological chicks might roam around while the artificial chicks do not.**\n\nTo clarify, our PPO and LSTM PPO policies have an entropy parameter governing how much the agents move at random (akin to roaming). For all of our models, the entropy parameter starts high and linearly decays throughout training. This parameter encourages the agents to explore early in learning.\n\nThat said, we agree that different types of rewards might promote more biologically inspired roaming and information-seeking behavior. We informally tried a number of different rewards and found that this imprinting reward produced behavior that best matched the imprinting behavior of newborn chicks. However, newborn animals learn through intrinsic rewards, so building ML systems that learn like brains will require \u2018turning off\u2019 our imprinting reward and adding a more biologically plausible intrinsic reward to the agents.\n\nIn our revision, we now note that we designed the benchmark so researchers can use their own custom-designed intrinsic rewards to drive the agent\u2019s behavior (i.e., researchers do not need to use our imprinting reward). This makes the NETT more flexible, allowing researchers to plug any encoder, policy network, and/or intrinsic reward into the artificial agents. We hope researchers will use this benchmark to discover reward structures that produce animal-like object perception in ML systems, perhaps by encouraging the artificial agents to more actively explore their environment, as the Reviewer suggests.\n\n**The Reviewer notes that the figures need improvement.**\n\nAs suggested, we improved the quality of the figures and added clear axis titles."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613784918,
                "cdate": 1700613784918,
                "tmdate": 1700613784918,
                "mdate": 1700613784918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9u6zugcViv",
                "forum": "qhkEOCcVX9",
                "replyto": "Zvvjg0cNe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for author's answer. Although the gap between biological chicks and AI chicks is still doubtful, the authors' rebuttle have solved most of my concerns and more detailed experimental results have been given. Hence, I decided to raise my rate to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725594796,
                "cdate": 1700725594796,
                "tmdate": 1700725594796,
                "mdate": 1700725594796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CWA6QbLjnf",
            "forum": "qhkEOCcVX9",
            "replyto": "qhkEOCcVX9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
            ],
            "content": {
                "summary": {
                    "value": "The authors aimed to put both state-of-art vision models and biological vision on similar levels of data diet. They do this by rearing newborn chicks in a very controlled visual environment where they have access to limited visual experience. By exploiting the imprinting ability of chicks, their ability to segment foreground from background is studied. Then vision models are trained with exact same data by instantiating them as the vision modules in a RL agent rewarded to go towards the imprinting object. Finally both the models and chicks are probed for their ability to segment new objects and/or new backgrounds and it was found that the models fare much worse compared to the newborn chicks. This indicates that the state-of-art models lack something that newborn chicks' visual system has."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Rarely do I see works with this novelty, quality and significance. \n\n* Studying deep learning models in a setting where the data diet is controlled is timely and needed. \n* The data collected from newborn chicks is compelling and novel. \n* The results showing that current models lack the necessary mechanisms can be very impactful. \n* The data collection procedure and analysis opens up a ton of possibilities for future work. \n* The paper is overall well written."
                },
                "weaknesses": {
                    "value": "* Perhaps the biggest weakness is that the vision models here are trained in a way that is not very indicative of the state-of-the-art. Most vision models that is used in the community are usually a combination of self-supervised training and supervision with a lot of exemplars. While this might diminish how relevant this work is for the community, it does not take away from the significance of the work since the goal of the work is to study models when they get only as much visual experience as newborn chicks (i.e newborn chicks are not getting a ton of supervision). \n* PPO agents are trained for a lot of episodes (original PPO paper (Schulman, 2017) did 1M episodes) usually while this work only does 1000 episodes. This is probably okay since this setting is much simpler. But I would still train one model for much more steps to rule out strange behaviors (like grokking (Power, 2022))\n* The paper might benefit from an expanded section on the differences between birds and mammal brains, especially since the audience is likely to be more from the computational side. While I do appreciate the effort in section 1.1, it might be helpful to expand it a bit more."
                },
                "questions": {
                    "value": "* Why does Figure 1 make it seem like this is a cyclic process? I don't see why it is put on a circle - step 1 does not follow step 6?\n* I wish this was called something other than a Turing test. I think most people (as far as I know) think of the imitation game when they hear Turing test while this is very different. I feel like this is significantly different enough to warrant a different. I almost feel like calling this Turing test is underselling it. Would another name that highlights the \"limited exposure to environment\" aspect of this work be better suited? Turing test says nothing about how much experience the models get to have?\n* Section 2.1 : Was the second display blank (all black) or just a background?\n* Section 2.2 : Do the RL agents need to be called \"artificial chicks\"? I think calling them \"RL agents\" or something is less confusing. \n\n\n\nImprovement to the paper -\n* All the figures need major revamp. The text is barely readable and they need to be of higher resolution. Figure 2 needs to say what \"F\" and \"N\" stand for. Figure 2 could also be made such that the backgrounds and foregrounds are easily distinguishable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695653654,
            "cdate": 1698695653654,
            "tmdate": 1699636642109,
            "mdate": 1699636642109,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lxsdYYIYIM",
                "forum": "qhkEOCcVX9",
                "replyto": "CWA6QbLjnf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! We will keep your words of encouragement close at hand as we forge ahead with this research program.\n\n**The Reviewer argues that the biggest weakness of the paper is that we did not use state-of-the-art systems in object segmentation. The reviewer notes that this contradicts the goals of the benchmark (which requires matching the training data across animals and machines), but nevertheless convincingly argues that this would help integrate the paper into the ICLR community.**\n\nWe agree and implemented the suggestion in two ways:\n\nFirst, we inserted a variety of powerful pre-trained visual encoders into the agents, including DINOv2 (Oquab et al., 2023), Ego4D (Radosavoic et al., 2022), and Segment Anything (Kirillov et al., 2023). These pre-trained models perform well on standard computer vision benchmarks for object segmentation; nevertheless, artificial agents equipped with these pretrained encoders still failed to solve the one-shot learning task solved by newborn chicks. As shown in Fig. S3, agents with pre-trained encoders learned background-dependent object representations.\n\nSecond, we pre-trained the visual encoders using simulated first-person images from inside the controlled-rearing chambers, mimicking the visual experiences of the chicks (Fig. S2). The encoders were thus pre-trained in the same environment in which we tested the artificial agent. We then inserted those frozen pre-trained encoders into the artificial agents, allowing the policy network to learn from a stable visual encoder. We reasoned that training the encoder and policy networks sequentially, rather than simultaneously, might help the agents solve the task, since the frozen encoders would provide more robust visual experiences for learning the embodied imprinting task. However, like all the other agents we tested, the agents failed to solve the task, largely learning background-dependent object representations.\n\n**The Reviewer notes that since we only trained for 1K episodes, it is possible that more powerful generalization abilities might emerge with more training.**\n\nGreat suggestion. To test this possibility, we trained the artificial agents with 10X more episodes (10,000 episodes). As shown in Fig. 3G, the agents still learned background-dependent object representations, akin to agents trained for 1,000 episodes. We also emphasize in the revision that we chose the 1,000 training episodes after observing that the artificial agents successfully learned to imprint in 1,000 episodes (as shown in Fig. S5).\n\n**The Reviewer wonders why we visualized the NETT as a circular process.**\n\nIn the Figure 1 caption, we now explain that we see this NETT as the foundation for a closed-loop scientific paradigm, in which researchers build ML models that learn like newborn animals, and then use those models to generate predictions for new experiments with newborn animals to validate and refine those models.\n\n**The Reviewer wishes we named this something other than a Turing test, since Turing tests don\u2019t involve matching the experiments across biological and artificial systems.**\n\nWe appreciate the feedback and agree with the sentiment. After reading this comment, our team spent a long time iterating on names. Ultimately, there were 4 elements we wanted to emphasize: (1) matching the experience between biological and artificial systems, (2) focusing on the origins of intelligence, (3) the fact that the models are embodied agents, rather than disembodied visual systems, and (4) comparing intelligence between animals and machines. We\u2019ve settled on referring to the benchmark as a matched-experience Newborn Embodied Turing Test (NETT).\n\n**The Reviewer suggests adding an expanded section comparing bird and mammalian brains.**\n\nGreat suggestion. We added a more detailed discussion of the differences between avian and mammalian brains to the Supplementary Methods (Section A.1).\n\n**The Reviewer suggests using the term \u201cartificial agents\u201d rather than \u201cartificial chicks.\u201d**\n\nWe agree and have changed the manuscript accordingly.\n\n**The Reviewer suggests clarifying the stimuli presented on the 2nd display walls and Fig. 3 labels.**\n\nWe revised the manuscript accordingly to indicate that the 2nd display wall was a white screen. We also improved the quality of the figures and clarified what \u201cF\u201d and \u201cN\u201d stand for in the figures."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613422470,
                "cdate": 1700613422470,
                "tmdate": 1700613422470,
                "mdate": 1700613422470,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NuFUZUqQvI",
            "forum": "qhkEOCcVX9",
            "replyto": "qhkEOCcVX9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_oTEc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_oTEc"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to benchmark the intelligence of embodied AI systems against that of biological ones (animals). Towards this end, they propose a new benchmark, the Newborn Embodied Turing Test (NETT). \n\nIn NETT, an agent is born into an environment with a display showing a singular rotating object. The agent must then learn to properly identify that object when presented with distractor objects or when it is placed on novel backgrounds. The biological agents are chicks that learn this skill via filial imprinting and the artificial agents are DRL agents that learn this via an filial-imprinting-like reward.\n\nThe authors find that while biological chicks are well suited to learn this task, their artificial counterparts are not. The authors consider a variety of DNN architectures and find that none perform significantly above chance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents a convincing experiment that directly compares the capabilities of biological and artificial chicks. I really like how controlled the setup is. You can't perfectly control for all the differences between biological and artificial systems (as the authors note in their limitation section), but this paper does an admirable job at reducing this gap as much as possible.\n\nThe authors examine a variety of different DNN architectures with the aim to close the gap between biological and artificial chicks.\n\nThe discussion section is well-written and presents a balanced discussion that points out the limitation of the experiments presented and contains interesting pieces of information."
                },
                "weaknesses": {
                    "value": "Figures in the paper are very low resolution (and possibly heavily compressed with lossy compression). This is particularly noticeable for figure 3, which is nearly illegible.\n\nThe number of trials used for training artificial chicks seems rather small at 1000. Similarly, the episode length of 1000 seems long given how small the environment is. Why were these numbers chosen? Are they similar to the number of times the biological chicks turn towards their object and how long they stay facing it?\n\nThere has been work in deep reinforcement learning algorithms that may be applicable here. One example is as DAAC and IDAAC (\"Decoupling Value and Policy for Generalization in Reinforcement Learning\", Raileanu and Fergus)."
                },
                "questions": {
                    "value": "### Questions\n\n1. Did the authors perform the biological chick experiments themselves or are those directly from Woods & Woods 2021? (I was unable to get access to this paper to check myself.) If the authors performed this experiment, has it been checked by an Institutional Review Board (IRB) or similar?\n\n### Suggestions for Improvement\n\nThis paper reminded me of \"Exploring Exploration: Comparing Children with RL Agents in Unified Environments\" by Kosoy et al. These two works perform different experiments and have different goals, but the authors may want to mention this work.\n\nSome \"high-water\" marks for artificial chicks would be useful. If one was trained on the test set, how would it do? Ideally it should perfectly solve the task. If one was trained with a singular object but many different backgrounds, how would it do?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper contains experiments performed with real animals in not great conditions -- newborn chicks are confided to a small box in isolation with only a rotating object on a screen for company. While the experimental design seems to come from prior work (Woods & Woods, \"One-shot object parsing in newborn chicks\", 2021), I don't feel confident making a judgement on if this experiment is okay. \n\nI asked the authors if they performed this experiment and if they did, has it been check by an IRB or similar."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698970152044,
            "cdate": 1698970152044,
            "tmdate": 1699636642013,
            "mdate": 1699636642013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KNH74E3RJf",
                "forum": "qhkEOCcVX9",
                "replyto": "NuFUZUqQvI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you: we appreciate your time and feedback.\n\n**The Reviewer notes the low resolution figures.**\n\nWe improved the resolution quality of the figures.\n\n**The Reviewer notes that since we only trained for 1K episodes, it is possible that more powerful generalization abilities might emerge with more training.**\n\nGreat suggestion. To test this possibility, we trained the artificial agents with 10X more episodes (10,000 episodes). As shown in Fig. 3G, the agents still learned background-dependent object representations, akin to agents trained for 1,000 episodes. We also emphasize in the revision that we chose the 1,000 training episodes after observing that (1) the artificial agents successfully learned to imprint in 1,000 episodes, and (2) the artificial agents developed a robust form of object recognition in 1,000 episodes. \n\n**The Reviewer also suggests a related paper by Kosoy et al. and points us toward new work in deep reinforcement learning models (DAAC and IDAAC) that might be applicable.**\n\nOur revision now cites the Kosoy et al. paper. We unfortunately were not able to test DAAC and IDAAC within this rebuttal period, but we will add this to our list of models to try next. We also note in the revision that we designed our benchmark to be as easy to use as possible, so we hope other researchers will try DAAC, IDAAC, and any other candidate models that might be interesting to researchers across artificial intelligence and the mind sciences.\n\n**The Reviewer wonders whether we performed the chick experiments ourselves to check that necessary animal welfare/ethics procedures were followed.**\n\nThe chick experiments were conducted by Wood & Wood (2021) and not conducted for this paper. We have revised the text to make this clearer for readers. Our paper extends this prior work by turning the behavioral results into a NETT benchmark. The original paper (Wood & Wood, 2021) specifies that the work was approved by an IACUC (pg. 2411).\n\n**The Reviewer suggests adding \u201chigh-water\u201d marks for the artificial chicks, by training and testing them on the test set.**\n\nGreat idea. While we did not have time to add \u201chigh-water\u201d makers to this revision, we plan to add them to the camera-ready version if the paper is accepted."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612910230,
                "cdate": 1700612910230,
                "tmdate": 1700612910230,
                "mdate": 1700612910230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CfKa7fQWhr",
            "forum": "qhkEOCcVX9",
            "replyto": "qhkEOCcVX9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_gdnS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5993/Reviewer_gdnS"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts a comparison study on the one-shot object segmentation ability of real animals and machines. Specifically,  the authors create simulated \u2018digital twin\u2019 environments that mimic the rearing condition of real biological newborn chicks. In the simulated environments, the \u2018artificial chick\u2019 is trained via deep reinforcement learning to segment objects. The experimental results show that \u2018artificial chick\u2019 failed in the one-shot object segmentation task while biological chicks are able to solve it."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality and Significance:    \nThe reviewer found the idea of comparing machine and real animals' learning ability in a strictly controlled environment interesting. This line of study might help us learn more about the similarity and difference of animal brain and deep neural network. \n \n\nQuality:   \nWhile the idea of the paper is interesting, the reviewer found some claims are not well-supported by the experiments. Please see the Weakness section for more details.\n\n\nClarity:    \nThe presentation is mostly clear. However, many improvements, particularly on the figures, are needed.  Please see the Weakness section for more details."
                },
                "weaknesses": {
                    "value": "1. The technical contribution of the paper is limited. Specifically, the proposed \u2018artificial chick\u2019 is a PPO agent trained with \u2018imprinting reward\u2019, which encourages the agent to move close to an object. It is not surprising that this simple baseline with a heuristic reward fails to solve the one-shot object segmentation task.   \n\n2. The reviewer found main claims in the paper are not well-supported by the experiments. Specifically, the authors claimed that \u2018\u2019... none of the algorithms learned background-invariant object representations that could generalize across novel backgrounds and viewpoints\u201d. However, the ML algorithm the authors used in the paper are just  PPO with different architectures and a heuristic reward. The reviewer thinks those simple methods well-represent the state-of-the-art one-shot object segmentation methods. There are many works on one-shot segmentations [1, 2]. Incorporating those methods in the \u2018artificial chick\u2019 might make the experiments more convincing.     \n\n3.  The reviewer has concerns on the significance of the paper. The main finding here is that the RL trained agent failed the one-shot segmentation task. It seems expected. Providing more insights and analysis of why it fails may make the paper more valuable.   \n \n4.  The presentation in the experimental section is unclear. Particularly, the text and numbers in figure 3, which show the only experimental result of the paper, are ineligible.  \n\n\n\n[1] Learning to Segment Rigid Motions from Two Frames, Yang et al., CVPR 2021.   \n[2] One-Shot Learning for Semantic Segmentation, Shaban et al., BMVC 2017"
                },
                "questions": {
                    "value": "1. According to section 2, the biological chicken data is from previous work (Wood et al. 2021). However, in the abstract, the authors claim that \u201cwe raised newborn chicks in controlled environments \u2026\u201d. This seems contradicting. Could you clarify?    \n\n\n2.  In Introduction, it reads \u201cHowever, most studies with newborn subjects have produced data with a low signal-to-noise ratio,\u201d. The reviewer has difficulty following the content. Please give citations for \u2018some studies\u2019 and elaborate what is the \u2018produced data\u2019 here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699207462456,
            "cdate": 1699207462456,
            "tmdate": 1699636641921,
            "mdate": 1699636641921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2Wz7vSFxL8",
                "forum": "qhkEOCcVX9",
                "replyto": "CfKa7fQWhr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5993/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the helpful feedback.\n\n**The Reviewer argues that the claims are not well-supported by the experiments because we used a PPO agent trained on a heuristic (imprinting) reward. The Reviewer suggests adding state-of-the-art object segmentation models into the artificial agents to make the experiments more convincing.**\n\nThis is a great idea. We implemented this suggestion in three ways:\n\nFirst, we took three top-performing object segmentation models\u2014DINOv2 (Oquab et al., 2023), Ego4D (Radosavoic et al., 2022), and Segment Anything (Kirillov et al., 2023)\u2014and inserted them into the artificial agents. The pre-trained encoders were trained on millions of images across one thousand object categories, far exceeding the sparse visual experiences of the chicks. Nevertheless, artificial agents equipped with these pre-trained encoders still failed to solve the one-shot learning task solved by newborn chicks. As shown in Fig. S3, agents with pre-trained encoders learned background-dependent object representations.\n\nSecond, to allow the policy network to learn from a stable visual encoder, we pre-trained visual encoders using simulated first-person images from inside the controlled-rearing chambers (Fig. S2). The encoders were thus pre-trained in the same environment in which we trained the artificial agents, mimicking the visual experiences of the chicks. We reasoned that training the encoder and policy networks sequentially, rather than simultaneously, might help the agents solve the task, since the frozen encoders would provide more stable visual features for learning the embodied object recognition task (as demonstrated by Parisi et al., 2022). However, like all the other agents we tested, the agents failed to solve the task, largely learning background-dependent object representations.\n\nThird, we emphasize that we used both PPO and LSTM PPO policy networks in the paper.\n\nIn future work, we will focus on inserting more powerful policy networks into the artificial agents. As the Reviewer suggests, it is possible that PPO and LSTM PPO policy networks are insufficient to solve this task. We hope researchers use our benchmark to find policy networks that can match the rapid and robust learning of chicks.\n\n**The Reviewer was concerned about the heuristic (imprinting) reward.**\n\nWe agree with this concern. Our aim is to offer a challenging benchmark that requires deep RL algorithms to be trained and tested in the same environment as real animals in controlled-rearing experiments. To succeed at this task, researchers may need to use ML systems that use different rewards, like curiosity. In our revision, we emphasize that we designed the benchmark to allow researchers to use their own custom-designed intrinsic rewards to drive the agent\u2019s behavior (i.e., researchers do not need to use our imprinting reward). This makes the NETT more flexible: researchers can plug different encoders, policy networks, and/or intrinsic rewards into the artificial agents. We hope this NETT will be useful for discovering reward structures that produce robust object segmentation in embodied ML systems.\n\nThat said, we informally tried a number of different rewards and found that this imprinting reward produced behavior that best matched the imprinting behavior of newborn chicks. However, we agree with the reviewer that newborn animals learn through intrinsic motivations like curiosity, so building ML systems that learn like brains will ultimately require \u201cturning off\u201d our imprinting reward and using a more biologically plausible intrinsic reward.\n\n**The Reviewer argued that parts of the experimental description were unclear.**\n\nTo fix this problem, we (1) improved the resolution of the figures, (2) clarified what we mean by low-signal-to-noise data including citation, and (3) clarified that Wood & Wood (2021a) collected the chick data used to create the NETT benchmark."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5993/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612715784,
                "cdate": 1700612715784,
                "tmdate": 1700612715784,
                "mdate": 1700612715784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]