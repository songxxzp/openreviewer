[
    {
        "title": "Information Flow in Self-Supervised Learning"
    },
    {
        "review": {
            "id": "fe0xTw4jIZ",
            "forum": "WfjJOEfAf7",
            "replyto": "WfjJOEfAf7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at providing better understanding for existing methods (contrastive and feature decorrelation based methods) by leveraging the principles of matrix mutual information and joint entropy. In addition, the paper proposes the \"matrix variational masked auto-encoder\" (M-MAE) method. The paper reports empircal results that show the effectiveness of M-MAE compared with the state-of-the-art methods for representation learning on ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper tackles important questions in the field of self-supervised learning"
                },
                "weaknesses": {
                    "value": "- The proofs are not always clear/complete (see questions below).\n- There are propositions and theorems in the paper. Then in the appendix, there are only proofs to \"theorems\" and the reader must match correct theorem/proposition with the correct proof.\n- The experimental setup lacks many details."
                },
                "questions": {
                    "value": "- Theorems 1 and 2 do not seem to be proved clearly. The proof to theorem 1 in the appendix (the one that includes \"lemma 1\"!) does not seem to proof things by following a clear mathematical reasoning. Can you clarify how the last sentences leads to a valid proof? It it the same for Theorem 2 (the proof on page 15 given the other one seems to refer to Proposition 2).\n- Can you provide additional technical details about the experimental setup. The appendix does not seem to contain any information related to that and only limited information is given in the main paper (what are exactly the training objectives and hyper-parameters such as batch size, etc.).\n- It is not fully clear how Theorems 1 and 2 are not going against each other given Definition 2. Can you provide some information about that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Reviewer_j36B"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669834133,
            "cdate": 1698669834133,
            "tmdate": 1699636048564,
            "mdate": 1699636048564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RihqKJcRGG",
                "forum": "WfjJOEfAf7",
                "replyto": "fe0xTw4jIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer j36B"
                    },
                    "comment": {
                        "value": ">Q1: Theorems 1 and 2 do not seem to be proved clearly. The proof to theorem 1 in the appendix (the one that includes \"lemma 1\"!) does not seem to proof things by following a clear mathematical reasoning. Can you clarify how the last sentences leads to a valid proof? It it the same for Theorem 2 (the proof on page 15 given the other one seems to refer to Proposition 2).\n\nA1: Thank you for your feedback. **We stated in the general response that \"the last sentence\" $K_1 = Z_1$ and $K_2 = Z_2$ is a typo and we have corrected it to $K_1=Z^T_1Z_1$ and $K_2=Z^T_2Z_2$ in the revised version of our manuscript.**\n\n>Q2: Can you provide additional technical details about the experimental setup. The appendix does not seem to contain any information related to that and only limited information is given in the main paper (what are exactly the training objectives and hyper-parameters such as batch size, etc.).\n\nA2: **In our initial manuscript, the loss of M-MAE is given by equation (11) and the definition of TCR is given by definition 3.4 and hyper-parameters like batch size are given in section 5.1.** We appreciate the reviewer's question and their valuable input. However, based on the information provided, we are unable to fully understand the specific implementation details that are lacking. If possible, we kindly request the reviewer to provide further clarification or specific requirements, so that we can better assist and address their concerns. We are more than happy to provide additional guidance and clarification to ensure that the reviewer receives the necessary and accurate information they require.\n\n>Q3: It is not fully clear how Theorems 1 and 2 are not going against each other given Definition 2. Can you provide some information about that?\n\nA3: **Maximizing mutual information is not contradictory to maximizing joint entropy.** For example, in traditional information theory $X=Y$ $\\sim$ uniform distribution, then $H(X)=H(Y)=H(X, Y)=I(X, Y)$ and are all maximized. We have similar cases here in matrix information theory, if $K_1=K_2=I_d$, then $H_2(K_1)=H_2(K_2)=H_2(K_1, K_2)=I_2(K_1, K_2)$ and are all maximized."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324451527,
                "cdate": 1700324451527,
                "tmdate": 1700411039586,
                "mdate": 1700411039586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8nUWxxuQkw",
            "forum": "WfjJOEfAf7",
            "replyto": "WfjJOEfAf7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that the optimal point of BarlowTwins and Spectral Contrastive learning objective functions satisfy the maximal matrix mutual information and the maximal matrix joint entropy. Then, this paper proposes a \"matrix variational masked auto-encoder (M-MAE) loss\" that is a combination of the original loss and the total coding rate, defined as $\\log det (\\mu I + ZZ^T)$, where $\\mu$ is a hyperparameter. Experimental results show that compared to MAE and U-MAE, the proposed M-MAE performs better in terms of linear probing and fine-tuning when the hyperparameter $\\mu$ is well-tuned."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper attempts to understand self-supervised learning methods in terms of the mutual information maximization framework, where each random variable is from the online encoder and the target encoder. It could be a somewhat valuable attempt to understand how self-supervised learning methods work."
                },
                "weaknesses": {
                    "value": "1. There is no detailed discussion between the introduced matrix entropy (definition 1) and the original Shannon's information entropy. In fact, The Renyi entropy is defined by a special matrix family named density matrix. It is not defined for an arbitrary matrix, but only for a Gram matrix. However, this paper misuses the concept of matrix entropy throughout the whole paper. For example, In page 14, the last paragraph says that \"Take K1 = Z1 and K2 = Z2, the results follow similarly\". However, K1 and K2 should be Gram matrices, while Z1 and Z2 are feature matrices, that are not a Gram matrix. I think this paper should clarify the relationship between the matrix entropy (defined by a Gram matrix of the samples from a probability distribution) and the original Shannon's information entropy (defined by a random variable following a probability density function).\n2. There is no connection between Section 3 and 4. Note that the main difference between MAE and M-MAE is $TCR(Z)$. TCR is used for measuring a joint information quantity in Section 3, but in Section 4, this paper uses TCR for measuring information quantity for a single random variable. As the previous discussions are based on the relationship between Z1 and Z2, the newly introduced regularization term for MAE is irrelevant to the previous results.\n3. The proposed M-MAE is sensitive to the choice of the hyperparameter, as shown in Table 2. The gap between each $\\mu$ varies a lot, and it means that we need to access the original target labels to tune the hyperparemeter. It violates the spirit of self-supervised learning; we should not access the original labels\n4. The results in Section 3 are not generalizable to the generic self-supervised learning methods; these results are only applicable to Barlow twins and spectral contrastive learning. In fact, as mentioned in my previous comment, the proof is wrong for spectral contrastive learning because K1 and K2 should be a Gram matrix. In other words, the proof only works for a special case of self-supervised learning, where the objective function coincides with \"Proposition 1\" and K1 and K2 are Gram matrices of the feature matrices Z1 and Z2.\n5. There is no discussion of why the mutual information maximization (or joint entropy) of Z1, Z2 is a good measure of a good self-supervised learning method. As there is no connection between mutual information maximization and goodness of self-supervised learning methods, the motivation of M-MAE is somewhat weak. What is the benefit of making an MAE model maximize mutual information? (Note that, even more it is actually not about mutual information. See comment 2)\n6. The experimental results only show the comparisons between MAE, U-MAE and M-MAE. There are a lot of self-supervised learning methods. I think this paper needs more comparisons with other self-supervised learning methods (e.g., BalowTwins, MoCo, SimCLR, BYOL, DeepClustering, Swav, Data2Vec, DINO, iBot, SimMIM, ...) in terms of both information quantity and performance. I also think that it would be good for this paper to compare with other MAE variants (or MIM methods, such as SimMIM), but it could depend on the scope of this paper; as I think this paper needs a heavy non-trivial revision, as of now, I don't argue that M-MAE should be compared with other MAE variants, but I think additional comparisons with MAE variants will make the submission stronger. I recommend this survey paper to search more recent MAE variants: \"A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond\""
                },
                "questions": {
                    "value": "Please check my previous comment. I think the current version of this paper will need a non-trivial heavy revision, including re-checking the major motivation (W2, W5), the mathematical notations and theoretical results (W1, W2, W4), adding more experiments (W6), fixing the fundamental flaw -- hyperparameter sensitivity -- of the proposed method (W3)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678629882,
            "cdate": 1698678629882,
            "tmdate": 1700735615816,
            "mdate": 1700735615816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1oMwcQeLxs",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Total response to reviewer qQv9"
                    },
                    "comment": {
                        "value": "Thank you for taking your time to review our paper. As your concerns (W1, W2, W4, W5) are caused by our typo. We will make further clarification here. As our matrix information quantities are defined on **square** matrices whose diagonals are all $1$. Taking $K_1=Z_1$ and $K_2=Z_2$ is a typo, the correct one should be $K_1=Z^T_1Z_1$ and $K_2=Z^T_2Z_2$. **This is why we say in the proof part that it is similar to Barlow twins loss proof where the $K_1$ and $K_2$ defined there are square matrices whose diagonals are all $1$.** We have corrected this typo in the revised manuscript.\n\nWe propose to use a unified matrix information-theoretic view on self-supervised learning. We want to further stress that our main contributions are given a theoretical understanding of both contrastive and non-contrastive methods and we also use matrix information theory to improve masked image modelling."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364190378,
                "cdate": 1700364190378,
                "tmdate": 1700410963330,
                "mdate": 1700410963330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1p7Lc7Qb1Z",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1 to reviewer qQv9"
                    },
                    "comment": {
                        "value": ">Q1: There is no detailed discussion between the introduced matrix entropy (definition 1) and the original Shannon's information entropy. In fact, The Renyi entropy is defined by a special matrix family named density matrix. It is not defined for an arbitrary matrix, but only for a Gram matrix. However, this paper misuses the concept of matrix entropy throughout the whole paper. For example, In page 14, the last paragraph says that \"Take K1 = Z1 and K2 = Z2, the results follow similarly\". However, K1 and K2 should be Gram matrices, while Z1 and Z2 are feature matrices, that are not a Gram matrix. I think this paper should clarify the relationship between the matrix entropy (defined by a Gram matrix of the samples from a probability distribution) and the original Shannon's information entropy (defined by a random variable following a probability density function).\n\nA1: First of all, we want to point out that neither Renyi (matrix) entropy nor Shannon's information entropy are defined by us, we merely use the standard definition in the literature. Secondly, although both are entropies, these two types of entropy are defined from completely different perspectives. This means that it is not straightforward to simply transfer the self-supervised results based on Shannon entropy to Renyi (matrix) entropy with slight modifications. Therefore, we need to start from scratch, and this is not a trivial outcome. We also discuss why we use matrix information theory instead of traditional information theory in the first few paragraphs in section 3.1 of the initial manuscript. \n\n**For the \"Take K1=Z1\" typo, we have discussed this in the general response and total response, we have corrected this typo.** The relationship between Renyi (matrix) entropy and Shannon entropy is that Renyi entropy can be seen as applying Shannon entropy to the spectrum of (density) matrix. We have already discussed this in the Related work part of our initial manuscript. We want to further emphasize that in this paper, our focus are mainly on **apply** the tools from matrix information theory.\n\n>Q2: There is no connection between Section 3 and 4. Note that the main difference between MAE and M-MAE is TCR(Z). TCR is used for measuring a joint information quantity in Section 3, but in Section 4, this paper uses TCR for measuring information quantity for a single random variable. As the previous discussions are based on the relationship between Z1 and Z2, the newly introduced regularization term for MAE is irrelevant to the previous results.\n\nA2: Section 3 deals with contrastive and feature-decorrelation based SSL methods. As we point out, in traditional information theory, when the Siamese structure degenerates into a single branch like MAE, the joint entropy and mutual information are exactly the entropy. So motivated by section 3, we add a entropy regularizer TCR. **We have already discussed this connection of section 3 and 4 at the beginning of section 4 in our initial manuscript.** Please note that our TCR is performed on matrices, and it can be computed either jointly or individually, depending on the input matrix. On the other hand, in Section 4, we focus on computation based on representations. We have never claimed that there is a theoretical result on MAE, the theoretical results from the Siamese architecture only served as motivation for improvements in the MAE approach (only one branch).\n\n>Q3: The proposed M-MAE is sensitive to the choice of the hyperparameter $\\mu$, as shown in Table 2. The gap between each \n varies a lot, and it means that we need to access the original target labels to tune the hyperparemeter. It violates the spirit of self-supervised learning; we should not access the original labels\n\nA3: Robustness may not be our primary focus; rather, we are highlighting the potential for improvement with matrix information theory. Additionally, this theoretically motivated improvement is closely connected to previous method U-MAE. **Moreover, the closely related method U-MAE is also sensitive to hyper-parameter.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364494431,
                "cdate": 1700364494431,
                "tmdate": 1700411360620,
                "mdate": 1700411360620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "24i5rUzBDC",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to reviewer qQv9"
                    },
                    "comment": {
                        "value": ">Q4: The results in Section 3 are not generalizable to the generic self-supervised learning methods; these results are only applicable to Barlow twins and spectral contrastive learning. In fact, as mentioned in my previous comment, the proof is wrong for spectral contrastive learning because K1 and K2 should be a Gram matrix. In other words, the proof only works for a special case of self-supervised learning, where the objective function coincides with \"Proposition 1\" and K1 and K2 are Gram matrices of the feature matrices Z1 and Z2.\n\nA4: **As we have discussed above, the typo of $K_1=Z_1$ may make you confused, we have corrected this typo.** Secondly, using the concept introduced in the paper [1], spectral contrastive learning is a special type of sample-contrastive method and Barlow twins is a special type of dimension-contrastive method. **Following the exactly same proof of our theorem, our theoretical results can be generalized to sample contrastive and dimension contrastive methods.** As pointed out by [1], sample and dimension contrastive methods contain many generic self-supervised methods (Proposition 3.2 of [1]).\n\n[1]: On the duality between contrastive and non-contrastive self-supervised learning, Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun, ICLR 2023 (notable-top-5%, ICLR 2023 Outstanding Paper Honorable Mention) \n\n>Q5: There is no discussion of why the mutual information maximization (or joint entropy) of Z1, Z2 is a good measure of a good self-supervised learning method. As there is no connection between mutual information maximization and goodness of self-supervised learning methods, the motivation of M-MAE is somewhat weak. What is the benefit of making an MAE model maximize mutual information? (Note that, even more it is actually not about mutual information. See comment 2)\n\nA5: Our theoretical results show that the loss function of many SSL methods can be seen as **exactly** maximizing matrix information theoretic quantities (matrix mutual information or matrix entropy). As the algorithms we analyze are empirically good and our theorems show **exact** maximization relationship of matrix information quantities and \"good\" SSL methods' losses. We think this connection is straight forward. For why adding matrix entropy may improve MAE, as we pointed out the **exact** relationship of effective rank and matrix entropy, [2] shows in their proposition 5.1 that bigger effective rank is better for SSL. **Note these motivations are already discussed in our initial paper.**\n\n[2]: RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank, Quentin Garrido, Randall Balestriero, Laurent Najman, Yann LeCun, ICML 2023\n\n>Q6: The experimental results only show the comparisons between MAE, U-MAE and M-MAE. There are a lot of self-supervised learning methods. I think this paper needs more comparisons with other self-supervised learning methods (e.g., BalowTwins, MoCo, SimCLR, BYOL, DeepClustering, Swav, Data2Vec, DINO, iBot, SimMIM, ...) in terms of both information quantity and performance. I also think that it would be good for this paper to compare with other MAE variants (or MIM methods, such as SimMIM), but it could depend on the scope of this paper; as I think this paper needs a heavy non-trivial revision, as of now, I don't argue that M-MAE should be compared with other MAE variants, but I think additional comparisons with MAE variants will make the submission stronger. I recommend this survey paper to search more recent MAE variants: \"A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond\"\n\nA6: There may be unfair comparisons when comparing our method with other SSL approaches. The MAE and its variants have different underlying principles compared to other self-supervised methods. Additionally, U-MAE only compares with MAE variants. Note we plan to add a reproduction of SimMIM in the future. We agree with you that \"I don't argue that M-MAE should be compared with other MAE variants\" and our comparisons already cover the most related and canonical methods MAE and U-MAE. **It's worth noting that we have cited the paper \"A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond\" to provide a broader context for our work.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369932530,
                "cdate": 1700369932530,
                "tmdate": 1700411286597,
                "mdate": 1700411286597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UFqxv3p2C6",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Q7: Please check my previous comment. I think the current version of this paper will need a non-trivial heavy revision, including re-checking the major motivation (W2, W5), the mathematical notations and theoretical results (W1, W2, W4), adding more experiments (W6), fixing the fundamental flaw -- hyperparameter sensitivity -- of the proposed method (W3).\n\nA7: Thank you for your feedback. **We have made minor revisions, which consist of correcting the typos and add a new reference.**\n\nWe also conduct additional experiments on CIFAR-100 (pretrained for 1k epochs), the linear probing and fine-tuning accuracies are as follows:\n\n| Method | linear@1 | linear@5 | finetune@1 | finetune@5 |\n| --- | --- | --- | --- | --- |\n| M-MAE(vit-base) | **60.9** | **88.1** | 83.8 | **97.0** |\n| U-MAE(vit-base) | 59.7 | 86.8 | 84.1 | 96.5 |\n| MAE(vit-base) | 59.2 | 86.8 | 84.5 | 96.6 |\n\n\nIt is clear that our method has good results compared to existing works, which is promising as we don't have time to search for better hyper-parameters during rebuttal period."
                    },
                    "title": {
                        "value": "Response 3 to reviewer qQv9"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371471247,
                "cdate": 1700371471247,
                "tmdate": 1700411080885,
                "mdate": 1700411080885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oRPRl4hGPE",
                "forum": "WfjJOEfAf7",
                "replyto": "UFqxv3p2C6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "content": {
                    "comment": {
                        "value": "I have tried to understand the proof of Theorem 3.2. As the comment by Reviewer j36B, the \"proof\" looks incomplete. As far as I understood, the proof argues that\n\n1. Mutual information is maximized when the RHS of proposition 3.1 is minimized.\n2. The RHS of Prop3.1 has a solution when each element of $K\\_1$ and $K\\_2$ is zero (i.e., when $\\| K_1 \\|_F^2 = \\| K_2 \\|_F^2 = 0$)\n3. Barlow Twins **\"encourages\"** $z_i^{(1)} \\approx z_i^{(2)}$ and $z_i^{(1)} \\approx z_j^{(2)} \\approx 0$\n4. We can **approx** $z_i^{(1)} \\approx z_i^{(2)} \\approx 0$\n5. The RHS of Prop3.1 becomes zero when $z_i^{(1)} \\approx z_i^{(2)} \\approx 0$\n6. Therefore, Balow Twins is the same as MI maximization\n7. Assume $K_i Z_i^T Z_i$ and repeat 3-6, then spectral contrastive learning is also MI maximisation.\n\nThis looks like incomplete (and actually technically unacceptable) proof in terms of math because,\n\n- (The statement and the proof solve a different problem) When we say \"a loss function is the **exactly** same as another loss function\", we should show that the two functions are reduced in the same form, not a solution having a loss value 0 for function B also satisfies making function A's value 0. It is because, in practice, function B will rarely become 0. In other words, it could be impossible to make $\\| K_1 \\|_F^2 = \\| K_2 \\|_F^2 = 0$ by function B (SSL losses) therefore, the intermediate solution of function B will not minimize function A (matrix MI, in this case). If we do not have latent embeddings satisfying the approximation, the proof will not hold anymore. This should be clarified in the proof.\n- (Number 3-6 is an improper **proof**) How can we define \"encourages\"? Will Barlow Twins always make $z_i^{(1)} \\approx z_i^{(2)}$ and $z_i^{(1)} \\approx z_j^{(2)} \\approx 0$? Number 3-6 shows that just assuming two convenient **orthogonal** feature matrices, then the matrix MI will be maximized. However, it cannot guarantee that Barlow Twins, Spectral contrastive learning loss is the **exactly** same as the mutual information maximisation.\n\nSometimes, we may accept this kind of non-rigorous proof, if the paper has more advantages (and if the revised paper tones down the overall argument). However, I found this paper has other flaws as well.\n\n[A1] Still, there is no specific discussion regarding matrix entropy. What is matrix entropy? What does it mean? How is it related to classical information entropy? I feel that we need a more precise understanding of matrix entropy before just applying them as a tool.\n\n[A2] \"the joint entropy and mutual information are exactly the entropy\". Joint entropy and mutual information are defined for two random variables, while entropy is defined for a single random variable. I think it is a misuse of the terminologies joint entropy, mutual information and information entropy. I don't think the knowledge of joint entropy, mutual information is directly applied to the single random variable entropy. It is not trivial.\n\n[A3] I don't agree the argument. This paper tackles \"self-supervised learning\", which aims to learn representations without accessing labels. However, the method hacks the SSL benchmark by directly tuning the hyperparameter on the test set. I don't think it is a reliable evaluation. The sensitivity of U-MAE does not resolve this weakness. It does means that U-MAE also has the same weakness.\n\n[A4] Although it is clarified, I think the proof is somewhat nonrigorous, as my first comment. What if a loss function cannot make $z_1 = z_2$? Certainly, a teacher-student framework (or online-target framework) will aim to make $z_1 = z_2$, but it is hard to say that such framework actually ensures to make $z_1 = z_2$.\n\n[A5] As my previous comment, if one says **A is exactly the same as B\", the proof should be more rigorous. Furthermore, even if we can argue that SSL is exactly the same as matrix information maximisation, it does not support that it is a good measure of a good self-supervised learning method. I still feel that there is no connection between mutual information maximization and the goodness of self-supervised learning methods\n\n[A6] \"The MAE and its variants have different underlying principles compared to other self-supervised methods\" what is the different underlying principle of MAE? It has the same SSL spirit as the others. We should not have access to the target labels. MAE has a certain difference from others: MAE is evaluated by fine-tuning rather than linear probing, but I don't think it makes a different principle from others.\n\n\nI raised some concerns in my initial review. I feel that most of them are not well addressed in the rebuttal. Particularly, I think this paper needs more rigorous proof than the current version (as far as I understood, Reviewer j36B has a similar concern to me). Also, I think the experiment violates the spirit of SSL (no access to target labels). Overall, I think this paper needs more improvements to be accepted at ICLR."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499049761,
                "cdate": 1700499049761,
                "tmdate": 1700499049761,
                "mdate": 1700499049761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FmLzEryLWb",
                "forum": "WfjJOEfAf7",
                "replyto": "oRPRl4hGPE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "content": {
                    "comment": {
                        "value": "**Additional comment regarding the proof for clarification**\n\nEventually, **Theorem 3.2** shows that the matrix mutual information between two features is maximized when two features are the same, where it is a fairly acceptable argument even without proof. I don't argue that the proof for showing this is wrong (i.e., before \"As the loss of Barlow Twins ...\").\n\nHowever, I don't think it is proof for the statement *\"Barlow twins and spectral contrastive learning seek the maximal mutual information\"*, because there is no guarantee that Barlow twins and spectral contrastive learning guarantee that the two features (target and online features) become the same."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499864874,
                "cdate": 1700499864874,
                "tmdate": 1700499864874,
                "mdate": 1700499864874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i0Pa5gnqFu",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1 to reviewer qQv9's new questions"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and considering our rebuttal. \n**However, it appears that the reviewer may have some misunderstandings about certain aspects of our paper, particularly our proof of Theorem 3.2.** In order to provide further clarification and address the reviewer's concerns more comprehensively, we will address each of the reviewer's questions individually as follows.\n\n\n>Q8: I have tried to understand the proof of Theorem 3.2. As far as I understood, the proof argues that ... (7 steps).\n\nA8: We'd like to make inline comments to the 7 steps provided by the reviewer.  \n\n>1. Mutual information is maximized when the RHS of proposition 3.1 is minimized.\n\nA: Your understanding of this point is correct.\n\n>2. The RHS of Prop3.1 has a solution when each element of $K_1$ and $K_2$ is zero (i.e., when $\\|K_1\\|^2_F = \\|K_1\\|^2_F=0$)\n\nA: Your understanding of this point is **incorrect**. The minimum will be attained if each **off-diagonal** element in $K_1$ and $K_2$ is 0. Also, the F norm will **not** be 0, because each diagonal element in $K_1$ and $K_2$ will be 1.\n\n>3. Barlow Twins \"**encourages**\" $z^{(1)}_i \\approx z^{(2)}_i$ and $z^{(1)}_i \\approx z^{(2)}_j \\approx 0$\n\nA: Your understanding of this point is **incorrect**. Minimizing the loss will encourage $(z^{(1)}_i)^T z^{(2)}_j \\approx 0$ not $z^{(1)}_i \\approx z^{(2)}_j \\approx 0$. \n\n>4. We can **approx** $z^{(1)}_i \\approx z^{(2)}_i \\approx 0$\n\nA: Your understanding of this point is **incorrect**. You can only obtain $z^{(1)}_i \\approx z^{(2)}_i$.\n\n>5. The RHS of Prop3.1 becomes zero when $z^{(1)}_i \\approx z^{(2)}_j \\approx 0$\n\nA: Your understanding of this point is **incorrect**. The RHS of Prop 3.1 is minimized when $z^{(1)}_i = z^{(2)}_i$ and $(z^{(1)}_i)^Tz^{(2)}_j=0$.\n\n>6. Therefore, Balow Twins is the same as MI maximization\n\nA: Your understanding of this point is correct.\n\n>7. Assume $K_iZ^T_iZ_i$ and repeat 3-6, then spectral contrastive learning is also MI maximisation.\n\nA: Your understanding of this point is correct.\n\n\n>Q9: This looks like incomplete (and actually technically unacceptable) proof in terms of math because, ......If we do not have latent embeddings satisfying the approximation, the proof will not hold anymore. This should be clarified in the proof.\n\nA9: **We did not say \"a loss function is the exactly same as another loss function\".** Instead, we show that when these loss functions reach their optimal points the matrix mutual information and matrix joint entropy are maximized. Secondly, we have consistently emphasized that our analysis focuses on the loss functions themselves. In practice, factors such as optimization algorithms and network expressive power can affect the ability to optimize towards optimal points but these are out of the scope of this paper. Analyzing the loss directly is a common approach, as seen, for example, in the work of [1].\n\n[1]: On the duality between contrastive and non-contrastive self-supervised learning, Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun, ICLR 2023 (notable-top-5%, ICLR 2023 Outstanding Paper Honorable Mention)\n\n>Q10: (Number 3-6 is an improper proof) How can......same as the mutual information maximisation.\n\nA10: We use **\"encourage\"** here following the spirit of prior work \"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere\". As what you summarized in your summary part of our paper \"This paper shows that the optimal point of BarlowTwins and Spectral Contrastive learning objective functions satisfy the maximal matrix mutual information and the maximal matrix joint entropy\". The \"encourage\" here means the optimal point behavior of losses. To better clarify the confusion you raised, we have changed the statement of our theorems from \"Barlow twins and spectral contrastive learning seek the maximal mutual information\" into \"The optimal point of Barlow twins and spectral contrastive learning losses maximize the matrix mutual information\". The **approx**  in the proof will also become equality, other stuff in the proof will remain unchanged.\n\n\nUnderstanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere (https://arxiv.org/pdf/2005.10242.pdf), citation>1200.\n\nThe following **taken from section 4 (named Feature Distribution on the Hypersphere ) of their paper**\n\n\"The contrastive loss **encourages** learned feature representation for positive pairs to be similar, while pushing features from the randomly sampled negative pairs apart.\""
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586880460,
                "cdate": 1700586880460,
                "tmdate": 1700588204497,
                "mdate": 1700588204497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rC7p5Uvs0e",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2 to reviewer qQv9's new questions"
                    },
                    "comment": {
                        "value": ">Q11: [A1] Still, there is no specific discussion regarding matrix entropy. What is matrix entropy? What does it mean? How is it related to classical information entropy? I feel that we need a more precise understanding of matrix entropy before just applying them as a tool.\n\nA11: **In our previous rebuttal, we explicitly pointed out that the discussion on relationship is included in the related work part, can be understood in terms of spectrum.** For a detailed discussion, we refer you to the paper we cited (DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies, https://arxiv.org/pdf/2301.08164.pdf), where the background section provides a thorough explanation. As we pointed out previously, we did not extensively discuss matrix entropy because it is not our focal point and not a concept we defined.\n\n\n>Q12: [A2] \"the joint entropy and mutual information are exactly the entropy\". Joint entropy and mutual information are defined for two random variables, while entropy is defined for a single random variable. I think it is a misuse of the terminologies joint entropy, mutual information and information entropy. I don't think the knowledge of joint entropy, mutual information is directly applied to the single random variable entropy. It is not trivial.\n\nA12: **Please provide a complete restatement of our original rebuttal without only quoting a portion.** The initial response we provided is \"As we point out, in traditional information theory, when the Siamese structure degenerates into a single branch like MAE, the joint entropy and mutual information are exactly the entropy.\" not just the last half you mentioned. In traditional information theory, when there are two random variables X=Y, then the joint entropy H(X, Y)= mutual information I(X;X) = H(X) = H(Y).\n\n>Q13: [A3] I don't agree the argument. This paper tackles \"self-supervised learning\", which aims to learn representations without accessing labels. However, the method hacks the SSL benchmark by directly tuning the hyperparameter on the test set. I don't think it is a reliable evaluation. The sensitivity of U-MAE does not resolve this weakness. It does means that U-MAE also has the same weakness.\n\nA13: We agree with your statement. As U-MAE can be seen as our second-order approach, it is natural that our \"weaknesses\" align with those of U-MAE. It is worth noting that U-MAE is a pioneering work that modifies the MAE loss based on theoretical foundations, and we are pleased to have found close connections to U-MAE. **Additionally, we would like to emphasize that sensitivity to hyperparameters in SSL is quite normal (as seen in renowned works like SimCLR and VICReg, which require searching for suitable hyperparameters).**\n\nSimCLR (https://arxiv.org/pdf/2002.05709.pdf),Table 5 in Section 5.1, citation> 13000\n\nVICReg (https://arxiv.org/pdf/2105.04906.pdf),Table 7 in Section D.4, citation > 600\n\n>Q14: [A4] Although it is clarified, I think the proof is somewhat nonrigorous, as my first comment. What if a loss function cannot make $z_1=z_2$? Certainly, a teacher-student framework (or online-target framework) will aim to make $z_1=z_2$, but it is hard to say that such framework actually ensures to make $z_1=z_2$.\n\nA14: **We have consistently emphasized that our analysis focuses on the loss functions themselves.** In practice, factors such as optimization algorithms and network expressive power can affect the ability to optimize towards optimal points but these are out of the scope of this paper.  To better clarify the confusion you raised, we have changed the statement of our theorems from \"Barlow twins and spectral contrastive learning seek the maximal mutual information\" into \"The optimal point of Barlow twins and spectral contrastive learning losses maximize the matrix mutual information\". The **approx**  in the proof will also become equality, other stuff in the proof will remain unchanged."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587092863,
                "cdate": 1700587092863,
                "tmdate": 1700587092863,
                "mdate": 1700587092863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MHqQPCxwOT",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 3 to reviewer qQv9's new questions"
                    },
                    "comment": {
                        "value": ">Q15: [A5] As my previous comment, if one says **A is exactly the same as B\", the proof should be more rigorous. Furthermore, even if we can argue that SSL is exactly the same as matrix information maximisation, it does not support that it is a good measure of a good self-supervised learning method. I still feel that there is no connection between mutual information maximization and the goodness of self-supervised learning methods\n\nA15:  **Firstly, from a theoretical perspective.** Following the exactly same proof of our theorem, our theoretical results can be generalized to sample contrastive and dimension contrastive methods. As pointed out by [1], sample and dimension contrastive methods contain many \"good\" self-supervised methods (Proposition 3.2 of [1]). **Secondly, from an empirical perspective,** our matrix information quantities act as strong indicator for good representation. From the matrix mutual information plot is in the Figure 1 (https://i.postimg.cc/J7bhbTdm/matrix-mutual-information.png) and the matrix joint entropy plot is in Figure 2 (https://i.postimg.cc/Pr0yD5LP/matrix-joint-entropy.png). We set temperatures as 0.3, 0.5, 0.7 in SimCLR to plot these Figures. From Figure 1 and 2, we can observe that the increase of matrix mutual information or matrix joint entropy during training ties closely with the final accuracy. As temperature = 0.3 outperforms 0.5 and 0.7 in KNN accuracy, it also has the biggest matrix mutual information and matrix joint entropy value.\n\n\n[1]: On the duality between contrastive and non-contrastive self-supervised learning, Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun, ICLR 2023 (notable-top-5%, ICLR 2023 Outstanding Paper Honorable Mention)\n\n>Q16: [A6] \"The MAE and its variants have different underlying principles compared to other self-supervised methods\" what is the different underlying principle of MAE? It has the same SSL spirit as the others. We should not have access to the target labels. MAE has a certain difference from others: MAE is evaluated by fine-tuning rather than linear probing, but I don't think it makes a different principle from others.\n\nA16: **We understand your viewpoint, and we gracefully disagree. We think MAE is quite different from the other SSL methods.**\n\n\n\n>Q17: Additional comment regarding the proof for clarification\nEventually, Theorem 3.2 shows that the matrix mutual information between two features is maximized when two features are the same, where it is a fairly acceptable argument even without proof. I don't argue that the proof for showing this is wrong (i.e., before \"As the loss of Barlow Twins ...\").\nHowever, I don't think it is proof for the statement \"Barlow twins and spectral contrastive learning seek the maximal mutual information\", because there is no guarantee that Barlow twins and spectral contrastive learning guarantee that the two features (target and online features) become the same.\n\nA17:  **We also do not agree with your summary** of  \"Eventually, Theorem 3.2 shows that the matrix mutual information between two features is maximized when two features are the same, where it is a fairly acceptable argument even without proof.\". In fact, we show matrix mutual information maximized when $z^{(1)}_i = z^{(2)}_i $ and $(z^{(1)}_i)^T(z^{(2)}_j)=0$ in the proof of Theorem 3.2. **We have consistently emphasized that our analysis focuses on the loss functions themselves.** In practice, factors such as optimization algorithms can affect the ability to optimize towards optimal points but these are out of the scope of this paper. Analyzing the loss directly is a common approach, for example, in the work of [1]. To better clarify the confusion you raised, we have changed the statement of our theorems from \"Barlow twins and spectral contrastive learning seek the maximal mutual information\" into \"The optimal point of Barlow twins and spectral contrastive learning losses maximize the matrix mutual information\".\n\n[1]: On the duality between contrastive and non-contrastive self-supervised learning, Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun, ICLR 2023 (notable-top-5%, ICLR 2023 Outstanding Paper Honorable Mention)"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587508485,
                "cdate": 1700587508485,
                "tmdate": 1700587508485,
                "mdate": 1700587508485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c62yc0i0YA",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for my typos at the beginning of the response (e.g., $z \\approx 0$). I made mistakes when I wrote the comments from my notes (it could be slightly out of context, but the current notations are really really confusing. I would like to encourage the authors to make the notations clearer, if possible). The inline comments by the authors are indeed correct. \n\n[A9] The rebuttal specifies \"a loss function is the **exactly** same as another loss function\", e.g., \"Our theoretical results show that the loss function of many SSL methods can be seen as **exactly** maximizing matrix information theoretic quantities\". Please check [A10] for the overall comment of the theoretical part.\n\n> A5: Our theoretical results show that the loss function of many SSL methods can be seen as exactly maximizing matrix information theoretic quantities (matrix mutual information or matrix entropy). As the algorithms we analyze are empirically good and our theorems show exact maximization relationship of matrix information quantities and \"good\" SSL methods' losses. We think this connection is straight forward. For why adding matrix entropy may improve MAE, as we pointed out the exact relationship of effective rank and matrix entropy, [2] shows in their proposition 5.1 that bigger effective rank is better for SSL. Note these motivations are already discussed in our initial paper.\n\n[A10] \"The approx in the proof will also become equality, other stuff in the proof will remain unchanged\". I mean that the assumption makes the approximation (or equality, as the revised version) itself is not rigorous. As my previous comment, the proof fundamentally shows that \"the matrix mutual information between two features is maximized when two features are the same\". I don't think this statement is extendable to the current format. For example, a classical approach to show whether the optimal point is the same is whether two methods have the same Lagrange multiplier. I feel that assuming method A makes $z^1 = z^2$ is a logical jump.\n\n[A11] \"As we pointed out previously, we did not extensively discuss matrix entropy because it is not our focal point and not a concept we defined.\" As this paper mainly employs matrix entropy, I think this paper should be built upon a high understanding of matrix entropy. As my previous comment,  I feel that we need a more precise understanding of matrix entropy before just applying them as a tool ([A1])\n\n[A12] The current answer makes sense. I thought that the original response (\"when the Siamese structure degenerates into a single branch like MAE\") does not mean to set H(X, X), but I thought that it introduces a new single variable entropy again.\n\n[A13] In my opinion, the fully fine-tuning evaluation as MAE protocol and the linear probing evaluation as SimCLR and VICReg are not directly comparable in terms of the test set hacking. However, I think it could be somewhat acceptable if the other reviewers and the area chair agree that the hyperparameter sensitivity and the test set hacking are not a serious problem as reject.\n\n[A14] Please check [A10].\n\n[A15] I think the answer is not a perfect one, but it could be an alternative with an empirical analysis. I think citing the proposition of Garrido et al. could be helpful for making the paper better\n\n[A16] \"MAE is quite different from the other SSL methods\" does not mean that it is okay to hack the test set for SSL. As my comment in [A13], I think the current evaluation needs an agreement among all the reviewers and AC whether the evaluation protocol has no problem.\n\n[A17] Thanks for the clarification. I forgot to mention the second condition. I will re-state my argument: Theorem 3.2 shows that the matrix mutual information between two features is maximized when two Gram matrices are the same and each feature dimension is invariant -- the Gram matrix has 0 off-diagonal. \n\nBasically, I think the paper should be revised as soon as possible. I made my decision based on the initial version, and the current revised paper does not show significant difference with the initial one.\n\nI think some of my concerns are addressed in the response, and some are not. I think the current theoretical analysis is not acceptable (even if the typo is corrected). If the paper argues for a weaker theoretical contribution with proper citations and additional logic, the weakness becomes weaker than the initial version. I think the presentation of the current paper should be revised, including many clarifications in the responses. Finally, I still think that this paper violates the spirit of SSL by hacking the test set with direct hyperparameter tuning, but I will respect all the opinions of the other reviewers and AC.\n\nPlease update the paper. I will re-evaluate the paper based on the revised version."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591499248,
                "cdate": 1700591499248,
                "tmdate": 1700591641778,
                "mdate": 1700591641778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rlk9yZS2lE",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 4 to reviewer qQv9's new questions"
                    },
                    "comment": {
                        "value": ">Q18: This is a question to our response [A9]. The rebuttal specifies \"a loss function is the exactly same as another loss function\"......Note these motivations are already discussed in our initial paper.\n\nA18: As our initial response to [A9]. The wrong thing in the sentence you raised \"a loss function is the exactly same as another loss function\" is not the word **exactly**. We highlight the wrong place as \"a loss function is the exactly same as **another loss function**\". In fact, the matrix information quantities are not a **loss function** in Theorems 3.2 and 3.8. What we prove is that optimal point of losses will make matrix information **quantities** also achieve its optimal. \n\n\n\n>Q19: This is a question to our response [A10]. \"The approx in the proof will also become equality, other stuff in the proof will remain unchanged\". I mean that the assumption makes the approximation (or equality, as the revised version) itself is not rigorous. As my previous comment, the proof fundamentally shows that \"the matrix mutual information between two features is maximized when two features are the same\". I don't think this statement is extendable to the current format. For example, a classical approach to show whether the optimal point is the same is whether two methods have the same Lagrange multiplier. I feel that assuming method A makes $z^1=z^2$ is a logical jump.\n\nA19: Your statement \"As my previous comment, the proof fundamentally shows that \"the matrix mutual information between two features is maximized when two features are the same\".\" is wrong. **Yourselves find it wrong in response to our answer [A17].** From your next question, take the loss $\\|z^1-z^2\\|^2$ for example, its optimal is obtained when $z^1=z^2$. Thus, we do not agree with the point you raised \"For example, a classical approach to show whether the optimal point is the same is whether two methods have the same Lagrange multiplier.\".  \n\n\n>Q20: This is a question to our response [A11]. \"As we pointed out previously, we did not extensively discuss matrix entropy because it is not our focal point and not a concept we defined.\"......as a tool ([A1])\n\nA20: As we emphasized previously, this paper mainly uses matrix information theory as a tool. **All the matrix information theoretic quantities are well-defined and easy to understand from their definition.**\n\n\n\n>Q21: This is a question to our response [A12]. The current answer makes sense. I thought that the original response (\"when the Siamese structure degenerates into a single branch like MAE\") does not mean to set H(X, X), but I thought that it introduces a new single variable entropy again.\n\nA21: **We are glad that you found that we are correct and you misunderstood what we discussed previously.**\n\n\n\n>Q22: This is a question to our response [A13]. In my opinion, the fully fine-tuning evaluation as MAE protocol and the linear probing evaluation as SimCLR and VICReg are not directly comparable in terms of the test set hacking. However, I think it could be somewhat acceptable if the other reviewers and the area chair agree that the hyperparameter sensitivity and the test set hacking are not a serious problem as reject.\n\nA22: **We do ablation studies on hyper-parameters just like the work SimCLR and VICReg did.** So we can not agree with you that \"In my opinion, the **fully fine-tuning** evaluation as MAE protocol and the linear probing evaluation as SimCLR and VICReg are not directly comparable in terms of the test set hacking.\" **Note the ablation study we done is linear probing.**\n\n\n\n>Q23: This is a question to our response [A14]. Please check [A10].\n\nA23: As your question is same as Q19. We have already discussed it.\n\n\n>Q24: This is a question to our response [A15]. I think the answer is not a perfect one, but it could be an alternative with an empirical analysis. I think citing the proposition of Garrido et al. could be helpful for making the paper better\n\nA24: Thank you for your suggestion. We will add a new remark at the end of Appendix A based on our answer [A15]."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657659916,
                "cdate": 1700657659916,
                "tmdate": 1700707406004,
                "mdate": 1700707406004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "No9uoXAnkQ",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 5 to reviewer qQv9's new questions"
                    },
                    "comment": {
                        "value": ">Q25: This is a question to our response [A16]. \"MAE is quite different from the other SSL methods\" does not mean that it is okay to hack the test set for SSL. As my comment in [A13], I think the current evaluation needs an agreement among all the reviewers and AC whether the evaluation protocol has no problem.\n\nA25: We have discussed in our initial response [A16]. We do ablation studies on hyper-parameters just like the work SimCLR and VICReg did. Moreover, in the initial MAE paper (https://arxiv.org/pdf/2111.06377.pdf), its Figure 5 is ablation study on the hyper-parameter mask ratio. **In MAE's Figure 5, we can see its linear probing accuracy drops from 71.8 to 66.1 when mask ratio changes from 0.8 to 0.9.**\n\n>Q26: This is a question to our response [A17]. Thanks for the clarification. I forgot to mention the second condition. I will re-state my argument: Theorem 3.2 shows that the matrix mutual information between two features is maximized when two Gram matrices are the same and each feature dimension is invariant -- the Gram matrix has 0 off-diagonal.\n\nA26: **We are glad that you found that we are correct and you misunderstood what we discussed previously.**"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657719997,
                "cdate": 1700657719997,
                "tmdate": 1700707504164,
                "mdate": 1700707504164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nMtmZqFadu",
                "forum": "WfjJOEfAf7",
                "replyto": "8nUWxxuQkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Reviewer_qQv9"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nBefore starting, I listed my initial concerns:\n\n- Re-checking the major motivation (W2, W5)\n- The mathematical notations and theoretical results (W1, W2, W4)\n- Insufficient experiments (W6)\n- Fixing the fundamental flaw -- hyperparameter sensitivity -- of the proposed method (W3)\n\nThe rationale for the strong reject was mainly because (1) I suspect that the concept of matrix entropy is misused without a careful understanding of the concept, (2) I think the empirical contribution is also somewhat insufficient. Therefore, I evaluated this paper as having poor soundness and contribution. Similarly, as the link between Section 3 and 4 is not clear and the concept of the matrix information is not properly provided, I evaluated the presentation as 1.\n\nAfter long discussions, I re-evaluated the paper based on the revised version as weak reject (5).\n\n- First, my strongest concern was the concept of matrix entropy itself is misused by the paper due to the \"typo\" in the paper. I don't think the current theoretical results are precise enough (e.g., \"encouraging\" or \"assuming the solutions\" is not proper proof in my opinion), but I think some of the theoretical contributions are acceptable but below the borderline. Regarding **Misinterpretation of simple math concepts** in the comment for Reviewer znaA, I clarified that I did make a mistake when I wrote down notations in markdown, not did not mean that I misunderstood the theorem. I correctly understood the proof and therefore raised concerns.\n- Second, I still think that Section 3 and 4 are somewhat unrelated. Section 3 is for contrastive learning methods and Section 4 is newly introduced for MAE. Therefore I raised concerns about (1) Section 3 is based on joint entropy but 4 is based on single entropy. Although the authors clarified that the joint entropy of self-variables is the same as the single entropy, but I still think that the connection between theorems in section 3 and the newly introduced regularization in section 4 are not well linked. I partially agree with the comments (e.g., by citing DiME), but these discussions are not included in the revised paper. Even if assuming all the comments are updated in the revised paper, I am still slightly skeptical about the relationship between the theorems and the proposed regularization method\n- Third, in terms of the evaluation, I still think that directly searching for the best hyperparameter on the test set is not acceptable. However, (1) assuming that the main contribution of this paper is based on bridging the concept of matrix information and SSL (although theorems are for contrastive learning) and (2) as my opinion is the only strong opinion for the evaluation, I modified my evaluation on the empirical evaluation as \"okay\". As I am not certain about this part, I modified my confidence from 5 to 4. Certainly, more comparisons with other SSL methods will make this evaluation more stronger.\n\nBased on these criteria, I modified my score to borderline reject, which means I think this paper still has a large room for improvement, but at the same time, I will respect all the decisions by the other reviewers and area chair."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735571046,
                "cdate": 1700735571046,
                "tmdate": 1700735664789,
                "mdate": 1700735664789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IJXhQcPogI",
            "forum": "WfjJOEfAf7",
            "replyto": "WfjJOEfAf7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_cGgm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_cGgm"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider a theoretical framework to analyze and enhance self-supervised learning (SSL) methodologies utilizing matrix information theory. The work is particularly focused on providing a unified lens for examining both contrastive and feature decorrelation-based SSL paradigms through the application of matrix mutual information and joint entropy.\n\nSSL, a significant branch of unsupervised learning, leverages unlabeled data to learn representations by predicting certain input parts from others. The authors' investigation into the utility of matrix mutual information in SSL is notable. By extending mutual information to the matrix domain, the manuscript aims to elucidate the dependencies among various features or representations within SSL models, shedding light on the information propagation mechanisms within neural networks. Additionally, the manuscript's exploration of joint entropy in the context of SSL is insightful. Assessing how the uncertainty in the input data influences the learning process and the quality of the learned representations can be crucial for enhancing model robustness and efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Theoretical Innovation**: The manuscript presents a novel theoretical framework for analyzing self-supervised learning (SSL) methods through the prism of matrix information theory. The use of matrix mutual information and joint entropy is an innovative approach that could provide new insights into the dependencies between features and the propagation of information within neural networks. This theoretical advancement has the potential to deepen our understanding of SSL mechanisms, making it a significant contribution to the field.\n\n**Unified Analysis for Diverse SSL Approaches**: By offering a unified analytical lens for both contrastive and feature decorrelation-based SSL paradigms, the paper bridges a gap in the current literature. This comprehensive approach allows for a more holistic understanding of SSL and its various implementations, enhancing the ability to compare and improve upon different methods within a common theoretical framework.\n\n**Potential for Enhanced Robustness and Efficiency**: The exploration of joint entropy in SSL models addresses the critical aspect of input data uncertainty. By theoretically examining how this uncertainty affects the learning process, the paper lays the groundwork for developing more robust and efficient SSL algorithms that can better handle real-world data variability."
                },
                "weaknesses": {
                    "value": "**Scalability of Information-Theoretic Measures**: A potential weakness could be the lack of a clear discussion on the scalability of the proposed matrix mutual information and joint entropy measures. Calculating these metrics can be computationally intensive, especially for large-scale datasets and high-dimensional feature spaces typical in self-supervised learning. Any insights on the computational overheads is much appreciated"
                },
                "questions": {
                    "value": "**Generalization to Diverse Architectures**: Your paper appears to focus on a specific class of self-supervised learning models. How generalizable is your matrix information-theoretic approach to other SSL architectures, such as transformer-based or recurrent neural networks? Can you provide empirical evidence or theoretical justification for the generalizability of your approach?\n\n**Robustness and Sensitivity Analysis**: How robust are your matrix information-theoretic measures to variations in SSL hyperparameters, such as temperature in contrastive learning or weight decay? Could you provide a sensitivity analysis that examines the stability of your proposed metrics under different hyperparameter settings?\n\nThese questions are intended to probe the empirical validation of theoretical insights, the generalizability of the approach to various architectures, and the robustness of the proposed metrics to hyperparameter variations. Addressing these points could significantly strengthen the paper's contributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698932677248,
            "cdate": 1698932677248,
            "tmdate": 1699636048397,
            "mdate": 1699636048397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "im4Nnr5iKj",
                "forum": "WfjJOEfAf7",
                "replyto": "IJXhQcPogI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cGgm"
                    },
                    "comment": {
                        "value": "The three questions you raised are very insightful, and we will discuss them in details as follows.\n\n>Q1: Scalability of Information-Theoretic Measures: A potential weakness could be the lack of a clear discussion on the scalability of the proposed matrix mutual information and joint entropy measures. Calculating these metrics can be computationally intensive, especially for large-scale datasets and high-dimensional feature spaces typical in self-supervised learning. Any insights on the computational overheads is much appreciated\n\nA1: Thank you for your suggestion. **Though our paper mainly focus on give theoretical understanding of SSL using matrix information theory, we will discuss the computational aspect of it as follows.** Our metrics are calculated based on embeddings. In self-supervised learning, the dimension $d$ of embeddings usually does not exceed 1024, which is not that high dimensional. When we calculate the empirical covariance matrix, the size will always be $d \\times d$, so the large scale dataset is not a bottleneck for calculating the information-theoretic measures. To reduce the computation burden, one may sample batches of samples from the data and approximately calculate the information-theoretic quantities. Thus the overall computation overhead of calculating matrix information-theoretic measures in SSL will be moderate.\n\n>Q2: Generalization to Diverse Architectures: Your paper appears to focus on a specific class of self-supervised learning models. How generalizable is your matrix information-theoretic approach to other SSL architectures, such as transformer-based or recurrent neural networks? Can you provide empirical evidence or theoretical justification for the generalizability of your approach?\n\nA2: Thank you for your feedback. Our improvement M-MAE is agnostic to the underlying structure and solely focuses on the loss function. **The reason we only implemented the transformer is that MAE and its follow-up works depend on the transformer architecture, not because our method is reliant on it.** Recurrent neural networks would be interesting to explore, but we haven't come across any papers that demonstrate how to incorporate them in vision SSL. If you can provide some references, we would be willing to experiment with them.\n\n>Q3: Robustness and Sensitivity Analysis: How robust are your matrix information-theoretic measures to variations in SSL hyperparameters, such as temperature in contrastive learning or weight decay? Could you provide a sensitivity analysis that examines the stability of your proposed metrics under different hyperparameter settings?\n\nA3: That is a great question!  We conduct empirical studies on a famous contrastive learning method SimCLR which uses the InfoNCE loss. One of the important hyper-parameters in SimCLR is the temperature in InfoNCE loss. We plot the matrix mutual information and matrix joint entropy during the pretraining of SimCLR on CIFAR-10 with different temperatures. The matrix mutual information is in the Figure 1 (https://i.postimg.cc/J7bhbTdm/matrix-mutual-information.png). The matrix joint entropy is in Figure 2 (https://i.postimg.cc/Pr0yD5LP/matrix-joint-entropy.png). We set temperatures as 0.3, 0.5, 0.7. **From the Figure 1 and 2, we can observe that the increase of matrix mutual information or matrix joint entropy during training ties closely with the final accuracy.** As temperature = 0.3 outperforms 0.5 and 0.7 in KNN accuracy, it also has the biggest matrix mutual information and matrix joint entropy value."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443808498,
                "cdate": 1700443808498,
                "tmdate": 1700446041456,
                "mdate": 1700446041456,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o4yV4lvD4F",
            "forum": "WfjJOEfAf7",
            "replyto": "WfjJOEfAf7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_znaA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1221/Reviewer_znaA"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the information flow of three mainstream self-supervised learning methods: contrastive learning; feature de-correlation; and masked auto-encoding. It successfully connects all three methods with matrix information theory and thus offering a unified view. And beyond that it proposes to add an additional term to the original MAE loss. The loss regularizes the latent codes and is shown to be helpful on image classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper introduces matrix information theory to understand and connect mainstream methods in self-supervised learning, which is a very meaningful and valuable contribution.\n+ The writing is fairly clear, although I did not delve into the mathematical details, I believe they are sounds.\n+ The initial results on image classification (both the linear-proving and the fine-tuning results) are great."
                },
                "weaknesses": {
                    "value": "- While the initial empirical results are great, I do hope to see the final results after having a complete run on MAE. The current version of the paper uses U-MAE's implementation and the hyper-parameters (e.g., batch size) do not follow the settings in MAE. This can cause some discrepancies. MAE's ViT-L, after convergence, can achieve an accuracy of ~85.5 on ImageNet. While the paper's result is promising, it is unclear the trend can still hold. So I would be curious to see. If it is too much of a computation burden, I am fine to see results on CIFAR-100.\n- There are some definitions used before they are defined (e.g., TCR is defined in the appendix). It would be great to at least point to them."
                },
                "questions": {
                    "value": "* The TCR loss on the latent codes, how does it contribute to the entropy of the model? Is there a similar plot one can show as the training of M-MAE proceeds to Figure 1/2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699150464826,
            "cdate": 1699150464826,
            "tmdate": 1699636048314,
            "mdate": 1699636048314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uRclaxndcH",
                "forum": "WfjJOEfAf7",
                "replyto": "o4yV4lvD4F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer znaA"
                    },
                    "comment": {
                        "value": ">Q1: While the initial empirical results are great, I do hope to see the final results after having a complete run on MAE. The current version of the paper uses U-MAE's implementation and the hyper-parameters (e.g., batch size) do not follow the settings in MAE. This can cause some discrepancies. MAE's ViT-L, after convergence, can achieve an accuracy of ~85.5 on ImageNet. While the paper's result is promising, it is unclear the trend can still hold. So I would be curious to see. If it is too much of a computation burden, I am fine to see results on CIFAR-100.\n\nA1: Thank you for your suggestion. We do experiments mainly based on U-MAE for two reasons. One is that it improves the accuracy of MAE based on a modification of MAE's loss function and our work also changes the loss function by adding an additional entropy regularizer. **The other is that, as we have shown in our initial manuscript, U-MAE can be seen as a second-order approximation of our M-MAE loss, making it a much-related baseline compared to MAE.** We did our initial experiments following the configuration of U-MAE, so we only implemented 200 epochs. \n\nWe have conducted experiments on CIFAR-100. The hyper-parameters are similar to U-MAE, and we set $\\mu=1$ and pretrain CIFAR-100 for 1000 epochs. M-MAE may use hyperparameters that are not identical to U-MAE to fully reflect its potential. However, due to time constraints, we were unable to extensively search for these hyperparameters. We believe that with more reasonable hyperparameters, M-MAE can achieve even better results. As shown in the table below, our method performs remarkably well, even without an exhaustive hyperparameter search.\n\n| Method | linear@1 | linear@5 | finetune@1 | finetune@5 |\n| --- | --- | --- | --- | --- |\n| M-MAE(vit-base) | **60.9** | **88.1** | 83.8  | **97.0**  |\n| U-MAE(vit-base) | 59.7 | 86.8 | 84.1 | 96.5 |\n| MAE(vit-base) | 59.2 | 86.8 | 84.5 | 96.6 |\n\n>Q2: There are some definitions used before they are defined (e.g., TCR is defined in the appendix). It would be great to at least point to them.\n\nA2: Thank you for your feedback. **The TCR is already defined in definition 3.4 of the initial manuscript, which is before they are used.**\n\n>Q3: The TCR loss on the latent codes, how does it contribute to the entropy of the model? Is there a similar plot one can show as the training of M-MAE proceeds to Figure 1/2?\n\nA3: Thank you for your question. In our paper, the entropy of the model means the matrix entropy of the latent codes' covariance matrix. As we show in the paper, matrix entropy is directly related with effective rank. This definition of model's entropy aligns with the literatures that use effective rank to measure the dimension of the model's representation [1] [2]. For the matrix entropy plot you required, we plot it along our pretraining on CIFAR-100 (Figure is shown in https://i.postimg.cc/tT48m774/matrix-entropy.png). **From the Figure, it is clear that M-MAE has an increasing entropy during training, which aligns well with our theory.**\n\n\n\n\n[1]: How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders, Qi Zhang, Yifei Wang, Yisen Wang, NeurIPS 2022\n\n[2]: RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank, Quentin Garrido, Randall Balestriero, Laurent Najman, Yann LeCun, ICML 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401767947,
                "cdate": 1700401767947,
                "tmdate": 1700410777909,
                "mdate": 1700410777909,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]