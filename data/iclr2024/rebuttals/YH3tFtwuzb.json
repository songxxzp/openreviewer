[
    {
        "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models"
    },
    {
        "review": {
            "id": "081Fqy4OeW",
            "forum": "YH3tFtwuzb",
            "replyto": "YH3tFtwuzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes how to perform BitFit parameter-efficient fine-tuning of bias terms only with the addition of noise for differential privacy. BitFit is known to be effective and very parameter efficient, and it works well with DP because the number of trained parameters is so low."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A strong case is made for using the combination of BitFit and DP-SGD. The memory and time savings are efficient, both in theory and according to the extensive experiments. It feels somewhat incremental, but the experimental contribution demonstrating that the comination works well is still significant."
                },
                "weaknesses": {
                    "value": "Contrary to the claims in the paragraph \"novelty\", it seems to me there is not a huge creative leap in putting together BitFit and DP-SGD as is done here. Is there something I am missing? What is the \"substantial algorithmic innovation\"? What is the naive way of combining these ideas that your algorithm improves upon?\n\nThe writing is unclear in places:\n* The authors should study how to use \\citep and \\citet appropriately.\n* Why is $C_i$ given as an input to Algorithm 1? I wonder if you mean the clipping function $C$.\n\nI think this work is useful, and should be shared at a workshop maybe, but unless my concerns about novelty are satisfied I'm not sure it merits publishing at a top-tier conference."
                },
                "questions": {
                    "value": "* See question about novelty above.\n* Is this any better from a clipping perspective? Do you have to materialize the per-example bias gradients in order to compute their norms? You could combine this with ghost clipping, right?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698178076018,
            "cdate": 1698178076018,
            "tmdate": 1700696343690,
            "mdate": 1700696343690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "924eZxIDQ1",
                "forum": "YH3tFtwuzb",
                "replyto": "081Fqy4OeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comment. Here is a point-to-point response.\n\n1. Contrary to the claims in the paragraph \"novelty\", it seems to me there is not a huge creative leap in putting together BitFit and DP-SGD as is done here. What is the \"substantial algorithmic innovation\"? What is the naive way of combining these ideas that your algorithm improves upon?\n\n**Response** We agree that BiTFiT and DP-SGD are both existing methods. However, our main contributions also include the complexity analysis and engineering implementation. 1) Complexity analysis of DP parameter-efficient fine-tuning (PEFT): DP-BiTFiT in main text, DP-LoRA and so on in appendix C. This is a missing piece in previous DP PEFT literature and significantly helpful in determining the benefit of applying different PEFT methods. E.g. we rigorously show the complexity saving is 50%, and the benefit of BiTFiT on long-context language tasks or high-resolution image tasks, i.e. the computation overhead of DP-BiTFiT in the last column of Table 2 is independent of T. This analysis is also missing in the non-DP BiTFiT. Without this analysis, our understanding of efficient fine-tuning methods will be at most empirical, and certain special properties of DP-BiTFiT (like activation-free optimization and feature-dimension-free overhead) will not be revealed. 2) Engineering effort: at the time of writing this paper, none of existing codebases including GhostClip and Opacus remove the forward hooks, because no analysis has established that only BiTFiT, not LoRA/Adapter/Compactor or full fine-tuning, can be activation-free. This is a simple yet effective trick to save memory in large model training. Our code will be open source to benefit the whole DP community.\n\nIn summary, the algorithmic innovation that improves the naive way of combining is visualized in Figure 2: we significantly simplify the computation graph from left and upper to the DP-BiTFiT (bottom right), with complexity guarantee.\n\n2. Why is Ci given as an input to Algorithm 1? I wonder if you mean the clipping function C.\n\n**Response** We meant Ci is initialized as 1, so that if one ignores all red lines in Algorithms 1, one can recover the non-DP SGD (which still uses Ci in line 8 because we want to write the DP/non-DP SGD in a unified way).\n\n3. Is this any better from a clipping perspective? Do you have to materialize the per-example bias gradients in order to compute their norms? You could combine this with ghost clipping, right?\n\n**Response** No. We discussed this above Algorithm 1, in the \"Novelty\" paragraph that ghost clipping only works for the weight, not the bias. We have to materialize the per-sample bias gradients but this is extremely cheap. There are about 0.1% parameters in biases and materializing per-sample bias gradients only incurs $O(Bp)$ time/space complexity, in contrast to $O(BTpd)\\approx O(BTp^2)$ of per-sample weight gradients. This is empirically confirmed in Figure 4, including on GPT2-large.\n\nWe hope the reviewer can reconsider the score given that the complexity analysis is an important component of this work. Even though the methods are not new, a theoretical analysis could be insightful and novel, like those convergence analysis to the SGD algorithm."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533485069,
                "cdate": 1700533485069,
                "tmdate": 1700533485069,
                "mdate": 1700533485069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OpNWHURaSm",
                "forum": "YH3tFtwuzb",
                "replyto": "924eZxIDQ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment.\n\nRegarding novelty: the \"complexity analysis\" in Appendix C seems pretty basic. (I don't think it is even strictly correct: you probably want to be using $O$ notiation since you are adding the time of different types of operations, and summation of $B$ elements can be done in fewer than $B$ addition operations.) The engineering aspect to remove the forward hooks is nice, but not really so challenging to merit publication in a top-tier conference. So while I would even say this method looks very promising and I could see it being very useful, the novelty and creativity is on the low side.\n\nRegarding Algorithm 1, it is confusing that $C$ is both a function and $C_i$ is the clip of a round, and $C_i$ can be input to the function. Maybe rewrite it so line 8 is different depending on whether DP is added (clips or does not clip during summation).\n\nI don't understand why you cannot combine with ghost clipping. The paragraph you mentioned says that they are \"orthogonal\", which to me sounds like they can both be used and each be beneficial. In what sense does ghost clipping \"only work on the weights\"? It scales the entire gradient, weights and biases alike.\n\nI hope you can answer these questions while the discussion period is ongoing. Because I think the method is useful (if not very original) I will provisionally raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665780462,
                "cdate": 1700665780462,
                "tmdate": 1700665780462,
                "mdate": 1700665780462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SVJn6HJNSC",
                "forum": "YH3tFtwuzb",
                "replyto": "081Fqy4OeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for extending the discussion. We would like to keep the current complexity analysis without $O$ notation because 1) The complexity is computed correctly up to leading terms, e.g. if the actual complexity is 3BTp+5Bp, the leading term is cubic and leaves 3BTp. 2) The big $O$ notation is an asymptotic symbol hence not precise. In full fine-tuning, standard non-DP optimization takes 6BTpd time complexity and GhostClip takes 10BTpd, the significant slowdown is only depicted without writing both as $O(BTpd)$. 3) We agree that the **time** of B operations is not B times, due to parallelism of GPU; however, it still holds that the theoretical complexity increases by B times.\n\nWe have rewritten Algorithm 1 to include your suggestion.\n\nRegarding ghost clipping, it is a technique that computes the per-sample weight gradient norm without materializing the per-sample weight gradient. It specifically leverages activation and output gradient (see Equation 3 of  https://arxiv.org/abs/2110.05679) via an algebraic trick, but the bias gradients do not need the activation (see our Equation 4; this activation-free property is key to BiTFiT). Therefore, 1) GhostClip only applies on the weights, not the bias; 2) GhostClip has time/space complexity $O(BT^2)$ making it inefficient on large $T$ tasks such as long context and high-resolution image (see Section 3.1 of https://arxiv.org/pdf/2210.00038.pdf), but per-sample bias gradients are computed with complexity independent of $T$. These two distinct differences make the computation of weight/bias gradients \"orthogonal\".\n\nWe are happy to discuss further if needed."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669308726,
                "cdate": 1700669308726,
                "tmdate": 1700706196932,
                "mdate": 1700706196932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rfo2i7mih0",
                "forum": "YH3tFtwuzb",
                "replyto": "SVJn6HJNSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "content": {
                    "comment": {
                        "value": "I still think it is incorrect and misleading to count the number of \"operations\" without big O notation. It is not meaningful to say that \"the total time complexity is $3Bp$\". For example, you are considering taking a square (a multiplication) to be equivalent to summing (an addition). These operations may take different amounts of time, which is why we usually hide the constants behind the big O. Furthermore, as I tried to allude earlier, finding the sum of $n$ elements requires fewer than $n$ additions. To say \"big O notation is an asymptotic symbol hence not precise\" has it precisely backwards: big O conveys what is important (asymptotic behavior) while supressing what is unimportant (whether an addition or multiplication operation takes longer).\n\nI don't understand what you mean \"the bias gradients do not need the output gradient\". All of the parameters' gradients depend on the output: it's only via the output that a parameter has any influence on the loss. You even say in the paper \"the output gradient is used to compute the per-sample gradient of weights and biases\". What am I misunderstanding?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696312458,
                "cdate": 1700696312458,
                "tmdate": 1700696312458,
                "mdate": 1700696312458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CZmqkfzMle",
                "forum": "YH3tFtwuzb",
                "replyto": "081Fqy4OeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to clarify that it is norm to measure time complexity in float-point operation (FLOP), so that either a multiplication or an addition is one operation, despite two operations may cost different wall-clock time. Within this measure, our analysis can give precise calculation of complexity, without using big O. One particular benefit of using our notation (instead of using big O) is that we can infer the wall-clock time: e.g. standard SGD takes 6BTpd time complexity, GhostClip takes 10BTpd, standard BiTFiT takes 4BTpd, and our DP-BiTFiT takes 4BTpd+Bp. This translates to very accurate estimate of running time: GhostClip takes 1.7X more time than SGD, BiTFiT (DP and standard) takes 66% time of SGD.\n\nWe can also write DP-BiTFiT complexity in different operations, e.g. (only for illustration purpose, not the actual breakdown) $1.8BTpd\\*O_{multiplication}+2.2BTpd\\*O_{addition}+Bp\\*O_{other}$. Notice that our main conclusions will hold in either notation: for instance, 1) DP-BiTFiT is almost as fast as BiTFiT, 33% faster than full fine-tuning; 2) the overhead ($Bp\\*O_{other}$) is independent of $T$, hence DP-BiTFiT is uniquely suitable for high dimension tasks.\n\nI made a typo in my previous response. Let me correct it by stating \"the bias gradients do not need the activation\", not the output gradient. I believe there is a misalignment of output gradient here. It is not the gradient of loss w.r.t. the last layer's output. In fact, it is the gradient of loss w.r.t. each layer's output. It would be clear to say: 1) the k-th layer's weight gradients need k-th layer's activation and k-th layer's output gradient. 2) the k-th layer's weight gradients need only k-th layer's output gradient. Therefore, GhostClip does not apply to weight gradients because it explicitly works with the outer product of activation."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706147071,
                "cdate": 1700706147071,
                "tmdate": 1700706285732,
                "mdate": 1700706285732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUVz3itrdR",
                "forum": "YH3tFtwuzb",
                "replyto": "CZmqkfzMle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_tcjD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713547154,
                "cdate": 1700713547154,
                "tmdate": 1700713547154,
                "mdate": 1700713547154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h7Hzjjy9fW",
            "forum": "YH3tFtwuzb",
            "replyto": "YH3tFtwuzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_BrY1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_BrY1"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the differentially private fine-tuning of large pre-trained models. The key novelty of the paper is that the authors show that fine-tuning only the bias term can match or even outperform the SOTA DP fine-tuning algorithm in different tasks. By such bias-term fine-tuning idea, the number of trainable parameters can be even smaller than previous parameter-efficient fine-tuning methods (e.g., LORA), and faster and more memory-efficient than the full-parameter DP fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents the key idea with detailed and persuasive motivation.\n2. The idea proposed in this paper can bring significant advantages in reducing computation costs while improving model utility.\n3. The authors demonstrate relatively comprehensive experiments to support the claim advantages, including text classification, natural language generation, and image classification."
                },
                "weaknesses": {
                    "value": "The paper is pretty much self-contained, there may be only some concerns on how general this method can be.\n1. The paper can provide more intuitions about why fine-tuning the bias term can be effective enough with pre-trained models. As a comparison, LoRA is supported by a strong intuition that fine-tuning updates can be considered low-ranks.  \n2. The limitation of the proposed algorithm may deserve some more discussion. While remark 4.2 mentioned that the method may be less effective for models with convolutional layers without bias terms, it is not clear whether all modules' bias terms (attention, fully connected, convolutional, etc.) have the same effect or some of them are more important than others. Besides, as some recent LLM architecture may not include biases in some layers (PaLM [0]) to increase training stability, it is unclear how general the proposed method can be in practice.\n\n\n[0]Chowdhery, Aakanksha, et al. \"Palm: Scaling language modeling with pathways.\" arXiv preprint arXiv:2204.02311 (2022)."
                },
                "questions": {
                    "value": "1. Is there any potential limitations of the proposed algorithm in terms of the fine-tuning tasks? Can we expect similar improvement if we fine-tune for more complicated tasks, for example fine-tune LLaMA for GSM8k or for MMLU tasks?\n2. Is there any requirement or conclusion about how well-pre-trained a model should be or how complex a model should be so that the proposed method can be effective? An inappropriate extreme example might be that one should not expect tuning the bias in a linear model to fit arbitrarily shifted distribution.\n3. Maybe related to the above question, is there intuition to support the proposed method?\n4. Are the bias terms in different modules (attention, fully connected, convolutional, etc.) have the same effects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Reviewer_BrY1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698538845545,
            "cdate": 1698538845545,
            "tmdate": 1699636741904,
            "mdate": 1699636741904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VKBlz9dFQv",
                "forum": "YH3tFtwuzb",
                "replyto": "h7Hzjjy9fW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comment and the positive feedback. Here is a point-to-point response.\n\n1. The paper can provide more intuitions about why fine-tuning the bias term can be effective enough with pre-trained models. As a comparison, LoRA is supported by a strong intuition that fine-tuning updates can be considered low-ranks.......is there intuition to support the proposed method?\n\n**Response** We agree more understanding of why BiTFiT works is desirable. This has been difficult even for the original paper in the non-DP regime. Looking backwards from the effectiveness of BiTFiT, we may have a intuition that the fine-tuning updates are generally in low-dimension, as is also the case for LoRA (despite the rank, the number of trainable parameters is small). We may test this hypothesis by randomly selecting a small number of parameters to update at each layer. However, even if any small number of parameters would work, BiTFiT has its unique advantage of being activation-free.\n\n2. While remark 4.2 mentioned that the method may be less effective for models with convolutional layers without bias terms, it is not clear whether all modules' bias terms (attention, fully connected, convolutional, etc.) have the same effect or some of them are more important than others. Besides, as some recent LLM architecture may not include biases in some layers (PaLM [0]) to increase training stability, it is unclear how general the proposed method can be in practice.......Are the bias terms in different modules (attention, fully connected, convolutional, etc.) have the same effects?\n\n**Response** We haven't looked into a finer-grained analysis of bias terms. This could be an interesting future work had more people become interested in our method. We agree that not all models are suitable for DP-BiTFiT, but a long list of models in our Table 1 should be. Additionally, we study DP-BiTFiT as a standalone method for fair comparison to other standalone methods. In practice, one can always combine two parameter-efficient methods, such as DP-BiTFiT and last-layer training in Section 4.3. Specifically, turning on DP-BiTFiT only needs one line of code: [param.requires_grad_(False) for name, param in model.named_parameters() if 'bias' in name].\n\n3. Is there any potential limitations of the proposed algorithm in terms of the fine-tuning tasks? Can we expect similar improvement if we fine-tune for more complicated tasks, for example fine-tune LLaMA for GSM8k or for MMLU tasks?\n\n**Response** I am experimenting with LLaMA and do observe similar behavior.\n\n4. Is there any requirement or conclusion about how well-pre-trained a model should be or how complex a model should be so that the proposed method can be effective? An inappropriate extreme example might be that one should not expect tuning the bias in a linear model to fit arbitrarily shifted distribution.\n\n**Response** Our experiments in Table 3,4,5 show that DP-BiTFiT tends to be more comparable to full fine-tuning when model sizes are larger. This also implies that a better pre-trained model can benefit from DP-BiTFiT more: tuning only bias terms from scratch is not going to work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534846042,
                "cdate": 1700534846042,
                "tmdate": 1700534846042,
                "mdate": 1700534846042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jtFDcw8PRs",
                "forum": "YH3tFtwuzb",
                "replyto": "VKBlz9dFQv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_BrY1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Reviewer_BrY1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response from the authors. I leave my score as it is now, as the authors partially agree that my concerns may need more effort to be resolved in the future."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676505707,
                "cdate": 1700676505707,
                "tmdate": 1700676505707,
                "mdate": 1700676505707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vT1o1tUOLx",
            "forum": "YH3tFtwuzb",
            "replyto": "YH3tFtwuzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_3Kry"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_3Kry"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies private fine-tuning of large models using a parameter-efficient method. This work proposes DP-BitFit, differentially private bias-term fine-tuning, which updates bias terms during training under the DP-SGD framework.\n\nWhile updating full parameters in DP-SGD would increase memory consumption and slow down the training process, DP-BitFit only needs to update 0.1% of the parameters, making it much faster than updating all parameters. This work supports this claim by providing both the time and space complexity and experimental comparisons. Another advantage of DP-BitFit is that it does not require a forward hook.\n\nThis work conducts experiments on both NLP tasks and computer vision (CV) tasks, considering privacy constraints within the ranges of [3, 8] for NLP and [1, 8] for CV."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated and easy to follow. Specifically, the theoretical analysis of space and time complexity of different methods is comprehensive. \n\n2. The experiments include both cv and nlp tasks, that are two of the main applications of foundation model use. The evaluation is comprehensive and ablates the model size, mode architectures, privacy levels."
                },
                "weaknesses": {
                    "value": "1. My main concern with this paper is the novelty of this method. It appears that this method directly adapts the existing method BitFit to the DP-SGD. \n\n2. The performance of DP-BitFit is limited in some scenarios and requires the additional design for two phases, which makes the results of DP-BitFit less significant.\n\nMinors:\n1. Presentation issues: The table number and title should appear before the table (Table 7- 16). It would be better to be more careful with the use \\citep and \\citet."
                },
                "questions": {
                    "value": "1. In Table 4, it is somewhat weird to me that the perplexity of non-private results is worse than private results in several cases. Also, Table 4 and Table 13 show that DP-BitFiT on GPT2-large is better than DP-BiTFiT is better than DP-SGD(full), I wonder if this is due to the dimensionality issue in DP-SGD, or sub-optimal hyperparameters for DP-SGD (full), or if it is because the comparison is not averaged across several runs for statistical significance.  \n\n2. Comparison with DP parameter efficient methods for efficiency. The authors provide the theoretical comparison of DP-BitFit with other DP parameter-efficient methods. Figure 3 and Figure 4 compared the memory, speed, maximum throughput and batch size for DP-BitFit and full parameter updating methods. It would be better to also include the comparison to other DP parameter efficient methods such as DP-LoRA."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6562/Reviewer_3Kry"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613380681,
            "cdate": 1698613380681,
            "tmdate": 1699636741795,
            "mdate": 1699636741795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i4jsI5vkMl",
                "forum": "YH3tFtwuzb",
                "replyto": "vT1o1tUOLx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comment. We would like to clarify a misunderstanding in the \"Weakness\". **We do not require \"the additional design for two phases.\"** This was an alternative method presented in older version of this paper, hence it should not be treated as a main part of this work on which the review is based.\n\n1. Regarding the novelty, we claim at least 2 types of new contributions: 1) Complexity analysis of DP parameter-efficient fine-tuning (PEFT): DP-BiTFiT in main text, DP-LoRA and so on in appendix C. This is a missing piece in previous DP PEFT literature and significantly helpful in determining the benefit of applying different PEFT methods. E.g. we rigorously show the complexity saving is 50%, and the benefit of BiTFiT on long-context language tasks or high-resolution image tasks. This analysis is also missing in the non-DP BiTFiT. 2) Engineering effort: at the time of writing this paper, none of existing codebases including GhostClip and Opacus remove the forward hooks, because no analysis has encouraged to do so. Note our code will be open source to benefit the whole DP community.\n\n2. Regarding Table 4, we believe the situation is natural: the perplexity is only one of many performance measures other than BLEU. In fact, it is not unusual sometimes the cross-entropy loss (i.e. perplexity) is at odds with the performance measure, e.g. in \\url{https://arxiv.org/pdf/2106.07830.pdf} Figure 8. We are not sure why DP-BiTFiT seems to outperform DP-full on GPT2-large, though we do consistently observe that the gap is smaller as we scale up the model sizes. We have tuned the hyperparameters of DP-full carefully so our hypothesis is the dimensional issue.\n\n3. We haven't compared to DP-LoRA in all experiments. We are happy to do so in the camera-ready revision if the reviewer can consider raising the score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531816018,
                "cdate": 1700531816018,
                "tmdate": 1700531816018,
                "mdate": 1700531816018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9S5Qq5BUpE",
                "forum": "YH3tFtwuzb",
                "replyto": "vT1o1tUOLx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe have addressed your comments point-by-point, trying our best to clarify any confusion. We have changed all \\cite to \\citep in this revision and corrected the table captions.\n\nSpecifically, **we do not need two-phase training in this paper** (it's merely an alternative in the appendix, hence should not be counted towards *Weakness*). We sincerely hope you would reconsider your score if the score is based on the mis-understanding. \n\nWe would love to elaborate the significance of our complexity analysis and in the engineering efforts: removing the forward hooks is **never implemented in previous codebases** because only DP-BiTFiT is activation-free, not DP-LoRA or any other methods. Being activation-free is an important characteristics that make DP-BiTFiT uniquely suitable for high dimension (e.g. long context or high resolution image) tasks, which was not discovered in existing DP parameter-efficient fine-tuning literature which lacks the analysis on the computation graph and complexity.\n\nYour evaluation is important to us and we look forward to further discussion if needed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673381708,
                "cdate": 1700673381708,
                "tmdate": 1700706335177,
                "mdate": 1700706335177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BLbo76E3yy",
            "forum": "YH3tFtwuzb",
            "replyto": "YH3tFtwuzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_regQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_regQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of differentially private (DP) fine-tuning of large pre-trained models. Existing research has shown that high accuracy can be achieved under strong privacy constraints, but it often comes at the cost of significant computational overhead or modifications to the network architecture.\n\nThe authors propose  \"differentially private bias-term fine-tuning\" (DP-BiTFiT)  to strike a balance between accuracy and efficiency. DP-BiTFiT achieves state-of-the-art accuracy levels for DP algorithms while maintaining the efficiency of standard BiTFiT (fine-tuning without privacy constraints). The efficiency enables the application of DP fine-tuning to language and vision tasks involving long-sequence texts and high-resolution images, which were previously computationally challenging using existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Efficiency and Scalability: The paper presents an efficient and scalable approach to differentially private fine-tuning, which is crucial for handling large models and complex tasks. Compared with existing parameter-efficient DP fine-tuning, DP-BiTFiT is model-agnostic, i.e., it does not require modifications to the network architecture. The paper demonstrates that DP-BiTFiT outperforms DP full fine-tuning in terms of speed and memory usage, even surpassing the efficiency of standard full fine-tuning. \n\nPractical Applicability: The ability to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images expands the practical applications of privacy-preserving machine learning."
                },
                "weaknesses": {
                    "value": "**Limited contribution** Although the paper emphasizes that DP-BiTFiT is not merely adding differential privacy to an existing method (BiTFiT), it is hard to find much evidence for this point. Removing the forward hook is quite natural as one does not have to compute the gradient on weights. This is a natural choice when combining BiTFiT and differential privacy. Given existing results of DP full fine-tuning and parameter-efficient fine-tuning, the contribution of this paper is rather incremental.\n\nMoreover, the empirical evaluation shows performance drop of DP-BiTFiT compared with other parameter-efficient fine-tuning techniques.\nThe paper mentions that DP-BiTFiT is efficient for a wide range of tasks, but it would be helpful to provide specific examples and use cases to illustrate its versatility."
                },
                "questions": {
                    "value": "1. In the Contribution 4, \"DP-BiTFiT is a unique algorithm in that the computation overhead is independent of the feature dimension T\" where T is the sequence length. The author should be more specific for this claim."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper provides new algorithms for private machine learning with differential privacy guarantee, which could be used to protect privacy in machine learning pipeline."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699239957242,
            "cdate": 1699239957242,
            "tmdate": 1699636741680,
            "mdate": 1699636741680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQGBesawAs",
                "forum": "YH3tFtwuzb",
                "replyto": "BLbo76E3yy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comment. Here is a point-to-point response.\n\n1. Removing the forward hook is quite natural as one does not have to compute the gradient on weights. This is a natural choice when combining BiTFiT and differential privacy. \n\n**Response:** We agree it is natural to remove the forward hooks *with the benefit of hindsight*. But being natural does not conflict the significance of our choice. At the time of writing this paper, none of existing codebases including GhostClip and Opacus remove the forward hooks, because no analysis has encouraged to do so. We are the first to point this rigorously with complexity analysis and computation graph in Figure 2. We also point out that even the original non-DP BiTFiT paper does not investigate these aspects, e.g. the complexity saving is 50%, and the benefit of BiTFiT on long-context language tasks or high-resolution image tasks. Besides, we complement the missing piece of complexity analysis in the existing literature of DP parameter-efficient fine-tuning (PEFT), e.g. DP-LoRA, making the comparison among methods more solid in theory. We believe this is good practice that can benefit the community and provide this in Appendix C.\n\n2. Moreover, the empirical evaluation shows performance drop of DP-BiTFiT compared with other parameter-efficient fine-tuning techniques.\n\n**Response:** We agree that there is sometimes a minor drop of DP-BiTFiT, but overall DP-BiTFiT is comparable when applied alone. For fair comparison, we study DP-BiTFiT as a standalone method to compare with other PEFT. A promising application is actually to apply DP-BiTFiT together with LoRA or last-layer tuning, as we have shown in Section 4.3. Also, a small performance drop is acceptable in the cases when we emphasize the memory efficiency (since DP-BiTFiT is activation-free) and implementability (only need one line of code $$[param.requires\\_grad\\_(False) for name, param in model.named\\_parameters() if 'bias' in name]}$$).\n\n3. In the Contribution 4, \"DP-BiTFiT is a unique algorithm in that the computation overhead is independent of the feature dimension T\" where T is the sequence length. The author should be more specific for this claim.\n\n**Response:** We have explained the computation overhead of DP-BiTFiT in the last column of Table 2, where both the time and the space complexity is O(Bp), independent of T. This is also empirically verified in Section 3.3: we quote \"DP-BiTFiT is amazingly scalable since its *computational overhead* is negligible and independent of T (though the *total complexity*, mainly due to forward and output gradient, is still linear in T).\"\n\nWe hope the reviewer can raise the score if satisfied."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530534617,
                "cdate": 1700530534617,
                "tmdate": 1700531925401,
                "mdate": 1700531925401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EaGOmfrYPM",
            "forum": "YH3tFtwuzb",
            "replyto": "YH3tFtwuzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_oSFo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6562/Reviewer_oSFo"
            ],
            "content": {
                "summary": {
                    "value": "This work studies a differentially private version of bitfit, termed DP-Bitfit. DP-bitfit, like bitfit, works by tuning only the bias terms in the model. However, the key innovation here is recognizing that bitfit is highly parameter efficient which is crucial for DP learning. Second, is recognizing that the gradients are much computationally cheaper to calculate due to it being activation-free. Overall, this work shows impressive empirical results evaluated on both image and text models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, this work shows to be a strong submission. This work includes comprehensive comparison between different existing methods (e.g., ghost clipping) and shows impressive benefits in both memory, throughput, and final model utility. It is perhaps unsurprising that this performs so well given the performance of non-DP bitfit, however, the core benefit of this work is recognizing its potential for the DP setting.\n\nThe organization of the work is clear, and the work includes several key figures and diagrams that help the reader follow the work. For example, Figure 1 clearly shows the empirical benefits of the approach and Section 3/ Figure 2 the  asymptotic benefits.\n\nThough the novelty is lower because this is essentially applying DP-SGD to the existing \"bitfit\" algorithm, this work include smany empirical results evaluated across different model families (e.g., ViT models,resnets, and roberta models) on both text and image classification as well as text generation."
                },
                "weaknesses": {
                    "value": "This works lacks a clear empirical exploration of the difference between the activation and the model memory. Right now, it is clear that the algorithm uses much less memory and compute. However, it is unclear how the total memory in figure 1 is split between storing the model, materializing activations, etc. \n\nFigure 4 is extremely confusing. How can DP-BITFIT be both on the x-axis and in the legend? What does maximum through of algorithm mean?"
                },
                "questions": {
                    "value": "In table 5, how were the results of Bu et al. obtained? I could not find any 98.9% performance (Cifar-10 result) in their work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6562/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699253898247,
            "cdate": 1699253898247,
            "tmdate": 1699636741573,
            "mdate": 1699636741573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ujHmxUrfaY",
                "forum": "YH3tFtwuzb",
                "replyto": "EaGOmfrYPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6562/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comment and appreciating our work. Although in Figure 1 we didn't precisely split the memory between storing the model, materializing activations, etc., this information can be inferred (approximately in the middle sub-plot): given that DP compactor/LoRA/Adapter have similar (though larger) number of trainable parameters to DP-BiTFiT, we claim most of the memory difference is indeed in the activations.\n\nWe would like to clarify Figure 4. The x-label and legend are both correct. The DP-BiTFiT is serving as a reference line diagonally. Any method such as \"non-DP full\" can be comparable to DP-BiTFiT by looking at the slope (less steeper means less scalable). Each dot represent the actual throughput or memory (from left to right is large to small models). The throughput means number of examples processed per second by maximizing the batch size, as a measure of training speed. We will clarify it in the camera-ready revision.\n\nWe are grateful that the reviewer spot this typo in Table 5: the vit-large result is Ours, obtained through full fine-tuning. The horizontal line should be one row higher."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6562/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528784192,
                "cdate": 1700528784192,
                "tmdate": 1700532139037,
                "mdate": 1700532139037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]