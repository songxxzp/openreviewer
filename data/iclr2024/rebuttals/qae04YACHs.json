[
    {
        "title": "Transformer-Modulated Diffusion Models for Probabilistic Multivariate  Time Series Forecasting"
    },
    {
        "review": {
            "id": "iicEvqcmJQ",
            "forum": "qae04YACHs",
            "replyto": "qae04YACHs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a method for multivariate time series forecasting. It utilises the (conditional) diffusion model, while starts from a \"condition\" which is from the output of a Transformer based model.  The advances of the proposed model is measured with (mainly) QICE and CRPS. MSE and MAE, as well as some qualitative results are also provided to show the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Inspired by the conditional diffusion process for multivariate regression from Han et al. (2022), the manuscript proposes the conditional diffusion process for multivariate time series forecasting.\nTo the best of my understanding, the core contribution of the manuscript is to use the \"condition\" from a Transformer for the diffusion model, while both the Transformer and conditional diffusion model are from the literature.  \n\nThe manuscript is overall clearly written and easy to read. Some of the missing details are listed in Questions.\n\nThe proposed method for probabilistic multivariate time series forecasting has the potential to contribute to the community."
                },
                "weaknesses": {
                    "value": "- The manuscript does not successfully highlight its contribution. \nSince both components of TMDM are from the literature, it is necessary to highlight why this is not trivial.\nIn my humble opinion, the 3rd contribution is quite weak, as both metrics are from existing works.\n\n - There are also other works that employs a Transformer and a probabilistic model for time series modelling. \ne.g., Transformer + Probabilistic Circuit is proposed in [1] for time series forecasting, and uncertainty estimation (similar to Fig. 2 in the manuscript) is provided. \nA discussion with such works might help to stress the novelty and contribution of the manuscript.\n\n - The generation of the conditional representation $\\hat{y}_{0:M}$ is not clear to me. Some details are omitted from eq(9) to eq(10).\n\n - The references are not up-to-date, many arXiv versions are already published.\n\n---\n[1] Yu, Zhongjie, et al. \"Predictive Whittle networks for time series.\" Uncertainty in Artificial Intelligence. PMLR, 2022."
                },
                "questions": {
                    "value": "- can you provide more details on generating  $\\hat{y}_{0:M}$? What is the dimension of $z$? As in eq(10), is $z$ a scalar? How are the NNs for $\\tilde{\\mu}_z$, $\\tilde{\\sigma}_z$ and $\\mu_z$ formulated?\n\n- How $\\mu_z(z)$ differs from $\\mathcal{T}(x_{0:N})$? What happens if the NNs are omitted and $\\mathcal{N}(\\mathcal{T}(x_{0:N}), \\mathbf{I})$ is used instead of $\\mathcal{N}(\\mu_z(z), \\mathbf{I})$?\n\n - $f(x_{0:N})$ is in $\\mathbb{R}^C$ in introduction but $x_t$ and $y_t$ are in  $\\mathbb{R}^d$. Does $C=d$ hold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K",
                        "ICLR.cc/2024/Conference/Submission2407/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697923390143,
            "cdate": 1697923390143,
            "tmdate": 1700684305560,
            "mdate": 1700684305560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vbdCvhcOBc",
                "forum": "qae04YACHs",
                "replyto": "iicEvqcmJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:**\n\nThank you for your insightful comment and suggestions. We intend to revise our paper to emphasize the following contributions:\nFirstly, as you mentioned, both components of TMDM originate from existing literature. However, amalgamating these techniques into a unified framework presents several technical challenges, such as effectively organizing the information captured by transformers and optimizing the entire framework using a single ELBO.\nAdditionally, we bifurcate the probabilistic multivariate time series forecasting tasks into two parts: utilizing a well-designed Transformer to capture the mean of the MTS and considering covariate-dependence across both the forward and reverse processes within the diffusion model. We believe this amalgamation of the advantages from both models holds significant importance.\nMoreover, TMDM functions as a plug-and-play framework, seamlessly compatible with existing well-designed transformer-based forecasting models. This aspect represents another significant contribution of TMDM, bridging the gap between point estimates and distribution estimates. We believe this will reshape the community's perception regarding the relationship between point estimates and distribution estimates, as well as address practical application needs.\n\nDespite PICP and QICE metrics originating from existing methodologies, they were initially introduced by us in the context of probabilistic multivariate time series forecasting tasks. As illustrated in Table 2 of our paper, D3VAE and TimeDiff were initially crafted for point-to-point forecasting tasks, with less emphasis on probabilistic forecasting. Consequently, their performance in distribution estimation (depicted in Figure 2) is inadequate, yet they exhibit competitive CRPS and $CRPS_{SUM}$ scores in Table 2. This underscores the insufficiency of existing metrics to handle the advancements in distribution estimation. Following these experiments, we conclude that the incorporation of QICE and PICP holds significant value. Hence, we advocate for future research to adopt these new metrics as standard evaluation criteria. Thank you once again for your valuable suggestion. \nWe have revised and enhance our contribution based on your suggestions:\n\nWe summarize our contributions as follows:\n(1) In the realm of probabilistic multivariate time series forecasting, we introduce TMDM, a transformer-based diffusion generative framework. TMDM harnesses the representations captured by well-designed transformer-based time series models as priors. We consider the covariate-dependence across both the forward and reverse processes within the diffusion model, resulting in a highly accurate distribution estimation for future time series.\n(2)TMDM integrates diffusion and transformer-based models within a cohesive Bayesian framework, employing a hybrid optimization strategy, it serves as a plug-and-play framework, seamlessly compatible with existing well-designed transformer-based forecasting models, leveraging their strong capability to estimate the conditional mean of time series, facilitating the estimation of complete distributions.\n(3)In our experimental evaluation, we introduce two novel metrics for the probabilistic multivariate time series forecasting task: Prediction Interval Coverage Probability (PICP)and Quantile Interval Coverage Error (QICE). These metrics enhance the assessment of the uncertainty estimation prowess of probabilistic multivariate time series forecasting models. Evidenced by the state-of-the-art performance in four distribution metrics across six real-world datasets, TMDM shows effective potency in probabilistic MTS forecasting."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281568596,
                "cdate": 1700281568596,
                "tmdate": 1700281568596,
                "mdate": 1700281568596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9uem4CdFkw",
                "forum": "qae04YACHs",
                "replyto": "iicEvqcmJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3:**\n\nWe genuinely appreciate the time and effort you dedicated to a thorough reading of our paper. Your detailed review has been invaluable, and we apologize for any shortcomings in our references. Rest assured, we have diligently reevaluated all references to ensure they are brought up to date and meet the highest standards of academic rigor and accuracy. Your feedback is highly valuable, and we are committed to enhancing the quality and reliability of our work based on your suggestions. Thank you once again for your contribution to our research.\n\nThe updated references are listed as follows, and we'd appreciate your further feedback if you find additional refinances worth mentioning. \n\n[1] Juan Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. Transactions on Machine Learning Research, 2022.\n        \n[2] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT, pp. 4171\u2013 4186, 2019.\n        \n[3] Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs M Bergmann, and Roland Vollgraf. Mul- tivariate probabilistic time series forecasting via conditioned normalizing flows. International Conference on Learning Representations, 2020.\n        \n[4] Lifeng Shen and James Kwok. Non-autoregressive conditional diffusion models for time series prediction. International Conference on Machine Learning, pp. 6657\u20136668, 2023.\n        \n[5] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations, 2020.\n        \n[6] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. International Conference on Learning Representations, 2021.\n        \n[7] Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quan- tification for bayesian neural network inference. CoRR, abs/1906.09686, 2019. URL http: //arxiv.org/abs/1906.09686.\n        \n[8] Huangjie Zheng. Truncated diffusion probabilistic models and diffusion-based adversarial auto- encoders. Advances in Neural Information Processing Systems, 36, 2022.\n\n**Q4:**\n\nThank you for your comment and suggestion.\nDue to the full text page limit, details are omitted from eq(9) to eq(10), we have provided in the Appendix will  provide a more detailed explanation of this section to enhance clarity.\nIn the context of a Bayesian generative model, we approach this matter from a generative standpoint. Initially, we create a sample $z$ drawn from the standard normal distribution $\\mathcal{N}(0, \\boldsymbol{I})$. Subsequently, we employ an MLP represented as $\\mu_z$ to derive the distribution parameters of $P(y{0:M})$. This methodology enables the sampling of $y_{0:M}$ from the distribution $P(y_{0:M})$. \nIt's important to note that the dimension of $z$ is $\\mathbb{R}^{N \\times M}$, with $N$ set as 512 across all datasets. \n\nAlternatively, considering this segment as a hierarchical VAE, the neural networks representing $\\mu_z$ and $\\sigma_{z}$ function as parts of the encoder, while $\\mu_z$ serves as part of the decoder. In our paper, these three neural networks are implemented using MLPs with a dimension of 512."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282195712,
                "cdate": 1700282195712,
                "tmdate": 1700289144264,
                "mdate": 1700289144264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q9waidudgM",
                "forum": "qae04YACHs",
                "replyto": "VnjIPS81WN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the clarification. In the current version I still see $\\mathbb{R}^C$. Could you make sure it is updated?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510272964,
                "cdate": 1700510272964,
                "tmdate": 1700510272964,
                "mdate": 1700510272964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sl2FJggLq4",
                "forum": "qae04YACHs",
                "replyto": "y9XFPkE4Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for updating the introduction.\\\n - When looking back, I have a follow-up question to your answer marked as Q2. I don't think the comparison in the table is fair enough, since TMDM employs NSformer while PWN uses Transformer. Therefore a fair comparison would be either\n1) Transformer-PWN with Transformer-TMDM with Transformer, or \n2) NSformer-PWN with NSformer-TMDM with Transformer,\n\n    is it? In Table 5, in terms of MSE and MAE, TMDM with NSformer has smallest improvement compared to NSformer.\n    Anyway, I think a general discussion might still be helpful in the revision.\n\n - Typo in the answer of Q3, reference [3] was published in 2021. \n\n - I still think the claim of \"**we introduce** two novel metrics for ...\" is too strong."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606322660,
                "cdate": 1700606322660,
                "tmdate": 1700606322660,
                "mdate": 1700606322660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JulWj7RhWO",
                "forum": "qae04YACHs",
                "replyto": "iicEvqcmJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "|                   |                    |                    |                    |                    |                    |                    |\n|:-----------------:|:------------------:|:------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n|      Dataset      |      Exchange      |        ILI         |       ETTm2        |    Electricity     |      Traffic       |       Wether       |\n|      Metric       |      MSE  MAE      |      MSE  MAE      |      MSE  MAE      |      MSE  MAE      |      MSE  MAE      |      MSE  MAE      |\n|     Informer      |     1.31  0.85     |     5.33  1.59     |     5.74  1.99     |     0.35  0.43     |     0.75  0.42     |     0.48  0.47     |\n|        PWN        |     1.11  0.81     |     4.73  1.29     |     5.45  2.02     |     0.28  0.41     |     0.67  0.38     |     0.40  0.43     |\n| TMDM+Transformer  |     1.16  0.82     |     4.48  1.41     |     1.18  0.86     |     0.27  0.33     |     0.62  0.33     |     0.55  0.49     |\n| TMDM+STransformer |     1.10  0.83     |     4.35  1.22     |     5.21  1.96     |     0.26  0.32     |     0.62  0.36     |     0.38  0.39     |\n|     TMDM+PWN      |     1.03  0.77     |     4.15  1.20     |     4.33  1.62     |     0.25  0.32     |     0.61  0.36     |     0.37  0.35     |\n|   TMDM+NSformer   | **0.26**  **0.37** | **1.99**  **0.85** | **0.27**  **0.35** | **0.19**  **0.27** | **0.60**  **0.35** | **0.28**  **0.25** |\n|                   |                    |                    |                    |                    |                    |                    |\n\nFor a fair comparison, we utilized the STransformer in a setting similar to PWN as the basic Transformer. When comparing TMDM+STransformer with PWN, TMDM shows improvements across most datasets. Furthermore, using PWN as the basic Transformer, TMDM+PWN yielded the best results among the three experiments (PWN, TMDM+STransformer, and TMDM+PWN)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661881028,
                "cdate": 1700661881028,
                "tmdate": 1700662801267,
                "mdate": 1700662801267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EKQprJgfBG",
                "forum": "qae04YACHs",
                "replyto": "JulWj7RhWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_vL8K"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the answer and the updated results. Most of my questions are addressed and I have increased my rating from 6 to 8."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684314412,
                "cdate": 1700684314412,
                "tmdate": 1700684314412,
                "mdate": 1700684314412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hLrcsBCbSR",
            "forum": "qae04YACHs",
            "replyto": "qae04YACHs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_SgLT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_SgLT"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of probabilistic time series\nforecasting. The authors propose a conditional diffusion\nprocess that convex combines the point estimate from\na transformer model with the noise of the diffusion process.\nIn experiments on 6 datasets they show that their approach\noutperforms state-of-the-art baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "s1. generic approach that will work with any point estimate model.\ns2. consistently good results against several strong baselines.\ns3. ablation studies demonstrate the impact of different components\n  as well as the ability to wrap different point models."
                },
                "weaknesses": {
                    "value": "w1. the results for CRPS_sum of the baselines (appendix, tab. 6)\n  varies from the published results."
                },
                "questions": {
                    "value": "The paper proposes a generic approach to wrap a diffusion\nmodel around any point estimate model for time series\nforecasting to make it probabilistic (s1). Most of the results\nshown in tab. 2 are pronounced improvements and almost\nall but the very last are consistently better than several\nstrong baselines (s2). Due to the ablation studies one can\nclearly see the impact of different modelling choices (s3).\n\nI only would like to discuss one point:\nw1. the results for CRPS_sum of the baselines (appendix, tab. 6)\n  varies from the published results.\n- e.g., tab. 6 reports CRPS_sum 3.92 on Exchange for TimeGrad,\n  but the TimeGrad paper reports 0.006.\n  your tab. 6 reports CRPS_sum 4.54 on Electricity for CSDI,\n  but the CSDI paper reports 0.017.\n- I think these differences need to be clearly explained, likely\n  due to different experimental conditions?\n  If so, it would be convincing to reproduce the experiments\n  of the strongest baseline papers and compare them on\n  the published settings, too."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424553579,
            "cdate": 1698424553579,
            "tmdate": 1699636175870,
            "mdate": 1699636175870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IJq6rnAwZP",
                "forum": "qae04YACHs",
                "replyto": "hLrcsBCbSR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:**\n\nThank you for your comment and suggestion. The variation in the $CRPS_{sum}$ results of the baseline models compared to the published results is primarily due to differences in the experimental settings, specifically concerning the history length and prediction length.\nWe have included the experiments on the settings in TimeGrand and CSDI in our paper, and the results can also be found as follows:\n\n| TimeGrand Setting |                |                |                |\n|:-----------------:|:--------------:|:--------------:|:--------------:|\n|      Dataset      |    Exchange    |  Electricity   |    Traffic     |\n|      Metric       | QICE  CRPS-sum | QICE  CRPS-sum | QICE  CRPS-sum |\n|     TimeGrand     |  3.63  0.006   |  2.51  0.0206  |  1.91  0.044   |\n|       TMDM        |  2.48  0.004   |  1.31  0.016   |  1.07  0.013   |    \n|                   |                |                |                |\n\n| CSDI Setting |                |                |                |\n|:------------:|:--------------:|:--------------:|:--------------:|\n|   Dataset    |    Exchange    |  Electricity   |    Traffic     |\n|    Metric    | QICE  CRPS-sum | QICE  CRPS-sum | QICE  CRPS-sum |\n|  TimeGrand   |      ---       |  2.59  0.021   |  2.02  0.044   | \n|     CSDI     |      ---       |  2.28  0.017   |  1.60  0.020   |  \n|     TMDM     |      ---       |  1.36  0.014   |  1.23  0.015   |    \n|              |                |                |                |\n\nCSDI occasionally matches or slightly surpasses TimeGrand, yet TMDM consistently outperforms both models across various datasets and metrics. TMDM showcases superior performance in probabilistic forecasting, reflected in its lower QICE and CRPS-sum values. These results emphasize TMDM's significant advancements in predictive accuracy and distributional modeling compared to TimeGrand and CSDI across diverse datasets and evaluation metrics."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281398770,
                "cdate": 1700281398770,
                "tmdate": 1700281398770,
                "mdate": 1700281398770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "InFgjxhoOH",
            "forum": "qae04YACHs",
            "replyto": "qae04YACHs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_YJEP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2407/Reviewer_YJEP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. Extensive experiments are conducted"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-presented and well-organized.\n2. The paper introduces a Transformer-Modulated Diffusion Model (TMDM), uniting conditional diffusion generative process with transformers into a unified framework to enable precise distribution forecasting for MTS. \n3. Extensive experiments are conducted"
                },
                "weaknesses": {
                    "value": "1. This paper states many existing work did not consider the uncertainty of data, but more SOTA should be compared and considers like cST-ML which tries to capture traffic dynamics with VAE. Please provide detailed explanations or experiments accordingly.\n2. If the time-series data is in different granularities, does this model still work?"
                },
                "questions": {
                    "value": "Please address the questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698538000712,
            "cdate": 1698538000712,
            "tmdate": 1699636175799,
            "mdate": 1699636175799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ldlJIZHIWg",
                "forum": "qae04YACHs",
                "replyto": "InFgjxhoOH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1:**\n\nThank you for your valuable comments and suggestions. In light of your feedback, our paper has been revised to encompass more state-of-the-art baselines, such as cST-ML [1] and DAC-ML [2], both of which utilize VAE for dynamic capture. These new inclusions serve to deepen our analysis and broaden the scope of our research. The experimental results, now integrated into the updated version, are presented as follows:\n|         |                    |                    |                    |                    |                    |                    |\n|:-------:|:------------------:|:------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| Dataset |      Exchange      |        ILI         |       ETTm2        |    Electricity     |      Traffic       |       Wether       |\n| Metric  |      QICE  CRPS    |     QICE  CRPS     |     QICE  CRPS     |     QICE  CRPS     |     QICE  CRPS     |     QICE  CRPS     |\n|   VAE   |     8.28  1.02     |     9.13  2.41     |     8.99  0.79     |     7.04 0.51      |     5.37 0.67      |     9.07 0.47      |\n| cST-ML  |     7.94  0.94     |     9.02  1.94     |     7.29  0.64     |     5.99  0.47     |     5.24  0.60     |     8.29  0.51     |\n| DAC-ML  |     7.36  0.85     |     8.71  1.23     |     6.60  0.59     |     5.76  0.43     |     4.31  0.50     |     7.91  0.46     |\n|  TMDM   | **4.38**  **0.32** | **6.74**  **0.92** | **3.75**  **0.37** | **3.81**  **0.33** | **2.36**  **0.26** | **3.87**  **0.36** |\n|         |                    |                    |                    |                    |                    |                    |\n\nThe results in the above Table show that the intricately structured CNN-RNN and Bayesian framework in cST-ML [1] led to an 8\\% average reduction in QICE compared to the VAE model. DAC-ML [2], which builds upon cST-ML by incorporating model adaptation capabilities, achieved a 15\\% reduction in QICE relative to the VAE-based approach. However, it's noteworthy that the proposed TMDM still surpasses both cST-ML [1] and DAC-ML [2] in performance, despite their enhancements over the standard VAE. The reasons for this significant outperformance by TMDM are detailed as follows:\n\n1. Transformer-based architectures have shown remarkable success across a wide range of tasks. However, models like DAC-ML and cST-ML, which are based on CNN-RNN structures, face challenges in capturing long-term correlations. In the field of time series forecasting, specialized transformers are developed to address the nuances of multivariate time series. These specialized transformers often outperform generic transformer models. TMDM leverages these specifically tailored transformers, thereby gaining a significant edge over RNN-based methodologies.\n\n2. The diffusion model, recognized as a state-of-the-art deep generative model, has been extensively validated for its enhanced generative capabilities over basic VAEs in numerous studies. TMDM integrates a conditional diffusion generative process, which facilitates accurate distribution forecasting in multivariate time series. This attribute equips TMDM with a substantial advantage in performance over other VAE-based models like cST-ML.\n\n3. TMDM effectively utilizes transformers to distill crucial insights from historical time series data. These insights serve as prior knowledge, enabling the model to capture covariate-dependence effectively during both the forward and reverse phases of the diffusion process. This synergistic integration of transformers and diffusion models positions TMDM at the forefront in probabilistic multivariate time series forecasting.\n\n4. TMDM's design transcends a mere combination of two modules. Instead, it integrates these models within a cohesive Bayesian framework, employing a hybrid optimization strategy. As detailed in Equation 15 of our paper, the first term guides the denoising model to predict uncertainty while subtly adjusting the conditional generative model to provide a more appropriate conditional representation. The second term facilitates the generation of improved conditional representations by harnessing the capabilities of a well-designed transformer. The two parts of the model adapt to each other within a hybrid framework, further enhancing the model's performance.\n\n[1] Zhang, Y., Li, Y., Zhou, X., \\& Luo, J. (2020, November). cST-ML: Continuous spatial-temporal meta-learning for traffic dynamics prediction. In 2020 IEEE International Conference on Data Mining (ICDM), pp. 1418-1423. \n\t\n[2] Zhang, X., Li, Y., Zhou, X., Mangoubi, O., Zhang, Z., Filardi, V., \\& Luo, J. (2021, December). DAC-ML: domain adaptable continuous meta-learning for urban dynamics prediction. In 2021 IEEE International Conference on Data Mining (ICDM), pp. 906-915."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280935428,
                "cdate": 1700280935428,
                "tmdate": 1700288863624,
                "mdate": 1700288863624,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "802awjgBGF",
                "forum": "qae04YACHs",
                "replyto": "InFgjxhoOH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q2:**\n\nThank you for your insightful question. Evaluating model performance with time-series data at different granularities is of significant importance in real-world applications. We believe TMDM can excel in such scenarios for the following reasons:\n \n1. Similar to most time series forecasting models [4,5,6], we incorporate the actual timestamps as learnable time embeddings for each data point. Leveraging a well-designed Transformer, we can effectively capture the temporal correlations within the data. This design ensures that TMDM can adapt to various granularities of time-series data.\n \n2. Within the diffusion model component, we also account for the time embedding in the data. This allows the model to generate multivariate time series with information from these embeddings, accommodating time series data at different granularities.\n \n3. As shown in Table 1 in our paper, the selected datasets covered different granularities ranging from 10 minutes to 1 day, and TMDM demonstrated competitive performance across all databases. This confirms the model's ability to handle situations where time-series data is available at different granularities.\n \n4. To further evaluate TMDM's ability to handle varying time-series granularities, we conducted an experiment where we randomly removed $D$ data points from a given time series $y_{0:M}$ and used this modified dataset to test TMDM with the same settings as described in the paper. This challenging experiment simulates scenarios where the time intervals in multivariate time series vary, making it a rigorous test of the model's performance under changing granularities.\n\n|         |                |            |             |            |            |\n|:-------:|:--------------:|:----------:|:-----------:|:----------:|:----------:|\n| Dataset |    Exchange    |   ETTm2    | Electricity |  Traffic   |   Wether   |\n| Metric  |   QICE  CRPS   | QICE  CRPS | QICE  CRPS  | QICE  CRPS | QICE  CRPS |    \n| TMDM-60 |   4.78  0.32   | 3.72  0.39 | 3.73  0.31  | 2.36  0.28 | 3.91  0.37 | \n| TMDM-30 |   4.14  0.30   | 3.93  0.38 | 3.85  0.35  | 2.34  0.25 | 3.79  0.34 | \n|  TMDM   |   4.38  0.32   | 3.75  0.37 | 3.81  0.33  | 2.36  0.26 | 3.87  0.36 |\n|         |                |            |             |            |            |\n\nIn the presented table, TMDM-60 refers to our model with a prediction length of 192 + 60, where we randomly exclude 60 samples from the data, introducing random time intervals between consecutive points. In this scenario, TMDM is tasked with forecasting multivariate time series (MTS) with varying granularities based on temporal embeddings. Remarkably, the results from the 30 and 60 settings display similar scores to the original setting, showcasing the efficacy of TMDM in handling time-series data with diverse granularities.\n\n[4] Liu, Y., Wu, H., Wang, J., \\& Long, M. (2022). Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35, pp. 9881-9893.\n\t\n[5] Wu, H., Xu, J., Wang, J., \\& Long, M. (2021). Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34, pp. 22419-22430.\n\t\n[6] Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., \\& Zhang, W. (2021, May). Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, Vol. 35, No. 12, pp. 11106-11115."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281082749,
                "cdate": 1700281082749,
                "tmdate": 1700288976051,
                "mdate": 1700288976051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JC6sco5PKy",
                "forum": "qae04YACHs",
                "replyto": "802awjgBGF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_YJEP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2407/Reviewer_YJEP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. My concerns have been addressed."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664983543,
                "cdate": 1700664983543,
                "tmdate": 1700664983543,
                "mdate": 1700664983543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]