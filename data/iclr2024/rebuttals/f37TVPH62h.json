[
    {
        "title": "Compound Returns Reduce Variance in Reinforcement Learning"
    },
    {
        "review": {
            "id": "1wrYsfBIg8",
            "forum": "f37TVPH62h",
            "replyto": "f37TVPH62h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_fKNs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_fKNs"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the variance reduction property of compound returns. While compound returns, such as the lambda-return, are often viewed as helping with variance reduction via averaging, the authors claim that this variance properties is formally investigate for the first time. Under certain assumptions on the variance/covariance model, the authors prove for the first time that any compound return with the same contraction rate as a given n-step return has strictly lower variance. The studies shed light on the theoretical understanding of using compound returns in learning value functions. Subsequently, the authors propose a computationally friendly piecewise lambda-return and verify the efficacy of their approach on one Atari Freeway environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper considers an interesting question. While it may be commonly believed that averaging helps with variance and hence learning in RL, the authors formally study the problem and show that compound returns admit a better bias-variance trade-off. The writing is overall very clear and organized. The proposed piecewise lambda-return is theoretically sound and seems to also perform in the limited experimental evaluations."
                },
                "weaknesses": {
                    "value": "While a formal study on the variance reduction property is valuable, the theoretical contributions of this paper seem limited. The assumptions help abstract a lot of the difficulties and with the uniform variance/correlation assumptions, the derivation in this paper seems to be straightforward/follow standard arguments. As such, the technical depth is limited. Consequently, for such paper with limited theoretical innovations, one might expect a more comprehensive experimental evaluations, ablation studies and comparisons. The current manuscript unfortunately only evaluates on the Atari Freeway environment."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697580444170,
            "cdate": 1697580444170,
            "tmdate": 1699636134967,
            "mdate": 1699636134967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GnAojm4sAw",
                "forum": "f37TVPH62h",
                "replyto": "1wrYsfBIg8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer fkNs"
                    },
                    "comment": {
                        "value": "Thank you for the review. We respond to each of your concerns below:\n\n> While a formal study on the variance reduction property is valuable, the theoretical contributions of this paper seem limited. The assumptions help abstract a lot of the difficulties and with the uniform variance/correlation assumptions, the derivation in this paper seems to be straightforward/follow standard arguments. As such, the technical depth is limited.\n\nOur work is the first to make progress on this question in the last 12 years. Please see \u201c**Strength of Theoretical Contributions**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=TJYI9LMG6P) for more details about the novelty of our theory.\n\n> One might expect a more comprehensive experimental evaluations, ablation studies and comparisons. The current manuscript unfortunately only evaluates on the Atari Freeway environment.\n\nThank you for this suggestion. We have added results for three more MinAtar games to the paper; please see \u201c**Additional Experiments**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125249288,
                "cdate": 1700125249288,
                "tmdate": 1700125249288,
                "mdate": 1700125249288,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TsTBD36SCD",
            "forum": "f37TVPH62h",
            "replyto": "f37TVPH62h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the widely used $\\lambda$-compounded returns and show that they have lower variance than $n$-steps return if the temporal difference errors have equal correlation strictly less than one.\n\nIn addition they propose PILAR which is a practical deep RL approximation of the TD($\\lambda$) compatible with experience replay."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper discovers few new characteristics of a very common method in RL that is TD($\\lambda$)."
                },
                "weaknesses": {
                    "value": "1) I think that most of the results are slightly incremental and not clearly novel.\n\n2) Assuming that all temporal differences error have the same correlation is a strong assumption in my opinion.\n\n3) In general in RL it is not clear if minimizing the variance of the return estimators is helpful to improve the sample complexity of an algorithm. Check for example this paper investigating the role of the minimum variance baseline in policy gradient as in [1].\n\n4) The experiments in Deep RL are limited to only one environment. I think that a larger empirical evaluation is necessary.\n\n[1] Wesley Chung, Valentin Thomas, Marlos C Machado, and Nicolas Le Roux.\nBeyond variance reduction: Understanding the true impact of baselines on policy optimization"
                },
                "questions": {
                    "value": "Is it possible to use the results in this paper to show more informative results regarding the performance of TD($\\lambda$). For example that having a lower variance in the returns improves the sample complexity needed for either policy evaluation or for learning an $\\epsilon$-optimal policy ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698315588108,
            "cdate": 1698315588108,
            "tmdate": 1699636134876,
            "mdate": 1699636134876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yVnJENJz1F",
                "forum": "f37TVPH62h",
                "replyto": "TsTBD36SCD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer Ucxg"
                    },
                    "comment": {
                        "value": "Thank you for the review. We would like to note that our theoretical analysis extends far beyond just TD($\\lambda$) and $\\lambda$-returns. Our analysis encompasses all arbitrary weighted averages of $n$-step returns, which is a vast family of return estimators used by all value-based RL methods. TD($\\lambda$) and $\\lambda$-returns are just one interesting example because of their widespread use in RL.\n\nWe also want to emphasize that, until now, no one has shown that TD($\\lambda$) reduces variance compared to $n$-step TD. It was previously believed [1, ch. 12.1] that the main benefit of $\\lambda$-returns was their convenient implementation using eligibility traces, and were otherwise equivalent to $n$-step returns.\n\nWe respond to each of your concerns below:\n\n> I think that most of the results are slightly incremental and not clearly novel. Assuming that all temporal difference errors have the same correlation is a strong assumption in my opinion.\n\nOur work is the first to make progress on this question in the last 12 years. Please see \u201c**Strength of Theoretical Contributions**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=TJYI9LMG6P) for more details about the novelty of our theory.\n\n> In general in RL it is not clear if minimizing the variance of the return estimators is helpful to improve the sample complexity of an algorithm. Check for example this paper investigating the role of the minimum variance baseline in policy gradient as in [1].\n\nThis is an excellent point. It is true that reducing the return variance is not guaranteed to improve the sample efficiency of *policy gradient* methods. The paper that you cited shows that the variance of the *baseline* can sometimes be beneficial because of its interaction with the exploration policy (what the authors call committal and non-committal baselines). Our value-based setting is fundamentally different because it does not rely on a critic to determine which actions should be reinforced. Instead, the policy is directly determined as a function of the value function (e.g., $\\epsilon$-greedy) and so the main bottleneck for learning is how quickly the returns can be estimated accurately.\n\nFurthermore, it is widely accepted that variance reduction is beneficial for *learning value functions*. This is why TD-based returns (e.g., n-step returns [2] or lambda-returns [3]) are almost always used in deep RL instead of Monte Carlo returns, as the latter have extremely high variance. To isolate the effects of return estimation itself, we chose to focus on DQN, a well-studied deep RL method that only learns a value function. The main benefit of our method, PiLaR, is to apply similar variance reduction in off-policy experience-replay settings where computing the full $\\lambda$-return is infeasible. \n\n> The experiments in Deep RL are limited to only one environment. I think that a larger empirical evaluation is necessary.\n\nThank you for this suggestion. We have added results for more MinAtar games to the paper; please see \u201c**Additional Experiments**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD).\n\n> Is it possible to use the results in this paper to show more informative results regarding the performance of TD($\\lambda$). For example that having a lower variance in the returns improves the sample complexity needed for either policy evaluation or for learning an $\\epsilon$-optimal policy?\n\nThis is a great question. Please see \u201c**Why Variance Reduction Leads to Faster Learning**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD) for a detailed discussion of how the contraction rate corresponds to expected policy improvement in Q-Learning methods. We have also added this discussion to our paper\u2019s appendix.\n\n\n**References**\n\n[1] Reinforcement Learning: An Introduction. Sutton and Barto, 2018.\n\n[2] Rainbow: Combining Improvements in Deep Reinforcement Learning. Matteo Hessel et al., AAAI 2018.\n\n[3] High-Dimensional Continuous Control Using Generalized Advantage Estimation. John Schulman et al., ICLR 2016."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124975119,
                "cdate": 1700124975119,
                "tmdate": 1700124975119,
                "mdate": 1700124975119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uQnS7in1XW",
                "forum": "f37TVPH62h",
                "replyto": "yVnJENJz1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answer and for adding new experiments.\n\nUnfortunately, I feel that the discussion regarding \u201cWhy Variance Reduction Leads to Faster Learning\u201d should be formalised better to push the paper at the level of acceptance.\n\nFor example, could you combine the novel variance bound with the techniques in [1] and compare the result you would obtained with the bounds given in [1, Theorem 4] ?\n\nBest,\n\nReviewer Ucxg\n\n\n[1]   A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation    https://arxiv.org/pdf/1806.02450.pdf"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213599891,
                "cdate": 1700213599891,
                "tmdate": 1700213599891,
                "mdate": 1700213599891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g0VZSEgotd",
                "forum": "f37TVPH62h",
                "replyto": "yxJVhJq5cT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2033/Reviewer_Ucxg"
                ],
                "content": {
                    "comment": {
                        "value": "dear authors,\n\nthanks a lot for implementing this change! i think they go in the right direction.\n\nI am not be able to check the proof before the end of the reviewer authors discuss so I am keeping my original score for th time being. However I will check the proof in the AC reviewer discuss phase.\n\nbest,\n\nreviewer"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695439609,
                "cdate": 1700695439609,
                "tmdate": 1700695439609,
                "mdate": 1700695439609,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lXRfzjOQAP",
            "forum": "f37TVPH62h",
            "replyto": "f37TVPH62h",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_U4yb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2033/Reviewer_U4yb"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the variance of compound returns in reinforcement learning both theoretically and empirically. Under the uniform covariance assumption between TD error at different steps, it proves that any compound return has lower variance than corresponding $n$-step return with the same contraction rate as long as the TD errors are not perfectly correlated. The contraction rate measures that convergence speed of a $n$-step TD estimator for value function estimation of a policy. Therefore, it concludes that compound return in general has lower variance under the same convergence rate. They also conduct experiments to verify this effect in value estimation tasks. Empirically, the paper proposes an approximation of $\\lambda$-return using only the mixture of two multi-step returns named Piecewise $\\lambda$-Return (PiLaR). Experiments with DQN on a tabular example shows the effectiveness of PiLaR on top of the standard $n$-step TD learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The variance of compound returns is a fundamental problem in reinforcement learning. This paper provides new insights on this problem by verifying the compound returns have lower variance than $n$-step returns under uniform covariance assumption. The paper clearly lists convincing theoretical and empirical evidence to support this claim."
                },
                "weaknesses": {
                    "value": "1. It is unclear whether the uniform covariance assumption is reasonable in real-world problems, since the hardness to approximate the covariance between different steps should not be an evidence to support the validity of this assumption. Intuitively, the variance of TD errors at further steps should be larger since the entropy of state should increase along the diffusion over the MDP. Therefore, it is appreciated to verify this assumption empirically on synthetic examples.\n\n2. The contraction rate measures the contraction level of the policy evaluation process. It is not clear the effect of this rate in the policy optimization process, nor is it discussed in the paper. Therefore, it is still not clear whether the faster learning with DQN is a consequence of smaller variance of PiLaR or smaller contraction rate in the policy optimization process as $n_2$ is generally larger than $n$. \n\n3. The theoretical results of the paper are mostly conceptual in the sense that it proves some variance reduction results but do not discuss how this lower variance accelerate the learning of optimal policies. The \"equally fast\" claim for two estimators with the same contraction rate is also conceptual without solid evidence. Does it correspond to smaller sample complexity in theory? The insight of this paper is also limited in both practice and theory, since the baseline is the $n$-step TD learning and DQN, which is away from current SOTA algorithms used in RL. Is is possible to compare the PiLaR (or more refined compound error with even smaller variances) with some SOTA RL algorithms such PPO or CQL?"
                },
                "questions": {
                    "value": "See above.\n\nEqn. (8): the second $S_t$ --> $s$ \n\nEqn. (12): missing $\\kappa$ in the RHS"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822657895,
            "cdate": 1698822657895,
            "tmdate": 1699636134809,
            "mdate": 1699636134809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kDaFRllaDh",
                "forum": "f37TVPH62h",
                "replyto": "lXRfzjOQAP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer U4yb"
                    },
                    "comment": {
                        "value": "Thank you for the review and for pointing out the two typos in our equations. We have fixed them in the paper. We also respond to each of your concerns below:\n\n> It is unclear whether the uniform covariance assumption is reasonable in real-world problems, since the hardness to approximate the covariance between different steps should not be an evidence to support the validity of this assumption. Intuitively, the variance of TD errors at further steps should be larger since the entropy of state should increase along the diffusion over the MDP. Therefore, it is appreciated to verify this assumption empirically on synthetic examples.\n\nOur work is the first to make progress on this question in the last 12 years. Please see \u201c**Strength of Theoretical Contributions**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=TJYI9LMG6P) for a detailed discussion.\n\nWe think it is a great idea to measure how well our assumptions hold in practice. We added two experiments to the paper (Appendix B) that plots the actual variance of the n-step returns compared to the lower and upper bounds predicted by our model.\n\n> The contraction rate measures the contraction level of the policy evaluation process. It is not clear the effect of this rate in the policy optimization process, nor is it discussed in the paper. Therefore, it is still not clear whether the faster learning with DQN is a consequence of smaller variance of PiLaR or smaller contraction rate in the policy optimization process as $n2$ is generally larger than $n$.\n\nThank you for bringing up this point. Please see \u201c**Why Variance Reduction Leads to Faster Learning**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD) for a detailed discussion of how the contraction rate corresponds to expected policy improvement in Q-Learning methods.\n\nFurthermore, it can be seen from the MinAtar experiments (Figures 5 and 6) that a larger value of $n$ actually hurts performance due to the increased variance, so it is not true that a larger $n_2$ value for PiLaR is responsible for the faster learning. Since each pair of returns are chosen to have the same contraction rate, the only remaining explanation is the reduced variance, which is supported both by our theory and random walk experiments in Figure 1.\n\n> The theoretical results of the paper are mostly conceptual in the sense that it proves some variance reduction results but do not discuss how this lower variance accelerates the learning of optimal policies. The \"equally fast\" claim for two estimators with the same contraction rate is also conceptual without solid evidence. Does it correspond to smaller sample complexity in theory?\n\nThis is a great question. We would first like to point out that our random walk experiment in Figure 1 does provide solid evidence that two returns with the same contraction rate learn equally fast in expectation (small step size) but the one with lower variance learns faster with limited samples (large step size). Again, please see \u201c**Why Variance Reduction Leads to Faster Learning**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD) for answers to your questions regarding contraction rate and sample complexity.\n\n> The insight of this paper is also limited in both practice and theory, since the baseline is the n-step TD learning and DQN, which is away from current SOTA algorithms used in RL. Is it possible to compare the PiLaR (or more refined compound error with even smaller variances) with some SOTA RL algorithms such PPO or CQL?\n\nThe principal contribution of our paper is the theory. The purpose of our experiments is to test how well our theory is reflected in practice when the assumptions required for the math do not always hold, as well as to demonstrate the feasibility of using variance-reducing compound returns in deep RL. We further discuss this in \u201c**Additional Experiments**\u201d in our [general response](https://openreview.net/forum?id=f37TVPH62h&noteId=GK2F7B8JpD), where we provide results for more MinAtar games.\n\nStudying actor-critic methods such as PPO or CQL would add a confounding factor since they simultaneously train a stochastic policy (as you correctly noted above). To isolate the effects of return estimation itself, we chose to focus on DQN, a well-studied deep RL method that only learns a value function.\n\nWe also note that PPO is an on-policy method and commonly uses the $\\lambda$-return with GAE [1] rather than the high-variance Monte Carlo return, which provides further evidence that variance reduction is beneficial in practice. Thus, the main benefit of PiLaR is to apply similar variance reduction in off-policy experience-replay settings where computing the full $\\lambda$-return is infeasible.\n\n**References**\n\n[1] High-dimensional Continuous Control Using Generalized Advantage Estimation. John Schulman et al., ICLR 2016."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124609792,
                "cdate": 1700124609792,
                "tmdate": 1700125910054,
                "mdate": 1700125910054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]