[
    {
        "title": "Linearity of Relation Decoding in Transformer Language Models"
    },
    {
        "review": {
            "id": "GdRn1xbN8d",
            "forum": "w7LU2s14kE",
            "replyto": "w7LU2s14kE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_zSUm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_zSUm"
            ],
            "content": {
                "summary": {
                    "value": "It is well known that language models acquire knowledge of subjects, objects and their relations. However, to date, it is not well understood how these relations are represented in the model. The paper posits that relations are implemented as Linear Relational Embeddings, that is, affine transformations mapping from the embedding of the subject to the embedding of the object. The paper proposes to compute these mappings from the Jacobian $\\delta o / \\delta s$, where o and s are object and subject embeddings at certain layers, respectively.\nThe paper validates this approach by measuring a) faithfulness, i.e., whether decoding from the LRE mapping matches decoding from the true model when given a prompt expressing the relation, and b) to what extend this mapping is a causal explanation by providing an inverse of the LRE capable to change s such that a different, desired o results.\nThe experiments show that the method results in relatively high, but far from perfect, faithfulness and causality scores. The method is compared to number of reasonable baselines/ablations. Finally, the method is applied to the use case of detecting when the language model outputs false relationships despite knowing the true relationship."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* the paper covers an important topic that will certainly raise interest among the ICLR attendees\n* the methods and results are interesting - I learned something.\n* the presentation is excellent. The paper is easy to follow and results are presented in a comprehensive fashion."
                },
                "weaknesses": {
                    "value": "* Limitations are not discussed in enough detail. For example, given a fixed subject and relation, it is possible that there are multiple true objects. This has consequences both for the generation of the datasets (where examples are filtered out if they're not generated by the model) and questions to what extend an invertible function can be a reasonable approximation of the true function, since invertibility implies that the function/relation is injective. Discussing these aspects would improve the paper.\n\nI am a bit skeptical with regards to some results:\n* Table 4 (Appendix A) shows the number of correctly predicted relations per language model. It is quite remarkable that some models in some categories get zero percent of the facts right, whereas other models get a much larger percentage right. Zero percent hints at systematic errors that remain undiscussed in the paper. For example, the LLaMA-13B achieves zero percent on the task of predicting president's birth year and election year. This is quite curious, given that these are quite well-known facts that appear a lot in the pretraining data, and the other models get them right. There is a reason for the poor performance of LLaMA on this particular task: Their tokenizer represents every digit as its own token. However, the caption of Table 4 states that only the first token was used to determine correctness. Since the LLaMa model would require 4 tokens, it had no chance of predicting the \"correct\" token. IMO, this is shortcoming of the proposed measurement, and accepting this bias should at least be discussed. I suspect that similar reasons might cause zero percent accuracy on some of the other tasks.\n* Table 4 (Appendix A) shows that GPT-J gets zero examples from the \"occupation gender\" category right. Hence, none of these examples from this category should remain in the dataset for GPT-J according to the description in Section 4 (Dataset). Curiously, Figure 3 shows that \"occupation gender\" has a near 100% faithfulness success rate. How can that be if there are no such examples?"
                },
                "questions": {
                    "value": "Please respond to the weaknesses raised. I am happy to raise my score if the explanations are satisfactory."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776278035,
            "cdate": 1698776278035,
            "tmdate": 1699636651090,
            "mdate": 1699636651090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KgthiE1cJ4",
                "forum": "w7LU2s14kE",
                "replyto": "GdRn1xbN8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review and your many helpful comments! We are glad to hear that you found our work insightful and interesting, and we think we have addressed your concerns in our revision.\n\n---\n\n> Limitations are not discussed in enough detail. \n\nThanks for the suggestion. We have added a Limitations discussion in the appendix (Appendix I) that discusses dataset size, the risk for false positives when looking only at first tokens, and the assumption that every subject maps to a single object.\n\n---\n\n> For example, given a fixed subject and relation, it is possible that there are multiple true objects. This has consequences both for the generation of the datasets (where examples are filtered out if they're not generated by the model) and questions to what extend an invertible function can be a reasonable approximation of the true function, since invertibility implies that the function/relation is injective. Discussing these aspects would improve the paper.\n\nYou are correct that some of the relations we investigate can have one-to-many mapping where our dataset only records one object for each example. We have discussed this issue in Limitations (Appendix I).\n\nOur measurements are based on the following observation: the role of the LRE is not to recover the idealized relation, but **the LM\u2019s predictions for that relation**. An LM presented with any specific sentence will assign the highest probability on **one** specific token, so the approach we have taken asks: can an LRE recover that same token? By its nature, the top-token prediction is well-defined (each subject maps to one object).\n\n---\n\n> Table 4 (Appendix A) shows that GPT-J gets zero examples from the \"occupation gender\" category right.\n\nGood catch! It turns out there was a bug in the generation of Table 4: instead of collating our full experiment setup on $n = 8$ examples across 24 trials, we generated it based on $n = 5$ examples on a single trial, which erroneously resulted in some tabulated zeros. We have fixed the script and updated Table 4, and now there are no cells with zeros.\n\n---\n\n> For example, the LLaMA-13B achieves zero percent on the task of predicting president's birth year and election year. \n\nWe appreciate the thorough error analysis! Indeed, LLaMA\u2019s tokenizer separates each digit in the years, Since we only look at the first token of both the true object and the predicted object to determine correctness, it is trivial in the opposite direction: the first token of the true object is always `\"1\"` for *president birth year*, so LLaMA need only predict `\"1\"` (which it does). This is also true for most of the examples in *president election year*. The reason LLaMA was scoring 0 in Figure 18 is because we weren\u2019t accounting for spacing in front of the `\"1\"`.\n\nNote that for these cases specifically, **we had already excluded them from the faithfulness and causality results in the original draft**. We realize this was very unclear in the text, so we have added extra wording to Tables 4, 9 and Figure 18  to explain why LLaMA was not evaluated on relations where the object is a year. Additionally, in our new limitations section we discuss this as a pitfall of the first-token evaluation procedure."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700293201388,
                "cdate": 1700293201388,
                "tmdate": 1700293201388,
                "mdate": 1700293201388,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TZxVxRXMVI",
            "forum": "w7LU2s14kE",
            "replyto": "w7LU2s14kE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_y12e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_y12e"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes how the knowledge of relational triples (i.e., subject-relation-object) can be decoded in large language models (LLMs). Specifically, the authors hypothesize that the object embedding can be computed using an affine function (i.e. LRE) from the subject embedding and show that this holds for the majority of relations tested in this work. They also show the causality of the relationship through intervention experiments where a different object can be obtained by changing the subject embedding according to the LRE.  The authors also present the Attribute Lengs, an interesting application of their observation similar to the Logit Lens, which allows one to examine what entities are predicted to be the object in intermediate layers of a transformer model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a novel insight into how relational knowledge is represented in an LLM, which should contribute to a deeper understanding of LLMs in the field.\n- Extensive experiments are carried out to confirm the findings.\n- The paper is well written and easy to follow"
                },
                "weaknesses": {
                    "value": "- The configuration for estimating the parameters of the affine functions could be further explored."
                },
                "questions": {
                    "value": "- In experiments, n = 8 examples are used to estimate W and b. Would it be difficult to use more examples?  They seem to be too few to reliably estimate W and b.  I would also be interested in the variance as well as the mean.  \n- Does a \u201csample\u201d mean a single example?  In statistics, a sample usually means a collection of examples (data points).\n- p. 4: of LM\u2019s decoding -> of the LM\u2019s decoding?\n- p. 5: by (Merullo et al., 2023) -> by Merullo et al. (2023)?\n- p. 7: is a higher -> is higher?\n- p. 8: visualizes a next -> visualizes next?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826181275,
            "cdate": 1698826181275,
            "tmdate": 1699636650983,
            "mdate": 1699636650983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T2zMjhFas6",
                "forum": "w7LU2s14kE",
                "replyto": "TZxVxRXMVI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive review! We are pleased to know that you found our work novel and insightful.\n\n---\n\n> In experiments, n = 8 examples are used to estimate W and b. Would it be difficult to use more examples? They seem to be too few to reliably estimate W and b. I would also be interested in the variance as well as the mean.\n\nWe find that for most of the relations LRE performance saturates after $n = 5$. We have included a Figure (Figure 12) in the appendix which shows our measurements up to $n = 11$, and the plateau after 5. The reason that our LRE estimation procedure is so sample efficient is because it is estimated as a **first-order approximation of the LM computation itself**. We don\u2019t *train* a linear classifier (or affine transformation) to match a set of input-output pairs. Our choice of $n=8$ for large-scale experiments is largely a practical one, determined by the memory consumption of GPT-J on the A6000 GPU available for our research.\n\nWe agree the variance is also important to visualize: Table 10 (previously Table 8) in our appendix shows the standard deviation in LRE performance across 24 trials, each time calculated on different sets of training examples.\n\n---\n\n> Does a \u201csample\u201d mean a single example? In statistics, a sample usually means a collection of examples (data points).\n\nYes! In our draft by sample we mean a single example or a datapoint. To avoid confusion, we have updated the paper to call these \u201cexamples\u201d.\n\n---\n\n> various typos\n\nThanks for catching these typos.  We have addressed all of them in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292616851,
                "cdate": 1700292616851,
                "tmdate": 1700292616851,
                "mdate": 1700292616851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UYFVy9HYvz",
                "forum": "w7LU2s14kE",
                "replyto": "T2zMjhFas6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6053/Reviewer_y12e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6053/Reviewer_y12e"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and the revision. I remain positive about the paper and would like to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628261385,
                "cdate": 1700628261385,
                "tmdate": 1700628261385,
                "mdate": 1700628261385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8fi4H1byOC",
            "forum": "w7LU2s14kE",
            "replyto": "w7LU2s14kE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_wBgc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6053/Reviewer_wBgc"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on analyzing the computation of LLM in the tasks of knowledge decoding. The authors find that a certain kind of computation in relation decoding can be approximated by linear relational embeddings such that R(s) = \\beta Ws + b. Specifically, the intermediate hidden representation of subject is used in this linear transformation. With experiments on 47 different relations, the authors find that for some relations the linear approximation hold. However, this linear attribute is not universal. Furthermore, the authors conducted experiments to show the causality of the linearity of relation decoding. Finally, a visualization tool called attribute lens is proposed to show where and when the LM finishes retrieving knowledge about a specific relation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work focused on an interesting question about what computations the LMs perfrom while resolving relations. The authors smartly use the local derivative to obtain the affine transformation approximation. This is aligned with the traditional design in training knowledge graph embedding. Also, it is very useful to show that causality of the linearly decoding behavior, by using a low-rank pseudoinverse to obtain the perturbation of subject."
                },
                "weaknesses": {
                    "value": "It is unknown that how the context prompt (e.g., [s] plays the) affects the conclusion. For example, will the conclusion hold if we change some other contexts that express same meaning of relation? Though it is interesting to show that some relations matches the linearity hypothesis (e.g., occupation gender, adjective comparative) which aligning traditional methods of training knowledge graph embedding, my concern is that the faithfulness is not very high for most of relations. This paper is more like a case study instead of a systematic measurement which covers a broader range of relations. It is hard to conclude how much percent of relations match the hypothesis. \nFurthermore, it is unclear, if I did not miss some texts, why the causality is usually higher than faithfulness."
                },
                "questions": {
                    "value": "In term of handling objects that have multiple tokens, I saw in Table 4: only the first token was used to determine correctness. Do we apply this strategy to all experiments? Will this lead to false positive? For causality experiment, do we also use the first token of o\u2019 for experiment?\nDo we have some examples showing that `when LRE can not fully capture the LM's computation of the relation, the linear approximation can still perform a successful edit`?\nIn which level of faithfulness, we can say that the linearity holds for the relation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698948987766,
            "cdate": 1698948987766,
            "tmdate": 1699636650869,
            "mdate": 1699636650869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4tOgvMlcHK",
                "forum": "w7LU2s14kE",
                "replyto": "8fi4H1byOC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6053/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive review. We are pleased to know that you found the question we are investigating to be \u201cinteresting\u201d and our findings to be \u201cuseful\u201d! \n\n---\n\n> It is unknown that how the context prompt (e.g., [s] plays the) affects the conclusion. For example, will the conclusion hold if we change some other contexts that express same meaning of relation?\n\nDuring early versions of our experiments, we averaged results over multiple different prompts to account for potential difference in LRE performance. We later decided to use only one prompt per relation due to time constraints and because we noticed similar LRE performance across different prompts. We realize this confound is important to account for, so we\u2019ve added Table 7 to the appendix which shows how faithfulness and causality scores vary when LRE is estimated with different prompt templates. Ultimately, we find that most scores do not change dramatically.\n\n---\n\n> In term of handling objects that have multiple tokens, I saw in Table 4: only the first token was used to determine correctness. Do we apply this strategy to all experiments? Will this lead to false positives? For causality experiment, do we also use the first token of o\u2019 for experiment?\n\nWe *always* use the first token for the objects that get tokenized to multiple tokens. This is true for the causality experiments as well.  For some specific relations this strategy can indeed lead to false positives, though we find in practice that such collisions are rare. \n\nTo clarify this, we have made two adjustments to our submission. First, we now discuss this issue explicitly in the limitations. Second, we have included a new table (Table 9) in the appendix which shows that first token collisions are rare: on average, over $93$% of the objects per relation in our dataset can be uniquely identified with their first token. It is worth noting that this number is skewed downwards by relations such as *person mother*, *person father*, and *president birth year*. And, the LRE performs poorly on these relations anyway.\n\n---\n\n> Do we have some examples showing that `when LRE can not fully capture the LM's computation of the relation, the linear approximation can still perform a successful edit`?\n\nHere is one illustrative example where the LRE is not faithful yet we have observed a successful causal edit. Given the prompt `\"Malcolm MacDonald works as a\"`, GPT-J correctly predicts the expected answer, `\" politician\"`. However, an LRE (estimated based on the following examples) will assign the highest probability to `\" writer\"`. \n\n```\n[\n    (subject='Janet Gunn', object='actor'),\n    (subject='Tony Pua', object='politician'),\n    (subject='Valentino Bucchi', object='composer'),\n    (subject='Christopher Guest', object='comedian'),\n    (subject='Giovanni Battista Guarini', object='poet'),\n    (subject='Menachem Mendel Schneerson', object='rabbi'),\n    (subject='Cristina Peri Rossi', object='novelist'),\n    (subject='Nils Strindberg', object='photographer')\n]\n```\nAlthough the correct answer `\" politician\"`  is within the top-5 prediction of the LRE, our faithfulness metric stringently measures top-1 accuracy and will consider it a failure case.\n\nIn contrast, when we compute the edit direction as  $\\Delta \\mathbf{s} = W_r^{-1}(\\mathbf{o}' - \\mathbf{o})$, we find that $\\mathbf{s} + \\Delta \\mathbf{s}$ is often enough to change the LM output from $o$ to $o'$ even when LRE($\\mathbf{s'}$) does not assign highest probability to $o\u2019$.\n\nContinuing this example: LRE failed to predict the correct occupation of $s\u2019$ = `\"Malcolm MacDonald\"`. However, we can use GPT-Js object representation $\\mathbf{o'}$ corresponding to its encoding of Malcom Macdonald\u2019s profession to successfully change the profession of $s$ = `\"Walter Hines Page\"`, from $o$ = `\" journalist\"` to $o\u2019$ = `\" politician\"`, which matches the correct profession of `\"Malcom Macdonald\"`, even though LRE($\\mathbf{s\u2019}$) incorrectly predicted `\" writer\"`.  \n\nWhen causality exceeds faithfulness, it reflects relations for which such examples are prevalent. \n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292188533,
                "cdate": 1700292188533,
                "tmdate": 1700292343701,
                "mdate": 1700292343701,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]