[
    {
        "title": "Backdiff: a diffusion model for generalized transferable protein backmapping"
    },
    {
        "review": {
            "id": "2zbDfBWXjR",
            "forum": "RBs0IfPj5e",
            "replyto": "RBs0IfPj5e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission790/Reviewer_gYSb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission790/Reviewer_gYSb"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a method based on a diffusion model for backmapping coarse-grained MD results to full atom coordinates. By incorporating self-supervised training strategies, the proposed method can be generalized to multiple different coarse-graining (CG) methods. Experimental results indicate that the proposed method achieves better performance than state-of-the-art methods in backmapping CG configurations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method can be applied to multiple different CG methods without the need for retraining.\n\n2. Experimental results demonstrate that the proposed method outperforms baseline methods."
                },
                "weaknesses": {
                    "value": "1. The Equivariance handling approach in the method constructs a reference coordinate system using the first three amino acids of each protein sequence. This implies that if the positions of the first three amino acids vary, the reference system will also differ, which may not be an ideal approach.\n\n2. In Table 3, the Mean Absolute Error (MAE) of bond length for BackDiff (cons) can reach 0, which appears too good to be true.\n\n3. In Table 3, the numerical values of the standard deviation (std) in the second and third rows are almost in the same range as the mean values, which is strange."
                },
                "questions": {
                    "value": "1. Is the model used in the method SE3 equivariant?\n\n2. Given that the model learns the displacement of omitted atoms from alpha carbons, why not directly learn displacement in the local coordinate system of each amino acid? This approach could ensure that the representation is SE3 equivariant.\n\n3. I am not familiar with the PED dataset. Why were only 92 proteins selected out of 227 for training and testing data?\n\n4. What do you mean by single- and multi-protein experiments? What is the primary difference? When frames are used for data partitioning, is it possible for different frames of the same protein to appear in both the training and testing sets, potentially leading to data leakage?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502647653,
            "cdate": 1698502647653,
            "tmdate": 1699636006440,
            "mdate": 1699636006440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0HFuRdNgtX",
                "forum": "RBs0IfPj5e",
                "replyto": "2zbDfBWXjR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of revised draft"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nI sincerely appreciate the time and effort you invested in reviewing my paper. Your insightful comments and suggestions have been instrumental in enhancing the quality of this work. Before moving on to specific questions, I would like to summarize major revisions made in the new version of the paper, as highlighted in light blue in the paper: \n\n1. Integration of Equivariant GNN: We have now adapted an equivariant GNN to parameterize the time-dependent score function. We have provided theoretical proof demonstrating how an SE(3)-equivariant score function can effectively lead to the desired equivariant conditional. The revision is made in Section 4.3 (Proposition), Appendix A.2 (Proof), and Appendix G (Model Architecture). \n\n2. Introduction of new metrics: In response to the need for more comprehensive evaluation criteria, new metrics have been introduced to assess the accuracy and diversity of the generated samples. The new metrics are briefly summarized, which you can find details in Evaluation Metrics in Section 5 and Appendix F:\n               \n               * Accuracy: the minimum RMSD of generated structures w.r.t the original all-atom configuration; sidechain COM MAE of the generated structure with the minimum RMSD from the original all-atom configuration. \n               * Diversity: generated diversity score which indicates diversity while avoiding giving high diversity scores (low DIV) to models that generate totally off configurations. This metric is suggested in [1]. \n               * Physical realistic: steric clash ratio \n\n3. Testing data adjustments: We have removed the disordered test protein (PED00025) from our testing data. In its place, we have included a mostly globular protein (PED00055) to provide a more robust and representative test case. The revision can be found in Section 5. \n\nAll training and inference are rerun on the same devices with new results and figures reported. \n \nReference: \n[1] Jones, Michael S., Kirill Shmilovich, and Andrew L. Ferguson. \"DiAMoNDBack: Diffusion-Denoising Autoregressive Model for Non-Deterministic Backmapping of C\u03b1 Protein Traces.\"\u202fJournal of Chemical Theory and Computation\u202f(2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255061014,
                "cdate": 1700255061014,
                "tmdate": 1700255061014,
                "mdate": 1700255061014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AkulCzCPr4",
                "forum": "RBs0IfPj5e",
                "replyto": "2zbDfBWXjR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to each question"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nI want to express my deepest gratitude for the great questions and advice you raise. Below is a detailed response to each question: \n1. Is the model used in the method SE3 equivariant? \n* Thank you for insightful questions regarding the SE(3) equivariance. Our model in the first draft indeed does not use an equivariant neural network to preserve the SE(3) equivariance. Instead, we built a coordinate framework internally for each CG configuration and we used the framework in the diffusion model. However, as you pointed out, this approach might limit the model's expressivity. Thus, we have re-implemented an SE(3) equivariant BackDiff and have updated all results using the new equivariant model. We also prove that with an equivariant score function, we can achieve the desired SE(3)-equivariance. \n2. Given that the model learns the displacement of omitted atoms from alpha carbons, why not directly learn displacement in the local coordinate system of each amino acid? This approach could ensure that the representation is SE3 equivariant. \n* Thank you for your excellent advice regarding the use of local coordinate systems for each amino acid to ensure SE(3)-equivariance. This approach presents a valuable alternative to our current method. We appreciate this advice and will explore its feasibility in future research. \n3. I am not familiar with the PED dataset. Why were only 92 proteins selected out of 227 for training and testing data? \n* I apologize for any confusion regarding the selection of proteins. We select proteins in the PED dataset following the guidelines discussed in the GenZProt paper. Metal ion-binding complexes, nucleotide-binding complexes, cofactor-binding complexes, PTM-including proteins except phosphorylation and oxidation, D-amino acid protein, and proteins simulated or experimentally measured in unnatural conditions are excluded. The 92 proteins selected are those computed from reliable MD simulations and sampling methods. \n4. What do you mean by single- and multi-protein experiments? What is the primary difference? When frames are used for data partitioning, is it possible for different frames of the same protein to appear in both the training and testing sets, potentially leading to data leakage?  \n* We apologize for the unclear presentation in the manuscript. To clarify: single-protein experiments involve using data from a single protein. We partition ensembles of frames from this single protein into training and testing datasets. This tests the model's ability to predict structures within a single protein's conformational space. In contrast, multi-protein experiments involve training and testing the model on multiple different proteins. This tests the model's ability to generalize across various proteins. Due to the large conformation space of long-chain proteins, and the comparatively small numbers of ensembles in our dataset, we believe data leakage is not an issue. Still, to make our results more convincing, in the new training of the single-protein experiments, we have reduced the number of training data. \n5. In Table 3, the Mean Absolute Error (MAE) of bond length for BackDiff (cons) can reach 0, which appears too good to be true. \n* Thank you for highlighting this point about the Mean Absolute Error (MAE) of bond length in Table 3.  The bond length MAE does not actually reach 0, but should be expressed more rigorously as \"<0.001\", which I have fixed in the revised draft. This extremely low MAE for bond length is expected.  Applying manifold constraint on score-based diffusion model is similar to applying a restraint potential to certain degrees of freedom in molecular dynamics, restraining the variation on these DOF. The effectiveness of manifold constraint is better for constraining bond length compared to bond angle. This observation is consistent with recent studies [Improving Diffusion Models for Inverse Problems using Manifold Constraints] and [DIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS] which implied that, empirically, a linear condition mapping (like bond lengths) tends to be more easily matched than a nonlinear condition mapping (like bond angles). Additionally, as shown in Table 6, the choice of manifold constraint weight plays a significant role. A small weight can lead to error in condition, while a large weight might over-deviate the sampling. The weight we choose is optimized by balancing the condition error and the sampling path.   \n6. In Table 3, the numerical values of the standard deviation (std) in the second and third rows are almost in the same range as the mean values, which is strange. \n* Thank you for raising questions regarding the bond angle MAE. Like in Table 1 and 2, we evaluate the bond angle MAE of generated all-atom configurations for each testing frame, and compute the mean and std across all testing frames. Thus, a large std should be expected as they are results from backmapping different CG configurations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255856532,
                "cdate": 1700255856532,
                "tmdate": 1700255856532,
                "mdate": 1700255856532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rbd4cuLtYB",
            "forum": "RBs0IfPj5e",
            "replyto": "RBs0IfPj5e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission790/Reviewer_np12"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission790/Reviewer_np12"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a generalized transferable backmapping method that can be applied to arbitrary CG mapping without the need for retraining. The paper formulates backmapping as an imputation problem, where the model generates C alpha-atom distance vectors conditioned on CG atoms and CG auxiliary variables (aggregated properties of groups of atoms). The model can achieve generalization across different CG mappings by training with (semi)-randomly selected CG atoms and auxiliary variables. The model generates output in Cartesian coordinate space but produces well-constrained bond lengths and angles by imposing manifold constraints. The model is compared to a recent transferable generative modeling work, with experiments conducted following similar settings including the dataset and metrics, as well as a recent all atom conformer generation model.   \n\nThis paper shows clear novelty and strengths, but I still have some questions regarding the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis is a first backmapping algorithm generalized for arbitrary CG mappings. \n2.\tThe idea of formulating the generalized backmapping problem as an imputation problem is novel and makes a lot of sense."
                },
                "weaknesses": {
                    "value": "-\tHave you re-implemented the baseline models (especially CGVAE) to condition them on other CG variables such as N and sidechain COM for the UNRES benchmark? If BackDiff is conditioned on C alpha, N, and side chain COM, while CGVAE is conditioned only on C alpha as in its original paper, it would be hard to tell if the performance difference is coming from the method or the difference in information given to the models. The same applies for MARTINI and Rosetta benchmarks. Alternatively, you could report the performance of BackDiff conditioned on C alpha only, with no CG auxiliary variable constraints on side chain COM. \n-\tTable 1 and Table 2 report the mean RMSD across 100 sampled structures. However, a large mean RMSD of the backmapped structures could also suggest high diversity among all atom conformations, rather than high error in the structures, since one CG structure can correspond to many all atom conformations. Reporting the minimum RMSD across 100 samples should be a better metric for assessing error.\n-\tHow did you select the PED entries for testing? The three test proteins all look pretty linear and disordered. How does the model perform on a globular protein?\n-\tCould you report the diversity of the generated structures conditioned on the same CG structure? For example, in the referred baseline [1], the authors reported quantitative metrics for diversity, such as the Earth Mover\u2019s Distance for side chain torsion angles. \n-\tCould you provide a speed analysis of your method, for example how much time required to backmap a frame, similar to what was done in [1]?\n-\tIt could be interesting to see how the model performance changes as we increase the CG resolution (the number of CG atoms and CG auxiliary variables), especially in terms of the diversity of generated all atom conformations. This could provide insights into the CG system\u2019s entropy. This is not a requirement, but just a curiosity."
                },
                "questions": {
                    "value": "-\tYang & Bombarelli\u2019s model is called GenZProt and not CGVAE."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission790/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803373360,
            "cdate": 1698803373360,
            "tmdate": 1699636006352,
            "mdate": 1699636006352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6tR2ctbJDw",
                "forum": "RBs0IfPj5e",
                "replyto": "Rbd4cuLtYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Summary of revised draft"
                    },
                    "comment": {
                        "value": "Dear reviewer, \nI sincerely appreciate the time and effort you invested in reviewing my paper. Your insightful comments and suggestions have been instrumental in enhancing the quality of this work. Before moving on to specific questions, I would like to summarize major revisions made in the new version of the paper, as highlighted in light blue in the paper: \n\n1. Integration of Equivariant GNN: We have now adapted an equivariant GNN to parameterize the time-dependent score function. We have provided theoretical proof demonstrating how an SE(3)-equivariant score function can effectively lead to the desired equivariant conditional. The revision is made in Section 4.3 (Proposition), Appendix A.2 (Proof), and Appendix G (Model Architecture). \n\n2. Introduction of new metrics: In response to the need for more comprehensive evaluation criteria, new metrics have been introduced to assess the accuracy and diversity of the generated samples. The new metrics are briefly summarized, which you can find details in Evaluation Metrics in Section 5 and Appendix F:\n               \n               * Accuracy: the minimum RMSD of generated structures w.r.t the original all-atom configuration; sidechain COM MAE of the generated structure with the minimum RMSD from the original all-atom configuration. \n               * Diversity: generated diversity score which indicates diversity while avoiding giving high diversity scores (low DIV) to models that generate totally off configurations. This metric is suggested in [1]. \n               * Physical realistic: steric clash ratio \n\n3. Testing data adjustments: We have removed the disordered test protein (PED00025) from our testing data. In its place, we have included a mostly globular protein (PED00055) to provide a more robust and representative test case. The revision can be found in Section 5. \n\nAll training and inference are rerun on the same devices with new results and figures reported. \n \nReference: \n[1] Jones, Michael S., Kirill Shmilovich, and Andrew L. Ferguson. \"DiAMoNDBack: Diffusion-Denoising Autoregressive Model for Non-Deterministic Backmapping of C\u03b1 Protein Traces.\"\u202fJournal of Chemical Theory and Computation\u202f(2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255017225,
                "cdate": 1700255017225,
                "tmdate": 1700255017225,
                "mdate": 1700255017225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lqRgNOBt8V",
                "forum": "RBs0IfPj5e",
                "replyto": "Rbd4cuLtYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission790/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to each question"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nI want to express my deepest gratitude for the great questions and advice you raise. Below is a detailed response to each specific question:\n1. Comment:  Have you re-implemented the baseline models (especially CGVAE) to condition them on other CG variables such as N and sidechain COM for the UNRES benchmark? If BackDiff is conditioned on C alpha, N, and side chain COM, while CGVAE is conditioned only on C alpha as in its original paper, it would be hard to tell if the performance difference is coming from the method or the difference in information given to the models. The same applies for MARTINI and Rosetta benchmarks. Alternatively, you could report the performance of BackDiff conditioned on C alpha only, with no CG auxiliary variable constraints on side chain COM.\n* Response: Thank you for highlighting the importance of a fair comparison in our experiments. We did reimplement the baseline models. I maintained all the original GenZProt architecture, except that the residue-residue message-passing and residue-atom message-passing will contain additional particles corresponding to additional CG beads other than alpha carbons. I think this is a fair comparison, as our method BackDiff, and the baseline GenZProt and modified Torsional Diffusion all utilize the same model architecture (e3nn package with E(3) equivariant convolution as message-passing layers).\n\n2. Comment:  Table 1 and Table 2 report the mean RMSD across 100 sampled structures. However, a large mean RMSD of the backmapped structures could also suggest high diversity among all-atom conformations, rather than high error in the structures, since one CG structure can correspond to many all-atom conformations. Reporting the minimum RMSD across 100 samples should be a better metric for assessing error. \n* Response:  Thank you for your valuable suggestion regarding the use of RMSD as a metric. I agree that the mean RMSD can indeed be ambiguous as it might reflect both the accuracy and the diversity of the backmapped structures. A single CG structure can correspond to multiple all-atom conformations, and thus, a high mean RMSD might not directly imply a high error.  We have revised our evaluation strategy to the minimum RMSD as per your suggestion.\n\n3. Comment:  How did you select the PED entries for testing? The three test proteins all look pretty linear and disordered. How does the model perform on a globular protein? \n* Response:  Thank you for your insightful observation regarding our selection of PED entries for testing.  Including only disordered proteins may limit the scope of our evaluation. To address this, we have revised our test set to incorporate a broader range of protein structures. Specifically, we have replaced one of the previously used test proteins (PED00025, a disordered protein) with PED00055, a well-characterized globular protein. This choice was guided by the aim of assessing the model's performance across a more diverse range of protein structures, including those with more defined and compact conformations.\n\n4. Comment:  Could you report the diversity of the generated structures conditioned on the same CG structure? For example, in the referred baseline [1], the authors reported quantitative metrics for diversity, such as the Earth Mover\u2019s Distance for side chain torsion angles. \n* Response:  Diversity is an important metric to evaluate, and we have included such a measurement (DIV) in the new draft. The result shows the BackDiff trained with transferable CG protocol generates samples with the highest diversity overall.\n\n5. Comment:  Could you provide a speed analysis of your method, for example how much time required to backmap a frame, similar to what was done in [1]?\n* Response:  Certainly! We have added a brief speed analysis in Appendix I: Sampling Efficiency. In general, the diffusion model is slow in sampling, especially with the manifold constraint technique. BackDiff roughly requires around 5 minutes to generate 100 samples for a single frame.\n\n6. Comment:  It could be interesting to see how the model performance changes as we increase the CG resolution (the number of CG atoms and CG auxiliary variables), especially in terms of the diversity of generated all-atom conformations. This could provide insights into the CG system\u2019s entropy. This is not a requirement, but just a curiosity. \n* Response:  This is indeed an interesting question to investigate. Although we do not put such a comparison in the text, we can observe Table 2 (multi-protein experiment for UNRES, a CG of lower resolutions) and Table 8 (multi-protein experiment for MARTINI, a CG of higher resolutions).  Backmapping from CG of lower resolutions will generate samples with higher diversity (lower DIV score). We suspect that for BackDiff, since MARTINI utilizes multiple sidechain beads, and all this information is used for manifold constraint, the generated samples all get closer to the original all-atom configuration."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission790/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255530031,
                "cdate": 1700255530031,
                "tmdate": 1700255530031,
                "mdate": 1700255530031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]