[
    {
        "title": "Invariant Attention: Provable Clustering Under Transformations"
    },
    {
        "review": {
            "id": "gLcJzpJdfz",
            "forum": "SirD4KYNRr",
            "replyto": "SirD4KYNRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_4tjm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_4tjm"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents invariant attention that can cluster images invariant to geometric transformations. It introduces an invariant kernel that computes the maximum similarity between two images after optimizing over transformations. This allows computing meaningful attention weights between transformed images. In addition, the paper presents a theoretical foundation for the approach, demonstrating its efficacy through some simple experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A new attention mechanism that incorporates invariance properties.\n2. The paper provides a solid theoretical foundation for the properties of invariant attention with proof.\n3. While the concept of invariance is not a new idea, it remains crucial for the transformer architecture."
                },
                "weaknesses": {
                    "value": "1. While the paper presents mathematical formulations specific to its method, it's not immediately clear how this approach can be adapted or generalized to ViT or other transformer architectures.\n2. The proposed method is still based on dynamic kernels [1, 2, 3]. Why the kernels based on averaging are better than previous attempts?\n3. Current empirical validation is limited - more quantitative experiments ($e.g.,$ overall accuracy over MNIST) would strengthen the claims. In addition,  more qualitative results on complex image datasets ($e.g.,$ CIFAR100) or the impact of downstream tasks would be useful.\n4. (Minor) There are numerous instances of \"??\", likely due to the separate submission of the main text and supplementary material. Also, there are some typos ($e.g.,$ in theorem 4.1, \" invariant attention\"). The authors should proofread carefully.\n\n\n[1] Spatial Transformer Networks. \n\n[2] LocalViT: Bringing Locality to Vision Transformers.\n\n[3] Learning from Few Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698201950580,
            "cdate": 1698201950580,
            "tmdate": 1699636775499,
            "mdate": 1699636775499,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eMDJqCqjqz",
                "forum": "SirD4KYNRr",
                "replyto": "gLcJzpJdfz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 4tjm, thank you for your valuable feedback. Below we respond to your comments in detail.\n\n1. We thank you for this important remark. We have improved the text to make the motivation and approach more clear, while acknowledging that incorporating IA into a ViT is yet to be shown. However, we believe that the following intuitions hold promise. One of the reasons that transformers require large amounts of data compared to CNNs to achieve the same accuracy is that transformers ignore structural properties of images, especially geometric and spatial properties. Images naturally contain repeated motifs that are some geometric transformation of each other due to factors such as symmetry, camera motion/perspective, object orientation, etc. These geometric transformations are combined with statistical variability to produce intra-class variability in these motifs. CNNs are equipped with the geometric property of translation-equivariance. Since attention mechanisms in transformers offer no such benefits, in our work, we propose Invariant Attention (IA) as a potential replacement to the kernel (self) attention sublayer in a ViT layer while keeping the feedforward sublayer intact. We believe that, enhanced with IA, the transformer architecture would be invariant to geometrical transformations in data, thus eliminating the need to learn invariance from scratch and reducing the total amount of data needed to train a Vision Transformer. Attention mechanisms possess intrinsic clustering properties, as elucidated in the Introduction and visually represented in Figure 2. This is particularly evident with image data that contain motifs subject to geometric transformations. In such cases, the Euclidean distance between transformed images belonging to the same class would not be small, thereby incorrectly clustering upon repeated application of kernel attention. It is this clustering nature of attention that we have tried to address through a generalized optimization framework, and provided a proof for one group of transformations. We believe this inherent clustering property of Invariant Attention will introduce benefits of transformation-invariance to transformers.\n\n2. Thank you for pointing out these excellent references. These works propose valuable ideas for endowing neural networks with invariance [1] and/or locality [2, 3]. For example, the authors of Spatial Transformers [1] present a learnable module that achieves invariance to transformations via a so-called localization network (a fully-connected network or a CNN). The authors in [2] present LocalViT, where locality is added to vision transformers via adding depth-wise convolution into the feed-forward network. Both works use approaches that take advantage of the 2D structure of images (e.g. adding convolutions). While these works provide substantial empirical results, they remain a black-box approach, and do not aim to provide theoretical analysis that would provide further insight or performance guarantees. Additionally, none of them propose an attention-mechanism based method. In contrast, the proposed Invariant Attention mainly provides theoretical results of provable clustering and simple proof-of-concept experiments that do not involve training. We agree that [1-3] are notable works in the field. Lastly, our proposed kernel is not based on averaging but on maximum similarity. The authors in [3] also propose such a kernel and use probabilistic analysis to show its strengths and valuable properties, e.g. positive definiteness with high probability. In our proposed Invariant Attention, however, we utilize this kernel to calculate invariant weights.\n\n3. Thank you for the constructive feedback on the limitations of empirical analysis. We acknowledge this need and are working to provide more quantitative experiments over MNIST. However, to run experiments on CIFAR100 or other datasets of natural images where statistical variability takes many forms other than geometry, one would need to incorporate a more complex model (e.g. a network with nonlinearities), which we leave for future directions of this work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734904769,
                "cdate": 1700734904769,
                "tmdate": 1700734904769,
                "mdate": 1700734904769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gDmBoclWCR",
            "forum": "SirD4KYNRr",
            "replyto": "SirD4KYNRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_9EKX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_9EKX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for calculating attention between images (or image patches), which is invariant to some pre-defined set of transformations. The attention mechanism itself is formulated as kernel attention, and the kernel is constructed to be invariant to a set of transformations. The paper states that iteratively applying their attention mechanism clusters the provided samples into their invariant means, and provides some theoretical guarantees of convergence of this procedure. Authors claim that their findings could help build novel, data-efficient, invariant attention mechanisms implemented into modern vision Transformer networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method has a nice clustering quality, in which it tends to cluster similar images together. This feature is proved theoretically and empirically. However, the proof is questionable."
                },
                "weaknesses": {
                    "value": "The paper is very poorly written and looks raw. Apart from the large number of typos, and repetitions (which I will list separately), there are some major issues with the statements themselves.\n\n1. The problem of the paper is not clear to me. What are the problems where the Invariant Attention is needed? Will it increase the overall data efficiency of ViT-type models? How is the clustering property helpful in this case?\n2. There are no proofs of the theorems and claims, and not even a sketch of the proof or an idea is provided. Though the authors claim to put it in the appendix, it is not possible right now to review the correctness. The definition of \u201cinfimal convex combination reach\u201d, which is an important part of the theory, is not provided even on an intuitive level.\n3. At the same time, there is a lot of redundancy. Some almost self-evident claims are explained in long, like the fact that the invariant mean is indeed invariant under image transformations, which is evident from its close form. \n4. It is not clearly stated which transformations are admissible. Authors claim that their method works for any transformation set, but provide theoretical guarantees only in the case of the T=SE(2). At the same time, they state that Invariant Attention enforces invariance under unknown transformations of the domain, which is clearly misleading, and the transformation set should be known beforehand to construct the kernel.\n5. The optimization procedure for finding invariant mean is not fully described. How are the transformation vectors \\tau_i parametrized in each experiment? Exactly, what parameters are we optimizing, and how? It would be nice to have a clearly described algorithm in the form of pseudo-code or something like that. Also, no information about the time complexity or needed resources is provided in the experiment part.\n6. Not a learnable algorithm. Though the authors claim that they are currently working at implementing learnable weights inside Invariant Attention, its real applicability to modern visual transformer models in the presented form is questionable. It requires running an optimization procedure for each attention head and each pair of image patches only to calculate the kernel weights. This is also dependent on the dimension and complexity of the transformation set and will require training separate models for different symmetry groups. The issue is not addressed in the paper.\n7. Novelty. The kernel attention mechanism was earlier introduced by (Tsai et. al., 2019), and the kernel used in the calculations was described by (Liu et.al, 2021), so the only novel part is in the theoretical results of the paper, which are not significant enough. It is no wonder that we will identify clusters in the data when the data itself is composed of the groups of samples varied through transformations, and we seek to find these exact transformations to match two samples. \n\nThe typos:\n\n1. Page 5: \u201c(distance given by (??))\u201d, \u201cThis is described in detail in the appendix section ??\u201d, \u201cAs illustrated in Figure ??, we have that\u201d. Also, $\\phi(\\mu)$ is not defined beforehand here.\n2. Page 6: \u201cThe definitions and its motivations are found in appendix section ??\u201d, \u201cand is found in Appendix Section ??\u201d\n3. The $\\beta$ in theorem 4.1 is not defined.\n4. Page 7: \u201cWe see that at the end of 50 iterations, we have an meaningful invariant mean!\u201d\n5. Page 8: The subtitle \u201cFigure 4: Invariant weights and invariant means of\u201d is not complete. Also, the same for Figure 5.\n6. Page 9: there are 2 almost exact paragraphs on the Invariant Point Attention"
                },
                "questions": {
                    "value": "1. Could you describe possible applications of the Invariant Attention for real-world data?\n2. Your results (Theorem 3.1) are formulated for SE(2) explicitly. How is that transferable to other groups of transformations?\n3. What kind of structural properties are preserved or exploited by Invariant Attention? How will it help in prediction? May invariance to transformations actually harm the prediction quality, when the focus is on orientation, for example? Like classifying the right arrow and left arrow, for example."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6740/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6740/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6740/Reviewer_9EKX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683953886,
            "cdate": 1698683953886,
            "tmdate": 1699636775382,
            "mdate": 1699636775382,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3EV7YGVZKL",
                "forum": "SirD4KYNRr",
                "replyto": "gDmBoclWCR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Responses to Weaknesses:\n1. Thanks for the comment! Clustering is an important problem in unsupervised machine learning. Currently there is no provable clustering technique for data that contains motifs subject to transformations. We have formulated a framework useful for this task for any general transformation group and have proved clustering for a certain class of transformations, the SE(2) group. Application to ViTs: A reason for transformers needing more data than CNNs to train up to the same level of accuracy is that transformers ignore structural properties of image data, especially geometric and spatial properties. Image patches have repeated motifs that are some geometric transformations of each other due to factors such as symmetry, camera motion and perspective, object orientation, etc. These geometric transformations are combined with statistical variability to produce intra-class variability in these motifs. CNNs are equipped with the geometrical property of translation equivariance. Since attention mechanisms in transformers offer no such benefits, in our work, we propose Invariant Attention (IA) as a potential replacement to the kernel attention (self-attention) sublayer in a ViT layer while keeping the feedforward sublayer intact. We believe that, enhanced with IA, the transformer architecture would be invariant to geometrical transformations in data and additional data that was otherwise needed by the transformer to learn this invariance will no longer be required. This, we believe, would reduce the total amount of data needed to train a Vision Transformer. Attention mechanisms have inherent clustering properties as we have explained in the introduction Section and through experiment in Figure 2. when our data contains images/image patches containing motifs subject to geometric transformations since Euclidean distance between transformed images of the same class is not small would not be clustered together on repeated application of Kernel attention. It is this clustering nature of attention that we have tried to address through a generalized optimization framework and proved theoretically for one group of transformations. We believe this inherent clustering property of Invariant Attention will introduce benefits of transformation-invariance to transformers.\n\n2. Thanks for the suggestion. The proof involves many geometric ideas, and we could not sketch them in the main body due to space limitations. However, we have improved the Section on Uniqueness, Section 3 to better motivate some ideas leading up to the main results.\n\n3. Thanks for the comment. From the formulation in equation 2.3, we have used 1. closure properties of a transformation group and norm preserving property of Euclidean transforms to remark that the optimization problem in Equation 2.3 contains solutions that are transformations of each other. This is also not readily evident for other groups of transformations. The closed form expression of the invariant mean also requires additional geometric properties of the data as required by Danskin's theorem and we have highlighted them in the updated version to signify the importance of these observations.\n\n4. Thank you for the feedback. We have added a new Section, Section 2.1 that details the optimization of transformations and discusses the generalization ability of this framework to other transformation groups We can impart invariance to the group of transformations we desire. Some examples of transformation groups that are geometric transformation groups are Affine group, Similarity group, etc. When implemented for these groups, invariant attention enforces the Invariance property to the domain of the selected transformation group. We have added to text to distinguish the generalizability of the framework to other transformation groups and the transformation group we have chosen for our proof.\n\n5. Thank you for the feedback. We have added two Sections: Section 2.1 and 2.4 that detail the optimization procedure. In these Sections we have included an algorithm box, optimization procedure and parallelizable nature of our computations.\n\n6. Thanks for the feedback. When compared to the current Kernel Attention mechanism, Invariant Attention does add computational overhead. However, as described in Section 2.4, most of this overhead is parallelizable and the benefits of invariance properties to transformations can be imparted for clustering or into ViT architectures."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733520900,
                "cdate": 1700733520900,
                "tmdate": 1700733520900,
                "mdate": 1700733520900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M1uy3Wo0Dd",
                "forum": "SirD4KYNRr",
                "replyto": "gDmBoclWCR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Response to Questions:\n\n2. Thank you for the question. We have added an new Section, Section 2.1 that details the optimization of transformations and discussues the generalizability of this framework. Some examples of transformation groups that are geometric transformation groups are affine group, similarity group, etc.\n\n3. Invariant attention will be helpful for problems in classification where the class label is invariant to transformations. For problems in which orientation is of interest, one solution is to extend the output of IA to output not only the invariant mean, but also the optimal transformations which generated the Invariant Mean. We note that these transformations and the Invariant Mean are jointly optimized in the computation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733629935,
                "cdate": 1700733629935,
                "tmdate": 1700733648857,
                "mdate": 1700733648857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ACFppUBx7F",
            "forum": "SirD4KYNRr",
            "replyto": "SirD4KYNRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_omyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_omyp"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an addition to self-attention that is invariant to various transformations. Mainly, while self-attention is based on the similarity between two entities, the proposed method is based on the maximum similarity between transformed samples. Essentially, the framework proposes replacing $k(x,y)$ with $max_{T_1,T_2} k(T_1(x),T_2(y))$. This way, any transformation $T$ applied to samples $x,y$ does not influence the similarity. Additional non-linearities or learnable parameters are ignored. Two results are proved: 1. The proposed invariant attention results in a unique solution (up to transformation) 2. The procedure converges."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strong Points:\n\n- The invariance of machine learning models is an important topic, useful for generalization and sample efficiency.\n- The method seems technically correct."
                },
                "weaknesses": {
                    "value": "Weak Points:\n\n- There are no details on how to obtain the optimal transformations from 2.6. How to obtain these transformations is crucial. Without an efficient way to obtain them, the proposed method cannot be applied in practice.\n\n- The method does not involve actual feature learning. It's hard to argue the importance of the method for machine learning methods when there is no actual representation learning happening.\n\n- The experiments are extremely simple: 6 transformations of the same image or 10 MNIST samples. These might be good for a first step to see that the method/implementation is sound, but more validations are needed for a novel machine learning method.\n\n- \u201cInvariant Attention, enforces invariance under unknown transformations of the domain by optimizing over these transformations\u201d How general are the transformations that Invarian Attention can optimize over? What kind of transformations can be optimized in practice?"
                },
                "questions": {
                    "value": "Typo? It seems like equation 3.1 needs two indices, one for sample $v_i$ and one for transformation $\\tau_j$. Also, the number of samples and number of transformations should be different.\n\nMinor: There are some broken references. E.g \u201cdistance given by (??))\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775699852,
            "cdate": 1698775699852,
            "tmdate": 1699636775265,
            "mdate": 1699636775265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vwFFZJy7xD",
                "forum": "SirD4KYNRr",
                "replyto": "ACFppUBx7F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Responses to Weaknesses:\n1. Thank you for the feedback. We have added two Sections: Section 2.1 and 2.4 that detail the optimization procedure.\n2. Thanks for the comment! Clustering is an important problem in unsupervised machine learning. Currently there is no provable clustering technique for data that contains motifs subject to transformations. We have formulated a framework useful for this task for any general transformation group and have proved clustering for a certain class of transformations, the SE (2) group. We have added details about the optimization procedure for the invariant mean in Section 2.4.\n3. Thank you for the constructive feedback on the limitations of empirical analysis. We acknowledge this need and are working to provide more quantitative experiments over MNIST. However, to run experiments on CIFAR100 or other datasets of natural iamges where statistical variability takes many forms other than geometry, one would need to incorporate a more complex model (e.g. a network with nonlinearities), which we leave for future directions of this work.\n4. Thank you for the question. We have added a new Section, Section 2.1 that details the optimization of transformations and discusses the generalizability of this framework. Some examples of transformation groups that are geometric transformation groups are affine group, similarity group, etc."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733032869,
                "cdate": 1700733032869,
                "tmdate": 1700733072419,
                "mdate": 1700733072419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6KKGpXjxMC",
            "forum": "SirD4KYNRr",
            "replyto": "SirD4KYNRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_pRB3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6740/Reviewer_pRB3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new attention mechanism called invariant attention. The paper shows that the proposed attention has theoretical guarantees and can be applied to solve image clustering problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-This paper aims to improve the attention mechanism, which is the foundation in the widely used transformer architecture.\n\n-This paper provides extensive theoretical analysis for the proposed attention mechanism."
                },
                "weaknesses": {
                    "value": "-The experiments are not sufficient to support the claims. First, there is no comparison with previous works in the experimental section. Second, there is no quantitative result. Without those, I cannot judge the if the proposed technic is useful or not and the significance of the proposed method."
                },
                "questions": {
                    "value": "-I cannot find the theoretical proof that shows that \"the Invariant Attention is far more successful than standard kernel attention\". I might miss this part because I am not an expert in theoretical ML."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6740/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699283405615,
            "cdate": 1699283405615,
            "tmdate": 1699636775129,
            "mdate": 1699636775129,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1lAPafsnX2",
                "forum": "SirD4KYNRr",
                "replyto": "6KKGpXjxMC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6740/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Response to Weaknesses:\n\nThank you for your constructive feedback. We acknowledge the need for quantitative results, and are currently performing a quantitative comparison of kernel attention and Invariant Attention to measure clustering accuracy. This experiment will be added to the next version of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6740/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733165991,
                "cdate": 1700733165991,
                "tmdate": 1700733741556,
                "mdate": 1700733741556,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]