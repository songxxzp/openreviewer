[
    {
        "title": "Contextual Molecule Representation Learning from Chemical Reaction Knowledge"
    },
    {
        "review": {
            "id": "r2yQWHCve5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_H5fY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_H5fY"
            ],
            "forum": "aqTipMg9CZ",
            "replyto": "aqTipMg9CZ",
            "content": {
                "summary": {
                    "value": "This study focuses on understanding molecular representation by anticipating the context of hidden atoms. \nWhile numerous prior research has delved into graph representation by reconstructing the masked components in the graph, this study seeks to reconstruct the \"reaction center,\" believed to be vital in grasping the context of chemical reactions. \nThe authors start by pinpointing reaction centers as previous works - by recognizing atom pairs with differing bond types between the reactant and product graphs. \nThey then introduce two unique training strategies: the masked reaction center reconstruction and reaction center identification. \nThe former targets the prediction of the specific type of concealed atoms, while the latter determines if the atoms in a molecule belong to the reaction center."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Learning the representations of molecules is important for various downstream tasks.\n2) Idea of identifying the reaction center is novel and the approach will definitely help the model to understand underlying chemical knowledge.\n3) Extensive experiments on various downstream tasks demonstrate the superiority of REMO."
                },
                "weaknesses": {
                    "value": "1) In paragraph 2 of the Introduction, the authors mentioned that \"in molecule graphs the relationships between adjacent sub-units are mostly irrelevant\". Is there any reference for this? I don't agree because the sub-units of the molecules are relevant to each other, and therefore, it will be helpful in reconstructing the molecule structure from the given molecule.\n\n2) Weak experimental results.\n- The most important baseline [1] is missing. It should be compared to demonstrate the effectiveness of reconstructing reaction centers instead of just learning from chemical formulas.\n- In Table 1, it would be more convincing why fingerprint-based methods outperform deep models [2].\n- In Table 2, comparing Mole-BERT and GraphLoG will be helpful since those methods outperform GraphMVP in Table 3.\n- In Table 4, why many baseline models in previous tables are missing? Overall, various self-supervised learning methods should be compared in all tasks since they can be applied to all tasks.\n- Moreover, in MolR [1], they have a task for chemical reaction classification, which aims to predict the reaction class that a chemical reaction belongs to. It would be great if REMO outperforms MolR in the task, thereby demonstrating the superiority of MolR in understanding underlying chemical reaction knowledge.\n\n[1] CHEMICAL-REACTION-AWARE MOLECULE REPRESENTATION LEARNING, ICLR 2022.\n\n[2] Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?, arxiv 2023.\n\n3) No codes are available."
                },
                "questions": {
                    "value": "Provided above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5944/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5944/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5944/Reviewer_H5fY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697207952631,
            "cdate": 1697207952631,
            "tmdate": 1699636633465,
            "mdate": 1699636633465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cDVmxBknRt",
                "forum": "aqTipMg9CZ",
                "replyto": "r2yQWHCve5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer H5fY 1/2"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your insightful comments and suggestions. We have diligently revised our manuscript in accordance with your guidance. Below, we address your raised concerns and inquiries:\n\n**On the consensus of our motivations**\n\nWe apologize for any inaccuracies in our initial explanation. Our intent is to illustrate that a diverse range of atoms can form stable molecules within the realm of organic chemistry, subject to chemical laws and principles, such as the Octet Rule. This diversity surpasses that seen in the dependencies between words in sentences, given the comparatively fewer constraints. For further reference, please see:\n\nChemistry LibreTexts. (n.d.). Combining Atoms to Make Molecules and Compounds. In Green Chemistry and the Ten Commandments of Sustainability (Manahan). Retrieved from https://chem.libretexts.org/Bookshelves/Environmental_Chemistry/Green_Chemistry_and_the_Ten_Commandments_of_Sustainability_(Manahan)/02%3A_The_Key_Role_of_Chemistry_and_Making_Chemistry_Green/2.12%3A_Combining_Atoms_to_Make_Molecules_and_Compounds\n\n**On the Concern of Weak Experiment Results**\n\nAdditional experiments were conducted to **evaluate REMO_IM on the MoleculeACE benchmark **(Table 1). Our results demonstrate state-of-the-art performance, **surpassing both ECFP+SVM and other deep-learning models. The average RMSE and RMSE_Cliff are 0.647(\u00b10.003) and 0.747(\u00b10.004), respectively**. The full result is available in https://anonymous.4open.science/r/ICLR2024_5944-55E3/remoIM_moleculeace.csv\n\n**On the concern of lacking import baselines**\n\nWe conducted a comparative analysis with MolR[1] using MoleculeNet for evaluation. Our findings indicate significant disparities between the claimed results of MolR and our own. After investigating MolR's methodology, we identified two key differences: their use of **random data partitioning** and a focus on single subtask prediction for multi-task datasets(**Please be aware that the choice of split method, whether random or strictly scaffold-based, significantly influences the performance on MoleculeNet.**). To address these differences, we concentrated on single-task datasets (HIV and BBBP) using the scaffold splitter method. Our re-evaluated results confirm REMO's superiority over MolR.\n\n| Dataset | BBBP | HIV |\n| --- | --- | --- |\n| MolR (random split, paper claimed) | 89.0 | 80.2 |\n| MolR (scaffold split) | 61.65 | 68.57 |\n| REMO (scaffold split) | 73.4 | 77.3 |\n\n\n**On More Baselines in the Benchmark ACNet**\n\nThank you for your suggestion. We have added Mole-BERT to our set of baseline models for ACNet. Below is a detailed breakdown of Mole-BERT's performance across the 13 tasks within the ACNet Few Subset, using three different random seeds.\n\n| Models                            | AUC             | Models                           | AUC             |\n| --------------------------------- | --------------- | -------------------------------- | --------------- |\n| GROVER                        | 0.753(0.010)    | ChemBERT                      | 0.656(0.029)    |\n| MAT                            | 0.730(0.069)    | Pretrain8                     | 0.782(0.031)    |\n| pretrainGNNs                   | 0.758(0.015)    | S.T.                          | 0.822(0.022)|\n| GraphLoG                       | 0.752(0.040)    | GraphMVP                      | 0.724(0.026)    |\n| ECFP                              | 0.813(0.024)    | *REMO*                           | **0.828(0.020)**|\n| Mole-BERT| 0.758(0.014)|\n\n**On More Baselines for the Benchmark Drug-Drug Interactions (DDI)**\n\nUpon applying GraphLoG and Mol-BERT to the DDI task, it became evident that REMO outperforms these methods. This superiority can be attributed to REMO's proficiency in capturing reaction information, as it learns from reaction data as opposed to single molecules. The results align with those observed in MolNet.\n\n| Models    | f1-score    | precision    | recall  | accuracy  |\n| ----------| ------------| -------------| ------- | --------- |\n| GraphLoG  | 0.846       | 0.879        | 0.846   | 0.908     |\n| Mol-BERT  | 0.863       | 0.885        | 0.861   | 0.922     |\n| REMO_IM | 0.953 | 0.932 | 0.932  | 0.928 |\n\n[1] CHEMICAL-REACTION-AWARE MOLECULE REPRESENTATION LEARNING, ICLR 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614559401,
                "cdate": 1700614559401,
                "tmdate": 1700734239434,
                "mdate": 1700734239434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hR8XkrK7eU",
                "forum": "aqTipMg9CZ",
                "replyto": "r2yQWHCve5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer H5fY 2/2"
                    },
                    "comment": {
                        "value": "**On the New Benchmark Reaction Type Classification**\n\nWe conducted a comprehensive evaluation of REMO_IM on the Reaction Type Classification benchmark. The fine-tuning pipeline we employed aligns with the methodology used in MolR. Specifically, REMO_IM generates graph-level representations for both the reactants and products graphs, which are then concatenated to serve as input for a two-layer downstream classifier. The dataset utilized, USPTO-1k-TPL, comprises 400,000 reactions for training and 45,000 for testing, with 5% of the training set reserved for validation purposes. The results of this evaluation are as follows:\n\n| Method    | Accuracy          |\n|-----------|-------------------|\n| MolR-TAG  | 0.962     |\n| **REMO-IM** | **0.980** |\n\n\n\n\nThe superior performance of REMO_IM, as evidenced by these results,  underscores its effectiveness compared MolR"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614654837,
                "cdate": 1700614654837,
                "tmdate": 1700618141332,
                "mdate": 1700618141332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "610itT7hyJ",
                "forum": "aqTipMg9CZ",
                "replyto": "r2yQWHCve5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**On the Concern of code**\n\nWe have restructured our code and shared it in an anonymous Git repository. Additionally, a comprehensive README file has been included, encompassing both pretraining and fine-tuning scripts, along with data processing details and the hyperparameters used during training. You can access the code repository through the following link: https://anonymous.4open.science/r/REMO-8E45/README.md.\n\nFurthermore, we have made available a pretrained model, accessible through this link: https://drive.google.com/file/d/1t62Xo5Akco9Z04El_wtbcNR_9HN0wnnf/view?usp=sharing.\n\nShould you have any questions regarding reproducibility or require further elaboration on specific details, please don't hesitate to reach out. We welcome inquiries and are ready to provide additional information as necessary."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642474509,
                "cdate": 1700642474509,
                "tmdate": 1700642493915,
                "mdate": 1700642493915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lV6e5FLzQc",
                "forum": "aqTipMg9CZ",
                "replyto": "r2yQWHCve5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe express our sincere gratitude for the invaluable suggestions you provided to enhance our manuscript. We kindly request your confirmation regarding any lingering issues that may require additional attention to meet your expectations and potentially elevate the overall assessment. Your time and feedback are highly valued, and we eagerly anticipate your response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663212085,
                "cdate": 1700663212085,
                "tmdate": 1700663212085,
                "mdate": 1700663212085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M679xlpk4Q",
            "forum": "aqTipMg9CZ",
            "replyto": "aqTipMg9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_gLxF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_gLxF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new self-supervised method for learning molecule representation. They propose to use masked reaction center reconstruction instead of conventional masked sub-unit reconstruction. The argument for that is in this way, the model can exploit underlying shared patterns in chemical reactions as context and can infer meaningful representations of common chemistry knowledge.\n\nThe main motivation for the paper was that  the traditional masked reconstruction loss is not enough for molecules due to:\n1)  the \"activity cliff\" property of molecules, where a single molecular change could lead to a big difference in the molecule property, the standard self-supervised learning techniques fall short when applied to molecules.\n2) BERT-like masked reconstruction loss omits the high complexity of atom combinations within molecules in nature that are quite unique compared to a simple sentence comprised of a few words.\n\nTherefore, instead of sub-units,  they proposed to reconstruct the reaction center from the given reactants as context. \nThis is due to the fact that molecule biochemical or physicochemical properties are determined and demonstrated by its reaction relations to other substances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation/intention of the method is very clear and well-justified.\n2. The paper is well-written and easy to follow.\n3. The results looks good"
                },
                "weaknesses": {
                    "value": "The paper overall is easy to follow and easy to understand, but in terms of the baselines and experimental set up some parts could be improved. For details please reach the questions section. It would be nice if the authors add a discussion around the limitation of the proposed method."
                },
                "questions": {
                    "value": "1. The graph formed part of the explanation is a bit confusing, would it be possible to make it more clear so one does not need to go back to the original paper to understand? \n\n2. In table 1, I was wondering why REMO_IM model is not presented but only REMO_I and REMO_M? This also extended to Table 2, here the REMO represents the model trained to do the reaction center reconstruction task, the identification task, or both.\n\n3. From Table 3, it seems often the model trained for reaction center reconstruction does not have as good performance as the one trained to predict the reaction center, any insights on this?\n\n4. Regarding the baselines for the drug-drug interaction task, I was wondering what happens if one uses simply the graph formed without self-supervised learning, what would be the result? I think one main baseline missing from the paper here is, what happens if we do not use self-supervised learning, but directly use the proposed graph network structures to do the task, would the result be a lot worse than having the self-supervised learning setup? \n\n5. Regarding the reaction center identification task, as the output is softmax over all the atoms, you will have a vector of [p_1, p_2,,, p_N] where 0<=p_i<=1,  what happens when you have multiple reaction center, maybe the answer to this question is very obvious but I am somehow failing to see it clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698408798359,
            "cdate": 1698408798359,
            "tmdate": 1699636633349,
            "mdate": 1699636633349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PkkVzrnOWN",
                "forum": "aqTipMg9CZ",
                "replyto": "M679xlpk4Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gLxF"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback. We greatly appreciate your guidance. Based on your suggestions, we have diligently revised our paper. Addressing your concerns, we provide the following responses:\n\n>Q1\n\nOur method constructs the graph through a systematic tokenization of nodes based on their types and adjacent bond types. This approach amalgamates these elements into a comprehensive dictionary of topology tokens for the dataset. For instance, a carbon atom linked with four single bonds constitutes a specific entry in this dictionary. Ultimately, our constructed dictionary encompasses approximately 2500 unique elements. The core objective of the reconstruction process is to accurately identify the corresponding label from this extensive dictionary.\n\n>Q2\n\nAdditional experiments have been conducted to assess REMO_IM's performance on MoleculeACE, with comprehensive results available at https://anonymous.4open.science/r/ICLR2024_5944-55E3/remoIM_moleculeace.csv. We observed an average RMSE of 0.647(\u00b10.003) and RMSE_Cliff of 0.747(\u00b10.004), affirming REMO_IM's state-of-the-art (SOTA) status when benchmarked against models like ECFP+SVM and other deep-learning frameworks. To clarify any misunderstandings, Table 2 in our paper features REMO_IM, which was concurrently pre-trained on both the reaction center reconstruction and identification tasks.\n\n>Q3\n\nTable 3 highlights the distinct strengths of REMO-I and REMO-M across various tasks. REMO-I demonstrates superior performance in pharmacology-related tasks (e.g., BBBP, Tox21, Toxcast, SIDER), while REMO-M is more adept in biophysical tasks like HIV and Bace. Notably, REMO_IM, integrating both Identification and Masking pre-training, achieves the highest overall performance at 74.7, illustrating the synergistic benefits of these tasks.\n\n>Q4\n\nTo demonstrate the efficacy of our pretraining techniques, we applied REMO without pretraining to the DDI task. The results, as tabulated below, clearly show that our pretraining methods substantially improve the model's performance:\n\n| Models           | f1-score | precision | recall | accuracy |\n| ---------------- | -------- | --------- | ------ | -------- |\n| REMO wo/ pretrain | 0.684    | 0.725     | 0.680  | 0.846    |\n| REMO_IM | 0.953 | 0.932 | 0.932 | 0.928 |\n\n>Q5\n\nWe apologize if there was any confusion. To clarify, we represent the presence of a specific atom as a reaction center using the variable p_i where p_i = 1/0. This representation is employed for a binary classification task, distinguishing whether a given atom serves as a reaction center or not. In cases where a molecule has multiple reaction centers, the vector [p_1, p_2, \u2026, p_N] may contain multiple instances of the value 1.\nIt's important to note that we do not extend the classification further to identify different types of reaction centers. If, however, there are multiple types of reaction centers (for example, 2 types), we can modify p_i to take values in the set {1, 2, 0). In this modified representation, p_i will denote whether a specific atom is a reaction center (when p_i > 0) and, if it is, indicate the type to which it belongs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613572751,
                "cdate": 1700613572751,
                "tmdate": 1700613572751,
                "mdate": 1700613572751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kJEp7AwqaY",
            "forum": "aqTipMg9CZ",
            "replyto": "aqTipMg9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_rKxi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_rKxi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes two novel masking approaches to pretraining on molecular data using reaction data: \n(1) predict the reaction center's atom type and adjacent bonds, \n(2) predict whether an atom belongs to a reaction centre. \nThe experiments contain different tasks, benchmarks, and focus on comparing to other masking approaches"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I agree that reaction data is a promising source which should be considered in pre-training.\n- The proposed approaches are straightforward / relatively simple, but make sense.\n- The experiments are nice in that they also include transformers as baselines, not just GIN, and they cover different tasks."
                },
                "weaknesses": {
                    "value": "- The related work is missing all references to masking approaches in SSL beyond graphs. ICLR is a more general ML conference and graph SSL has clearly been inspired by those. \n- The writing contains many statements I do not think there is clear consensus about\n    - \"in molecule graphs the relationships between adjacent sub-units are mostly irrelevant.\" - would at least need references\n    - \"while changing one word in a long sentence might have a relatively low impact on the semantic meaning of the full sentence,\" - adding \"not\" does not \n    - \"In such cases, traditional masked reconstruction loss is far from being sufficient as a learning objective.\" - The Molformer paper shows that simple masking can recover structure quite well if enough pre-training data is available.\n    - \"most biochemical or physiological properties of a molecule are determined and demonstrated by its reaction relations to other substances\" - also needs references, esp. for ML readers\n    - \"ACNet (Zhang et al., 2023) demonstrate that existing pre-trained models are incomparable to SVM based on fingerprint on activity cliff.\" - I doubt that activity cliffs should be a goal / considered in pre-training. This is a particularly challenging fine-tuning scenario, I agree on that. However, in pre-training the goal is to learn a generally good embedding space which can be easily adapted in various fine-tuning scenarios. In fact, in unsupervised learning more generally (i.e., not transfer learning), a uniform space is considered as goal in many papers. It is not clear to me how an embedding space needs to look like so that it is particularly beneficial also in AC scenarios. If the authors consider those, a more detailed investigation might be useful.\n- Table 1: I think the baselines are too basic and thus unrealistic, the SOTA is more advanced, e.g.\n    - ECFP: might be a concatenation of ECFP+MAACs or even additional, helpful descriptors rdkit provides\n    - GAT, GCN, etc. are all models from the GNN literature. There are others, e.g., D-MPNN (chemprop), which target chemical tasks.\n- My current main concern is the experiment design, which is not fully clear to me.\nThe Table 3 comparison may be lacking. Since there are no ablation results in terms of masking, I assume the authors intended to compare this in this table as well (i.e., beyond just comparing to SOTA works in general). The baselines from related works seem to be trained on other data. However, even if these are larger datasets, they do not necessarily have to be better. In fact, USPTO contains highly diverse, special data from patents. So it is not directly clear to me that this dataset is comparable to the pre-training data the other models use. Therefore, it is not at all clear how the proposed masking actually compares to related works. It is also not clear from the table if the REMO_x models are based on GIN or Graphormer. Only if the former is the case, the comparison to most of the baselines makes sense to me, in terms of masking."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699717246,
            "cdate": 1698699717246,
            "tmdate": 1699636633233,
            "mdate": 1699636633233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XU4gWW0V6U",
                "forum": "aqTipMg9CZ",
                "replyto": "kJEp7AwqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rKxi 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback. We greatly appreciate your guidance. Based on your suggestions, we have diligently revised our paper. Addressing your concerns, we provide the following responses:\n\n**On the missing references**\n\nWe adhere the references to masking approaches for SSL here, and this would be in the paper of the final version:\n\n>masked language model (MLM) (Devlin et al., 2019; Lample and Conneau, 2019)\n\n>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL). \n\n>Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. arXiv preprint arXiv:1901.07291. \n\n**On the consensus of our motivations**\n\n\"in molecule graphs the relationships between adjacent sub-units are mostly irrelevant.\" - We are sorry for the partially accurate information in the sentence when takes out along. We aim at pointing out that a wide variety of atoms can combine to form stable molecules under the context of organic chemistry, under the constraint of chemical laws and principles (e.g. Octet Rule). Especially, the variety is wider compares to the dependencies between words in sentences, as the constraints are less. A reference of this can be found  in:\n\n>Chemistry LibreTexts. (n.d.). Combining Atoms to Make Molecules and Compounds. In Green Chemistry and the Ten Commandments of Sustainability (Manahan). Retrieved from https://chem.libretexts.org/Bookshelves/Environmental_Chemistry/Green_Chemistry_and_the_Ten_Commandments_of_Sustainability_(Manahan)/02%3A_The_Key_Role_of_Chemistry_and_Making_Chemistry_Green/2.12%3A_Combining_Atoms_to_Make_Molecules_and_Compounds\n\n**\"In such cases, traditional masked reconstruction loss is far from being sufficient as a learning objective.\" - The Molformer paper shows that simple masking can recover structure quite well if enough pre-training data is available.**\n\nWe agree on the fact that many molecular encoder trained on masking reconstruction from single molecules could recover structures quite well. However, it could be credited by a lot of factors. Firstly, most small molecule datasets are biased, the atom types are overwhelmingly biased to carbon, as seen in Supplementary F. This imbalance in dataset make the reconstruction easier when randomly mask an atom type. Moreover, as stated in [1], the masked node prediction task on single molecules might be too easy, for the limited size of vocabulary, and the valence constraints make the prediction even easier. Moreover, as stated in the Introduction part of our paper, the model may not learn useful knowledge that could transfer to downstream tasks from simple masked reconstruction, even though they may perform good in pre-training tasks.\n\n**\"most biochemical or physiological properties of a molecule are determined and demonstrated by its reaction relations to other substances\" - also needs references, esp. for ML readers**\n\nTextbook such as [2] explains it well, we will include it in the final version of our paper.\n\n\n[1] Sun, R., Dai, H., & Yu, A. W. (2022). Does GNN Pretraining Help Molecular Representation? In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Retrieved from https://openreview.net/pdf?id=uytgM9N0vlR#:~:text=Different%20setups%20can%20lead%20to%20opposite%20conclusions.&text=In%20conclusion%2C%20different%20from%20the,is%20effective%20in%20molecular%20domain.\n\n[2]LibreTexts. (n.d.). 13: Introduction to Biochemistry. Chemistry LibreTexts. Retrieved November 21, 2023, from https://chem.libretexts.org/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613064351,
                "cdate": 1700613064351,
                "tmdate": 1700613064351,
                "mdate": 1700613064351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ts7UCj3NGQ",
                "forum": "aqTipMg9CZ",
                "replyto": "kJEp7AwqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rKxi 2/2"
                    },
                    "comment": {
                        "value": "**On Activity Cliff as a goal for pre-trained models.**\n\nActivity Cliff is defined as structural similar molecules may present significantly different binding affinities to targets. It is a good example to show the complexity for the property landscape of small molecules. Inferring from single molecules is challenging to predict activity cliff. Pre-trained model learns from single molecules, which learns a uniform space for molecular structures, though the structural-activity space is not uniform for the activity cliff problem, from a structural point of view, this discrepancy causes most pre-trained models could not yield optimised result, this problem is also studied in [3]. However, the intuition of our work aims at learning the chemical environment along with molecular structures, so the model would be aware of the molecules' position in chemical reaction space. Predicting activity cliff from structures is difficult, but activity cliff still obeys the rules in biochemistry. Intuitively, a model could be sensitive to activity cliff if it learns from the structural space and the chemical reaction space at the same time.\n\n**On baselines of MoleculeACE**\n\nThe result of ECFP+MACC followed by SVM is attached at: https://anonymous.4open.science/r/ICLR2024_5944-55E3/svm_moleculeace.csv\nthe average rmse and rmse_cliff is 0.658 and 0.744 respectively.\n\nThe result of D-MPNN (chemprop) is attached at: https://anonymous.4open.science/r/ICLR2024_5944-55E3/chemprop_moleculeace.csv\nthe average rmse and rmse_cliff is 0.750(0.002) and 0.829(0.004), respectively.\n\nit is worth noting that we evaluate our model REMO_IM on MoleculeACE after submission, the result is attached at: https://anonymous.4open.science/r/ICLR2024_5944-55E3/remoIM_moleculeace.csv\n**the average rmse and rmse_cliff is 0.647(0.003) and 0.747(0.004), respectively. The result reaches SOTA, compares to either ECFP+SVM, or (ECFP+MACCs)+SVM, or other deep-learning based models**.\n\n**On the difference in pretraining dataset**\n\n>However, even if these are larger datasets, they do not necessarily have to be better. In fact, USPTO contains highly diverse, special data from patents. So it is not directly clear to me that this dataset is comparable to the pre-training data the other models use.\n\nUndoubtedly, high-quality data can compensate for quantity limitations. However, the baseline models, which take only individual SMILES as input, may not fully exploit the wealth of chemical reaction information embedded in the USPTO data. To investigate this hypothesis, we disentangled chemical reactions into individual molecules, removing duplicates to create a dataset of approximately 2 million distinct SMILES, denoted as 'uspto-single.' Subsequently, we utilized this dataset to re-pretrain the GraphMAE for 100 epochs until the model's loss achieved convergence and stability.\n\nFollowing the pretraining phase, we conducted a retest using MoleculeNet and present the results in the table below.\n\n|                     | BBBP        | Tox21       | Toxcast     | Sider       | Cliontox    | MUV         | Hiv         | Bace        | Average |\n|---------------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|---------|\n| GraphMAE org + zinc | 72.0(0.006) | 75.5(0.006) | 64.1(0.003) | 60.3(0.011) | 82.3(0.012) | 76.3(0.024) | 77.2(0.010) | 83.1(0.009) | 73.8    |\n| GraphMAE + (uspto data) uspto single | 64.9(0.8)   | 75.7(0.3)   | 65.0(0.3)   | 62.5(1.4)   | 71.9(0.9)   | 75.4(2.3)   | 77.4(2.1)   | 83.1(1.3) | 72      |\n\nNotably, the performance experiences a decline when transitioning to the 'uspto-single' pretraining data. This observation suggests that the pretraining method, which is solely based on individual molecules, results in a loss of crucial chemical reaction information, leading to inferior performance. To address this limitation, there is a need to design a specialized pretraining strategy, such as REMO, to fully harness the rich information present in the USPTO data.\n\n> It is also not clear from the table if the REMO_x models are based on GIN or Graphormer.\n\nWe apologize for any confusion. Specifically, the REMO_IM-GraphMAE and REMO_IM-AttrMask configurations utilize GIN as the backbone. This choice is attributed to their continuous pretraining on the encoder provided by GraphMAE or AttrMask. In contrast, all other REMO settings, with the exception of these two, employ Graphormer as the backbone.\n\n[3] Why Deep Models Often cannot Beat Non-deep Counterparts on Molecular Property Prediction?, arxiv 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613124380,
                "cdate": 1700613124380,
                "tmdate": 1700620072152,
                "mdate": 1700620072152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6SGYpyarAC",
                "forum": "aqTipMg9CZ",
                "replyto": "kJEp7AwqaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe express our sincere gratitude for the invaluable suggestions you provided to enhance our manuscript. We kindly request your confirmation regarding any lingering issues that may require additional attention to meet your expectations and potentially elevate the overall assessment. Your time and feedback are highly valued, and we eagerly anticipate your response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663237703,
                "cdate": 1700663237703,
                "tmdate": 1700663237703,
                "mdate": 1700663237703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CuOUz6mIxX",
            "forum": "aqTipMg9CZ",
            "replyto": "aqTipMg9CZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_1YRR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5944/Reviewer_1YRR"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces REMO, a self-supervised learning framework for MRL, which leverages well-defined rules of atom combinations in chemical reactions. REMO pre-trains graphformer encoders on a large dataset of chemical reactions and proposes two pre-training objectives: masked reaction centre reconstruction and reaction centre identification. REMO supports diverse downstream molecular tasks with minimal finetuning and outperforms traditional masked modeling approaches in various experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-written and easy to follow.\n2.\tThe experiment results are comprehensive."
                },
                "weaknesses": {
                    "value": "1. My main concern revolves around the masking strategies employed and the performance of the proposed method. I find it is hard to comprehend the additional information gained from the two pre-training strategies, as they appear similar in their tasks. Besides, the inclusion of a conditional molecule as a constraint during pre-training seems necessary, yet this information is absent during the actual execution of the downstream task. This discrepancy causes a disconnect between the pre-training and downstream tasks.\n\n2. Additionally, in Table 1, the authors conclude that their approach is inferior to ECFP+SVM. It is unclear how this supports the claim of superiority for their own method. The lack of comparative advantage raises questions about the effectiveness of their approach in relation to existing methods."
                },
                "questions": {
                    "value": "1. What is the rationale behind conducting pretraining for the same task?\n2. Is there a way to evaluate the significance of conditional molecule generation?\n3. In Table 2, why is the prediction of reaction centers useful for cliff? Especially considering that ECFP performs better than most other pretraining methods, could we explore if other methods incorporating chemical reactions for pretraining also provide information gain?\n4. Why does REMO-IM outperform REMO-IM Attrmask in Table 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759530592,
            "cdate": 1698759530592,
            "tmdate": 1699636633126,
            "mdate": 1699636633126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A28FmpOt33",
                "forum": "aqTipMg9CZ",
                "replyto": "CuOUz6mIxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1YRR"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable feedback and have meticulously revised our manuscript in light of your insightful recommendations. Below, we address your concerns and queries in a detailed manner:\n\n**On the Additional Information Gain of Distinct Pre-training Strategies**\n\nWe acknowledge the similarity between the tasks of Reaction Centre Identification and Masked Reaction Centre Reconstruction, but emphasize their distinct characteristics. Reaction Centre Identification involves locating the reaction center on a molecule, whereas Masked Reaction Centre Reconstruction entails rebuilding the reaction center given its location, incorporating adjacent intra-molecular information and conditional molecules. As demonstrated in Table 3 of our paper, the REMO_I (Pretrained by Identification) and REMO_M (Pretrained by Masking) models exhibit specific advantages in different sub-tasks. Most notably, REMO_IM (Pretrained by both Identification and Masking) delivers the highest overall performance, highlighting the synergistic effect of these tasks.\n\n**On the Discrepancy Between Pre-training and Downstream Tasks**\n\nDuring pre-training, REMO does incorporate conditional molecules, despite some downstream tasks using only single molecules. REMO's training approach is designed to concurrently encode conditional and primary molecules using a singular encoder, with weight updates driven by gradients from both task branches. This method enables REMO to learn not just the atomic combinations within a molecule, but also its broader chemical context. Consequently, REMO's graph encoder, which is effective for single molecules, remains applicable to these tasks.\n\n**On REMO's Effectiveness in Addressing Activity Cliffs**\n\nWe evaluated the REMO_IM model on the MoleculeACE benchmark (as seen in Table 1), with results accessible via https://anonymous.4open.science/r/ICLR2024_5944-55E3/remoIM_moleculeace.csv. **The model's average RMSE and RMSE_cliff for the test set are 0.647(0.003) and 0.747(0.004), respectively, surpassing the ECFP+SVM method and achieving state-of-the-art performance**. While REMO-M may not universally outperform fingerprint-based models, it demonstrates improvements in 16 specific tasks (RMSE metric) and 17 tasks (RMSE_cliff metric). REMO-I, on the other hand, outperforms fingerprint-based models in 16 and 15 tasks, according to RMSE and RMSE_cliff, respectively (detailed in Supplementary C.1, Figures 5 and 6).\n\n>Q1\n\nPlease refer to our explanation above regarding the additional information gain of distinct pre-training strategies.\n\n>Q2\n\nWe are unclear about the context of this question as REMO does not propose any method for conditional molecule generation. Nevertheless, we acknowledge the value of this direction for future research and appreciate the suggestion.\n\n>Q3\n\nThe 'Activity Cliff' concept highlights the challenge of predicting binding affinity differences in structurally similar molecules. This complexity mirrors the chemical reaction space of small molecules, as depicted in Figure 1A, where similar structures can react differently. REMO's training on chemical reaction information aims to capture this complexity, potentially aiding in understanding activity cliffs in small molecules. We believe that integrating chemical reaction knowledge into pre-training could be advantageous, and exploring this connection is a key direction for future research.\n\n>Q4\n\nThe observed differences are attributable to the choice of backbone models. REMO-IM utilizes the more advanced Graphformer, while REMO-IM Attrmask employs GIN, aligning with Attrmask for a balanced comparison."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612582279,
                "cdate": 1700612582279,
                "tmdate": 1700619059738,
                "mdate": 1700619059738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o3blNSpBSS",
                "forum": "aqTipMg9CZ",
                "replyto": "CuOUz6mIxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe extend our heartfelt appreciation for the invaluable suggestions you offered to improve our manuscript. We kindly seek your confirmation on whether any outstanding issues remain that may need further attention to meet your expectations and potentially enhance the overall assessment. Your time and feedback are greatly valued, and we eagerly await your response."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663283906,
                "cdate": 1700663283906,
                "tmdate": 1700663283906,
                "mdate": 1700663283906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]