[
    {
        "title": "An empirical investigation of generalization dynamics in deep ReLU networks via nonlinear mode decomposition"
    },
    {
        "review": {
            "id": "PcQrZs185t",
            "forum": "Y8V6JdVdLw",
            "replyto": "Y8V6JdVdLw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_wkT2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_wkT2"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the singular value decompostion of the Jacobians of networks for the teacher-student senario.\nThey empirically investigate the behavior of the modes for linear and nonlinear networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The relationship between the generalization property of neural networks and spectral propseties of Jacobians of the networks has been investigated, and an interesting topic.\nThe authors show several numerical results related to this topic."
                },
                "weaknesses": {
                    "value": "The experimental setting should be more realistic.\nIn my understanding, the authors constructed a network with squared weight matrices, whose sizes are 12 by 12.\nThe number of training samples is only 256.\nThey also considered a CNN and MNIST.\nIn practical situations, we often come across larger networks and a larger number of training samples.\nIf the paper is based on empirical results, the experiments should be conducted in more pratical and various settings, e.g., AlexNet and VGGNet."
                },
                "questions": {
                    "value": "The authors show the reslts of ReLU networks. Does the empircal results for other activation functions differ from those of ReLU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Reviewer_wkT2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698440903246,
            "cdate": 1698440903246,
            "tmdate": 1699637058321,
            "mdate": 1699637058321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mtY7MM2bzC",
            "forum": "Y8V6JdVdLw",
            "replyto": "Y8V6JdVdLw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_LBAp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_LBAp"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies zero-bias deep ReLU networks with a teacher-student framework. The paper has provide a series of expriments, most synthetic data and some MNIST dataset. The key technique used in this paper is the singular value/mode decomposition (SVD) of the Jacobian computed at every input point for both teacher and student networks. Authors explore the evolution over training of the singular values and vectors, averaged over all inputs. They show over the course of training increasing student singular value magnitudes and increasing alignments of student singular vectors with teacher singular vectors, as observed in deep linear networks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "There are a series of experiments to support the claim."
                },
                "weaknesses": {
                    "value": "1 writing is poor.\n2 condition is too restrictive.\n3 related works are not surveyed enough.\ndetailed is in Questions."
                },
                "questions": {
                    "value": "1 the writing are extremely poor. Many concepts are not clearly defined or explained. For examples, a) how to plot loss by mode? b) at the end of page 4, Fig. 1E should be a typo. c) In Fig. 2, there are two sets of curves with the same color, what is the meaning of each curve? d) at the end of page 7, what is the linear and nonlinear components? e) Fig. 5C is not clear explained.\n\n2 why the loss in the Fig. 1B of different modes are larger than the total loss?\n\n3 There are a series of works analyzing spectral of neural network, however, authors seems now aware of such works, such as \nTraining behavior of deep neural network in frequency domain.\nOn the spectral bias of deep neural networks.\nThe convergence rate of neural networks for learned functions of different frequencies.\nA fine-grained spectral perspective on neural networks.\nOn the exact computation of linear frequency principle dynamics and its generalization.\nIt is really hard to see a novel contribution of this paper.\n\n4 zero-bias a very restrictive condition."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565251292,
            "cdate": 1698565251292,
            "tmdate": 1699637058213,
            "mdate": 1699637058213,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "vTqYzybPBr",
            "forum": "Y8V6JdVdLw",
            "replyto": "Y8V6JdVdLw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_dQaM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_dQaM"
            ],
            "content": {
                "summary": {
                    "value": "The authors' empirically study the behavior of deep ReLU networks, extending work that focused on deep linear networks. The authors use a mode decomposition technique to study how different modes evolve with training time, how they are coupled, and how this affects the loss behavior of the networks under study. Comparing their results to deep linear networks, they seem similarities (increasing modes increase their strength), but the shape the evolution of the strength of the modes takes, as well as the coupling between modes, is different than deep linear networks. The authors test a prediction made previously in work done on deep linear networks, namely that noise modes and signal modes would couple. Finally, the authors provide evidence that their analysis can hold for networks trained on more standard data sets (e.g., MNIST)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper describes how linear deep neural networks have been deeply studied, but this work has largely been limited in its application to nonlinear deep neural networks. This motivates the problem nicely. \n\n2. The authors do a good job (for the most part) comparing their results to the case of deep linear networks. \n\n3. The authors perform the same analysis on a nonlinear residual task, which I thought was clever. \n\n4. The paper is (for the most part) well written."
                },
                "weaknesses": {
                    "value": "Despite the strengths, there are several major weaknesses of the paper. I have decided to award a score of 6 to the authors, but this is viewed as a temporary score: if the points below are addressed I would be inclined to raise my score; if they are not I will decrease my score. \n\n1. The decomposition of the ReLU network, Eq. 4, which is the key to all the analysis that follows, has no proof or citations. Perhaps it's an obvious conclusion, but it was not entirely clear to me why this would always be true. Discussion on this, or maybe a brief proof in an Appendix, would start the paper off on a stronger footing. \n\n2. Figures 1, 2 and 4 have a number of issues: \n    i. It is quoted in the main text that dashed lines are used to denote noise modes but, aside from Fig. 2D, there are no dashed lines (Fig. \n       2).\n    ii. There are more bolded lines in the plots than there are in the legend (Fig. 2 and 4). \n    iii. What \"Nonlinear loss\" refers to in Fig. 4A, where the network is linear, is not clear. \n    iv. The y-axis of Fig. 1C, D, G, H should read strength not loss correct?\nThese issues make it difficult to interpret the results.\n\n3. I find the increase in alignment of, what I believe are the noise modes - although again, the plots make this challenging to be sure, with index in Fig. 2E, F very interesting. However, this is not commented upon at all in the main text. Discussion is needed on this point. \n\n4. I thought I understood what normalized modes meant (Fig. 1D, H), but the fact that several of the modes in Fig. 1H (the nonlinear network) do not end at 1 makes me think I misunderstood. What exactly is meant by normalized should be discussed in more detail. \n\n5. It is said that \"The teacher network has two linear layers and a ReLU with a leak ...\" (end of page 4). Does this mean a single ReLU at the end of two linear layers? If so, isn't this different than what is considered in Eq. 3?"
                },
                "questions": {
                    "value": "All my questions are stated in detail in the section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I found no ethics concerns in this work."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Reviewer_dQaM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623598649,
            "cdate": 1698623598649,
            "tmdate": 1699637058094,
            "mdate": 1699637058094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "CV58VPVf1K",
            "forum": "Y8V6JdVdLw",
            "replyto": "Y8V6JdVdLw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_CB8g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8473/Reviewer_CB8g"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the generalization dynamics of deep ReLU networks in a teacher-student framework. The authors use singular value decomposition (SVD) of the Jacobian to measure the globally nonlinear behavior of the network transform. They find that the singular values and vectors of the student network evolve over training, and that the network's ability to learn from the teacher network is influenced by the number of singular modes greater than zero. The authors conclude that their findings have implications for the development of more effective deep learning algorithms and models. Overall, the paper contributes to our understanding of the generalization dynamics of deep ReLU networks and provides insights into how to improve their performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors use a novel approach of applying singular value/mode decomposition (SVD) to measure the globally nonlinear behavior of the network transform in deep ReLU networks in a teacher-student framework. This is a unique and original approach that has not been extensively explored in previous research.\n\nThe authors use singular value decomposition (SVD) to analyze the evolution over training of the singular values and vectors of the Jacobian of the network transform with respect to every input point. They show that for a deep ReLU student network trained on data from a deep ReLU teacher network with a user-specified singular value spectrum, there is an increase in student singular value magnitudes and increasing alignments of student singular vectors with teacher singular vectors, as observed in deep linear networks. \n\nThe authors also decompose the loss over training by singular mode and directly observe nonlinear coupling of noise to student signal singular modes as well as coupling due to competition between signal modes.\n\nThis paper provides insights into how to improve the performance of deep ReLU networks and represent a valuable contribution to the field of deep learning."
                },
                "weaknesses": {
                    "value": "Insufficient experiments: expanding the experimental evaluations to encompass more diverse datasets and network architectures would better demonstrate the generalizability of the phenomenon. The current experiments focus heavily on MNIST with deep ReLU networks. Evaluating the effects across more tasks and models could further substantiate the claims. \n\nLack of clarity: there are some sections that could benefit from more detailed explanations. For example, the authors could provide more information on the methodology used to compute the exact local transforms for each exemplar in the train and validation sets. Additionally, the authors could provide more information on the decomposition of the loss over training by singular mode, including how this was computed and what insights it provides.\n\nPresentation: There are also points worth improving in the writing of the article. For example, the conclusion of the article can be appropriately added to the introduction to help readers read the article; and the conclusions in the Results section of the article can also be described in a better way to show the relationship between each subsection."
                },
                "questions": {
                    "value": "In the experiments, you used the MNIST dataset for image classification. Have you considered using other datasets to test the generalization dynamics of deep ReLU networks, such as CIFAR-10 or ImageNet?\n\nIn the paper, you mention that the teacher network was trained on the entire MNIST dataset, while the student network was trained on a subset of the dataset. How was the subset chosen, and what was the size of the subset relative to the entire dataset?\n\nThe paper discusses the behavior of deep ReLU networks in a teacher-student framework. Have you considered comparing the results to those obtained from other types of networks, such as convolutional neural networks or recurrent neural networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8473/Reviewer_CB8g"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8473/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651042218,
            "cdate": 1698651042218,
            "tmdate": 1699637057991,
            "mdate": 1699637057991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]