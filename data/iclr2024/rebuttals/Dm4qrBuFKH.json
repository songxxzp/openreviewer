[
    {
        "title": "Training Binary Neural Networks in a Binary Weight Space"
    },
    {
        "review": {
            "id": "sE3d97Kdrh",
            "forum": "Dm4qrBuFKH",
            "replyto": "Dm4qrBuFKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_2PTe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_2PTe"
            ],
            "content": {
                "summary": {
                    "value": "The paper trains BNNs witthout real-valued weights to save memory. They define an update probability for binary weights, determined by the current binary weights and real-valued gradients. The binary weights generated by the method match those obtained by SGD in the real-space training of BNNs in the expectation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a new method to train BNNs without using real-valued weights, which is novel.\n\nThe theoretical analysis of the paper is sufficient."
                },
                "weaknesses": {
                    "value": "The experimental results in Tab.3 seem extremely bad on medium-sized datasets such as CIFAR-10 and Tiny-ImageNet, for both SGD+STE method and the proposed method. This raises the question of whether it is reasonable to abandon the real-valued weights when training BNNs. All the previous and proposed methods sacrifice too much classification accuracy for training efficiency.\n\nThe author claims that this training strategy is useful for edge devices. However, people seldom train models on edge devices from scratch. Thus, it is more reasonable for the author to conduct experiments on finetuning a pre-trained BNN model with binary weights only.\n\nSince the results given in the paper are far from satisfactory, it is hard to judge the effectiveness of the proposed method.\n\nYou still have real-valued gradients during training, how could you reduce memory usage by 33? Besides, memory consumption should be listed in the table."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697686876626,
            "cdate": 1697686876626,
            "tmdate": 1699636631220,
            "mdate": 1699636631220,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DMCohBs5jv",
                "forum": "Dm4qrBuFKH",
                "replyto": "sE3d97Kdrh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 2PTe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for suggesting an experimental design to better evaluate our method. We have answered each of the reviewer's questions below.\n\n> The experimental results in Tab.3 seem extremely bad on medium-sized datasets such as CIFAR-10 and Tiny-ImageNet, for both SGD+STE method and the proposed method. This raises the question of whether it is reasonable to abandon the real-valued weights when training BNNs.\n\nThe underperformance of BNNs trained on CIFAR-10 and Tiny-ImageNet (shown in Table 3 of the submitted version) is mainly due to the limited generalization ability of the 4-layer MLP, not the proposed training algorithm. To show that our method (or STE) has a capability to achieve satisfactory performance on CIFAR-10 and the larger CIFAR-100, we conducted additional experiments using AdamBNN (MobileNetV1 architecture) and ReActNet (ResNet-18 architecture), which are detailed in the global response. In summary of the global response, our method achieved a top-1 accuracy of 91.94% on CIFAR-10 and 74.64% on CIFAR-100, demonstrating that our binary-space approach performs sufficiently on these medium-sized datasets.\n\n> The author claims that this training strategy is useful for edge devices. However, people seldom train models on edge devices from scratch. Thus, it is more reasonable for the author to conduct experiments on fine-tuning a pre-trained BNN model with binary weights only.\n\nWe thank the reviewer for suggesting a more practical experimental setting. We conducted experiments where binary models are pre-trained with ImageNet-1k on a central server and are fine-tuned on a medium-sized dataset obtained on an edge device. The detailed settings and results have been described in the global response.\n\n> Since the results given in the paper are far from satisfactory, it is hard to judge the effectiveness of the proposed method.\n\nThe underfittings were mainly due to the limited generalization performance of the 4-layer MLP, so we experimented with CNN-based BNNs with more expressive power, and have reported the results in the global response. In the experiments, our method achieved a top-1 accuracy of 91.94% on CIFAR-10 and this accuracy is close to the state-of-the-art in BNN training algorithms.\n\n> You still have real-valued gradients during training, how could you reduce memory usage by 33x Besides, memory consumption should be listed in the table.\n\nThe fact that our method can achieve a memory reduction of 33x has been described in detail in Section 4. Simply stated, the proportionality coefficients of the maximum memory usage for binary- and real-space training w.r.t. the number of layers L are (D_{hid} + 17B)D_{hid} and (33D_{hid} + 17B)D_{hid}, and the memory usage that is not proportional to L can be ignored when L is sufficiently large. Thus, if L is large and D_{hid}>>B, our binary-space training can achieve 33x memory reduction. However, as already mentioned in Section 4, this is not a realistic setting, and in training 4-layer MLPs on MNIST, the memory reduction is 2x due to the real-valued gradient bottleneck. In order not to confuse the reader, we have revised the introduction and Section 4 to clarify the 33x memory reduction is asymptotic analysis. Additionally, in response to the points raised, we have reported the maximum memory consumption in the additional experiments in the global response. In the training of AdamBNN, which is a middle-sized binary CNN, the memory reduction was >3.6x.\n\nWe hope these answers and the global response have addressed all your queries. Should you have further questions, we are ready to provide additional clarifications promptly."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486355099,
                "cdate": 1700486355099,
                "tmdate": 1700550580423,
                "mdate": 1700550580423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vUlr98ywVi",
                "forum": "Dm4qrBuFKH",
                "replyto": "sE3d97Kdrh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2PTe,\n\nWith less than 24 hours remaining before the deadline, we kindly request any feedback you might have. We wish to ensure that all your concerns have been adequately addressed in our response letter.\n\nWe appreciate your dedication and effort."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661937824,
                "cdate": 1700661937824,
                "tmdate": 1700661937824,
                "mdate": 1700661937824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UFaB1WFkKZ",
                "forum": "Dm4qrBuFKH",
                "replyto": "sE3d97Kdrh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your dedicated advice"
                    },
                    "comment": {
                        "value": "We have addressed the new concerns the reviewer 832g raised (especially, the use of Adam optimizer). As the deadline is approaching, we kindly request a re-evaluation if all concerns have been resolved. We are truly grateful for the dedicated advice provided in this rebuttal process."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726320742,
                "cdate": 1700726320742,
                "tmdate": 1700727183999,
                "mdate": 1700727183999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jmRgGWqnwJ",
            "forum": "Dm4qrBuFKH",
            "replyto": "Dm4qrBuFKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes a way to train BNNS in binary space, i.e., without retaining real-valued weights, by modeling the real-weight distribution and approximating a bit-flip probability for the binary weights. The real-weight distribution is modeled by a Gaussian distribution, from which the hypermasks, the analogous to the learning rate in real-valued training, is sampled and used in the binary space update rule. The method is evaluated by training a neural network with 4 fully-connected layers on Digit, MNIST, CIFAR10, and Tiny-ImageNet datasets. The proposed method shows comparable or better error rates when compared against real-valued training using STE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The notion of binary-space training is an important future direction for BNNs in my opinion, as training is quite complex and compute-heavy for BNNs at the moment\n- The paper reads well, with clear motivations for design choices in the proposed methodology\n- Mathematical claims are well-made, with reasonable practical assumptions"
                },
                "weaknesses": {
                    "value": "- While I like the overall method of the submission, the experimental section leaves a lot of questions as it is not at the scale of most well-received computer vision or even BNN evaluations.\n- Particularly, the error rates on CIFAR10 and Tiny-ImageNet are very high, regardless of wether STE training or the proposed EMP mask training is used. Comparing such underfitted (in my opinion) models does not paint a clear picture.\n- The experimental setup is limited to fully-connected layers. While these layers are certainly well utilized, most modern DNNs and BNNs employ either convolutional or transformer layers, which questions the practical applicability of the method."
                },
                "questions": {
                    "value": "- What obstacles do the authors forsee in applying the proposed method to BNNs using convolution layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717997528,
            "cdate": 1698717997528,
            "tmdate": 1699636631116,
            "mdate": 1699636631116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DcKJ9Y2D3o",
                "forum": "Dm4qrBuFKH",
                "replyto": "jmRgGWqnwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer 832g"
                    },
                    "comment": {
                        "value": "We thank the reviewer for suggesting an experimental design to better evaluate our method. We have answered each of the reviewer's questions below.\n\n> the experimental section leaves a lot of questions as it is not at the scale of most well-received computer vision or even BNN evaluations.\n\n> Particularly, the error rates on CIFAR10 and Tiny-ImageNet are very high, regardless of whether STE training or the proposed EMP mask training is used. Comparing such underfitted (in my opinion) models does not paint a clear picture.\n\nAs the reviewer mentioned, the results for CIFAR-10 and Tiny-ImageNet in Table 3 show underfitting and this is mainly due to the limited expressive power of the 4-layer MLP. To adequately evaluate the training performance of our method (and STE) on the middle-sized datasets, we conducted additional experiments using AdamBNN (MobileNetV1 architecture) and ReActNet (ResNet-18 architecture), and have reported the detail in the global response. In summary of the global response, our method achieved a (top-1) accuracy of 91.94% on CIFAR-10 and 74.64% on CIFAR-100 (we utilized instead of Tiny-ImageNet), which were comparable to STE, demonstrating that our binary-space approach performs sufficiently on the middle-sized datasets.\n\n\n> The experimental setup is limited to fully-connected layers. While these layers are certainly well utilized, most modern DNNs and BNNs employ either convolutional or transformer layers, which questions the practical applicability of the method.\n\n> What obstacles do the authors forsee in applying the proposed method to BNNs using convolution layers?\n\nWe agree with the reviewer that many modern BNNs are CNN-based, and the training performance on these architectures is important for the realistic evaluation of training algorithms. Therefore, as described above, we evaluated the binary-space training using the popular ResNet-18 and memory-efficient MobileNetV1, and have reported the results in the global response.\nIn the original manuscript, the evaluations of the proposed method were limited to MLP, not because our method is unable to train CNN-based BNNs, but to provide a pure comparison of real- and binary-space training in the most basic structure. In fact, as the additional experimental results (Ex.2 in global response) show, our method has been experimentally demonstrated to achieve comparable performance to STE in training convolutional layers.\n\nWe hope these answers and the global response have addressed all your queries. Should you have further questions, we are ready to provide additional clarifications promptly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486289002,
                "cdate": 1700486289002,
                "tmdate": 1700488061781,
                "mdate": 1700488061781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "17lqaul1Um",
                "forum": "Dm4qrBuFKH",
                "replyto": "jmRgGWqnwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 832g,\n\nWith less than 24 hours remaining before the deadline, we kindly request any feedback you might have. We wish to ensure that all your concerns have been adequately addressed in our response letter.\n\nWe appreciate your dedication and effort."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661912199,
                "cdate": 1700661912199,
                "tmdate": 1700661912199,
                "mdate": 1700661912199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dW8Iz9QiQu",
                "forum": "Dm4qrBuFKH",
                "replyto": "DcKJ9Y2D3o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Reviewer_832g"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "While I appreciate the detailed experimental results, I still have concerns as the numbers for ReActNet (and potentially other architectures) are lower than what was reported in their respective papers. Also, I noticed the authors used SGD when training other BNNs but it is well-known that Adam outperforms SGD, particularly for BNN training, which further questions the experimental results put up by the authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698550410,
                "cdate": 1700698550410,
                "tmdate": 1700698550410,
                "mdate": 1700698550410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YfQE6J6y3J",
                "forum": "Dm4qrBuFKH",
                "replyto": "jmRgGWqnwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time for us. We are grateful for the further advices. However, it seems there is a bit of a misunderstanding regarding underfitting, so let us explain in more detail.\n\nIn Ex. 1, the lower accuracy of ReActNet are not indicative of our method's lack of training capability, but rather caused by **the limited feature extraction capability of ReActNet**.\nIn fact, the following reasons support this:\n1. In the fine-tuning experiments of Ex. 1, we used a fixed backbone to extract features from CIFAR-10/100, and this backbone employs **officially provided pre-trained weights** trained on ImageNet-1k.\n2. We conducted a further additional experiment; training the real-valued header by **Linear Probing** with ReActNet backbone, and the performance was 49.15%\u00b10.09 (on CIFAR-10). **The low accuracy of the LP indicates that the features of ReActNet themselves are inseparable**. Note that our method outperformed the LP.\n3. Our method achieved a comparable performance to baseline (SoTA) real-space training methods, including ReSTE.\n4. In AdamBNN in Ex. 2, the performance exceeds 91.9% on CIFAR-10 (which cannot be considered underfitting), indicating our method performs well with separable features (, or in full-model fine-tuning).\n\nAs already mentioned in the global response, while ReActNet's results indeed seems to be underfitting, we showed the results of ReActNet to demonstrate that **our method achieves comparable performance with other real-space training methods, regardless of the type of backbone used**. The performance in non-underfitting situations has already been demonstrated with AdamBNN, **proving sufficient training capability of our method through experiments**.\n\nAlso, regarding the 'respective paper,' no results of **CIFAR-10 fine-tuning experiments** were found in this paper. Therefore, there is **no basis to conclude that these results are lower than those reported in the paper** (in fact, as mentioned above, the performance is higher than linear probing, indicating that the underfitting is a problem with ReActNet's feature extraction)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712594877,
                "cdate": 1700712594877,
                "tmdate": 1700733384707,
                "mdate": 1700733384707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aez3W1jMv4",
                "forum": "Dm4qrBuFKH",
                "replyto": "jmRgGWqnwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Experiment using Adam Optimizer"
                    },
                    "comment": {
                        "value": ">Also, I noticed the authors used SGD when training other BNNs but it is well-known that Adam outperforms SGD, particularly for BNN training\n\nThank you for your valuable advice to improve our experiments. Following the reviewer's suggestion, we have conducted a new experiment using the Adam optimizer.\n\nWe conducted the header fine-tuning experiment in Ex.1 **using Adam** as the optimizer. The experimental setup was the same as in Ex.1, and we used AdamBNN trained on ImageNet-1k as the backbone. The results are shown in Table 3. As shown in Table 3, the performance of ReSTE improved by using Adam, but **our binary-space training still achieved the comparable (within the error range) performance to ReSTE**, the SoTA real-space training method.\n\n| Table 3               |  AdamBNN on CIFAR-10 | AdamBNN on CIFAR-100 |\n| :---                  | :---: | :---: |\n| ReSTE w/ SGD          | 85.53\u00b10.34 | 62.40\u00b10.42 |\n| EMP Mask w/ SGD       | 85.63\u00b10.16 | 62.03\u00b10.39 |\n||||\n| ReSTE w/ Adam         | 85.92\u00b10.39 | 63.36\u00b10.38 |\n| EMP Mask w/ Adam      | 85.80\u00b10.18 | 63.05\u00b10.24 |\n\nOur method can reduce the memory consumption proportional to the number of layers by immediately discarding gradients after propagating them to the layer below; however **it is also possible to combine our binary-space training with Adam**. When using Adam, there is an increase in memory consumption in exchange for performance. Note that, in Ex.2, where the full models are fine-tuned, an optimizer like Adam that retains real gradients of all layers consumes considerable memory footprint, which may not be affordable by low-end devices.\n\nThese results and discussion will be added in the appendix."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725670196,
                "cdate": 1700725670196,
                "tmdate": 1700727839594,
                "mdate": 1700727839594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z7bP9z0C8e",
                "forum": "Dm4qrBuFKH",
                "replyto": "jmRgGWqnwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your dedicated advice"
                    },
                    "comment": {
                        "value": "We have also addressed the new concerns the reviewer raised. As the deadline is approaching, we kindly request a re-evaluation if all concerns have been resolved.\nWe are truly grateful for the dedicated advice provided in this rebuttal process.\n\nIn addition, if there are any additional comments regarding the following statement made by the reviewer, please let us know. We have not been able to confirm this as a fact.\n>lower than what was reported in their respective paper"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726225866,
                "cdate": 1700726225866,
                "tmdate": 1700727219000,
                "mdate": 1700727219000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nNwFkXIe4l",
            "forum": "Dm4qrBuFKH",
            "replyto": "Dm4qrBuFKH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_Wfp6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5932/Reviewer_Wfp6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new optimizer that aims at eliminating the usage of latent floating-point weights when training a binarized neural network. It uses only binary weights, which will significantly reduce the memory footprint. The evaluation of the proposed optimizer is done on MNIST, CIFAR-10, and Tiny-ImageNet datasets using a 4-layer fully connected network. Models optimized by the new optimizer show a close accuracy to those optimized by conventional STE methods using latent weights."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The effort in the paper indeed aligns with an interesting and important domain of efficiently training a binarized network.\n\n2. The paper makes a good analogy to the SGD algorithm, which eases reading."
                },
                "weaknesses": {
                    "value": "1. The analogy from Figure 1(a) to 1(b) is more like binarizing the gradients in a real-valued space. It is not obvious to the reader that $w_t^{*}$ is the ideal target weights in the binary space since the loss landscape may have been changed a lot after binarization. It needs more discussion or justification.\n\n2. It seems that the proposed update rule cannot be applied to propagating the gradients to 1-bit activations, which means STE is still needed when training a BNN.\n\n3. The experiments need to be done using at least some popular models, such as ResNet.\n\n4. The paper does not take BOP [1] as a baseline. BOP should be a closely related work that aims at not using latent weights for training BNNs.\n    * [1] K. Helwegen, Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization, NeurIPS\u201919."
                },
                "questions": {
                    "value": "Questions are included in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827971752,
            "cdate": 1698827971752,
            "tmdate": 1699636631003,
            "mdate": 1699636631003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZVme1HrzTX",
                "forum": "Dm4qrBuFKH",
                "replyto": "nNwFkXIe4l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer Wfp6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading of our paper and constructive comments in detail. We have addressed each of the reviewer's questions below.\n\n> The analogy from Figure 1(a) to 1(b) is more like binarizing the gradients in a real-valued space.\n\nLet us confirm your question. Do you mean, \"In Figure 1 (b), it appears that w is being updated in the real-valued space\"? This confusion might have stemmed from the placement of omega in Figure 1(a) and w in Figure 1(b) at identical positions; therefore, we have updated the figure in the revised paper to alter the positioning of the weights.\n\n> It is not obvious to the reader that wt\\* is the ideal target weights in the binary space since the loss landscape may have been changed a lot after binarization. It needs more discussion or justification.\n\nThe target weight merely represents the direction, toward which a point that reduces the loss is found. Here, we use the word 'merely', since such a point may not be necessarily located in the binary coordinates and w\\* might increase the loss. We added a footnote that describes the word choice of 'target weight' so that readers won't experience the same confusion you had.\n\nIn addition, previous studies like XNOR-Net and AdamBNN have shown that reflecting changes in the loss landscape of BNNs in the optimizer is crucial for enhancing BNN training performance. Our study mainly focuses on emulating the real-space SGD for BNNs, and the loss landscape problem is not directly addressed; however, the combining these methods with our binary-space training is not difficult. For instance, by using the gradient scaling of the XNOR-Net in our binary-space trainining, we can partially alleviate the change in the loss landscape, and in fact, we used this combination in Ex. 1 in the global response.\n\n>  It seems that the proposed update rule cannot be applied to propagating the gradients to 1-bit activations, which means STE is still needed when training a BNN.\n\nWe do use STE in our method, but the real-valued gradients are computed strictly at the points in the binary coordinates. It means we do not hold real weights at all during training. \n\n- For your interest, our proposed random mask relies solely on the direction of the gradient, not its magnitude. As demonstrated in Table 4 of our original paper, this hypermask performs nearly as well as the EMP mask in larger networks. This indicates that our method is effective even when the gradient is binary. Therefore, in future work, if gradients can be computed entirely within the binary space, our random mask could enable complete binary-space training of BNNs, both in terms of weights and gradients. This is a particularly intriguing possibility for the field.\n\n>  The experiments need to be done using at least some popular models, such as ResNet.\n\nAcknowledging the reviewer's feedback, we conducted further experiments with ReActNet, which is based on the well-known ResNet-18 structure, and AdamBNN, which is based on the memory-efficient MobileNetV1 structure. The results of these additional evaluations have been detailed in the global response. Notably, in the fine-tuning experiments (Ex. 2), our method attained a top-1 accuracy of 91.94% on CIFAR-10 and 74.64% on CIFAR-100.\n\n> The paper does not take BOP [1] as a baseline. BOP should be a closely related work that aims at not using latent weights for training BNNs.\n\nWe thank the reviewer for suggesting a proper baseline. We have added BOP to the baseline methods in the additional experiment (Ex. 1) shown in the global response and the revised paper.\n\n- Just for the record, BOP by Helwegen et al. is a study that proposed to reinterpret the latent real-valued weight in the BNN as a real-valued accumulator of the gradient. In contrast, our binary-space training eliminates the real-valued accumulator because the l-th gradient can be deleted after computing the (l-1)-th gradient and there is no need to maintain gradients for all layers at the same time. Thus, our binary-space training and BOP are essentially different optimizations.\n\nWe hope these answers and the global response have addressed all your queries. Should you have further questions, we are ready to provide additional clarifications promptly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486167987,
                "cdate": 1700486167987,
                "tmdate": 1700491572796,
                "mdate": 1700491572796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tD6r1Xvfx6",
                "forum": "Dm4qrBuFKH",
                "replyto": "nNwFkXIe4l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Wfp6,\n\nWith less than 24 hours remaining before the deadline, we kindly request any feedback you might have. We wish to ensure that all your concerns have been adequately addressed in our response letter.\n\nWe appreciate your dedication and effort."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661890880,
                "cdate": 1700661890880,
                "tmdate": 1700661890880,
                "mdate": 1700661890880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZPUJQHgLgO",
                "forum": "Dm4qrBuFKH",
                "replyto": "nNwFkXIe4l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your dedicated advice"
                    },
                    "comment": {
                        "value": "We have addressed the new concerns the reviewer 832g raised (especially, the use of Adam optimizer). As the deadline is approaching, we kindly request a re-evaluation if all concerns have been resolved. We are truly grateful for the dedicated advice provided in this rebuttal process."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726389819,
                "cdate": 1700726389819,
                "tmdate": 1700727202436,
                "mdate": 1700727202436,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]