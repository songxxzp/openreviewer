[
    {
        "title": "Reliable Test-Time Adaptation via Agreement-on-the-Line"
    },
    {
        "review": {
            "id": "FiN7U7VgPq",
            "forum": "iEFMwP5wng",
            "replyto": "iEFMwP5wng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission385/Reviewer_gmmt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission385/Reviewer_gmmt"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates a practical and valuable issue of Test-Time Adaptation, that is, how to pre-estimate the TTA performance of TTAed models. The authors found that a strong correlation existed between TTAed models\u2019 OOD performance/agreement and ID performance/agreement. Based on these findings, the authors further proposed methods for the TTAed model\u2019s calibration and hyperparameter selection. The overall findings are novel but still not convincing enough, necessitating further justifications. So I currently give a borderline score and I will re-evaluate the paper after rebuttal. My detailed comments are as follows."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The studied problem is invaluable in TTA but often overlooked by the current community. As many TTA methods modify model parameters during inference and may suffer from an instability issue, in real-world applications, it is essential to pre-estimate the Adaptation Performance to determine whether TTA is needed or how to achieve the best TTA performance using only unlabeled test data. \n\nThe observed correlation between TTA performance and ID performance is novel and interesting. The resulting methods for Calibration and Hyper-parameter Selection are simple yet effective."
                },
                "weaknesses": {
                    "value": "1. The claim of \u201cOOD and ID performance/agreement correlation\u201d requires more empirical evidence. Please refer to Question 1.\n2. The proposed methods rely on full access to the ID data and work in an offline manner, which slightly weakens its application scenarios.\n3. Algorithm 1 requires to maintenance of $n$ models for TTA, which may be memory-consuming and inefficient."
                },
                "questions": {
                    "value": "1. Could the authors include more results regarding more advanced network models in Figures 1 -3, including but not limited to VisionTransformers(Tiny/Small/Base/Large and with different input resolutions) and  SwinTransformers from the Timm repository? These model architectures are quite different from the considered ones and are also trained with more advanced strategies (such as data augmentation, stochastic depth, EMA, dropout, etc.) I am wondering whether the \u201ccorrelation\u201d still holds under these scenarios. \n\n2. Are there any (or potential) solutions that can modify the proposed method to be an online version? In other words, how about the performance of Algorithm 1 work in an online setting?\n\n3. Are there any sensitivity analyses about $n$ in Algorithm 1?\n\n4. For Calibration and Hyper-parameter Selection, how many ID samples are needed? Will this ID sample number affect the performance significantly? \n\n5. How about the performance of the proposed methods under wild test settings proposed by SAR [ICLR 2023]\uff1f \n\n6. If possible, I am also curious about the ID-OOD correlation of MEMO [Memo: Test time robustness via adaptation and augmentation]  and Contrastive Learning Objectives [Contrastive Test-Time Adaptation]."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission385/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission385/Reviewer_gmmt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698397714622,
            "cdate": 1698397714622,
            "tmdate": 1699635965689,
            "mdate": 1699635965689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k92TQ2S2TU",
                "forum": "iEFMwP5wng",
                "replyto": "FiN7U7VgPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gmmt [1/2]"
                    },
                    "comment": {
                        "value": ">### Could the authors include more results regarding more advanced network models in Figures 1 -3, including but not limited to VisionTransformers(Tiny/Small/Base/Large and with different input resolutions) and SwinTransformers from the Timm repository?\n\nWe hope our responses in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=vZZSNTMlWr) clarified the reviewer's concern about the empirical evidence of the OOD and ID performance/agreement correlation.\n\n--- \n\n>### Are there any (or potential) solutions that can modify the proposed method to be an online version? In other words, how about the performance of Algorithm 1 work in an online setting? \n\nWe hope our responses in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=pAkj6nKxb2) sufficiently addressed the concern about the offline nature of the proposed methods.\n\n---\n\n>### Are there any sensitivity analyses about $n$ in Algorithm 1?\n\nWe hope our responses in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=pAkj6nKxb2) sufficiently addressed the concern about the sensitivity of our method to the number of models $n$.\n\n--- \n\n>### For Calibration and Hyper-parameter Selection, how many ID samples are needed? Will this ID sample number affect the performance significantly?\n\nWe examined the sensitivity of our method on the number of ID samples [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=3aU3nNw8Bt), and we hope this sufficiently addressed the concern."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505967598,
                "cdate": 1700505967598,
                "tmdate": 1700505967598,
                "mdate": 1700505967598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PDFhcuZPFI",
                "forum": "iEFMwP5wng",
                "replyto": "k92TQ2S2TU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Reviewer_gmmt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Reviewer_gmmt"
                ],
                "content": {
                    "title": {
                        "value": "Further questions"
                    },
                    "comment": {
                        "value": "Thanks for the authors's extensive response. Most of my concerns have been addressed but I am still unsure how you modify the method to an online version. Could you please provide more details and give a pseudo-code/algorithm?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706140012,
                "cdate": 1700706140012,
                "tmdate": 1700706140012,
                "mdate": 1700706140012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "axWQhmEYxh",
                "forum": "iEFMwP5wng",
                "replyto": "FiN7U7VgPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gmmt - Details on Online Version Method"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's feedback, and we are pleased to provide more details about the modifications made in our method for an online setup. The algorithms (https://tinyurl.com/5n72hev6) directly compare the online version (**above**) and the offline-version (**below**). Blue-colored comments highlight the newly added detail from the algorithm in the original submission.\n\nA key modification is that **we apply ALine-S/D only to the predictions of current ID and OOD batch at each iteration (Line 10 in Algorithm 1) in the online version**. In this case, $\\mathcal P_\\text{ID}$ and $\\mathcal P_\\text{OOD}$ contain predictions of $n$ models on the current test batch ($x_\\text{ID}$ and $x_\\text{OOD}$), and ALine-S/D is applied to these. \n\nOn the contrary, in the offline version, we apply ALine-S/D at the end of the algorithm (Line 11 in Algorithm 2) after saving predictions on the entire test set ($\\mathcal X_\\text{ID}$ and $\\mathcal X_\\text{OOD}$) in $\\mathcal P_\\text{ID}$ and $\\mathcal P_\\text{OOD}$, with ALine-S/D then applied to these aggregated predictions.\n\nPlease let us know if there are any additional questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710869829,
                "cdate": 1700710869829,
                "tmdate": 1700712644552,
                "mdate": 1700712644552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3d2llbwota",
            "forum": "iEFMwP5wng",
            "replyto": "iEFMwP5wng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission385/Reviewer_72ab"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission385/Reviewer_72ab"
            ],
            "content": {
                "summary": {
                    "value": "This study addresses critical challenges in Test-Time Adaptation (TTA) methods, used to bolster model robustness against distribution shifts, by leveraging unlabeled data from the test distribution. The authors identify a consistent \"agreement-on-the-line\" phenomenon in adapted models, regardless of varied hyperparameters and across diverse distribution shifts. Utilizing this insight, they enhance TTA reliability by introducing strategies for estimating Out-Of-Distribution (OOD) accuracy, recalibrating models, and tuning hyperparameters without labeled test data. Comprehensive experiments validate these approaches, demonstrating evaluation of TTA methods and improvements in both OOD accuracy and calibration error, akin to scenarios having access to ground-truth labels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Highlighting a Critical Problem: The researchers focused on the test-time adaptation (TTA) issue, showing that current methods aren't great at evaluating how well TTA methods work. This work is important because it tells us we need better ways to check if these methods are doing what we want them to do without always having to rely on labeled data.\n\nInteresting Observation about Model Behavior: They found something pretty unexpected in the models after TTA. These models tended to show a \"line agreement\" behavior more than they did before. It means that after trying to adapt the models to new situations, they started to behave in a predictable pattern, which was interesting and useful to know for future work."
                },
                "weaknesses": {
                    "value": "**Insufficient Evidence for OOD and ID Performance Correlation Claims**: The authors' assertions about model behavior correlations lack broad empirical support. The research predominantly revolves around CNNs, which is just one structure among the diverse architectures and applications in deep learning. To substantiate these findings' universality and efficacy, it's crucial to extend the studies to other network models and datasets, such as Vision Transformers (ViT) and Swin Transformers, and more complex tasks like object detection and semantic segmentation. Confirming similar observations across a wider range of contexts would instill greater confidence in the authenticity and applicability of this phenomenon.\n\n**Heavy Reliance on Full Access to In-Distribution Data**: The methods proposed in the study assume complete access to in-distribution training data, an assumption impractical in many real-world scenarios. Constraints like computational resources or data privacy issues might restrict access to comprehensive in-distribution training data, leaving practitioners with only the trained model. Under these circumstances, the proposed evaluation method may become inapplicable, losing its value. The authors need to tackle this limitation, potentially necessitating the design of novel experiments or methods that support evaluation and adaptation in these more restrictive environments.\n\n**Discrepancy Between the Offline Nature of Methods and Practical Needs**: The methods introduced in this research operate within a static, offline setting, overlooking the dynamic, continuous arrival of data in real-world applications. In real-world scenarios, models must accommodate online or incremental data streams, known as online test-time adaptation. The researchers should contemplate this online setting and investigate whether their strategies remain effective when handling continuous data flows. This exploration might require new experimental setups and adaptability tests to simulate the reception conditions of data in the real world."
                },
                "questions": {
                    "value": "Please address the concerns above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698398969915,
            "cdate": 1698398969915,
            "tmdate": 1699635965598,
            "mdate": 1699635965598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7sbyenSzJI",
                "forum": "iEFMwP5wng",
                "replyto": "3d2llbwota",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 72ab"
                    },
                    "comment": {
                        "value": ">### Insufficient Evidence for OOD and ID Performance Correlation Claims (Transformers, More complex tasks like object detection and semantic segmentation)\n\nWe additionally included Vision Transformers (ViT) and Swin Transformers as our models to observe AGL/ACL in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=vZZSNTMlWr).\n\nTo the best of our understanding, the utilization of agreement to assess the generalization of models is well-established in the existing literature [1,2,3,4,5], particularly in the context of image classification tasks. Our work aligns with this convention, concentrating on leveraging the AGL phenomena to enhance reliability of TTA in image classification. However, we agree with the reviewer\u2019s point that it is promising to extend the exploration of these phenomena to more complex tasks beyond image classification. We will make sure to augment our discussions on this aspect in the camera-ready version of our work.\n\n[1] Madani et al., \u201cCo-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms\u201d, NeurIPS\u201904 \n\n[2] Nakkiran and Bansal, \u201cDistributional generalization: A new kind of generalization\u201d, Arxiv\u201921\n\n[3] Jiang et al., \u201cAssessing generalization of SGD via disagreement\u201d, ICLR\u201922\n\n[4] Baek et al., Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift, NeurIPS\u201922\n\n[5] Lee et al., \u201cDemystifying Disagreement-on-the-Line in High Dimensions\u201d, ICML\u201923\n\n---\n\n>### Heavy Reliance on Full Access to In-Distribution Data\n\nWe hope our responses in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=3aU3nNw8Bt) clarified the reviewer's concern about the reliance on full access to ID data.\n\n--- \n\n>### Discrepancy Between the Offline Nature of Methods and Practical Needs\n\nWe hope our responses in [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=pAkj6nKxb2) sufficiently addressed the concern about the offline nature of the proposed methods."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505120700,
                "cdate": 1700505120700,
                "tmdate": 1700694385040,
                "mdate": 1700694385040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q27nI5Mwc6",
            "forum": "iEFMwP5wng",
            "replyto": "iEFMwP5wng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission385/Reviewer_a5t4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission385/Reviewer_a5t4"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to achieve reliable test-time adaptation (TTA) by tackling three bottlenecks, including performance evaluation without labeled data, miscalibration after TTA, and hyperparameter selection for TTA methods. Specifically, the authors empirically verify the strong correlation in agreement and accuracy between in-distribution (ID) data and out-of-distribution (OOD) data. Based on this phenomenon, the authors adopt the ALine-S/D method to estimate OOD accuracy based on ID data and labels. With the estimated OOD accuracy, they further calculate the optimal scaling temperature to calibrate the model prediction for a lower expected calibration error. Moreover, the authors select the hyperparameters that enable the adapted model to obtain the best performance on the ID data as the optimal TTA hyperparameters on the OOD data. However, some significant issues are required to be further addressed. My detailed comments are as follows."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors reveal and verify the agreement-on-the-line and accuracy-on-the-line phenomenon in various TTA methods and various datasets, suggesting a new idea of hyperparameter tuning for TTA methods.\n2. The authors propose a new evaluation method to estimate accuracy on out-of-distribution (OOD) data without OOD labels, which further enables a more precise model calibration via temperature scaling.\n3. Experimental results demonstrate the effectiveness of the proposed unsupervised calibration methods. For example, the proposed unsupervised calibration method is able to reduce the expected calibration error from 13.40 to 2.10 while using TENT under CIFAR100-C."
                },
                "weaknesses": {
                    "value": "1. The authors have empirically observed the occurrence of agreement-on-the-line (AGL) and accuracy-on-the-line (ACL) following test-time adaptation (TTA). To enhance the manuscript, it would be beneficial for the authors to provide a more extensive explanation and discussion regarding this observed phenomenon.\n2. The proposed temperature scaling method may not be applicable in latency-sensitive real-world applications when considering efficiency. As described in section 3.2, the optimal temperature is calculated after the network makes predictions on the full test set. Therefore, the proposed method require a significant delay for model adaptation. More discussion on efficiency is required.\n3. Although the authors conduct experiments on different TTA methods, the problem setting of this paper is different from that of TTA. As shown in Algorithm 1, the proposed performance estimation method requires in-domain data and labels, which, however, are inaccessible under the settings of TTA.\n4. Figures 1-3 are difficult to understand. For example, it is unclear what the values of the horizontal and the vertical axes in Figure 1 represent. More explanations should be provided.\n5. The authors only analyze the agreement-on-the-line phenomenon on CNN-based models. However, powerful transformer-based models should also be involved in the experiments, such as ViT[1], Swin Transformer[2], PoolFormer[3], and so on.\n6. In the Introduction, the authors claim that \"Baek et al. (2022) show that the ID and OOD agreement between classifiers shows a strong linear correlation\". However, Baek et al. (2022) only study the correlation of disagreement and error between classifiers. It would be preferable to provide a more precise description.\n7. In Table 2, the authors only provide insufficient results on several domains of the test datasets to demonstrate the effectiveness of the proposed performance estimation. It would be better if the authors could provide more experimental results under these datasets, such as all 15 corrupted datasets in ImageNet-C.\n8. In Figure 3, the authors demonstrate that the phenomenon of AGL and ACL also occurs when varying TTA hyperparameters, such as learning rate and batch size. However, from Table 1 to Table 3, the authors only study the effect of the proposed methods using different architectures or different training checkpoints. More ablation study using different hyperparameter setup is suggested.\n9. As shown in Figure 4, varying learning rate in TTT exhibits a negative correlation between ID and OOD accuracies. To establish a comprehensive study, more experiments should be conducted to verify if this phenomenon occurs in other test-time training methods, such as TTT++[4] and TTT-MAE[5].\n10. On page 7, the sentence \u201clet X the random variable\u201d should be changed to \u201clet X be the random variable\u201d.\n\n\n[1] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021.\n\n[2] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021.\n\n[3] PoolFormer: MetaFormer Is Actually What You Need for Vision, CVPR 2022.\n\n[4] TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive? NeurIPS 2021.\n\n[5] Test-Time Training with Masked Autoencoders, NeurIPS 2022."
                },
                "questions": {
                    "value": "Please refer to the Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760188120,
            "cdate": 1698760188120,
            "tmdate": 1699635965518,
            "mdate": 1699635965518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3q4xV7tjGW",
                "forum": "iEFMwP5wng",
                "replyto": "Q27nI5Mwc6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a5t4 [1/2]"
                    },
                    "comment": {
                        "value": ">### The authors have empirically observed the occurrence of agreement-on-the-line (AGL) and accuracy-on-the-line (ACL) following test-time adaptation (TTA). To enhance the manuscript, it would be beneficial for the authors to provide a more extensive explanation and discussion regarding this observed phenomenon.\n\nWe have added detailed explanations about their definitions as well as characteristics in the related work (Section 4) in our revised manuscript.\n\n---\n\n>### Figures 1-3 are difficult to understand. For example, it is unclear what the values of the horizontal and the vertical axes in Figure 1 represent. More explanations should be provided.\n\nThe figures mainly show the TTAed models\u2019 accuracy (%) tested on ID (x-axis) and OOD (y-axis), represented as dots with blue color. At the same time, these models\u2019 agreement (%) tested on ID and OOD are plotted in the same x-axis and y-axis, denoted as pink dots. In addition, we plot the blue and pink lines that represent the linear fit of each dot, corresponding to the linear correlations between ID and OOD. In Figures 1 and 2, each dot represents the accuracy/agreement of the models with different architectures, while in Figure 3, they denote that of the models adapted with different adaptation hyperparameters. We provided such detailed explanations on each caption of the figures in the revised version of our submission.\n\n--- \n\n>### In the Introduction, the authors claim that \"Baek et al. (2022) show that the ID and OOD agreement between classifiers shows a strong linear correlation\". However, Baek et al. (2022) only study the correlation of disagreement and error between classifiers. It would be preferable to provide a more precise description.\n\nWe would like to clarify that our original claim in the Introduction was that \u201cBaek et al. (2022)[1] show that the OOD agreement between classifiers shows a strong linear correlation with their ID agreement.\u201d To expound on our intention, we quote the specific sentence from Baek et al.(2022)[1] that encapsulates the core observation: \u201c Whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement.\u201d We clarified our claim in the Introduction (Section 1) of our revised submission.\n\n[1] Baek et al., Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift, NeurIPS\u201922\n\n---\n\n>### In Table 2, the authors only provide insufficient results on several domains of the test datasets to demonstrate the effectiveness of the proposed performance estimation.\n\nWe supplemented Table 2 by providing the performance prediction results across every 15 corruption of CIFAR10-C, CIFAR100-C, and ImageNet-C in Section A.6 and Table 7 of the appendix in our revised submission. The results show that our methods consistently predict the actual effectiveness of the tested TTA methods across diverse corruptions and datasets, without relying on the labels.\n\n--- \n\n>### From Table 1 to Table 3, the authors only study the effect of the proposed methods using different architectures or different training checkpoints. More ablation study using different hyperparameter setup is suggested.\n\nTo supplement the incomplete results the reviewer pointed out, we provide the results of accuracy estimation and calibration using different hyperparameter setups, which include learning rates, adapt steps, batch sizes, and checkpoints. In the Table below, we compare the estimation results (MAE (%)) of our methods (on ETA) with other baselines on ImageNet-C (averaged over every corruption). The results show that our methods with different hyperparameters achieve state-of-the-art performances across various hyperparameters and datasets (Results on CIFAR10-C/CIFAR100-C (TENT) are provided in Section A.6 and Table 8 of the appendix in our revised submission).\n\n**ImageNet-C [MAE (\\%)]**\n| Setup | ATC | DOC-feat | AC | Agreement | ALine-S | ALine-D\n| ---------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| Learning rate | 5.60 | 5.28 | 37.12 | 18.22 | 4.46 | **3.70**|\n| Adapt step | 11.22 | 9.31 | 31.51 | 32.60 | 6.01 | **5.75** |\n| Batch size | 8.19 | 6.70 | 26.18 | 14.29 | 3.45 | **2.34** |\n| Checkpoint | 2.96 | 5.77 | 12.06 | 21.28 | 3.01 | **2.70** |\n\nWe also provide the calibration results using different hyperparameters when applying TENT on CIFAR10-C and CIFAR100-C (averaged over every corruption). Similar to Table 3, our methods effectively reduce the calibration error using the adapt steps, learning rates, or batch sizes.\n\n| Setup | CIFAR10-C | CIFAR100-C |\n| ---------- | ---------- | ---------- | \n| Uncalibrated | 7.76 |13.40 |\n| Learning rate | 3.23 | 3.29 |\n| Adapt step | 3.40 | 4.56 |\n| Batch size | 3.50 | 3.67 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504441081,
                "cdate": 1700504441081,
                "tmdate": 1700504441081,
                "mdate": 1700504441081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o9eiEYbMZ5",
            "forum": "iEFMwP5wng",
            "replyto": "iEFMwP5wng",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission385/Reviewer_mLJy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission385/Reviewer_mLJy"
            ],
            "content": {
                "summary": {
                    "value": "Test-time adaptation (TTA) improves model performance in the presence of distribution shifts that happen at test-time, however, its reliability is hard to evaluate as it requires access to labels at test time. To this end, this work proposes to use the, introduced in prior work, agreement-on-the-line phenomenon in order to improve TTA. Agreement between two models, $h$ and $h\u2019$ is defined as $\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}}[\\\\mathbb{1}\\\\{h(x) = h\u2019(x)\\\\}]$ and the main intuition is that, after updating a model with TTA, there is a linear relationship between the agreement on the in-distribution (i.e., the distribution used to train the original model) and the agreement on the out-distribution, i.e., the distribution shift observed at test time. The authors then use this observation in order to estimate performance on out-of-distribution data after TTA (without needing access to test labels), perform hyperparameter optimization and improve calibration after TTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Reliability of TTA is an important topic and thus this work is a relevant and timely contribution\n- The results are convincing, so they could be useful in general for future research on TTA\n- The experiments are extensive and cover various tasks"
                },
                "weaknesses": {
                    "value": "- The novelty of this method is relatively limited; it is an application of Baek et al. (2022) to the TTA setup\n- This method requires adapting multiple models during test time at the specific distribution shift in order to compute the agreement and use it to improve TTA. This might limit practical applications where training multiple models is expensive and, furthermore, the improvements on TTA seem to be on hindsight as one first needs to train multiple models in order to understand whether TTA will yield improvement. Furthermore, given that TTA usually operates on a stream of OOD data and provides a stream of predictions, it is unclear how predictions will be made at test time; does one use the ID model, (one of) the TTA models or does one wait up until the agreement-on-the-line has been computed before making any predictions on the stream?\n- Some of the details about the method are not clear."
                },
                "questions": {
                    "value": "Overall, I believe this work is a nice contribution in the field of TTA, as it addresses important issues in TTA. Having said that, the novelty is relatively small, hence my rating. As for questions to the authors:\n- What is considered to be $h(x)$? Is it the most probable class under $h$? If so, why not define agreement in terms of the entire distribution on the output space for models, e.g., some kind of expected divergence between $h(x)$ and $h\u2019(x)$? \n- How many models were used to get the agreement / accuracy lines at the figures?\n- At algorithm 1, it seems that predictions on the ID and OOD data are done under a model that is continuously updated on OOD data, therefore, the ordering of the batches might have an effect on the agreement lines. Do the results change significantly under different random seeds?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772907974,
            "cdate": 1698772907974,
            "tmdate": 1699635965436,
            "mdate": 1699635965436,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c3VAbaFqrl",
                "forum": "iEFMwP5wng",
                "replyto": "o9eiEYbMZ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mLJy [1/2]"
                    },
                    "comment": {
                        "value": "> ### The novelty of this method is relatively limited; it is an application of Baek et al. (2022) to the TTA setup. \n\nWhile acknowledging the insights from Baek et al. (2022), we want to emphasize that our work explores the AGL phenomenon in adaptive setups. In contrast, Baek et al. (2022) primarily focused on pretrained models with their weights unchanged during test time. We specifically identify an unexpected condition where linear trends persist or even **hold stronger**, as evidenced by the extensive results across datasets, TTA baselines, and experimental setups.\n\nFurthermore, our proposed methods for accuracy estimation, calibration, and hyperparameter optimization, represent a novel contribution. These methods facilitate the reliable utilization of TTA, an aspect rarely explored or addressed by previous studies due to a lack of access to OOD labels. \n\n---\n\n> ### Given that TTA usually operates on a stream of OOD data and provides a stream of predictions, it is unclear how predictions will be made at test time.\n\nWe elaborate on the detailed procedure for making predictions during TTA in our method. TTA operates as it usually does: it updates model parameters online with a stream of OOD data. At each iteration, after parameter updates, we perform inference on batches of OOD and ID data, storing their predictions. After running these procedures on multiple models, we calculate their accuracy and agreement using predictions. Subsequently, we utilize ALine-S/D to estimate OOD accuracy based on the AGL/ACL. We have provided the detailed explanation on how we obtain models\u2019 predictions in the ID and OOD during TTA in Section 3.1 of the revised submission.\n\nMoreover, as demonstrated [here](https://openreview.net/forum?id=iEFMwP5wng&noteId=pAkj6nKxb2), our method is applicable in the online setup. To make predictions, for each iteration, we update models with streamed OOD batch and perform inference on both OOD and ID batches to obtain predictions. Instead of storing predictions until the entire test set, we calculate accuracy and agreement on the current batch using AGL/ACL and apply our methods for accuracy estimation and calibration.\n\n---\n\n>### What is considered to be h(x)? Is it the most probable class under h? If so, why not define agreement in terms of the entire distribution on the output space for models, e.g., some kind of expected divergence between h(x) and h'(x)?\n\nAs noted by the reviewer, h(x) denotes the most probable class predicted by the model h given data x. We clarified the definition of h(x) in Section 2.1 of the revised version of our submission.\n\nWe want to clarify that in our work, we employed the definition of agreement by following prior studies that have established the concept of (dis)agreement between classifiers to assess their generalizations in both in-domain contexts [1,2,3] and OOD scenarios [4]. These studies empirically showcased the superiority of assessing generalizations of the models via (dis)agreement between models trained in different setups. A recent paper [5] attempted to demystify the effectiveness of employing such (dis)agreement between models for predicting OOD performances within a theoretical framework.\n\n[1] Madani et al., \u201cCo-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms\u201d, NIPS\u201904\n\n[2] Nakkiran and Bansal, \u201cDistributional generalization: A new kind of generalization\u201d, Arxiv\u201921\n\n[3] Jiang et al., \u201cAssessing generalization of SGD via disagreement\u201d, ICLR\u201922\n\n[4] Baek et al., Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift, NeurIPS\u201922\n\n[5] Lee et al., \u201cDemystifying Disagreement-on-the-Line in High Dimensions\u201d, ICML\u201923\n\n---\n\n>### How many models were used to get the agreement / accuracy lines at the figures?\n\nHere\u2019s the detailed setups for the figures:\n* Figures 1,2: a total of 11 and 9 models of different architectures for CIFAR10-C, and ImageNet-C, respectively. \n* Figure 3 first row: a total of 10 models of different learning rates, five of adapt steps, 9 of batch sizes, and 10 of checkpoints in CIFAR10-C\n* Figure 3 second row: a total of 7 models of different learning rates, five of adapt steps, 8 of batch sizes and 10 of checkpoints in ImageNet-C"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502929902,
                "cdate": 1700502929902,
                "tmdate": 1700502929902,
                "mdate": 1700502929902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RUeNVbQNYj",
                "forum": "iEFMwP5wng",
                "replyto": "uiTvVqkfbU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission385/Reviewer_mLJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission385/Reviewer_mLJy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the response from the authors which clarified my questions. I will maintain my score for now and will update appropriately after the discussion with the other reviewers."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676171729,
                "cdate": 1700676171729,
                "tmdate": 1700676171729,
                "mdate": 1700676171729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]