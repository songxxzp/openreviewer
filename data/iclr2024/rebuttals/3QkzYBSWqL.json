[
    {
        "title": "Universal Backdoor Attacks"
    },
    {
        "review": {
            "id": "Y0sigSVVqG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_EfVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_EfVD"
            ],
            "forum": "3QkzYBSWqL",
            "replyto": "3QkzYBSWqL",
            "content": {
                "summary": {
                    "value": "The paper presents a new approach for crafting universal backdoor attacks, i.e. backdoor attacks that target several classes at inference time, as opposed to traditional backdoor attacks that target a single class. In order to mount a universal backdoor attack, the adversary crafts triggers that increase the ASR on several classes simultaneously.  To that end, the authors leverage a pretrained model to extract the feature representation of the training samples, and then craft triggers that correlate with features used by samples from several classes. \n\nThe authors evaluate their attack on several subsets of ImageNet-21k, and against BadNet's baseline presented in Guo et al. By poisoning 0.39% of the training data, the authors are able to mount an effective backdoor attack when no defense is applied. The authors then test the effectiveness of their attack when several defenses are applied, and notice a drop in ASR although the attack remains effective. \n\nFinally, in order to test how much triggers applied to a single class help triggers applied to other classes, the authors fix the number of triggers in some classes, then vary the number of triggers in other classes, and observe the ASR over the fixed classes increases as more poisoned samples are added to other classes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper presents an interesting approach to backdoor attacks where triggers affect several classes simultaneously\n- the authors validate the effectiveness of their attack on a large scale dataset, and against several defenses"
                },
                "weaknesses": {
                    "value": "- the required number of poisoned samples seems a bit high, even for imagnet. other papers have shown that around 300-500 samples are enough to mount an effective backdoor attack [1, 2]. this is in contrast with the results observed in Table 1, where the baseline attack is not successful even with 2k poisoned samples.\n- the authors only consider a single baseline model against which their attack is compared. this comparison is helpful, however, given the large number of poisoned samples required, it would be nice to see how other baselines would compare at that scale\n- the parameters of the defenses were tuned for a simple baseline (BadNets). the effectiveness of the attack might be very different if the parameters of the defense were tuned to the authors' attack\n\n[1] POISONING AND BACKDOORING CONTRASTIVE LEARNING, Carlini et al., 2022\n[2] WITCHES\u2019 BREW: INDUSTRIAL SCALE DATA POISONING VIA GRADIENT MATCHING, Geiping et al., 2021"
                },
                "questions": {
                    "value": "- can you please look into a setup with fewer poisoned samples? it should be possible to have a successful backdoor attack with close to 500 samples on ImageNet\n- can you also tune the parameters of the defense against each attack you are considering?\n-  if possible, can you provide a good baseline for attacks to compare against?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697396508011,
            "cdate": 1697396508011,
            "tmdate": 1699636225742,
            "mdate": 1699636225742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P0kaLsAB1q",
                "forum": "3QkzYBSWqL",
                "replyto": "Y0sigSVVqG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work and valuable feedback. Please find our responses below. \n\n> the required number of poisoned samples seems a bit high, even for imagnet. other papers have shown that around 300-500 samples are enough to mount an effective backdoor attack [1, 2]. this is in contrast with the results observed in Table 1, where the baseline attack is not successful even with 2k poisoned samples.\ncan you please look into a setup with fewer poisoned samples? it should be possible to have a successful backdoor attack with close to 500 samples on ImageNet\n\nWe thank the reviewer for spotting this detail. It is difficult to compare across papers because the training parameters (learning rate, model architecture, and number of epochs) differ across papers. Our setup is described on p5 in Section 4.1 - Model Training. Geiping et al. poison 0.1% of ImageNet-1K to target a single class, whereas we poison 0.15% on the same dataset to target all classes. They attack a ResNet-34 model, whereas we attack a slightly smaller ResNet-18 model. Carlini et al. propose attacks against multimodal, contrastive models on different datasets with different training objectives. Their targeted backdoor attacks are substantially more effective than any attacks against models trained in a supervised manner. Carlini et al.\u2019s attack requires poisoning only 0.0001% of samples, which is a 100 times reduction compared to Geiping et al.\u2019s attack (and ours).  We do not believe these results are comparable since the experimental setup is too different. \n\nWe are happy to include this discussion in the revised paper. \n\n> the authors only consider a single baseline model against which their attack is compared. this comparison is helpful, however, given the large number of poisoned samples required, it would be nice to see how other baselines would compare at that scale\nif possible, can you provide a good baseline for attacks to compare against?\n\nWe only consider a single baseline we created because we are the first to propose a universal backdoor attack and there are no baselines in the literature. Tables 1, 2, and 3 show the improvement of our work against our created baseline under many different settings, and Figure 3  shows the difference between both approaches during training. We believe that our baselines already provide a comprehensive overview, but we are happy to compare our work to more natural baselines if the reviewer has a suggestion. \n\n> the parameters of the defenses were tuned for a simple baseline (BadNets). the effectiveness of the attack might be very different if the parameters of the defense were tuned to the authors' attack\ncan you also tune the parameters of the defense against each attack you are considering?\n\nAs stated in the paper, we used the same procedure as Lukas et al. [A] to tune the parameters of a defense. Lukas et al. assume the defender has access to one backdoor attack (BadNets [B]) against which they tune the parameters of their defense. Then, the defender tests the defense against many _unknown_ attacks.\n\nWe replicate this setup in the paper, where the defender does not know of the existence of our attack but instead tunes against one known BadNets attack. On further analysis, we have optimized our defense hyperparameters for fine-tuning and pruning defenses over the ranges described in [A] and found no significant improvement in any of the defense\u2019s effectiveness when optimizing its hyperparameters against our attacks. Our approach follows Lukas et al., and our paper shows conditions under which our backdoor lacks robustness (see Figure 5). \n\nWe will revise the paper to emphasize these points. \n\n------\n\n[A] Lukas, Nils, and Florian Kerschbaum. \"Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification.\" arXiv preprint arXiv:2305.09671 (2023).\n\n[B] Gu, Tianyu, et al. \"Badnets: Evaluating backdooring attacks on deep neural networks.\" IEEE Access 7 (2019): 47230-47244."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978808828,
                "cdate": 1699978808828,
                "tmdate": 1699978808828,
                "mdate": 1699978808828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CKpdkHhLZQ",
                "forum": "3QkzYBSWqL",
                "replyto": "P0kaLsAB1q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Reviewer_EfVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Reviewer_EfVD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification.\n\n**Number of poisoned samples.** \n\nI agree with the reviewers that the setups are different, however, an investigation into a regime with more limited number of poisoned samples reflects a more realistic setup. \n\n**Single Baseline.**\n\nI apologize for the confusion. I understand that this work is the first (to the best of my knowledge) that looks into this problem, and I appreciate the research direction it opens. What I meant by more baselines is more types of attacks, not just trigger based. In particular, your method for selecting samples to poison should be agnostic to the attack type, so you could image running BadNets with your selected samples, or some other hidden-trigger attacks with your samples, etc. Such an analysis would provide more clarity about the portability of your selection mechanism.  \n\nMy impression of the paper remains positive, and I hope the authors could address the above concerns to further improve their results."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682916067,
                "cdate": 1700682916067,
                "tmdate": 1700682916067,
                "mdate": 1700682916067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RnqJwNElWt",
            "forum": "3QkzYBSWqL",
            "replyto": "3QkzYBSWqL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_Xx2A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_Xx2A"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a universal backdoor attack, a data poisoning method that targets arbitrary categories. Specifically, the authors crafted triggers by utilizing the principal components of LDA in the latent space of a surrogate classifier. Experiments showed that the generated triggers can attack any category by poisoning a certain percentage of samples in the training data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "$\\bullet$ The authors proposed a method that was designed to poison any class, instead of targeting a single class.\n\n$\\bullet$ The proposed attack is effective than the previous method, especially when the poisoning rate is low."
                },
                "weaknesses": {
                    "value": "$\\bullet$ It is not clear why the proposed method improves the inter-class poison transferability and, in particular, how it ensures that an increase in attack success against one class improves attack success against other classes. Does the proposed method increase the transferability (attack success rate) of any two classes, even if these two classes differ significantly in the latent space?\n\n$\\bullet$ The formula in Section 3.2 needs to be formulated more appropriately and clearly. Specifically, do y' and y in the formula refer to any two categories or any two similar categories? If they refer to any two categories, please explain why categories that are very different in the latent space can also improve the success rate of the attack; otherwise, if they refer to any two similar categories, please give a clear definition of similarity.\n\n$\\bullet$ The experimental results require further discussion and analysis. For example, in Table 1, the proposed method significantly outperforms the baseline method when the poisoning samples are 5000 (i.e., the attack success rate is 95.5% vs. 2.1%), but the proposed method is suddenly worse than the baseline method when the poisoning samples are 8000 (95.7% vs. 100%). The potential reasons for the sudden improvement in the performance of the baseline method need to be discussed. Similarly, in Table 2, the attack success rate of the baseline method suddenly drops from 99.98% for ImageNet-2K to 0.03% for ImageNet-4K, which also needs to be discussed."
                },
                "questions": {
                    "value": "What are the requirements for the surrogate image classifier? The proposed method requires sampling in the latent space of the surrogate image classifier, not the original classifier. Is it possible to use any latent space of any surrogate classifier? For example, if there is a significant difference in the distribution of the hidden spaces between the surrogate classifier and the original classifier, will this result in a significant decrease in the attack success rate of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652030117,
            "cdate": 1698652030117,
            "tmdate": 1699636225675,
            "mdate": 1699636225675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LPhip0GBrZ",
                "forum": "3QkzYBSWqL",
                "replyto": "RnqJwNElWt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable suggestions that further improve the paper. Please find our responses to the questions below. \n\n> It is not clear why the proposed method improves the inter-class poison transferability and, in particular, how it ensures that an increase in attack success against one class improves attack success against other classes. Does the proposed method increase the transferability (attack success rate) of any two classes, even if these two classes differ significantly in the latent space?\n\nThank you for asking this clarifying question! On a high level, the main idea of our attack is to associate the trigger with principal components in the (surrogate) classifier\u2019s latent space instead of associating a trigger with a target class. An attacker creates a trigger for each target class by encoding its location in the latent space. \n\nExisting attacks, such as BadNets [A], associate a trigger with a target class by stamping the trigger on an image and labeling it with the target label. This poisoning strategy causes the model to associate the presence of a trigger with the target class. Thus, an effective attack requires injecting at least as many poisoned samples as there are classes.\nIn our attack, we associate triggers with principal components in the dimensionality-reduced latent space of the surrogate classifier, which allows for more sample-efficient attacks. We use Latent Discriminant Analysis to obtain principal components from the latent space. Then, we bit-wise encode these components with the trigger and inject poisoned samples into the training data. During inference, given any target label, the attacker can create a trigger that encodes it as long as they can binary encode it with the principal components. \n\nUpon further analysis, we tested whether our attack succeeded in poisoning all classes and found that a few classes had only a low success rate. Please see the table below for the success rate across 1000 classes from ImageNet. \n\n| Poison Samples (p) | Min (%) | Max (%) | Mean (%) | Median (%) |\n|--------------------|---------|---------|----------|------------|\n| 2000               | 0       | 100     | 81       | 98         |\n| 5000               | 0       | 100     | 95.4     | 100        |\n\nThe table shows the min, max, mean, and median values for the attack success rate against ImageNet-1K using 2000 or 5000 poisoned samples. The min value is 0%, indicating that some classes cannot be successfully attacked. Upon further investigation, we see that the number of principal components in our reduced latent space is too low and that a small number of classes (the ones our attack cannot reach) have the same encoding. Increasing the number of principal components could increase the number of reached classes but may require injecting more samples (as the model needs to learn to associate a trigger with more principal components).\n\nWe will include these results in the revised paper and clarify the motivation of inter-class poison transferability. \n\n> The formula in Section 3.2 needs to be formulated more appropriately and clearly. Specifically, do y' and y in the formula refer to any two categories or any two similar categories? If they refer to any two categories, please explain why categories that are very different in the latent space can also improve the success rate of the attack; otherwise, if they refer to any two similar categories, please give a clear definition of similarity.\n\nThank you for your feedback! This definition is too vague and can absolutely be reformulated to encompass the concept of inter-class transferability better. To do this, we refine inter-class transferability over two distinct sets of classes, that is, how improving an attack on one set of classes A indirectly improves the attack on the second set of classes B, which matches our experiments:\n\n$$ \n\\frac{1}{|\\textbf{A}|} \\sum\\_{a \\in \\textbf{A}} \\text{ASR}_{a}\\uparrow \\implies  \\frac{1}{|\\textbf{B}|} \\sum\\_{b\u00a0\\in B} \\text{ASR}\\_{b}  \\uparrow\n$$\n\n| Untargeted classes poisoned (%) | ASR on observed classes (%) |\n|---------------------------------|-----------------------------|\n| 90                              | 72.77                       |\n| 60                              | 70.45                       |\n| 30                              | 71.78                       |\n\nFor our experiments in Section 4.4, we choose A and B randomly, where A contains 90% of classes and B contains 10% of classes. We will update that section to use the new definition. What set of classes (A) is best for improving attack success against a certain set of classes B is a very interesting question. We have run an additional experiment that indicates that the choice of B does not strongly influence the magnitude of inter-class transferability. Specifically, whether 90%, 60%, or 30% of classes are used in B does not have a large impact on attack success in A (total poisoning rate held constant)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978608989,
                "cdate": 1699978608989,
                "tmdate": 1699978608989,
                "mdate": 1699978608989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1fZ60Zaof8",
            "forum": "3QkzYBSWqL",
            "replyto": "3QkzYBSWqL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_czTr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_czTr"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the utilization of a small number of poisoned samples to achieve many-to-many backdoor attacks. The authors leverage inter-class poison transferability and generate triggers with salient characteristics. The proposed method is evaluated on the ImageNet dataset, demonstrating its effectiveness. The authors provide evidence of the transferability of data poisoning across different categories."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The paper demonstrates clear logic.\n2.The topic is intriguing and warrants further exploration."
                },
                "weaknesses": {
                    "value": "1.The design motivation of the algorithm is unclear.\n2.The concealment of the patches is poor.\n3.The comparative methods are outdated."
                },
                "questions": {
                    "value": "1.\tThe related work lacks a specific conceptual description of \"many-to-many\" and an introduction to recent works in this area.\n2.\tIn Section 3.3, the encoding method used in the latent feature space is rather simplistic, where values greater than the mean are encoded as 1 and others as 0. What is the motivation behind this encoding method, and how does it contribute to improving the transferability of inter-class data poisoning?\n3.\tThe author employs a patch and blend approach to add triggers, resulting in poor concealment of the backdoor triggers. Visually, the differences between poisoned and clean samples can be distinguished. Has the author considered more covert methods for backdoor implantation, such as injecting triggers in the latent space and decoding them back to the original samples to reduce the dissimilarity between poisoned and clean samples?\n4.\tSelection of baselines. The chosen comparative methods are both from 2017. It is recommended to include comparative experiments with the latest backdoor attack methods.\n5.\tThe experimental results in the paper compare the average attack success rates across all categories. It is suggested to provide individual attack success rates for representative categories or other statistical results such as minimum, maximum, and median values.\n6.\tThe authors validated the effectiveness of the method under model-side defense measures. It is recommended to include defense methods in data-side."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Reviewer_czTr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669952059,
            "cdate": 1698669952059,
            "tmdate": 1699636225589,
            "mdate": 1699636225589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQod8o5yfC",
                "forum": "3QkzYBSWqL",
                "replyto": "1fZ60Zaof8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable suggestions for further improvement of the paper. Please find our detailed responses below. \n\n> The related work lacks a specific conceptual description of \"many-to-many\" and an introduction to recent works in this area.\n\nThank you for bringing this to our attention. We will revise the paper to include a conceptual description of \u201cmany-to-many\u201d attacks. If the reviewer would kindly point us to related work that we might have overlooked, we will be happy to include it in the revised paper. \n\n> In Section 3.3, the encoding method used in the latent feature space is rather simplistic, where values greater than the mean are encoded as 1 and others as 0. What is the motivation behind this encoding method, and how does it contribute to improving the transferability of inter-class data poisoning?\n\nWe chose a binary encoding method for two reasons. The first reason is to demonstrate the existence of an effective and sample-efficient Universal Backdoor Attack. Our attacks require only a small increase in the number of injected samples. As we state in the paper, one would na\u00efvely expect that such attacks require vastly more poisoned samples. The second reason is to have a robust encoding of the principal components in the model\u2019s latent space. It is possible that improved encoding strategies exist that we did not consider in the paper (e.g., by encoding principal components with floating point numbers), which we leave to future work. \n\nWe will revise the paper\u2019s discussion section to address this question. \n\n> A) The author employs a patch and blend approach to add triggers, resulting in poor concealment of the backdoor triggers. Visually, the differences between poisoned and clean samples can be distinguished. Has the author considered more covert methods for backdoor implantation, such as injecting triggers in the latent space and decoding them back to the original samples to reduce the dissimilarity between poisoned and clean samples?\n\n> B) The authors validated the effectiveness of the method under model-side defense measures. It is recommended to include defense methods in data-side.\n\nThank you for raising this question! Our attack is agnostic to the trigger, meaning that _any_ trigger can be used if it can encode a multi-bit payload (the encoding of the principal components). We agree that the triggers we use are visually perceptible and could be detected by data-side defenses, but then the attacker could modify their trigger to break this defense, as shown by Koh et al. [B], and we believe this cat-and-mouse game is outside of the scope of our work. \n\nThe ability to detect some triggers does not protect the defender from Universal Backdoor Attacks, as the attacker could have used another trigger that the defense does not detect. This comes at a trade-off, as shown by Frederickson et al. [A], where an attacker may need to inject more poisoned samples to achieve the same effectiveness. Unless a data-side defense exists that can detect any trigger, a defender always needs to consider our Universal Backdoor. Hence, the study of data-side defenses is an important problem but orthogonal to the contributions made in our paper. \n\nWe refer to the excellent analysis by Koh et al. [B] for further information on this topic and will revise the paper to include this discussion.\n\n> Selection of baselines. The chosen comparative methods are both from 2017. It is recommended to include comparative experiments with the latest backdoor attack methods.\n\nWe created the baselines that we compare by re-using the trigger from a known attack from Gu et al. [C]. \nSince we are the first to propose Universal Backdoor Attacks against vision classifiers, there are no other baselines to compare our attack. \nWe will revise the paper to mention the lack of existing baselines in our discussion section. \n \n\n----\n[A] Frederickson, Christopher, et al. \"Attack strength vs. detectability dilemma in adversarial machine learning.\" 2018 international joint conference on neural networks (IJCNN). IEEE, 2018.\n\n[B] Koh, Pang Wei, Jacob Steinhardt, and Percy Liang. \"Stronger data poisoning attacks break data sanitization defenses.\" Machine Learning (2022): 1-47.\n\n[C] Gu, Tianyu, et al. \"Badnets: Evaluating backdooring attacks on deep neural networks.\" IEEE Access 7 (2019): 47230-47244."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977791222,
                "cdate": 1699977791222,
                "tmdate": 1699977791222,
                "mdate": 1699977791222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hNWh5JFvFX",
            "forum": "3QkzYBSWqL",
            "replyto": "3QkzYBSWqL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_H32b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2821/Reviewer_H32b"
            ],
            "content": {
                "summary": {
                    "value": "Whereas in traditional backdoor literature attacks focus on a specific target class, the proposed work introduces a method to embed backdoors from any source class to any target class. The method proceeds in three steps: 1) finding the class-wise centroids of clean-data feature extractions (using CLIP), 2) encoding each centroid into a N-dimensional bit-string, and 3) generating triggers corresponding to each bit-string (and, hence each target class). Classes with similar features are encoded to have similar embeddings. They show that their method performs and scales well with ResNets on four ImageNet-21k subsets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The writing was clear and easy to follow\n- Their bit-string encoding approach is a novel and elegant way to share feature information between classes while generating a class-specific backdoor trigger.\n- The experiments section was well-motivated and well-explained."
                },
                "weaknesses": {
                    "value": "- In general, each experiment should be averaged over multiple seeds for statistical significance\n- A major part of the backdoor attack regime is the preservation of clean accuracy, and there is no analysis on how well the proposed method protects a model's clean accuracy. This should certainly be included in future versions of the paper.\n- The proposed triggers in Fig. 2 seem quite obvious to the human eye and may be susceptible to input-space defenses. I would like to see some analysis on the necessary intensity of these triggers and their brittleness to input-space defenses like STRIP.\n- On the defense side, the authors \"[halt] any defense that degrades the model\u2019s clean accuracy by more than 2%.\" I'm open to feedback here, but this has the potential to straw-man some defense mechanisms in scenarios where removing a backdoor is worth the cost of clean accuracy. Including some results without this limitation would be nice.\n- In addition to the above, the attack was not evaluated on data-cleaning defenses like SPECTRE, which I think would be particularly effective against this regime. I would like to see these defenses evaluated as well--and not limited to specific target classes.\n- The experiments are limited to ResNet variants. It would be nice to show generality by including one other architecture in the experiments section.\n  - Since most vision models rely on pretraining, one idea I would find particularly compelling would be to run the attack on a pretrained ViT.\n- In Section 4.4, only a single setting of observed percentage is tried. The analysis here would be stronger if more percentages were tried\n- I'm not sure about the timing here, but the authors claim that they \"are the first to study how to target every class in the data poisoning setting.\" However, while [1,2] address slightly different settings, they seem to be *at least* related and possibly published earlier.\n  - Depending on the nature of this relationship, I would like to see 1) these statements qualified, 2) a more thorough analysis of how the work is positioned in relation to similar work including but not limited to the papers mentioned.\n\n**Citations:**\n\n[1] Du et al., \"UOR: Universal Backdoor Attacks on Pre-trained Language Models.\"\n\n[2] Zhang et al., \"Universal backdoor attack on deep neural networks for malware detection.\""
                },
                "questions": {
                    "value": "There are a few questions embedded in the above weaknesses. In addition to those I'm curious about the effect of pretraining on the proposed attack. Could the attack be injected in a fine-tuning regime?\n\n**Note:** I'm happy to raise my score after the weaknesses and questions have been addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2821/Reviewer_H32b"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766034943,
            "cdate": 1698766034943,
            "tmdate": 1699636225516,
            "mdate": 1699636225516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C86NPrmHAE",
                "forum": "3QkzYBSWqL",
                "replyto": "hNWh5JFvFX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments!"
                    },
                    "comment": {
                        "value": "We appreciate that the reviewer liked our paper and gave us many valuable suggestions for further improvements. Please find our point-by-point responses below.\n\n> In general, each experiment should be averaged over multiple seeds for statistical significance\n\nWe agree with the reviewer, and the only reason we did not average over multiple random seeds is the high running time required for training an ImageNet classifier from scratch. Training a single model requires around 100 GPU hours. While we do not repeat experiments for the _same_ parameters with different seeds, our paper contains ablation studies where we repeat the experiment while varying one parameter. To further enhance reproducibility, we promise to release our source code, which can be used to reproduce all experiments shown in the paper. \n\n> A major part of the backdoor attack regime is the preservation of clean accuracy, and there is no analysis on how well the proposed method protects a model's clean accuracy. This should certainly be included in future versions of the paper.\n\nWe thank the reviewer for raising this question. We have included a table showing the clean accuracy for each model for various poisoning rates and methods below. The clean accuracy of the official HuggingFace ResNet-18 model trained on ImageNet is *69.7%*. Since we do not know if Huggingface selected the best model from multiple runs or used a single run, we are re-training a clean model from scratch with our training parameters and will report on its clean accuracy when the training has finished. Please find our table below for the poisoned model\u2019s clean accuracies. \n\n| Poison Samples (p) | Poison % | Patch (%) |      | Blend (%) |      |\n|--------------------|----------|-----------|------|-----------|------|\n|                    |          | Ours      | Baseline | Ours     | Baseline |\n| 2000               | 0.16     | 68.94%    | 68.94% | 68.51%    | 69.43% |\n| 5000               | 0.39     | 68.92%    | 68.89% | 68.77%    | 68.66% |\n| 8000               | 0.62     | 68.91%    | 69.43% | 69.78%    | 69.22% |\n\nThe table shows that all backdoor attacks studied in our paper do not substantially degrade the model\u2019s accuracy. \n\n> A) The proposed triggers in Fig. 2 seem quite obvious to the human eye and may be susceptible to input-space defenses. I would like to see some analysis on the necessary intensity of these triggers and their brittleness to input-space defenses like STRIP.\n\n> B) In addition to the above, the attack was not evaluated on data-cleaning defenses like SPECTRE, which I think would be particularly effective against this regime. I would like to see these defenses evaluated as well--and not limited to specific target classes.\n\nWe agree with the reviewer that there may exist data-cleaning defenses that remove our backdoor when using visible triggers. Our proposed Universal Backdoor Attack is agnostic to the trigger, meaning that the attacker can choose _any_ trigger that can encode multiple bits and use it to poison the model. Consequently, if one specific trigger can be detected reliably, any attack that uses it becomes detectable and ineffective. However, Universal Backdoor Attacks remain a threat because there are other triggers an attacker could have used that are not easily detectable by a defender. This cat-and-mouse game has been studied by Koh et al. [A]. However, there exists a trade-off that an attacker has to consider, which we do not study in our paper: While choosing more stealthy triggers can make it more difficult to detect and remove poisoned samples, this likely comes at the expense of injecting more poisoned samples (with less visible perturbations) to achieve comparable effectiveness (this was studied by Frederickson et al. [B]). \n\nWe will expand on this trade-off in the discussion section of our revised paper. \n\n> Halting a defense at 2% has the potential to straw-man some defense mechanisms.\n\nWe have run additional experiments to measure the trade-off between the clean accuracy (CDA) and the attack success rate (ASR) for a weight decay defense up to 3.5% model accuracy. Please see a table summarizing these results below.\n\n| CDA (%) | ASR (%) |\n|---------|---------|\n| 69.45%  | 81.37%  |\n| 68.54%  | 77.88%  |\n| 67.39%  | 72.53%  |\n| 66.62%  | 69.24%  |\n| 65.94%  | 66.40%  |\n\nThe table shows that the ASR decreases linearly with the CDA, and the Pearson correlation coefficient between the line of best fit to our CDA/ASR data is 0.9794. We used the same 2% cutoff that Lukas et al. [C] use. \n\nWe will include a scatter plot of the results in the revised paper. \n\n-------\n\n[A] Koh, Pang Wei, Jacob Steinhardt, and Percy Liang. \"Stronger data poisoning attacks break data sanitization defenses.\" Machine Learning (2022): 1-47.\n\n[B] Frederickson, Christopher, et al. \"Attack strength vs. detectability dilemma in adversarial machine learning.\" 2018 international joint conference on neural networks (IJCNN). IEEE, 2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977198450,
                "cdate": 1699977198450,
                "tmdate": 1699977198450,
                "mdate": 1699977198450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HJzfgI9juh",
                "forum": "3QkzYBSWqL",
                "replyto": "nsGN3zwXhx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2821/Reviewer_H32b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2821/Reviewer_H32b"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response here! While many of my questions are answered, I would still like to see experimentation on more advanced defenses than STRIP like SPECTRE for the final version of this paper. Overall, my impression of the paper remains positive."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723567281,
                "cdate": 1700723567281,
                "tmdate": 1700723567281,
                "mdate": 1700723567281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]