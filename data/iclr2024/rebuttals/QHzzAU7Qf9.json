[
    {
        "title": "Soft Merging of Experts with Adaptive Routing"
    },
    {
        "review": {
            "id": "IR1EhJUNDr",
            "forum": "QHzzAU7Qf9",
            "replyto": "QHzzAU7Qf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_y2t1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_y2t1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple method to avoid discrete routings in mixture-of-expert models by merging weights of experts during training and inference. Experiments over GLUE and DomainNet with T5-base and ResNet models demonstrate that the proposed approach is computationally efficient and achieves better performance than other MoE methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is simple, efficient, and achieves better performance than learned routing in presented experiments\n- The choice of baselines are extensive, ranging from ensemble methods, learned routings, and pre-defined routings.\n- I appreciate analysis of learned routing by the proposed method."
                },
                "weaknesses": {
                    "value": "- The main weakness is the small scale of the data and models studied in the paper. I believe the challenge of reducing computational cost with mixture-of-expert models is more relevant to larger models. The authors however only presented results on ResNet and T5-base (with only 200M parameters). Experiment results with larger models are appreciated.\n\n- If experiments with larger models are not feasible, I hope authors can discuss potential limitations of the study under those larger-scale scenarios. Do you expect the findings in Figure 2 change in larger scale setups? \n\n- The author's hypothesis about the shortcomings of learned routings in existing works that \"stem from the gradient estimation techniques used to train modular models that use non-differentiable discrete routing decisions\" is not supported with evidence other than the performance of final models in Figure 2. If you visualize the learned routing of these baselines (as in Figure 3), will you notice flaws in these learned routing? Do they yield degenerated routing, or routing that is uniform across all the experts?"
                },
                "questions": {
                    "value": "Following listed items in weakness, I hope authors can address:\n- What do you expect the performance looks like in larger-scale models?\n- How does the routing learned by reported baselines look like?\n\nFinally, the appendix and full text does not have to be in separate pdfs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6566/Reviewer_y2t1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728747114,
            "cdate": 1698728747114,
            "tmdate": 1700597074780,
            "mdate": 1700597074780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HVsMHR7x4x",
                "forum": "QHzzAU7Qf9",
                "replyto": "IR1EhJUNDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the reviewers' thoughtful and detailed feedback. Here are our responses to the points raised:\n\n> The main weakness is the small scale of the data and models studied in the paper. I believe the challenge of reducing computational cost with mixture-of-expert models is more relevant to larger models. The authors however only presented results on ResNet and T5-base (with only 200M parameters). Experiment results with larger models are appreciated.\n\nWhile we agree that past MoE work has primarily focused on reducing computational costs of large models, our work instead focuses on the benefits of modularity in MoEs with the performance advantages they offer. This includes exploiting similar experts for examples from similar tasks to foster positive transfer and reduce negative interference among tasks. We specifically chose multitask learning as our benchmark because the MoE framework is intuitively appropriate for this setting. It's crucial to mention that each experiment, such as those with T5-GLUE, required approximately 100 hours on a single A6000 GPU, and the ResNet-DomainNet experiments took about 11 hours. This is because these benchmarks contain a set of datasets (eight in GLUE and six in DomainNet), making them quite extensive. Given these considerations, we opted for medium-sized models for our experiments. While we recognize the value of testing larger models, the current scope and our computational resources guided our choice of model scale.\n\n> If experiments with larger models are not feasible, I hope authors can discuss potential limitations of the study under those larger-scale scenarios. Do you expect the findings in Figure 2 change in larger scale setups?\n\nOur method is designed to be agnostic to the backbone architecture or model size. We incorporate MoE blocks into the existing architecture and focus on learning the router and experts within these blocks. Consequently, we anticipate that our method should effectively scale to larger-sized models. As for scaling the number of experts, as detailed in Section 4.1 of our paper, we have found that SMEAR does scale well. In the ResNet-DomainNet setting, it successfully narrows the gap to Ensemble routing. In contrast, in the T5-GLUE setting, the results show no significant difference, which might be attributed to performance saturation. If the reviewer has any specific large-scale experiment in mind, we would be willing to run them, resources permitting.\n\n> The author's hypothesis about the shortcomings of learned routings in existing works that \"stem from the gradient estimation techniques used to train modular models that use non-differentiable discrete routing decisions\" is not supported with evidence other than the performance of final models. If you visualize the learned routing of these baselines, will you notice flaws in these learned routing? Do they yield degenerated routing, or routing that is uniform across all the experts?\n\nPast work on gradient estimators such as  [https://arxiv.org/abs/1711.00123], [https://openreview.net/forum?id=r1lgTGL5DE, https://arxiv.org/abs/2109.11817] supports our hypothesis regarding the challenges associated with learning using gradient estimator techniques. In our paper, we adopt an empirical approach to compare learned routing methods, and therefore use final model performance as the metric for evaluating routing efficacy. Additionally, we have provided a qualitative analysis of learned routing methods in Appendix G. Here's a summary of their characteristics: In ResNet-DomainNet, Top-k demonstrates uniform routing in the initial layers but chooses a single expert in the last layer.  REINFORCE, ST-Gumbel, and Dselect-k tend to mostly exhibit degenerate single-expert routing policies. Interestingly, all these gradient estimators learn to assign a distinct expert for the Quickdraw dataset. However, this degree of specialization is insufficient for achieving superior performance scores. In T5-GLUE, these estimators display degenerate routing in some layers and a tendency to share a few experts (approximately 3 out of 8) in other layers across tasks. Methods like Latent Skills, Ensemble, and SMEAR, which utilize most experts in the MoE layer, generally outperform these estimators. It is important to note that Latent Skills requires task metadata, and Ensemble routing incurs significant additional computational costs. SMEAR not only performs better than Latent Skills but also approaches the effectiveness of Ensemble, all without needing task metadata and while maintaining a computational cost nearly equivalent to that of discrete routing methods. We incorporated this summary into the main text. \n\n> Finally, the appendix and full text does not have to be in separate pdfs.\n\nThank you for your suggestion. We have now combined them into a single PDF for improved readability and accessibility.\n\nThanks again for your time and please let us know if you have any further questions or clarifications."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177071787,
                "cdate": 1700177071787,
                "tmdate": 1700177071787,
                "mdate": 1700177071787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRR9eqEh3L",
                "forum": "QHzzAU7Qf9",
                "replyto": "HVsMHR7x4x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_y2t1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_y2t1"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response. The authors improved the soundness of their results after revision. Please also include your discussion about learned mappings of baselines in the paper, either in main text or appendix. \n\n> Past work on gradient estimators such as [https://arxiv.org/abs/1711.00123], [https://openreview.net/forum?id=r1lgTGL5DE, https://arxiv.org/abs/2109.11817] supports our hypothesis regarding the challenges associated with learning using gradient estimator techniques. In our paper, we adopt an empirical approach to compare learned routing methods, and therefore use final model performance as the metric for evaluating routing efficacy. \n\nBut the abstract says \"In this paper, we hypothesize that these shortcomings stem from the gradient estimation techniques used to train modular models that use non-differentiable discrete routing decisions. \" Do you consider the hypothesis to be one of the novel contributions? If the hypothesis is cited from prior works, the wording of the sentence should be updated to avoid the confusion. I think reviewer dSc4 raised a similar issue."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597716721,
                "cdate": 1700597716721,
                "tmdate": 1700597716721,
                "mdate": 1700597716721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yBr96TxJc3",
            "forum": "QHzzAU7Qf9",
            "replyto": "QHzzAU7Qf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_uHR4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_uHR4"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on shortcomings of models with discrete routing among experts that can lead them to underperform heuristic non-learned routing. The authors hypothesize that issues with conditional computation stem from issues with gradient estimation, which is a technique utilized to provide approximate gradients for models involving discrete adaptive routing. The authors introduce SMERA for training modular models by computing a weighted average of the parameters of the individual experts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Underperformance of models that use conditional computation is an important topic.\n2. This paper provides some empirical analyses to verify the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The implementation details of the proposed method are not clear enough. For example, what routing function $R(\\cdot)$ do you use? And what is the specific form of the objective function in the training process?\n2. This paper lacks sufficient novelty. Can you explain what are the differences and advantages of the proposed method, compared to the $\\pi$-Tuning method proposed in [1] (Section 2.2 and Section 3.5)?\n   \n[1] Wu, Chengyue, et al. \"$\\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation.\" International Conference on Machine Learning. 2023."
                },
                "questions": {
                    "value": "1. What routing function $R(\\cdot)$ do you use? \n2. What is the specific form of the objective function in the training process?\n3. Can you explain what are the differences and advantages of the proposed method, compared to the $\\pi$-Tuning method proposed in [1] (Section 2.2 and Section 3.5)?\n   \n[1] Wu, Chengyue, et al. \"$\\pi$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation.\" International Conference on Machine Learning. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6566/Reviewer_uHR4",
                        "ICLR.cc/2024/Conference/Submission6566/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804861048,
            "cdate": 1698804861048,
            "tmdate": 1700621748829,
            "mdate": 1700621748829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9V4mBIyKCF",
                "forum": "QHzzAU7Qf9",
                "replyto": "yBr96TxJc3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable comments and suggestions. Below we address each point raised:  \n\n> The implementation details of the proposed method are not clear enough. For example, what routing function R(.)  do you use? And what is the specific form of the objective function in the training process?\n\nFor the routing function R(.), we follow a standard approach of using a single linear layer which is a weight matrix of shape d x N, where d is the model dimension and N is the number of experts in the MoE layer. It takes in as input the average hidden states of tokens at the MoE layer and generates a probability distribution over N experts by applying softmax to the outputs. We included these details in our updated draft for greater clarity.  \n\nRegarding training objectives, there are no additional loss functions added for the router. We use the language modeling loss for T5-GLUE and cross-entropy classification loss for ResNet-DomainNet and train expert and router parameters end-to-end with standard backpropagation. The benefit of our method being fully differentiable with these loss objectives is that it allows the router to be trained directly without any need for approximate gradient estimators or custom losses.\n\n> This paper lacks sufficient novelty. Can you explain what are the differences and advantages of the proposed method, compared to the $\\pi$-Tuning method proposed in [1] (Section 2.2 and Section 3.5)?\n\nWe want to emphasize that our method is simple, eliminating the need for gradient estimation and it outperforms previous routing methods. Moreover, our approach involves per-example adaptive routing without requiring any task meta-data, which is often difficult to obtain in real-world scenarios. In contrast,  $\\pi$-Tuning employs a set of existing task specific experts, retrieving the top k experts for a downstream task and learns to interpolate among these experts for the downstream task. In our case, firstly, we do not assume access to a pool of task specific experts and secondly, we do not have access to task metadata. While  $\\pi$-Tuning enables transfer learning to a new downstream task by learning to interpolate, our focus is on developing a routing algorithm among the combined dataset of tasks without distinct task boundaries and achieve positive transfer among existing tasks and reduce negative transfer which can be seen by the improved performance of our method. We added a more explicit discussion of $\\pi$-Tuning in our updated draft.\n\nThank you for your time. Please let us know if you have any further questions or clarifications."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176871311,
                "cdate": 1700176871311,
                "tmdate": 1700179409105,
                "mdate": 1700179409105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xyHDXzd6Le",
                "forum": "QHzzAU7Qf9",
                "replyto": "9V4mBIyKCF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_uHR4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_uHR4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623242398,
                "cdate": 1700623242398,
                "tmdate": 1700623242398,
                "mdate": 1700623242398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QHGVbwwgra",
            "forum": "QHzzAU7Qf9",
            "replyto": "QHzzAU7Qf9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_dSc4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6566/Reviewer_dSc4"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel method for routing experts in Mixture of Experts (MoE) models. Compared to methods that seek discrete routing policies and require gradient estimators (REINFORCE, Gumbel-Softmax), the authors propose an adaptive technique which is fully differentiable. Motivated by work on _merging models_,  SMEAR is presented, which computes a weighted average of expert parameters within each MoE block; thus enabling a fully differentiable routing policy. Authors validate on GLUE and DomainNet. They show that the method outperforms other discrete MoE models at equivalent inference rates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is very well written paper, clear it its aims, hypotheses, presented method and results. The manuscript offers an extensive analysis of the literature and corresponding baseline methods (gradient estimators, routing policies (top-k, d-select k etc.), heuristic routing, modular models) in addition to related work.\n\n2. Merging expert parameters in MoE networks is under-explored and a straightforward *yet* elegant solution for adaptive routing of experts. The results are convincing although the improvement in accuracy does seem marginal at best (1% over REINFORCE Fig 2a, 2-3% TopK etc.)\n\n3. The paper is a nice and interesting addition to the modular/MoE literature and presents interesting new research direction as pointed by the authors and beyond."
                },
                "weaknesses": {
                    "value": "1. One weakness of the manuscript is the lack of a detail explanation on the central hypothesis of the work. The authors claim:\n\n_\"As we will later show in section 4, the gradient estimation techniques used to train models with discrete routing often fail to produce performant routing strategies.\"_\n\nThis is shown throughout quantitative results but as a reader, I was expecting a substantial theoretical explanation as to why discrete routing methods may fail related to how gradients may be routed, instabilities etc. Additionally, Figure 3 offers qualitative explanation of the learned policy through SMEAR. How does this relate to Top-K, DSelect-K etc. Are the policies that much different?\n\n2. Many MoE methods often seen instabilities in training - mode collapse i.e. an expert being chosen much more, non-optimal solutions etc. The presented method is a weighted average of parameters. How does such a method regularise against particularly known failure modes of MoEs which often require explicit regularisation such as through load, importance, entropy, or Mutual Information?"
                },
                "questions": {
                    "value": "1. How would you go about encouraging exploration across experts in SMEAR to help determine which expert weighting is most beneficial per token/image etc.? In a discrete scenario, one might look at dropout and/or jitter noise on incoming representations? Did the authors try something similar to investigate performance?\n\n2. The computational cost of the averaging works _\"as long as there is a modest number of expert\"_. In papers such as Switch Transformer, they use 128 experts compared to the 16 used in Scaling experiments. Would this method work in this context?\n\n3. Can you foresee any pitfalls from where weight averaging might not be beneficial? What are the main limitations of the method?\n\n4. How much specialisation do you learn as get one gets deeper into the network? Can you control this and would it get better results if you learned? There is a lot of learned sparsity in Figure3a but is this necessarily good? It could be a limitation that not enough exploration occurs through the proposed learning scheme.\n\n[extra]\nFormatting - not a question. but a comment on legibility. Section 4.1. needs to be broken up into paragraphs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698942279652,
            "cdate": 1698942279652,
            "tmdate": 1699636743428,
            "mdate": 1699636743428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g5Kx29ZroV",
                "forum": "QHzzAU7Qf9",
                "replyto": "QHGVbwwgra",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments and constructive feedback. We have carefully considered your points and below is our detailed rebuttal:\n\n> The results are convincing although the improvement in accuracy does seem marginal at best (1% over REINFORCE Fig 2a, 2-3% TopK etc.)\n\nWhile the absolute improvements may appear small, we argue that they are meaningful since they are average values across a diverse range of datasets\u20148 in GLUE and 6 in DomainNet\u2014conducted over 5 random seeds. In addition, improvements on these benchmarks are often small \u2013 e.g. most historically state-of-the-art methods on GLUE only improved over the prior state-of-the-art by a few tenths of a percentage. This not only underlies the robustness of our approach but also its adaptability across datasets and architectures. We added discussion to our paper on the significance of these gains.\n\n> I was expecting a substantial theoretical explanation as to why discrete routing methods may fail related to how gradients may be routed, instabilities etc\n\nWe agree that theoretical analysis concerning the limitations of gradient estimators in discrete routing methods can be valuable. We chose not to include such analysis in our paper for two reasons: First, relevant discussions on this topic can be found in existing literature, notably in sources such as [https://arxiv.org/abs/1711.00123], [https://openreview.net/forum?id=r1lgTGL5DE, https://arxiv.org/abs/2109.11817]. Second, the theoretical advantage of our approach is simple: it facilitates the exact gradient computation via backpropagation. All past approaches for learning discrete routing require some form of gradient estimation, and these estimators incur bias or variance with respect to the true gradient. Our method has no such issues. We have edited our paper to more explicitly discuss this advantage and its relation to past theoretical analysis.\n\n>Additionally, Figure 3 offers qualitative explanation of the learned policy through SMEAR. How does this relate to Top-K, DSelect-K etc. Are the policies that much different?\n \nTo answer your question, we have added routing policies for the various methods we considered in a new Appendix G. Here's a summary of their characteristics: In ResNet-DomainNet, Top-k demonstrates uniform routing in the initial layers but chooses a single expert in the last layer.  REINFORCE, ST-Gumbel, and Dselect-k tend to exhibit mostly degenerate single expert routing policies. Interestingly, all these gradient estimators learn to assign a distinct expert for the Quickdraw dataset. However, this degree of specialization is insufficient for achieving superior performance scores. In T5-GLUE, these estimators display degenerate routing in some layers, while showing a tendency to share a few experts (approximately 3 out of 8) in other layers across tasks. \n\nMethods like Latent Skills, Ensemble, and SMEAR, which utilize most experts in the MoE layer, generally outperform these estimators. However, it's important to note that Latent Skills requires task metadata, and Ensemble routing incurs significant additional computational costs. SMEAR not only performs better than Latent Skills but also approaches the effectiveness of Ensemble routing, all without needing task metadata and while maintaining a computational cost nearly equivalent to that of discrete routing methods. We incorporated this summary into the main text. We have not conducted a quantitative comparison of the routing policies across different methods due to the absence of established metrics, and such comparisons are nuanced. Therefore, we provide a qualitative analysis and use the final performance as a practical metric for evaluating the effectiveness of various routing methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176639266,
                "cdate": 1700176639266,
                "tmdate": 1700176639266,
                "mdate": 1700176639266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jytouy6I7h",
                "forum": "QHzzAU7Qf9",
                "replyto": "QHGVbwwgra",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The presented method is a weighted average of parameters. How does such a method regularise against particularly known failure modes of MoEs which often require explicit regularisation such as through load, importance, entropy, or Mutual Information? How would you go about encouraging exploration across experts in SMEAR to help determine which expert weighting is most beneficial per token/image etc.? In a discrete scenario, one might look at dropout and/or jitter noise on incoming representations?\n\nOur method optimizes the routing policy using the loss function's gradient, focusing on minimizing loss. All past load balancing techniques are equally applicable to SMEAR, but they are inappropriate in the experimental context we consider because dataset sizes in the multitask mixture vary by multiple orders of magnitude and the load should therefore not necessarily be balanced. As concrete evidence, note that tag routing is an extremely unbalanced routing strategy but nevertheless has higher performance than most gradient estimators we considered. So, to promote exploration, we implemented expert dropout, randomly removing some experts in the MoE block and averaging the weights of the remaining ones. This strategy proved beneficial in specific contexts, notably in the T5-GLUE setting, as evidenced in our ablation study (Table 1). In contrast, we found that jitter noise did not offer additional benefits. In the case of other routing methods, we use expert dropout regularizer if it was found to improve performance (Table 1).  \n\n>The computational cost of the averaging works \"as long as there is a modest number of expert\". In papers such as Switch Transformer, they use 128 experts compared to the 16 used in Scaling experiments. Would this method work in this context?\n\nOur reference to a \u201cmodest number of experts\u201d is based on the computational ratio for discrete routing versus SMEAR, which is calculated as (1 + N x 2 / L x 4). Here, N represents the number of experts and L the number of tokens per example. As long as the second term of this ratio remains less than 1, the computational requirements are close. In our experiments with T5-GLUE, where L is 128 and N is approximately 8, the runtime difference was minimal. Moreover, tensor averaging typically benefits from GPU optimization. Therefore, we believe scaling up to N ~ 100 is feasible if the sequence length is around 1000, a scenario similar to what is observed in Switch models. This approach should maintain a computational cost comparable to discrete routing. We added this example to the paper for greater clarity. \n\n\n>Can you foresee any pitfalls from where weight averaging might not be beneficial? What are the main limitations of the method?\n\nWhile we recognize that SMEAR shows improved performance compared to other routing methods, a performance gap w.r.t. Ensemble routing still exists in the ResNet-DomainNet setting. Additionally, our approach necessitates that the experts be homogeneous, since the method of weight averaging works only in this case. Exploring better merging techniques to resolve this gap and allow for heterogeneous expert architectures would be exciting directions for future research.\n\n>How much specialisation do you learn as get one gets deeper into the network? Can you control this and would it get better results if you learned? There is a lot of learned sparsity in Figure3a but is this necessarily good? It could be a limitation that not enough exploration occurs through the proposed learning scheme.\n\nWhile we did not compute a quantitative measure of specialization as one progresses through the network's layers, our qualitative observations offer some insights. In the ResNet-DomainNet setting, we observed no significant difference in specialization across layers. Similarly, in the T5-GLUE setting, we did not find any obvious patterns in terms of layer-specific specialization.\nRegarding sparsity, a possible advantage is that it allows experts to specialize to certain tasks, but it is not obvious a priori that sparsity is valuable. Our method does not explicitly constrain sparsity and therefore can trade-off sparsity as appropriate. In the T5-GLUE experiments, we observed considerable sparsity, while in ResNet-DomainNet, despite the presence of specialization, sparsity was less pronounced. We believe exploring how to effectively regulate this specialization could be a promising direction for future research, potentially leading to improved routing algorithms.\n\n> Formatting - not a question. but a comment on legibility. Section 4.1. needs to be broken up into paragraphs.\n\nThanks for the comment. We have updated the draft accordingly.\n\nThanks again for your time. Please let us know if you have any further questions or clarifications."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176738164,
                "cdate": 1700176738164,
                "tmdate": 1700176776498,
                "mdate": 1700176776498,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ce8eBYvuVK",
                "forum": "QHzzAU7Qf9",
                "replyto": "g5Kx29ZroV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_dSc4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6566/Reviewer_dSc4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I\u2019ve read the rebuttal across all reviews. \n\nMy score will stay the same. Many thanks"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707462772,
                "cdate": 1700707462772,
                "tmdate": 1700707462772,
                "mdate": 1700707462772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]