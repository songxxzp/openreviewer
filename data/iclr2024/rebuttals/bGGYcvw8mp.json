[
    {
        "title": "Understanding In-Context Learning from Repetitions"
    },
    {
        "review": {
            "id": "5AEnrBVGF0",
            "forum": "bGGYcvw8mp",
            "replyto": "bGGYcvw8mp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines the phenomenon of token co-occurrence reinforcement, whereby tokens occurring in the context have a higher chance of being predicted by an LM.\nAn interesting consequence is that In-Context Learning (ICL) can go wrong when a label that is different from the target label is repeated often in the in-context examples, potentially helping explain some failure modes of ICL.\n\nIt first shows that self-reinforcement occurs with randomly constructed sequences of words. The longer the repeated part, the stronger the reinforcement effect (Figure 3).\nIt then shows that even discontinuous repeated subsequences give rise to the reinforcement effect.\nSection 4 studies the effect of the reinforcement effect on ICL. The effect can help constrain the output space and the desired pattern, but it can also lead to incorrect results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- systematic study using openly available LLM family\n\n- interesting results about the factors impacting the success and failure of ICL. For instance, in Figure 7, the authors seem to show that that in a specific CoT prompting setup (GSM8K), replacing the questions and CoT answers in the demonstrations with random tokens does not hurt ICL performance nearly as much as replacing a separator with random tokens."
                },
                "weaknesses": {
                    "value": "- The paper remains somewhat unclear regarding the overall contributions and implications. The paper argues that the results helps understand both limitations of ICL and the inner workings of ICL.\n To the extent that the paper aims to illuminate the \"inner workings\" of ICL, the contribution is left somewhat unclear. Do the authors argue that co-occurrence reinforcement is implicated in LMs' ability to pick up input-label mappings? And what \"inner workings\" are responsible for the co-occurrence effect? A range of recent research discussed in Section 5 aims to explain how ICL works, and observations on the self-reinforcement effect could help shed light -- and indeed the paper hints at this repeatedly (e.g., end of page 4, \"In the context of ICL, this pattern corresponds to demonstrations like\" -- it seems that the experiment is understood as some kind of simple prompt-like structure, but this idea and its implications are then not made explicit)."
                },
                "questions": {
                    "value": "- Clarity: Section 3.1 uses the term \"sentence\" for the examples, but in the experiment they are \"randomly generated\". How are they sampled -- just as random sequences of symbols sampled i.i.d. from the vocabulary? Are all symbols in the vocabulary equally like to appear? The same question about how masked subsequences are resampled also applies to the other experiments.\n\n- Section 4.1, \"Learning to follow patterns\" -- when masking the Question, does this mean that the questions in the demonstrations are masked (i.e., replaced with random word sequences) but the question in the final element of the prompt (the one to respond to) is not masked?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8",
                        "ICLR.cc/2024/Conference/Submission1749/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797059722,
            "cdate": 1698797059722,
            "tmdate": 1700571462706,
            "mdate": 1700571462706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yA36x7ipQ5",
                "forum": "bGGYcvw8mp",
                "replyto": "5AEnrBVGF0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Ydk8"
                    },
                    "comment": {
                        "value": "Thanks for your time and insightful reviews. \n\n**Q: \u201cunclear contributions\u201d**\n\nA: Thanks for your advice. Here, we restate our contributions as follows:\n\n- We propose a novel perspective to understand icl with repetitive text generations, where the key effect causing repetitions appears to be an important feature in ICL.\n- We empirically establish the existence of token reinforcement across various LLMs, alongside with the inevitable reason behind learning process and inner workings responsible for it.\n- We show that token reinforcement plays crucial role in ICL for both beneficial and detrimental effects. It constrains output space and desired patterns, but it is also responsible for spurious connections and possible failure of ICL. Our findings provide a novel research basis for analyze the mechanism of in-context learning.\n\n**Q: \u201cwhat inner-working are responsible for token reinforcement\u201d**\n\nA: We conduct more analysis over the attention weights and find that in our reinforcement experiments, the LLMs tend to attend the preceding adjacent token with high attention weights. Thus, we hypothesize that the attention to adjacent token is crucial for token reinforcement. To validate our hypothesis, we mask the each token\u2019s attention to its preceding adjacent token and find the strength of token reinforcement is reduced by ~10 times. \n\nHence, the reinforcement is not simply realized by attending to similar tokens. Rather, the information is propagated through a process wherein each token iteratively attends to its adjacent token.\n\nFor more details about our experiments, we refer you to Appendix C. \n\n**Q: \u201cclearer implications\u201d**\n\nA: Thanks for your advice about implications and it helps improve our paper! Instead of the traditional thinking of ICL as a mapping from input to label, in this paper, we think of ICL as a combination of reinforcement of tokens, even though some of them only appear once and not get strongly reinforced. Currently, in the last paragraph of Section 3 and the conclusion of our revised paper, we highlight this implication, and we are making more efforts to make the paper clearer. \n\n**Q1 \u201crandom sentences\u201d:** \n\nA: Yes. All random sentences are generated by uniformly sampled from the vocabulary. All the symbols are equally likely to appear, except the special tokens. We add the description to Appendix B.1.\n\n**Q2 \u201cmask question\u201d:** \n\nA: Yes. In this experiment, we are trying to test which reinforcement is helpful for following patterns. Thus, we do not touch the test query, and only modify the demonstrations. \n\nWe add the above mentioned experiments to the paper and promise to update the main content accordingly. We hope these results resolve your concern and will be really appreciated if you re-consider the score of our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571103225,
                "cdate": 1700571103225,
                "tmdate": 1700571103225,
                "mdate": 1700571103225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "klqwja60tg",
                "forum": "bGGYcvw8mp",
                "replyto": "L2LzNBHLQ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for taking the time for these additional results. I agree that they can address to  some extent the concerns about lack of clarity about contribution raised in my review. I\u2019ll reconsider my scores based on this."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571393662,
                "cdate": 1700571393662,
                "tmdate": 1700571393662,
                "mdate": 1700571393662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hErmeBjue6",
                "forum": "bGGYcvw8mp",
                "replyto": "yA36x7ipQ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_Ydk8"
                ],
                "content": {
                    "comment": {
                        "value": "I\u2019ve increased the contribution score to 3."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571484053,
                "cdate": 1700571484053,
                "tmdate": 1700571484053,
                "mdate": 1700571484053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4lUOdM5jGX",
            "forum": "bGGYcvw8mp",
            "replyto": "bGGYcvw8mp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_5Dh6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_5Dh6"
            ],
            "content": {
                "summary": {
                    "value": "The authors delve into an investigation of the impact of token repetition on in-context learning (ICL) performance, revealing both beneficial and detrimental effects through empirical analysis. On the positive side, repetition aids in narrowing down the output space, fostering consistency in subsequent predictions. However, the downside is evident when non-informative or incorrect messages are repeated in the demonstrations, resulting in diminished prediction accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors conducted extensive experiments to explore the nuances of token repetition and its effects on ICL."
                },
                "weaknesses": {
                    "value": "1. **Lack of Clear Motivation**: The rationale behind investigating token repetition patterns is not adequately clear, leaving the reader uncertain about the study's purpose. Additionally, the abstract does not effectively capture the paper's core idea, necessitating further clarification to provide a concise summary of the work.\n2. **Need for Enhanced Clarity**: The manuscript's presentation is complex, making it challenging for readers to navigate through the content. A more detailed explanation of certain terms and concepts, such as *surface pattern*, *self-reinforcement effect*, and *token reinforcement*, would significantly enhance comprehension, particularly if these have been previously established in prior literature.\n3. **Specificity of Experiments**: The experiments are overly specific and do not sufficiently connect to real-world applications. Demonstrating how the findings on repetition patterns can be applied in practical scenarios would strengthen the paper's relevance and impact."
                },
                "questions": {
                    "value": "While I do not have any specific questions at this moment, I would appreciate it if the authors could address the concerns raised in the Weakness section, particularly regarding the need for clearer motivation and enhanced clarification of certain terms and concepts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Reviewer_5Dh6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808846180,
            "cdate": 1698808846180,
            "tmdate": 1699636104040,
            "mdate": 1699636104040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "blZfhNlHiO",
                "forum": "bGGYcvw8mp",
                "replyto": "4lUOdM5jGX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 5Dh6"
                    },
                    "comment": {
                        "value": "Thanks for reviewing our paper. \n\nWe provide three additional experiments, aiming to resolve the reasons of token reinforcement: (1) The similar token reinforcement lies in the pretraining dataset and thus it is learned by LLMs; (2) repeating features responsible for token reinforcement are effective in the pre-training stage, and thus would be utilized by LLMs; (3) Attending preceding token is responsible for reinforcement.\n\nIn addition, we clarify our contributions and implications and refer you to our general response and newly revised paper(with revised part highlighted with blue).\n\nWe add the above mentioned experiments to the paper and promise to update the main content accordingly. We hope these results resolve your concern and will be really appreciated if you re-consider the score of our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571028702,
                "cdate": 1700571028702,
                "tmdate": 1700571028702,
                "mdate": 1700571028702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7c5jw4rOUw",
            "forum": "bGGYcvw8mp",
            "replyto": "bGGYcvw8mp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_iqcg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_iqcg"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the emergent in-context learning (ICL) ability of LLM. The authors try to probe the ICL performance through surface repetitions, and establish a theory called token co-occurrence reinforcement, which explains the reasons for possible failures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work provides meaningful explanations on the possible failures of ICL. \n- The experiments seem comprehensive and convincing."
                },
                "weaknesses": {
                    "value": "- This work tries to understand the inherent ICL behavior of LLMs, yet is in lack of theoretical analysis. For example, how is such token co-occurrence reinforcement established? This may involve the detailed interactions between prompts and self-attention mechanism, etc., which I would like the authors to delve into.\n- As an experiment-oriented work, the authors should examine their assumptions on more LLMs; otherwise, it's hard to reach a common conclusion.\n- The findings of this work are not completely new. As far as I'm concerned, the findings are based on the distributional bias in the demonstrations. The impact of spurious correlations are widely discussed in out-of-distribution generalization literature. In this regard, more insights are welcome. Also, the authors could discuss on how to address such (inevitable) distributional bias in the demonstrations."
                },
                "questions": {
                    "value": "As mentioned above, how could me mitigate the biased effect in ICL demonstrations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698894794663,
            "cdate": 1698894794663,
            "tmdate": 1699636103976,
            "mdate": 1699636103976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ojumiycYCs",
                "forum": "bGGYcvw8mp",
                "replyto": "7c5jw4rOUw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer iqcg"
                    },
                    "comment": {
                        "value": "Thanks for your time and insightful reviews. \n\n**Q: \u201chow is such token co-occurrence reinforcement established?\u201d**\n\nA: We provide three additional experiments, demonstrating that (1) The similar token reinforcement lies in the pretraining dataset and thus it is learned by LLMs; (2) repeating features responsible for token reinforcement are effective in the pre-training stage, and thus would be utilized by LLMs; (3) Attending preceding token is responsible for reinforcement.\n\nBased on these experiments, we explain the inevitable reason of token reinforcement is the models\u2019 effort to maximize training likelihood.\n\nFor more details, we refer you to the general response and Appendix C in our revised version of paper. \n\n**Q: \u201cexamine assumptions on more LLMs\u201d**\n\nA: Thanks for your advice! We further consolidate our findings on LLaMA-2(7,13,70b), GPT-J and Mistral. We now examine 16 LLMs in total. All results (Appendix D.2, marked as blue) accord with our findings in main content. \n\n**Q: \u201cdistributional bias in the demonstrations\u201d:** \n\nA: This is an insightful perspective to understand our work. \n\nIn our understanding, our findings provide an interesting perspective of distributional bias. \n\nIf we consider the distributional bias between the pre-training stage and testing, in-context learning itself should be regarded as a result of distributional bias. In this regard, we show that this distributional bias is helpful through token reinforcement, where the distributional bias studied in OOD is generally harmful. The beneficial part of the bias should be enhanced, and the detrimental part should be mitigated. \n\nWe would like to discuss more related work of distributional bias in our next version. \n\n**Q: \u201cHow to mitigate such bias?\u201d** \n\nA: Currently, we do not have an experiment showing how to resolve this. Our intuition is that such reinforcement indicates concentration in the representation space at certain direction. If we can detect the direction, we can selectively enhance or mitigate the beneficial and detrimental parts of the bias. \n\nWe add the above mentioned experiments to the paper and promise to update the main content accordingly. We hope these results resolve your concern and will be really appreciated if you re-consider the score of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570972654,
                "cdate": 1700570972654,
                "tmdate": 1700570972654,
                "mdate": 1700570972654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LAUJXKUyVh",
                "forum": "bGGYcvw8mp",
                "replyto": "ojumiycYCs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_iqcg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_iqcg"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' tremendous efforts made in rebuttal, which has addressed some of my concerns, and I will re-consider the score after discussing with other reviewers.\n\nIf I understand correctly, this paper proposed a new perspective that tries to consider ICL as the combination of a series of token reinforcement effects that have been potentially established during LLM pre-training, and can be triggered with few in-context demonstrations. This perspective is sufficient since in pre-training, LLM already sees every word requisite for the generative inference, and hence there can always be some \"token reinforcement\" effect which connects some pre-trained word (or the in-context word) to the generated results. However, as far as I'm concerned, it cannot necessarily prove that token reinforcement is the very cause that enables LLM to understand the intended task given only several demonstrations.\n\nRegarding the distributional bias, I believe that the biases discussed in OOD literature and in this paper are essentially the same. A bias is considered \"good\" when it is in line with the desired pattern in inference, and hence there is no bias that is purely \"good\" or \"bad\".\n\nIn summary, I believe that the proposed token reinforcement effect can indeed explain some common behaviors of ICL, but I feel that the actual working mechanism of ICL is beyond token reinforcement.  I would also welcome the authors' and the other reviewers' further input in case I have missed something."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640768046,
                "cdate": 1700640768046,
                "tmdate": 1700640768046,
                "mdate": 1700640768046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rZ2puYTm2z",
            "forum": "bGGYcvw8mp",
            "replyto": "bGGYcvw8mp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_d42T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1749/Reviewer_d42T"
            ],
            "content": {
                "summary": {
                    "value": "This paper quantitatively investigates in-context learning in terms of surface patterns. \nIt shows that there is an inherent correlation between surface patterns, self-reinforcement, iterative generation and their important role in text generation.\nIn particular, it shows the role of surface surface patterns in text generation and the existence of token co-occurrence reinforcement that strengthens the relationship between two tokens based on their contextual co-occurrences.\nThe experiments on MMLU and GSM8K show that the reinforcement helps to constrain the output space and format the output according to a  demonstration such as \u2018Let\u2019s think step by step\u2019 ."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provide a novel framework to understanding in-context learning via  the notion of token co-occurrence reinforcement.\nThrough various experiments, the authors have shown how token reinforcement causes spurious correlations in in-context learning."
                },
                "weaknesses": {
                    "value": "Althogh  there is a novelty in showing experimentally that token reinforcement can cause some problems in in-context learning,\nthe paper lacks the important perspective of analyzing why token reinforcement exists and causes problems. \nFor example, the following paper, which is only briefly mentioned in this paper, analyzes the impact of repetition structures in a corpus on in-context learning from an information-theoretic perspective.\nA Theory of Emergent In-Context Learning as Implicit Structure Induction\nMichael Hahn, Navin Goyal\nThey showed that the performance of in-context learning is represented by a complexity that repetition structures can be represented by a small PCFG tree, and  experimentally investigated theoretical finindings."
                },
                "questions": {
                    "value": "Is it possible to provide hypothesis about the reason for the token reinforcement phenomenon and check it, even experimentally?\nThe authors may argue that analyzing reasons is the next step, i.e., outside the scope of this paper; however, it is fandamental in machine learning research."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1749/Reviewer_d42T"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1749/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698925968801,
            "cdate": 1698925968801,
            "tmdate": 1700646034403,
            "mdate": 1700646034403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DnhWIWr0aB",
                "forum": "bGGYcvw8mp",
                "replyto": "rZ2puYTm2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer d42T"
                    },
                    "comment": {
                        "value": "Thanks for your time and insightful reviews. \n\n**Q: \u201cthe reason for the token reinforcement effect\u201d:**\n\nA: We provide three additional experiments, demonstrating that (1) The similar token reinforcement lies in the pretraining dataset and thus it is learned by LLMs; (2) repeating features responsible for token reinforcement are effective in the pre-training stage, and thus would be utilized by LLMs; (3) Attending preceding token is responsible for reinforcement.\n\nBased on these experiments, we explain the inevitable reason of token reinforcement is the models\u2019 effort of maximizing training likelihood.\n\nFor more details, we refer you to the general response and Appendix C in our revised version of paper. (At the first glance, we really gonna say it\u2019s the next step of this paper. However, we reckon that this perspective is an important complement for our paper and we are also very interested in the reasons.)\n\n**Q: \u201crelated work\u201d**\n\nA: Thanks for pointing out the related work! We have updated our related work section. \n\nWe add the above mentioned experiments to the paper and promise to update the main content accordingly. We hope these results resolve your concern and will be really appreciated if you re-consider the score of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570843204,
                "cdate": 1700570843204,
                "tmdate": 1700570843204,
                "mdate": 1700570843204,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TTmqSQHsFb",
                "forum": "bGGYcvw8mp",
                "replyto": "DnhWIWr0aB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_d42T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1749/Reviewer_d42T"
                ],
                "content": {
                    "comment": {
                        "value": "The author's reply and additional experiments seem interesting to me. \nThus, I raise my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1749/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645981662,
                "cdate": 1700645981662,
                "tmdate": 1700645981662,
                "mdate": 1700645981662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]