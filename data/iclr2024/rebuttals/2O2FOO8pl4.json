[
    {
        "title": "Label Privacy Source Coding in Vertical Federated Learning"
    },
    {
        "review": {
            "id": "FMdOlomolc",
            "forum": "2O2FOO8pl4",
            "replyto": "2O2FOO8pl4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_wj8U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_wj8U"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of protecting label privacy in vertical federated learning. In this setting, training data is vertically split into features and labels. There is one special player named Active party that owns labels and some features; there can be multiple Passive Parties that own disjoint features. Previous works have identified severe privacy issues when applying vanilla vertical federated learning algorithms. This work proposes a novel method of adjusting weight and label of each sample without leaking raw label information. In this paper, the authors make the assumption that the active party has features that are informative of the labels, and use gradient boosting to learn the new label-ID joint distribution. The privacy can be further enhanced with adversarial training. The experiment results show that the proposed method can achieve better privacy-utility trade-offs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of adjusting sample weights are novel and interesting. The closed form solution of AdaBoost is clean and nice. The experiment results are promising, showing that the proposed method is strong against listed attacks."
                },
                "weaknesses": {
                    "value": "My biggest concern is that the setting may be too restricted. In particular, the active party must own informative features. Let us look at Theorem 1. In the very extreme case, assume that the KL-divergence of $p_{gt}$ and $p_{act}$ is 0. Then Theorem 1 says that the LPSC does not leak anything. However, in this case we do not need federated learning: the active party can just learn with its own features and no communication with passive parties are needed. It requires more justification for the setting (active party with informative features, label only privacy, etc) considered in this work.\n\nMutual information is known to be closely related privacy. [1] is an important reference.\n\n[1] Cuff, Paul, and Lanqing Yu. \"Differential privacy as a mutual information constraint.\" Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016."
                },
                "questions": {
                    "value": "- It looks like all the datasets are for binary classification. Can LPSC work with non-binary labels?\n- According to Eq.5, it seems that in adversarial training, the original labels are used. Will this bring extra privacy issues?\n- Is it possible to prove formal privacy guarantees, e.g. differential privacy for LPSC?\n- In Appendix B.1, you mentioned \" we can use boosting to reduce the bias of the local model by using passive parties\u2019 auxiliary features.\" I am a bit confused, does LPSC uses information from passive parties?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2638/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2638/Reviewer_wj8U",
                        "ICLR.cc/2024/Conference/Submission2638/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698139203444,
            "cdate": 1698139203444,
            "tmdate": 1700652106854,
            "mdate": 1700652106854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "57x68j6mx3",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 1"
                    },
                    "comment": {
                        "value": "> My biggest concern is that the setting may be too restricted. In particular, the active party must own informative features. Let us look at Theorem 1. In the very extreme case, assume that the KL-divergence of $p_{act}$ and $p_{gt}$ is 0. Then Theorem 1 says that the LPSC does not leak anything. However, in this case, we do not need federated learning: the active party can just learn with its own features, and no communication with passive parties is needed. It requires more justification for the setting (active party with informative features, label-only privacy, etc.) considered in this work.\n\n**Answer**:\n\nWe appreciate Reviewer `wj8U`'s concern regarding the VFL setting in our work. \nWe emphasize that it is a typical and realistic scenario in many cross-enterprise collaborations, where the active party possesses somewhat/partially informative features. For example, in **cross-company advertisement** [1], an e-commerce company (e.g., Amazon) with user purchase history and a social platform (e.g., Facebook) with user social data collaboratively predict the Click-Through Rate (CTR). Another example is **cross-bank fraud detection** faced by VISA [2], where each bank has some transaction records of a user to produce imprecise local predictions. In both cases, each party holds valuable but partial information, making VFL a viable approach to leverage this marginal information.\n\nThe authors acknowledge that the original Theorem 1 misleadingly implied that the active party must own very informative features for LPSC to be effective. To clarify, we have provided a new $\\epsilon$-MIP privacy guarantee in the General Response Part 3, as restated as follows for convenience:\n\n**Theorem 1** (Privacy Guarantee.)  \nLPSC satisfies $\\epsilon$-mutual information privacy ($\\epsilon$-MIP). The privacy leakage is bounded by $\\epsilon = H(p_{gt}(i,y)|p_{act}(i,y))$, the conditional entropy of the ground-truth label distribution $p_{gt}(i,y)$ given the active party's label distribution $p_{act}(i,y)$. Formally,\n$$\nI(p_{gt}(i,y); p^*_{lpsc}(i,y)) \\leq \\epsilon \\text{ bits},\n$$\nwhere $p^*_{lpsc}(i,y)$ represents the optimal solution of the LPSC problem.\n\nAddressing the extreme case where $D_{KL}(p_{act}|p_{gt}) = 0$, it's true that VFL may seem unnecessary if the active party can independently learn $p_{gt}$. \nHowever, our framework's focus, as reflected in the updated privacy guarantee (Theorem 1), is on **ensuring minimal unnecessary privacy leakage** of $p_{act}(i,y)$ even when the active party seeks **marginal** performance improvements through VFL, given $D_{KL}(p_{act}|p_{gt})$ is low.\n\nLastly, when the active party has no informative features but only labels, the problem setting degrades to another line of research named **Split Learning** [3], where a label-holder and feature-holders train a split neural network. However, in VFL settings [4,5], it is a widely adopted assumption that the active party holds partially informative features.\n\n------\n\n**References:**\n\n[1] Penghui Wei, Hongjian Dou, Shaoguo Liu, Rongjun Tang, Li Liu, Liang Wang, and Bo Zheng. \"FedAds: A Benchmark for Privacy-Preserving CVR Estimation with Vertical Federated Learning.\" arXiv preprint arXiv:2305.08328 (2023).\n\n[2] https://usa.visa.com/about-visa/visa-research/research-areas.html\n\n[3] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. \"Split learning for health: Distributed deep learning without sharing raw patient data.\" arXiv preprint arXiv:1812.00564 (2018).\n\n[4] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. \"A survey on federated learning.\" Knowledge-Based Systems 216 (2021): 106775.\n\n[5] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang Yang. \"Vertical federated learning.\" arXiv preprint arXiv:2211.12814 (2022)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307615325,
                "cdate": 1700307615325,
                "tmdate": 1700307615325,
                "mdate": 1700307615325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mHfUIU7vqu",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 2"
                    },
                    "comment": {
                        "value": "> Mutual information is known to be closely related privacy. [1] is an important reference.\n\n>[1] Paul Cuff, and Lanqing Yu. \"Differential privacy as a mutual information constraint.\" Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.\n\n**Answer**:\n\nWe appreciate the valuable reference [1] provided by the reviewer to clarify the relationship between mutual information (MIP) adopted by our work and differential privacy (DP). \n\nWe provide a general response to the relationship between MIP and DP in the **General Response**.\nWe have also added Appendix C.4 \"Comparison of MIP v.s. DP\" in our revision to clarify the relationship between MIP and DP, based on the mutual information-variant differential privacy, MI-DP, proposed in [1].\n\nFor your convenience, we briefly summarize the relationship between MIP and DP as follows:\n\n> **$\\epsilon$-MI-DP:**\n> According to [1], a mechanism $\\mathcal{M}$ satisfies $\\epsilon$-mutual-information differential privacy for some $\\epsilon \\in \\mathbb{R}^{+}$ if, for **any neighboring inputs $X, X'$**, the conditional mutual information between $X$ and $Y=\\mathcal{M}(X)$ conditioned on $X'$ satisfies\n> $$\n>        I(X; Y | X') \\leq \\epsilon \\text{ bits}.\n> $$\n\n> **$\\epsilon$-MIP:**\n> According to [2], a mechanism $\\mathcal{M}$ satisfies $\\epsilon$-MIP for some $\\epsilon \\in \\mathbb{R}^{+}$ if, for **any input $X$**, the mutual information between $X$ and the output $Y = \\mathcal{M}(X)$ is bounded by $\\epsilon$ bits, formally:\n> $$\n>     I(X; Y) \\leq \\epsilon \\text{ bits}.\n> $$\n\nThe fundamental difference between MIP and DP, including its variant MI-DP, lies in their underlying **threat models**. \nDP operates under a strong adversary assumption, considering adversaries that have access to neighboring input databases $X'$. In contrast, MIP is designed under the assumption that the adversary lacks prior knowledge of the database, which aligns more closely with the threat model in our VFL setting.\n\nGiven the assumption in our VFL setting where an adversary has **no** prior knowledge of the active party's data $\\\\{\\mathbf{i}, \\mathbf{y}, \\mathbf{X}_0\\\\}$, MIP emerges as a more fitting choice. Our threat model, as elaborated in Section 3, coincides with the principles of MIP, making it a natural fit for our research.\n\nMeanwhile, [2] reveals a fundamental connection between mutual information and differential privacy bridged by _identifiability_.\n\nWe have added the two references [1,2] in our updated version.\n\n------\n\n**References:**\n\n[1] Paul Cuff, and Lanqing Yu. \"Differential privacy as a mutual information constraint.\" Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. 2016.\n\n[2] Wang Weina, Lei Ying, and Junshan Zhang. \"On the relation between identifiability, differential privacy, and mutual-information privacy.\" IEEE Transactions on Information Theory 62, no. 9 (2016): 5018-5029."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308041608,
                "cdate": 1700308041608,
                "tmdate": 1700308041608,
                "mdate": 1700308041608,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQaGyazR9P",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 1 and 2"
                    },
                    "comment": {
                        "value": "### Question 1\n\n> It looks like all the datasets are for binary classification. Can LPSC work with non-binary labels?\n\n**Answer**:\n\nLPSC is indeed adaptable to non-binary (multi-class) labels, aligning with the multi-class variants of boosting algorithms like AdaBoost and LogitBoost. The transition from binary-class to multi-class settings has been thoroughly explored [1, 2]. As an example, consider the AdaBoost algorithm's extension to a $K$-class problem.\n\nIn the binary-class AdaBoost:\n$$\n    p_{lpsc}(i)=\\frac{ e^{-\\alpha \\\\;y_i f_{\\theta}(i)}}{\\sum_{i \\in \\mathbf{i}} e^{-\\alpha \\\\; y_i f_{\\theta}(i)}},\n$$\nwhere $\\alpha=\\frac{1}{2}\\ln(\\frac{1-\\epsilon}{\\epsilon})$, and $\\epsilon$ is the classification error of the local model $f_{\\theta}$.\n\nExtended to $K$-classes (SAMME [1]), the equation modifies as:\n$$\n    p_{lpsc}(i)=\\frac{ e^{-\\alpha \\cdot \\mathbb{I}(y_i \\neq f_{\\theta}(i))}}{\\sum_{i \\in \\mathbf{i}} e^{-\\alpha \\cdot \\mathbb{I}(y_i \\neq f_{\\theta}(i))}},\n$$\nwith $\\alpha=\\frac{1}{2}\\ln(\\frac{1-\\epsilon}{\\epsilon}) + \\frac{1}{2} \\log(K-1)$.\n\nThis modification adds a simple term $\\log(K-1)$ to $\\alpha$. When $K=2$, SAMME simplifies to AdaBoost. \n\nWe are actively working to incorporate additional experiments on multi-class classification datasets in our later revised manuscript. \nIt's important to note, however, that fully exploring LPSC's extension to multi-class scenarios and assessing privacy-utility trade-offs across different multi-class boosting algorithms is beyond the scope of our current research. This area requires a dedicated and extensive study. \nA promising future research direction could be the development of a tailored multi-class boosting algorithm specifically designed to satisfy LPSC, rather than adapting existing algorithms.\n\n**References:**\n\n[1] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. \"Multi-class adaboost.\" Statistics and its Interface 2, no. 3 (2009): 349-360.\n\n[2] Mohammad Saberian, and Nuno Vasconcelos. \"Multiclass boosting: Theory and algorithms.\" Advances in neural information processing systems 24 (2011).\n\n\n\n--------\n\n### Question 2\n\n> According to Eq.5, it seems that in adversarial training, the original labels are used. Will this bring extra privacy issues?\n\n**Answer:**\n\nThank you for your pertinent query regarding the use of original labels in our adversarial training process. While these labels are indeed utilized, our methodology is designed to **neutralize or eliminate** any sensitive label information within the LPSC-encoded residuals. The key is the strategic use of the weight $\\beta$ in our algorithm, which is meticulously tuned to ensure a harmonious balance between privacy protection and utility enhancement.\n\nThe utilization of adversarial loss to protect privacy has recently emerged as an effective strategy for protecting both feature and label privacy in federated learning [1, 2]. \nThese methods demonstrate how a finely adjusted adversarial loss term that uses **original** private labels or features can mitigate privacy leakage, facilitating a nuanced privacy-utility trade-off without introducing additional privacy risks.\n\n**References:**\n\n[1] Jiankai Sun, Yuanshun Yao, Weihao Gao, Junyuan Xie, and Chong Wang. \"Defending against reconstruction attack in vertical federated learning.\" arXiv preprint arXiv:2107.09898 (2021).\n\n[2] Jingwei Sun, Zhixu Du, Anna Dai, Saleh Baghersalimi, Alireza Amirshahi, David Atienza, and Yiran Chen. \"Robust and IP-Protecting Vertical Federated Learning against Unexpected Quitting of Parties.\" arXiv preprint arXiv:2303.18178 (2023)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308436928,
                "cdate": 1700308436928,
                "tmdate": 1700308436928,
                "mdate": 1700308436928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fkY8wxReCJ",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 3 and 4"
                    },
                    "comment": {
                        "value": "### Question 3:\n\n> Is it possible to prove formal privacy guarantees, e.g. differential privacy for LPSC?\n\n**Answer**:\n\nWe appreciate the reviewer for concerning the privacy guarantee of the proposed privacy mechanism LPSC. \nWe provide a formal privacy guarantee with proof that LPSC satisfies $\\epsilon$-mutual information privacy ($\\epsilon$-MIP) in the **General Response Part 3** and the revised version Appendix B.3 \"Privacy Guarantee\". \nAs discussed in the \"MIP v.s. DP\" in the **General Response Part 4**, the choice of privacy definition $\\epsilon$-MIP is determined by our threat model, which is different from DP. \n\nWe restate the $\\epsilon$-MIP privacy guarantee for your convenience:\n\n> **Theorem 1** (Privacy Guarantee.)\n> LPSC satisfies $\\epsilon$-mutual information privacy ($\\epsilon$-MIP). The privacy leakage is bounded by $\\epsilon = H(p_{gt}(i,y)|p_{act}(i,y))$, the conditional entropy of the ground-truth label distribution $p_{gt}(i,y)$ given the active party's label distribution $p_{act}(i,y)$. Formally,\n> $$\n>    I(p_{gt}(i,y); p^*_{lpsc}(i,y)) \\leq \\epsilon \\text{ bits},\n> $$\n> where $p^*_{lpsc}(i,y)$ represents the optimal solution of the LPSC problem.\n\n>_Proof._ We approach the LPSC problem, optimizing $p_{lpsc}(i, y)$ with respect to:\n>$$\n>    p^*_{lpsc}(i,y) = \\text{arg} \\max_{p_{lpsc}(i,y)} I(p_{gt}(i, y); p_{lpsc}(i, y)) \n>$$\n>\n>$$\n>    s.t.  \\\\; I(p_{act}(i, y); p_{lpsc}(i, y)) = 0.  \n>$$\n>\n>The key constraint is that $p_{act}(i, y)$ and $p_{lpsc}(i, y)$ must remain independent, which implies that mutual information between \\( p_{gt}(i, y) \\) and \\( p_{lpsc}(i, y) \\) excludes any shared information with $p_{act}(i, y)$. Analytically, we express this as:\n>$$\n>    I(p_{gt}(i, y); p^*_{lpsc}(i, y)) \n>    =  I(p_{gt}(i, y); p^*_{lpsc}(i, y)|p_{act}(i, y)) \n>    \\leq  H(p_{gt}(i, y)|p_{act}(i, y))\n>   =  \\epsilon \\text{ bits},\n>$$\n>where $H(p_{gt}(i, y)|p_{act}(i, y))$ represents the conditional entropy, or the remaining uncertainty in $p_{gt}(i, y)$ after observing $p_{act}(i, y)$.\n>\n>Therefore, the solution $p^*_{lpsc}(i, y)$ satisfies the $\\epsilon$-MIP criterion, effectively bounding the mutual information and safeguarding label privacy in accordance with the $\\epsilon$-MIP definition.\n\n\n------\n\n\n### Question 4\n\n> In Appendix B.1, you mentioned \"we can use boosting to reduce the bias of the local model by using passive parties' auxiliary features.\" I am a bit confused, does LPSC use information from passive parties?\n\n**Answer**:\n\nWe apologize for our unclear writing.\nAs clarified in the **General Response Part 2** and the Appendix B in the updated version, LPSC is a type of privacy mechanism $M_{lpsc}$ that maps the active party's dataset distribution $p_{gt}(i,y,X_0)$, as defined by $D^{loc} = \\\\{i^{loc}, y^{loc}, X^{loc}_0\\\\}$ and generates a new joint distribution:\n\n$$\n p_{lpsc}(i,y) = M_{lpsc}(p_{gt}(i, y, X_0)).\n$$\n\nThis sentence depicts the __entire two-phase VFGBoost__ pipeline including: \n\"1) fitting labels $\\rightarrow$ 2) computing residuals  $\\rightarrow$ 3) fitting residuals\".\nHowever, LPSC only corresponds to the first *offline-phase* with two steps: \"1) fitting labels $\\rightarrow$ 2) computing residuals\", which **only uses information from the active party**. \nInstead, step 3 \"fitting residuals\" corresponds to the *federated training phase* and uses information from passive parties to fit residuals. \n\nIn the updated **Appendix C.1** \"Gradient Boosting\", we have revised this sentence as follows:\n\"Therefore, we can use boosting to reduce the bias of the local model by _training passive parties to fit weighted-residuals._\nThe predicted residuals are then added to the active party's local predictions to reduce the bias.\""
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309239364,
                "cdate": 1700309239364,
                "tmdate": 1700309239364,
                "mdate": 1700309239364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ovu691kpwr",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder regarding our response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the rebuttal period is approaching its end, we kindly remind you to review our submitted response. Your feedback is essential for finalizing our work. We would greatly appreciate any additional feedback you may have.\n\nThank you for your attention.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634299367,
                "cdate": 1700634299367,
                "tmdate": 1700634299367,
                "mdate": 1700634299367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "im9XbhSWRT",
                "forum": "2O2FOO8pl4",
                "replyto": "FMdOlomolc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_wj8U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_wj8U"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thorough rebuttal, which effectively tackles several of my concerns. I appreciate that the reference to Mutual Information Privacy (MIP) proves beneficial for your work. However, it appears that significant editing is required to seamlessly integrate MIP into your existing framework. Notably, MIP is not formally defined in the main text at present. Recognizing the constraints of time during the rebuttal period, I understand the challenges of extensively polishing the paper. In light of this, I suggest carefully revising the draft to incorporate reviewer comments and considering submission to other top-tier venues."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652491831,
                "cdate": 1700652491831,
                "tmdate": 1700652491831,
                "mdate": 1700652491831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XNAD64x9rP",
            "forum": "2O2FOO8pl4",
            "replyto": "2O2FOO8pl4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_3hcA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_3hcA"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an offline-phase data cleansing approach to protect label privacy without compromising utility. Specifically, the idea is to formulate a Label Privacy Source Coding (LPSC) problem to remove the redundant label information in the active party\u2019s features from labels, by assigning each sample a new weight and label (i.e., residual) for federated training. \n\nThe authors propose the Vertical Federated Gradient Boosting (VFGBoost) framework to address the LPSC problem with a theoretical guarantee. Moreover, given that LPSC only provides upper-bounded privacy enhancement, VFGBoost further enables a flexible privacy-utility trade-off by incorporating adversarial training during federated training. Experimental results on four real-world datasets substantiate the efficacy of LPSC and the superiority of our VFGBoost framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ A unique perspective from the offline phase and present an interesting idea.\n+ Provide theoretical guarantee and soundness.\n+ Provide comprehensive experiments conducted on four real-world datasets in practical scenarios, such as recommendation and healthcare."
                },
                "weaknesses": {
                    "value": "- Related works need to be improved. Section 2 contains many repeated contents.\n- The experimental findings could be more detailed."
                },
                "questions": {
                    "value": "1. The authors redefine privacy and introduce a privacy-utility trade-off. In the related work, the authors also mentioned differential privacy, which has a similar trade-off. Could the authors elaborate on the difference between them? \n2. How does adversarial training impact the privacy in your experimental findings? Could the authors explicate the insights/findings?\n3. The authors give a comprehensive analysis of newly defined privacy and its leakage. The idea is interesting since it introduces a new perspective on privacy. Actually, I feel a little confused about why Definition 1 and Definition 2 are required. What is the insight/intuition of privacy guarantee? What does the newly defined privacy essentially protect? How do you measure the privacy loss in practice/experiments? Why privacy leakage is defined as mutual information?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2638/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2638/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2638/Reviewer_3hcA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604176815,
            "cdate": 1698604176815,
            "tmdate": 1699636203829,
            "mdate": 1699636203829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gGeK5oR8HP",
                "forum": "2O2FOO8pl4",
                "replyto": "XNAD64x9rP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3hcA, Weakness 1 and Question 1, 2"
                    },
                    "comment": {
                        "value": "### Response to Reviewer 3hcA\n\nWe sincerely thank you for your insightful comments and concerns regarding privacy issues in our work. \nYour feedback has been invaluable in enhancing the clarity and depth of our analysis in these areas.\nWe believe that your queries about privacy issues are thoroughly addressed in our **General Response** and the detailed discussions in **Appendix B: \"Threat Model and Privacy Analysis\"** of the revised manuscript, where we have rigorously redefined our privacy model, refined our threat model, and provided rigorous privacy guarantees. \n\n\n--------\n\n### Weakness 1\n\n>Related works need to be improved. Section 2 contains many repeated contents.\n\n**Answer**:\n\nWe are grateful to Reviewer 3hcA for highlighting the issue of repetitive content in Section 2. We have carefully revised this section to remove redundancy and improve the flow of information. \n\n\n----------\n\n### Question 1\n\n> The authors redefine privacy and introduce a privacy-utility trade-off. In the related work, the authors also mentioned differential privacy, which has a similar trade-off. Could the authors elaborate on the difference between them?\n\n**Answer**:\n\nThank you for your inquiry about the distinctions between the privacy-utility trade-offs in our LPSC framework and differential privacy (DP). We answer this question from two aspects:\n\n**Comparison of Privacy Definitions:**\nWe have refined our manuscript to clarify that our privacy definition aligns with $\\epsilon$-Mutual Information Privacy ($\\epsilon$-MIP), as detailed in the updated Theorem 1 (**General Response, Part 3**).\nMoreover, we compare MIP and DP in the **General Response, Part 4**.\n\n**Comparison of Privacy-Utility Trade-offs:**\nThe VFGBoost framework achieves a privacy-utility balance through a two-phase process: 1) *offline-phase cleansing*, enhancing privacy by removing redundant label information, and 2) *training-phase perturbation*, adjusting the privacy-utility trade-off through controlled learning from perturbed data. This approach efficiently utilizes the redundant label information $p_{act}(i,y)$ to enhance utility without compromising privacy.\n\nIn contrast, DP primarily relies on adding noise to labels, a method that does not capitalize on redundant label information for utility improvement. Our framework's nuanced approach, therefore, offers a more tailored balance in VFL settings, as evidenced by the shift in trade-off curves (Figure 5), indicating improved privacy with minimal impact on utility.\n\n---------\n\n### Question 2\n\n> How does adversarial training impact the privacy of your experimental findings? Could the authors explicate the insights/findings?\n\n**Answer**:\n\n\n**Impact of Adversarial Training on Privacy:**\nAdversarial training in VFGBoost significantly enhances label privacy by neutralizing the label information in the LPSC-encoded $p_{lpsc}(i,y)$. This is achieved through a negative adversarial loss term in the overall objective, resulting in gradients that effectively eliminate sensitive label information. Consequently, the updated bottom model, trained with these neutralized gradients, preserves label privacy by not learning or retaining critical label knowledge.\n\n**Insights from Experimental Findings:**\nOur experimental results (Figure 5) confirm the effectiveness of this approach. VFGBoost's trade-off curves are positioned closer to the upper-left corner than those of LPSC+Marvell and LPSC+LabelDP, showcasing a better balance between privacy and utility. These findings underscore the advantages of adversarial training in the VFGBoost framework, demonstrating its capability to bolster privacy protection while maintaining utility in practical applications."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305578547,
                "cdate": 1700305578547,
                "tmdate": 1700310281134,
                "mdate": 1700310281134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rvEz36RYk6",
                "forum": "2O2FOO8pl4",
                "replyto": "XNAD64x9rP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 3"
                    },
                    "comment": {
                        "value": "We thank Reviewer `3hcA` for the insightful feedback on our privacy definitions. The questions raised have been thoroughly addressed in our **General Response**. Each sub-question related to privacy issues is methodically tackled in this comprehensive response, ensuring a clear understanding of our approach and its implications.\n\n\n--------\n\n### Q3.1: Definition 1 and Definition 2 are not required.\n\n**Answer**:\n\nUpon reflection and considering the valuable suggestions from Reviewers `3hcA`, `wj8U`, and `WYNH`, we have decided that Definitions 1 and 2 are indeed not essential for our framework. \nAccordingly, we have revised **Section 3** of our manuscript, focusing on rigorously defining the threat model and demonstrating how our LPSC approach aligns with $\\epsilon$-mutual information privacy ($\\epsilon$-MIP). This change streamlines our presentation and strengthens the theoretical foundation of our work.\n\n--------\n\n### Q3.2: What is the insight/intuition of privacy guarantee? \n\n**Answer**:\n\nWe appreciate your inquiry about the original privacy guarantee's intuition. \nRecognizing the need for a more rigorous approach, we revised our privacy guarantee to align with $\\epsilon$-mutual information privacy ($\\epsilon$-MIP), as detailed in **General Response Part 3**. This update shifts the focus to conditional entropy, reflecting a more precise measurement of privacy leakage.\n\n**Theorem** (Privacy Guarantee.) \nLPSC satisfies $\\epsilon$-mutual information privacy ($\\epsilon$-MIP). The privacy leakage is bounded by $\\epsilon = H(p_{gt}(i,y)|p_{act}(i,y))$, the conditional entropy of the ground-truth label distribution $p_{gt}(i,y)$ given the active party's label distribution $p_{act}(i,y)$. Formally,\n\n$$\n    I(p_{gt}(i,y); p^*_{lpsc}(i,y)) \\leq \\epsilon \\text{ bits},\n$$\n\nwhere $p^*_{lpsc}(i,y)$ represents the optimal solution of the LPSC problem.\n\n**Intuition:** The core intuition of the revised guarantee is that privacy leakage in LPSC is inversely related to the amount of label information the active party can infer from its local features. The more label information learned by the active party, the more effective LPSC is in preserving privacy for passive parties' bottom models.\n\n\n---------\n\n\n### Q3.3: What does the newly defined privacy essentially protect?\n\n**Answer**:\n\nIn our framework, the newly defined privacy mechanism LPSC, as delineated by our privacy guarantee, strategically decouples the private label information $p_{gt}(i,y)$ into two distinct parts. \nIt primarily focuses on protecting the label information $p_{act}(i,y)$ that the active party has learned from its local features. \nMeanwhile, LPSC releases the other component, $p_{lpsc}(i,y)$, which is the minimum-sufficient label knowledge to train passive parties. \n\n-------\n\n### Q3.4: How do you measure the privacy loss in practice/experiments? \n\n**Answer**:\n\nIn our experiments, privacy loss is quantified by employing various attack methods targeting the label information derived from the bottom models' forward embeddings. \nThis approach aligns with our threat model (referenced in **General Response, Part 1**), where the adversary aims to minimize the expected error using functions $A(\\cdot)$ from a set of attacks (e.g., Norm, PMC) as follows:\n\n$$\n    \\min_{A \\in \\mathbb{A}} R_{p_{gt}(i,y)}(A \\circ h_{\\psi_k}) = \\min_{A \\in \\mathbb{A}} E_{i \\sim p_{gt}(i)}[D_{KL}( p_{gt}(y|i) || A(h_{\\psi_k}(i)) )],\n$$\n\nPractically, rather than computing the expected KL-divergence directly, we assess the efficacy of these attacks by calculating the Label Leakage AUC score (LL-AUC) from the attack outcomes. Such LL-AUC metric aligns with the adversary's objective in our threat model. \n\n\n--------\n\n### Q3.5: Why privacy leakage is defined as mutual information? \n\n**Answer**:\n\nAs detailed in **General Response Part 2** and restated for convenience, we define privacy leakage as mutual information for specific reasons in our context. \n\nOur rationale for quantifying privacy leakage as mutual information is rooted in the specific adversary's objective in our threat model. \nLPSC involves a privacy mechanism $M_{lpsc}$ that transforms the active party's dataset distribution $p_{gt}(i, y, X_0)$ into a new joint distribution $p_{lpsc}(i,y)$. \nThis mechanism is designed to maximize **expected error** of label estimation, which we have determined is equivalent to minimizing the mutual information between $p_{gt}(i,y)$ and $p_{lpsc}(i,y)$. Formally, \n\n$$\n    \\max_{p_{lpsc}(i,y)} R_{p_{gt}(i,y)}(p_{lpsc}(i,y)) \\iff \\min_{p_{lpsc}(i,y)} I(p_{gt}(i,y); p_{lpsc}(i,y)),\n$$\n\nwhere $I(\\cdot; \\cdot)$ denotes the mutual information.\n\nThus, minimizing mutual information effectively enhances label privacy by maximizing the difficulty for an adversary to accurately infer sensitive label information."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307075818,
                "cdate": 1700307075818,
                "tmdate": 1700307075818,
                "mdate": 1700307075818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjvp3GZVs2",
                "forum": "2O2FOO8pl4",
                "replyto": "XNAD64x9rP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder regarding our response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the rebuttal period is approaching its end, we kindly remind you to review our submitted response. Your feedback is essential for finalizing our work. We would greatly appreciate any additional feedback you may have.\n\nThank you for your attention.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634238674,
                "cdate": 1700634238674,
                "tmdate": 1700634238674,
                "mdate": 1700634238674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7NpoWr82ox",
                "forum": "2O2FOO8pl4",
                "replyto": "xjvp3GZVs2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_3hcA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_3hcA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\nI would like to keep my rating."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639553474,
                "cdate": 1700639553474,
                "tmdate": 1700639553474,
                "mdate": 1700639553474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u0upqfSni4",
            "forum": "2O2FOO8pl4",
            "replyto": "2O2FOO8pl4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_pUxt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_pUxt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel framework to defend the data leakage problems in VFL, an important issue in collaborative learning scenarios.   The proposed LPSC approach is based on  mutual information optimization, which is solved by boosting approaches, and is able to reduce label leakage while preserving model utility in the evaluated experimental settings. Combination of LPSC and other methods also demonstrate promising utility-privacy trade-off."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposed a novel approach to defend label attacks in VFL settings and demonstrate experimentally the effectiveness of the method on multiple datasets.\n\n2. Theoretical analysis is conducted to guide the design of the framework. \n\n3. The proposed approach is able to achieve privacy protection without hurting model utility."
                },
                "weaknesses": {
                    "value": "1. Notations are inconsistent and confusing, making the paper hard to follow. There are many inconsistent notations such as X_0^loc and X_0, y^loc and y, i^loc and i, both denoting active party's data, p*_{lpsc} and p_{lpsc} etc. In Figure 3, it is unclear whether it illustrates the joint distribution or weight distribution. Eq.4 is also confusing, in which the federated model training is a function of i, the ID. The authors are suggested to proofread the entire manuscript to make notations easy to understand and consistent. \n\n2. Experimental evaluations are insufficient to support the main claims of the work. The comparison with other methods are only conducted  on PMC attack, when the adversarial loss of the proposed method is trained also against PMC attack. No comparison on other label leakage attacks or feature attacks are provided, whereas feature leakage attacks are also considered in the security definition. \n\n3. It appears that the effectiveness of proposed method depends on the quality of the local features of the active party. In the extreme case that no features are available on the active party but labels, the method may not apply. The impact of the importance of the local features of the active party needs to be evaluated. \n\n4. Some implementation details are missing. For example, how is LPSC+DP or LPSC+Marvell trained? why not LPSC+MID?"
                },
                "questions": {
                    "value": "1. How are importance of the local features affect the evaluation?\n\n2. How are the proposed method compared with other approaches on attacks other than PMC? \n\n3. Can you explain why it is reasonable to assume the conditional distribution to be the same in Theorem 2?\n\n4.how is LPSC+DP or LPSC+Marvell trained? why not LPSC+MID?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751050678,
            "cdate": 1698751050678,
            "tmdate": 1699636203734,
            "mdate": 1699636203734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UpQgWtijRK",
                "forum": "2O2FOO8pl4",
                "replyto": "u0upqfSni4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Notations are inconsistent and confusing, making the paper hard to follow. There are many inconsistent notations such as $X_0^{loc}$ and $X_0$, $y^{loc}$ and $y$, $i^{loc}$ and $i$, both denoting active party's data, $p^*_{lpsc}$ and $p_{lpsc}$ etc. In Figure 3, it is unclear whether it illustrates the joint distribution or weight distribution. Eq.4 is also confusing, in which the federated model training is a function of $i$, the ID. The authors are suggested to proofread the entire manuscript to make notations easy to understand and consistent.\n\n**Answer**:\n\nWe sincerely apologize for any confusion caused by the unclear writing and appreciate your valuable feedback. To clarify, the notations used are consistent and carefully defined in __Section 3__, Subsection *Vertical Federated Learning Setting*. We understand that the complexity of the subject might lead to some initial confusion, and we have conducted a thorough review and update of the manuscript to improve clarity and consistency in notations.\n\n1. **Local Data Notations**: We differentiate between aligned data $\\mathcal{D} = \\\\{\\mathbf{i}, \\mathbf{y}, \\mathbf{X}_0, \\ldots, \\mathbf{X}_K \\\\}$ and local data $\\mathcal{D}^{loc} = \\\\{\\mathbf{i}^{loc}, \\mathbf{y}^{loc}, \\mathbf{X}_0^{loc}\\\\}$, which includes both aligned samples in $\\mathcal{D}$ and extra unaligned local samples. We have clarified this in Figure 1(a) in the revised version.\n\n2. **Simplified Notations for Functions**: We use sample ID $i$ as a concise notation to represent feature vectors, such as $f_{\\theta}(i)$ for $f_{\\theta}(x_{0,i})$ and $h_{\\psi_k}(i)$ for $h_{\\psi_k}(x_{k,i})$. We restated the simplified notations in Equation (4) for clarity.\n\n3. **Optimal LPSC Solution**: The notation $p^*_{lpsc}$ represents the optimal solution to the LPSC problem, fulfilling specific criteria for mutual information. In Figure 3, we have revised the notations $p_{gt}, p_{act}, p^*_{lpsc}$ to clearly indicate $p_{gt}(i,y), p_{act}(i,y), p^*_{lpsc}(i, y)$, denoting $p^*_{lpsc}(i, y)$ represents the optimal joint distribution.\n\nWe extend our gratitude to the reviewer for this insightful feedback. Our revisions aim to not only rectify the noted issues but also to significantly improve the overall readability and understanding of our manuscript."
                    },
                    "title": {
                        "value": "Weakness 1"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302989910,
                "cdate": 1700302989910,
                "tmdate": 1700303008783,
                "mdate": 1700303008783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8NZ0yuhX05",
                "forum": "2O2FOO8pl4",
                "replyto": "u0upqfSni4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 2, the same as Question 2"
                    },
                    "comment": {
                        "value": "> How are the proposed method compared with other approaches on attacks other than PMC? Experimental evaluations are insufficient to support the main claims of the work. The comparison with other methods is only conducted on PMC attack, when the adversarial loss of the proposed method is trained also against PMC attack. No comparison on other label leakage attacks or feature attacks are provided, whereas feature leakage attacks are also considered in the security definition.\n\n**Answer**:\n\n> **Weakness 2.1: \"Comparisons against PMC attack are insufficient to support the main claims of the work.\"**\n\nWe gratefully acknowledge your valuable feedback on the scope of our evaluations in Section 5.3. \n\nWe would like to humbly clarify that the core focus of our research revolves around the **LPSC** mechanism, primarily aimed at bolstering label privacy in VFL through a novel two-phase process: 1) offline-phase cleansing and 2) training-phase perturbation. The experiments are designed to support the main claim that **LPSC-based two-phase framework is superior in enhancing label privacy in VFL**. While VFGBoost (i.e., LPSC+Adv. Training) is just one among several implementations of LPSC-based two-phase methods, including LPSC+DP and LPSC+Marvell.\n\nLPSC's effectiveness against various attacks is validated in Section 5.2. Specifically, in Section 5.3, the integration of LPSC in three perturbation methods (adversarial training, LabelDP, and Marvell) consistently outperforms baselines, validating LPSC's effectiveness. In other words, _even if replacing adversarial training in VFGBoost with other perturbations, the LPSC-based two-phase framework still outperforms baselines_.\n\nMoreover, the PMC attack is particularly threatening to our setting for two key reasons:\n\n1. **Prior knowledge.** The PMC attack assumes the adversary knows some labeled data **prior**, which is a strong adversary assumption. This contrasts with other attacks like Norm and Spectral, which do not assume prior knowledge, leading to heuristic attack functions. The superiority of the PMC attack is also confirmed in our experimental results in _Table 2_. \n\n2. **Tailored threat model.** PMC specifically targets label inference from forward embeddings, aligning closely with our focus on label privacy in VFL settings. While the other attacks were initially designed to attack labels from **gradients**, they are adopted to fit our setting.\n\nIn summary, while PMC was a primary focus due to its relevance and stringency, we acknowledge the value in broader attack comparisons and are actively exploring this in ongoing research.\n\n**Weakness 2.2: Feature Attack**: \n\nRegarding feature privacy, we have evaluated the protection against the Model Inversion attack in **Appendix G.3**. While not our primary focus, LPSC indirectly enhances feature privacy by enabling the use of lower-dimensional embeddings, thus reducing potential feature information leakage."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303747879,
                "cdate": 1700303747879,
                "tmdate": 1700565193193,
                "mdate": 1700565193193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eqBaoVGqHn",
                "forum": "2O2FOO8pl4",
                "replyto": "u0upqfSni4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 4 and Question 3"
                    },
                    "comment": {
                        "value": "### Weakness 4, the same as Question 4\n\n> Weakness 4: Some implementation details are missing. For example, how is LPSC+DP or LPSC+Marvell trained? Why not LPSC+MID?\n>\n> Question 3: How is LPSC+DP or LPSC+Marvell trained? Why not LPSC+MID?\n\n**Answer**:\n\nWe appreciate your query on the implementation details of LPSC+LabelDP and LPSC+Marvell. Both methods and our VFGBoost are grounded in our key insight that label privacy protection in VFL should be a two-phase process: 1) **offline-phase cleansing** through **LPSC**, enhancing privacy without utility compromise, and 2) **training-phase perturbation** for further privacy-utility balancing through various perturbation methods (e.g., Adv. training, LabelDP, Marvell, etc.)\n\nFor LPSC+LabelDP and LPSC+Marvell, the active party first conducts offline-phase cleansing via LPSC, then proceeds to federated training with LPSC-encoded results. LPSC+LabelDP uses the Multi-stage Training algorithm in [1] with modified sample weights and labels, while LPSC+Marvell employs the Marvell algorithm [3] with LPSC-encoded weights.\n\nRegarding LPSC+MID, the MID [3] approach **inherently integrates cleansing and perturbation** in one step during training, using a mutual information regularization term. This makes MID a parallel baseline to the combined approaches like AdaBoost (i.e., LPSC+adversarial training), LPSC+DP, and LPSC+Marvell, rather than a complementary method that can be combined with LPSC.\n\n**References**:\n\n[1] Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang. \"Deep learning with label differential privacy.\" Advances in neural information processing systems 34 (2021): 27131-27145.\n\n[2] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and Chong Wang. \"Label leakage and protection in two-party split learning.\" arXiv preprint arXiv:2102.08504 (2021).\n\n[3] Tianyuan Zou, Yang Liu, and Ya-Qin Zhang. \"Mutual Information Regularization for Vertical Federated Learning.\" arXiv preprint arXiv:2301.01142 (2023).\n\n\n-------\n\n### Question 3\n\n> Can you explain why it is reasonable to assume the conditional distribution to be the same in Theorem 2?\n\n**Answer**:\n\nWe appreciate your query regarding the assumption of a constant conditional distribution in Theorem 2. This assumption simplifies the LPSC problem to align with AdaBoost for theoretical analysis, but it is **not a strict requirement for all scenarios**, such as those involving LogitBoost and L2-Boost.\n\nWhen optimizing both the marginal (weights) and conditional (residuals) distributions, the LPSC problem aligns with LogitBoost, as demonstrated in our updated version with the following formulation:\n\n$$\n    p_{lpsc}(i) = \\frac{f_{\\theta}(i)(1 - f_{\\theta}(i))}{\\sum_{j \\in \\mathbf{i}} f_{\\theta}(j)(1 - f_{\\theta}(j))}, \\\\; \\\\;\n    p_{lpsc}(y|i) = \\frac{y_i - f_{\\theta}(i)}{f_{\\theta}(i)(1-f_{\\theta}(i))}.\n$$\n\nOur experimental results (Table 3, Section 5.4) further confirm that optimizing both components in LogitBoost yields superior privacy protection compared to AdaBoost, which holds the conditional distribution constant."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700304601485,
                "cdate": 1700304601485,
                "tmdate": 1700304773167,
                "mdate": 1700304773167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pyy5EA8qvp",
                "forum": "2O2FOO8pl4",
                "replyto": "u0upqfSni4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder regarding our response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the rebuttal period is approaching its end, we kindly remind you to review our submitted response. Your feedback is essential for finalizing our work. We would greatly appreciate any additional feedback you may have.\n\nThank you for your attention.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634183642,
                "cdate": 1700634183642,
                "tmdate": 1700634183642,
                "mdate": 1700634183642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQ2eBGAmT9",
                "forum": "2O2FOO8pl4",
                "replyto": "8NZ0yuhX05",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_pUxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Reviewer_pUxt"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the authors' responses, which addressed some of my concerns. The experimental evaluations are still limited, as comparisons against other baselines are still only performed on one attack whereas various label and feature attacks have been proposed in VFL setting in recent years. I think the paper will be much stronger if it demonstrates its superiority over other methods on various attacks.  The writing of the final draft has been improved overall, but is still notation heavy and not easy to follow. It also introduced new questions and confusions. For example, it appears that there are unaligned data samples which result in the different notations in X^loc and X. However the significance or reason of this assumed setting is not explained. It is also not clear in the experiments how these unaligned samples are set. In summary, I think this paper still needs improvement in experimental evaluations and presentation."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645850822,
                "cdate": 1700645850822,
                "tmdate": 1700645850822,
                "mdate": 1700645850822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8uP2TdlA2I",
            "forum": "2O2FOO8pl4",
            "replyto": "2O2FOO8pl4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_WYNH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2638/Reviewer_WYNH"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach to enhancing label privacy in vertical federated learning (VFL). The paper first introduces a privacy notion based on the concept of mutual information, then formalizes the privacy protection task into the Label Privacy Source Coding (LPCS) problem, and demonstrates that gradient boosting is a suitable method for solving the LPCS problem. Subsequently, the Vertical Federated Gradient Boosting framework is proposed to efficiently optimize the LPCS problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed solution is interesting.\n\n2. The experimental evaluation is extensive."
                },
                "weaknesses": {
                    "value": "1. The new privacy definition needs to be more formally and rigorously defined. The adversary model and the privacy guarantee provided by the new privacy definition should be clearly delineated. Without a clear description of the adversary's capabilities, the privacy definition's theoretical foundations remain unclear. \n\n2. The paper does not provide a formal analysis of the new privacy definition's properties. In particular, it remains unclear whether this definition adheres to the axioms laid out in https://www.cse.psu.edu/~duk17/papers/axioms.pdf. The paper would be greatly strengthened by a discussion on this aspect, identifying any axioms that are not met and providing a reasoned argument for why such deviations are acceptable within the context of the problem being tackled.\n\n3. An in-depth discussion of why VFGBoost surpasses label DP would be beneficial."
                },
                "questions": {
                    "value": "See the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2638/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889035647,
            "cdate": 1698889035647,
            "tmdate": 1699636203647,
            "mdate": 1699636203647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VcxOLhaqhC",
                "forum": "2O2FOO8pl4",
                "replyto": "8uP2TdlA2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 1"
                    },
                    "comment": {
                        "value": "> The new _privacy definition_ needs to be more formally and rigorously defined. \nThe _adversary model_ and the _privacy guarantee_ provided by the new privacy definition should be clearly delineated. \nWithout a clear description of the adversary's capabilities, the privacy definition's theoretical foundations remain unclear.\n\n__Answer__:\n\nThank you for your insightful comments regarding the need for a more formal and rigorous definition of our privacy model. Your feedback has been instrumental in enhancing the clarity and depth of our manuscript. In response to your concerns, as well as those echoed by other reviewers, we have comprehensively addressed the threat model, privacy definition, and privacy guarantee in the **General Response**. \n\n1. **Threat Model:**  \nFor a detailed description of the threat model, which forms the cornerstone of our privacy approach, please refer to **General Response, Part 1**.\n\n2. **Privacy Definition:**  \nWe acknowledge the previous ambiguity in our terminology and have rectified this in the revised manuscript. The term \"private label information,\" previously referred to as the privacy definition, is now clearly defined as $p_{gt}(i,y)$ to prevent any confusion. This clarification is elaborated upon in **General Response, Part 2**. Our choice of privacy definition, $\\epsilon$-MIP, is directly informed by our threat model and is rigorously defined and justified within our framework.\n\n3. **Privacy Guarantee:**  \nFurthermore, we have substantiated our approach with a formal privacy guarantee, demonstrating that our LPSC mechanism satisfies $\\epsilon$-MIP, as detailed in **General Response, Part 3**.\n\nAccordingly, we have revised **Section 3** of our manuscript to provide a clearer exposition of the threat model and privacy definition. We have also introduced **Appendix B: \"Privacy Threats and Analysis\"** which offers an in-depth exploration of these concepts, including the threat model (B.1), the formal definition of $\\epsilon$-Mutual Information Privacy ($\\epsilon$-MIP) as our privacy definition (B.2), a detailed privacy guarantee with proof (B.3), and a comparative analysis of MIP and Differential Privacy (DP) (B.4)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300387984,
                "cdate": 1700300387984,
                "tmdate": 1700300387984,
                "mdate": 1700300387984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MbgpPxRSW7",
                "forum": "2O2FOO8pl4",
                "replyto": "8uP2TdlA2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 2"
                    },
                    "comment": {
                        "value": ">  The paper does not provide a formal analysis of the new privacy definition's properties. \nIn particular, it remains unclear whether this definition adheres to the axioms laid out in [1]. \nThe paper would be greatly strengthened by a discussion on this aspect, identifying any axioms that are not met and providing a reasoned argument for why such deviations are acceptable within the context of the problem being tackled.\n\n>[1] Daniel Kifer, and Bing-Rong Lin. \"An axiomatic view of statistical privacy and utility.\" Journal of Privacy and Confidentiality 4, no. 1 (2012).\n\n**Answer**:\n\nWe are grateful to receive the valuable suggestion to examine our privacy definition against the axioms outlined in [1]. This has enabled us to further solidify the theoretical underpinnings of our work.\n\nRecall that, in response to Weakness 1, we have clarified that $p_{gt}(i,y)$ is now referred to as **private label information** in our revised manuscript, enhancing clarity.\n\nOur Label Privacy Source Coding (LPSC) corresponds to a category of privacy mechanism that satisfies the privacy definition of **$\\epsilon$-Mutual Information Privacy ($\\epsilon$-MIP)**, as proved in General Response Part 3. As per the perspective in [1], a privacy definition encompasses a set of privacy mechanisms. Thus, LPSC, with its underlying boosting algorithms (such as AdaBoost, and LogitBoost), can be seen as a privacy definition with specific mechanisms for implementation.\n\nUpon reviewing the axioms in [1], we confirm that LPSC satisfies the following axioms:\n\n**Axiom 1: Transformation Invariance**. This axiom posits that the privacy of sanitized data is maintained through post-processing, provided no sensitive information is directly utilized. __LPSC complies with Axiom 1__ as the post-processing in our approach (specifically, the federated training phase in Algorithm 1, lines 3-9) exclusively utilizes LPSC-encoded residuals and does not directly access ground-truth label information. Consequently, adversarial passive parties cannot glean additional label privacy during federated training beyond what is revealed through LPSC-encoded results.\n\n**Axiom 2: Convexity**. This axiom suggests that the choice of privacy mechanism should be independent of the actual input data. Thereby, a privacy mechanism that randomly selects between two privacy mechanisms also satisfies the privacy definition. \n__LPSC adheres to Axiom 2__ because the selection of specific boosting algorithms (e.g., AdaBoost or LogitBoost) as privacy mechanisms is not influenced by the input data. The choice between these algorithms is arbitrary and does not depend on the nature of the data being processed, thus satisfying the convexity criterion.\n\n-------\n\n**Reference**:\n\n[1] Daniel Kifer, and Bing-Rong Lin. \"An axiomatic view of statistical privacy and utility.\" Journal of Privacy and Confidentiality 4, no. 1 (2012)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300683805,
                "cdate": 1700300683805,
                "tmdate": 1700300683805,
                "mdate": 1700300683805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G1KvmVIDsI",
                "forum": "2O2FOO8pl4",
                "replyto": "8uP2TdlA2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 3"
                    },
                    "comment": {
                        "value": "> An in-depth discussion of why VFGBoost surpasses label DP would be beneficial.\n\n**Answer**:\n\nThank you for the opportunity to discuss the advantages of our VFGBoost framework over LabelDP [1] in the context of VFL. While LabelDP provides a robust solution for **centralized deep learning** settings by partitioning the centralized training data into multiple subsets and adding noise to labels in multiple stages, it doesn't cater to the unique characteristics of VFL. Specifically, algorithms proposed in LabelDP [1] do not explicitly address scenarios where an active party possesses labeled local data, and passive parties have auxiliary features without labels.\n\nIn contrast, VFGBoost is designed explicitly for VFL. It utilizes Label Privacy Source Coding (LPSC) to optimally leverage this unique data distribution structure. By first computing and releasing LPSC-encoded weighted residuals, VFGBoost ensures efficient and tailored label privacy protection. Additionally, it trains the model on perturbed datasets, allowing for a flexible balance between privacy and utility. This specific alignment with VFL's requirements, combined with our framework's practical efficiency and adaptability, positions VFGBoost as a superior alternative to LabelDP in VFL environments.\n\n--------\n\n**Reference**:\n\n[1] Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang. \"Deep learning with label differential privacy.\" Advances in neural information processing systems 34 (2021): 27131-27145."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301457036,
                "cdate": 1700301457036,
                "tmdate": 1700301457036,
                "mdate": 1700301457036,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wozgjyNE7X",
                "forum": "2O2FOO8pl4",
                "replyto": "8uP2TdlA2I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2638/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder regarding our response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the ICLR discussion period nears its conclusion, we kindly remind you to review our submitted response. Your feedback is essential for finalizing our work. We would greatly appreciate any additional feedback you may have.\n\nThank you for your attention. \n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2638/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634099065,
                "cdate": 1700634099065,
                "tmdate": 1700634099065,
                "mdate": 1700634099065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]