[
    {
        "title": "AutoM3L: Automated Multimodal Machine Learning with Large Language Model"
    },
    {
        "review": {
            "id": "t5KGvGxnez",
            "forum": "IRtIHp7vsM",
            "replyto": "IRtIHp7vsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_HozN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_HozN"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework for applying AutoML in a multimodal setup using Large Language Models. The system comprises several stages: 1) modality inference, 2) automated feature engineering, 3) model selection, 4) pipeline assembly and 5) hyperparameter optimization. The authors divide the experiment section in 2 parts: 1) quantitative evaluation, 2) user study"
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem is important as it is very common to find different use cases where many modalities are available for the prediction. Moreover, there are not many tools that aim to solve this problem, directly, so far."
                },
                "weaknesses": {
                    "value": "- The paper is very hard to follow, with many different acronyms, stages, and components. At some points, it gives the impression to be a technical report of a very complex software, rather than a scientific paper introducing a novel method.\n- Lack of strong benchmarking: the authors compare with only AutoGluon (one method) in four datasets. Although I understand that there are not many tools, the authors should include more datasets, and demonstrate that the tool also performs relatively well in uni-modal cases. Moreover, a valid baseline would be to aggregate the predictions of models that are obtained after optimizing per mode type.\n- The authors do not report standard deviation to assess the significance of the results. In most of the experiments, the improvement is very small."
                },
                "questions": {
                    "value": "- Could the authors elaborate on the time, hardware, and/or price needed for the execution? From my perspective, using an LLM for AutoML seems still very impractical, as it demands a lot of hardware, which many final users can probably not afford."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2600/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697899128146,
            "cdate": 1697899128146,
            "tmdate": 1699636197946,
            "mdate": 1699636197946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rWIymx91v2",
                "forum": "IRtIHp7vsM",
                "replyto": "t5KGvGxnez",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Your comprehensive review and astute suggestions are truly appreciated and we're thankful for the guidance you've provided to improve our research.\n\n**W1:**\n\nThank you for your feedback. We'd like to clarify that our paper aligns with the scope of papers accepted by ICLR, which includes articles similar to technical reports [1]. In our submission, we not only introduce an LLM-driven multi-modal AutoML framework but also propose a systematic set of evaluation metrics. This encompasses both objective quantitative analysis and subjective user experiment evaluation metrics. We approach the design of our AutoML system from the perspective of Human-Computer Interaction, conducting human-machine interaction experiments to showcase the framework's usability and intelligence. These aspects go beyond the typical content found in a standard technical report.\n\n[1] Rogozhnikov A. Einops: Clear and reliable tensor manipulations with Einstein-like notation [C]//International Conference on Learning Representations. 2021.\n\n**W2:**\n\nDue to the current scarcity of open-source multi-modal datasets encompassing images, text, and tabular data, we proactively gathered four datasets covering classification, regression, and retrieval tasks. While the number of open multi-modal datasets is limited, these datasets accurately mirror real-world business scenarios where various modalities coexist. Examples include product images paired with textual descriptions and categorical/numerical information, as well as financial datasets storing user photos, names, ages, addresses, transaction details, credit ratings, and more.\n\nOur decision to exclusively compare with AutoGluon in the paper was intentional, aiming to highlight the efficacy and intelligence of our approach in handling multi-modal data. In multi-modal scenarios, the ability to distinguish between different modal inputs and adapt suitable data preprocessing methods and models becomes indispensable. The overall framework design becomes more intricate and challenging, underscoring the importance of introducing AutoML in such contexts. Single-modal experiments were omitted from the paper as AutoGluon has demonstrated optimal accuracy in single-modal scenarios [2].  We have also benchmarked AutoM3L against H2O AutoML on the single-tubular modality datasets from OpenML(https://www.openml.org/), which cover regression and binary/multiclass classification tasks. The results demonstrate AutoM3L's strong performance even in single-modality settings.\n\n| Task            | Type       | Metric  | H2O AutoML   | AutoM3L       |\n| --------------- | ---------- | ------- | ------------ | ------------- |\n| Australian      | binary     | auc     | 0.934(0.024)  | 0.943(0.022)  |\n| wilt            | binary     | auc     | 0.994(0.007)  | 0.996(0.006)  |\n| numerai28_6     | binary     | auc     | 0.532(0.005)  | 0.532(0.004)  |\n| phoneme         | binary     | auc     | 0.967(0.009)  | 0.955(0.008)  |\n| credit-g        | binary     | auc     | 0.798(0.033)  | 0.805(0.04)   |\n| APSFailure      | binary     | auc     | 0.992(0.002)  | 0.991(0.004)  |\n| jasmine         | binary     | auc     | 0.884(0.018)  | 0.862(0.015)  |\n| yeast           | multiclass | logloss | 1.058(0.094)  | 1.034(0.1123) |\n| dionis          | multiclass | logloss | 3.351(0.120)  | 0.2923(0.005) |\n| jannis          | multiclass | logloss | 0.669(0.006)  | 0.6701(0.006) |\n| Diabetes130US   | multiclass | logloss | 0.833(0.006)  | 0.842(0.0058) |\n| eucalyptus      | multiclass | logloss | 0.689(0.052)  | 0.676(0.058)  |\n| Moneyball       | regression | rmse    | 22(2.2)       | 22(0.84)      |\n| diamonds        | regression | rmse    | 5.1e+02(18)   | 5.2e+02(25)   |\n| Yolanda         | regression | rmse    | 8.8(0.041)    | 8.7(0.047)    |\n\nThese additional experiments reinforce that in addition to handling multimodal data, AutoM3L also serves as an effective general-purpose AutoML solution.\n\nIt's crucial to emphasize that our primary focus is not on accuracy improvement alone. The key contribution of our method lies in elevating the automation level and user-friendliness of the pipeline. This aspect transcends dataset accuracy measurement. Therefore, we designed user experiments, evaluated from a human perspective through human-machine interaction. The experiments substantiated that our framework exhibits higher usability.\n\n[2] Gijsbers P, Bueno M L P, Coors S, et al. Amlb: an automl benchmark[J]. arXiv preprint arXiv:2207.12560, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605030840,
                "cdate": 1700605030840,
                "tmdate": 1700605030840,
                "mdate": 1700605030840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MUa4KwxwHV",
            "forum": "IRtIHp7vsM",
            "replyto": "IRtIHp7vsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_Lpkx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_Lpkx"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study using LLMs for multimodal AutoML. Specifically, the authors propose AutoM3L, which can automate ML for multimodal data using natural language instructions, covering automated pipeline construction, automated feature engineering, automated hyper-parameter optimization, etc. Experimental results showcase the usage of the proposed method over AutoGluon baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) Exploring the potential of LLMs for multimodal AutoML is an interesting unexplored direction.  \n(2) The proposed method (or system) can leverage natural languages in the pipeline, enhancing user-friendly.  \n(3) The authors have conducted user studies for the proposed method.   \n(4) The authors have provided the source codes for reproduction and showcases."
                },
                "weaknesses": {
                    "value": "(1) This paper neglects neural architecture search (NAS), which is one of the most important components in AutoML, if not the single most important one, especially in the deep learning era. There exist many multimodal NAS methods, which should be compared or added into the proposed system. Actually, I find such negligence kind of surprising, considering that NAS has received more attention than other AutoML techniques nowadays.  \n(2) Experiments are somewhat weak considering essentially only AutoGluon is compared. Though other methods may focus on a certain aspect of multimodal AutoML, e.g., HPO, the authors need to properly compare with them.  \n(3) Since the proposed AutoM3L is more like a library/system than a technical method, I would suggest adding more documentation, tutorials, etc., to help users get familiar with the system.  \n(4) Though LLMs have been constantly improving in their abilities to follow instructions, I wonder how the uncertainty and fragileness in LLMs may potentially have on the system. This is especially important if the proposed system is applied in real production scenarios."
                },
                "questions": {
                    "value": "See Weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2600/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646001496,
            "cdate": 1698646001496,
            "tmdate": 1699636197830,
            "mdate": 1699636197830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y3727s3Fd7",
                "forum": "IRtIHp7vsM",
                "replyto": "MUa4KwxwHV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your insightful and detailed review. Your thoughtful suggestions are instrumental in refining our research, and we're grateful for the expertise you bring to the reivew process.\n\n**W1:**\n\nNAS is a pivotal component of AutoML that has garnered significant attention in recent years. In AutoM3L, we match specific modal inputs with corresponding pretrained models and merge multi-modal features. We consider NAS as an extensible aspect of our framework for the following reasons:\n\nA. Scalability through model_zoo:\n\nConsidering the extensibility of the model_zoo we introduce, we can describe different neural network structures in the NAS search space as corresponding model_cards. For instance, different-scale convolutional layers, dilated convolutions, etc. In our framework, user requirements, including training resources, deployment devices, etc., can interact with LLM in natural language. This prompts LLM to search and assemble structures in the search space based on prompt engineering.\n\nB. HPO-LLM in AutoM3L:\n\nIn AutoM3L, we have designed HPO-LLM to infer the hyperparameter search space based on the training configuration file. The configuration file inputted into LLM can also include model configuration containing parameters such as num_trans_blocks, num_atte_head, ffn_dropout, etc. Leveraging the characteristics of HPO-LLM, we can infer the search space for model parameters and search them alongside other training hyperparameters (e.g., learning rate).\n\n**W2:**\n\nWe supplemented the comparison experiment with autokeras framework in multimodal scenarios. Autokeras framework is an earlier work,  but the multi-modal training is a recently launched feature. Considering that Autokeras requires manual predefinition of  data types, which is different from our task of parsing structured table data, we did not compare with it before.\n\n| Method               | PAP\u2191   | PPC\u2193    | GMRR\u2191  | SPMG\u2191  |\n| -------------------- | ------ | ------- | ------ | ------ |\n| AutoGluon w/o HPO    | 0.4121  | 1.0129   | 0.4091  | 0.9851  |\n| AutoGluon w/ HPO     | 0.4455  | 1.0128   | 0.4272  | 0.9894  |\n| AutoKeras            | 0.3808  | 1.1743   | 0.3542  |   -     |\n| AutoM3L              | 0.4435  | 1.0118   | 0.4499  | 0.9903  |\n\nWe only conducted classification and regression experiments as AutoKeras does not support retrieval tasks. The primary focus of AutoKeras is on searching for network structures. In our analysis, we attribute the lower accuracy of AutoKeras to the network structures obtained within its limited search space, which lacks pretraining on large-scale datasets. In contrast, our approach leverages the strength of pretrained models by linking with open-source communities such as HuggingFace and Timm. This integration allows us to access more powerful pretrained models, contributing to the improved performance demonstrated in our work.\n\nBesides, we have also benchmarked AutoM3L against H2O AutoML on the single-tubular modality datasets from OpenML(https://www.openml.org/), which cover regression and binary/multiclass classification tasks. The results demonstrate AutoM3L's strong performance even in single-modality settings.\n| Task            | Type       | Metric  | H2O AutoML   | AutoM3L       |\n| --------------- | ---------- | ------- | ------------ | ------------- |\n| Australian      | binary     | auc     | 0.934(0.024)  | 0.943(0.022)  |\n| wilt            | binary     | auc     | 0.994(0.007)  | 0.996(0.006)  |\n| numerai28_6     | binary     | auc     | 0.532(0.005)  | 0.532(0.004)  |\n| phoneme         | binary     | auc     | 0.967(0.009)  | 0.955(0.008)  |\n| credit-g        | binary     | auc     | 0.798(0.033)  | 0.805(0.04)   |\n| APSFailure      | binary     | auc     | 0.992(0.002)  | 0.991(0.004)  |\n| jasmine         | binary     | auc     | 0.884(0.018)  | 0.862(0.015)  |\n| yeast           | multiclass | logloss | 1.058(0.094)  | 1.034(0.1123) |\n| dionis          | multiclass | logloss | 3.351(0.120)  | 0.2923(0.005) |\n| jannis          | multiclass | logloss | 0.669(0.006)  | 0.6701(0.006) |\n| Diabetes130US   | multiclass | logloss | 0.833(0.006)  | 0.842(0.0058) |\n| eucalyptus      | multiclass | logloss | 0.689(0.052)  | 0.676(0.058)  |\n| Moneyball       | regression | rmse    | 22(2.2)       | 22(0.84)      |\n| diamonds        | regression | rmse    | 5.1e+02(18)   | 5.2e+02(25)   |\n| Yolanda         | regression | rmse    | 8.8(0.041)    | 8.7(0.047)    |\n\nWe agree with the reviewer that accuracy is not the only crucial metric. AutoM3L's critical advantage lies in enabling the entire ML pipeline to be automated through natural language interaction. This substantially reduces the manual effort and learning curve for users, as validated quantitatively in our user studies. Such intuitive human-AI interaction and usability are lacking in other AutoML approaches."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603674794,
                "cdate": 1700603674794,
                "tmdate": 1700603674794,
                "mdate": 1700603674794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qYMOI5tAAS",
                "forum": "IRtIHp7vsM",
                "replyto": "MUa4KwxwHV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Reviewer_Lpkx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Reviewer_Lpkx"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in the rebuttal, which helped to improve the paper's quality. However, based on the current form, I decide to keep my score and encourage the authors to further improve the work, e.g., by realizing the incorporation of NAS more thoroughly (e.g., differential NAS such as DARTS is more complicated and the proposed approaches in the rebuttal seem not to be able to incorporate) and packing the proposed library in a more mature format."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641965675,
                "cdate": 1700641965675,
                "tmdate": 1700642327364,
                "mdate": 1700642327364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BHGIYGWNuJ",
            "forum": "IRtIHp7vsM",
            "replyto": "IRtIHp7vsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_4U2h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_4U2h"
            ],
            "content": {
                "summary": {
                    "value": "In the paper \"AutoM3L: Automated Multimodal Machine Learning with Large Language Model\", the authors present an AutoML approach based on large language models to tackle multi-modal learning tasks. In their study, they compare their approach to AutoGluon, a state-of-the-art AutoML tool that is also able to tackle multi-modal datasets, achieving competitive performance. Furthermore, a user study is conducted to compare AutoM3L to AutoGluon in terms of the time required for learning the handling of the framework, the accuracy of user actions, the usability of the framework, and the user workload."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel paradigm for designing complex AutoML tools based on LLMs\n- Competitive performance to AutoGluon across different types of tasks that exhibit multi-modality\n- User study to test the AutoML tools with respect to their usability"
                },
                "weaknesses": {
                    "value": "- Tiny scope of datasets and it appears that only a single train test split has been used for the evaluation\n- No significance test is applied to the evaluation results with respect to the performances and standard deviations for repetitions are missing.\n- Only single runs of the AutoML tools are considered. However, AutoML tools are known to be quite noisy, so repeated runs would be required to tell how stable the performances are.\n- The participants are not fully described in terms of their priming regarding the tools etc and detailed background. In particular, no previous experiences with LLMs or other AutoML tools are mentioned.\n- Ablation studies regarding the effect of the different modules are lacking.\n- Limitations should be elaborated more, in particular, what are the pitfalls of AutoM3L and how to deal with biases contained in LLMs? E.g., gender or racial biases? To what extent is a corresponding bias even endangering the usage of AutoM3L?"
                },
                "questions": {
                    "value": "- How stable are the performances obtained by the AutoML tools?\n- What is the background of the study participants? To what extent did they already touch on LLMs and AutoML or HPO tools beforehand? To what extent are they already capable of handling multi-model data on their own?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "LLMs, especially GPT3.5, are known to have issues with biases, e.g., gender or ethnical biases. The impact of such biases on the overall approach is not addressed in the paper. From my point of view, when automating machine learning tasks based on LLMs, such biases should at least be acknowledged and discussed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2600/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760461728,
            "cdate": 1698760461728,
            "tmdate": 1699636197737,
            "mdate": 1699636197737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kC7FvKYrhi",
                "forum": "IRtIHp7vsM",
                "replyto": "BHGIYGWNuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We are grateful for your meticulous evaluation and constructive suggestions. Your professional perspectives have played a pivotal role in improving the depth and precision of our research.\n\n**W1:**\n\nWe appreciate the reviewer raising this valuable point about the scope of our evaluation datasets and validation methodology. In this work, our key focus was introducing and demonstrating the capabilities of our novel AutoM3L framework on representative multimodal datasets, rather than an exhaustive benchmarking across a wide array of datasets. Nonetheless, we recognize the merit of more comprehensive evaluations.\n\nRegarding the datasets, we strategically selected four multimodal datasets that cover common tasks like classification, regression, and retrieval. These datasets exhibit diversity in terms of size, modality types, labels, and complexity. While not exhaustive, we believe they enable reasonable assessments of AutoM3L's automation capabilities.\n\nWe agree that using a single train-test split limits robustness. Ideally, techniques like k-fold cross-validation would enable more rigorous performance validation. Hence, we have now incorporated 10-fold CV experiments on all datasets:\n\n| Method            | PAP\u2191         | PPC\u2193          | GMRR\u2191         | SPMG\u2191            |\n| ----------------- | ------------ | ------------- | ------------- | ---------------- |\n| AutoGluon w/o HPO | 0.4149(0.011) | 0.9987(0.035) | 0.3945(0.038) | 0.9846(0.0033)   |\n| AutoGluon w/ HPO  | 0.4421(0.008) | 0.9932(0.021) | 0.4122(0.021) | 0.9901(0.0023)   |\n| AutoM3L           | 0.4402(0.012) | 0.9941(0.037) | 0.4332(0.035) | 0.9921(0.0028)   |\n\nTo compensate for dataset size limitations, we employed stratified sampling to create train-validation-test splits that preserve label distributions. We additionally introduced perturbations like random feature masking and noisy data injection to further stress test AutoM3L.\n\n**W3:**\n\nYour suggestion makes a lot of sense. In our framework, randomness primarily stems from LLM inference and hyperparameter search.\n\nIn the hyperparameter search experiment, we combined the search space recommended by HPO-LLM with the ray.tune tool, which supports repeated experiments to explore the most suitable hyperparameters. We conducted 256 repetitions in our hyperparameter search experiment.\n\nIn addition, the 10 fold cross-validation table in W1 indicates the relative stability of our framework. Regarding the stability assessment of MS-LLM, we conducted the following experiment:\n\n**Exp Setting:** We utilized GPT3.5 to generate 10 sentences expressing the same user command in different ways. The objective was to examine whether MS-LLM could successfully retrieve the appropriate model. \n\n| id | sentences                                                                                                  |\n|---------|--------------------------------------------------------------------------------------------------------|\n| 1       | I hope to see the model efficiently running on mobile devices, optimizing for lightweight performance. |\n| 2       | The model's deployment on CPU devices, especially on lightweight and mobile platforms, is my preference. |\n| 3       | My goal is to have the model effectively deployed on CPU devices, with a focus on mobile and lightweight configurations. |\n| 4       | It would be great to have the model running seamlessly on various CPU devices, prioritizing mobility and lightweight hardware. |\n| 5       | I'm aiming for the model to be deployed on specific CPU hardware, emphasizing mobility and lightweight characteristics. |\n| 6       | Optimizing the model for mobile platforms and ensuring efficient operation on CPU devices aligns with my preferences. |\n| 7       | The deployment of the model on CPU devices, particularly on lightweight and mobile configurations, is my desired outcome. |\n| 8       | I'm specifically interested in the model's deployment on CPU devices, emphasizing efficiency and suitability for mobile platforms. |\n| 9       | My preference is for the model to be tailored for deployment on CPU devices, with a keen focus on mobile and lightweight capabilities. |\n| 10      | Ensuring the model's inference speed on CPU devices, especially in mobile and lightweight scenarios, is my priority. |\n\n**Results:**\nAll 10 sentences were successfully indexed to Model:{\"google/flan-t5-small\",'mobilenetv3_large_100\",\"categorical_mlp\",\"numerical_mlp\"} by MS-LLM."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600751545,
                "cdate": 1700600751545,
                "tmdate": 1700600751545,
                "mdate": 1700600751545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TpA3EaNxzZ",
                "forum": "IRtIHp7vsM",
                "replyto": "BHGIYGWNuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "**W2:**\n\nThank you for your suggestion. We have supplemented the 10-fold cross-validation results on 4 multi-modal datasets. Please refer to the response of W1 for details.\n\nIn addition, we supplemented the significance test of the method performance on 4 multi-modal datasets, as follows:\n\nWe formulate null hypotheses:\n- **H1:** On the PAP dataset, the performance of AutoM3L does not significantly exceed that of AutoGluon.\n- **H2:** On the PPC dataset, the performance of AutoM3L does not significantly exceed that of AutoGluon.\n- **H3:** On the GMRR dataset, the performance of AutoM3L does not significantly exceed that of AutoGluon.\n- **H4:** On the SPMG dataset, the performance of AutoM3L does not significantly exceed that of AutoGluon.\n\n**Normality Testing:** To ensure the validity of our subsequent statistical analyses, we conducted a normality test on AutoM3L's performance on 4 datasets using Q-Q plots, as depicted in (https://i.ibb.co/2SxcdDC/Whzr7-OLUY5.jpg).\n\nWe performed paired two-sample t-tests (essentially one-sample, one-sided t-tests on differences) for the aforementioned variables across two experimental conditions: AutoGluon and AutoM3L. These tests were conducted at a significance level of 5%. The hypothesis testing results from paired two-sample one-sided t-tests as below:\n\n| Hypothesis | T Test Statistic | P-value        | Reject Hypothesis |\n|------------|-------------------|----------------|--------------------|\n| H1         | 0.473             | 0.3238         | No                 |\n| H2         | 0.044             | 0.4830         | No                 |\n| H3         | -2.545            | 0.0157         | Yes                |\n| H4         | -1.496            | 0.0845         | No                 |\n\nExperiment results show that AutoM3L does not significantly surpass AutoGluon in all datasets. It should be noted that accuracy is not the only crucial metric. AutoM3L's critical advantage lies in enabling the entire ML pipeline to be automated through natural language interaction. This substantially reduces the manual effort and learning curve for users, as validated quantitatively in our user studies. Such intuitive human-AI interaction and usability are lacking in other AutoML approaches."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653130256,
                "cdate": 1700653130256,
                "tmdate": 1700707037014,
                "mdate": 1700707037014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Znrwd1IKWo",
            "forum": "IRtIHp7vsM",
            "replyto": "IRtIHp7vsM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_jKdu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2600/Reviewer_jKdu"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets to devise an univeral AutoML framework for multimodal tasks, which has been rarely explored. In specific, this paper combines the powerful reasoning ability to their framework. Firstly, the design MI-LLM to identify the data type and AFE-LLM to facilitate the feature engineering. Then an MS-LLM is devised to select the suitable encoder for each modaliyu. Finally, PA-LLM and HPO-LLM generates corresponding excutable codes and optimal hyper-parameters for training model. The experiments of comparison with AutoGluon show the proposed method can outperform the competing baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+S1: This paper has explored how to combine the LLMs with AutoML framework at an early stage.\n+S2: The authors provide many details of implementation for their framework, which can ease the reproduction of the work.\n+S3: The paper is well-writen, which is easy to understand."
                },
                "weaknesses": {
                    "value": "-W1: Though the motivation to combine the LLMs is clear, no technical difficulty is seen for combining LLMs with AutoML. It seems only a simple application of LLMs to AutoML, which may degrade the contributions of this paper.\n-W2: This paper only introduce few related works, but lack of sufficient relevant work collection. The authors claim that AutoGluon is the only work for automl multi-modal, but I find several other related works [1][2][3].\nonly one baseline is compared. In my view, you can compare with the variants of some existing approach.\n-W3: Some designs in the proposed framwork seems abundant. For example, is it necessary to design the modality inference module? In general, the data format is pre-defined and given by the dataset.\n-W4: Some errors exist in the paper. For example, in figure 2(a), the text in outputs_1 should be \"state\" but not \"stage\"?\n-W5: Lack of related baselines, which is relevant to the weakness W2. Also, I find some baselines in AutoGluon compared, such as H2O AutoML. In my view, these baselines also should be included in the experiments.\n\n[1] Jin, H., Chollet, F., Song, Q., & Hu, X. (2023). Autokeras: An automl library for deep learning. Journal of Machine Learning Research, 24(6), 1-6.\n[2] Sun, P., Zhang, W., Wang, H., Li, S., & Li, X. (2021). Deep RGB-D saliency detection with depth-sensitive attention and automatic multi-modal fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 1407-1417).\n[3] Erickson, N., Shi, X., Sharpnack, J., & Smola, A. (2022, August). Multimodal automl for image, text and tabular data. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 4786-4787)."
                },
                "questions": {
                    "value": "Q1: Does the AFE-LLM only can handle the tabular features, instead of multi-modal features? If it is, the idea is much similar to [4]. Besides, it seems that you conduct such feature engineering for each sample in dataset. I think it is extremely time-consuming, which may conflict the intuition of AutoML.\nQ2: Besides, there seems no specific multi-modal information is utilized in the proposed method. Only text path or image path are adopted. If it is, all other single-modal AutoML framework may be adpated to such task. \nQ3: Please also respond the questions mentioned in weakness.\n\n[4] Borisov, V., Sessler, K., Leemann, T., Pawelczyk, M., & Kasneci, G. (2022, September). Language Models are Realistic Tabular Data Generators. In The Eleventh International Conference on Learning Representations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2600/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2600/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2600/Reviewer_jKdu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2600/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699677144554,
            "cdate": 1699677144554,
            "tmdate": 1699677144554,
            "mdate": 1699677144554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FozSuwte5T",
                "forum": "IRtIHp7vsM",
                "replyto": "Znrwd1IKWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2600/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable suggestions. We appreciate your professional insights, which are crucial for enhancing the quality of our research.\n\n**W1:**\n\nWe understand the concern that our work may seem like a straightforward application of LLMs to AutoML. However, we believe there are several key technical contributions and innovations in AutoM3L that go beyond simply combining existing methods:\n\n1. **Adaptability and Scalability:** Our framework is designed to be highly adaptable to new models and techniques. Adding a new model to AutoM3L simply involves appending a new model card to the model zoo. In contrast, expanding the capabilities of rule-based AutoML systems like AutoGluon often requires extensive code changes. This adaptability comes from AutoM3L's core use of LLMs to dynamically assemble pipelines based on user needs.\n\n2. **Automated Feature Engineering:** To our knowledge, AutoM3L is the first AutoML system to leverage LLMs to automate feature engineering for multimodal data. This includes filtering irrelevant features and imputing missing values, which are challenging for rule-based methods. Our ablation studies demonstrate the benefits of LLM-powered feature engineering.\n\n3. **Interactive Customization:** A key advantage of LLMs is enabling intuitive human-AI interaction through natural language. AutoM3L allows users to customize pipelines through simple directives at each stage, rather than grappling with configuration files. This interactivity and ease of use is a substantive differentiation from existing AutoML systems.\n\n4. **Generalizability:** We demonstrate AutoM3L's capabilities across a diverse range of tasks (classification, regression, retrieval) and modalities (text, image, tabular). The strong performance across these datasets underscores AutoM3L's general applicability as an AutoML solution.\n\nThe technical challenge that AutoM3L solves lies in making LLM more effectively drive each component of AutoML. For instance, in MI-LLM and AFE-LLM, we leverage In-context learning to enable LLM to learn how to perform specific tasks with minimal samples. In the model retrieval stage, we generate model descriptions (model_cards) using LLM-based document reading tools like ChatPaper, automating the continuous addition of model_cards to the model library (model_zoo) for the latest models, thereby improving the framework's scalability and practicality. In HPO-LLM, to enhance LLM's inference of the parameters to be optimized, we use LLM to generate textual descriptions of configuration file-related parameters in the training context.\n\nAnother difficulty in applying LLM to AutoML arises from the challenge of assessing the framework's usability and intelligence. This is a common issue faced by many LLs-Agent-related works. To address this, we designed user experiments from a human-computer interaction perspective. The experimental results demonstrate that our framework, compared to existing ones, exhibits higher usability, significantly reducing user learning costs. This user-friendly aspect is particularly beneficial for users without a background in model training and underscores the significance of AutoML.\n\nAdditionally, this research looks into the novelty beyond the applications of LLM but also from the perspective of human-computer interaction. Specifically, our method simplifies user involvement, and eliminates the need for intensive manual feature engineering and hyperparameter optimization. Simultaneously, users can interact with the framework in natural language to customize pipelines, enhancing both the usability and intelligence of the framework."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2600/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596711729,
                "cdate": 1700596711729,
                "tmdate": 1700596711729,
                "mdate": 1700596711729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]