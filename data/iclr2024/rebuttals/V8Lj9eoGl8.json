[
    {
        "title": "Proximal Curriculum with Task Correlations for Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "DVdeVGS3fy",
            "forum": "V8Lj9eoGl8",
            "replyto": "V8Lj9eoGl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_MJvx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_MJvx"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a curriculum learning method for multi-task reinforcement learning agents to solve a target distribution of tasks. It balances between selecting tasks of moderate difficulty for the learning agent to solve and proposing tasks that are similar to the target distribution. The metric for sampling tasks is derived in a specific learning scenario and is then applied to other more general settings. In implementation, the method involves sampling a discrete pool of tasks from the uniform and target distribution of tasks and prioritizing the learning of the tasks proportional to the proposed metric. The experiments are conducted in both binary sparse reward and dense reward environments to solve various target distributions of tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method can deal with curriculum learning towards various target distributions of tasks, different to other works that consider a uniform target distribution. \n2. The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. It is not unconvincing to directly apply the curriculum strategy derived from the simple scenario in Sec 3.1 to the general case in Sec 3.2, since the simple scenario is with specific action and reward settings.\n2. The metric for task similarity is a bit specific to the tasks. Currently, the metric is defined based on the L2 distance of context parameters. However, there are many cases where such a metric is not positively correlated with the intuitive similarity between tasks. For example, consider a table-top manipulation environment with a drawer and a cube on the table. Suppose the initial state is with the drawer closed and the cube on the table outside the drawer. Consider a desired state A with the cube on top of the closed drawer and a desired state B with the cube inside the closed drawer. Suppose the context parameters are defined as the open width of the drawer and the pose of the cube. We can see that the L2 distance between A and B is small, but the complexity of strategies to reach A and B is different: reaching state A only requires one pick-and-place while reaching state B requires opening the drawer, placing the cube, and then closing the drawer. It is inappropriate for the curriculum to treat these tasks as similar, and I think a better metric for task similarity is required.\n3. Fig. 2(b) shows that PROXCORL performs similarly to PROXCORL-UN in PointMass-s:2G environment. Since the target task distribution is non-uniform in this environment, the result weakens the contribution of the proposed curriculum that takes task correlation into consideration."
                },
                "questions": {
                    "value": "Please refer to the \"weaknesses\" part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697902503422,
            "cdate": 1697902503422,
            "tmdate": 1699636529862,
            "mdate": 1699636529862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BUe2qGGdI5",
                "forum": "V8Lj9eoGl8",
                "replyto": "DVdeVGS3fy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MJvx"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our paper! We greatly appreciate your feedback. Please see below our responses to your comments.\n\n-----\n\n**1. Extending the presented curriculum analysis for the contextual bandit setting to a general RL setting.**\n\nOur investigation into developing a curriculum strategy for the multi-task RL setting (outlined in Section 2) begins by observing that the following three essential factors must be taken into account when selecting a source-target task pair for the student component: (i) Learning potential in the source task, (ii) Transfer potential of the source task, i.e., similarity with the target task, and (iii) Potential performance gain in the target task. \n\nThe challenge lies in integrating these quantities to devise an effective curriculum strategy. In pursuit of this goal, we opt to analyze a contextual bandit setting. Intriguingly, our analysis leads us to an intuitive curriculum strategy (Eq. (1)), involving the geometric mean of the three aforementioned factors. We extend this form to general learning settings with natural modifications. Furthermore, our empirical investigation demonstrates the effectiveness of our curriculum strategy.\n\nIn the revised version of this paper, we present a high-level approximation for the expected improvement in the training objective in a general RL setting (Section 3.1). Additionally, in the Appendix C.2, we conduct an extra analysis of a tree-structured contextual MDP setting. This analysis underscores the significance of the ZPD-related terms in our proposed curriculum strategy.\n\nWhile it remains intriguing to delve into more complex learning settings and explore the potential for uncovering richer curriculum strategies, we acknowledge the general absence of such analyses in the existing literature.\n\n-----\n\n**2. The metric for task similarity is a bit specific to the tasks. Currently, the metric is defined based on the L2 distance of context parameters.**\n\nWe thank the reviewer for pointing this out; it is indeed an interesting observation. In our work, we design our curriculum strategy to be agnostic to the task similarity metric, allowing for the application of any such metric. Nevertheless, for the purpose of the empirical evaluation, we opted to employ a standard similarity metric consistently across all domains. Exploring more generalized similarity metrics spanning various domains or even learning the similarity metric itself represents an interesting avenue for future research.\n\n-----\n\n**3. Clarification regarding the narrow performance gap between ProxCoRL and ProxCoRL-Un in the PointMass-s:2G environment with a non-uniform target distribution.**\n\nBased on the experimental results presented in the paper, we hypothesize that, for the PointMass-s:2G environment, the first two quantities (1) and (2) in Eq. (1), aligned with the ZPD principle, are enough to shift the training task distribution toward harder tasks. This observation aligns with similar trends identified in [1]. Through this shift towards harder tasks, the training task distribution ultimately converges to the target task distribution, even without explicit knowledge of the latter.\n\nConversely, in the PointMass-s:1T environment, where the target distribution is non-uniform, we observe that the performance gap between ProxCoRL and ProxCoRL-Un increases. Specifically, ProxCoRL converges more rapidly to the target distribution compared to ProxCoRL-Un. This indicates that, in the single target task setting, the incorporation of terms (3) and (4) intensifies the preference for tasks from the target distribution. This increased preference contributes to a more substantial distribution shift towards the target distribution, consequently improving the agent's speed in solving the target task.\n\n-----\n\n[1] Georgios Tzannetos, B\u00b4arbara Gomes Ribeiro, Parameswaran Kamalaruban, and Adish Singla. Proximal Curriculum for Reinforcement Learning Agents. In TMLR, 2023.\n\n-----\n\nWe hope that our responses can address your concerns and are helpful in improving your rating. If you have any other comments or feedback, please let us know! We are looking forward to hearing back from you! Thank you again for the review."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393389918,
                "cdate": 1700393389918,
                "tmdate": 1700393389918,
                "mdate": 1700393389918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VQt30c5kKh",
                "forum": "V8Lj9eoGl8",
                "replyto": "BUe2qGGdI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Reviewer_MJvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Reviewer_MJvx"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the rebuttal. My concern in weakness 2 is addressed. \n\nFor weakness 3, I still feel confused about why terms (1) and (2) in Eq. 1 can shift the distribution to a seemingly non-trivial target distribution (bimodal Gaussian) in PointMass-s:2G while not to a delta distribution in PointMass-s:1T. Could you provide more evidence? \n\nFor weakness 1, I appreciate the added analysis, but the theoretical derivation to general RL settings still requires more studies. \n\nOverall, I would like to keep my score unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617167672,
                "cdate": 1700617167672,
                "tmdate": 1700617167672,
                "mdate": 1700617167672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jMfTWI2aJS",
            "forum": "V8Lj9eoGl8",
            "replyto": "V8Lj9eoGl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_LEfH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_LEfH"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the curriculum learning setting in reinforcement learning problems. It focuses on the selection strategy of tasks from the target task distribution. The goal is to maximize the expected return of the learned policy on target task distribution. This work approximates this objective in a simplified learning setting (a single state and two actions in contextual MDP) with a single target task. And the approximation yields a curriculum strategy. According to the theoretical derivation, the authors propose the task selection strategy in the general setting of contextual MDP with arbitrary target task distributions.\n\nThe experiments are conducted on the tasks of sparse goal-reaching and bipedal walker stump tracks. The proposed method almost outperforms all baselines, including SOTA algorithms for curriculum learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is well-motivated with a theoretical foundation. \n\nThis paper is generally well-written and the proposed approach is clearly presented."
                },
                "weaknesses": {
                    "value": "The theoretical contribution is not strong enough, since the theorem is derived from a super simplified setting. And the adaptation to more general setting is not directly supported by the theorem.\n\nThe proposed algorithm is not obviously better than the baselines.\n\nThe experiments are only conducted on relatively simple tasks with state-based policy. It will be much more impressive if the proposed method can handle vision-baed RL policy."
                },
                "questions": {
                    "value": "As for the theoretical derivation in Section 3, is it possible to make the theoretical contribution stronger? Can we extend Theorem 1 by relaxing its assumption about the MDP or the target task distribution? The theoretical derivation looks a bit trivial when only considering the contextual MDP with singleton state space and a single target task.\n\nAs for the changes from Equation 1 to Equation 2, why we can replace V*() with V_{max}? Is there any mathematical derivation behind it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698298286553,
            "cdate": 1698298286553,
            "tmdate": 1699636529764,
            "mdate": 1699636529764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SaTPARiHpO",
                "forum": "V8Lj9eoGl8",
                "replyto": "jMfTWI2aJS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LEfH"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our paper! We greatly appreciate your feedback. Please see below our responses to your comments.\n\n-----\n\n**1. Extending the presented curriculum analysis for the contextual bandit setting to a general RL setting.**\n\nOur investigation into developing a curriculum strategy for the multi-task RL setting (outlined in Section 2) begins by observing that the following three essential factors must be taken into account when selecting a source-target task pair for the student component: (i) Learning potential in the source task, (ii) Transfer potential of the source task, i.e., similarity with the target task, and (iii) Potential performance gain in the target task. \n\nThe challenge lies in integrating these quantities to devise an effective curriculum strategy. In pursuit of this goal, we opt to analyze a contextual bandit setting. Intriguingly, our analysis leads us to an intuitive curriculum strategy (Eq. (1)), involving the geometric mean of the three aforementioned factors. We extend this form to general learning settings with natural modifications. Furthermore, our empirical investigation demonstrates the effectiveness of our curriculum strategy.\n\nIn the revised version of this paper, we present a high-level approximation for the expected improvement in the training objective in a general RL setting (Section 3.1). Additionally, in the Appendix C.2, we conduct an extra analysis of a tree-structured contextual MDP setting. This analysis underscores the significance of the ZPD-related terms in our proposed curriculum strategy.\n\n-----\n\n**2. The proposed algorithm is not obviously better than the baselines.** \n\nIn the empirical evaluation we made, the proposed curriculum is better or at least comparable with the baselines across all the environments. \n\nIn the SGR environment, Fig (2a), ProxCoRL outperforms the other techniques. \n\nIn the PointMass-s:2G environment, Fig (2b), we can observe a strong performance of ProxCoRL-Un, equivalent to the proposed technique. We hypothesize that, for the PointMass-s:2G environment, the first two quantities (1) and (2) in Eq. (1), aligned with the ZPD principle, are enough to shift the training task distribution toward harder tasks. This observation aligns with similar trends identified in [1]. Through this shift towards harder tasks, the training task distribution ultimately converges to the target task distribution, even without explicit knowledge of the latter.\n\nIn the PointMass-s:1T environment, Fig (2c), where the target distribution is non-uniform, we observe that the performance gap between ProxCoRL and ProxCoRL-Un increases. Specifically, ProxCoRL converges more rapidly to the target distribution compared to ProxCoRL-Un. This indicates that, in the single target task setting, the incorporation of terms (3) and (4) intensifies the preference for tasks from the target distribution. This increased preference contributes to a more substantial distribution shift towards the target distribution, consequently improving the agent's speed in solving the target task. CURROT and GRADIENT, both of which are curriculum strategies capable of converging to a single target, demonstrate effectiveness in this environment. Notably, ProxCoRL exhibits a faster convergence rate.\n\nIn the BipedalWalker environment, Fig (2d), the uniform nature of the target distribution suggests that ProxCoRL-Un should perform well in this setting. Interestingly, despite the uniform target distribution, ProxCoRL performs comparably to ProxCoRL-Un.\n\n-----\n\n**3. The experiments are only conducted on relatively simple tasks with state-based policy.**\n\nThe experiments are carried out in environments commonly employed in state-of-the-art curriculum techniques ([2], [3]). For example, the BipedalWalker environment features a high-dimensional state space in $R^{24}$ and incorporates LIDAR measurements.\n\n-----\n\n**4. Rationale behind replacing $V^\\*()$ with $V_{\\max}$ in the practical curriculum strategy.**\n\nIn the general practical setting, we simplify by assuming that the optimal value $V^\\*()$ of the environment can be replaced with the maximum achievable value $V_{\\max}$ within this environment. In goal-based sparse reward reinforcement learning environments, this value is commonly set to $1$. In domains where obtaining a prior estimate of $V_{\\max}$ proves challenging, a practical approach involves dynamically estimating it during training. This is achieved by continuously monitoring and updating the running maximum of the observed returns.\n\n-----\n\n[1] Tzannetos et al. Proximal Curriculum for Reinforcement Learning Agents. In TMLR, 2023.\n\n[2] Klink et al. Curriculum Reinforcement Learning via Constrained Optimal Transport. In ICML, 2022.\n\n[3] Romac et al. Teachmyagent: a Benchmark for Automatic Curriculum Learning in Deep RL. In ICML, 2021.\n\n-----\n\nWe hope that our responses can address your concerns and are helpful in improving your rating. Thank you again for the review."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393331555,
                "cdate": 1700393331555,
                "tmdate": 1700393331555,
                "mdate": 1700393331555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xu3qz1jk2k",
                "forum": "V8Lj9eoGl8",
                "replyto": "jMfTWI2aJS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Reviewer_LEfH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Reviewer_LEfH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thank authors for the detailed response. I appreciate your efforts on the newly updated theoretical results in C.2. However, I'm still concerned that the proposed method is not significantly better than baselines and the theoretical contribution is super limited to over-simplified cases. Thus, I'd keep my score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518884530,
                "cdate": 1700518884530,
                "tmdate": 1700518884530,
                "mdate": 1700518884530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ca0eJV2B06",
            "forum": "V8Lj9eoGl8",
            "replyto": "V8Lj9eoGl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_eLrA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_eLrA"
            ],
            "content": {
                "summary": {
                    "value": "* This paper introduces a curriculum strategy that applies the \u201cZone of Proximal Development\u201d concept to accelerate the learning progress of RL agents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The paper is well-written. \n* The method is motivated well; the proper background is introduced, and the technical details are clear. \n* The paper provides good context for related work, and the appropriate baselines are used for evaluation.\n* The results are convincing."
                },
                "weaknesses": {
                    "value": "* Use \u201ccitep\u201d instead of \u201ccitet\u201d for citations where the citation is not part of the sentence. In the Section 2, there are many instances of this error (e.g. \u201cHallack et al., 2015\u2026\u201d, \u201cSutton et al., 1999). \n* The above error occurs in following sections as well (see paragraph 3 in Section 3.1; First paragraph in Section 4). \n* Minor: Figure 2 plotlines are a bit thick, making it somewhat difficult to read. I would suggest slightly decreasing the line width."
                },
                "questions": {
                    "value": "* None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775122059,
            "cdate": 1698775122059,
            "tmdate": 1699636529663,
            "mdate": 1699636529663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OtkBGV6CEB",
                "forum": "V8Lj9eoGl8",
                "replyto": "Ca0eJV2B06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eLrA"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our paper! We greatly appreciate your feedback. Please see below our responses to your comments.\n\n-----\n\n**1. Use \u201ccitep\u201d instead of \u201ccitet\u201d for citations where the citation is not part of the sentence\u2026 Minor: Figure 2 plotlines are a bit thick, making it somewhat difficult to read. I would suggest slightly decreasing the line width.**\n\nWe thank the reviewer for these comments. We have updated the PDF with the suggested changes.\n\n-----\n\nIf you have any other comments or feedback, please let us know! We are looking forward to hearing back from you! Thank you again for the review."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393132611,
                "cdate": 1700393132611,
                "tmdate": 1700393132611,
                "mdate": 1700393132611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sCztuTXtIU",
            "forum": "V8Lj9eoGl8",
            "replyto": "V8Lj9eoGl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_ynrZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5294/Reviewer_ynrZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors suggest a curriculum approach to reinforcement learning within a contextual multi-task framework. Initially, this framework is used to formulate a teacher-student curriculum learning strategy, and the authors analyze the learning objective from the perspective of the teacher. Following this, the authors propose a strategy called ProxCoRL, which selects tasks $c_t\\$ based on their analysis. The paper ultimately demonstrates that the proposed strategy empirically outperforms previous curriculum-based reinforcement learning methods and baseline algorithms across a variety of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper emphasizes the significance of a balanced task selection strategy, which ensures that the tasks presented to the agent are neither too difficult nor too simple. This strategy facilitates the agent's progressive learning towards the target distribution. The approach employs a task selection mechanism and is supported by a mathematical analysis within a simplified setting."
                },
                "weaknesses": {
                    "value": "The paper does not present a method that is fundamentally different from those in prior work[1]. The authors attempt to extend the idea proposed in ProCuRL[1] to a general target distribution, yet the core of this extension appears to be a mere application of an existing concept. Moreover, the simplified setting used to conduct the mathematical analysis diverges significantly from a typical RL setting, given it encompasses only two possible actions and a single isolated state space. This raises questions about the scalability of such an analysis in a general reinforcement learning framework. Additionally, while the authors claim that their proposed approach eliminates the need for domain-specific hyperparameter tuning, it nonetheless requires the determination of $V_{max}$, which represents the maximum possible value.\n\n[1] Georgios Tzannetos, B\u00b4arbara Gomes Ribeiro, Parameswaran Kamalaruban, and Adish Singla. Proximal Curriculum for Reinforcement Learning Agents. In TMLR, 2023."
                },
                "questions": {
                    "value": "1. Answer the concerns in the above weakness section.\n\n2. While the authors assert that their proposed method, ProxCoRL, is robust to the target distribution $\\mu$ in contrast to ProxCuRL, the results presented in Figure 2 do not substantiate this claim. It is evident that ProxCoRL does not demonstrate effectiveness as the performance gap between ProxCoRL and ProxCuRL narrows in the case of a non-uniform task distribution (PointMass-s:2G). Could you clarify these results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5294/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5294/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5294/Reviewer_ynrZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699364764531,
            "cdate": 1699364764531,
            "tmdate": 1699636529568,
            "mdate": 1699636529568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "35Ix2Y5Owi",
                "forum": "V8Lj9eoGl8",
                "replyto": "sCztuTXtIU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5294/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ynrZ"
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing our paper! We greatly appreciate your feedback. Please see below our responses to your comments.\n\n-----\n\n**1. Difference between the proposed method and the one presented in prior work [1].**\n\nThe setting considered in the prior work [1] differs notably. Specifically, it does not account for a target task distribution, opting instead to gauge the agent's performance relative to a uniform distribution of tasks across the context space. In our work, the curriculum is developed with consideration for a target task setting, resulting in the introduction of additional terms (3) and (4) in Eq. (1). These terms enhance the preference for selecting tasks correlated with the target task.\n\n-----\n\n**2. Extending the presented curriculum analysis for the contextual bandit setting to a general RL setting.**\n\nOur investigation into developing a curriculum strategy for the multi-task RL setting (outlined in Section 2) begins by observing that the following three essential factors must be taken into account when selecting a source-target task pair for the student component: (i) Learning potential in the source task, (ii) Transfer potential of the source task, i.e., similarity with the target task, and (iii) Potential performance gain in the target task. \n\nThe challenge lies in integrating these quantities to devise an effective curriculum strategy. In pursuit of this goal, we opt to analyze a contextual bandit setting. Intriguingly, our analysis leads us to an intuitive curriculum strategy (Eq. (1)), involving the geometric mean of the three aforementioned factors. We extend this form to general learning settings with natural modifications. Furthermore, our empirical investigation demonstrates the effectiveness of our curriculum strategy.\n\nIn the revised version of this paper, we present a high-level approximation for the expected improvement in the training objective in a general RL setting (Section 3.1). Additionally, in the Appendix C.2, we conduct an extra analysis of a tree-structured contextual MDP setting. This analysis underscores the significance of the ZPD-related terms in our proposed curriculum strategy.\n\nWhile it remains intriguing to delve into more complex learning settings and explore the potential for uncovering richer curriculum strategies, we acknowledge the general absence of such analyses in the existing literature.\n\n-----\n\n**3. The proposed method requires domain-specific hyperparameter tuning of $V_{\\max}$.**\n\n$V_{\\max}$ denotes the maximum attainable value in the given environment. In goal-based sparse reward reinforcement learning environments, this value is commonly set to $1$. In domains where obtaining a prior estimate of $V_{\\max}$ proves challenging, a practical approach involves dynamically estimating it during training. This is achieved by continuously monitoring and updating the running maximum of the observed returns.\n\n-----\n\n**4. Clarification regarding the narrow performance gap between ProxCoRL and ProxCoRL-Un in the case of a non-uniform task distribution (PointMass-s:2G).**\n\nBased on the experimental results presented in the paper, we hypothesize that, for the PointMass-s:2G environment, the first two quantities (1) and (2) in Eq. (1), aligned with the ZPD principle, are enough to shift the training task distribution toward harder tasks. This observation aligns with similar trends identified in [1]. Through this shift towards harder tasks, the training task distribution ultimately converges to the target task distribution, even without explicit knowledge of the latter.\n\nConversely, in the PointMass-s:1T environment, where the target distribution is non-uniform, we observe that the performance gap between ProxCoRL and ProxCoRL-Un increases. Specifically, ProxCoRL converges more rapidly to the target distribution compared to ProxCoRL-Un. This indicates that, in the single target task setting, the incorporation of terms (3) and (4) intensifies the preference for tasks from the target distribution. This increased preference contributes to a more substantial distribution shift towards the target distribution, consequently improving the agent's speed in solving the target task.\n\n-----\n\n[1] Georgios Tzannetos, B\u00b4arbara Gomes Ribeiro, Parameswaran Kamalaruban, and Adish Singla. Proximal Curriculum for Reinforcement Learning Agents. In TMLR, 2023.\n\n-----\n\nWe hope that our responses can address your concerns and are helpful in improving your rating. If you have any other comments or feedback, please let us know! We are looking forward to hearing back from you! Thank you again for the review."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700393023808,
                "cdate": 1700393023808,
                "tmdate": 1700393023808,
                "mdate": 1700393023808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]