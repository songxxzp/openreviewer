[
    {
        "title": "Feasibility with Language Models for Open-World Compositional Zero-Shot Learning"
    },
    {
        "review": {
            "id": "cs3fKTCUdZ",
            "forum": "hgDDyoWQt3",
            "replyto": "hgDDyoWQt3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use Large Scale Language Model to identify infeasible pairs for open world compositional zero shot learning (ow-czsl). Speicifically, the authors propose In-context Learning to ensure the feasibility of the composition, which utilize a few examples of true feasible pairs, allowing the LLMs to learn from these instances and better understand what constitutes feasibility within the dataset. The experiments demonstrate that the proposed method improves over the existing methods that identify compositions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper is well-written and easy to follow.\n2.According to the experiments, FLM achieves noteworthy improvement in performance."
                },
                "weaknesses": {
                    "value": "1.Time costs need to be taken into account. As is known to all, the open-word setting will produce a large number of virtual compositions, which will bring a huge amount of calculation to the model (the proposed model process all possible pairs once and predict the score).\n2.According to the paper, the In-context Learning seems not to be trained in the process, which means that the performance relies on the LLMs. However, there existing some objects and states that are totally unknown to LLMs. In this situation, the proposed model cannot transfer the knowledge to the unknown compositions.\n3.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632889725,
            "cdate": 1698632889725,
            "tmdate": 1699637015941,
            "mdate": 1699637015941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TzJHZFdfB9",
                "forum": "hgDDyoWQt3",
                "replyto": "cs3fKTCUdZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful comments and questions as well as for pointing out that FLM achieves noteworthy improvement in CZSL performance. We address the reviewer\u2019s questions below.\n\n> Time costs need to be taken into account. As is known to all, the open-word setting will produce a large number of virtual compositions, which will bring a huge amount of calculation to the model (the proposed model process all possible pairs once and predict the score).\n\nFor the MIT-States dataset, obtaining feasibility scores for 28,175 pairs with Vicuna 13B takes approximately 40 minutes on a single A100 GPU with a batch size of 16. For the UT-Zappos dataset, obtaining scores for 192 pairs takes about 1 minute with a batch size of 16. Finally for the CGQA dataset, obtaining scores for 278,362 pairs takes approximately 250 minutes with a batch size of 5 (as it requires longer input sequences due to more in-context examples). Note that this is a one-time preprocessing step and does not affect inference time. Note that our method does not require any training, which would otherwise incur a time cost and it allows to update a previously computed list of state-objects pairs with additional states/objects without invalidating previously computed scores (because no re-training is required).\n\n> According to the paper, the In-context Learning seems not to be trained in the process, which means that the performance relies on the LLMs. However, there existing some objects and states that are totally unknown to LLMs. In this situation, the proposed model cannot transfer the knowledge to the unknown compositions.\n\nLLMs have a vast vocabulary that includes all practically relevant states and objects considered in CZSL datasets (fine-grained shoe dataset UT-Zappos, common real-world objects in MIT states and C-GQA) and their language understanding is generally more comprehensive compared to GloVe and ConceptNet. Nonetheless, when we test the LLM with unknown words (e.g. made-up), it defaults to considering their pairs as infeasible unless their feasibility is very clear from the in-context example, i.e., it cannot as easily relate objects or states by their semantic similarity.\n\nWhen new states and objects become available (e.g. neologism or recent product names), any model used for feasibility prediction (LLMs for FLM, GloVe, ConceptNet) would have to be updated with relevant training data. In the case of FLM, we would likely use the most recently available LLM. For anything in-distribution (e.g. real-world states/objects), we find that in-context examples are effective in adjusting the LLM to the specific task without requiring costly re-training.\n\n> The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper.\n\nAs explained in our previous answers, we believe that in-context learning allows LLMs to transfer and adapt to new datasets, which has also been shown by Brown et al [A]. Our ablations in Table 3 show how the performance of directly applying the model without in-context learning is much worse (e.g. dropping from 17.4% to 15.4% in harmonic mean on MIT-States). We see this as a strength of our approach as the LLM can be readily replaced by a more capable model in the future without requiring task-specific fine-tuning.\n\n[A] Language models are few-shot learners. NeurIPS, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8190/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158596858,
                "cdate": 1700158596858,
                "tmdate": 1700158596858,
                "mdate": 1700158596858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VK7PeSRalx",
            "forum": "hgDDyoWQt3",
            "replyto": "hgDDyoWQt3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel feasibility calibration scheme for open-world compositional zero-shot learning. The key ideas leverage large language models as the intermediate agency for feasibility decisions. The authors conducted extensive experiments to show that the proposed scheme can improve compositional zero-shot recognition. The authors also include ablation experiments to address effects of underlying LLMs and prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "To the best of the reviewer\u2019s knowledge, this method proposed in this paper is novel. This paper is clearly motivated and the intuition behind the proposed methods are also very clear. The idea of using LLMs for solving feasibility conflicts is simple yet quite effective. The authors also show that as an orthogonal component to existing compositional zero shot learning methods, LLM-guided feasibility calibration can clearly boost the performance for most of the scenarios."
                },
                "weaknesses": {
                    "value": "Despite the work\u2019s obvious merit, the idea itself is very simple. Within the ablations, it would be helpful if the authors are to thoroughly examine more variants of prompts since LLMs output can vary a lot. The performance variations under such scenarios would be very informative to the community."
                },
                "questions": {
                    "value": "Despite the method being effective as the authors have shown, the idea of using a single LLM for decisions may suffer from the common issues of hallucinations. I wonder if the authors have tapped into potential solutions to get around this issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769998240,
            "cdate": 1698769998240,
            "tmdate": 1699637015828,
            "mdate": 1699637015828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V4yiFzPky6",
                "forum": "hgDDyoWQt3",
                "replyto": "VK7PeSRalx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments and suggestions. We appreciate that the reviewer highlights the novelty of FLM and that our conceptually simple method is effective in boosting existing CZSL methods.\n\n> Despite the work\u2019s obvious merit, the idea itself is very simple.\n\nWe agree with the simplicity of our FLM method. However, it is important to emphasize that simplicity doesn\u2019t negate the effectiveness where our method consistently improves the performance across different benchmarks and various CZSL models. Moreover, its versatility enables the integration with any existing CZSL models.\n\n> Within the ablations, it would be helpful if the authors are to thoroughly examine more variants of prompts since LLMs output can vary a lot. The performance variations under such scenarios would be very informative to the community.\n\nTo examine a broader range of prompt variations, we added two more instruction sentences, i.e. \u201cAnswer with yes or no.\u201d and \u201cAnswer with yes or no, followed by an explanation\u201d, and one more query sentence, i.e. \u201cConsidering the list above, does \u201c{s} {o} align with the contents?\u201d. By adding these sentences to the list described in Appendix B, we experiment  with 64 (=4^3) different prompts. We report the results on MIT-States as a box plot in Figure 5 in Appendix B. We first observe the results vary with different prompts. For instance, the Harmonic mean accuracy in CSP ranges from 15.5% to 18.1%. However, despite this variability, most of the prompts outperform the baselines. For instance, the Glove result always lies close to or below the lower quartile (25th percentile) of the box plot, and ConceptNet even further below that.\n\n> Despite the method being effective as the authors have shown, the idea of using a single LLM for decisions may suffer from the common issues of hallucinations. I wonder if the authors have tapped into potential solutions to get around this issue.\n\nAs we only probe the LLM for the tokens \u201cYes\u201d and \u201cNo\u201d, FLM is not directly affected by LLMs making up facts otherwise commonly observed as hallucinations. In the context of feasibility prediction, one could most closely relate this to making an incorrect prediction, e.g., assuming a pair to be infeasible although it exists in the dataset. From our ablation study in Table 3, we find that the in-context examples contribute the most to mitigating prediction errors, but it is not straightforward to identify mistakes as \u201challucinations\u201d or any other limitation of the LLM."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8190/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158497476,
                "cdate": 1700158497476,
                "tmdate": 1700158497476,
                "mdate": 1700158497476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mb6pFxVPVl",
            "forum": "hgDDyoWQt3",
            "replyto": "hgDDyoWQt3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to determine the feasibility of state-object (s, o) combinations (i.e., while the phrase\u201chot fire\u201d is feasible, the phrase\u201cwet fire\u201d is not feasible). This paper proposed to use large language models (LLM), such as Vicuna and ChatGPT to classify whether the phrase is feasible or not. This is implemented by entering ``Does a/an [THE PHRASE] exist in the real world?\u201d as the input to the LLM and the probability of LLM answering \u201cyes\u201d is the feasibility score. The author also tries to show several feasible phrases to the LLM to help the LLM decide whether [THE PHRASE] is feasible or not."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Figure 1 is well designed and helps the reader to understand the content.\n2. The proposed method is simple and easy to understand."
                },
                "weaknesses": {
                    "value": "1. The main concern of this work is its contribution. The paper basically uses the existing LLM to determine the feasibility of a state-object combination. This only shows that the existing LLM is able to determine the feasibility of a state-object combination, but what is the author\u2019s contribution throughout the process? \n2. Since different threshold will affect the binary classification performance, wouldn\u2019t a metric like ROC curve suits the tasks better?\n3. For Figure 2, it seems that both green and red block only show the feasible (s,o) pairs. The author is suggested to show some infeasible (s,o) pairs and the model prediction on those infeasible (s,o) pairs.\n4. It is challenging to tell whether GloVe or the proposed FLM separates the feasible and infeasible better by only looking at the figures. The author is suggested to show some numerical results to support the claim.\n5. In the evaluation metric, the author mentioned that the calibration bias is varied. Does it mean that different calibration bias is used for different metric?\n6. Typo: such \u201cat\u201d ChatGPT"
                },
                "questions": {
                    "value": "1. For the rebuttal, the author is suggested to highlight the contribution of this work. After reading the submission, this paper is more like showing the observation that existing LLM already did a great job on identifying infeasible (s,o) pairs. There is no modification of the LLM, and no loss function is proposed. \n2. Some clarification problem in the weakness section needs to be addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817422754,
            "cdate": 1698817422754,
            "tmdate": 1699637015710,
            "mdate": 1699637015710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PedRUTUjMA",
                "forum": "hgDDyoWQt3",
                "replyto": "Mb6pFxVPVl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thorough evaluation and for acknowledging that our method is simple and easy to understand. We address the reviewer's questions below.\n\n> The main concern of this work is its contribution. The paper basically uses the existing LLM to determine the feasibility of a state-object combination. This only shows that the existing LLM is able to determine the feasibility of a state-object combination, but what is the author\u2019s contribution throughout the process?\n\nTo the best of our knowledge, we are the first to leverage LLMs and in particular their in-context learning capability to enhance the CZSL performance. Though simple, our FLM substantially improves CZSL performance. Moreover, our FLM is an orthogonal work that can be integrated with any CZSL methods to boost performance in the open-world setting. By replacing an important step in the CZSL pipeline, our approach has an impact on both present and future CSZL methods. This approach also has been acknowledged as innovative by reviewer niw2.\n\n> Since different threshold will affect the binary classification performance, wouldn\u2019t a metric like ROC curve suits the tasks better?\n\nWe report AUC of the CZSL performance in Table 1 which represents the ROC curve as we vary the calibration bias. In Table 2, we then report the feasibility accuracies for the fixed threshold that is used in Table 1 to make a direct connection to the downstream task of CZSL.\n\n> For Figure 2, it seems that both green and red block only show the feasible (s,o) pairs. The author is suggested to show some infeasible (s,o) pairs and the model prediction on those infeasible (s,o) pairs.\n\nIn the CSP model on MIT-States dataset, our FLM scores \u201cpeeled sauce\u201d and \u201cmolten milk\u201d, which are infeasible pairs, as -0.46 and \u20130.30, respectively, while GloVe's scores are 0.1 and 0.18 and ConceptNet's scores are 0.09 and 0.05. However, there are also \u201cfailure cases\u201d in predicting an infeasible pair. For example, \u201cmashed tomato\u201d (0.04, 0.28, 0.30 by FLM, GloVe, and Conceptnet, respectively) and \u201cfresh apple\u201d (0.04, 0.02, 0.07), though not included in the closed-world ground-truth pairs, are considered feasible by all methods. One could argue that those are correct predictions, but in the context of the dataset they become confusing pairs for the model as there are no related images in the dataset. It is difficult to find cases where a human would clearly consider a pair to be infeasible, but FLM predicts it to be feasible. Examples that come closest are \u201ccracked shore\u201d or \u201cdented building\u201d. As mentioned in Section 4.3, it is important to note that some realistically feasible pairs are simply not included in the dataset and, thus, we cannot show images for any infeasible pairs from the datasets.\n\n> It is challenging to tell whether GloVe or the proposed FLM separates the feasible and infeasible better by only looking at the figures. The author is suggested to show some numerical results to support the claim.\n\nAs shown by the numerical results in Table 2 that support Figures 2 and 3, our FLM performs the best on either feasible or infeasible accuracies. Considering both metrics together through arithmetic and harmonic means, our method best separates the feasible and infeasible pairs across all datasets.\n\n> In the evaluation metric, the author mentioned that the calibration bias is varied. Does it mean that different calibration bias is used for different metric?\n\nYes, we follow Nayak et al. (CSP) in that the calibration bias is different for every metric. The calibration bias, which is subtracted from the CZSL model outputs of the seen classes but not the unseen classes, is varied to plot the ROC curve of the seen class and unseen class accuracies. The seen class accuracy (metric S) corresponds to the minimal bias value, the unseen class accuracy (metric U) aligns with the maximum bias value, and the H metric is when the bias value reaches the best harmonic mean between the seen class and unseen class accuracies. The AUC metric, measuring the area under the ROC curve, remains unaffected by the value of the calibration bias as it takes all possible values into account.\n\n> Typo: such \u201cat\u201d ChatGPT\n\nThank you for pointing this out. We rectified the typo."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8190/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158353247,
                "cdate": 1700158353247,
                "tmdate": 1700158353247,
                "mdate": 1700158353247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aQBQ35PlOu",
            "forum": "hgDDyoWQt3",
            "replyto": "hgDDyoWQt3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a novel approach that leverages large language models (LLMs) to predict the feasibility of the state-object pair for the open-world compositional zero-shot learning (OW-CZSL). A prompt is designed to query the feasibility score by leveraging the autoregressive nature of LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: The studied problem about open-world compositional zero-shot learning is significant important and can apply to the real-world scene.\n\nS2: The large-language models are used to reduce the gap between machines and humans.\n\nS3: Extensive experiments on many prompt variants and six LLMs shows the best performence."
                },
                "weaknesses": {
                    "value": "W1: Is this the first paper to solve the CZSL problem by using  the LLMs? If yes, I am curious about the motivation or some motivation experiments to demonstrate the effectiveness of LLMs? If no, I tend to see some differents compared with other published related works.\n\nW2: This method in this paper is not novel and performance improvement depends entirely on the language model. If the language model introduces biases, such as racial discrimination, during training, will this also affect downstream tasks?\n\nW3: Does a more powerful language model perform best in this paper?\n\nIn my opinion, simply introducing a language model to solve downstream tasks does not reach the upper limit of ICLR acceptance."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904133371,
            "cdate": 1698904133371,
            "tmdate": 1699637015595,
            "mdate": 1699637015595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lGdVxClCFF",
                "forum": "hgDDyoWQt3",
                "replyto": "aQBQ35PlOu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8190/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments that help us improve the paper and for pointing out that we study an important problem where we show with extensive experiments that our LLM-based method achieves the best performance.\n\n> Is this the first paper to solve the CZSL problem by using the LLMs? If yes, I am curious about the motivation or some motivation experiments to demonstrate the effectiveness of LLMs?\n\nTo the best of our knowledge, we are the first to leverage LLMs and in particular their in-context learning (ICL) ability to enhance CZSL. Contextual understanding of LLMs would enhance their ability to predict feasibility, as illustrated by the example of \u201cdark fire\u201d in the Introduction section. This motivated us to use the ICL capability of LLMs rather than a simple prompt, resulting in a substantial improvement in CZSL performance. By replacing an important step in the CZSL pipeline, our approach has an impact on both present and future CSZL methods. This approach also has been acknowledged as innovative by reviewer niw2.\n\n> I tend to see some differents compared with other published related works.\n\nWe tackle the issue of feasibility prediction while most other published works focus on changing the model pipeline (model architecture or loss function) to improve the performance. We argue that our FLM is an orthogonal work that can be integrated with any CZSL methods to boost performance in the open-world setting.\n\n> This method in this paper is not novel and performance improvement depends entirely on the language model. If the language model introduces biases, such as racial discrimination, during training, will this also affect downstream tasks?\n\nThis is a valid concern. If biased attributes are involved in the composition, the LLMs\u2019 response for feasibility prediction may be affected, leading to a wrong prediction. Understanding and addressing biases in the LLMs could be important for the CZSL downstream task. The general benchmarks in CZSL task, i.e. MIT-States, UT-Zappos, and C-GQA, involve common objects and states (e.g. shoes or animals, colors or conditions) without including any humans. These state-object pairs are not typically considered social biases, such as racial discrimination. Hence, these states might not be biased in the same way as human-related attributes, such that biases in LLMs could have a smaller effect on CZSL downstream tasks. Nonetheless, investigating the effect of biases in LLMs to the CZSL downstream task would be an intriguing open question.\n\n> Does a more powerful language model perform best in this paper?\n\nThe general trend of CZSL downstream performance appears to align with the language model\u2019s capabilities. In Figure 4, particularly on MIT-States, we observe that ChatGPT and Claude-2 outperform Vicuna-13B with LLaMA-2 being the last in the binary setup, reflecting a ranking similar to the LLM leaderboard. However, we also observed the exception where GPT-4 did not follow the expected trend in the CZSL downstream task."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8190/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158104387,
                "cdate": 1700158104387,
                "tmdate": 1700158104387,
                "mdate": 1700158104387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D9dpMeO83K",
                "forum": "hgDDyoWQt3",
                "replyto": "lGdVxClCFF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for authors\u2018 the detailed response"
                    },
                    "comment": {
                        "value": "Thank you for authors\u2019 the detailed response.\n\nI have carefully read the author's response and the comments of other reviewers. I still feel that the main contribution of this paper in introducing the LLM is insufficient. Therefore, I keep my score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8190/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665019463,
                "cdate": 1700665019463,
                "tmdate": 1700665019463,
                "mdate": 1700665019463,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]