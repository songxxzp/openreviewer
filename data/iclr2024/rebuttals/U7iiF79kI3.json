[
    {
        "title": "CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception"
    },
    {
        "review": {
            "id": "bseZG3n38a",
            "forum": "U7iiF79kI3",
            "replyto": "U7iiF79kI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors try to fill the hole in pre-training for Camera-LiDAR BEV perception. The authors apply contrastive learning on both LiDAR and camera modalities in two stages. The authors develop point-wise positive and negative pairs to balance both region- and scene-level contrasts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "In sum, I think the proposed solution is simple and straightforward. Thus, I think the proposed approach is easily reproducible. The particular strengths I see are the following:\n1. Point-wise positive and negative pairs to balance both region- and scene-level contrasts.\n2. Strong empirical results across many datasets compared to prior work.\n3. The paper includes ablation experiments on several of the components"
                },
                "weaknesses": {
                    "value": "I have some concerns about the proposed method:\n1. How does it identify the semantic-less and semantic-rich points? How does it calculate the 4th dimension of the points? What is the range of 4th dimension of the points?\n2. Is T2 identity transform in Fig. 1? If not, how does it generate the images from the original images? Were any related augmentations applied to the images, when the lidar points were changed?"
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698491406882,
            "cdate": 1698491406882,
            "tmdate": 1699636359774,
            "mdate": 1699636359774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "drmf53qlN7",
                "forum": "U7iiF79kI3",
                "replyto": "bseZG3n38a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ksX5"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer found our method effective and comprehensive. We appreciate the opportunity to address the points you have raised.\n\n> How does it identify the semantic-less and semantic-rich points? How does it calculate the 4th dimension of the points? What is the range of the 4th dimension of the points?\n\nIn our approach, the differentiation between semantic-rich and semantic-less points is achieved through a novel unsupervised technique termed **semantic pooling**. This process is designed to identify points potentially associated with objects of interest in an unsupervised manner, such as cars, cyclists, and pedestrians, categorizing them as *semantic-rich*. As our semantic pooling is based on the modified DBSCAN clustering algorithm with heuristics, these pooled points are extracted into different clusters based on distance metrics. The representation of these clusters can be seen in different colors in Figures 6, 7, and 8 in Appendix C. Points that are not selected by our semantic pooling, typically associated with less relevant features like buildings, ground, or grass, are marked as *semantic-less*. \n\nRegarding the 4th dimension of the points, it serves as a cluster identifier for the semantic-rich points. Each point is assigned an integer value in this dimension that corresponds to the cluster it belongs to, as determined by our semantic pooling operation. For semantic-less points, this value is set to -1, distinguishing them from their semantic-rich counterparts. It's important to note that the 4th dimension is used as metadata during our pretraining process and does not directly feed into the model learning.\n\nWe have recognized the need for a more detailed explanation of these concepts and have included additional clarifications in the revised version of our paper. \n\n> Is $T_2$ identity transform in Fig. 1? If not, how does it generate the images from the original images?\n\nThe second transformation $T_2$  does not necessarily need to be the identity transformation. We would like to clarify that Figure 1 is only for illustrative purposes. There are three transformations in the actual implementation of our framework.\n\nIn the first stage of CALICO, namely point-region contrast (PRC), we employ two transformations, $T_1$ and $T_2$, to facilitate effective contrastive pretraining. \n\nIn the second stage of CALICO, which is the region-aware distillation (RAD), we introduce another transformation on the xy-plane. This transformation, denoted as $T_3$, is similar to $T_1$ and $T_2$ and serves as a data augmentation method for the LiDAR modality. Concurrently, the view transformation module in the camera modality, LSS [a,b], incorporates $T_3$ when transforming the features from the perspective view to the bird\u2019s eye view (BEV). This ensures that the feature maps from both LiDAR and camera modalities remain well-aligned in the distillation process. \n\nWe hope our response addresses all the comments and that the reviewer will consider raising the rating accordingly. We are more than glad to answer any further questions.\n\n[a] Philion, Jonah, and Sanja Fidler. \"Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV 16. Springer International Publishing, 2020.\n\n[b] BEVFusion Codebase https://github.com/mit-han-lab/bevfusion/blob/main/mmdet3d/models/vtransforms/base.py (disclaimer: this link contains a public codebase that does not leak any information from the authors.)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096203468,
                "cdate": 1700096203468,
                "tmdate": 1700096203468,
                "mdate": 1700096203468,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2arRF0hkX6",
                "forum": "U7iiF79kI3",
                "replyto": "drmf53qlN7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_ksX5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. It solved my concerns. I tend to raise my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575022907,
                "cdate": 1700575022907,
                "tmdate": 1700575022907,
                "mdate": 1700575022907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GoVN3s1O7i",
            "forum": "U7iiF79kI3",
            "replyto": "U7iiF79kI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a self-supervised learning method of Image and point-cloud input that consists of point-region contrast (PRC) and region-aware distillation (RAD). \nDiffering from the previous works, PRC utilizes both point- and region-level contrastive learning on point cloud. RAD aligns the feature maps of Images and points.\nThe proposed method is evaluated and shows substantial performance improvement on 3D detection and BEV map segmentation tasks on nuScene and Waymo datasets.\nAblation studies and robustness tests are also thorough to demonstrate the effectiveness of the proposed method.\nAfter the author discussion phase, I will adjust or fix my decision."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[Originality]\n+ Differing from the previous works, the proposed method welly utilizes both point- and region-level contrastive learning on point cloud.\n\n[Quality & Significance]\n+ The proposed method is evaluated and shows substantial performance improvement on 3D detection and BEV map segmentation tasks on nuScene and Waymo datasets.\n+ Ablation studies and robustness tests are also thorough to demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "[Quality]\n- The proposed method adopts the BEVFusion architecture. Lidar backbone is PointPillars, and Image backbone is Swin-T. \nEvaluation of different Lidar/Image backbone models could have a high impact.\n- The necessity of negative samples of P_PLRC and P_RAPC is unclear. The ablation study (w/o negative samples in P_PLRC and P_RAPC) supports the necessity."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3981/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3981/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760809140,
            "cdate": 1698760809140,
            "tmdate": 1700594843088,
            "mdate": 1700594843088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5LplitmxNX",
                "forum": "U7iiF79kI3",
                "replyto": "GoVN3s1O7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fxyA [1/2]"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer found our method effective and comprehensive. We appreciate the opportunity to address the points you have raised.\n\n> The proposed method adopts the BEVFusion architecture. Lidar backbone is PointPillars, and Image backbone is Swin-T. Evaluation of different Lidar/Image backbone models could have a high impact.\n\nWe adopted the BEVFusion codebase for its state-of-the-art performance and its versatility as a general sensor fusion framework at the time of our implementation. In Section 3.5 of our paper, we have conducted an ablation study using the VoxelNet backbone and Transfusion head to specifically demonstrate the effectiveness of our point-region contrast (PRC) approach in the LiDAR modality. The results, as illustrated in Figure 4, clearly show that our PRC method consistently outperforms other baseline approaches in terms of performance.\nAdditionally, in response to your valuable suggestion, we have integrated ResNet as an alternative backbone for the camera modality (as configured in the BEVFusion codebase). Specifically, we use our PRC pretrained LiDAR backbone as the teacher model and leverage the 10% finetuning data setting in this experiment.\nThis further step is taken to demonstrate the adaptability and generalizability of our framework across different backbone architectures.\n\n| ResNet as the Camera Backbone with 10% Finetuning Data  | mAP  | NDS  |\n|---------------------------------------------------------|------|------|\n| **PRC**+Rand. Init. (C)                                         | 47.9 | 52.0 |\n| SimIPU                                                  | 46.6 | 51.4 |\n| **PRC**+BEVDistill                                          | 48.5 | 52.5 |\n| **CALICO**                                                  | **49.0** | **53.1** |\n\nThe results in the above table illustrate a consistent trend in performance between Swint-T and ResNet as the image backbone. More importantly, our CALICO using the ResNet backbone consistently achieves the best performance compared to other baseline methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096317578,
                "cdate": 1700096317578,
                "tmdate": 1700096317578,
                "mdate": 1700096317578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZDdCdTBuqB",
                "forum": "U7iiF79kI3",
                "replyto": "GoVN3s1O7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fxyA [2/2]"
                    },
                    "comment": {
                        "value": "> The necessity of negative samples of $P_{PLRC}$ and $P_{RAPC}$ is unclear. \n\nTo address this, we first acknowledge the fundamental role of negative samples in contrastive learning. The absence of negative samples can lead to model collapse, as highlighted in several studies [a, b, c]. Additionally, methods focusing solely on positive pairs often implicitly benefit from statistics hidden negative samples during model training [d].\n\nTherefore, we hypothesize that you are specifically inquiring about the necessity and role of negative sample augmentation through semantic-less points in our framework. Our approach addresses a unique challenge in LiDAR data, notably the presence of multiple objects of interest within a single frame. For instance, a LiDAR frame capturing a traffic intersection may contain numerous cars and pedestrians. If we restrict our framework to only use pooled points for generating negative pairs, we risk encountering the **class collision** problem. This occurs when points from different clusters (extracted by our semantic pooling), although distinct, belong to the same object class (e.g., car). There will be performance degradation when treating points from the same class of object as negative pairs. Our negative sample augmentation is thus proposed to alleviate this problem.\n\nTo empirically validate our approach and address the concerns raised, we conducted experiments specifically focusing on the impact and necessity of our negative sample augmentation strategy. \n\n| 10% Finetuning Data                               | mAP  | NDS  |\n|---------------------------------------------------|------|------|\n| PLRC                                              | 41.9 | 50.9 |\n| + semantic pooling                                | 42.8 | 51.5 |\n| + semantic pooling + negative sample augmentation | **43.3** | **51.9** |\n\n| 50% Finetuning Data                               | mAP  | NDS  |\n|---------------------------------------------------|------|------|\n| PLRC                                              | 52.5 | 60.0 |\n| + semantic pooling                                | 53.0 | 60.5 |\n| + semantic pooling + negative sample augmentation | **53.2** | **60.8** |\n\nThese experimental results clearly demonstrate the effectiveness and necessity of our negative sample augmentation method in improving the overall model performance.\n\nWe have included all the experimental results in the revised manuscript and hope our response addresses all the comments and that the reviewer will consider raising the rating accordingly. We are more than glad to answer any further questions.\n\n[a] He, Kaiming, et al. \"Momentum contrast for unsupervised visual representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[b] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[c] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284.\n\n[d] Li, Alexander C., Alexei A. Efros, and Deepak Pathak. \"Understanding collapse in non-contrastive siamese representation learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096359993,
                "cdate": 1700096359993,
                "tmdate": 1700096392843,
                "mdate": 1700096392843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F9EporcT5Y",
                "forum": "U7iiF79kI3",
                "replyto": "ZDdCdTBuqB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_fxyA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. My concerns are resolved!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594817627,
                "cdate": 1700594817627,
                "tmdate": 1700594817627,
                "mdate": 1700594817627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IzdBxliZwf",
            "forum": "U7iiF79kI3",
            "replyto": "U7iiF79kI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a method for LiDAR-Camera BEV fusion is explored via contrastive and self-supervised training. Using currently fashionable machinery - Lift Splat Shoot type camera encoder, Voxelnet LiDAR encoder, Transfusion decoder, etc. - for feature processing in the BEV literature, the paper adapts contrastive learning ideas for the problem. At a high level, delineations are made between between point level and region level contrast, with heuristics (e.g. clustering) being applied to extract more discriminative features. \n\nEvaluations are presented to compare with other contrastive methods on the NuScenes and Waymo datasets. After contrastive (unsupervised/self-supervised) pretraining, the setup is fine tuned for object detection and segmentation tasks with varying amounts of training data to show efficacy of the pretraining step. Furthermore, they also investigate robustness to adversarial attacks (inserting fake objects at some distance from ego vehicle) and corruption of data (as might occur in bad weather, degraded sensors)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Contrastive training is generally less studied in the BEV perception literature. This work adds to the body of work present in the area. \n+ The methods are generalizable, and can be applied to any setup. \n+ Effectiveness is shown across modalities in camera, camera+lidar and lidar. This is convincing. I was particularly impressed with the saliency maps with and without pre-training."
                },
                "weaknesses": {
                    "value": "- The paper is entirely empirical, and is as such a purely application based work. One may or may not take this as a weakness, of course. \n- On the same lines as above, the paper is heavy on tables, but I feel that qualitative analysis of where the improvement comes from is light. Some analysis through carefully designed experiments that show improvement with and without various fittings would shed insight. It appears that semantic pooling (feature rich vs feature less regions) plays a part from the figure 5, but I would like more examples of failure cases."
                },
                "questions": {
                    "value": "- Could the authors explain how adversarial robustness is relevant in this context? \n- The clustering methods look rather empirical. More experiments on how they work in different cases would be useful. \n- Calibration error analysis: I think it would help to learn if the system can demonstrate robustness against calibration error, a common occurrence in autonomous driving setups. \n- Superfluous lines from possible prior submission (Appendix C, under 'Visualization') \n\n\"As we mentioned in the rebuttal,camera features trained from scratch are not salient to contribute to robustness improvement.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3981/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699115820366,
            "cdate": 1699115820366,
            "tmdate": 1699636359526,
            "mdate": 1699636359526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7MNsI6GEQ3",
                "forum": "U7iiF79kI3",
                "replyto": "IzdBxliZwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2HK8 [1/2]"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer found our study valuable to this field and that our method is effective and generalizable. We appreciate the opportunity to address the points you have raised.\n\n> The paper is entirely empirical, and is as such a purely application based work. One may or may not take this as a weakness, of course.\n\nWe agree with the reviewer's observation regarding the empirical nature of our study. While it is true that our paper focuses on application-driven research, we believe this approach is not only valid but also crucial in the context of pretraining methods. As the reviewer notes, the empirical focus should not inherently be seen as a weakness.\n\nIn the field of pretraining, whether contrastive (e.g., MoCo [a]) or generative (e.g., MAE [b]), the practical application of methods often precedes and informs theoretical understanding. This is particularly true in rapidly advancing areas where empirical results can guide and refine theoretical models. Our work contributes to this tradition by providing valuable insights through real-world scenarios in autonomous driving perception.\n\nTo comply with the suggestions from the reviewer, we have included a new discussion in Appendix D.1 of our revised manuscript on potential avenues for theoretical exploration in pretraining methods, acknowledging the current limitations while highlighting the importance of empirical contributions like ours in laying the groundwork for future theoretical advancements.\n\n>  The paper is heavy on tables, but I feel that qualitative analysis of where the improvement comes from is light.\n\nWe appreciate the reviewer's insightful observation regarding the balance between quantitative and qualitative analysis in our paper. We agree that the abundance of tables could potentially overshadow the qualitative insights. The choice to emphasize tabular data was driven by our intent to provide a comprehensive and detailed comparison across different metrics.\n\nIn response to the reviewer's suggestion, we have included a more focused qualitative analysis in Appendix B.1. This analysis presents key insights derived from our results. Notably, we observe that the benefits of pretraining diminish with the increase in fine-tuning data. This highlights the nuanced trade-off between pretraining and fine-tuning in model performance, which is as expected. Furthermore, when comparing our proposed PRC method (augmented with the RAPC component) to the PLRC approach, it becomes evident that our method demonstrates more significant improvements, especially when finetuning on 50% of the data. Moreover, our ablation study in Table 5 also showcases that the RAPC component in our design serves as a good regularization term to balance the performance of our framework among different levels of availabilities in finetuning data.\n\nWe hope that this enhanced qualitative discussion complements the extensive quantitative data presented and provides a more holistic understanding of our research contributions.\n\n> Could the authors explain how adversarial robustness is relevant in this context?\n\nThe relevance of adversarial robustness in our study is directly tied to the phenomenon of LiDAR spoofing attacks, as presented in [c]. These attacks involve the injection of malicious data points into LiDAR sensors, posing significant driving hazards. Our work included an evaluation of the resilience of models pretrained using our CALICO framework, as well as other baseline methods, against these black-box sensor attacks. To simulate the real-world implications of such attacks, we crafted scenarios with varying degrees of complexity, introducing 60, 100, and 200 spoofing points to create the illusion of false objects 10 meters in front of the ego vehicle, adhering to the protocols established in [c].\n\nOur results are promising. We observed that CALICO notably diminished the success rates of these attacks in comparison to models trained from scratch and LiDAR-only methods. This increased robustness can be attributed to CALICO's ability to achieve a **more balanced** pretraining process, integrating both LiDAR and camera modalities effectively. In contrast, models trained from scratch or using LiDAR-only pretraining methods exhibited a higher susceptibility to attacks, because we found their end-to-end models mainly or entirely rely on the LiDAR modality.\n\nIn conclusion, while CALICO's primary aim is to enhance the efficiency and accuracy of 3D object detection models, it also plays a crucial role in fortifying these models against sophisticated adversarial attacks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096516326,
                "cdate": 1700096516326,
                "tmdate": 1700096516326,
                "mdate": 1700096516326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xbAb84RkGH",
                "forum": "U7iiF79kI3",
                "replyto": "IzdBxliZwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2HK8 [2/2]"
                    },
                    "comment": {
                        "value": ">  It appears that semantic pooling (feature rich vs feature less regions) plays a part from the figure 5, but I would like more examples of failure cases. The clustering methods look rather empirical. More experiments on how they work in different cases would be useful.\n\nIn our exploration of the unsupervised nature of our semantic pooling operation in the CALICO framework, we encountered certain limitations. For instance, the semantic pooling sometimes erroneously clusters tree points as semantic-rich. Nonetheless, these instances did not significantly impact overall performance. The primary objective during the pretraining phase is to learn robust and prominent representations within the model\u2019s backbone, a process that remains largely independent from downstream tasks. Misclassifying a small number of random objects as semantic-rich does not detrimentally affect the contrastive pretraining for the model backbone because the objective is to learn the **correspondence** between regions. Furthermore, the core concept of semantic pooling is pivotal, transcending the specifics of its implementation. Our current implementation utilized a basic approach to achieve region partitioning in LiDAR BEV space relying on independent LiDAR frames. Enhancements to semantic pooling could include integrating temporal data to refine point aggregation or to better aggregate points and identify moving points. However, such an approach necessitates temporally continuous data, introducing an additional layer of assumption and complexity. CALICO is designed to be versatile with respect to the choice of clustering algorithms, as long as the region partitioning is reasonable, as the core of CALICO is its following contrastive learning design based on the partitioned regions. \n\nWe have added an ablation study in Appendix B to quantitatively demonstrate the effectiveness of our semantic pooling compared to the random partitioning leveraged in original PLRC. We have included visualizations and a discussion of this in our revised manuscript, as shown in Figure 7 and 8  in Appendix C. In particular, Figure 7 visualizes a LiDAR frame with some failure clusters and Figure 8 showcases that our semantic pooling could successfully cluster some small but interesting objects.\n\n> I think it would help to learn if the system can demonstrate robustness against calibration error, a common occurrence in autonomous driving setups.\n\nWe are grateful for the reviewer's insightful suggestion to assess our system's resilience to calibration errors, a challenge in autonomous driving systems. Initially, it's worth clarifying that the primary objective of our framework is to enhance the efficiency and overall performance of self-driving perception tasks, with the robustness analysis serving as a supplementary aspect.\n\nOur current evaluation framework operates under the premise of accurate alignment between LiDAR and camera data. This alignment is pivotal for the efficacy of our pretraining methodology, particularly due to the necessity of data synchronization for successful knowledge distillation in RAD. In scenarios where we intentionally alter the calibration matrix between the two modalities, our method demonstrates similar performance to other baseline approaches.\n\nOn one hand, we acknowledge that in real-world scenarios, misalignments due to calibration errors can occur, potentially impacting the performance of systems like ours. Although our current study did not specifically focus on this aspect, we recognize its importance in the broader context of autonomous driving. On the other hand, evaluating calibration errors is challenging, as misalignments result in differing localization ground truths for both modalities (i.e., the ground truth bounding boxes also do not align with each other). Therefore, determining a method and metric to quantify the evaluation is controversial.\n\nIn response to the reviewer's valuable feedback, we have incorporated the above discussion in our revised manuscript about the potential implications of calibration errors on systems pre-trained using CALICO. This addition not only acknowledges the issue but also underlines it as a vital area for future research, aiming to further refine the robustness and reliability of autonomous driving technologies.\n\nWe thank the reviewer for pointing out the superfluous lines in our manuscript and we have removed them in our revised manuscript.\n\nWe hope our response addresses all the comments and that the reviewer will consider raising the rating accordingly. We are more than glad to answer any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096635789,
                "cdate": 1700096635789,
                "tmdate": 1700096635789,
                "mdate": 1700096635789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tSrlZdOQ4t",
                "forum": "U7iiF79kI3",
                "replyto": "IzdBxliZwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[a] He, Kaiming, et al. \"Momentum contrast for unsupervised visual representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[b] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[c] Sun, Jiachen, et al. \"Towards robust {LiDAR-based} perception in autonomous driving: General black-box adversarial sensor attack and countermeasures.\" 29th USENIX Security Symposium (USENIX Security 20). 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096651355,
                "cdate": 1700096651355,
                "tmdate": 1700096651355,
                "mdate": 1700096651355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kPqprYFilG",
                "forum": "U7iiF79kI3",
                "replyto": "IzdBxliZwf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
                ],
                "content": {
                    "title": {
                        "value": "Further clarifications"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful response. I still feel that the work is very empirical and does not shed theoretical insight (despite the additional detail provided in the appendix). Nonetheless, I really enjoyed reading the paper and can see that distillation + sensible ideas such as clustering could be very useful in a practical problem such as sensor fusion.\n\nThe ablations in appendix B show that the distillation idea works for different backbones. I would have hoped for more ablations on methods but am fairly convinced with the response. \n\nPerhaps I am not reading this correctly, but I notice that the paper does not beat (nor even match) BEVFusion's results - the paper reports an mAP of 60.1 (table 1) for Lidar+Camera, whereas in BEVFusion, they get a number of 75 or so (table 1 of BEVFusion [1]). As the paper uses the BEVFusion architecture, one might hope that it can match BEVFusion's results. Could the authors explain this? Can the authors also report distance based results - how does detection perform in near and far distances, and per-class metrics (car, pedestrian, etc.)? Can the authors shed light on these aspects? \n\n\n[1] BEVFusion: https://arxiv.org/pdf/2205.13542.pdf"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616995451,
                "cdate": 1700616995451,
                "tmdate": 1700619207513,
                "mdate": 1700619207513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "umjSpIrOEC",
                "forum": "U7iiF79kI3",
                "replyto": "KoQVzmuLri",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3981/Reviewer_2HK8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. It makes sense that the numbers are lower as the full train set wasn't used."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3981/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706024495,
                "cdate": 1700706024495,
                "tmdate": 1700706024495,
                "mdate": 1700706024495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]