[
    {
        "title": "Cosine Similarity Knowledge Distillation for Individual Class Information Transfer"
    },
    {
        "review": {
            "id": "6rmNpsZ73Y",
            "forum": "kKmi2UTlBN",
            "replyto": "kKmi2UTlBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_j6ZX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_j6ZX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new Knowledge Distillation (KD) approach that relies on cosine similarity to effectively transfer knowledge about each specific class from the teacher model to the student model during the knowledge distillation process. By treating the predictions from student and teacher models as vectors, the method utilizes the scale-invariant property of cosine similarity to optimize student learning. The authors also introduced the \"Cosine Similarity Weighted Temperature\" (CSWT) technique to enhance the knowledge transfer efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work employs cosine similarity for individual class information transfer, which is a departure from traditional KD techniques.\n2. Efficient Learning from the student model. The authors of the paper leverage the scale-invariant property of cosine similarity to optimize the student's learning from the teacher.\n3. With the introduction of the \"Cosine Similarity Weighted Temperature\" (CSWT) technique the student model refines the knowledge transfer process to obtain the most relevant information for every sample."
                },
                "weaknesses": {
                    "value": "1. The proposed model depends on the batch size. This could be a limitation when the batch size that needs to be adjusted for reasons like memory constraints."
                },
                "questions": {
                    "value": "1. Can this learning paradigm be used in an Out-of-distribution task or experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Reviewer_j6ZX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789158676,
            "cdate": 1698789158676,
            "tmdate": 1699636084699,
            "mdate": 1699636084699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2a5BLmZxU9",
                "forum": "kKmi2UTlBN",
                "replyto": "6rmNpsZ73Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the review of Reviewer  j6ZX"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer for their thorough and constructive comments. We are genuinely pleased that the novelty of the proposed method was recognized. In response to the reviewer's concerns, we are currently conducting additional experiments related to out of distribution. ***We would appreciate your review of these updated responses and kindly ask for your consideration in improving the score!***\n\nThank you again for your time, feedbacks and patience.\n\n**Answer about W1)**\n\nAs illustrated in Figure 4 of the main paper, our method demonstrates superior performance across various batch sizes, with the exception of very large batches (e.g., 512), when compared to vanilla KD. This observation implies that our approach, which achieves high performance even in small batch sizes, holds a distinct advantage over vanilla KD methods in real-world applications where memory constraints pose challenges.\n\n\n**Answer about Q1)**\n\nWe are following your advice and conducting experiments on the out-of-distribution task. We are pleased to share our findings from the following experiment, in which the in-domain data is CIFAR-100, and the out-domain data is CIFAR-10. \n\n|Teacher-Student|Res34-Res18|WRN40_2-WRN16-1|WRN40_2-WRN40-1|WRN40_2-WRN16-2|\n|:---:|:---:|:---:|:---:|:---:|\n|KD|72.67|46.08|59.48|62.07|\n|Ours|**73.01**|**49.91**|**61.51**|**64.19**|\n|Difference|**+0.34**|**+3.83**|**+2.04**|**+2.13**|\n\nWe conducted three runs for each architecture and reported the average. While this approach is tailored for standard KD tasks, we demonstrate its effectiveness even in OOD tasks, surpassing vanilla KDs. We plan to incorporate this into our future research. Your comments are greatly appreciated."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029329679,
                "cdate": 1700029329679,
                "tmdate": 1700029329679,
                "mdate": 1700029329679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DQxWepagR5",
                "forum": "kKmi2UTlBN",
                "replyto": "2a5BLmZxU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Reviewer_j6ZX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Reviewer_j6ZX"
                ],
                "content": {
                    "title": {
                        "value": "Comments after rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for addressing my concerns and for conducting the experiment. I recommend this paper. Nice work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639274284,
                "cdate": 1700639274284,
                "tmdate": 1700639274284,
                "mdate": 1700639274284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4urijx3NqH",
            "forum": "kKmi2UTlBN",
            "replyto": "kKmi2UTlBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_RUAZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_RUAZ"
            ],
            "content": {
                "summary": {
                    "value": "The author sims to involve a cosine similarity KD loss based on a batch-level KD signal into student model learning. A combination of fix-temp softmax and a adaptive-temp softmax is also introduced in KD process. Comprehensive experiments including entropy analyses and ablation study are conducted."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. clearly introduce the research gap between existing KD techniques and the proposed\n\nS2. Comprehensive experiments are conducted"
                },
                "weaknesses": {
                    "value": "W1. Cosine similarity KD loss is the key but the explanation is unclear\n\nW2. presentation structure might be reorganized\n\nW3. experimental results are not convincing due to unclear methodology"
                },
                "questions": {
                    "value": "C1. In Eq. (2) and (3), what does \":\" mean? What's the difference between \"[:,j]\" in (2) and \"[i,:]\" in (3)? Does \"[:,j]\" mean by concatnating all vectors from 1 to j (if so, please refer to C3)? It said that p_s, p_t \\in R^{B\u00d7C} which is a matrix. How to compute the consin similarity for two matrix based on Eq (1)? \n\nC2. By the explanation in the below of Eq (3), it seems that the loss only focuses on one class j? I got the answer Yes from Eq (9). So it's suggested to make the definition clearer or reorganize the presentation order. It's better to explain all notations with a formal way without the assumption that audiences also understand what's presented.\n\nC3. Though cosine similarity is scale-invariant, the significance of a cosine similarity value depends on vector length (i.e., the dimension of the vector). For a 200-d vector, 0.4 may be a relative large value. But for a 5-d vector, 0.7 does not mean two vectors are similar. For classification, the number of classes and  batch size are two factors to affect the vector length. That means, even for the same task, for difference batch size, the proposed would get different student models.\n\nC4. The authors must take a long time to conduct extensive experiments in following sections to show the advantage of the proposed. However, concerns exist in the results due to unclear definitions and intuitions (please refer to C1 to C3)\n\n\n==========\n\nThe authors response help my understanding on the methodology. I would like to raise my review score.\n\nHowever, the concern is still on the batch-wise cosine similarity. Overall, I don't think it meet the acceptance bar."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1563/Reviewer_RUAZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877042981,
            "cdate": 1698877042981,
            "tmdate": 1700687805768,
            "mdate": 1700687805768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kHAyVYlY1J",
                "forum": "kKmi2UTlBN",
                "replyto": "4urijx3NqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the review of Reviewer RUAZ-Part-1"
                    },
                    "comment": {
                        "value": "We appreciate the valuable feedback provided for our work. Over the past few days, we have diligently worked to address all the concerns raised. ***We kindly request the reviewer to reconsider and potentially raise the score, and we have outlined our responses to each point below.*** We will update the paper and appendices with the following your comments. Please feel free to let us know if you have any further concerns or questions to discuss.\n\n**Answer about Q1 and Q2**\n\nFirst of all, I apologize for any confusion regarding the notation. I explain this first and then share the amendments at the end. We adopted the meaning of '$:$' from Python array slicing. I am truly sorry for not being able to indicate this. The notation $i$ refers to the $i$-th element in the batch size, and the notation $j$ refers to the $j$-th class in the total number of classes.\n\nAs a result, $p_{[i, :]}$ represents the distribution of prediction about single image sample, taking the form of vectors in $\\mathbb{R}^{C}$. In contrast, $p_{[:, j]}$ denotes the distribution of prediction across samples for a single class, expressed as vectors in $\\mathbb{R}^{B}$.\n\nTherefore, these expression allows us to easily compute the cosine similarity between the two vectors, not two matrix.\n\nSeveral concerns have been raised about the methodology, prompting the following correction. Your feedback on these adjustments would be greatly appreciated.\n\nThe modification of Equations transforms it into the notation of \u2018$tr$\u2019 because the meaning of $[:, j]$ means a transpose function of $[i, :]$. (i.e., $p_{[i, :]}^{s, t}=p_{i}^{s,t} \\in \\mathbb{R}^{C}, p_{[:, j]}^{s, t}= p_{tr, j}^{s,t}\\in \\mathbb{R}^{B}$)\n\nThe $p^s$ denotes the student's prediction, and the $p^t$ denotes the teacher\u2019s prediction. $B$ refers to the size of the batch and $C$ refers to the number of classes.\n\nPrevious formation eq (2):\n\n#### $L_{CSKD} ( p_{[:, j]}^s, p_{[:, j]}^t, T ) =$ $1-\\cos (\\theta) =  1- $$p_{[:, j]}^s\\cdot p_{[:, j]}^t\\over\\vert p_{[:, j]}^s\\vert\\vert p_{[:, j]}^t \\vert$\n\nAfter transformation eq (2):\n\n#### $L_{CSKD} (p_{tr, j}^{s}, p_{tr, j}^{t}, T)  =$ $1-\\cos (\\theta) = 1- $$p_{tr, j}^{s}\\cdot p_{tr, j}^{t}\\over\\vert p_{tr, j}^s\\vert\\vert p_{tr, j}^t \\vert$\n\n\nPrevious formation eq (3):\n\n### $p_{[i,:]}^{s,t}  =$ $e^{z_{i}^{s,t}/T}\\over{\\sum_{k=1}^{C}{e^{z_{k}^{s,t}/T}}}$\n\n\nAfter transformation eq (3):\n\n### $p_{i}^{s,t}  =$ $e^{z_{i}^{s,t}/T}\\over{\\sum_{k=1}^{C}{e^{z_{k}^{s,t}/T}}}$\n\n\n\nPrevious formation eq (4):\n\n### $cs_{i}=$$p_{[i, :]}^s\\cdot p_{[i, :]}^t\\over\\vert p_{[i, :]}^s\\vert\\vert p_{[i, :]}^t \\vert$\n\nAfter transformation eq (4):\n\n### $cs_{i}=$$p_{i}^s\\cdot p_{i}^t\\over\\vert p_{i}^s\\vert\\vert p_{i}^t \\vert$\n\nPrevious formation eq (7) and eq (8):\n\n#### $L_{CSWT} ( p_{[:, j]}^s, p_{[:, j]}^t, T_{i} ) =$ $1-\\cos (\\theta) = 1- $$p_{[:, j]}^s\\cdot p_{[:, j]}^t\\over\\vert p_{[:, j]}^s\\vert\\vert p_{[:, j]}^t \\vert$\n\n### $p_{[i,:]}^{s,t}  =$ $e^{z_{i}^{s,t}/T_{i}}\\over{\\sum_{k=1}^{C}{e^{z_{k}^{s,t}/T_{i}}}}$\n\nAfter transformation eq (7) and eq (8):\n\n#### $L_{CSWT} ( p_{tr, j}^s, p_{tr, j}^t, T_{i} ) =$ $1-\\cos (\\theta) = 1- $$p_{tr, j}^s\\cdot p_{tr, j}^t\\over\\vert p_{tr, j}^s\\vert\\vert p_{tr, j}^t \\vert$\n\n### $p_{i}^{s,t}  =$ $e^{z_{i}^{s,t}/T_{i}}\\over{\\sum_{k=1}^{C}{e^{z_{k}^{s,t}/T_{i}}}}$\n\nPrevious formation eq (9):\n\n#### $L_{Total} ( p^s, p^t, \\theta_{s}, \\theta_{t}, T, T_{i} ) =$ $L_{CE} ( p^s ; \\theta_{s}) + \\alpha($$1\\over{C}$$\\sum_{j}^{C}L_{CSKD}(p_{[:,j]}^s, p_{[:,j]}^t, T))+$$1\\over{C}$$\\sum_{j}^{C}L_{CSWT}(p_{[:,j]}^s, p_{[:,j]}^t, T_i)$\n\nAfter transformation eq (9):\n\n#### $L_{Total} ( p^s, p^t, \\theta_{s}, \\theta_{t}, T, T_{i} ) =$ $L_{CE} ( p^s ; \\theta_{s}) + \\alpha($$1\\over{C}$$\\sum_{j}^{C}L_{CSKD}(p_{tr, j}^s, p_{tr, j}^t, T))+$$1\\over{C}$$\\sum_{j}^{C}L_{CSWT}(p_{tr, j}^s, p_{tr, j}^t, T_i)$\n\nWe provide an explanation for the notation $p_i^{s,t}$ because, when computing cosine similarity between $p^{s,t}_{tr, j}$ , the initial step involves obtaining $ p_i^{s,t} $ and subsequently transposing to derive \n\n$p^{s,t}_{tr, j}$\n\nFor instance, for $p_i^{s,t}$, we pass the logits of the $i$-th image through the softmax function, and then we obtain the probability value for that image. Extending this to a batch size(let's say, with 64 images), we acquire the probability distribution for each of the 64 images ($p^{s,t}$), presented in the form of a $B \\times C$ matrix. By transposing this matrix $(C \\times B)$ and isolating the distribution for $j$-th class, we obtain \n\n$p^{s,t}_{tr, j}$\n\n\u2014our desired output. This $p^{s,t}_{tr, j}$ can then be utilized for calculating cosine similarity.\n\n\nI hope the above modifications aid your understanding."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038874981,
                "cdate": 1700038874981,
                "tmdate": 1700044734027,
                "mdate": 1700044734027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QipOP3g610",
                "forum": "kKmi2UTlBN",
                "replyto": "kHAyVYlY1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Reviewer_RUAZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Reviewer_RUAZ"
                ],
                "content": {
                    "title": {
                        "value": "authors response addressed some of my concerns"
                    },
                    "comment": {
                        "value": "Thank you very much for the response. The clarification help me understand the methodology. I would like to raise my review score.\n\nHowever, the concern is still on the batch-wise cosine similarity. Overall, I don't think it meet the acceptance bar."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687671678,
                "cdate": 1700687671678,
                "tmdate": 1700687671678,
                "mdate": 1700687671678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QMhFi0oDqV",
            "forum": "kKmi2UTlBN",
            "replyto": "kKmi2UTlBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_9Nip"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1563/Reviewer_9Nip"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a method for knowledge distillation (KD) using cosine similarity, which yields favorable results on commonly used datasets such as CIFAR-100 and ImageNet. Despite its significant effectiveness, the paper still faces several issues that currently place it significantly below the acceptance threshold for ICLR.                          \n\nIssues:\n1. The author's concept of cosine distance is commendable but lacks a comparison and discussion with existing KD methods based on KL divergence, such as SHAKE [1] and DKD [2]. Incorporating relevant comparative analysis in the next version would be beneficial.\n2. The paper does not adequately explore the relationship between the proposed method and existing KD techniques; it merely provides result comparisons. This leaves readers struggling to understand the unique significance of the proposed approach. A deeper discussion of the differences between these two types of KD methods is needed in the related work section.\n3. The motivation behind the entire loss function is unclear. While the author introduces a temperature parameter (T) in Equation 3, its specific setting is absent in subsequent explanations. In Equation 9, where two loss functions are introduced, there is only one balancing factor, leading to reader confusion.\n4. There are overall writing issues in the paper, including citation formatting and writing errors such as inconsistent tenses and mixed usage of abbreviations (e.g., Fig. vs. Figure, Table vs. Tab.). Careful proofreading and editing are required to enhance professionalism.\n5. Figure 4 lacks a detailed explanation, making it challenging for readers to understand the purpose of the experiment and the impact of batch size on the results. More background information and clarification are needed.\nI hope this feedback helps in further improving your research. \n\n[1] Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer. 2022. In NeurIPS.  \n[2] Decoupled Knowledge Distillation. 2022. In CVPR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "See summary"
                },
                "weaknesses": {
                    "value": "See summary"
                },
                "questions": {
                    "value": "See summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698930608495,
            "cdate": 1698930608495,
            "tmdate": 1699636084545,
            "mdate": 1699636084545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SkRooXeOgW",
                "forum": "kKmi2UTlBN",
                "replyto": "QMhFi0oDqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the review of Reviewer 9Nip-Part-I"
                    },
                    "comment": {
                        "value": "We appreciate the valuable feedback provided for our work. Over the past few days, we have diligently worked to address all the concerns raised. ***We kindly request the reviewer to reconsider and potentially raise the score, and we have outlined our responses to each point below***. We will update the paper and appendices with the following your comments. Please feel free to let us know if you have any further concerns or questions to discuss.\n\n**Answer about Q1 and Q2)**\n\nSince both SHAKE and DKD are KL-divergence-based KD methodologies, our focus was on comparing cosine similarity distances with KL-divergence itself. KL-divergence is characterized by attempting to 'exactly match' the distribution of student predictions with the distribution of teacher predictions. This implies that the logit values of students and teachers must be 'exactly identical' to make the loss value zero. However, these matching criteria impose overly strict constraints on students, hindering their ability to acquire flexible dark knowledge from the teacher distribution and resulting in a decline in student performance.\n\nTo address these constraints, we propose losses using cosine similarity. Cosine similarity possesses scale-invariant properties. Therefore, it is distinguished by providing flexibility to the logits of students without requiring them to be exactly the same as the teacher distribution. Consequently, the use of cosine similarity makes the acquisition of dark knowledge by the student distribution more flexible (as referred in Figure 2 of the main paper). We validated this through entropy analysis experiments (as referred in Figure 3 of the main paper), demonstrating that our method exhibits greater entropy compared to conventional KD methods. (It is important to note that higher entropy corresponds to a greater amount of dark knowledge.)\n\nThat can be expressed using the following formula.\n\nKL divergence: $D=D_{KL}(p^{t}, p^{s})=0$ when $p^{t}=p^{s}$\n\nCosine Similarity distance remains scale-invariance:\n\n$D=D_{cs}(p^{t}, p^{s})=0$ when $p^s=kp^t$, where $k>0$.\n\n***Proofs of invariance of consine similarity***\n\nWe demonstrate through the following equation that cosine similarity becomes scale-invariant.\n\nConsider that the student and teacher predictions are denoted as $p^s$ and $p^t$, respectively, and magnitude scaling on $p^s$ and $p^t$ can be formulated as $ap^t$ and $bp^s$, where $a\u00d7b>0$.\n\n$D_{cs}(ap^t, bp^s)=$$(ap^t)(bp^s)\\over\\vert ap^t \\vert\\vert bp^s \\vert$=$\\sum(ap^t)(bp^s)\\over\\sqrt{\\sum(ap^t)^2}\\sqrt{\\sum(bp^s)^2}$=$ab\\sum{(p^t)(p^s)}\\over{a\\sqrt{\\sum{(p^t)^2}}b\\sqrt{\\sum{(p^s)^2}}}$=$D_{cs}(p^t,p^s)$\n\n***Related Works***\n\nIn addition, we will provide comparison with existing KD methods, DKD and SHAKE, as follows:\n\nDKD suggested dividing KL-divergence into TCKD and NCKD to minimize the suppression of dark knowledge (here, referred as NCKD) as the confidence of the teacher model in the training sample increases. In contrast, our approach addresses this challenge by introducing cosine similarity with scale-invariant properties. These scale-invariant properties alleviate constraints on the student model, enhancing its flexibility to acquire information from the teacher model. This, in turn, facilitates the acquisition of a more diverse range of dark knowledge.\n\nSHAKE generates an optimized model for student model by introducing a proxy teacher, departing from the traditional KD method of transferring knowledge from pretrained teachers to students. This approach aligns with our belief that conventional KL methods do not provide the most effective information to students. However, SHAKE requires additional training for the individual shadow head for this purpose, incurring an extra cost of approximately 1.28 times that of conventional KDs. On the other hand, our method can transfer optimal information without the need for extra training.\n\nMoreover, we aim to conduct further experiments by exploring mutual training between students and teachers using cosine similarity, similar to SHAKE. We intend to compare the results of these experiments with those of SHAKE to gain a comprehensive understanding of their effectiveness. \n\nWe are currently working on acquiring further comparative analyses, including visualizations such as \"tSNE\" and \"logits correlation\", which we will incorporate."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028217108,
                "cdate": 1700028217108,
                "tmdate": 1700028700704,
                "mdate": 1700028700704,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wpucnh1xpE",
                "forum": "kKmi2UTlBN",
                "replyto": "QMhFi0oDqV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the review of Reviewer 9Nip-Part-2"
                    },
                    "comment": {
                        "value": "**Answer about Q3)**\n\nInitially, the balancing factor for 'CSWT' was set to 1, resulting in only one balancing factor in Equation 9. However, I will modify the formula as follows to incorporate your suggestion.\n\n$L_{Total}=L_{CE}+\\alpha($$1\\over{C}$$\\sum_{j=1}^{C}L_{CSKD}(p^s, p^t, T))+\\beta($$1\\over{C}$$\\sum_{j=1}^{C}L_{CSWT}(p^s, p^t, T_i))$\n\nThe temperature $(T)$ applied to the CSKD loss was consistently set to 4 throughout the entire experiment, a commonly used value in other KD papers.\n\nFurthermore, CSWT is employed in conjunction with the CSKD loss in entire loss function, utilizing cosine similarity to ensure that the student model receives optimal dark knowledge. This involves applying different temperature scaling $(T_i)$ for each sample, dynamically adjusting the distribution according to the sample. Specifically, a small temperature is applied when the cosine similarity value between the student distribution and the teacher distribution is high, and a higher temperature is applied when it is low.\n\n**Answer about Q4)**\n\nThank you for your careful and detailed comments; I will reflect them into main paper and the appendix.\n\n**Answer about Q5)**\n\nOur method is designed to leverage scale-invariant characteristics, providing the student model with richer dark knowledge from the teacher model. For comparing dark knowledge, specifically non-target class information, we utilized entropy, as illustrated in Figure 3 of main paper.\n\nIn contrast to the previous KD method, which uses class distribution for one image as logits, we utilize the batch distribution for one class as logits. This choice is motivated by the fact that cosine similarity learns relational information rather than accurate value matching of the distribution. Using batch predictions allows us to obtain more diverse relational information, considering the constraint that class predictions have a total sum of 1.\n\nHowever, it's important to note that as the batch size increases, the number of relations that need to be satisfied also increases. Consequently, we infer that our method approaches KL-divergence-based KD with an exact matching property as the batch size increases. To illustrate this relationship with constraints, we present an entropy analysis in Figure 4 of main paper.\n\nIn general, devices with limited memory that require knowledge distillation necessitate high performance at a small batch size. Therefore, our method holds a significant advantage over conventional KL methods."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700028609009,
                "cdate": 1700028609009,
                "tmdate": 1700037226065,
                "mdate": 1700037226065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]