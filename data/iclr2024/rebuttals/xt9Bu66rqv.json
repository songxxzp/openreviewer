[
    {
        "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning"
    },
    {
        "review": {
            "id": "TH5rp7zdRI",
            "forum": "xt9Bu66rqv",
            "replyto": "xt9Bu66rqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_aLZ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_aLZ7"
            ],
            "content": {
                "summary": {
                    "value": "This paper unifies a number of recent methods in offline RL and IL through the lens of regularized policy learning and Lagrange duality. Specifically, the paper views the regularized policy learning problem as a convex optimization problem with equality constraints on the state-action visitation distribution. Then, deriving the dual version of two equivalent primal problems, each named dual-Q and dual-V, the authors show that many recent offline IL and RL methods such as IQLearn and XQL, are specific instance of those problems with a particular choice of the f-divergence as the regularizer. Based on this analysis, the authors propose two algorithms for fixing shortcomings of previous algorithms. For offline IL, ReCOIL is presented as a new dual-Q algorithm that can leverage suboptimal demonstration data and that does not require the coverage assumption needed in prior work. For offline RL, f-DVL is proposed as a new dual-V algorithm as an extension of XQL, which results in more stable training by choosing a different f-divergence. The proposed methods are evaluated on the locomotion and manipulation tasks from the D4RL benchmark, and baseline comparisons show superior performance of ReCOIL and f-DVL on various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The major novel contribution of the paper is the theory which shows that various existing offline IL and RL methods can be derived from a Lagrange dual formulation of a single regularized policy learning problem. This itself has a huge consequence. For instance, showing that pessimistic value learning such as CQL is essentially a dual RL problem brings about a new insight into analyzing their properties and deficiencies. The self-contained review as well as derivations of key theoretical results in Appendix C has a core value in the paper, where the math is relatively easy to follow with the basic knowledge of convex optimization. This analysis itself paves a new way to developing many practical algorithms for both offline IL and RL, and has a significant potential impact on future research in this domain."
                },
                "weaknesses": {
                    "value": "On the other hand, I have found a handful of math errors throughout the paper, especially in Appendix C, which critically degrades the quality of such a theory-driven paper. While many of them can be simple typos, some may actually require careful consideration. I will list them below.\n\n* ~~For the state-action visitation distribution $d(s,a)$ to satisfy the Bellman-flow constraint in the form of equation (2), I think that the definition of $d$ has to involve the $(1 - \\gamma)$ correction term: $d(s, a) = (1 - \\gamma) \\pi(a \\mid s) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s \\mid \\pi)$.~~\n\n* ~~In equation (9) on p.6 and p.28, the conjugate of the f-divergence should be just $f^*$, not $f^*_p$ because the positivity constraint on the visitation distribution is not necessary in the dual-Q formulation.~~\n\n* ~~In equation (29) you have the term $\\gamma \\sum_{s'} p(s' \\mid s, a) \\pi(a' \\mid s') Q(s', a')$. I believe that the sum must be taken over both $s'$ and $a'$.~~\n\n * There is a typo in equation (35). The second term inside $f^*$ should be $\\gamma \\sum_{s', a'} p(s' \\mid s, a) \\pi(a' \\mid s') Q(s', a')$, not $\\gamma \\sum_{s'} p(s' \\mid s, a) \\pi(a \\mid s') Q(s', a')$.\n\n~~* There are two typos in equation (38). First, the expectation in the first term needs to be over both $(s, a) \\sim d(s, a)$, not $s \\sim d(s, a)$. Second, I believe that the last term has to be $\\sum_a d(s, a)$, not just $d(s, a)$.~~\n\n* ~~The Lagrangian stationarity condition as stated on p.20 does not seem to be consistent with equation (43). Taking the derivative of the Lagrangian with respect to the primal variables, I suppose that the stationarity is $d^o(s, a) \\delta_V(s, a) -d^o(s, a) f'(w^*(s, a)) + \\lambda^*(s, a) = 0 ~\\forall s \\in S, a \\in A$. Also, there seems to be an implicit assumption that $d^o(s, a)$ is non-zero everywhere, and the degenerate case of $d^o(s, a) = 0$ is not discussed carefully. It seems that the Lagrangian stationarity is not informative enough to uniquely determine the relationship between $f'(w^*(s, a))$ and $\\delta_V(s, a)$ in the degenerate case.~~\n\n* ~~In equation (110), $\\alpha$ should be replaced by $\\beta$.~~\n\n\nRegarding the empirical results in Section 7, the quality of presentation has much room for improvement, as described below.\n* It is unclear what the left-most figure in Figure 1(a) represents and how it is related to the squared distribution gap plot right next to it.\n\n* In Table 2, it is unclear how the authors determined to highlight some of the entries with blue bold texts as an indication of the most-performant policy. For instance, In the hopper task with the medium few-expert dataset, the RCE seems to have the highest mean but not highlighted in blue.\n\n* ~~Similarity in Table 3, it is unclear what rules were applied to highlight entries in blue. For example, IQL on the antmaze-large-diverse-v0 dataset had a larger return than f-DVL, but both IQL and f-DVL are highlighted.~~\n\n* ~~Moreover, I do not see a huge performance gap between f-DVL and prior methods, and it is hard to judge whether the proposed RL methods empirically outperform existing ones (partially because there is no standard deviation information in the table). In fact, a recent study [1] finds that just reporting the mean and the standard deviation of different RL policies is insufficient, and suggests reporting confidence intervals or performance profiles as more reliable performance measures. The authors should consider leveraging some statistical tools like [1] in comparing policies, especially in situations like Table 3 where many methods have similar average returns.~~\n\n[1] Agarwal, Rishabh, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc Bellemare. \"Deep reinforcement learning at the edge of the statistical precipice.\" Advances in neural information processing systems 34 (2021): 29304-29320."
                },
                "questions": {
                    "value": "* ~~I do not see how one can directly derive equation (14) from equation (5), as a rewriting of dual-V with the temperature parameter $\\lambda$. Can you elaborate on the derivation? I believe that the discount factor $\\gamma$ is determined by the problem specification and is not a hyperparameter that can be tuned arbitrarily.~~\n\n* ~~In equations (32) to (34), can you elaborate on why the interchangeability principle holds in this specific case? In other words, can you explain why $\\max$ and $\\mathbb{E}$ can be exchanged?~~\n\n* ~~All the analysis presented in the paper implicitly assumes discrete state and action spaces (which I think is good for readers in the RL community to follow the math). Do the results naturally extend to continuous spaces? I would be surprised if they did not, but I bet we need more sophisticated tools from real-analysis and probability theory to prove the same results.~~"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Reviewer_aLZ7",
                        "ICLR.cc/2024/Conference/Submission7641/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698061131,
            "cdate": 1698698061131,
            "tmdate": 1700505947696,
            "mdate": 1700505947696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3Rau1sAABi",
                "forum": "xt9Bu66rqv",
                "replyto": "TH5rp7zdRI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aLZ7 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and thoughtful review. We have uploaded a revision with all the suggested typos corrected. The changes are marked in red. Please find the clarification to the queries below:\n\n1. **There seems to be an implicit assumption that $d^O$ is non-zero everywhere**\n\nWe had earlier discussed the assumption of sufficient coverage of $d^O$ over $\\mathcal{S}\\times \\mathcal{A}$ above Equation 32, but make this more clear in the revised paper. The degeneracy without this assumption is expected as a result of using importance sampling (requires coverage on importance sampled distribution) as well as $f$-divergences which may be ill-defined without enough support. We discuss a potential solution for this in Theorem 2 (see Assumption 1 in theorem 2) where we constrain the search set of policies within the support of offline dataset (similar to the idea of pessimism). In this case, the best we can expect to do is return a policy with highest performance within the dataset.\n\n\n2. **It is unclear what the left-most figure in Figure 1(a) represents and how it is related to the squared distribution gap plot right next to it.**\n\nWe have updated the caption in the revised paper as well as included a detailed explanation of the experiment in Appendix H.3. Please let us know if you need more clarification.\n\n\n3. **In Table 2, it is unclear how the authors determined to highlight some of the entries with blue bold texts as an indication of the most-performant policy.**\n\nWe have updated the highlighting to be more accurate and have highlighted all offline imitation algorithms that lie within the standard deviation of the algorithm with the highest mean performance. The revised paper mentions this in the caption now.\n\n4. **Similarity in Table 3, it is unclear what rules were applied to highlight entries in blue.**\n\nWe have updated the highlighting to mark all offline RL algorithms within one point performance of the top-performing algorithm. Note that in the offline RL experiments, the results extend the performance reported by prior works and the standard deviation is unavailable. Comparing using standard deviation would require rerunning all the baselines, which we believe will take more than the discussion time limit and compute capacity available to us at the moment. We have updated the revised paper to mention this in the caption now. \n\n\n5. **Moreover, I do not see a huge performance gap between f-DVL and prior methods, and it is hard to judge whether the proposed RL methods empirically outperform existing ones.**\n\nThe locomotion benchmark in D4RL for offline RL is simpler with performance to certain extent saturated, compared to the Antmaze or Kitchen environments. A detailed discussion on the properties of these environment and datasets can be found in [2]. We notice that $f$-DVL outperforms in 6/9 tasks in antmaze and kitchen environments which pose a harder challenge of stitching suboptimal trajectories compared to recently proposed IQL[3], XQL[4] and DT[5]. We believe that these improvements over prior work are significant by the standards used in prior offline RL papers. \n\nOne of the contributions through $f$-DVL is also to provide another axis of improvement in a stable learning alternative to XQL. Indeed, using statistical metrics like IQM would be more suitable but unfortunately prior works in offline RL(XQL, IQL, etc) have used mean as an estimate, and computing IQL would require rerunning all the baselines which would take more time and more compute budget than we have available. \n\n\n6. **I do not see how one can directly derive equation (14) from equation (5), as a rewriting of dual-V with the temperature parameter.**\n\n Note that Equation 14 and Equation 5 are not equivalent - we consider a simpler objective for optimization with identical properties. Indeed the discount factor in $\\hat{Q}$ (Eq. 14) remains unchanged and $\\lambda$ is used as a substitute for the temperature parameter ($\\alpha$) of the second term (multiplying by $\\frac{1-\\gamma}{1-\\lambda}$, results in exactly the same first term as Equation 5 and the weight on second term $\\frac{\\lambda(1-\\gamma)}{1-\\lambda}$ acts as a tunable temperature parameter).  A more detailed discussion about this conversion is in Section G.4 in the appendix.\n\n7. **In equations (32) to (34), can you elaborate on why the interchangeability principle holds in this specific case?**\n\nConsider the unconstrained optimization in Equation 33 w.r.t $d$. The inner optimization objective is an upper semicontinuous concave function. The idea is simply that for every $s,a$ in the expectation one can pointwise maximize $d$, and the maximizer is guaranteed to exist by assumption of concavity and upper semicontinuity. So taking the maximum inside or outside the expectation will lead to same maximizer per $s,a$. We refer the reviewer to Lemma 1 in [1] for a formal proof."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152052375,
                "cdate": 1700152052375,
                "tmdate": 1700152052375,
                "mdate": 1700152052375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1V6kwScRSJ",
                "forum": "xt9Bu66rqv",
                "replyto": "iA40YEi9kN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_aLZ7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_aLZ7"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for making revisions and providing detailed comments. Most of my concerns are addressed, with only a few questions remaining.\n\n1. Equation (35) remains unchanged in the revised manuscript. Could you please look at my comments in the original review again and confirm whether (35) is correct as is? My concern is that $a'$ is not marginalized out in the expectation.\n\n2. You mentioned in the response that the caption in Table 2 was updated to mention the highlighting rule, but I do not see it. Can you double-check?\n\n3. Figure 1(a) Left is not totally clear yet. What does the bottom row represent? What does \"Truth\" mean and which \"policy\" is plotted here?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506580452,
                "cdate": 1700506580452,
                "tmdate": 1700506580452,
                "mdate": 1700506580452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ay1PjMg1lD",
            "forum": "xt9Bu66rqv",
            "replyto": "xt9Bu66rqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_JKJA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_JKJA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework called dual RL based on the LP formulation of RL. The author shows dual RL can be used to derive offline RL and offline IL algorithms, providing a unification of existing approaches. Based on this insight, they propose new offline IL and RL algorithms (ReCOIL and f-DVL), which remove existing assumptions. These algorithms are tested in the D4RL benchmarks empirically and achieve competent performance compared with other baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed dual RL framework based on the duality of a regularized RL problem is quite clean. The authors provide good background information and detailed derivations, which makes the paper self-contained and easy to follow. The dual RL framework makes some connections of existing works and suggests potential generalization (which the authors adopt) to design new algorithms. The empirical results are promising. Overall, this is an insightful paper."
                },
                "weaknesses": {
                    "value": "There are some technical parts, which I think are not fully correct (see below). If they're indeed wrong, then the contribution of this paper would be compromised. The current writing is also quite dense as well, though the details are provided in the appendix. It would be better if the main paper is more readable. Lastly, in some sense, this paper lacks novelty. The unifying perspective and connection are interesting, but at the end the resulting algorithms can be viewed as minor tweaks from existing ones. Nonetheless, I think this last point on novelty is minor."
                },
                "questions": {
                    "value": "1. Proposition 4: The authors write that CQL can be cast as a dual-Q problem with a right choice of f-divergence (Pearson). However, in the proof of Proposition 4 in Appendix D.1, Eq (70) actually corresponds to the objective of ATAC [Cheng et al. 2022], not CQL. See Eq (1) in [Cheng et al. 2022], which is exactly the same Eq (70) here. To my understanding, the main difference between CQL and ATAC is that ATAC takes a maxmin formulation, whereas CQL doesn't, though they have a similar objective for learning Q. The dual-Q formulation here is a maxmin problem. The authors should update Proposition 4 to make the connection to ATAC, which would be more appropriate and precise.\n\n2. Bug in the proof of Theorem 1. There is a minor bug in the proof of Theorem 1. In the block starting \"Imitation from Arbitrary data (dualQ)\" after Eq (108), max_d is moved inside min_Q, whereas max_d is outside min_Q in Eq (108). So Eq (109) is not just a summary, and that \"we summarize the result of the derivation so far\" can be misleading. But this can be easily fixed though, by applying strong duality before moving forward. Another minor suggestion: I think it would be good to write out the proof Eq (112) for completeness.\n\n3. Validity Eq (10). In the proof, the author assume d_0 = d_E. I think that this is a fairly strong assumption, as it implies that the initial state at the test time would be drawn from the same distribution of the expert. This is rarely the case in practice, and in my experience it's not a common assumption in offline or off-policy RL. The authors should highlight this assumption in the main paper, or the authors should present Eq (140) as the main result. A minor bug in the proof is the derivation misses a 1/4 factor in front of the square loss.\n\n4. Bug in Proof of Proposition 3. There're a few issues in the derivation, and one is major. \n    a. f', (f')^-1 are not strictly increasing. it's only non-decreasing. e.g. see f(t) = | t-1|. \n    b. why is f*(x) = -f(0) for x<0? \n    c. why is f(0+)>0?\n    d. You write (f')^{-1}(t) >0 when t>0 and 0 otherwise. What does \"0 otherwise\" mean?\n    e. (Major issue). From Eq (79) to Eq (80), it doesn't use \"(f')^{-1}(t) >0 when t>0\" (though the paper writes so). It uses the other way (f')^{-1}(t) >0  --> t>0 and likewise (f')^{-1}(t) <0  --> t<0. But they are not true. See e.g. f(t) = | t-1| again. This error step prevents me from validating the remaining part. \n\n5. The experiment design of offline IL can be a bit confusing. It says the agent is given 1 expert demonstration. But the suboptimal transition dataset actually contains 200 expert demonstrations, though I understand they're not marked as expert demonstrations to the algorithm. I think a more convincing experiment is to completely remove the expert demonstrations in the suboptimal data. Table 2 writes 1000 expert transitions. Is that the same as one expert trajectory?\n\n6. I don't know how to interpret Fig 1. Can you explain more?\n\nMinor: \n\n1. This sentence is confusing to read. \"Unfortunately, one of the most successful classes of offline RL methods .. has evaded connections to regularized policy optimization. Proposition 2 shows, perhaps surprisingly, that XQL can be cast as a dual of regularized policy learning, concretely as a dual-V problem.\"\n\n2. I don't understand the connection between the sentence \"To prevent extrapolation error in the offline setting, we rely on an implicit maximizer [Garg et al., 2023] that estimates the maximum over the Q-function conservatively with in-distribution samples.\" and Eq (12)\n\nReference: \nCheng, Ching-An, et al. \"Adversarially trained actor critic for offline reinforcement learning.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820667722,
            "cdate": 1698820667722,
            "tmdate": 1699636928855,
            "mdate": 1699636928855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B6pDgCWHCy",
                "forum": "xt9Bu66rqv",
                "replyto": "ay1PjMg1lD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JKJA (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide a thoughtful and thorough review of the paper. We uploaded the revised paper with the changes highlighted in red. Please find the clarification to the queries below:\n\n1. **Proposition 4: The authors write that CQL can be cast as a dual-Q problem with a right choice of f-divergence (Pearson). However, in the proof of Proposition 4 in Appendix D.1, Eq (70) actually corresponds to the objective of ATAC [Cheng et al. 2022], not CQL.**\n\nWe appreciate reviewer's suggestion of expanding the family of algorithms unified by dual-RL by including ATAC[1]. We have updated Proposition 4 and it's proof to point out the connection to ATAC. \n\nWe believe Conservative Q-learning (CQL) and ATAC share some similarities in their objectives, which makes it hard to discard CQL from this family.  The reviewer mentions that CQL does not formulate their objective as a min-max game. However, we believe this is not completely correct. Directing the reviewer's attention to Equation 3 in CQL, a min-max game is proposed between an action-proposal distribution $\\mu$ and the Q-function. The authors point out that $\\mu$ can be taken as the last policy iterate (also used in their implementation for high-dimensional action spaces) making it particularly close to the objective coming out of dual-RL. A straightforward difference between CQL and ATAC is the reliance on Gradient-Descent Ascent vs the Stackelberg game strategy of updating the two-player game. We have included both ATAC and CQL in Proposition 4 in the revised paper.\n\n\n2. **Eq (109) is not just a summary, and that \"we summarize the result of the derivation so far\" can be misleading. But this can be easily fixed though, by applying strong duality before moving forward.**\n\nThanks for pointing this out. We have made the correction.\n\n\n\n3. **Validity Eq (10). In the proof, the author assume $d_0 = d_E$. I think that this is a fairly strong assumption, as it implies that the initial state at the test time would be drawn from the same distribution of the expert.**\n\nWe think there might be a misunderstanding here. We do not assume $d_0=d^E$ but rather assume $d_0=d^S$. We clarify that Eq.10 is a simplified objective and the full derivation under $\\chi^2$ can be found in Appendix E.3. In the full derivation we downweigh the $Q$-function on states obtained from mixture distribution of offline data ($1-\\gamma$) and expert data ($\\gamma$), and the actions obtained from current policy. In practice, the simplified objective contains the offline data term only as our offline data is a superset of expert data and $\\gamma$ is close to 1.\n\nThe offline distribution $d^S$ is a wider distribution encompassing the starting state $d_0$ distribution. This strategy allows us to learn a near-optimal policy from a wider initial state distribution and should not be detrimental to function representation with sufficient capacity. A similar strategy is common in (eg. [2], off-policy RL [3],[4] ). \n\nWe have corrected Eq.10 with the missing constant of 1/4 and also updated the sentence before Eq.10 to say that we are presenting a simplified objective here.\n\n4. **Bug in Proof of Proposition 3.**\n\n\nWe apologize for the phrasing of this section in the appendix leading to confusion. We have rewritten the paragraph to increase clarity. To summarize answers to your concerns briefly:\n\n- We have corrected that $f'$, $(f')^{-1}$ are not strictly increasing (only non-decreasing), but note that we do not need them to be strictly increasing for the following proof.\n-  A common issue when dealing with $f$-divergences is the limited domain of $\\mathbb{R}^+$ on which $f$ divergences are defined and subsequently limits the domain and range of $f'$ or ${f'}^{-1}$. For us to be able to create an optimization objective defined on $\\mathbb{R}$, we utilize a surrogate extension of ${f'}^{-1}$ on $\\mathbb{R}$. This was initially expanded upon in the practical algorithm section but we have now made this change in Section 6 as well.  \n- $(f')^{-1}$ for TV divergence (which the reviewer points out) is not well defined and $f^*$ for RKL and Sq. Hellinger divergences are discontinuous on $\\mathbb{R}^+$. These divergences require special treatment and have expanded this discussion in the proof of Proposition 3. This treatment was earlier discussed in our proposed practical objective for $f$-DVL in Section 6."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151204008,
                "cdate": 1700151204008,
                "tmdate": 1700151221941,
                "mdate": 1700151221941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1m8ugE1drc",
            "forum": "xt9Bu66rqv",
            "replyto": "xt9Bu66rqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new approach in offline IL, adopting the f-MAX objective function instead of the commonly utilized SMODICE objective. Unlike f-MAX, which employs GAIL for optimization, this work leverages the DICE technique to derive a dual form and subsequently applies an IQL-style algorithm for resolution. Additionally, the framework exhibits versatility, with potential applications in both the online and offline RL settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper offers a thorough and insightful analysis of recent developments within the dual-RL paradigm.\n2. The proposed framework demonstrates adaptability across IL, offline RL, and online RL settings.\n3. By eliminating the discriminator in offline IL, the paper potentially enhances stability and reliability in this domain."
                },
                "weaknesses": {
                    "value": "1. The paper's structure appears ill-suited for a conference format, dedicating 3.5 pages to prior knowledge while relegating significant analyses and experimental results (pertaining to online and offline RL) to the Appendix.\n2. There are instances of overclaims and problematic expressions that require rectification."
                },
                "questions": {
                    "value": "Your work is commendable, but improvements in presentation and accuracy are needed. My specific questions and suggestions are as follows:\n\n1. XQL's instability and heavy reliance on hyperparameters are noticeable, with subpar performance in both online and offline RL under unsuitable conditions. As an alternative example, it might be more suitable to reference a more general and effective implicit policy improvement algorithm, like IQL[1]. I suggest you use a more general and effective implicit policy improvement algorithm, IQL, as an example.\n\n2. In the last paragraph of Section 4.1, it is not suitable to say, \"This insight also allows us to cast IL method OPOLO .....\" as the objective function here originates from SMODICE. Referencing \"this objective function\" or similar expressions would be more appropriate.\n\n3. In the first paragraph of Section 4.2, it is strange to use \"Unfortunately\" just because XQL is not the policy regularization style. \n\n4. Figure 1 (a) is quite hard to follow. Please refine the caption to explain what the floats and the colors mean.\n\n5. The first paragraph of Section 7.1 lacks supportive evidence for this claim. Incorporating illustrative figures and evaluations would substantiate this statement.\n\n6. On page 36, typo in \"We base the implementation of SAC off pytorch_sac ...\"\n\n7. On page 37, I disagree with your claim that \"Our experiments with the popular off-policy method SAC [Haarnoja et al., 2018] reveal its brittleness to off-policy data\". SAC is not brittle to off-policy data. Filling the replay buffer arbitrarily with expert data at the beginning is not fair for an ADP-style algorithm. Please rewrite this paragraph.\n\n8. On page 40, the statement \"f-DVL leads to noticeable improvements even in the online RL benchmarks\" appears overstated:\n\n   * The baselines are too few.\n\n   * The performance improvement is not noticeable in 75% tasks you conducted experiments in.\n\n   If you would like to claim noticeable,  please include more baselines (PPO[2], TD3[3], AlgeaDICE[4], and some other more recent algorithms, e.g., RRS[5], BAC[6], REDQ[7]) and more benchmark tasks. And please broaden the discussion of related works in the online RL setting beyond SAC.\n\n9. Some crucial experimental outcomes, currently in Appendix H, should be integrated into the main paper for better accessibility and coherence, especially those results referenced and analyzed within the main paper.\n\n10. Better not to claim \"New\" in the title directly. Just saying \"A unified framework\" is ok.\n\nBy addressing these points, your work could gain greater clarity and impact. And then, I would like to raise the score.\n\n[1] Kostrikov I, Nair A, Levine S. Offline Reinforcement Learning with Implicit Q-Learning[C]//International Conference on Learning Representations. 2021.\n\n[2] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n[3] S Fujimoto, H van Hoof, and D Meger. Addressing function approximation error in actor-critic methods. Proceedings of Machine Learning Research, 80:1587\u20131596, 2018.\n\n[4] Nachum O, Dai B, Kostrikov I, et al. Algaedice: Policy gradient from arbitrary experience[J]. arXiv preprint arXiv:1912.02074, 2019.\n\n[5] Hao Sun, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, and Bolei Zhou. Optimistic curiosity exploration and conservative exploitation with linear reward shaping. In Advances in Neural Information Processing Systems, 2022.\n\n[6] Ji T, Luo Y, Sun F, et al. Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic[J]. arXiv preprint arXiv:2306.02865, 2023.\n\n[7] Chen X, Wang C, Zhou Z, et al. Randomized Ensembled Double Q-Learning: Learning Fast Without a Model[C]//International Conference on Learning Representations. 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ",
                        "ICLR.cc/2024/Conference/Submission7641/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699616425215,
            "cdate": 1699616425215,
            "tmdate": 1700276167070,
            "mdate": 1700276167070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rBygYLvY0N",
                "forum": "xt9Bu66rqv",
                "replyto": "1m8ugE1drc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7FxJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide a thoughtful and thorough review of the paper.\n\nWe have made the changes for all the suggested typos in the revision. The changes are highlighted in red. We appreciate the reviewer's attention for pointing these out. We provide clarification to reviewers queries below:\n\n1. **Figure 1 (a) is quite hard to follow. Please refine the caption to explain what the floats and the colors mean.**\n\nWe have elaborated on the experiment in Figure 1 (a) in the caption of the revised paper as well as added a detailed explanation of the experiment setup in Appendix H.3. \n\n2. **The first paragraph of Section 7.1 lacks supportive evidence for this claim. Incorporating illustrative figures and evaluations would substantiate this statement.**\n\nWe want to request clarification on this question. It seems like Section 7.1 is the experimental setup section and perhaps the reviewer meant some other section.\n\n3.  **Filling the replay buffer arbitrarily with expert data at the beginning is not fair for an ADP-style algorithm. Please rewrite this paragraph.**\n\nIn this section of the appendix, our intention was to compare popular ADP-based off-policy methods vs dual-RL/DICE style off-policy methods. Any off-policy algorithm, by definition, should be able to make use of any kind of data obtained from the environment -- expert data is a special case. We have reworded this section to make our claims more accurate since as the reviewer points out considering one special case is not enough to say the the algorithm is brittle.\n\n\n4. **On page 40, the statement \"f-DVL leads to noticeable improvements even in the online RL benchmarks\" appears overstated. And please broaden the discussion of related works in the online RL setting beyond SAC.**\n\nWe have corrected our claim to say that we are competitive with SAC and XQL in online RL experimeents. In this work, our focus was limited to offline setting and we leave a more in-depth analysis of online RL algorithms for future work. We have added the suggested recent online RL methods in the related work section.\n\n\n5. **Some crucial experimental outcomes, currently in Appendix H, should be integrated into the main paper for better accessibility and coherence, especially those results referenced and analyzed within the main paper.**\n\n We appreciate the reviewer's suggestion. We acknowledge that our appendix contains a number of interesting experiments. In this work, we were constrained by the page limit to strike a balance between facilitating understanding, demonstrating our contributions concisely, and providing empirical evidence to support the theoretical contributions. Currently, we take $\\approx 0.6$ page for related works and $\\approx 1.4$ page for preliminary. We believe introduction should not be considered a part of prior work and a similar format can be found in a number of influential ICLR works [1,2,3]. We would be happy to receive suggestions on what the reviewer thinks might be traded off in favor of more experiments.\n\n\n6. **Better not to claim \"New\" in the title directly. Just saying \"A unified framework\" is ok.**\n\nWe clarify that the use of the word 'New' was to signify that we leveraged the existing framework of duality in RL to propose new and improved algorithms for offline RL and IL and not to signify the novelty of the duality framework. \n\n-------------------\n\nWe appreciate the reviewer's suggestions and are happy to incorporate other suggestions into our submission. Please let us know if there are other issues or concerns we can address. We would be happy to expand this discussion.\n\n-------------------\n\nReferences:   \n[1]: Eyuboglu, Sabri, et al. \"Domino: Discovering systematic errors with cross-modal embeddings.\" arXiv preprint arXiv:2203.14960 (2022).   \n[2]: Flennerhag, Sebastian, et al. \"Bootstrapped meta-learning.\" arXiv preprint arXiv:2109.04504 (2021).   \n[3]: Ren, Tongzheng, et al. \"Latent variable representation for reinforcement learning.\" arXiv preprint arXiv:2212.08765 (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150501804,
                "cdate": 1700150501804,
                "tmdate": 1700150501804,
                "mdate": 1700150501804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8X2COFs2jo",
                "forum": "xt9Bu66rqv",
                "replyto": "rBygYLvY0N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "content": {
                    "title": {
                        "value": "Discussion regarding to \"The first paragraph of Section 7.1\""
                    },
                    "comment": {
                        "value": "I wanted to clarify a point I previously mentioned regarding the term \"LOW COVERAGE OF EXPERT\u201d, as I realize my initial expression might not be clear. This term could potentially be misleading. The support set of 30 expert demonstrations may not have a clear difference compared to 100 expert demonstrations, but I still think this is a much more difficult setting. Thus I suggest you provide some support for this claim or change the expression here."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151796236,
                "cdate": 1700151796236,
                "tmdate": 1700151796236,
                "mdate": 1700151796236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ybbJhfYdr",
                "forum": "xt9Bu66rqv",
                "replyto": "1m8ugE1drc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "content": {
                    "title": {
                        "value": "Discussion about putting the interesting experiments in your appendix into main text."
                    },
                    "comment": {
                        "value": "I wanted to discuss the possibility of moving some of the interesting experiments from your appendix into the main text of your paper.\n\nPlease don't worry too much about this comment. I think your work is really good, and this is just a suggestion from a writing perspective. Feel free to disregard it if it doesn't suit your needs, and don't feel any pressure about it.\n\nIn terms of shortening some parts of the paper, I feel that the sections on basic MDP definitions, value functions, and Bellman operators are a bit lengthy. Perhaps you could consider condensing them slightly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152405258,
                "cdate": 1700152405258,
                "tmdate": 1700152405258,
                "mdate": 1700152405258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3GKf5LfR34",
                "forum": "xt9Bu66rqv",
                "replyto": "wfQ1hqCWzg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Reviewer_7FxJ"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for their rebuttal!"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal! I would like to raise my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276137788,
                "cdate": 1700276137788,
                "tmdate": 1700276137788,
                "mdate": 1700276137788,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xqhzQb8KPW",
            "forum": "xt9Bu66rqv",
            "replyto": "xt9Bu66rqv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_NyfP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7641/Reviewer_NyfP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a unified dual RL framework that connects several recent offline RL and IL methods. New algorithms called ReCOIL (for IL) and f-DVL (for RL) are presented that aim to address limitations of prior approaches. Experiments across locomotion, manipulation and navigation tasks generally demonstrate improved or comparable performance to baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The theoretical dual RL formulation provides a common lens to view recent offline RL/IL methods. This is a useful conceptual contribution.\n- The methods are evaluated extensively on a diverse set of tasks using standardized benchmarks. Implementation details are clearly described.\n- For offline IL, ReCOIL relaxes assumptions like coverage and avoids instability of density ratio estimation. The results showcase strong performance on imitation tasks.\n- The modifications in f-DVL seem to improve training stability compared to XQL, a prior state-of-the-art in offline RL."
                },
                "weaknesses": {
                    "value": "- While f-DVL outperforms XQL, its gains over other offline RL methods like IQL are marginal. The gains are not as significant as claimed over the full spectrum of baselines.\n- The presentation of empirical results could be improved by using standardized metrics, showing confidence intervals, and increasing clarity around performance highlights.\n- There are open questions around design choices, estimation procedures, and other technical details that warrant clarification."
                },
                "questions": {
                    "value": "Overall, I am moderately positive about this submission. The theoretical framework is clean and impactful. The empirical results are reasonably strong but could use tighter presentation and analysis. \n\nI have the following specific questions:\n\n- In Eq. 10, is the reward assumed to be zero in the Bellman consistency term?\n- Can you expand on how ReCOIL estimates the policy visitation and reward for the results in Sec 7.1? Also clarify if Fig 1 and Fig 10 use OPOLO or SMODICE for comparison.\n- What do the distributions D and d^O refer to in Eq. 12 and 13? Are they both d_{mix}^{E,S}?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7641/Reviewer_NyfP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7641/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700496224854,
            "cdate": 1700496224854,
            "tmdate": 1700496224854,
            "mdate": 1700496224854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d0OC8fXDHx",
                "forum": "xt9Bu66rqv",
                "replyto": "xqhzQb8KPW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NyfP (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful review. The modifications are marked in red in the paper revision. We address your concerns and questions below. \n\n\n1. **While f-DVL outperforms XQL, its gains over other offline RL methods like IQL are marginal. The gains are not as significant as claimed over the full spectrum of baselines.**\n\nThe locomotion benchmark in D4RL for offline RL is simpler with performance to a certain extent saturated, compared to the Antmaze or Kitchen environments. Our proposed approach is competitive in these domains. A detailed discussion of the properties of these environments and datasets can be found in [1]. We notice that $f$-DVL outperforms in 6/9 tasks in antmaze and kitchen environments which pose a harder challenge of stitching suboptimal trajectories compared to recently proposed IQL[2], XQL[3], and DT[4].  We believe that these improvements over prior work are meaningful by the standards used in prior offline RL papers.\nOne of the contributions through $f$-DVL is also to provide another axis of improvement in a stable learning alternative to XQL. We would like to bring attention to the fact that XQL (ICLR 23) is a more recent method than IQL (ICLR 22) and our proposed method drives direct improvement over XQL.\n\n2. **The presentation of empirical results could be improved by using standardized metrics, showing confidence intervals, and increasing clarity around performance highlights.**\n\nWe appreciate the reviewer's suggestions and note that the following changes have been made in our revised paper: a) In the results for imitation learning (ReCOIL) we highlight all methods that lie within the standard deviation of the best-performing method. b) In the offline RL (f-DVL) results we highlight methods that are within 1 performance point of the best-performing method. Additionally, we have also added standard deviations for our method in Table 10. Indeed, using statistical metrics would be more suitable but unfortunately prior works in offline RL like XQL have used mean as an estimate. We extend prior results in our work and computing metrics like Inter-Quartile mean (IQM) or std-dev would require rerunning all the baselines which would take more time and more compute budget than we have available. \n\nQuestions:\n\n1. **In Eq. 10, is the reward assumed to be zero in the Bellman consistency term?**\n\nTo clarify for Eq. 10, we are in the setting of imitation learning where we are not provided with task rewards but rather only expert demonstrations. Eq. 10 is a direct consequence of Theorem 1 which provides a principled objective for imitation learning signifying that bellman backup with zero rewards in conjunction with a contrastive loss on Q is sufficient and pricipled for imitation learning."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512255138,
                "cdate": 1700512255138,
                "tmdate": 1700512584497,
                "mdate": 1700512584497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OAqvoA1BX8",
                "forum": "xt9Bu66rqv",
                "replyto": "xqhzQb8KPW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7641/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NyfP (2/2)"
                    },
                    "comment": {
                        "value": "2. **Can you expand on how ReCOIL estimates the policy visitation and reward for the results in Sec 7.1?**\n\n\nWe have added a detailed explanation for estimating policy visitation in Appendix H.3 in the revised paper and also edited the caption of Figure 1 to give more clarity. In brief, any dual-RL objective written in the $Q$-form allows us to extract density ratios by considering the inner optimization of Equation 9.\n\n For reward estimation, the paragraph below Theorem 1 explains how rewards can be inferred from learned Q-function and policy. In brief, given $Q^*$ and $\\pi^*$, the rewards can be extracted as $r(s,a)= Q^*(s,a)-\\mathcal{T}^{\\pi^*}_0(Q^*(s,a))$\n \n 3. **Also clarify if Fig 1 and Fig 10 use OPOLO or SMODICE for comparison.**\n \n We appreciate the reviewer pointing out this typo, we correctly mentioned in the figures that we used SMODICE but incorrectly in the text that OPOLO was used for comparison. We have made this correction in the text. SMODICE can be understood as an offline counterpart to OPOLO and in our work we focus on the offline setting. A more detailed discussion of this experiment and baselines can be found in Appendix H.3.\n\n4. **What do the distributions D and $d^O$ refer to in Eq. 12 and 13? Are they both $d_{mix}^{E,S}$?**\n\nWe appreciate the reviewer bringing attention to this typo. They are both indeed $d_{mix}^{E,S}$ and we have fixed this in the revised paper.\n\n-----------------\n\n\nPlease let us know if our clarifications resolve your questions and concerns. We would be happy to expand the discussion on any remaining specific concerns about design choices, and other technical details . **Since the review is posted near the end of the discussion period we would greatly appreciate if we are made aware of any remaining concerns and issues soon.**\n\n\n\n\n-----------------\n-----------------\n\n\n\nReferences:     \n[1]: Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020).      \n[2]: Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline reinforcement learning with implicit q-learning.\" arXiv preprint arXiv:2110.06169 (2021).      \n[3]: Garg, Divyansh, et al. \"Extreme q-learning: Maxent RL without entropy.\" arXiv preprint arXiv:2301.02328 (2023).      \n[4]: Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512337744,
                "cdate": 1700512337744,
                "tmdate": 1700702788306,
                "mdate": 1700702788306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]