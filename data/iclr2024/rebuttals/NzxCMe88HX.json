[
    {
        "title": "Toward effective protection against diffusion-based mimicry through score distillation"
    },
    {
        "review": {
            "id": "ti4JjuMhKE",
            "forum": "NzxCMe88HX",
            "replyto": "NzxCMe88HX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates methods to protect images from unauthorized editing by diffusion models, referred to as \"diffusion mimicry\". The authors make the insight that the encoder module is more vulnerable than the denoiser when attacking latent diffusion models (LDMs). Based on this finding, they propose faster protection generation using score distillation sampling. They also explore minimizing the semantic loss for protection, obtaining more imperceptible perturbations. The experimental results on divers image datasets support the proposed techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper reveals that the encoder is more vulnerable than the denoiser, which is novel and insightful.\nThe proposed SDS is a more efficient protection compared with prior works."
                },
                "weaknesses": {
                    "value": "1. The major concern I have is that how to evaluate the diffusion model still works well after the perturbation? This paper only provides results that the proposed SDS can defend against adversarial attack, but does not show the perturbation does not affect the original function of the diffusion model. The paper lacks both analysis and experiments on this point.\n\n2. The threat model is not clear. Based on my understanding, the core of this paper is adversarial attack and defense on diffusion model. However, the introduction of \"diffusion mimicry\" confuses the reader a lot. If authors really want to mention \"diffusion mimicry\", it should be explained that how diffusion model is used for mimicry and why the proposed method is a good defense? \n\n3. The paper lacks insight into why minimizing semantic loss works better."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv",
                        "ICLR.cc/2024/Conference/Submission5336/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515439849,
            "cdate": 1698515439849,
            "tmdate": 1700503690436,
            "mdate": 1700503690436,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qnI8gSyDDi",
                "forum": "NzxCMe88HX",
                "replyto": "ti4JjuMhKE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your valuable comments!"
                    },
                    "comment": {
                        "value": "Thanks for your time to review our paper, thanks for acknowledging our contributions, here let us answer your questions, hopefully it can address your concern.\n\n> The major concern I have is that how to evaluate the diffusion model still works well after the perturbation? This paper only provides results that the proposed SDS can defend against adversarial attack, but does not show the perturbation does not affect the original function of the diffusion model. The paper lacks both analysis and experiments on this point.\n\nI think there may be some misunderstanding about the task, we only do perturbation to the image, and do not change the diffusion model, so it will not `affect original function of the diffusion model`.  SDS is only an alternative loss function, and it can be easily applied without changing the original diffusion model. We are looking forward to get your response if there are still some concerns.\n\nWe also clarify our problem in the General Response.\n\n\n\n> The threat model is not clear. Based on my understanding, the core of this paper is adversarial attack and defense on diffusion model. However, the introduction of \"diffusion mimicry\" confuses the reader a lot. If authors really want to mention \"diffusion mimicry\", it should be explained that how diffusion model is used for mimicry and why the proposed method is a good defense?\n\n\nDiffusion mimicry is a term used in [1], and here we mention mimicry as malicious editing of a individual image, which is a real-world problem, we ground it to protection against diffusion-based editing in our paper. Our experiments includes three scenarios: global image to image, inpainting and textual-inversion, all can be found in some real-world applications such as face swapping, subject-driven image generative, meme generation, style copying\u2026 \n\nIn conclusion, just like other works [1, 2, 3], the problem settings are the same, we ground the real-world problem into reasonable settings by attacking the diffusion model.\n\nWe also clarify our problem settings again in the General Response.\n\n\n\n> The paper lacks insight into why minimizing semantic loss works better.\n\n\n\nIt is an interesting phenomenon that minimizing the semantic loss $\\mathcal{L}_{S}(x)$ counter-intuitively fools the diffusion model, here we provide some insights into the possible reasons behind it. \n\nRemember we have the semantic loss in SDS form can be regarded as the weighted probability density distillation loss (Eq.10 in Appendix G).\n\nHere we can get some insights from the equation, minimizing the semantic loss means minimizing the KL divergence between $q(z_t|x)$ and $p_{\\theta}(z_t)$, where $p_{\\theta}(z_t)$ is the marginal distribution sharing the same score function learned with parameter $\\theta$. Since $p_{\\theta}(z_t)$ does not depend on $x$, it actually describes the learned distribution of the dataset smoothed with Gaussian. \n\nLet's assume in a simple case that the data distribution is exactly the Dirac Delta Distribution of data points in the dataset. Then we have $p_{\\theta}(z_t)$ as a Gaussian-smoothed composition of Dirac Delta Distribution if $s_{\\theta}$ is perfectly learned. Then minimizing the semantic loss $\\mathcal{L}_{S}(x)$ turns into making $q(z_t|x)$ closer to the Gaussian-smoothed composition of Dirac Delta Distribution, and the optimization direction is to make $x$ closer to the spatially average value in the dataset, which brings blurred $x$, and it turns out to be a good protection.\n\n\n\n\n\n[1] Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models\n\n[2]  Raising the cost of malicious ai-powered image editing.\n\n[3]  Mist: Towards improved adversarial examples for diffusion models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104501278,
                "cdate": 1700104501278,
                "tmdate": 1700104501278,
                "mdate": 1700104501278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5LCH7j8zFk",
                "forum": "NzxCMe88HX",
                "replyto": "gHIXDP54s4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "content": {
                    "title": {
                        "value": "About the third bullet (Please number your answers): Clarification of contribution and problem settings."
                    },
                    "comment": {
                        "value": "The authors do not clarify the problem settings."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464001728,
                "cdate": 1700464001728,
                "tmdate": 1700464001728,
                "mdate": 1700464001728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a40N3rZRKl",
                "forum": "NzxCMe88HX",
                "replyto": "90XEsc5FK4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your quick response. But I am still confused about the problem setting and threat model."
                    },
                    "comment": {
                        "value": "1. Why 'the way to protect them is grounded to generate adversarial samples to attack the diffusion model'?\n\n2. Based on my understanding, the perturbation is added during the sampling of diffusion model? Are there experiments you could present to show that the original function of diffusion model is not affected after perturbation? I am ok if you just describe the experiments without showing real results for now. \n\n3. One sentence confused me a lot is the first sentence in Section 4: \n'The protection against diffusion-based mimicry is one type of attack against the LDMs.' I may better understand this sentence if you could provide a clear answer to my question 1.\n\nMore questions for you to understand my confusion:\n4. Can you provide an example showing what diffusion model is originally used for? e.g., for generating image\n\n5. Then, what attacker will do? e.g., leverage the diffusion model to discover training data.\n\n6. How to protect the diffusion model? e.g., adding perturbation to the prompt? When you say adding perturbation to the image, what is the image refers to? What about the text-image diffusion models?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466764082,
                "cdate": 1700466764082,
                "tmdate": 1700466764082,
                "mdate": 1700466764082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cVba8s6ytO",
                "forum": "NzxCMe88HX",
                "replyto": "ozhsYebJNq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_7HTv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your patience and detailed clarifications. My concerns have been fully addressed. After considering the questions raised by other reviewers and your thorough rebuttal, I have decided to increase my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503660043,
                "cdate": 1700503660043,
                "tmdate": 1700503660043,
                "mdate": 1700503660043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BBJl6XnR7C",
            "forum": "NzxCMe88HX",
            "replyto": "NzxCMe88HX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_9rgQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_9rgQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to generate adversarial examples to protect images from diffusion-based mimicry pipelines, such as image editing, inpainting, and textual inversion. Specifically, the authors explore the robustness of encoder and denoiser modules in the Latent Diffusion Model. And they conclude that the encoder is much more vulnerable than the denoiser. Therefore, they leverage the existing method, Score Distillation Sampling, to calculate the gradient towards minimizing the semantic loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper focuses on protecting images from diffusion-based mimicry pipelines, this research region is interesting.  \n2. Motivated by the observation that the encoder is more vulnerable, the authors resort to Score Distillation Sampling to simplify the gradient update.   \n3. The authors attack multiple tasks: image editing, inpainting, and textual inversion."
                },
                "weaknesses": {
                    "value": "The major concern is the limited contribution of this paper.    \n1. The observation that the encoder is more vulnerable than the denoiser only contributes to this region, it is limited in developing more secure LDM. Besides, the experiments empirically indicate this conclusion. It lacks theoretical proofs.   \n2. The authors resort to the existing method, Score Distillation Sampling,  to fasten the backpropagation of the semantic loss.    \n3. The authors find that applying gradient descent over the semantic loss leads to more harmonious adversarial images with the original ones. The authors should delve into this phenomenon."
                },
                "questions": {
                    "value": "Given the fact that the encoder is more vulnerable than the denoiser, what is the performance when reducing the gradient of the denoising module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Reviewer_9rgQ",
                        "ICLR.cc/2024/Conference/Submission5336/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655570381,
            "cdate": 1698655570381,
            "tmdate": 1700529578529,
            "mdate": 1700529578529,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yc4PuPhG1K",
                "forum": "NzxCMe88HX",
                "replyto": "BBJl6XnR7C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your valuable comments!"
                    },
                    "comment": {
                        "value": "Thanks for your time to review our paper, thanks for acknowledging our contributions, here let us answer your questions, hopefully it can address your concern.\n\n\n> The observation that the encoder is more vulnerable than the denoiser only contributes to this region, it is limited in developing more secure LDM. Besides, the experiments empirically indicate this conclusion. It lacks theoretical proofs.\n\nWe believe that `the encoder is more vulnerable` is  a meaningful finding and can contribute new insights to the community to show that the bottleneck of attacking a LDM is the encoder. \n\n\nWe further show in the **Appendix C** in revision that attacking a pixel-based DM using semantic loss can not work, which proves that attacking a DM is very different from attacking a LDM, where the sucess of attacking a LDM really comes from the encoder.\n\nWe showed a lot of evidences in Section 4 to support our claim,  we do not try to focus on give a theoretical analysis but rather call emphasis on our insights. We write further clarification about our contribution in the **General Response**.\n\n\n\n\n\n> The authors resort to the existing method, Score Distillation Sampling, to fasten the backpropagation of the semantic loss.\n\n\nSDS is a free lunch, but it is fully omitted by previous methods [1,2,3], which turns out to be too expensive to run. We are the first to apply SDS into the protection framework and did extensive experiments to show that it is effective. Also, we provide theoratical analysis to better explain the SDS loss from the perspective of gradient of KL divergence in revision of **Appendix F** in the protection framework.\n\n\n> The authors find that applying gradient descent over the semantic loss leads to more harmonious adversarial images with the original ones. The authors should delve into this phenomenon.\n\nIt is an interesting phenomenon that minimizing the semantic loss $\\mathcal{L}_{S}(x)$ counter-intuitively fools the diffusion model, here we provide some insights into the possible reasons behind it. \n\nRemember we have the semantic loss in SDS form can be regarded as the weighted probability density distillation loss (Eq.10 in Appendix G).\n\nHere we can get some insights from the equation, minimizing the semantic loss means minimizing the KL divergence between $q(z_t|x)$ and $p_{\\theta}(z_t)$, where $p_{\\theta}(z_t)$ is the marginal distribution sharing the same score function learned with parameter $\\theta$. Since $p_{\\theta}(z_t)$ does not depend on $x$, it actually describes the learned distribution of the dataset smoothed with Gaussian. \n\nLet's assume in a simple case that the data distribution is exactly the Dirac Delta Distribution of data points in the dataset. Then we have $p_{\\theta}(z_t)$ as a Gaussian-smoothed composition of Dirac Delta Distribution if $s_{\\theta}$ is perfectly learned. Then minimizing the semantic loss $\\mathcal{L}_{S}(x)$ turns into making $q(z_t|x)$ closer to the Gaussian-smoothed composition of Dirac Delta Distribution, and the optimization direction is to make $x$ closer to the spatially average value in the dataset, which brings blurred $x$, and it turns out to be a good protection."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104463562,
                "cdate": 1700104463562,
                "tmdate": 1700104463562,
                "mdate": 1700104463562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WT61xL4WVX",
                "forum": "NzxCMe88HX",
                "replyto": "yc4PuPhG1K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_9rgQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_9rgQ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses"
                    },
                    "comment": {
                        "value": "I have read your response and other comments carefully. \nYour responses partly solve my concerns. I have raised my score to 5.\nHowever, the semantic loss is the main contribution of this paper. I suggest the authors should provide more theoretical analysis, and include them in the main paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529757785,
                "cdate": 1700529757785,
                "tmdate": 1700529757785,
                "mdate": 1700529757785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dVqAGfl4hK",
            "forum": "NzxCMe88HX",
            "replyto": "NzxCMe88HX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
            ],
            "content": {
                "summary": {
                    "value": "This paper wants to protect images from unauthorized use by perturbing the images using projected gradient descent. It uses the existing score distillation sampling idea to approximate the gradient of the sample instead of backward propagation through the UNet to save resources and time. It also chooses to minimize the semantic loss (ie, the diffusion model's loss) instead of maximizing it to generate more natural perturbations. Experiments on different domains and tasks show it outperforms the existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper targets an important issue of preventing unauthorized image use.\n2. Reducing the time cost and resource requirement makes the method more affordable and practical.\n3. Experiments show it's better than existing baselines."
                },
                "weaknesses": {
                    "value": "1. From my understanding, this method needs the gradient from the misused models. That is, the protection perturbation is generated on the exact model used to maliciously manipulate the image. It's not clear if this generated perturbation is only effective on that model, or if it can protect the image from misuse by an unknown model.\n\n2. No defense methods are evaluated. The malicious people may apply image transformations to remove the adversarial perturbations.\n\n3. It's not clear why minimizing the loss (encouraging LDM to make better predictions) can protect the images. It's very counter-intuitive and needs a better explanation. I can understand that minimizing the loss with respect to certain targets as Photoguard can work because it encourages the LDM to generate an image different from the original image to protect. But why can encouraging the LDM to generate a better image of the original image (the opposite goal of AdvDM) can help? Maybe it's kind of trapping the sample into a local optimum so that the later optimization/editing is also stuck? But this also needs to be justified.\n\n4. In Section 4.3, the conclusion of denoiser being more robust needs more evidence. Figure 2 shows the $\\delta_z$ can be less than 10x $\\delta_x$ for many cases. So I think the 10x budget (Figure 10 has even larger budgets) should be large enough to conduct some successful attacks. It's not clear why they fail.\n\n5. Photoguard has a diffusion attack. Why is only the encoder attack mentioned in the related work and evaluated in the experiments?\n\n6. The detailed setup for the experiments is missing. For example, how many iterations are used for the baselines and proposed method? What is the learning rate? What's the perturbation budget? How large is the fixed $\\delta_x$ in Figure 2?\n\n7. The pseudocode in Appendix A doesn't allow `x` to have gradients because `x=x.detach().clone()` detaches `x` from the computational graph.\n\n8. I suggest briefly introducing the metrics and human studies and having a more detailed explanation of the experimental results in the main text.\n\n9. To show the effectiveness of SDS approximation, one may visualize the loss changes and the gradient landscapes with the original loss function and the SDS loss."
                },
                "questions": {
                    "value": "1. Can this protection effect transfer to different LDMs? That is, assume the image is protected using a LDM A, when the adversary uses another LDM B to edit the image, will it fail?\n2. Can image transformations such as JPEG compression, cropping, rotation, and rescaling invalidate the protection?\n3. Why can minimizing the loss (encouraging LDM to make better predictions) protect the images?\n4. How are the $z$ and $z_{adv}$ visualized in the figures such as Figure 2?\n5. Could you explain, why attacking the diffusion model via $\\mathcal{L}_S$ outperforms $\\mathcal{L}_T$ if the encoder is less robust than the diffusion model as claimed in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper has user studies. Not clear if the authors have got the IRB approval."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi",
                        "ICLR.cc/2024/Conference/Submission5336/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682945234,
            "cdate": 1698682945234,
            "tmdate": 1700528930868,
            "mdate": 1700528930868,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FWIXvb2lGj",
                "forum": "NzxCMe88HX",
                "replyto": "dVqAGfl4hK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you so much for thoroughly reviewing our paper!  [Part (1)]"
                    },
                    "comment": {
                        "value": "Thank you so much for thoroughly reviewing our paper, your careful attention to detail is greatly appreciated. We are happy to answer some of your questions and we hope that we can address your concerns:\n\n\n> From my understanding, this method needs the gradient from the misused models. That is, the protection perturbation is generated on the exact model used to maliciously manipulate the image. It's not clear if this generated perturbation is only effective on that model, or if it can protect the image from misuse by an unknown model.\n\nThat is a good question. Here we focus on the **white-box settings** the same as previous works  [1, 2, 3]. For the black-box settings as the reviewer mentioned, recent paper [4] found that the adversarial samples can transfer between the most popular publicly-avaible LDMs like SD-v1.4, SD-v1.5 and SD-v2. We also did experiments to show that the adversarial samples generated on the basic SD-v1.4 can be transferred to SD-v1.5 and SD-v2. We put the results in Appendix E of the revision following your suggestion.\n\n\n> No defense methods are evaluated. The malicious people may apply image transformations to remove the adversarial perturbations.\n\nWe test some famous defending methods, including JPEG compression, AdverseClean (https://github.com/lllyasviel/AdverseCleaner) and crop-and-rescale on the adversarial samples. We put additional results in Appendix D following your suggestion, we found that crop-and-resize turns out to be relatively effective, but it still cannot work well to remove the perturbations. This finding is in accordance with that also found in [3].\n\n\n\n> It's not clear why minimizing the loss (encouraging LDM to make better predictions) can protect the images. It's very counter-intuitive and needs a better explanation. I can understand that minimizing the loss with respect to certain targets as Photoguard can work because it encourages the LDM to generate an image different from the original image to protect. But why can encouraging the LDM to generate a better image of the original image (the opposite goal of AdvDM) can help? Maybe it's kind of trapping the sample into a local optimum so that the later optimization/editing is also stuck? But this also needs to be justified.\n\n\n\n\nIt is an interesting phenomenon that minimizing the semantic loss $\\mathcal{L}_{S}(x)$ counter-intuitively fools the diffusion model, here we provide some insights into the possible reasons behind it. \n\nRemember we have the semantic loss in SDS form can be regarded as the weighted probability density distillation loss (Eq.10 in Appendix G).\n\nHere we can get some insights from the equation, minimizing the semantic loss means minimizing the KL divergence between $q(z_t|x)$ and $p_{\\theta}(z_t)$, where $p_{\\theta}(z_t)$ is the marginal distribution sharing the same score function learned with parameter $\\theta$. Since $p_{\\theta}(z_t)$ does not depend on $x$, it actually describes the learned distribution of the dataset smoothed with Gaussian. \n\nLet's assume in a simple case that the data distribution is exactly the Dirac Delta Distribution of data points in the dataset. Then we have $p_{\\theta}(z_t)$ as a Gaussian-smoothed composition of Dirac Delta Distribution if $s_{\\theta}$ is perfectly learned. Then minimizing the semantic loss $\\mathcal{L}_{S}(x)$ turns into making $q(z_t|x)$ closer to the Gaussian-smoothed composition of Dirac Delta Distribution, and the optimization direction is to make $x$ closer to the spatially average value in the dataset, which brings blurred $x$, and it turns out to be a good protection.\n\n\n\n\n\n\n> In Section 4.3, the conclusion of denoiser being more robust needs more evidence. Figure 2 shows that $\\delta_z$  can be less than 10x$\\delta_x$ for many cases. So I think the 10x budget (Figure 10 has even larger budgets) should be large enough to conduct some successful attacks. It's not clear why they fail.\n\n\nThat is a good question. This exactly supports our claim that the bottleneck is the encoder, when attacking the z-space alone, we do not use the gradient from the encoder, which brings a weak attack to the encoder itself. As we can see in Figure 10, although the budgets are getting even larger, the edited results do not diverge a lot from x_adv (which may be quite noisy since we perturb the z-space).\n\n\nIn conclusion, even given the same budget, simply attacking the z-space can not give us a good direction to attack the LDM. Encoder is really important and is the key when attacking a LDM.\n\n\nTo better support our conclusion, we did experiments (in **Appendix C** in revision) on a pixel-based diffusion model without encoder-decoder, and we found that gradient-based methods fail to work. That is, if we attack the DM using PGD and semantic loss, the generated adversarial sample cannot even fool the DM, the reason behind this may be the randomness brought by Gaussian noise term, making the denoiser quite robust to be attacked directly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104205021,
                "cdate": 1700104205021,
                "tmdate": 1700104205021,
                "mdate": 1700104205021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eitZkSF6bN",
                "forum": "NzxCMe88HX",
                "replyto": "dVqAGfl4hK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer-author discussion period ends in Two Days"
                    },
                    "comment": {
                        "value": "Thank you again for your really careful review. We hope that we were able to address your concerns in our response. If possible, please let us know if you have any further questions before the reviewer-author discussion period ends. We are glad to address your further concerns, thanks."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507458492,
                "cdate": 1700507458492,
                "tmdate": 1700507485162,
                "mdate": 1700507485162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N9D6aYtHy1",
                "forum": "NzxCMe88HX",
                "replyto": "eitZkSF6bN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed response."
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed response. I am satisfied with the answers and new illustrations, except for one thing about the photoguard.\n\nI ran [the photoguard diffusion attack](https://github.com/MadryLab/photoguard/blob/main/notebooks/demo_complex_attack_inpainting.ipynb) locally and it only took 27G GPU memory. It's much less than the claimed 50G. Could you verify your claim?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528497102,
                "cdate": 1700528497102,
                "tmdate": 1700528497102,
                "mdate": 1700528497102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2gG03g17Yv",
                "forum": "NzxCMe88HX",
                "replyto": "dVqAGfl4hK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Reviewer_vkfi"
                ],
                "content": {
                    "title": {
                        "value": "I have raised my score, but photoguard needs to be correctly discussed."
                    },
                    "comment": {
                        "value": "I have raised my score.\n\nBut I suggest correcting the discussion about photoguard in the paper (because they do have an encoder attack and a diffusion attack) and talking about why only the simple encoder attack is evaluated, especially double-checking the claim regarding the +50G memory request."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528840232,
                "cdate": 1700528840232,
                "tmdate": 1700528880284,
                "mdate": 1700528880284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ym6DlRjsWC",
                "forum": "NzxCMe88HX",
                "replyto": "dVqAGfl4hK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks again for your really careful review and meaningful discussions."
                    },
                    "comment": {
                        "value": "We are glad that we have solved most of your concerns and thank you for raising the score. We have added a clarification about Photoguard in our main paper in the new revision of **Section 2**.\n\nFor the memory consumption of Photoguard, in the early stage of this project, we found that when we use `num_inference_steps=8` in the demo provided by Photoguard, it will take up `~47G`  on one A6000 GPU. \n\nI just tried if we set `num_inference_steps=4` as default, it will take up `29G` on one A6000 GPU, and 30 minutes for one image, which is also too expensive. In conclusion, the diffusion attack of the Photoguard is too expensive to run. We also tried to run it on one 3090 and it did not work.\n\nAlso, in the official repo of photoguard(https://github.com/MadryLab/photoguard/blob/main/notebooks/demo_complex_attack_inpainting.ipynb), it said: `Below is the implementation of an end2end attack on the stable diffusion pipeline using PGD. This requires a GPU Memory >= 40Gbs (we ran on 1 A100)`.\n\nThanks again for your really careful review and meaningful discussions."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538917727,
                "cdate": 1700538917727,
                "tmdate": 1700541930141,
                "mdate": 1700541930141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rwV0XpECm5",
            "forum": "NzxCMe88HX",
            "replyto": "NzxCMe88HX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_hmR8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5336/Reviewer_hmR8"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a faster attack method for encoder-diffusion style model. They find out that the encoder is a weak point of the whole model. Therefore, they specifically design an attack method by removing the gradient propagation from the diffusion parts. It makes the running speed faster. They evaluate against three latest methods and show competitive results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ There are some interesting observations including the different robustness regarding diffusion and encoding parts.\nI believe this is a good observation which could make attack fast.\n+ The methods are effective against latest benchmarks."
                },
                "weaknesses": {
                    "value": "- Missing details when comparing the magnitude of embedding change versus input change\nThe authors use absolute value to compare adversarial noise. While the embedding can be of different scale. The authors shall provide a relative scale of perturbation magnitude.\n\n- Unclear math derivation\nIn equation 5, why can we approximately remove the gradient parts from denoiser?"
                },
                "questions": {
                    "value": "The reason behind equation 5 and the relative scale of perturbation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5336/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699351011726,
            "cdate": 1699351011726,
            "tmdate": 1699636536276,
            "mdate": 1699636536276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YA9wxmO480",
                "forum": "NzxCMe88HX",
                "replyto": "rwV0XpECm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your valuable comments!"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for conducting a careful review and providing valuable feedback. We highly appreciate the reviewer's recognition of the insights we provided and the effectiveness of our method.\n\n> Missing details when comparing the magnitude of embedding change versus input change The authors use absolute value to compare adversarial noise. While the embedding can be of different scale. The authors shall provide a relative scale of perturbation magnitude.\n\nAs we mentioned in Sec 4.1: line 5, we normalized the perturbations in the two spaces for comparison. Specifically, we scale the perturbations in the z-space back to [0, 1] as in the x-space.\n\n> Unclear math derivation In equation 5, why can we approximately remove the gradient parts from the denoiser?\n\n\nThis is a good question. Approximating the jacobian of U-Net as an identity matrix is used in many works, such as distillation from 2D [1, 2] and gradient approximation in adversarial attack [3]. While approximation is the most straightforward explanation, it can also be regarded as the weighted probability density distillation loss, we provide detailed analysis in **Appendix G** in the revision.\n\nAlso, by visualizing the loss curve in **Appendix F**, we better support the claim and it turns out that using SDS loss is a free lunch that should be noticed and applied when designing a protection method.\n\n\n[1] DreamFusion: Text-to-3D using 2D Diffusion\n\n[2] ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation\n\n[3] Diffusion-based adversarial sample generation for improved stealthiness and controllability"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104028365,
                "cdate": 1700104028365,
                "tmdate": 1700104028365,
                "mdate": 1700104028365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hfjk9vcfPt",
                "forum": "NzxCMe88HX",
                "replyto": "rwV0XpECm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5336/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer-author discussion period ends in Two Days"
                    },
                    "comment": {
                        "value": "Thank you again for your review. We hope that we were able to address your concerns in our response. If possible, please let us know if you have any further questions before the reviewer-author discussion period ends. We are glad to address your further concerns, thanks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5336/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507412213,
                "cdate": 1700507412213,
                "tmdate": 1700507412213,
                "mdate": 1700507412213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]