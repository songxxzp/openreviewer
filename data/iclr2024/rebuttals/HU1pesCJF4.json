[
    {
        "title": "Pixel Reweighted Adversarial Training"
    },
    {
        "review": {
            "id": "fxhw1Idif4",
            "forum": "HU1pesCJF4",
            "replyto": "HU1pesCJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_h86N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_h86N"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Pixel-reweighted AdveRsarial Training (PART), a novel framework within Adversarial Training (AT). It focuses on optimizing the perturbation budget \u03b5 by assigning higher allocations to pixels crucial for the model's performance and lower allocations to less influential ones. PART shows improved model robustness and accuracy compared to existing defenses. This performance enhancement is observed across datasets such as CIFAR-10, SVHN, and Tiny-ImageNet, even in the face of diverse attack strategies, including adaptive ones."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces an original approach in Adversarial Training (AT) by addressing perturbation budgets and proposing Pixel-reweighted AdveRsarial Training (PART), offering a fresh perspective on adversarial defense. It maintains a solid research quality with a well-defined methodology, rigorous experiments, and comprehensive result documentation. Besides, this paper effectively communicates its problem formulation, methodology, and results, ensuring clear presentation."
                },
                "weaknesses": {
                    "value": "1. The introduced CAM technology seems to have a weak improvement in robustness, and the authors did not analyze the impact on the training speed of the original AT framework, so I think it is not clear whether the performance trade-off brought by CAM is worth the cost of training speed and memory;\n2. The authors mentioned that in the process of AT, it actually needs to be combined with standard AT for warm-up. I think they should specify the number of training sessions required for AT and PRAG during training;\n3. The CAM technique seems to be a visualization technique for problems based on classification, which may limit the applicability of the PART framework proposed by the authors on other applications besides classification.\n4. The authors\u2019 description of the results in the figures and tables is not clear enough. For example, some of the tables seem to have standard deviations, while some do not. The authors didn't mention how many runs the standard deviation was calculated; The typography of the font, shadows, and content in the table are not compact enough.\n5. Authors should probably consider more AT methods using CAM techniques mentioned in related works for comparison, instead of using many experimental results to compare different CAM techniques, since these techniques including naive CAM techniques seem to be existing\n6. The authors do not seem to consider the impact of the number of attack iterations on the robustness in the results of white-box attack defense, such as the performance of PGD-10/50 and other attacks. In fact, the number of attacks may also have an important impact on the role of CAM technology."
                },
                "questions": {
                    "value": "1. Regarding the use of CAM technology, can the authors provide a more detailed analysis of its impact on training speed and memory consumption when integrated with the original AT framework? Are the performance gains achieved through CAM technology worth the potential trade-offs?\n2. The paper mentions combining standard AT for warm-up during training. Could the authors specify the number of training sessions required for both AT and PRAG in this process? How does this affect the training performance?\n3. CAM technology appears to be a visualization technique primarily applicable to classification problems. How can the authors address concerns about the limited applicability of the PART framework in domains beyond classification? Are there plans to extend its use to other areas?\n4. The clarity of results presentation is a concern. Could the authors provide more information about how standard deviations were calculated and specify the number of runs used in this calculation? \n5. Instead of comparing various CAM techniques, could the authors consider comparing the PART framework with a broader range of AT methods that utilize CAM technology, thereby offering a more comprehensive comparison of its effectiveness?\n6. White-box attack defense results are discussed, but the impact of the number of attack iterations on the performance of CAM technology isn't addressed. Can the authors provide insights into how the number of attack iterations influences the effectiveness of CAM technology in adversarial defense?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Reviewer_h86N",
                        "ICLR.cc/2024/Conference/Submission1033/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697972091189,
            "cdate": 1697972091189,
            "tmdate": 1700630790219,
            "mdate": 1700630790219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NwyQEMspUl",
                "forum": "HU1pesCJF4",
                "replyto": "fxhw1Idif4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h86N (part 1)"
                    },
                    "comment": {
                        "value": "Thank you so much for your time and efforts spent on reviewing our paper! Your thorough main review and comments are very important to the improvement of our work! Please find our replies below.\n\n>Q1. Regarding the use of CAM technology, can the authors provide a more detailed analysis of its impact on training speed and memory consumption when integrated with the original AT framework? \n\nA1: Incorporating CAM technologies will naturally lead to an increase in training speed. How to effectively integrate these technologies into the training process presents an intriguing challenge. If we generate the mask m for each epoch, the computational cost will be considerably high. **To address this problem, we update the mask m every 10 epochs.** We show that the performance of our method remains competitive (see Table 1) given the mask is updated for every 10 epochs. Regarding memory consumption, the majority of the memory is allocated for storing checkpoints, with only a small portion attributed to CAM technology. In general, the additional costs (i.e., training speed and memory consumption) are affordable (see Table 2 and 3). **This information has been included in the updated version of our manuscript.** For more details, please refer to Appendix M in the updated version.\n\nTable 1: Robustness (\\%) of defense methods on CIFAR-10. The target model is ResNet-18. We report the averaged results and standard deviations of three runs. We show the most successful defense in **bold**.\n\n| Method | Natural     | PGD-20      | MMA         | AA          |\n|--------|-------------|-------------|-------------|-------------|\n| AT     | 82.58 \u00b1 0.14| **43.69 \u00b1 0.28**| 41.80 \u00b1 0.10| 41.63 \u00b1 0.22|\n| PART (update m for every epoch) | 83.42 \u00b1 0.26 | 43.65 +- 0.16 | **41.98 \u00b1 0.03** | **41.74 \u00b1 0.04** |\n| PART (update m for every 10 epochs) | **83.77 \u00b1 0.15** | 43.36 +- 0.21 | 41.83 \u00b1 0.07 | 41.41 \u00b1 0.14 |\n| TRADES | 78.16 \u00b1 0.15| 48.28 \u00b1 0.05| 45.00 \u00b1 0.08| 45.05 \u00b1 0.12|\n| PART-T (update m for every epoch) | 79.36 \u00b1 0.31 | **48.90 \u00b1 0.14**|**45.90 \u00b1 0.07**| **45.97 \u00b1 0.06** |\\\n| PART-T (update m for every 10 epochs) | **80.13 \u00b1 0.16** | 48.72 \u00b1 0.11|45.59 \u00b1 0.09|45.60 \u00b1 0.04|\n| MART   | 76.82 \u00b1 0.28 | 49.86 \u00b1 0.32  | 45.42 \u00b1 0.04| 45.10 \u00b1 0.06 |\n| PART-M (update m for every epoch) |78.67 \u00b1 0.10| **50.26 \u00b1 0.17**|**45.53 \u00b1 0.05** | **45.19 \u00b1 0.04**|\n| PART-M (update m for every 10 epochs) |**80.00 \u00b1 0.15**| 49.71 \u00b1 0.12|45.14 \u00b1 0.10| 44.61 \u00b1 0.24|\n\nTable 2: Training speed (hours:minutes:seconds) of defense methods on CIFAR-10. The target model is ResNet-18.\n|GPU              |Method  | Training Speed | Difference |\n|-----------------|--------|----------------|------------|\n| 1 \\* NVIDIA A100| SAT    | 02:14:37       | \\          |\n|                 | PART   | 02:43:45       | 00:29:08   |\n|                 | TRADES | 02:44:19       | \\          |\n|                 | PART-T | 03:15:06       | 00:30:47   |\n|                 | MART   | 02:09:23       | \\          | \n|                 | PART-M | 02:39:37       | 00:30:14   |\n\n\nTable 3: Memory consumption (MB) of defense methods on CIFAR-10. The target model is ResNet-18.\n| Method   | Memory Consumption | Difference |\n|----------|--------------------|------------|\n| SAT      | 5530MB             |   \\        |\n| PART     | 5877MB             | 347MB      |\n| TRADES   | 5369MB             |    \\       |\n| PART-T   | 5688MB             | 319MB      |\n| MART     | 5553MB             |     \\      |\n| PART-M   | 5894MB             | 341MB      |\n\n\n> Q2. Are the performance gains achieved through CAM technology worth the potential trade-offs?\n\nA2: From Table 1 and 3, we believe the extra cost is marginal compared to the performance gains. Besides, it is reasonable that our method gains marginal improvement in robustness, as PART decreases the attack strength for unimportant pixel regions. **What we want to highlight is that PART can improve the robustness-accuracy trade-off by notably increasing the natural accuracy.** Despite a marginal reduction in robustness by 0.04\\% on PGD-20, PART gains more on natural accuracy (e.g., 2.08\\% on SVHN and 1.36\\% on Tiny-ImageNet). In most cases, PART can improve natural accuracy and robustness simultaneously. Compared to TRADES and MART, our method can still boost natural accuracy (e.g., 1.20\\% on CIFAR-10, 2.64\\% on SVHN and 1.26\\% on Tiny-ImageNet for PART-T, and 1.85\\% on CIFAR-10, 1.66\\% on SVHN and 1.07\\% on Tiny-ImageNet) with at most a 0.10\\% drop in robustness. It is widely acknowledged that AT will hurt natural accuracy compared to standard training. **Our work can bridge this gap without compromising robustness.** As demonstrated in [1], advancements in the field of AT have been incremental over the past several years. As a result, making every slight improvement in this area is noteworthy.\n\n[1]  Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In *ICLR*, 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377584296,
                "cdate": 1700377584296,
                "tmdate": 1700377584296,
                "mdate": 1700377584296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "don4pR2goN",
                "forum": "HU1pesCJF4",
                "replyto": "fxhw1Idif4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h86N (part 2)"
                    },
                    "comment": {
                        "value": "Thank you so much for your time and efforts spent on reviewing our paper! Your thorough main review and comments are very important to the improvement of our work! Please find our replies below.\n\n> Q3. The paper mentions combining standard AT for warm-up during training. Could the authors specify the number of training sessions required for both AT and PRAG in this process?\n\nA3: **The warm-up epoch number for PART is 20.** Specifically, we use AT to train the model for the first 20 epochs and then use PART for the remaining 60 epochs. For fair comparisons, all the baseline methods (i.e., AT, TRADES and MART) are trained for 80 epochs. **We have provided a detailed experiment setting that includes this information in Appendix F in the original submission.** \n\n> Q4. How does this affect the training performance? \n\nA4: Without the warm-up, the classifier is not properly learned initially, and thus may badly identify pixel regions that are important to the model's output in the early epochs. **Empirically, we add extra experiments to compare the performance of PART on CIFAR-10 with/without warm-up (see Table 4).**\n\nTable 4: Comparison of the performance of PART on CIFAR-10 with/without warm-up. The target model is ResNet-18. The number of warm-up epochs is 20. We report the averaged results and standard deviations of three runs. We show the most successful defense in **bold**.\n| Method | Natural     | PGD-20      | MMA         | AA          |\n|--------|-------------|-------------|-------------|-------------|\n| PART (with warm-up) | **83.42 \u00b1 0.26** | **43.65 +- 0.16** | **41.98 \u00b1 0.03** | **41.74 \u00b1 0.04** |\n| PART (without warm-up) | 83.25 \u00b1 0.17 | 43.27 +- 0.11 | 41.80 \u00b1 0.08 | 41.36 \u00b1 0.06 |\n\n\n> Q5. CAM technology appears to be a visualization technique primarily applicable to classification problems. How can the authors address concerns about the limited applicability of the PART framework in domains beyond classification? Are there plans to extend its use to other areas?\n\nA5: Thanks for your insightful question. You are right, CAM technology is initially designed for classification problems. However, one important thing we want to highlight is that **PART is a general idea rather than a specific method.** Specifically, PART is a general idea that we should count the fundamental discrepancies of pixel regions across images. **CAM technology, however, is only a tool to implement this idea.** If there is a tool that can be extended to other areas, we can integrate it into our framework. **The key is that the idea itself can be applied in other domains beyond classification, which is more important than how we implement it.** Similarly, most AT methods are designed to solve classification problems, but their ideas can be extended to other areas.\n\n> Q6. The clarity of results presentation is a concern. Could the authors provide more information about how standard deviations were calculated and specify the number of runs used in this calculation?\n\nA6: Thanks for your suggestion! We apologize that we missed stating the number of runs used in mean and standard deviation calculations. **We follow [1] and report the averaged results and standard deviations of 3 runs through the whole paper. We have specified this information in the updated version of our manuscript.** In our original submission, we did not report the standard deviations in Table 2 because of the limited page margin. **We have fixed this issue** by shrinking the font size to \u2018footnotesize\u2019, which also makes our tables more compact than before.\n\n[1]  Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In *ICLR*, 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377726540,
                "cdate": 1700377726540,
                "tmdate": 1700377726540,
                "mdate": 1700377726540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "am09JmbuFb",
                "forum": "HU1pesCJF4",
                "replyto": "fxhw1Idif4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h86N (part 3)"
                    },
                    "comment": {
                        "value": "Thank you so much for your time and efforts spent on reviewing our paper! Your thorough main review and comments are very important to the improvement of our work! Please find our replies below.\n\n> Q7: Instead of comparing various CAM techniques, could the authors consider comparing the PART framework with a broader range of AT methods that utilize CAM technology, thereby offering a more comprehensive comparison of its effectiveness?\n\nA7: Thanks for your suggestion! To the best of our knowledge, we can find two adversarial defenses that incorporate CAM technologies. We have cited them in our related work (see Appendix C in the original submission for more details). \n\nSpecifically, [1] proposes to use class activation features to remove adversarial noise. This method can be regarded as an adversarial purification method, which purifies adversarial examples towards natural examples. **Our method is fundamentally different from [1]** because that PART is an AT framework. Therefore, we cannot compare our method to [1] due to the different problem settings. \n\nOn the other hand, [2] proposes a framework that uses GradCAM to rectify and preserve the visual attention area, which aims to improve the robustness against adversarial attacks by aligning the visual attention area between adversarial and original images. This method is an AT method and thus it is fair to compare our method to [2]. **However, we cannot find any source code of [2]. Therefore, it may take some time for us to realize their code. We will update the results once we finish the comparison.**\n\n[1] Dawei Zhou, Nannan Wang, Chunlei Peng, Xinbo Gao, Xiaoyu Wang, Jun Yu, and Tongliang Liu. Removing adversarial noise in class activation feature space. In *ICCV*, 2021.\n\n[2] Shangxi Wu, Jitao Sang, Kaiyuan Xu, Jiaming Zhang, and Jian Yu. Attention, please! adversarial defense via activation rectification and preservation. *ACM Trans. Multim. Comput. Commun. Appl*., 2023.\n\n> Q8. White-box attack defense results are discussed, but the impact of the number of attack iterations on the performance of CAM technology isn't addressed. Can the authors provide insights into how the number of attack iterations influences the effectiveness of CAM technology in adversarial defense?\n\nA8: **There might be a misunderstanding.** In Section 4.2 'Defending against adaptive attacks', we **have compared** our method to the baseline AT methods (i.e., AT, TRADES and MART) against adaptive PGD-20/40/60/80/100 (see Table 2 in the original submission for more details). From the experiment results, with the increase of attack iterations, the robustness of defense methods will decrease. This is because the possibility of finding worst-case examples will increase with more attack iterations. However, **the effectiveness of CAM technology itself is rarely influenced by attack iterations**, as we show that our method can consistently outperform baseline methods. Without losing generality, **we conduct an additional experiment** against normal PGD-10/40/60/80/100 (see Table 5). Again, we can obtain the same conclusions from the results. **We have included Table 5 in Appendix I in the updated version of our manuscript.**\n\nTable 5: Robustness (\\%) of defense methods against PGD with different iterations on CIFAR-10. We report the averaged results and standard deviations of three runs. We show the most successful defense in **bold**.\n| Dataset   | Method | PGD-10              | PGD-40              | PGD-60              | PGD-80              | PGD-100             |\n|-----------|--------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| CIFAR-10  | AT     | 44.83 \u00b1 0.13        | 43.00 \u00b1 0.10        | 42.83 \u00b1 0.07        | 42.81 \u00b1 0.03        | 42.81 \u00b1 0.03        |\n|           | PART   | **45.20 \u00b1 0.17**    | **43.20 \u00b1 0.14**    | **43.09 \u00b1 0.09**    | **43.08 \u00b1 0.10**    | **42.93 \u00b1 0.07**    |\n|           | TRADES | 48.81 \u00b1 0.21        | 48.19 \u00b1 0.13        | 48.16 \u00b1 0.15        | 48.14 \u00b1 0.08        | 48.08 \u00b1 0.04        |\n|           | PART-T | **49.41 \u00b1 0.11**    | **48.65 \u00b1 0.10**    | **48.64 \u00b1 0.13**    | **48.64 \u00b1 0.04**    | **48.62 \u00b1 0.03**    |\n|           | MART   | 49.98 \u00b1 0.08        | 49.66 \u00b1 0.16        | 49.66 \u00b1 0.06        | 49.54 \u00b1 0.03        | 49.47 \u00b1 0.05        |\n|           | PART-M | **50.50 \u00b1 0.19**    | **50.19 \u00b1 0.15**    | **50.09 \u00b1 0.04**    | **50.06 \u00b1 0.05**    | **50.05 \u00b1 0.02**    |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377809741,
                "cdate": 1700377809741,
                "tmdate": 1700395957858,
                "mdate": 1700395957858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "anzagtBDI3",
                "forum": "HU1pesCJF4",
                "replyto": "fxhw1Idif4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 21 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer h86N,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript.  Just a quick reminder that discussion stage 1 is closing soon. \n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540072016,
                "cdate": 1700540072016,
                "tmdate": 1700607437499,
                "mdate": 1700607437499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vGIQHTMahv",
                "forum": "HU1pesCJF4",
                "replyto": "anzagtBDI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_h86N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_h86N"
                ],
                "content": {
                    "title": {
                        "value": "Further comments"
                    },
                    "comment": {
                        "value": "Many thanks to the authors for their careful responses. I think the authors provide adequate explanations for most of the questions, so I will raise my rating. Whereas, \n\n1. In the first table provided by the author, the experimental results of updating m every 10 epochs do not look satisfactory, and most of the results are not improved; \n\n2. I would like to thank the authors for providing the results of the fifth table, but actually I was always interested that the authors could include some analysis of the effect of the number of attack iterations during training on the performance of the method in the subsequent versions."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630769623,
                "cdate": 1700630769623,
                "tmdate": 1700630769623,
                "mdate": 1700630769623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YYPFVD2igl",
            "forum": "HU1pesCJF4",
            "replyto": "HU1pesCJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_tC3M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_tC3M"
            ],
            "content": {
                "summary": {
                    "value": "This submission proposes pixel-wise reweighting for adversarial training. The central observation presented in this work is that not all pixels of the adversarial perturbation contribute equally to the accuracy of the model. The authors propose a new framework for adversarial training called pixel-reweighed adversarial training (PART) which uses class activation mapping to identify important pixel regions. Authors evaluate their adversarial training framework on the CIFAR-10, SVHN, and Tiny-ImageNet datasets using a ResNet \nand a WideResNet model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents theoretical results on a toy model which help understanding the method.\n- The empirical evaluation is solid and the proposed PART method is compared against vanilla adversarial training, TRADES, and MART.\n- The paper reads mostly very well and is very understandable. I only found a few typos (see below)."
                },
                "weaknesses": {
                    "value": "- The idea is not completely novel. Adversarial attacks in combination with class activation mappings have for example been discussed in [1]. However, the authors use it for robustifying their models which is in my opinion sufficiently different. Nonetheless, authors should include a citation of that work.\n- The empirical evaluation can be extended by using different adversarial attacks, e.g., Carlini-Wagner attack or AutoAttack.\n- The literature review seems somewhat short. I suggest authors spend more time looking for relevant related works.\n- Performence of the PART method is somewhat underwhelming. The improvement is only incremental (usually only in the range of ~1%).\n- Section 3.1 \u201cAE generation process.\u201d is tough to read. Authors should work on the presentation of that section. Maybe a small table on the side would help to introduce the notation.\n- Figure 4: Authors should mention what is indicated by the shaded areas.\n\nOverall, the ideas in this paper are not ground-breaking, but the solid theoretical and empirical analysis justify its publication in ICLR, which is why I recommend to accept this submission.\n\nMinor details: \n- Missing whitespace \u201cTable 2: Robustness(%) of\u2026\u201d\n- Eq. 12-15 \u201csubject to\u201d should not be typeset in math mode\n- Lemma 1: \u201c(i).\u201d unusual period \n\nReferences\n\n[1] Dong, Xiaoyi, et al. \"Robust superpixel-guided attentional adversarial attack.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."
                },
                "questions": {
                    "value": "- Figure 3: What should the lock symbol next to the first CNN tell me? \n- Figure 3: In what sense are the activation maps of that CNN \u201cglobal\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Reviewer_tC3M"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698322482070,
            "cdate": 1698322482070,
            "tmdate": 1700576125796,
            "mdate": 1700576125796,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "szHBY9TGDs",
                "forum": "HU1pesCJF4",
                "replyto": "YYPFVD2igl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tC3M (part 1)"
                    },
                    "comment": {
                        "value": "Thank you so much for your positive comments and suggestions! It is our pleasure that our theoretical and empirical analysis can be recognized. Your thorough main review and comments are very important to the improvement of our work! Please find our replies below. \n\n> Q1. The idea is not completely novel. Adversarial attacks in combination with class activation mappings have for example been discussed in [1]. However, the authors use it for robustifying their models which is in my opinion sufficiently different. Nonetheless, authors should include a citation of that work.\n> [1] Dong, Xiaoyi, et al. \"Robust superpixel-guided attentional adversarial attack.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nA1: Thanks for your suggestion! **We have cited this paper in our related work in Appendix C in the updated version of our manuscript (highlighted in blue).** Here is the added content:\n\n**Adversarial attacks with class activation mapping.** Dong et al. (2020) proposes an attack method that leverages superpixel segmentation and class activation mapping to focus on regions of an image that are most influential in classification decisions. It highlights the importance of considering perceptual features and classification-relevant regions in crafting effective AEs.\n\n> Q2: The empirical evaluation can be extended by using different adversarial attacks, e.g., Carlini-Wagner attack or AutoAttack.\n\nA2: Thanks for your suggestion! **However, there might be a misunderstanding. For general attacks, we have evaluated our methods and baselines using AutoAttack (see Table 1 in the original submission).** For adaptive attacks, we conduct an additional experiment to test the robustness of defense methods against adaptive MMA (see Table 1 below). The choice of MMA over AA for adaptive attacks is due to AA's time-consuming nature as an ensemble of multiple attacks. Incorporating the CAM method into AA would further slow the process. MMA, in contrast, offers greater time efficiency and comparable performance to AA. **We have added Table 1 below in Appendix J, please refer to the updated version of our manuscript.**\n\nTable 1: Robustness (\\%) of defense methods against adaptive MMA on CIFAR-10. We report the averaged results and standard deviations of three runs. We show the most successful defense in **bold**.\n\n| Dataset   | Method | MMA-20              | MMA-40              | MMA-60              | MMA-80              | MMA-100             |\n|-----------|--------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| CIFAR-10  | AT     | 35.36 \u00b1 0.10        | 35.02 \u00b1 0.05        | 34.93 \u00b1 0.09        | 34.86 \u00b1 0.06        | 34.85 \u00b1 0.07        |\n|           | PART   | **35.67 \u00b1 0.07**    | **35.35 \u00b1 0.11**    | **35.29 \u00b1 0.13**    | **35.29 \u00b1 0.09**    | **35.17 \u00b1 0.05**    |\n|           | TRADES | 40.14 \u00b1 0.08        | 39.89 \u00b1 0.12        | 39.93 \u00b1 0.05        | 39.87 \u00b1 0.08        | 39.82 \u00b1 0.03        |\n|           | PART-T | **40.78 \u00b1 0.13**    | **40.57 \u00b1 0.11**    | **40.51 \u00b1 0.08**    | **40.49 \u00b1 0.05**    | **40.48 \u00b1 0.02**    |\n|           | MART   | 39.14 \u00b1 0.06        | 38.79 \u00b1 0.13        | 38.80 \u00b1 0.10        | 38.79 \u00b1 0.05        | 38.74 \u00b1 0.08        |\n|           | PART-M | **40.56 \u00b1 0.11**    | **40.26 \u00b1 0.07**    | **40.23 \u00b1 0.12**    | **40.21 \u00b1 0.08**    | **40.20 \u00b1 0.07**    |\n\n> Q3. The literature review seems somewhat short. I suggest authors spend more time looking for relevant related works.\n\nA3: Due to the limited space, the literature review in the main body of the paper is short. **However, we have provided a detailed literature review in Appendix C,** which covers the literature review of different AT methods, CAM methods, adversarial defenses with CAM and adversarial attacks with CAM. **We have also included the discussion regarding the literature suggested in your Q1.** Please check the full literature review in the updated version of our manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377346818,
                "cdate": 1700377346818,
                "tmdate": 1700377346818,
                "mdate": 1700377346818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ShRhqa7eMh",
                "forum": "HU1pesCJF4",
                "replyto": "YYPFVD2igl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tC3M (part 2)"
                    },
                    "comment": {
                        "value": "Thank you so much for your positive comments and suggestions! It is our pleasure that our theoretical and empirical analysis can be recognized. Your thorough main review and comments are very important to the improvement of our work! Please find our replies below. \n\n> Q4. Performence of the PART method is somewhat underwhelming. The improvement is only incremental (usually only in the range of ~1%).\n\nA4: It is reasonable that our method gains marginal improvement in robustness, as PART decreases the attack strength for unimportant pixel regions. **What we want to highlight is that PART can improve the robustness-accuracy trade-off by notably increasing the natural accuracy.** Despite a marginal reduction in robustness by 0.04\\% on PGD-20, PART gains more on natural accuracy (e.g., 2.08\\% on SVHN and 1.36\\% on Tiny-ImageNet). In most cases, PART can improve natural accuracy and robustness simultaneously. Compared to TRADES and MART, our method can still boost natural accuracy (e.g., 1.20\\% on CIFAR-10, 2.64\\% on SVHN and 1.26\\% on Tiny-ImageNet for PART-T, and 1.85\\% on CIFAR-10, 1.66\\% on SVHN and 1.07\\% on Tiny-ImageNet) with at most a 0.10\\% drop in robustness. It is widely acknowledged that AT will hurt natural accuracy compared to standard training. **Our work can bridge this gap without compromising robustness.** As demonstrated in [1], advancements in the field of AT have been incremental over the past several years. As a result, making every slight improvement in this area is noteworthy.\n\n[1]  Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In *ICLR*, 2022.\n\n> Q5. Section 3.1 \u201cAE generation process.\u201d is tough to read. Authors should work on the presentation of that section. Maybe a small table on the side would help to introduce the notation.\n\nA5: Thanks for your suggestion! **We have uploaded a reference list of the notations used in Section 3.1 in Appendix D. Please check the updated version of our manuscript.** For your convienience, we also show the notation table below.\n\n| Symbol | Description |\n| ------ | ----------- |\n| $\\ell$ | A loss function |\n| $f$ | A model |\n| $\\mathbf{x}$ | A natural image |\n| $y$ | The true label of $\\mathbf{x}$ |\n| $d$ | The data dimension |\n| $\\mathbf{\\Delta}$ | The adversarial perturbation added to $\\mathbf{x}$ |\n| $\\mathbf{\\Delta}^*$ | The optimal solution of  $\\mathbf{\\Delta}$ |\n| $\\|\\cdot\\|_{\\infty}$ | The $\\ell_{\\infty}$-norm |\n| $\\epsilon$ | The maximum allowed perturbation budget for important pixels |\n| $\\epsilon^{\\rm low}$ | The maximum allowed perturbation budget for unimportant pixels |\n| $\\mathcal{I}^{\\rm high}$ | Indexes of important pixels |\n| $\\mathcal{I}^{\\rm low}$ | Indexes of unimportant pixels |\n| $\\bf{v}$ | A function to transform a set to a vector |\n| $\\\\{\\delta_i\\\\}_{i \\in \\mathcal{I}^{\\rm high}}$ | A set consisting of important pixels in $\\mathbf{\\Delta}$, i.e., $\\mathbf{\\Delta}^{\\rm high}$ |\n| $ \\\\{\\delta_i\\\\}_{i \\in \\mathcal{I}^{\\rm low}}$ | A set consisting of unimportant pixels in $\\mathbf{\\Delta}$, i.e., $\\mathbf{\\Delta}^{\\rm low}$ |\n| $\\|\\mathcal{I}^{\\rm high}\\|$ | The dimension of important pixel regions, i.e., $d^{\\rm high}$ |\n| $\\|\\mathcal{I}^{\\rm low}\\|$ | The dimension of unimportant pixel regions, i.e., $d^{\\rm low}$ |\n\n> Q6. Figure 4: Authors should mention what is indicated by the shaded areas.\n\nA6: Thanks for your suggestion! We apologize that we missed stating the meaning of the shaded areas. **The shaded areas represent the standard deviation. We have included this information in the updated version of our manuscript.**\n\n> Q7. Minor details (e.g., missing whitespace \u201cTable 2: Robustness(%) of\u2026\u201d , Eq. 12-15 \u201csubject to\u201d should not be typeset in math mode and Lemma 1: \u201c(i).\u201d unusual period).\n \nA7: Thank you for pointing out these minor issues in our paper! **We have fix these issues in the updated version of our manuscript.**\n\n> Q8. Figure 3: What should the lock symbol next to the first CNN tell me?\n\nA8: The lock symbol means **the parameters of the model are fixed**. This is because during the generation of AE, the parameters of the model are unchanged. **We have explained the meaning of the lock symbol in the legend of the figure in our updated version of manuscript.**\n\n> Q9. Figure 3: In what sense are the activation maps of that CNN \u201cglobal\u201d ?\n\nA9: We apologize for the misuse the word 'global'. The feature maps generated by CAM methods are actually a form of visual explanation, showing which parts of the input image the network focuses on when making decisions. These feature maps usually concentrate more on local regions (i.e., the areas of the image most crucial for the classification decision) rather than the entire image. Therefore, referring to the feature maps extracted by CAM methods as 'global feature maps' might not be accurate. **To avoid confusion, we have removed the word 'global' in the updated version of our manuscript.**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377443698,
                "cdate": 1700377443698,
                "tmdate": 1700396906092,
                "mdate": 1700396906092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iXr3benSF5",
                "forum": "HU1pesCJF4",
                "replyto": "YYPFVD2igl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 21 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer tC3M,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript.  Just a quick reminder that discussion stage 1 is closing soon. \n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540024621,
                "cdate": 1700540024621,
                "tmdate": 1700607424785,
                "mdate": 1700607424785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F5utxHb3Ba",
                "forum": "HU1pesCJF4",
                "replyto": "ShRhqa7eMh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_tC3M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_tC3M"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification and the additional results. My concerns were adequately addressed and I have raised my score to 8."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576177422,
                "cdate": 1700576177422,
                "tmdate": 1700576177422,
                "mdate": 1700576177422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FXXgyexj5w",
            "forum": "HU1pesCJF4",
            "replyto": "HU1pesCJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_u6mu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_u6mu"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Pixel-reweighted Adversarial Training (PART), a adjusted adversarial training framework designed to enhance model robustness against adversarial attacks. PART introduces a dynamic perturbation allocation strategy, redistributing the perturbation across pixels according to their influence on model output. This is a departure from traditional AT methods, which apply a uniform noise across all pixels. It tested on the common benchmarks to proof that such reweighted perturbation enhances model performance by allowing the model to focus on more critical areas of the image that significantly impact model decisions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper takes a step further on the adversarial training. The concept of pixel influence on robustness and accuracy is well-motivated, grounded on the premise that not all parts of an image equally contribute to the decision-making process of a neural network. Overall, the paper is easy to follow. \n\nThe authors conducted experiments on common benchmarks, including CIFAR-10, SVHN and Tiny-ImageNet. Extensive results show the proposed PART generally outperforms standard AT."
                },
                "weaknesses": {
                    "value": "I am not surprised by the proposed method that introducing CAM to direction adversarial training to the semantic meaningful regions. \n\nAlso, I am unsure if the proposed method can be scaled up --- due to (1) CAM may not be scalable which means it may lose the ability to identify the semantic meaningful regions; (2) the author currently did not analyze the computation cost and training time cost of the proposed PART compared to AT and standard training.\n\n# Post-rebuttal\n\nI raised my score due to author provide more detailed comparisons. However, I am unsure if the proposed framework can be scale-up or not. Hope AC can examine this point."
                },
                "questions": {
                    "value": "I would be curious if the proposed method applied into larger dataset which also obtains improvements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Reviewer_u6mu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783675301,
            "cdate": 1698783675301,
            "tmdate": 1700724968403,
            "mdate": 1700724968403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VueVLEGMg9",
                "forum": "HU1pesCJF4",
                "replyto": "FXXgyexj5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer u6mu"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable comments and questions! Please find our replies below.\n\n> Q1. CAM may not be scalable which means it may lose the ability to identify the semantic meaningful regions.\n\nA1: **Based on our experiment results on Tiny-ImageNet, we believe CAM can be scaled to large datasets.** The scalability of CAM methods to large datasets primarily depends on the scalability of the model it integrates with. CAM itself is a visualization technique used to highlight the regions of an image that CNNs focus on when making decisions. If the network itself can effectively process large datasets (e.g., having sufficient learning capacity and appropriate computational efficiency), then CAM can also be applied to these datasets. Due to the high computational cost of AT, conducting experiments on larger datasets such as ImageNet would require significant resources. Typically, **Tiny-ImageNet, a scaled-down version of ImageNet, is already a relatively large dataset used in the field of AT.** When applying our method to a very large dataset such as ImageNet, we can **scale down** the size of images to fit the capacity of our network, and thus CAM methods will not lose the ability to identify the semantic meaningful regions.\n\n> Q2. The author currently did not analyze the computation cost and training time cost of the proposed PART compared to AT and standard training.\n\nA2: Incorporating CAM technologies will naturally lead to an increase in training speed. How to effectively integrate these technologies into the training process presents an intriguing challenge. If we generate the mask m for each epoch, the computational cost will be considerably high. **To address this problem, we update the mask m every 10 epochs.** We show that the performance of our method remains competitive (see Table 1) given the mask is updated for every 10 epochs. Regarding memory consumption, the majority of the memory is allocated for storing checkpoints, with only a small portion attributed to CAM technology. In general, the additional costs (i.e., training speed and memory consumption) are affordable (see Table 2 and 3). **This information has been included in the updated version of our manuscript.** For more details, please refer to Appendix M in the updated version.\n\nTable 1: Robustness (\\%) of defense methods on CIFAR-10. The target model is ResNet-18. We report the averaged results and standard deviations of three runs. We show the most successful defense in **bold**.\n| Method | Natural | PGD-20| MMA  | AA |\n|--------|-------------|-------------|-------------|-------------|\n| AT     | 82.58 \u00b1 0.14| **43.69 \u00b1 0.28**| 41.80 \u00b1 0.10| 41.63 \u00b1 0.22|\n| PART (update m for every epoch) | 83.42 \u00b1 0.26 | 43.65 +- 0.16 | **41.98 \u00b1 0.03** | **41.74 \u00b1 0.04** |\n| PART (update m for every 10 epochs) | **83.77 \u00b1 0.15** | 43.36 +- 0.21 | 41.83 \u00b1 0.07 | 41.41 \u00b1 0.14 |\n| TRADES | 78.16 \u00b1 0.15| 48.28 \u00b1 0.05| 45.00 \u00b1 0.08| 45.05 \u00b1 0.12|\n| PART-T (update m for every epoch) | 79.36 \u00b1 0.31 | **48.90 \u00b1 0.14**|**45.90 \u00b1 0.07**| **45.97 \u00b1 0.06** |\\\n| PART-T (update m for every 10 epochs) | **80.13 \u00b1 0.16** | 48.72 \u00b1 0.11|45.59 \u00b1 0.09|45.60 \u00b1 0.04|\n| MART   | 76.82 \u00b1 0.28 | 49.86 \u00b1 0.32  | 45.42 \u00b1 0.04| 45.10 \u00b1 0.06 |\n| PART-M (update m for every epoch) |78.67 \u00b1 0.10| **50.26 \u00b1 0.17**|**45.53 \u00b1 0.05** | **45.19 \u00b1 0.04**|\n| PART-M (update m for every 10 epochs) |**80.00 \u00b1 0.15**| 49.71 \u00b1 0.12|45.14 \u00b1 0.10| 44.61 \u00b1 0.24|\n\nTable 2: Training speed (hours:minutes:seconds) of defense methods on CIFAR-10. The target model is ResNet-18.\n|GPU |Method  | Training Speed | Difference |\n|-----------------|--------|----------------|------------|\n| 1 \\* NVIDIA A100| SAT| 02:14:37| \\ |\n| | PART| 02:43:45| 00:29:08   |\n| | TRADES | 02:44:19| \\ |\n| | PART-T | 03:15:06| 00:30:47   |\n| | MART   | 02:09:23| \\ | \n| | PART-M | 02:39:37 | 00:30:14   |\n\nTable 3: Memory consumption (MB) of defense methods on CIFAR-10. The target model is ResNet-18.\n| Method   | Memory Consumption | Difference |\n|-----|-----|-----|\n| SAT | 5530MB | \\ |\n| PART| 5877MB| 347MB|\n| TRADES| 5369MB| \\ |\n| PART-T | 5688MB | 319MB |\n| MART | 5553MB  | \\ |\n| PART-M| 5894MB  | 341MB |\n\n\n> Q3. I would be curious if the proposed method applied into larger dataset which also obtains improvements. \n\nA3: On Tiny-ImageNet, our method can consistently improve the performance compared to baseline methods. If the 'larger' here refers to the number of categories, Tiny-ImageNet with 200 categories can be considered a relatively large dataset. If the 'larger' here refers to the image dimensions, we can scale down the image size to fit the model capacity. As mentioned in Q1, the scalability of CAM methods to large datasets primarily depends on the scalability of the model it integrates with. In terms of the computation cost and memory consumption, we have provided a detailed comparison in Q2. In general, **based on our experiment results on Tiny-ImageNet, we believe CAM can be scaled to large datasets.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376999959,
                "cdate": 1700376999959,
                "tmdate": 1700396344847,
                "mdate": 1700396344847,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e13jwrgDvn",
                "forum": "HU1pesCJF4",
                "replyto": "FXXgyexj5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 21 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer u6mu,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript.  Just a quick reminder that discussion stage 1 is closing soon. \n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539986824,
                "cdate": 1700539986824,
                "tmdate": 1700607358780,
                "mdate": 1700607358780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hgR4kuioWZ",
                "forum": "HU1pesCJF4",
                "replyto": "FXXgyexj5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 22 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer u6mu,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript. Just a quick reminder that discussion stage 1 is closing soon.\n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further. \n\nYour feedback is very important to our work. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638539135,
                "cdate": 1700638539135,
                "tmdate": 1700638539135,
                "mdate": 1700638539135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xM3STC9Rjq",
                "forum": "HU1pesCJF4",
                "replyto": "FXXgyexj5w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 23 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer u6mu,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript. Just a quick reminder that discussion stage 1 is closing soon.\n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further.\n\nYour feedback is very important to our work. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723391461,
                "cdate": 1700723391461,
                "tmdate": 1700723391461,
                "mdate": 1700723391461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qgupkc53bU",
                "forum": "HU1pesCJF4",
                "replyto": "xM3STC9Rjq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_u6mu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_u6mu"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "I appreciate the efforts made by the authors. I believe adding the detailed time and memory cost can help readers understand the performance of the proposed method. \n\nI raised my score, while I'd like to confirm that your wordings cannot solve my concerns about if the proposed method can be scaled up or not. I will mention my concern to AC in this aspect."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724897948,
                "cdate": 1700724897948,
                "tmdate": 1700724897948,
                "mdate": 1700724897948,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rYopvOTzoU",
            "forum": "HU1pesCJF4",
            "replyto": "HU1pesCJF4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_QPEP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1033/Reviewer_QPEP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new framework of pixel-based re-weighting to gauge a more robust way of adversarial training. Authors begin with a proof of concept example showing how not all part of an image are equally informative, and then proceed to create an automated pipeline for adversarial training that can generalize and extend to multiple images based on gradient-based methods that show what parts of the image activate a certain class the most (parts that will later be weight more aggressively for the attack). Authors finish the paper with additional quantitative plots. I wish authors would have shown at the end how their adversarial images for networks trained on PART look like."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper introduces a way to improve adversarial robustness through part-based re-weighting given the interesting parts of information in an image.\n* Authors propose a modular framework that can be used for future adversarial training pipelines.\n* Authors show how PART is better than many other adversarial training pipelines but the increase is very incremental. Should the paper be accepted because of this last point? I am not entirely convinced."
                },
                "weaknesses": {
                    "value": "* The paper says towards the end that this framework in more aligned to human perception. I don't think this is true from what has been shown in the paper. I would have liked to see qualitative samples and attack comparing how PGD performs on the same network trained differently (without AT, and with AT either classical or PART based), and from there run a psychophysical experiment with human observers to see if indeed they are fooled more by the PART-based model. While running the psychophysical experiments may not be possible, even adding the resulting adversarial images from networks trained with PART would be a great addition to the paper.\n\n* I'd really recommend plotting the adversarial images for networks trained on PART similar to how this was done in Santurkar et al. (NeurIPS 2019), Berrios & Deza (ArXiv 2022) and Gaziv et al. (NeurIPS 2023)."
                },
                "questions": {
                    "value": "I think this paper is interesting but I am on fence of the contribution. Are all pixels equally important in an image? My gut feeling says that the answer is No, and perhaps it's a bit of a tautology (this seems quite obvious). Would it not be possible that performing adversarial training end-to-end with PGD-based type image diets automatically help a neural network find these critical parts in the image from which to then perturb the image at training? All-in-all, my question is: is PART really useful when there are many other adversarial training regimes that go beyond FGSM and that implicitly incorporate the image structure in the adversarial optimization?\n\nI am not an expert in the adversarial robustness literature, so I am curious to hear what other reviewers say about this proposed training framework. I am willing to change my mind depending on the rebuttal and on knowing what the other reviews have to say about this paper.\n\nPerhaps another interesting question that I would have liked authors answer is. How would PART work with training on other image distributions such as Textures or Scenes? Would PART be equally useful or will it only apply to objects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1033/Reviewer_QPEP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1033/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699333307736,
            "cdate": 1699333307736,
            "tmdate": 1700581116866,
            "mdate": 1700581116866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1io6qz6KYZ",
                "forum": "HU1pesCJF4",
                "replyto": "rYopvOTzoU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QPEP (part 1)"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable suggestions and intetersting questions! Please find our replies below.\n\n> Q1. Authors show how PART is better than many other adversarial training pipelines but the increase is very incremental. Should the paper be accepted because of this last point? I am not entirely convinced.\n\nIt is reasonable that our method gains marginal improvement in robustness, as PART decreases the attack strength for unimportant pixel regions. **What we want to highlight is that PART can improve the robustness-accuracy trade-off by notably increasing the natural accuracy.** Despite a marginal reduction in robustness by 0.04\\% on PGD-20, PART gains more on natural accuracy (e.g., 2.08\\% on SVHN and 1.36\\% on Tiny-ImageNet). In most cases, PART can improve natural accuracy and robustness simultaneously. Compared to TRADES and MART, our method can still boost natural accuracy (e.g., 1.20\\% on CIFAR-10, 2.64\\% on SVHN and 1.26\\% on Tiny-ImageNet for PART-T, and 1.85\\% on CIFAR-10, 1.66\\% on SVHN and 1.07\\% on Tiny-ImageNet) with at most a 0.10\\% drop in robustness. It is widely acknowledged that AT will hurt natural accuracy compared to standard training. **Our work can bridge this gap without compromising robustness.** As demonstrated in [1], advancements in the field of AT have been incremental over the past several years. As a result, making every slight improvement in this area is noteworthy.\n\n[1]  Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In *ICLR*, 2022.\n\n> Q2. The paper says towards the end that this framework in more aligned to human perception. I don't think this is true from what has been shown in the paper. I would have liked to see qualitative samples and attack comparing how PGD performs on the same network trained differently (without AT, and with AT either classical or PART based), and from there run a psychophysical experiment with human observers to see if indeed they are fooled more by the PART-based model. While running the psychophysical experiments may not be possible, even adding the resulting adversarial images from networks trained with PART would be a great addition to the paper. I'd really recommend plotting the adversarial images for networks trained on PART similar to how this was done in Santurkar et al. (NeurIPS 2019), Berrios & Deza (ArXiv 2022) and Gaziv et al. (NeurIPS 2023).\n\nA2: Thanks for your great recommendation and thanks for bringing these insightful papers to us, **but there might be a misunderstanding.** One thing we would like to point out is that 'more aligned to human perception' **does not mean** that AEs generated by PART-based methods can fool more human observers. As long as the adversarial noise itself is bounded by a small $\\epsilon$ (i.e., imperceptible to human eyes), every adversarial attack can fool human observers. In our paper, **'the alignment to human perception' refers to how well the model's attention aligns with the object (i.e., semantic meaning) in an image.** We find that the previously stated sentence may cause unnecessary confusion. Thus, we have changed **'human-aligned information'** into **'semantic information'** in the updated version of our manuscript.\n\nWe agree with your comment that adding the resulting adversarial images from networks trained with PART would be a great addition to the paper. In our original submission, we **have provided** examples of the high-contribution pixel regions learnt by the same network trained differently (e.g., without AT, with AT and with PART-based) in Appendix N. In Figure 6, it is clear that the network trained without AT tends to focus on the background of the image, and the network trained with PART is **more precisely aligned with the object** than the network trained with AT. \n\nTo further support our statements, **we conduct additional experiments** to visualize how the high-contribution pixel regions change every 10 epochs from the 30th epoch to the 60th epoch (see Figures 7 and 8 in the updated version of our manuscript). With the increase of epochs, PART-based methods are more precisely aligned to the object (i.e., semantic meaning) in an image compared to baseline AT methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376771608,
                "cdate": 1700376771608,
                "tmdate": 1700377186822,
                "mdate": 1700377186822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "up3QVN01YS",
                "forum": "HU1pesCJF4",
                "replyto": "rYopvOTzoU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QPEP (part 2)"
                    },
                    "comment": {
                        "value": "Thank you so much for your valuable suggestions and intetersting questions! Please find our replies below.\n\n> Q3. I think this paper is interesting but I am on fence of the contribution. Are all pixels equally important in an image? My gut feeling says that the answer is No, and perhaps it's a bit of a tautology (this seems quite obvious).\n\nA3: Thanks for thinking our paper is interesting. However, in our humble opinions, it is not very fair to say our contribution is limited just because this paper has a straightforward intuition. Instead, we treat this as a strength of our paper and we would like to elaborate on this point. While the intuition behind our paper might appear straightforward, its simplicity should not be mistaken for a lack of depth or contribution. We have validated this intuitive idea from both empirical and theoretical standpoints. Our work not only demonstrates the practical applicability of our intuition but also provides theoretical insights that reinforces its validity. We want to emphsize that **simple intuitions can also lead to significant advancements, as long as they are well investigated and validated.** \n\n\n> Q4. Would it not be possible that performing adversarial training end-to-end with PGD-based type image diets automatically help a neural network find these critical parts in the image from which to then perturb the image at training? All-in-all, my question is: is PART really useful when there are many other adversarial training regimes that go beyond FGSM and that implicitly incorporate the image structure in the adversarial optimization?\n\nA4: It is possible. PGD-based attacks aim to find the minimum perturbation that can fool the neural network. This process often highlights the most sensitive and critical features of the image that the network relies on for making decisions. **However, the problem is that we find these critical features learnt by PGD-based AT are not sufficient.** This is also one of the key contributions of our paper. We find that without explicit guidance, it is hard for AT methods to sufficiently mine robust features. **The advantage of PART, on the other hand, is the ability to provide external guidance to help models better extract features that are beneficial to robust classification.**\n\n> Q5. Perhaps another interesting question that I would have liked authors answer is. How would PART work with training on other image distributions such as Textures or Scenes? Would PART be equally useful or will it only apply to objects? \n\nA5: Thanks for your insightful question! **The applicability of PART primarily depends on the applicability of the model it integrates with.** PART is a general idea rather than a specific method and CAM technology is only a tool to implement this idea. As long as the network itself can effectively work with Textures or Scenes (i.e., can extract meaningful classification patterns), then PART is able to obtain high-contribution pixel regions which means PART will still be useful on these image distributions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376843767,
                "cdate": 1700376843767,
                "tmdate": 1700377143439,
                "mdate": 1700377143439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "16ADp4uXRh",
                "forum": "HU1pesCJF4",
                "replyto": "rYopvOTzoU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder - Discussion Stage 1 closing soon - 21 November"
                    },
                    "comment": {
                        "value": "Dear Reviewer QPEP,\n\nWe appreciate the time and effort that you have dedicated to reviewing our manuscript.  Just a quick reminder that discussion stage 1 is closing soon. \n\nHave our responses addressed your major concerns?\n\nIf there is anything unclear, we will address it further. We look forward to your feedback.\n\nBest,\n\nAuthors of Paper1033"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539920102,
                "cdate": 1700539920102,
                "tmdate": 1700607403149,
                "mdate": 1700607403149,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XCF8RJJmHj",
                "forum": "HU1pesCJF4",
                "replyto": "16ADp4uXRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_QPEP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1033/Reviewer_QPEP"
                ],
                "content": {
                    "title": {
                        "value": "Comments addressed, Increasing my Score"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nMy concerns have been addressed and I've raised my score from 5 to 6. The main point for me that has helped me raise my score is that the term \"perceptual alignment\" has been corrected to \"semantic information\", and also that I think the sentence above makes me interpret better the results:\n\n> **What we want to highlight is that PART can improve the robustness-accuracy trade-off by notably increasing the natural accuracy**\n\nI think this should be highlighted more in the Abstract or Introduction, as I would have not have guessed that was one of the main contributions from reading the paper and looking at the figures on a first glance.\n\nI think this paper would make a good poster at the conference, and I have enjoyed reading the other reviews and rebuttals."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1033/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581109148,
                "cdate": 1700581109148,
                "tmdate": 1700581109148,
                "mdate": 1700581109148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]