[
    {
        "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing"
    },
    {
        "review": {
            "id": "rjvcTxTQ0N",
            "forum": "xnhvVtZtLD",
            "replyto": "xnhvVtZtLD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of local fairness, which ensures the model fairness on the subregion split via unknown attributes.  They proposed an approach based on the idea of distributional robust optimization, under an adversarial learning paradigm. The desired fairness is implemented of instance-wise sample reweights and the experiments are conducted to demonstrate the proposed method can achieve Pareto dominance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The local fairness introduced in this paper is interesting, and the analysis to existing literature is almost thorough and clear. The method looks good, and the writing is well prepared."
                },
                "weaknesses": {
                    "value": "I have some doubts about concepts and formulations after reading the paper. Also, the technical contribution may need further clarifications."
                },
                "questions": {
                    "value": "1.\tThe motivation of this paper is that global fairness methods usually cannot guarantee a consistent fairness value on split subregions, shown as Fig. 1. However, such observations are based on fine-grained partition (e.g., age categories) of training data and each subregion might be with a relatively small size. In this case, the disparity tends to happen. For example, DI might be very large if a bin only includes two samples. How do we confirm that biases do widely exist and then precisely define \u201clocal fairness\u201d?\n2.\tFollowing question 1, I think it might be necessary to use some prior for split subregions. For example, in DRO, they introduced a lower bound of the subgroup ratio. With accuracy as a utility, BPF demonstrated that very small group would lead to uniform classifier. But from sec 3.2, as Softmax weights can be regarded as applying a Shannon entropy regularization. It would be better if the authors can point out such behind insights.\n\n        Blind Pareto Fairness and Subgroup Robustness, ICML 2021.\n\n3.\tStarting from Eq. (2), the input of $g_{w_g}(.)$ directly takes the output of $f_{w_f}(.)$. So, how do you implement $w_g$? Considering a binary case, $f_{w_f}(.)$ is a only probability in output space. \n4.\tFollowing question 3, please check the constraint of eq. (4), where s should be properly positioned.\n5.\tAbove Eq. (4), why distribution $q$ can help characterize local fairness is not clear for me. Please note that in DRO, it serves any subregion\u2019s upper bound.\n6.\tSince $\\lambda_g$ is not learnable (Sec A.5.4), are the best results selected to have a Pareto dominance as claimed?\n7.\tPlease justify the novelty/contribution compared to Michel et al 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6517/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6517/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672425162,
            "cdate": 1698672425162,
            "tmdate": 1700713485798,
            "mdate": 1700713485798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CCnQsjrWYo",
                "forum": "xnhvVtZtLD",
                "replyto": "rjvcTxTQ0N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 1. The motivation of this paper is that global fairness methods usually cannot guarantee a consistent fairness value on split subregions, shown as Fig. 1. However, such observations are based on fine-grained partition (e.g., age categories) of training data and each subregion might be with a relatively small size. In this case, the disparity tends to happen. For example, DI might be very large if a bin only includes two samples. How do we confirm that biases do widely exist and then precisely define \u201clocal fairness\u201d?\n\n\n\nWhile our definition of local fairness is given in Eq.(3) as means of expectations for a specific family of $q$ distributions, we acknowledge that measuring fairness from finite datasets may face difficulties. Given a considered partition of $X$, the presence of small subgroups can easily result in large DI values, and hence, it is essential to define a threshold to properly demonstrate the existence of the local fairness problem. This threshold is to be defined depending on the use case, and represents the minimum group size for a measure of local fairness to be deemed significant.\n\nIn our work, we do use such thresholds when measuring local fairness: we highlight the existence of the problem (Figure 1) by considering a minimum of 50 individuals in each represented age category (Adult and COMPAS). This threshold allowed us to observe significant disparity between sensitive groups, and conclude on the existence of the local unfairness problem. The local fairness results presented in Figure 2 and Figure 5 follow the same methodology: subgroups of minimum 50 individuals for the COMPAS dataset, and 20 for the Law and German datasets (details for all subgroups are given in Appendix A.5.3).\n\nBesides, to make sure that the problem of local fairness we observe is not an artifact of overfitting a threshold, we have conducted multiple experiments to test the robustness of our observations: in Figure 5, Figure 9, and Figure 10, we test the behavior of models trained once when this threshold varies. These experiments show that no matter the value of the threshold chosen (among a reasonable range), local bias problems could be observed. \n\nFinally, we would like to restate that these thresholds are only needed at evaluation time, to measure local fairness. During the training, no threshold is needed, as the locality of the attention is controlled by the temperature $\\tau$ hyperparameter.\n\n\n\n> 2 .Following question 1, I think it might be necessary to use some prior for split subregions. For example, in DRO, they introduced a lower bound of the subgroup ratio. With accuracy as a utility, BPF demonstrated that very small group would lead to uniform classifier. But from sec 3.2, as Softmax weights can be regarded as applying a Shannon entropy regularization. It would be better if the authors can point out such behind insights.\n  Blind Pareto Fairness and Subgroup Robustness, ICML 2021.\n  \nThanks to the reviewer for this very insightful remark. Since considering worst-case distributions, mentionned works [1] and [2] have indeed connections with our work, that we could discuss.\n \nThe DRO approach [1], as [3], assumes P as an unknown mixture of distributions and attempt to minimize the risk of the worst components of that mixture. Using the dual form of the problem, they consider an upperbound of the risk that depends on the marginal of the smallest component of the mixture. To avoid degenerating through trivial solutions that outputs uniform predictions, they indeed set a minimal threshold for the size of that smallest component they could focus on. This threshold is shown to be related to the maximal divergence allowed between their worst-case distribution $q$ and the training one $p$. \n\nPlease note that their setting is different than ours: their goal is to train models that perform uniformly well across all partitions of the population, while ours is to train a model that is uniformly fair (regarding a sentitive attribute) accross all subregions of the feature space, which is quite different. In particular, they usually consider convex losses and contraints to build their models, while we envison more complex architectures designed through neural networks. However, setting a prior on the  least populated group for which we can ensure fairness can be seen equivalent as tuning our hyper-parameter $\\tau$, that controls the Shannon entropy of our divergence ratio $r$ (defined as a maximum entropy distribution using softmax weights). \n\nWe thank again the reviewer for their comments, that led us to include several components of the above discussion in the paper in sections 2.2, 2.3 and 3. \n\n\n[1] John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for latent covariate mixtures. Operations Research, 71(2):649\u2013664, 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182697123,
                "cdate": 1700182697123,
                "tmdate": 1700182697123,
                "mdate": 1700182697123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ila3gOuYTX",
                "forum": "xnhvVtZtLD",
                "replyto": "wZA2CrZjMd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thanks for responding my questions. Your answers are clear and I would like to raise my rate score to 6.  For your explanation \"In our approach, we assign more weight to samples that are easier for the adversary to reconstruct $S$ instead of focusing on less classifier accuracy as in [Michel et al. 2022]. \" I agreed with this point. However, from my view, such reweighing strategy in an adversarial manner for fairness research is quit common according to literature, and I personally would see more exciting methods that could deal with such problems in top AI conferences."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713438024,
                "cdate": 1700713438024,
                "tmdate": 1700713438024,
                "mdate": 1700713438024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ml5Ip7Sum",
            "forum": "xnhvVtZtLD",
            "replyto": "xnhvVtZtLD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a notion of local fairness as opposed to group fairness. When local subgroups are unknown at training time, it is difficult to enforce local fairness constraints. This paper expands the method DRO to include an adversarial learning component that specifically minimizes the ability of an adversary to reconstruct sensitive attributes. The main idea is to boost the importance of regions where sensitive reconstruction is easiest. This is done at each optimization step. To do this efficiently, the paper introduces an adversarial importance re-weighting method. They provide the analytical formulation for this method as well as two implementations (one non-parametric and one parametric). Finally, they experimentally evaluate the proposed method including ablation studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The main contribution is novel and clearly specified. Combining DRO with adversarial learning is simple yet effective.\n\n(2) The paper is written with clarity and exposition is easy to follow throughout. Problem statement as well as the two implementations are concise."
                },
                "weaknesses": {
                    "value": "(1) The paper does not directly address limitations. For instance, is there a computational scalability issue with this technique compared to other more well known group fairness methods?\n\n(2) The reference to applied fairness research could be strengthened. There are many related works that are omitted. In particular, work on fairness and adversarial learning."
                },
                "questions": {
                    "value": "(1) Is there a reason why the first paragraph of the introduction omits citations in statements like \"models...have been shown to unintentionally perpetuate existing biases and discriminations\"? \n\n(2) What do you see as the main use case for this method? What are the limitations you anticipate other than sensitive attribute not being available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685808840,
            "cdate": 1698685808840,
            "tmdate": 1699636732295,
            "mdate": 1699636732295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fIvV4dxVE7",
                "forum": "xnhvVtZtLD",
                "replyto": "7ml5Ip7Sum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> (1) The paper does not directly address limitations. For instance, is there a computational scalability issue with this technique compared to other more well known group fairness methods?\n\nThank you for your comment. In terms of computational scalability, the training process of our method is relatively stable, and the networks do not require many neurons (3 layers with no more than 64 neurons per layer). Our study in the appendix A.4.1, titled \u201cDoes the complexity of the two adversarial networks have an impact on the quality of the results?\u201d shows that for network r, a simpler linear prediction such as logistic regression is quite efficient (please note that due to the exponential parameterization, an exponential activation is applied to it). We believe that our method strikes a balance between complexity and the potential improvements in local fairness it offers. For limitations, see our answer to Question 2 below.\n\n> (2) The reference to applied fairness research could be strengthened. There are many related works that are omitted. In particular, work on fairness and adversarial learning.\n\nWe appreciate the suggestion to strengthen the references to applied fairness research and related works. We have included the following references in the manuscript: \nApplied fairness:\n\n[5] Dissecting racial bias in an algorithm used to manage the health of populations, Obermeyer 2019\n\n[6] Achieving Fairness through Adversarial Learning: An Application to Recidivism Prediction, Wadsworth et al. 2018\n\n[7] A Fair Pricing Model via Adversarial Learning, Grari et al. 2022\n\n\nSome missing related works on adversarial learning for fairness:\n\n[8] Achieving Fairness through Adversarial Learning: An Application to Recidivism Prediction, Wadsworth et al. 2018\n\n[9] Learning to Pivot with Adversarial Networks, Louppe et al. 2017\n\n\n> (1) Is there a reason why the first paragraph of the introduction omits citations in statements like \"models...have been shown to unintentionally perpetuate existing biases and discriminations\"?\n\n\nThank you for your comment. In our revised manuscript, we have included more relevant literature to better situate our work within the broader context of fairness research. These include for instance:\n\n[10] Machine Bias, Angwin et al. 2016 (Propublica Compas study)\n\n[11] Weapons of Math Destruction, Cathy O\u2019Neil, 2016\n\n\n> (2) What do you see as the main use case for this method? What are the limitations you anticipate other than sensitive attribute not being available?\n\n\n*Usecases*\n\nOur contribution generally aims towards a better generalization of fairness for classification tasks. In this context, we believe it should be considered as an alternative in all problems and usecases already tackled by the field of algorithmic fairness: healthcare, credit obtention, etc.\nMore particularly, several works in applied fairness have observed and discussed problems of \u201cfairness generalization\u201d or \u201clocal unfairness\u201d. These usecases would thus represent an especially interesting area to explore. Some of them include: \n\n[12] focus on mitigating bias in a task of harmful tweet detection using several datasets. The sensitive feature is whether the tweet content is aligned with racial dialect (e.g. African American English). However, they observe that despite generally managing to enforce bias, the models showed poor generalization capabilities across datasets. In such situation, ROAD might help ensuring better fairness generalization.\n\n[13] observed that, in a context of car insurance pricing, disparities among groups persist in some geographical regions even after mitigating bias on the policyholder\u2019s residence using adversarial methods.  Our approach can be a powerful tool in addressing such local fairness issues.\n\n[12] Differential Treatment: Mitigating Racial Dialect Bias in Harmful Tweet Detection, Ball-Burack et al. 2021\n\n[13] A Fair Pricing Model via Adversarial Learning [Grari et al. 2022]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182529646,
                "cdate": 1700182529646,
                "tmdate": 1700182529646,
                "mdate": 1700182529646,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SS6UfEUdeT",
                "forum": "xnhvVtZtLD",
                "replyto": "Ei6NenKHRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses"
                    },
                    "comment": {
                        "value": "Thank you for responding and answering all of my questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580239558,
                "cdate": 1700580239558,
                "tmdate": 1700580239558,
                "mdate": 1700580239558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iSpcGelT0M",
            "forum": "xnhvVtZtLD",
            "replyto": "xnhvVtZtLD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
            ],
            "content": {
                "summary": {
                    "value": "Most prior work on group fairness measure the fairness discrepancy using global averages across pre-defined groups (e.g. males vs females).  However, in practice, one may care about fairness discrepancies in sub-regions of the feature space (e.g. males and females belonging to a particular age group). In this paper, the authors enforce fairness across such local sub-regions, by employing a form of distributionally robust optimization, equipped with an adversary that seeks to predict the sensitive attribute from the model predictions. Importantly, the proposed method does *not* assume the sub-regions to be known during training time, and instead optimizes over a set of possible sub-regions defined by an uncertainty set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A practically relevant problem statement with a clearly presented problem formulation\n- The solution approach presented makes for a satisfactory contribution\n- Strong experimental results\n- Well-written paper"
                },
                "weaknesses": {
                    "value": "- The problem formulation seems *overly complex*, with multiple levels of nested minimization problems. It is unclear if the complexity is absolutely necessary (see question 1).\n- Justifications needed for choice of uncertainty set $\\mathcal{Q}$.\n- *Lack of strong convergence guarantees* for the proposed optimization strategy. In fact, owing to the nested nature of the optimization problem, it appears (please correct me if I am incorrect) that it would be hard to provide theoretical guarantees even if under a simplistic scenario when $f$ and $g$ are linear models."
                },
                "questions": {
                    "value": "(1) **Complexity of problem for formulation**: The authors seek to impose a robust version of the demographic parity constraint in (3), by introducing a maximization over an uncertainty set in the constraint. Instead of directly seeking to solve this problem, they formulate a equivalent problem with an additional argmin on an adversary's reconstruction loss (4). *Could authors elaborate why they chose to work with (4) instead of the simpler form in (3)?* It appears that the trick of applying instance-level weighting function $r$ could be applied as well to solving (3).\n\n(2) **Assumptions on $q$**: Could the authors formally state the assumptions on the class of perturbed distributions $q$, preferably early on in the paper (e.g. Sec 2). My understanding is that the group priors are required to be the same for both q and p, i.e. $q(s) = p(s)$. However, its unclear if the validity constraint described in Sec 3.1 would include **all distributions $q$ for with the same group priors as $p$**. It might be cleaner to have the assumptions on $q$ described first, and then argue why the particular choice of weighting functions $r$ satisfy those assumptions.\n\n(3) **Rationale for assuming equal group priors:** Does the assumption $q(s) = p(s), \\forall s$ amount to saying e.g. that the proportion of male and female samples would be the same across all sub-regions (e.g. across all age groups)? If so, is that a reasonable assumption to make? I think the authors need to better explain how their choice of uncertainty set aligns with practical applications where one may want to enforce the form of local fairness they describe.\n\n(4) **Distribution shift in Sec 4.2:** In the experiments with the Adult/census distribution shift in Sec 4.2, do we have reasons to believe that uncertainty set used would capture the \"real-world\" distribution shift considered? I am again particularly curious about the relevance of the assumption $q(s) = p(s)$ here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903352999,
            "cdate": 1698903352999,
            "tmdate": 1699636732173,
            "mdate": 1699636732173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9YvBCMvll6",
                "forum": "xnhvVtZtLD",
                "replyto": "iSpcGelT0M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> (1) Complexity of problem for formulation: The authors seek to impose a robust version of the demographic parity constraint in (3), by introducing a maximization over an uncertainty set in the constraint. Instead of directly seeking to solve this problem, they formulate an equivalent problem with an additional argmin on an adversary's reconstruction loss (4). Could authors elaborate why they chose to work with (4) instead of the simpler form in (3)? It appears that the trick of applying instance-level weighting function could be applied as well to solving (3).\n\nThank you for your question. We chose to work with (4) because it allows us to introduce an adversary\u2019s reconstruction loss and make the problem differentiable, which greatly helps optimization, specifically when working with neural classifiers. Directly solving (3) would be intractable as the function $\\hat{f}$ is non-differentiable after thresholding ($\\hat{f}_{w_f}(x)=\\mathbb{1}_{f_{w_f}(x)>0.5}$), making it unsuitable for gradient-based optimization. Our approach is inspired by the fairness reconstruction in adversarial settings, as demonstrated by Zhang et al. (2018), which also propose to enforce fairness by mitigating the ability of an adversary to reconstruct the sensitive attribute S from the probability f(X). Theoretical justification of this kind of adversarial approach for fairness is given in the second paragraph of Appendix A.2.2. \n\n\n\n> (2) Assumptions on $q$: Could the authors formally state the assumptions on the class of perturbed distributions $q$, preferably early on in the paper (e.g. Sec 2). \n\nIn section 2, we give an overview of the problem, by giving a general formulation (eq 3) and then reviewing some related works that attempt to address fairness with DRO approaches. In section 2.3, we review different choices for the considered uncertainty set ${\\cal Q}$, which has a strong impact on the behavior of the methods in practice. We indeed see from eq (3) that if ${\\cal Q}$ is too large or far from $p_{test}$ (the target distribution for the classifier), the process might lead too overconstrain  the prediction, with the consideration of fairness for irealistic groups, which results in poor accuracy for the target classifier. Thus, for distribution shifts purposes followed by related works, the main requirement for ${\\cal Q}$ is that it contains $p_{test}$, and is tight around it. Assuming that $p_{test}$ is close to $p$, this is done for instance by a KL constraint w.r.t. $p$.   \n\nFor our local fairness objective, we also want that the discrepancies of $q$ w.r.t. $p$ are smooth in the feature space, so that the fairness constraint does not increase mitigation on specific disconnected individuals, but rather on local areas of the space, which would be somehow ignored in classical global fairness approaches. \n\nFollowing the reviewer's recommendation, we added this last sentence in section 2.3 to strengthen the definition of our requirements regarding ${\\cal Q}$. However, from our point of view, discussions about our validity constraints and the implied assumptions can difficultly be discussed here, for the reasons detailed below.  \n\n> My understanding is that the group priors are required to be the same for both $q$ and $p$, i.e. $q(s) = p(s)$.\n\nThe constraint of the initial problem from Eq.(3) compares expectations over prediction outputs for both sensitive groups, without any dependency on a given $q(s)$. $q$ only acts conditonnally on the binary sensitive value $s$ (the max is on posteriors $q(x|s)$).\n\nIn its transposition in an adversarial formulation that is more tractable (eq.5), each sample is considered via an individual loss. It could be tempting to act on $q(s)$ to induce more powerful perturbations, but we show theoretically in Appendix A2.2 that it would induce a shift of the optimum, supported by empirical results in Appendix A2.3, by allocating higher $r$ weights to the most populated group (we added a new figure 6 in section A.2.3), to emphasize that most of the fairness effort is supported by the most populated group in this setting).  In order to avoid over-mitigate one subpopulation of the sensitive compared to the other one, we thus proposed to add an additional constraint to ensure that the marginal $q(s)$ equals the observed prior $p(s)$ through the definition of $r$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182261720,
                "cdate": 1700182261720,
                "tmdate": 1700182261720,
                "mdate": 1700182261720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ToSSnu8eMh",
                "forum": "xnhvVtZtLD",
                "replyto": "iSpcGelT0M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response: follow-up questions"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response and clarifications, and for the revisions to the paper.\n\nI have a few follow-up questions:\n-  **Rationale for assuming equal priors**: Thanks for the clarification.  I am still not entirely convinced about the choice of uncertainty set. If \"s\" is a sensitive attribute that defines gender (for simplicity, male and female), am I correct that you consider perturbations to the data distribution that maintain the same **overall** proportion of male and females? If so, why are we only considering perturbations of this form? Wouldn't it natural to also expect data drifts that change the overall proportion of male and females?\n\n- **Differentiability of (3):** I understand that formulation (4) allows for differentiability of the objective. Couldn't the same be achieved by introducing a smooth surrogate objective for the non-differentiable indicators? Much of the initial works on group fairness employ some form of surrogate approximations to handle non-differentiable constraints (e.g. Zafar et al. (2015), Cotter et al. (2019)).\n\nP.S. Regarding my earlier point about the condition q(s) = p(s) imposing a constraint of equal priors across all sub-regions, let me be a bit more clear. When you compute the worst-case loss over all distributions q for which q(s) = p(s), you also include within this set distributions q that have *uniform density over samples in a particular age group*, and zero density on other samples. Such distributions define a sub-region of your data, but also additionally satisfy the condition q(s) = p(s) within the sub-region."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678349178,
                "cdate": 1700678349178,
                "tmdate": 1700683144876,
                "mdate": 1700683144876,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rusTFuZqEn",
            "forum": "xnhvVtZtLD",
            "replyto": "xnhvVtZtLD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_d8Gj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6517/Reviewer_d8Gj"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a practical issue in methods that ensure group fairness in machine learning models. The methods that aim for group fairness can often be unfair to a subpopulation. To this end, authors combine adversarial learning and distributionally robust optimization (DRO) to learn globally fair classification models. DRO reweighs the sample, assigning more weights to samples that are easier to adversary. This ensures that fairness constraints would hold on even worst-case subgroups. Notably, the proposed method does not use subgroup labels. They propose two methods for reweighing the samples --- One exploits the loss of adversary to compute the weights (BROAD), and the other uses a separate neural network (which is optimized alongside during training) to compute the weights (called ROAD). \n\nThe authors performed experiments on three fairness datasets. They consider worst-case demographic parity on subgroups as a fairness constraint in one scenario. In the other scenario, they learn the fair model under distribution shift and use equalized odds as the fairness measure. In both cases, they show that ROAD provides a better trade-off between accuracy and fairness than other methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses an important practical issue in group fair learning approaches and is a step towards ensuring the reliability of fair learning methods. \n- ROAD is consistently better than the other approaches for different levels of fairness and all datasets. In particular, I like using trade-off curves to report the results. \n- The setting does not require specifying subgroups during training and can be fair w.r.t. to any choice of subgroup due to worst-case distribution consideration. To this end, combining DRO with adversarial learning is interesting."
                },
                "weaknesses": {
                    "value": "- **Baseline**: In fig 3 and 4, some of the baselines do not span all ranges of fairness. Why do we see this behavior? Are the baselines tuned correctly?\n- The proposed approach can be tricky to implement with several additional hyperparameters and the adversarial nature (min-max optimization) of the problem. \n- **Results**: The experiments and reported results could be more elaborate:\n  - How does DRO affect global fairness compared to other baselines? Fig 3 only reports local fairness in the form of the worst DI. However, the global fairness results are not reported.  \n  - While extending this framework to EO measure is straightforward, results with EO and local fairness are not reported. Similarly, DI and distribution shift results would strengthen confidence in the method."
                },
                "questions": {
                    "value": "- **ROAD vs BROAD**: It is quite unintuitive that BROAD, an exact solution, performs poorly than ROAD, which uses a neural network to predict weights. The authors argued that ROAD is better because of the smoothing effects. Could it be that what is easy and hard for adversaries may change rapidly (or adversaries' output may be noisy) due to noisy batch gradients, and using an NN in ROAD may just be smoothing that out? In that case, would it make sense to use an EMA-like estimate with BROAD?\n\n- Fig 4 (Left Fig): Are these unnormalized values? Isn't weight, i.e., $r$, positive and less than 1?\n- **Adapting the method for EO**: \n  - **Parameterization of weight network:** If we use BROAD for EO, it will use the adversary's loss, which uses labels $y_i$. Thus $r_i$ is a function of $x_i, s_i$ and $y_i$. Should the weight network (in ROAD) also use $y_i$ to predict r$?\n  - **Validity constraint:** Would the validity constraint change for EO? That is, would it require normalization over both $s_i$ and $y_i$ or only $s_i$ as explained in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699082192308,
            "cdate": 1699082192308,
            "tmdate": 1699636732050,
            "mdate": 1699636732050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wSjMBkCADe",
                "forum": "xnhvVtZtLD",
                "replyto": "rusTFuZqEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments. We address the various questions/concerns below. \n\n>  Baseline: In fig 3 and 4, some of the baselines do not span all ranges of fairness. Why do we see this behavior? Are the baselines tuned correctly?\n\nWe assume that by \u201cfig 3 and 4\u201d, the reviewer instead meant fig 2 and 3. \nIn these figures, some curves indeed do not span all ranges of fairness. This is due to the fact that some methods do not offer sufficient control, depsite an extensive sweeping of their hyper-parameter domains via greed search,  that would enable to reach points in some areas of the support. For fig 2, this is accentuated by a filter that removes points that do not respect a minimal threshold of global fairness (as we are interested in inspecting local fairness for globally fair models). For fig 3, this is emphasized by distribution drifts: while for in-distribution test set (in the leftmost curves), every method allows a wide span of the global fairness support, results for other plots are impacted by drifts that make pareto fronts reduced on few points dominating every others for some methods. \n\n\n> How does DRO affect global fairness compared to other baselines? Fig 3 only reports local fairness in the form of the worst DI. However, the global fairness results are not reported.\n\nIn fact, that figure reports results of local fairness for a high requirement for global fairness (global DI < 0.05). We see that our approach achieves this global fairness level (while some approaches such as fairLR on COMPAS have no point in the plot since never satisfying this condition), with competitive levels of accuracy for any local fairness score.  \n\n> While extending this framework to EO measure is straightforward, results with EO and local fairness are not reported. Similarly, DI and distribution shift results would strengthen confidence in the method.\n\n\nYou are right. We launched experiments to complement this. We have added a new section in appendix \"A.6 Additional results on the Local Equalized-Odds criterion\" (page 20). You may find these results in Figure 12 in the current version of the paper. The local unfairness is here measured using worst Disparate Mistreatment Rate (worst-1-DMR), that we defined by adding worst-1-FPR and worst-1-FNR.\n\n\n>  ROAD vs BROAD: It is quite unintuitive that BROAD, an exact solution, performs poorly than ROAD, which uses a neural network to predict weights. The authors argued that ROAD is better because of the smoothing effects. Could it be that what is easy and hard for adversaries may change rapidly (or adversaries' output may be noisy) due to noisy batch gradients, and using an NN in ROAD may just be smoothing that out? In that case, would it make sense to use an EMA-like estimate with BROAD?\n\nThank you for this insightful question regarding the difference in performance between ROAD and BROAD. \n\nThe problems that may hinder BROAD performance go beyond smoothing. In the DRO literature, it is known, and deeply discussed (see [1, 2, 3, 4] for instance), that the non-parametric solution (such as BROAD) may lead to suboptimal performance compared to parametric ones. This may come from several causes, such as the fact it is essentially solving a problem that is fundamentally too pessimistic: i.e., the uncertainty set $\\mathcal{Q}$ may include distributions that are too difficult to learn, and not necessarily representative of real-world constraints. Thanks for the suggestion of using an EMA-like estimate with BROAD, which could alleviate learning unstability issues, but might still be limited by the use of individual weights: in BROAD, weights of training samples are only interlinked via the outputs from the classifier, hence at the risk of conflicting with our notion of local fairness (which is defined on the feature space). \n\nOn the other hand, the parametric, neural network-based, approach (such as ROAD), \u201chas much less (only parametric) freedom to shift the test distribution compared to the adversary that uses non-parametric weight\u201d (sic) [1]. The lipschitzness of neural networks can add additional implicit locality smoothness assumptions, thus helping define distributions $q$ as subregions of the feature space. In classical DRO, this leads to models that are robust to more meaningful distribution shift. In our setting of DRO for local fairness, this also helps focusing on local fairness with groups formed with similar individuals. As discussed in our paper, the network architecture defines the level of local smoothness of considered groups. And a network of infinite capacity that completes training would have, in theory, the same behavior as BROAD. \n\n[1] Hu, Weihua, et al. \u201cDoes distributionally robust supervised learning give robust classifiers?.\u201d International Conference on Machine Learning. PMLR, 2018.\n[2] Duchi et al. 2020 Distributionally Robust Losses Against Mixture Covariate Shifts. ROAD is a heuristic approximation to the objective"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182064203,
                "cdate": 1700182064203,
                "tmdate": 1700182064203,
                "mdate": 1700182064203,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kKQaKFVcbD",
                "forum": "xnhvVtZtLD",
                "replyto": "rusTFuZqEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "[2] Duchi et al. 2020 Distributionally Robust Losses Against Mixture Covariate Shifts. ROAD is a heuristic approximation to the objective\n\n[3] Michel et al. 2021 Modeling the Second Player in Distributionally Robust Optimization\n\n[4] Michel et al. 2022 Distributionally Robust Models with Parametric Likelihood Ratios\n\n\n>  Fig 4 (Left Fig): Are these unnormalized values? Isn't weight, i.e., $r$  positive and less than 1?\n\nNormalization is on the mean, not the sum: we impose $\\mathbb{E}_p[r(x,s)]=1$ to ensure that $q$ is a distribution that respects $\\int q(x,s)=1$. As we implement this constraint with an unbiased estimator based on training samples (i.e., $\\frac{1}{n} \\sum r(x_i,s_i)=1$), we get that $r$ values sum to $n$, not $1$. This can be seen for instance in our implementation of BROAD where the partition function is divided by $n$. \n\n>  Parameterization of weight network: If we use BROAD for EO, it will use the adversary's loss, which uses labels $y_i$. Thus $r_i$ is a function of $x_i$, $s_i$ and $w_i$. Should the weight network (in ROAD) also use $y_i$  to predict $r_i$?\n\nYes, we agree with the reviewer and confirm that in order to optimize for EO, $r$ needs to be a function of $x$, $s$ and $y$; and that the weight network uses $y$ to predict $r$. We have provided more details about this in the appendix of the paper (please see A.10: \"ADAPTING ROAD TO EQUALIZED ODDS\").\n\n>  EO, validity constraint: Would the validity constraint change for EO? That is, wuld it require normalization over both $s_i$ and $y_i$, or only $s_i$ as explained in the paper?\n\nThank you for your question. Following similar reasoning as for our validity constraint in the case of DP (section A2.2), we have indeed also considered adapting the validity constraint for EO by conditioning it on $y$. This effectively means imposing the following constraints:\n$\\forall s_i, \\forall y_i, \\mathbb{E}_{x | s=s_i, y=y_i}r(x,s,y)=1$\nHowever, depending on the dataset, conditioning the batches on both $s$ and $y$ may lead to very small subsamples,  i.e. non-significant expectation estimates. As a result, the obtained models were sometimes less robust, and this lead us to not pursuing this direction any further. We have added a discussion on this in Section A.7 with an experiment comparing the two constraints (Figure 13)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182106670,
                "cdate": 1700182106670,
                "tmdate": 1700183298110,
                "mdate": 1700183298110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]