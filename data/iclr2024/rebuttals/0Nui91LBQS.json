[
    {
        "title": "Planting a SEED of Vision in Large Language Model"
    },
    {
        "review": {
            "id": "wCe12Ato6W",
            "forum": "0Nui91LBQS",
            "replyto": "0Nui91LBQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method (SEED) to augment large language models (LLMs) with the capability of processing and generating visual data, i.e., images. The core contribution of SEED is a quantized tokenizer that learns to encode images into discrete visual tokens which can be again decoded using a pre-trained generative model. Once the tokenizer is trained, a LLM is trained and fine-tuned on interleaved image-text data such that the LLM can both process and generate the visual tokens which make it applicable to a variety of vision language tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- SEED is well motivated in learning a 1D token representation that better aligns with the auto-regressive generative process of LLMs.\n- Architectural choices are reasonable and have been validated with ablation studies to the most extend, i.e., text vs visual embedding reconstruction, embedding vs. discrete code, causal vs. bilateral visual codes, full fine-tuning after LoRA, instruction fine-tuning.\n- The quantitative evaluation convinces in either surpassing or being competitive in image-text, video-text tasks as well as generative image-to-image and text-to-image tasks.\n- The qualitative results showcase some interesting multi-modal capabilities of SEED, including compositional and in-context image generation.\n- Publishing both code and checkpoints of large-scale models enables future research and empowers the open-source community."
                },
                "weaknesses": {
                    "value": "- Some architectural design choices are missing an explanation.\n    - In general, it would help the clarity of the paper if all loss functions would be written out.\n    - What is the reason behind using two different image encoders, one for encoding the input to the causal q-former (BLIP-2 ViT) and one for the image generation conditioning (unCLIP-SD vision encoder)? This requires loading more weights into memory during training so an explanation is needed. Can we use the unCLIP-SD vision encoder for both cases?\n    - How important is the contrastive loss between the vision and text embeddings? An ablation could justify it's inclusion.\n    - Why was the original VQ codebook loss replaced by a simple reconstruction loss with cosine similarity? How are collapsing codebooks avoided? Are there any stop-gradient operation in the loss for the codes?\n- The arguments and ablation for using causal vs. bilateral visual codes is not convincing. In general, previous work on VQ models have demonstrated that transformers can learn complex and even low-level dependencies of non-causal codes. Enforcing a causal mask in the q-former restricts the information flow to fully utilize the tokens efficiently and effectively. The argument made in Sec. 4.3 is that the LLM struggles with generating the correct sequence length for images which should always be 32. It is surprising that this happens because the start token for images (as in Fig. 4) should be a clear signal to the LLM that 32 image tokens follow. In practice however, it would be straightforward to enforce the generation of 32 image tokens after the start token is observed by restricting the possible output tokens. How do these two models compare when the number of image tokens is enforced to be always 32?\n- It is not clear how videos are being processed. Are individual frames used to train the tokenizer or are multiple frames passed to the causal q-former? If multiple frames are passed, how do you adjust the reconstruction loss for the generative embedding (1 embedding per frame from unCLIP-SD vs. one embedding per video from your tokenizer)? Do you simply append the encoding of multiple frames when passing videos to the LLM?"
                },
                "questions": {
                    "value": "- How did you decide on using 32 tokens per image and 8192 codes? \n- Can you confirm that you are using a start and end token for images as shown in Fig. 4? Do you use the same start/end tokens for both images and videos? This information should be included in the paper.\n\nSuggestions:\n- It would help the read flow to already mention the codebook size in section 3.1.2 instead of only in 3.2.1 (i.e., 8192 visual codes).\n- When Table 2 is first discussed in Sec. 4.1, it is unclear what $\\text{SEED}^{\\text{text}}$ refers to. A short description and reference to the ablation in Sec. 4.3. would better facilitate immediate understanding for the reader. Similarly for the \"I\" suffix referencing the instruction-tuned model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705889770,
            "cdate": 1698705889770,
            "tmdate": 1699636082825,
            "mdate": 1699636082825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q474W1fqKn",
                "forum": "0Nui91LBQS",
                "replyto": "wCe12Ato6W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1-1: Write loss functions.**\n\n**Response:** Thank you for the advice. We have revised the method section and included all loss functions in our revised manuscript.\n\n**W1-2: Why use two different image encoders.**\n\n**Response:** We use two different image encoders in order to ease training by leveraging the pre-trained knowledge of BLIP-2 and unCLIP-SD. Specifically, we initialize the Causal Q-Former from the pre-trained BLIP-2 Q-Former that is compatible with BLIP-2 ViT, so we use the ViT encoder from BLIP-2 for image encoding. And we would like to decode images using the off-the-shelf SD (frozen), so we adopt the image encoder (disposable after training) from unCLIP-SD to provide the learning target for our generation embeddings. After training, the generation embeddings can be used in place of the original conditional embedding of SD-UNet. \n\nSince the image encoder of unCLIP-SD is frozen during training, only the parameters need to be loaded onto GPU, occupying 2944MiB memory (12.7% of total memory 23260MiB for batch size 512).\n\nIt is theoretically feasible to use only the unCLIP-SD encoder - Causal Q-Former requires random initialization. We will leave it for future work.\n\n\n**W1-3: The importance of contrastive loss.**\n\n**Response:** It is critical since the Causal Q-Former is trained using only the contrastive loss. Specifically, the training of visual tokenizer consists of two stages: 1) Causal Q-Former, and 2) VQ Codebook and MLP. \n\nWe have attempted to employ a single-stage approach to optimize all components but the codebook was hard to be optimized and eventually collapsed. This may be due to the continuous evolution of the reconstruction targets of discrete codes (i.e., the features of the Causal Q-Former).\n\nTo conclude, for stable training, the Causal Q-Former needs to be optimized individually whereas contrastive learning acts as the sole objective and thus cannot be removed.\n\n\n\n**W1-4: The training objectives of VQ.**\n\n**Response:** Sorry for missing details of training VQ codebook. We adopt (1) the original VQ codebook loss, (2) a reconstruction loss between reconstructed causal embeddings and causal embeddings of Causal Q-Former, (3) a reconstruction loss between generation embedding and image embedding of unCLIP-SD to optimize VQ codebook. To avoid codebook collapse, in addition to the two-stage training method mentioned above, we follow BEiT v2[1] to reduce the dimension of codebook embeddings to 32D. We use the stop-gradient operation in the original VQ codebook loss. We have revised the method section with detailed introduction.\n    \n> [1] BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\n\n\n**W2: The performance of bilateral visual codes when the number of tokens is enforced to 32.**\n\n**Response:** Such an operation is applicable only when the model predicts equal to or more than 32 tokens. We have attempted to truncate the visual tokens in such cases, however, there are still 2 failure cases given 5000 captions in COCO test set. The results of two models when the number of image tokens is enforced to be always 32 are listed as below. We can observe that the model with causal visual codes achieve higher similarity between generated images and groundtruth images (i2i), and between generated images and captions (i2t) than the model with bilateral visual codes.\n\n\n| | CLIP score (i2i) | CLIP score (i2t) |\n| -------- | -------- | -------- |\n|  Bilateral    | 70.08     | 23.06     |\n|  Causal    | 70.30     | 23.57     |\n\n\n    \n**W3: How to process videos.**\n\n**Response:** We train the SEED tokenizer with only image-text pairs (i.e., the Causal Q-Former and generation embeddings are only trained on image data), and only use video-text data for training SEED-LLaMA.\n\nWhen we train SEED-LLaMA on videos, we sample four frames from each video. Each frame is discretized into 32 codes. The input sequence of a video is `<IMG> the token of frame 1 </IMG> <IMG> the token of frame 2 </IMG> <IMG> the token of frame 3 </IMG> <IMG> the token of frame 4 </IMG>`.\n\n    \n\n**Q1: Why 32 tokens and 8192 codes?**\n\n**Response:** The numbers are selected based on the practice in existing works, i.e., 32 queries in BLIP-2[1] and a visual vocabulary of 8192 codes in BEIT V2[2].\n\n> [1] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\n> [2] BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers\n\n\n**Q2: Start and end token for images and videos.**\n\n**Response:** We use a start and end token for each image as shown in Fig.4. The same start and end token is used for both images and video frames (mentioned in the response to **W3**). We have included this information in section 3.2.1.\n\n**Q3: Suggestions.**\n\n**Response:** Thank you a lot for the useful suggestions. We have mentioned the codebook size in section 3.1.2, and added corresponding description and reference in the revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300721705,
                "cdate": 1700300721705,
                "tmdate": 1700300721705,
                "mdate": 1700300721705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MVlneUbf3z",
                "forum": "0Nui91LBQS",
                "replyto": "q474W1fqKn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed reply and the updates to the paper.\nBelow I have some follow-up comments.\n\n> **W1-1: Write loss functions.**  \n\nThis makes the manuscript much clearer.\n\n> **W1-3: The importance of contrastive loss.**  \n> Response: It is critical since the Causal Q-Former is trained using only the contrastive loss. Specifically, the training of visual tokenizer consists of two stages: 1) Causal Q-Former, and 2) VQ Codebook and MLP.  \n> We have attempted to employ a single-stage approach to optimize all components but the codebook was hard to be optimized and eventually collapsed. This may be due to the continuous evolution of the reconstruction targets of discrete codes (i.e., the features of the Causal Q-Former).  \n> To conclude, for stable training, the Causal Q-Former needs to be optimized individually whereas contrastive learning acts as the sole objective and thus cannot be removed.  \n\nSince the Causal Q-Former is trained in isolation with the contrastive loss, it is a bit surprising that the loss is only applied on the last token (if I understand correctly). In theory, the Causal Q-Former could learn to ignore the first 31 tokens and embed all information from the image into the last token to optimize the loss.\n\nBLIP-2 informed the choice of using 32 tokens, but BLIP-2 also use all tokens in their loss functions. This choice could have been justified a bit better, e.g., ablate number of tokens with 1 as the extreme case, or use the same contrastive loss as BLIP-2 which takes all pairwise similarities and then select the highest one.\n\n> **W2: The performance of bilateral visual codes when the number of tokens is enforced to 32.**  \n> Response: Such an operation is applicable only when the model predicts equal to or more than 32 tokens.\n\nI disagree. One could simply sample 32 tokens from the subset of image tokens once the start token is observed, i.e., take the softmax only over all image tokens and then sample, repeating this procedure exactly 32 times. This ensures that the next 32 tokens are only image tokens and the \"image end token\" could even be automatically appended as well. Therefore, it should come down to the actual performance difference instead of structural errors and failure cases in the sequence.\n\n> We can observe that the model with causal visual codes achieve higher similarity between generated images and groundtruth images (i2i), and between generated images and captions (i2t) than the model with bilateral visual codes.\n\nIt is good to see that it makes a difference, although I would argue it is a marginal one. It is also not clear which setting (e.g. dataset) is evaluated here. I still believe that the quite small benefit of creating causal tokens does not justify the focus it is given throughout the paper. It would be important to include these results in the paper, ideally in Sec. 4.3.\n\n> When we train SEED-LLaMA on videos, we sample four frames from each video. Each frame is discretized into 32 codes. The input sequence of a video is ```<IMG> the token of frame 1 </IMG> <IMG> the token of frame 2 </IMG> <IMG> the token of frame 3 </IMG> <IMG> the token of frame 4 </IMG>```.\n\nThe details about how the prompt looks like for video data should be included in the supplementary."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491122086,
                "cdate": 1700491122086,
                "tmdate": 1700491122086,
                "mdate": 1700491122086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qrfKM5LJr2",
                "forum": "0Nui91LBQS",
                "replyto": "jPV0QSLehH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for running the additional experiments in the short time and providing extensive explanations.\n\nBoth the individual token evaluations and the causal vs biliteral comparisons are very interesting and highly relevant.\nThey provide good support for your design choices and clarify my concerns.\n\nI think the ablation of individual tokens with text-image retrieval on the COCO is interesting enough that it warrants inclusion in the appendix.\n\nRegarding the updated causal vs biliteral paragraph, I just have some minor suggestions:\n- mention the table in the text\n- add the experimental setting for the generation column for instance like this: Generation **(i2i)**\n\n\nOverall, the rebuttal has improved the paper and strengthened my evaluation of acceptance. I think my initial score was already fair in this regard, and I will advocate for this decision in the reviewer discussion."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728920242,
                "cdate": 1700728920242,
                "tmdate": 1700728920242,
                "mdate": 1700728920242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OFcjdHZdiE",
            "forum": "0Nui91LBQS",
            "replyto": "0Nui91LBQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces an image tokenizer, which is capable of discretizing images into a series of tokens. These image tokens are transformed by Q-Former into pseudo-causal tokens that can serve as the input for Large Language Models (LLMs), most importantly, they can also act as the target. This allows the model to unify both visual understanding and generation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is the first (at least to my knowledge) to use an image tokenizer to unify visual understanding and generation tasks in LLM, providing a feasible pipeline."
                },
                "weaknesses": {
                    "value": "1. The authors claim that the use of q-former can establish a causal dependency, which I find questionable. This is because the attention in the visual encoder stage is bidirectional, leading to potential information leakage.\n\n2. Regarding the visual understanding task results shown in Table 3, why does SEED-LLaMA-I (14B) perform no better (or nearly the same) as SEED-LLaMA-I (8B) on some Image-Text Tasks? Does the proposed method not yield much gain on larger models, or has it already reached saturation?\n\n3. In Table 2, SEED-LLaMA-I achieves good results. However, I believe that CLIP similarity does not effectively reflect the quality of generation. Fr\u00e9chet Inception Distance (FID) is a widely accepted better evaluation method, but unfortunately, this paper does not provide it.\n\n4. Regarding Section 4.3 (Causal Visual Codes vs. Bilateral Visual Codes), the authors mention that some mode collapse may occur for generation tasks, but what about understanding tasks?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1543/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762803796,
            "cdate": 1698762803796,
            "tmdate": 1700661306542,
            "mdate": 1700661306542,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iaxbU32kzX",
                "forum": "0Nui91LBQS",
                "replyto": "OFcjdHZdiE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: The information leakage by bidirectional attention in ViT.**\n\n**Response:** The bidirectional attention in ViT is among 2D patches while the unidirectional causal attention in Causal Q-Former is among 1D queries. It signifies that the information of the current query is derived based on the context provided by previous queries. It is worth noting that the causal queries capture **sparse-to-dense** semantics from the **entire image** rather than only a part of an image, as demonstrated in Fig. 9 of Appendix. So it is ok for each query to see all ViT features via cross-attention. Also, according to the information flow (image$\\rightarrow$frozen ViT, w/ init queries$\\rightarrow$Q-Former), the semantics of $k$-th query would not be leaked to $\\{k-1\\}$-th query via ViT.\n\n\n**W2: SEED-LLaMA-I (14B) does not surpass SEED-LLaMA-I (8B) on some Image-Text tasks**.\n\n**Response:** These traditional VQA benchmarks (VQAv2, OKVQA, VizWiz) fail to properly reflect the model's multimodal capabilities since they require an exact match between the model prediction and the ground truth, as pointed out by recent works[2,3,4]. However, empowered by LLM, the model predictions that have been penalized may even be better than the ground truth with a single word or phrase. \n\nWe can observe that SEED-LLaMA-I (14B) outperforms SEED-LLaMA-I (8B) on SEED-Bench, which is specifically designed for evaluating MLLMs with open-form output. Such results have indicated the scalability of SEED-LLaMA.\n\nThe same phenomenon is also reported in IDEFICS (TODO CITE), where  IDEFICS-I (80B) achieves worse performance than IDEFICS-I (9B) on VQAv2, OKVQA, VizWiz but much better on SEED-Bench. Check out the results below.\n\n\n|  | VQAv2 | OKVQA | VizWiz | SEED-Bench|\n| -------- | -------- | -------- | -------- | -------- |\n|SEED-LLaMA-I (8B)    | 66.2     | 45.9    | 55.1     | 51.5     |\n| SEED-LLaMA-I (14B)    | 63.4     | 43.2      | 49.4     | 53.7   |\n| IDEFICS-I (9B)    | 65.8    | 46.1     | 41.2    | 41.2    |\n| IDEFICS-I (80B)     | 37.4     | 36.9    | 26.2     | 53.2    |\n\n\n> [1] Obelics: An open web-scale filtered dataset of interleaved image-text documents\n> [2] SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension\n> [3] MMBench: Is Your Multi-modal Model an All-around Player?\n> [4] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n\n\n**W3: Need FID for generation evaluation.**\n\n**Response:** We would like to clarify that FID is mostly adopted to measure the **quality** of generated images by comparing the distribution between a set of generated images and a set of real images. However, to properly evaluate our SEED tokenizer, we focus more on the **semantical consistency** between the original input image and the reconstructed image. FID fails to meet the requirements, and CLIP similarity is more suitable in such a case, which is also adopted by GILL[1]. Furthermore, SEED aims to better tokenize images and uses the off-the-shelf SD-UNet decoder. Quality is generally determined by the decoder and is not specifically optimized in our work.\nWe further evaluate the text-to-image generation on MS-COCO with FID as the evluation metric as below. We can observe that our model achieves competitve performance.\n\n|  | FID | \n| -------- | -------- |\n| GILL    | 12.20    | \n| Emu    | 11.66     | \n| SEED-LLaMA     | 12.33     | \n\n\n> [1] Generating Images with Multimodal Language Models\n\n\n**W4: \"Causal Visual Codes vs. Bilateral Visual Codes\" in understanding tasks.**\n\n**Response:** We evaluate the performance of zero-shot visual question answering tasks on VQAv2, OKVQA and GQA (accuracy as the metric), and image captioning on COCO (CIDEr as the metric) as below,\n\n|  | VQAv2 | OKVQA |GQA | COCO\n| -------- | -------- | -------- | -------- |-------- |\n| Bilateral    | 23.52    | 15.07    |18.34     | 118.8    | \n| Causal    | 39.40    | 21.54    |27.22    | 120.1 |\n\nWe can observe that the model with bilateral visual codes significantly drops the performance in understanding tasks. When only considering understanding tasks (text as the only output) for training, bilateral visual codes have a better theoretical performance due to the rich semantics. However, in SEED-LLaMA, where the understanding (text as output) and generation (image as output) tasks are unified, the autoregressive Transformer fails to be properly optimized when using bilateral codes as it struggles with next-visual-token prediction for generation tasks. As a result, it achieves inferior performance on understanding tasks as well."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300693532,
                "cdate": 1700300693532,
                "tmdate": 1700658966185,
                "mdate": 1700658966185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pfrzrQG3w6",
                "forum": "0Nui91LBQS",
                "replyto": "iaxbU32kzX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 efforts in rebuttal.  The response solves my concerns, and as a result, I have decided to increase my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661272917,
                "cdate": 1700661272917,
                "tmdate": 1700661272917,
                "mdate": 1700661272917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SCfwSnUuRp",
            "forum": "0Nui91LBQS",
            "replyto": "0Nui91LBQS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_y9Sh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1543/Reviewer_y9Sh"
            ],
            "content": {
                "summary": {
                    "value": "Making the language model to see the words is one of the key research direction. This paper present new image tokenization on this direction. Specifically, unlike prior attempts that uses simple 2d style image tokenization (usually VQ-VAE), this paper propose SEED, which makes image embedding to be left-to-right 1d tokenization similar to the text while keeping semantic meaning of images but discarding low-level information. This paper claim that capturing too low-level information hider the performance of LLMs to effectively perform multimodal comprehension."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Unifying vision and text representation is one of the hot research topic. \n(2) The assumption behind the proposal is reasonable. \n(3) The paper is generally well-written."
                },
                "weaknesses": {
                    "value": "(1) The methodology is not religiously explained and is not self-contained. Especially, section 3.1 is hard to follow. There are no equation, and it is hard to track which components are trained on which objective function. \n\n(2) In section 4.1, they compared SEED tokenization on image-text retrieval. As described in the paper, SEED generally outperform BLIP-2, in some case BLIP-2 exceed the proposed method. However, there are no explanation on this point. Similar criticism can be applied for the analysis on Table 3. \n\n(3) Regarding the Figure 7, I'm afraid the actual prompt is hard to imagine. \n\n(4) The proposed method contains several components, including image encoder, codebook, text encoder, and generation module. However, the importance of all components is less discussed, making me hard to access the importance of the specific choice of each component. \n\n(5) I'm afraid that I found a statement in the introduction is not fully validated. \"Moreover, we empirically found that the dominant tokenizer VQ-VAE (Van Den Oord et al., 2017) in existing works captures too low-level information for LLMs to effectively perform multimodal comprehension task\". Could you please clarify again which results support the above statement? \n\n(6) While the paper motivate to learn good 1D representation is key to incorporate visual information into pre-trained LLMs, but less discuss on why we should make the representation discrete rather than continuous. Table1 also seems to show that the continuous representation is generally better than discrete representation."
                },
                "questions": {
                    "value": "See weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699282327755,
            "cdate": 1699282327755,
            "tmdate": 1699636082659,
            "mdate": 1699636082659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DFvlFHxTpJ",
                "forum": "0Nui91LBQS",
                "replyto": "SCfwSnUuRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1543/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1: Not religiously explained and self-contained. Sec. 3.1 is hard to follow. No equation.**\n\n**Response:** Thank you for the suggestion. We designed the SEED tokenizer with two premises: (1) image tokens should capture high-level semantics, and (2) image tokens should be produced under 1D causal dependence. The architecture of SEED meets the above requirements with the sub-modules of a pre-trained CLIP-ViT encoder, a Causal Q-former, a VQ codebook, and a De-Tokenizer which is composed of an MLP and the pre-trained SD-UNet. \n\nIntuitively, the pre-trained **CLIP-ViT encoder** captures **high-level semantics**, the **causal Q-former** produces **1D causal embeddings**, which are subsequently discretized into **1D causal IDs** using the VQ codebook. The produced visual IDs enable LLM to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. The **de-tokenizer** is used to **reconstruct images**, employed in case the follow-up multimodal LLM (i.e., SEED-LLaMA) predicts image token IDs and needs to render them into pixels further. Note that, to pursue efficient training, we directly adopt the pre-trained and frozen **SD-UNet decoder**, and train a lightweight **MLP** to transform the visual codes to the conditional embedding space of UNet.\n\nDuring SEED training, only Causal Q-Former, VQ Codebook, and the MLP are tunable. There are two training stages. First, we optimize the causal Q-Former by image-text contrastive loss in the form of InfoNCE. Then, we jointly train the VQ codebook and the MLP. Besides the original VQ objectives, we adopt dual reconstruction, i.e., the reconstruction of continuous causal embeddings (to avoid code collapse) and the reconstruction of conditional generation embeddings (to be compatible with the off-the-shelf SD-UNet). The **auxiliary text encoder and image encoder (disposable after training)** are derived from BLIP-2 and unCLIP-SD respectively, providing learning targets. \n\nWe have carefully revised Sec. 3.1 in blue, adding equations and objective functions for each learnable component (Enq.1 for Causal Q-former, Enq.2 for VQ codebook, and Enq.3 for Multi-layer perceptron). Please find more details in our revised manuscript. \n\n**W2: Lack of analysis of Table 1 and Table 3.**\n\n**Response:** Thank you for the suggestion. \n\n(1) *Regarding Table 1:* We would like to first clarify that our goal is not to surpass BLIP-2, since it is a comprehension-only model whereas our SEED is tailored for unifying comprehension and generation in one framework. It is worth noting that to achieve the goal of unification, we would inevitably sacrifice the performance of comprehension to achieve autoregressive multimodal generation. Specifically, the 1D causal dependence (unidirectional attention) and discrete tokens are both designed to accommodate generation but are harmful to discriminative image representation learning. Here we took BLIP-2 as a baseline for comparison since we adopted its pre-trained ViT and Qformer as initialization (mentioned in Sec. 3). As expected, our causal embeddings (before VQ) achieve better results in terms of image-to-text retrieval due to more training data, while falling short in text-to-image retrieval that relies on more discriminative image features. Generally speaking, with our elaborate training, the Causal Qformer achieves competitive results even with unidirectional attention. \n\n(2) *Regarding Table 3:* The table benchmarks the ability of multimodal comprehension. Although we achieve multimodal generation at the expense of discriminative representations (discussed above), we still obtain competitive comprehension results compared to SOTAs and much better performance than the ones that also unify comprehension and generation (such as CM3Leon and Emu). SEED-LLaMA generally complies with the scaling law, i.e., more parameters lead to better results as shown in SEED-Bench (a benchmark tailored for MLLMs). And we found that the traditional benchmarks (e.g., VQAv2) are no longer sufficient to properly evaluate the MLLMs due to even better predictions than the ground-truth.\n\nMore explanations can be found in our revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300547415,
                "cdate": 1700300547415,
                "tmdate": 1700300760911,
                "mdate": 1700300760911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]