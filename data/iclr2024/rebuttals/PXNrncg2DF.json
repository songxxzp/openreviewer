[
    {
        "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation"
    },
    {
        "review": {
            "id": "WyB0vjpqB0",
            "forum": "PXNrncg2DF",
            "replyto": "PXNrncg2DF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_Rono"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_Rono"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SOHES, a method designed to eliminate the need for manual annotation and facilitate the learning of hierarchical associations for open-world entity segmentation. Specifically, the authors have devised a novel approach for generating initial pseudo-labels and subsequently enhancing the segmentation model through their utilization. When compared to previous research on open-world entity segmentation, SOHES demonstrates a significant reduction in the gap between self-supervised methods and the supervised SAM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is highly motivated, and the results of SOHES demonstrate tremendous potential in open-world entity segmentation, even surpassing SAM's performance on certain datasets. \n2. The paper is excellently structured and provides a clear and easy-to-follow presentation."
                },
                "weaknesses": {
                    "value": "1. The analysis of the hierarchical architecture is inadequate.\n2. Certain details in the method lack clarity, and there is a noticeable absence of some ablation experiments."
                },
                "questions": {
                    "value": "1. Regarding the generation of pseudo-labels, I am inquisitive about why the visual feature of merged patches is computed as the sum of original features rather than an average or any other operation?\n2. Concerning the second local re-clustering step, I'm curious about whether it has an impact on the results for large entities and how the threshold for small regions affects the final results. \n4. In the ablation study, there is no information regarding how the ancestor prediction head contributes to the final results. Further analysis of the hierarchical architecture is needed for clarification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3941/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3941/Reviewer_Rono",
                        "ICLR.cc/2024/Conference/Submission3941/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698548901680,
            "cdate": 1698548901680,
            "tmdate": 1700575710615,
            "mdate": 1700575710615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i7CB0axFtb",
                "forum": "PXNrncg2DF",
                "replyto": "WyB0vjpqB0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rono"
                    },
                    "comment": {
                        "value": "We appreciate the detailed feedback you provided for our submission. We are encouraged by your acknowledgement of our motivation, experimental results, and overall presentation. We provide the following clarifications in response to your concerns:\n\n1. Sum of features\n\n- In fact, summing and averaging the features when merging two regions are equivalent. The reason is that, we adopt the *cosine similarity* when measuring the semantic closeness between two regions, which normalizes the features\u2019 magnitude in computation. The sum of patch-level features within a region leads to the same cosine similarity as the average of patch-level features.\n\n- We chose the sum over average due to its calculation and implementation simplicity. Equivalent to computing and storing the feature sum $f_k=f_i+f_j$, we can also use the feature average $\\bar{f_k}=\\frac{N_i\\bar{f_i}+N_j\\bar{f_j}}{N_i+N_j}$, where $N_i,N_j$ are the numbers of patches in region $i$ and $j$, respectively. The sum formation does not require keeping track of the patch numbers.\n\n2. Impact of local re-clustering\n\n- We designed the local re-clustering to refine the pseudo-labels for small entities, because they are the most challenging to be discovered. The local re-clustering has minimal impact on the large entities. As shown in Table 2 of the original manuscript, the average recall for large entities (AR-Large) is slightly improved from 17.2 to 17.5 by the local re-clustering step. The reason for this change is that \u201clarge\u201d entities considered by the MS-COCO evaluation metric are regions that are larger than $96\\times 96$ pixels (https://cocodataset.org/#detection-eval), which could still fall under our relative area threshold $\\theta_\\text{small}=\\frac{1}{1024}$ in some high-resolution images from SA-1B (e.g., $1500\\times 8000$). Visually, these entities are not \u201clarge\u201d compared to the whole image, but are still counted by MS-COCO style AR$_L$.\n\n- To better understand the impacts of the hyper-parameter $\\theta_\\text{small}$, we run additional ablation study experiments. We observe that a threshold larger than $\\frac{1}{1024}$ does not further significantly improve the coverage of small entities but would introduce more processing time.\n| $\\theta_\\text{small}$ | 0 (No local re-clustering) | $\\frac{1}{2048}$ | $\\frac{1}{1024}$ | $\\frac{1}{512}$ |\n| --------------------- | -------------------------- | ---------------- | ---------------- | --------------- |\n| AR$_S$            \t| 1.9                    \t| 4.3          \t| 5.1          \t| 5.1         \t|\n| Time/Image (sec)  \t| 0.0                      \t| 6.0          \t| 8.4          \t| 10.0        \t|\n\n- We appreciate the reviewer\u2019s feedback and will integrate the explanation for the impact of $\\theta_\\text{small}$ into the revision.\n\n3. Ablating the ancestor prediction head\n\n- We proposed the ancestor prediction head to perform the additional task of hierarchical segmentation, which could be considered as an add-on extending the Mask2Former segmentation model. We did not expect it to help with mask prediction, since this ancestor prediction head operates in parallel to the mask prediction head. In fact, we trained a SOHES model without the ancestor prediction head, and its performance is very close to the standard SOHES model (e.g., on SA-1B, 33.0 Mask AR w/o ancestor prediction vs. 33.3 Mask AR w/ ancestor prediction)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372594214,
                "cdate": 1700372594214,
                "tmdate": 1700372729836,
                "mdate": 1700372729836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1dRzGV3Bxb",
                "forum": "PXNrncg2DF",
                "replyto": "i7CB0axFtb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_Rono"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_Rono"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the replies, after reading others' comments and feedback, I have updated my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575945349,
                "cdate": 1700575945349,
                "tmdate": 1700575945349,
                "mdate": 1700575945349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kw8jtsEZCj",
            "forum": "PXNrncg2DF",
            "replyto": "PXNrncg2DF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_vWUT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_vWUT"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Self-supervised Open-world Hierarchical Entity Segmentation (SOHES) approach for computer vision, which can segment entities in images beyond pre-defined classes without human annotations. SOHES uses three stages: self-exploration, self-instruction, and self-correction, generating high-quality pseudo-labels from visual feature clustering and refining them through mutual-learning. This method achieves a new standard in self-supervised open-world segmentation, eliminating the need for human-annotated masks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe task SOHES is seldom investigated.\n2.\tThis paper proposed a new method to generate hierarchical proposals.\n3.\tThis paper proposed an ancestor prediction head, which is novel.\n4.\tThe proposed method significantly outperformed the previous methods."
                },
                "weaknesses": {
                    "value": "1.\tAlthough this paper divide the stages into self-exploration, self-instruction, and self-correction. But it looks like previous papers[1] that generate pseudo-labels, then training from pseudo-labels, and apply self-training to improve the model. The framework is actually quite common. So what is the core difference from the previous works in the framework?\n2.     The authors claimed that \u201cExisting segmentation models cannot predict the hierarchical relations among masks. \u201d However, methods like Groupvit[2] already can predict the hierarchical relations among masks.\n3.\tThe proposed method introduced too many hyperparameters such as Theta_{merge}, Theta_{small}, Theta_{cover}. There is not explanation for the selection of some hyperparameters, such as Theta_{cover}. \n4.\tAlthough the authors proposed the ancestor prediction head for the hierarchical segmentation. However, the mask prediction of masks at each hierarchy is independent.  I am curious if the relation modeling of hierarchy can contribute to better mask predictions. I am also curious about the effect of just predicting the hierarchical relationships. It seems that there is no ablation to verify if the ancestor prediction head can bring the performance improvements.\n\n[1] Wang, Xinlong, et al. \"Freesolo: Learning to segment objects without annotations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[2] Xu, Jiarui, et al. \"Groupvit: Semantic segmentation emerges from text supervision.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739244730,
            "cdate": 1698739244730,
            "tmdate": 1699636354709,
            "mdate": 1699636354709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C2Id8skHUj",
                "forum": "PXNrncg2DF",
                "replyto": "kw8jtsEZCj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vWUT (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate the detailed feedback you provided for our submission. We are encouraged by your acknowledgement of our novel task, \u201cmethod to generate hierarchical proposals,\u201d \u201cnovel ancestor prediction head,\u201d and good performance. We provide the following clarifications in response to your concerns:\n\n1. Multi-phase framework\n\n- We agree with the reviewer that such a multi-phase learning paradigm (e.g., discovering pseudo-labels and then learning a detection/segmentation model) is a general framework in the existing self-supervised object localization/discovery literature. Recent methods which adopt this framework include LOST [R1], TokenCut [R2], FreeSOLO [R3] (as suggested by the reviewer), FOUND [R4], and CutLER [R5] (as mentioned by Reviewer PuvF). The contributions of these different methods do not lie in the introduction of a new framework, but instead in proposing unique components to improve this established framework for self-supervised object localization/discovery.\n\n- Our SOHES similarly adopts this general multi-phase framework, but makes significant contributions through **novel designs for each phase** involved in the learning process, including a global-to-local clustering algorithm for pseudo-label generation, a hierarchical relation learning module, and a teacher-student self-correction phase. In particular, none of the previously mentioned methods [R1-5] consider hierarchical structures among objects/entities, while learning such structures is involved throughout the learning process of SOHES. Given these technical novelties and advancements, our SOHES substantially improves performance over previous methods. \n\n- We appreciate the reviewer\u2019s feedback and will revise the Related Work section and the beginning of the Approach section, to better reflect this general framework and our novel components proposed in this work with a hierarchical perspective.\n\n2. Hierarchical segmentation model\n\n- We would like to first clarify that in the original manuscript, we stated that \u201cexisting segmentation models cannot predict the hierarchical relations among masks\u201d **in the context** of general segmentation model architectures like Mask2Former (Section 3.2). This drove us to design a novel module that could predict hierarchical structures among entities. Indeed, some *specialized* segmentation models (e.g., GroupViT [R6]) are capable of making hierarchical predictions, but do not directly fit into our context.\n\n- More importantly, although our SOHES and GroupViT both aim to produce hierarchical segmentation masks, there are some vital differences between the two approaches:\n\t- SOHES directly learns from unlabeled raw images, but GroupViT is weakly supervised by text.\n\t- SOHES can produce a variable number of hierarchical levels, but GroupViT is limited to pre-defined levels (e.g., 2 levels).\n\t- SOHES can produce a variable number of whole entities, parts, and subparts, but GroupViT is limited to a fixed number of pre-defined group tokens (e.g., 64 and 8 group tokens in the first and second grouping stages, respectively).\n\n- We appreciate the reviewer\u2019s feedback and will revise this sentence in Section 3.2 to more precisely describe the context about the widely-applied segmentation architectures. In addition, we will cite GroupViT [R6] and include the discussion above in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372406066,
                "cdate": 1700372406066,
                "tmdate": 1700372406066,
                "mdate": 1700372406066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tx1qFRZ5FP",
                "forum": "PXNrncg2DF",
                "replyto": "kw8jtsEZCj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_vWUT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_vWUT"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the responses from the authors. Most of my issues have been addressed. However, the \"ancestor prediction head\" does not have significant improvements. I decided to keep my original rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650850277,
                "cdate": 1700650850277,
                "tmdate": 1700650850277,
                "mdate": 1700650850277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p86IwNLQeH",
            "forum": "PXNrncg2DF",
            "replyto": "PXNrncg2DF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_PuvF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3941/Reviewer_PuvF"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduced the Self-supervised Open-world Hierarchical Entity Segmentation (SOHES) method, a three-phase approach for entity segmentation. The first phase, Self-exploration, uses a pre-trained DINO model to produce initial pseudo-labels. By clustering visual features, it identifies regions representing meaningful entities. In the Self-instruction phase, a Mask2Former segmentation model refines the segmentation by training on these initial labels. Even with some label noise, the model effectively averages out inconsistencies, resulting in better mask predictions. The final Self-correction phase uses a teacher-student mutual-learning framework to further refine the model's predictions and adapt to open-world segmentation. This approach only uses raw images without human annotations. A standout feature of SOHES is its ability to segment not just whole entities but also their parts and sub-parts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[Task] Unsupervised image segmentation holds significant importance, and this study successfully performs segmentation without human supervision, offering segmentation masks at multiple levels of granularity.\n\n[The generation of hierarchical masks] The approach to generate unsupervised hierarchical masks is pretty interesting. And surprisingly, this method surpassed SAM in recall on some evaluation benchmarks. \n\n[Paper writing] The paper is well-articulated, effectively communicating the central ideas."
                },
                "weaknesses": {
                    "value": "[Technical Contributions] The three phases proposed in this work are very similar to the Cut-and-Learn pipeline proposed by CutLER [1]. Self-exploration is pretty similar to the MaskCut stage in CutLER, which also leverages DINO feature for pseudo-label generation. Self-instruction is the same as the LEARN process of CutLER, which trains a model on pseudo-labels. And, the Self-correction stage can be viewed as a variant of CutLER's multi-round self-training, but with a teacher-student framework. I agree that while there are some implementation differences between the stages in SOHES and CutLER, their core concepts are largely analogous.\n\n[Model performance] SOHES performs much worse than CutLER (9.8 vs. 2.1 on COCO and 3.6 vs. 1.9 on LVIS) in terms of the mask AP. This works show stronger performance than the previous SOTA CutLER and SAM on some benchmarks, however, the main results are AR (averaged recall). Recall is important, however, for many downstream tasks, the AP is still the most valuable evaluation metric. \n\n[Unfair comparison] The main baseline CutLER used Cascade Mask RCNN as the segmentation model, while this work used Mask2Former, a stronger segmentation model, as the base model. This makes the performance comparison unfair. \n\n[1] Wang, Xudong, Rohit Girdhar, Stella X. Yu, and Ishan Misra. \"Cut and learn for unsupervised object detection and instance segmentation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3124-3134. 2023."
                },
                "questions": {
                    "value": "My main questions are listed in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3941/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698908394511,
            "cdate": 1698908394511,
            "tmdate": 1699636354644,
            "mdate": 1699636354644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wsFzCb2u6Y",
                "forum": "PXNrncg2DF",
                "replyto": "p86IwNLQeH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PuvF"
                    },
                    "comment": {
                        "value": "We appreciate the detailed feedback you provided for our submission. We are encouraged by your acknowledgement of our \u201cunsupervised segmentation\u201d task, \u201capproach to generate unsupervised hierarchical masks,\u201d and paper writing. We provide the following clarifications in response to your concerns:\n\n1. Technical contributions\n\n- We agree with the reviewer\u2019s perspective, but we would like to point out that such a multi-phase learning paradigm (e.g., discovering pseudo-labels and then learning a detection/segmentation model) is a general pipeline widely-adopted in the existing self-supervised object localization/discovery literature. Indeed, not only does CutLER [R5] (as suggested by the reviewer), but all recent methods also follow this pipeline including LOST [R1], TokenCut [R2], FreeSOLO [R3] (as mentioned by Reviewer vWUT), and FOUND [R4]. That means, the contributions of these different methods do not lie in the introduction of a new pipeline, but instead in proposing unique components to improve this established pipeline for self-supervised object localization/discovery.\n\n- Our SOHES similarly adopts this general multi-phase pipeline, but makes significant contributions through **novel designs for each phase** involved in the learning process, including a global-to-local clustering algorithm for pseudo-label generation, a hierarchical relation learning module, and a teacher-student self-correction phase. In particular, none of the previously mentioned methods [R1-5] consider hierarchical structures among objects/entities, while learning such structures is involved throughout the learning process of SOHES. Given these technical novelties and advancements and our substantial performance improvements over CutLER, we respectfully disagree with the reviewer that the distinctions between our SOHES and CutLER are merely minor implementation differences.\n\n- We appreciate the reviewer\u2019s feedback and will revise the Related Work section and the beginning of the Approach section, to better reflect this general pipeline and our novel components proposed in this work with a hierarchical perspective.\n\n2. Model performance\n\nWe would like to first clarify that, **regarding the open-world segmentation task on MS-COCO/LVIS**, the computed AP number is misleading and cannot accurately reflect the model\u2019s true segmentation performance. **The observed lower AP in our results compared with CutLER\u2019s on MS-COCO/LVIS is a consequence of limitations in the annotations within these datasets**. In contrast, on the SA-1B dataset which mitigates these annotation limitations, our AP (12.9) significantly surpasses CutLER\u2019AP (7.8). Below we explain in detail.\n\n- We chose average recall (AR) over average precision (AP) for the following reason: When evaluating open-world segmentation models (which try to segment \u201ceverything\u201d) on datasets with closed-world annotations (such as MS-COCO/LVIS which only include a pre-defined, limited set of entity classes), AP would penalize model predictions that are actually valid entities, but just not annotated by the dataset. Therefore, when the dataset annotations cannot cover all entities, AP becomes misleading for judging the performance of open-world models. Note that this choice of AR over AP is also commonly adopted in the open-world literature. Examples include (but are not limited to) detection [R6], segmentation [R7], and tracking [R8].\n\n- As an example (shown in Table 5), SAM\u2019s AP is lower than CutLER on MS-COCO (6.1 vs. 9.8) and PartImageNet (3.4 vs. 5.3), which definitely cannot imply SAM is a model inferior to CutLER. The lower AP of SAM is due to insufficient annotations in these two datasets, not the capability of SAM. Meanwhile, as the annotated entities increase, AP becomes more and more trustworthy. For instance, AP on SA-1B (the most densely annotated dataset evaluated in our work) matches qualitative comparison and follows the same trend as AR (FreeSOLO < CutLER < SOHES < SAM).\n\n3. Unfair comparison\n\n- We chose Mask2Former as our segmentation model mainly for making hierarchical predictions. In Cascade Mask R-CNN, each proposal is predicted independently, and therefore, analyzing hierarchical relations between entities would be challenging in Cascade Mask R-CNN. In contrast, the attention modules in Mask2Former allow information exchange among queries, so we can build our ancestor prediction head on Mask2Former.\n\n- For a more comprehensive comparison with CutLER, we train a Cascade Mask R-CNN segmentation model using SOHES without the entity hierarchy. This model still significantly outperforms CutLER with the same segmentation model architecture.\n| Method | Segmentation Model | LVIS | Entity | SA-1B |\n| ------ | ------------------ | ---- | ------ | ----- |\n| CutLER | Cascade Mask R-CNN | 20.2 | 23.1   | 17.0  |\n| SOHES  | Cascade Mask R-CNN | 27.0 | 29.2   | 31.9  |\n| SOHES  | Mask2Former    \t| 29.1 | 33.5   | 33.3  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372348888,
                "cdate": 1700372348888,
                "tmdate": 1700372348888,
                "mdate": 1700372348888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kyBMm840iH",
                "forum": "PXNrncg2DF",
                "replyto": "zoj9cdiMJo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_PuvF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3941/Reviewer_PuvF"
                ],
                "content": {
                    "comment": {
                        "value": "Hey authors, thank you for your time in addressing my comments. Most of them have been addressed and I will keep my original rating of borderline accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3941/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693495045,
                "cdate": 1700693495045,
                "tmdate": 1700693495045,
                "mdate": 1700693495045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]