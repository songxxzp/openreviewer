[
    {
        "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations"
    },
    {
        "review": {
            "id": "GzpXoR6NCS",
            "forum": "A2KKgcYYDB",
            "replyto": "A2KKgcYYDB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_fu3L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_fu3L"
            ],
            "content": {
                "summary": {
                    "value": "The training dynamics of over-parameterized DEQs are revisited in this study. The authors extend prior studies on ReLU DEQs and establish the linear convergence of training DEQs with general activations using a unique population Gramme matrix and a new kind of dual activation with Hermite polynomical expansion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides a fine-grained analysis of the gradient dynamics of DEQs. It extends the results of the ReLU case in [1] to more general cases.\n\n2. This paper proposes a novel population Gram matrix and develops a new form of dual activation with Hermite polynomial expansion. It appears that the proposed technical contributions can also be applied to the analysis of explicit neural networks.\n\n[1] Ling Z, Xie X, Wang Q, et al. Global convergence of over-parameterized deep equilibrium models. International Conference on Artificial Intelligence and Statistics. PMLR, 2023: 767-787."
                },
                "weaknesses": {
                    "value": "1 About the weight assumption. The authors assume that $W_{ij}\\sim N(0,2\\sigma_w^2/m)$ and $U\\sim N(0,2/d)$. I do not understand the reason of using the scaling parameter \"2\". In ReLU case the scaling parameter \"2\" is commonly used for simplicity, but this paper investigates general activations.\n\n2 About the existence and the uniqueness of $K$ (Proposition 12). In order to make sure Eq. (112) holds, one needs to make sure $2q^2\\tilde{L}_q\\sigma_w^2<1$ where \n$\\tilde{L}_q=\\frac{16L^2}{q^2} (\\frac{\\sigma_w^2}{m} \\mathbb{E}G+\\frac{3}{2})$\n(as implied by Eq.(111)). However, Proposition 12 only requires that Assumptions 1and 2 hold, i.e. $\\sigma_w^2<\\frac{1}{8L^2}$. I do not think this condition is sufficient to guarantee $2q^2\\tilde{L}_q\\sigma_w^2<1$.\n\n Moreover,  the properties of $\\mathbb{E}G_{11}$ are unclear. This makes the proof less rigorous.\n\n3 Lemma 10 (Proof of Lemma 4 in [1]) plays a key role in the proof. However, [1]' proof works for ReLU function. The authors should explain the applicability of the proof to general activation functions."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698069494263,
            "cdate": 1698069494263,
            "tmdate": 1699636392405,
            "mdate": 1699636392405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FyIaWdFftP",
                "forum": "A2KKgcYYDB",
                "replyto": "GzpXoR6NCS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer's comment"
                    },
                    "comment": {
                        "value": "Thank you very much for your detail reading of our paper, especially the proof. The following are our feedbacks about the weakness' comments:\n\n1. $\\textbf{About the weight assumption: }$  Our purpose is to show that under the similar set of conditions as Ling et al. (2022), the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function for a DEQ with more general activation functions (linear or non-linear), not only with the ReLU (linear) as Ling at el. (2022). We use the same scale factors as Ling et al. (2022) to emphasize the $\\textbf{similarity}$ in the conditions. In addition, by using the same conditions on $\\mathbf{U}$ and $\\mathbf{W}$ as Ling at el. (2022), we can use Lemma 4 in Ling at el. (2022) without any changes. \n\n2. $\\textbf{About the existence and the uniqueness of (Proposition 12):}$ We agree that there is a mistake here, and thank you for spotting this mistake. However, (112) still holds by assuming that $\\sigma_w^2 < 1/(48 L^2)$ in Assumption 1 since we have\n$$\n2q^2 \\tilde{L}\\_q \\sigma_w^2 =32 L^2 \\sigma_w^2 \\bigg(\\frac{\\sigma_w^2}{m}\\mathbf{E}[G\\_{11}]+\\frac{3}{2}\\bigg)=O(48 L^2 \\sigma_w^2)\n$$ as $m\\to \\infty$ (overparametrized DEQ)). \n\nAt the first sight, we can tighten the the above assumption to $\\sigma_w^2 <1/(24 L^2)$ by tightening the bound the Lipschitz constant $4L^2 \\max(\\alpha+1,\\beta+1)^2/q^2$ for $\\tilde{Q}_{\\alpha,\\beta}(\\cdot)$ in Lemma 9 to $2L^2 \\max(\\alpha+1,\\beta+1)^2/q^2$. We still think whether we can improve more in the revised version by tightening the above Lipschitz constant. \n\n$\\textbf{Moreover, the properties of $\\mathbf{E}[G\\_{11}]$ are unclear:}$ Sorry since we don't quite understand what you meant here. It seems to us that we forgot to state that we use the notation $\\mathbf{G}:=\\mathbf{G}(0)$, which leads to your concern. The matrix $\\mathbf{G}:=(\\mathbf{T}^*)^T \\mathbf{T}^*$ where $\\mathbf{T}^*$ is the equilibrium point of (2), which exists by using Lemma 2.  It is clear that $\\mathbf{G}$ is a random matrix, and $G_{11}$ is a random variable, and $\\mathbf{E}[G_{11}]$ is the expected value of this element.  \n\n3. $\\textbf{Lemma 10 (Proof of Lemma 4 in [1]) plays a key role in the proof:}$ The proof is exactly the same as the proof of Lemma 4 in [1], that is the reason why we remove the proof.  The idea is as follows. Observe that $\\mathbf{G}_{ij}^{(l+1)}=(\\mathbf{T}_i^{(l+1)})^T  \\mathbf{T}_j^{(l+1)}$ where $\\mathbf{T}_i^{(l+1)}$ is the $i$-th column of $\\mathbf{T}$.  By using (1), we have\n$$\n\\mathbf{T}_i^{(l+1)}=\\varphi\\big( [\\mathbf{W} \\mathbf{T}^{(l)}]\\_i+[\\mathbf{U}\\mathbf{X}]\\_i \\big).\n$$\nBy using the exact argument as the proof of Lemma 4 in [1], we can represent $[\\mathbf{W} \\mathbf{T}^{(l)}]\\_i+[\\mathbf{U}\\mathbf{X}]\\_i$ as $\\mathbf{M}\\mathbf{h}$ and $[\\mathbf{W} \\mathbf{T}^{(l)}]\\_j+[\\mathbf{U}\\mathbf{X}]\\_j$ as $\\mathbf{M}\\mathbf{h}'$.\n\nAs your concern, we will add this short idea to our revised version.\n\n\nFrom our analysis, the extension from ReLU to more general activation functions is not straightforward (especially our design of a novel matrix $\\mathbf{K}$, our development of Lemma 9, our introduction of new class of dual activation functions $\\tilde{Q}_{\\alpha,\\beta}$ to deal with non-linear activations which has not appeared in the existing research literature), we look forward to receiving your better evaluations. Thank you very much."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699699545210,
                "cdate": 1699699545210,
                "tmdate": 1699699545210,
                "mdate": 1699699545210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nwYeHeqpmV",
            "forum": "A2KKgcYYDB",
            "replyto": "A2KKgcYYDB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
            ],
            "content": {
                "summary": {
                    "value": "The authors extend the framework by Ling et al. who showed linear convergence rate for the gradient descent applied to the quadratic loss function for over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. The same rate is obtained when ReLU is replaced by an activation function with bounded first and second derivatives. To obtain this rate, the authors bound the least eigenvalue of the Gram matrix of the equilibrium point by means of Hermite polynomial expansions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The assumptions required in the main theorem 8 are fulfilled for commonly used activation functions such as sine and tanh.\n\nThe claimed theoretical statements evidence are supported by numerical experiments on MNIST and CFAR-10 datasets.  \n\nThe analysis techniques based on dual activation with Hermite polynomial expansion seem somehow original and elegant."
                },
                "weaknesses": {
                    "value": "The authors state several auxiliary results in Section 5 that are essential for their main theorem but are not proved in the main the appendix. \nNot being an expert in this field and with a very limited amount of time (too short to read in detail the 21 pages of supplementary material), it is quite hard to judge whether the framework is correct or not. I believe that this work, possibly sound and surely interesting, would be worth publishing but the current conference format (coming together with a short allocated review time) might not be the best fit."
                },
                "questions": {
                    "value": "Could you give examples of activation functions that would not fulfill the conditions of Theorem 8?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698271235822,
            "cdate": 1698271235822,
            "tmdate": 1699636392311,
            "mdate": 1699636392311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jxbvjo5NHR",
                "forum": "A2KKgcYYDB",
                "replyto": "nwYeHeqpmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer's comment"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedbacks on our paper. For your question related to give examples of activation functions that would not fulfill the conditions of Theorem 8, our answer is as follows.  \n\nThank you for your interesting question. The Hermite expansion is known to exist for some class of functions such as the weighted Hilbert space $L^2(\\mathbb{R}, e^{-x^2})$ since Hermite polynomials form an orthogonal basis for this Hilbert space. However, the coefficients of each orthogonal expansion depend on each function in $L^2(\\mathbb{R}, e^{-x^2})$. As far as we know, the exact form of Hermite expansion is only given for some specific functions such as ReLU, tanh, or sigmoid. \n\nEssentially, the condition in our Theorem 8 requires that there are infinitely many positive coefficients in the Hermite expansion of that function. In (46), we give a sufficient condition for this condition to hold, however it is not easy to find a necessary condition or find a counter-example as your question.  \n\nLooking forward to receive your better evaluation of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699787557506,
                "cdate": 1699787557506,
                "tmdate": 1699787557506,
                "mdate": 1699787557506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IiunMANHA8",
                "forum": "A2KKgcYYDB",
                "replyto": "Jxbvjo5NHR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_SDT1"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks for your answer, I will retain my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699909847373,
                "cdate": 1699909847373,
                "tmdate": 1699909847373,
                "mdate": 1699909847373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "40AomMMIsO",
            "forum": "A2KKgcYYDB",
            "replyto": "A2KKgcYYDB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends a previous result from Ling et al. on the global convergence of DEQs proved for ReLU activations to a more general class of activations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- the problem of understanding the theory of DEQs is interesting"
                },
                "weaknesses": {
                    "value": "- **contribution**: it is not clear what is the exact contribution of this work. To me it seems that it's merely an extension of the work of Ling et al. but it doesn't bring new theoretical ideas, proof techniques or closes a gap between theory and practice. This is largely reflected by the fact that a large portion of the text is extremely similar to the paper of Ling et al. It can also be seen in the fact that entire parts of the paper are dedicated to things that would usually be in the appendix like extended proofs, extended historical perspectives on generalization or examples. \nMoreover, while there is a claim that \"a novel population Gram matrix\" or \"new form of dual activation with Hermite polynomial expansion\" are introduced in this work, it is clear from reading them that they are direct extensions of Ling et al. or Daniely et al.\n- **clarity**: so many notations are introduced (some even very unusual like $T$ for the equilibrium point of DEQs) which makes the paper difficult to follow."
                },
                "questions": {
                    "value": "- what are the contributions of this work on top of extending the proof of Ling et al. to other activation functions?\n- why is it important to extend the proof of Ling et al. to other activation functions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Several parts of the submission are directly copy-pasted from the Ling et al. paper inspiring this work:\n\n- The first 4 paragraphs of the problem settings section, with just a slight change of notation. This change of notation appears to be made only to obfuscate plagiarism identification though since it's so bizarre: indeed $z$ is always used to denote the fixed point or equilibrium point of DEQs, rather than $t$ as introduced in the paper.\nOf course the problem setting was always going to be the same, but it's weird to make this slight unusual notation change and not clearly state that the whole problem setting section is a copy-paste of the Ling et al. work.\n\n- Some parts of the introduction also seem to have been copy-pasted in a somewhat random order.\n\n- The first part of Section 3 (up till theorem 4) is copy pasted from the \"2.2 Well-Posedness and Gradients\" section, with just an unjustified offset in the lemma numbering."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695361587,
            "cdate": 1698695361587,
            "tmdate": 1699636392242,
            "mdate": 1699636392242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "msp21kOW7t",
                "forum": "A2KKgcYYDB",
                "replyto": "40AomMMIsO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the reviewer's comments"
                    },
                    "comment": {
                        "value": "Thank you very much for your comments on our paper. The following are our feedbacks to your questions and comments:\n\n1. $\\textbf{What are the contributions of this work on top of extending the proof of Ling et al. to other activation functions?}$ As you can see, our work is to extend the work of Ling et. al to a more general setting. In Ling et. al work, the authors assume that the activation is the ReLU, which is a linear function. Our work extends it for more general activation functions which consist of non-linear activation functions such as sigmoid or tanh. To extend from a linear activation setting to a non-linear activation one, we need to extend some important concepts as follows:\n + Extend the dual activation concept in [Eq.11,Ling et. al] (or Daniely et. al 2016) to  a new class of dual activation functions in Definition 5. As you can see in [Eq.11,Ling et. al], for ReLU function, the dual activation function has a specific form, i.e., $Q(x)=\\frac{\\sqrt{1-x^2}+(\\pi-arccos(x))x}{\\pi}$. However, for the non-linear activation functions such as sigmoid or ReLU, it is very challenging to obtain an exact form for this dual activation function. Hence, our work proposes a method to overcome this difficulty by introducing a new concept called \"a class of dual activation functions\". \n + Right before [Lemma 3, Ling et. al], the authors mention that by using the homogeneity (linear) of ReLU, the authors can use the definition of population Gram matrices in Definition 1 to obtain (12). For the non-linear activation functions that we consider in this work, we need to define a new type of population matrices as you can see in our Definition 6. To come up with this new type of population Gram matrices requires a lot of our efforts from mathematical viewpoints. It is not a straightforward extension of the population Gram matrices in Ling et. al.\n+ In addition, the dual activation $Q(x)$ in [Lemma 3, Ling et. al] has a  Hermite expansion in [Lemma 7, Ling et. al] or [Daniely et. al 2016].  To deal with the non-linear activation functions, as mentioned above, we need to define a new class of dual activation functions. But, we need to show that the new class of dual activations also has a Hermite expansion. That is also a new contribution in this work.\n\n2.  $\\textbf{Why is it important to extend the proof of Ling et al. to other activation functions?}$. The Deep Equilibrium (DEQ) model was first introduced in [Bai et al.,2019]. This model has recently demonstrated impressive performance on a variety of large-scale vision and NLP tasks, often showing competitive performance relative to the state of the art [Bai et al., 2020].   In the above work, both linear and non-linear activation functions are usually used (eg. tanh is used in p.8,Bai et al.,2019). In Ling et. al, the authors only considered the ReLU DEQ. Our work contributes in understanding DEQ with non-linear activation functions. As we can see in practice, many facts hold for linear model but do not hold for non-linear ones. Hence, theoretical understanding of DEQ with non-linear activation functions are very important, that motivates our work.\n\n3. $\\textbf{About your comments about some text parts of Ling et al. paper appear in our work}$. We agree that that we use some texts in Ling et. al for the purpose of giving motivations why we need to design a novel Gram matrix $\\mathbf{K}$ in for both linear (Ling et. al) and non-linear activations (our work), especially understanding why we need to bound the least eigenvalue of $\\mathbf{G}(0)$ in Theorem 7 without reading Ling et. al paper to understand this. Based on your comments and avoid the using the texts from previous works, we will remove Lemma 2 and Lemma 3 and other repeated texts in our revised versions. We will only cite them in our revised version. \n\nBased on our contributions to develop theory for an important problem, we look forward to receiving your better evaluation. Thank you very much."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699742921078,
                "cdate": 1699742921078,
                "tmdate": 1699743040264,
                "mdate": 1699743040264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XSI6V1twaP",
                "forum": "A2KKgcYYDB",
                "replyto": "msp21kOW7t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_pmW6"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I want to thank the authors for engaging respectfully in the rebuttal process.\n\n- *dual activation function  specific form*: I think representing the dual activation function as its complex writing for the specific ReLU case is a mischaracterization. In Daniely et al. (a definition taken by Ling et al.) the dual activation function is defined in the general case as: $Q(x) = \\mathbb{E}_{u, v \\sim \\mathcal{N}\\left(0, A(x)\\right)}\\left[ \\phi(u) \\phi(v) \\right]$ where $A(x) = \\begin{bmatrix}1 & x\\\\\\\\x & 1 \\end{bmatrix}$. \n\nIn this submitted work, the generalized dual activation function is defined as \n\n$$\n\\tilde{Q}_{\\alpha, \\beta}(x) = \\frac{1}{\\alpha \\beta q^2} \\mathbb{E} \\left[ \\phi(\\alpha u) \\phi(\\beta v) \\right]\n$$\nwhere $u, v$ follow the same distribution, and $q$ is a normalization factor defined in (4) from $\\phi$.\nI don't think it's a \"new concept\".\n- *new type of Gram matrices*: The Gram matrices defined in the submitted paper follow the same logic as the gram matrices presented in Ling et al. with a bunch of normalization constants, in particular the $\\alpha, \\beta$ used in $\\tilde{Q}_{\\alpha, \\beta}$.\n- *hermite expansion*: the proof that the defined class of dual activation functions has hermite expansions is to the best of my understanding the following: \"Then, by using a similar proof as (Daniely et al., 2016, Lemma 11), it can be shown that the new\nactivation function (see Definition 5) satisfies\" (then Eq (46) is shown). To me this is not sufficient for a proof and/or shows that this is not a very important technical contribution.\nI also do not see which proof in Daniely et al. this refers to.\n- *importance of other activation functions*: I agree that DEQs sometimes use other activation functions. However, how does the submitted paper quantify the difference in behaviour that could be observed between the uses of these different activation functions?  \"many facts hold for linear model but do not hold for non-linear ones\": which facts observed in practice differ between DEQs using ReLU and DEQs using tanh? How are they highlighted in the theoretical framework proposed here? I want to clarify here that ReLU is not a linear activation function so I think it's a mischaracterization to say \"linear model\". I assume here that \"non-linear\" means non-ReLU.\n- *copy-pasting*: I respectfully disagree with the authors here. Some copy-pasted parts were not used to give motivation for one specific aspect, but rather for the whole problem with super slight modifications.\n\n\nGenerally, when comparing this work and the work of Ling et al. (coupled with Daniely et al.) for the second time, it seems to me that the appropriate way to show an important contribution would be:\n- use the same notations to have a clearer comparison: here the almost obfuscation of the notations compared to Ling et al. makes it a very tedious process. More generally as I said in my original review, there are simply too many notations (a lot unconventional) to have a legible work. Potentially, the same theorem numbering could be useful: theorem 1 ->, theorem 2 -> 7, theorem 3 -> 8 with some explanations of what changed.\n- explain clearly what is the difficulty in going from the proofs of Ling et al. to those of this work. For now it only says \"For example, (Ling et al., 2022, Eq. 11) only holds for ReLU\": why is this important? why are all the normalizing constants needed?\n- explain how the new conditions required for initialization and learning rate of Theorem 4. impact the behaviour of DEQs.\n\nBasically, a side-by-side comparison would be ideal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699791272497,
                "cdate": 1699791272497,
                "tmdate": 1699791272497,
                "mdate": 1699791272497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "StepMhCxxV",
            "forum": "A2KKgcYYDB",
            "replyto": "A2KKgcYYDB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the NTK-like analysis of wide Deep Equilibrium Models converging linearly with gradient descent, which was previously proven only for the ReLU activation ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)), to encompass general activation functions through advanced analysis of the Gram matrix.\n\nSpecifically, Deep Equilibrium Model ([Bai et al. (2019)](https://arxiv.org/abs/1909.01377)) defines the model output as the equilibrium value of a recursive equation, which corresponds to the output of infinitely-deep network with the same weight across all layers. The linear convergence of the overparameterized (wide) Deep Equilibrium Model using gradient descent is proven in ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)) for the ReLU activation, following the NTK analysis. The proof of linear convergence requires lower bounding a certain gram matrix. To extend the result for ReLU to general activation functions, the lower bound argument should be more abstract, which is the main contribution of this paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Convergence of Deep Equilibrium Model has not been proven for general activation functions\n\nThe previous work for the ReLU activation ([Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf)) provides a detailed literature review on the NTK-like analysis of Deep Equilibrium Models. As far as I understand, [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) is the first non-asymptotic analysis for the ReLU and there are no subsequent work for general activation functions. Thus I think the problem this paper addresses itself is new to a certain extent.\n\n### All proofs seem to be correct at high level.\n\nWhile there are some rough edges (e.g., in Lemma 2, a constant $C$ is introduced but not used anywhere in the statement), the overall argument leading to the theorem appears to be sound."
                },
                "weaknesses": {
                    "value": "### The proof follows [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), and the modification required for dealing with general activation functions looks very basic.\n\nIn reviewing this paper, I have thoroughly examined the proof method of [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), which is a prior study. Through this examination, I have identified that the fundamental flow of the proof in this paper is essentially the same. For example, the following correspondence exists:\n\nTheorem 2 in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) - Theorem 7\n\nTheorem 3 in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf) - Theorem 8\n\nWhile the introduction of Hermite decomposition distinguishes it from the case limited to ReLU, it is worth noting that a significant portion of the proof remains identical to the original one. In evaluating this paper, it seems that the crucial point to consider is the novelty and significance of using Hermite decomposition to establish a lower bound on the kernel's eigenvalues, especially in comparison to the prior work that did not incorporate this technique. However, I am aware that such an idea is widely used in other relevant literature (e.g., [Misiakiewicz (2022)](https://arxiv.org/abs/2204.10425)). Due to these reasons, the technical contribution of this paper appears somewhat incremental, which is why I have reservations about recommending its acceptance.\n\n### The literature review appears to be lacking in depth.\n\nThis paper seems to have overlooked several works on the application of NTK to DEQ, which are thoroughly explained in [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf), and only mentioning [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf). The Introduction chapter appears to begin with a very general discussion (introduction to deep learning) while omitting a review of literature directly relevant to this paper. As my suggestion, it might be beneficial to introduce DEQ first, followed by a presentation of existing analyses and challenges associated with it. This approach could provide a more comprehensive overview within a similar page length.\n\n### The proof sketch is a mere list of claims.\n\nThe paper seems to just list theorems without providing sufficient explanations about what is fundamentally novel. Many of these theorems can be linked to references in prior literature. In my understanding, the novelty lies in the proof methods of Theorem 7 and 8, so please provide detailed explanations about them.\n\nFurthermore, Section 6 appears to be overly verbose. In my opinion, it could be expected to hold, and even if it doesn't, the focus should be on proving it for just one activation function.\n\n### (Minor) ``any general activation that has bounded first and second derivatives.'' (abstract) requires modification\n\nIn Theorem 8, the authors assume non-vanishing Taylor coefficients on the dual activation function. I do not think think bounded first and second derivatives suffice to satisfy this assumption."
                },
                "questions": {
                    "value": "- It might be helpful if the paper could illustrate the challenges and difficulties of dealing with general activation functions by contrasting the proof methods with those from previous literature, which could highlight aspects that I might have overlooked.\n\n- Could you provide more detailed information about the relevant literature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4252/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699113474308,
            "cdate": 1699113474308,
            "tmdate": 1699636392157,
            "mdate": 1699636392157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Eylefzir5",
                "forum": "A2KKgcYYDB",
                "replyto": "StepMhCxxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the reviewer's comments"
                    },
                    "comment": {
                        "value": "Thank you very much for your comments and suggestions to improve the presentation of paper.  For your suggestions related to improving the presentation, we will try our best to improve the presentation following your suggestions such as providing more detailed explanations for Theorem 7 and Theorem 8.  The following are our answers to your questions:\n\n1. $\\textbf{It might be helpful if the paper could illustrate the challenges and difficulties of dealing with general activation functions by contrasting the proof methods with those from previous literature, which could highlight aspects that I might have overlooked.}$\n\nAs you know, our work is to extend the work of Ling et. al to a more general setting. In Ling et. al work, the authors assume that the activation is the ReLU, which is a linear function. Our work extends it for more general activation functions which consist of non-linear activation functions such as sigmoid or tanh. To extend from a linear activation setting to a non-linear activation one, we need to extend some important concepts as follows:\n  + Extend the dual activation concept in [Eq.11,Ling et. al] (or Daniely et. al 2016) to  a new class of dual activation functions in Definition 5. As you can see in [Eq.11,Ling et. al], for ReLU function, the dual activation function has a specific form, i.e., $Q(x)=\\frac{\\sqrt{1-x^2}+(\\pi-arccos(x))x}{\\pi}$. However, for the non-linear activation functions such as sigmoid or ReLU, it is very challenging to obtain an exact form for this dual activation function. Hence, our work proposes a method to overcome this difficulty by introducing a new concept called \"a class of dual activation functions\". \n  + Right before [Lemma 3, Ling et. al], the authors mention that by using the homogeneity (linear) of ReLU, the authors can use the definition of population Gram matrices in Definition 1 to obtain (12). For the non-linear activation functions that we consider in this work, we need to define a new type of population matrices as you can see in our Definition 6. To come up with this new type of population Gram matrices requires a lot of our efforts from mathematical viewpoints. It is not a straightforward extension of the population Gram matrices in Ling et. al.\n   + In addition, the dual activation $Q(x)$ in [Lemma 3, Ling et. al] has a  Hermite expansion in [Lemma 7, Ling et. al] or [Daniely et. al 2016].  To deal with the non-linear activation functions, as mentioned above, we need to define a new class of dual activation functions. But, we need to show that the new class of dual activations also has a Hermite expansion. That is also a new contribution in this work.\n\n2. $\\textbf{Could you provide more detailed information about the relevant literature?}$ Thank you very much for your introducing to some papers of which we were not aware such as Misiakiewicz (2022). We will add more relevant papers in our revised version. If you have any other paper suggestions, please let us know. \n\nBased on our feedback, we look forward to receiving your better evaluation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699798722435,
                "cdate": 1699798722435,
                "tmdate": 1699798722435,
                "mdate": 1699798722435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "53d0PuAgGD",
                "forum": "A2KKgcYYDB",
                "replyto": "StepMhCxxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThanks for your reply. \n\nFrom (253) to (254) (in the proof of Theorem 8), I found that the authors used $Q_{i,j}(\\nu_{i,j})=\\sum_{r=0}^\\infty \\mu_{r,\\alpha}^2(\\varphi)\\nu_{i,j}^r$. My questions are\n- What is $\\alpha$?\n- The assumption only states that $Q_{\\alpha,\\alpha}(x)=\\sum_{r=0}^\\infty \\mu_{r,\\alpha}^2(\\varphi)x^r$. But seemingly the authors applied this assumption to $Q_{i,j}$. Why?\n\nI would appreciate clarification from the authors."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699816787648,
                "cdate": 1699816787648,
                "tmdate": 1699817021264,
                "mdate": 1699817021264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eDCbCFLCnH",
                "forum": "A2KKgcYYDB",
                "replyto": "StepMhCxxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "content": {
                    "comment": {
                        "value": "I see. \nThen, I think that $Q_{i,j}(\\nu_{i,j})=R(\\nu_{i,j})$ holds for some fixed function $R$. All we need to show is the positive-semidefiniteness of the matrix with its $i,j$-th entry $R(\\nu_{i,j})$ (the positive definiteness of $K$ follows from this because all $\\rho$ is also positive). To show this, it is easy to see that $\\nu_{i,j}$ can be Taylor-expanded as $\\nu_{i,j}=\\sum_{r=0}^\\infty c_r (x_i^\\top x_j)^r$ with respect to $x_i^\\top x_j$ with non-negative coefficients, especially $c_1>0$. (by using (22) or (30) and the assumption of Theorem 8). When $R$ is expanded as $R(x)=\\sum_{r=0}^\\infty c_r' x^r$ with $c_r'>0$ (assumption of Theorem 8), $R(\\nu_{i,j})$ is Taylor-expanded w.r.t. $x_i^\\top x_j$, and especially all coefficients are positive, i.e., $R(\\nu_{i,j})=\\sum_{r=0}^\\infty c_r'' (x_i^\\top x_j)^r$. Is my understanding correct?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699825407634,
                "cdate": 1699825407634,
                "tmdate": 1699825677443,
                "mdate": 1699825677443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6IRu804hVQ",
                "forum": "A2KKgcYYDB",
                "replyto": "StepMhCxxV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4252/Reviewer_ubP5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for detailed clarification.\n\nIf I would evaluate my proof strategy, I do not think evaluation of the smallest eigenvalue of the kernel defined as $R_{i,j}=R(\\nu_{i,j})=\\sum_{r=0}^\\infty c_r'' (x_i^\\top x_j)^r$ is a significant contribution, while obtaining the recursive formula which gives $\\rho_{i,j}$ and $\\nu_{i,j}$ in the limit is non-trivial. This is because there are many literatures on bounding the smallest eigenvalue of the kernel matrix as I explained. \n- To see this, you can simply decompose $R$ into $R_{i,j}^1=\\sum_{r=0}^\\tau c_r'' (x_i^\\top x_j)^r$ and $R_{i,j}^2=\\sum_{r=\\tau+1}^\\infty c_r'' (x_i^\\top x_j)^r$. $R^1$ is positive semidefinite and $R^2$ is positive definite, because $(x_i^\\top x_j)^r$ with $r>\\tau$ is very small for all $i\\ne j$ when $\\tau$ is sufficiently large, $R$ is proven to be positive definite. \n\nRather, defining the limit kernel using a recursive formula (which gives $c_r''>0$ for all $r$ in my proposal) is interesting. However, this is due to the previous work of [Ling et al. (2023)](https://arxiv.org/pdf/2205.13814.pdf).\n\nI think your proof is also divided into the recursive formula and eigenvalue evaluation. If I assume there is the correspondence between your proof and mine, I do not think Theorem 8 is so important, while the recursive part is from the previous work. Therefore, I would like to choose to maintain my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4252/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699833123345,
                "cdate": 1699833123345,
                "tmdate": 1699833161555,
                "mdate": 1699833161555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]