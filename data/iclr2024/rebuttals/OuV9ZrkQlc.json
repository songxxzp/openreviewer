[
    {
        "title": "ImagenHub: Standardizing the evaluation of conditional image generation models"
    },
    {
        "review": {
            "id": "0U76CgQ4gL",
            "forum": "OuV9ZrkQlc",
            "replyto": "OuV9ZrkQlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_teCT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_teCT"
            ],
            "content": {
                "summary": {
                    "value": "Paper proposes a library for the evaluation of coditional image generation models. They consider 7 tasks (text-guided, subject-driven, control-gruided etc). They define two human evaluation scores (semantic consistency and perceptual quality), train the raters, and evaluate around 30 models for the various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. paper is well written and presentation is good. \n2. a fair comparison based on human raters is interesting for many users and scientists in this dense research field. \n3. evaluation setup and comparisons are well-designed and seem fair (many methods use their own curated datasets and are here compared on the same data)."
                },
                "weaknesses": {
                    "value": "1. I think the paper would have been stronger if the authors would have directly compared the human evaluations with existing automatic evaluation metrics. It would be very interesting to know the correlations. \n\n2. Ideally we would have computable metrics which correlate high with human evaluations. The paper does not explain very well what the main problems are of existing metrics. It would also be interesting to see what parts of human evaluation are missed by currently used metrics.\n\n3. It remains unclear how a third party with a new method could make use of this benchmark since it is based on human raters. It would be nice if there is some rater-training guide which would allow other researchers to also evaluate their method on the proposed benchmark. There are no safeguards for the maintenance of the benchmark by the authors.\n\n4. I found the discoveries and insights not especially surprising. They were often more based on looking at results than referring to the human evaluation rates."
                },
                "questions": {
                    "value": "I think the study is of interest for many people. However, I found the technical contribution still a bit shallow and the possible usage for future model evaluation unclear. If some results on weakness point 1 could be added to the paper, I would probably be willing to raise my score.\n\n- please address the mentioned weaknesses. \n\nminor remarks:\n- ref to Table 10 in section 4.2 is wrong (should be table 2)\n- spellcheck 'Inference: ' text on page 2.\n\n-------------------------------\nPOST REBUTTAL:\nI thank the reviewers for their feedback. I apologize for mising Table 8 in Appendix. And I appreciate the effort done to ensure ongoing usefulness of the proposed benchmark for future users (A7-A9). I have raised my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Reviewer_teCT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313585562,
            "cdate": 1698313585562,
            "tmdate": 1700599916482,
            "mdate": 1700599916482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cM8DWXCQBs",
                "forum": "OuV9ZrkQlc",
                "replyto": "0U76CgQ4gL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer teCT"
                    },
                    "comment": {
                        "value": "(w1) I think the paper would have been stronger if the authors had directly compared the human evaluations with existing automatic evaluation metrics. It would be very interesting to know the correlations...\n\nWe have compared the correlations in Table 8: Metrics correlation. (Appendix A1)\n\n(w2) Ideally we would have computable metrics which correlate highly with human evaluations. The paper does not explain very well what the main problems are of existing metrics. It would also be interesting to see what parts of human evaluation are missed by currently used metrics...\n\nThe computable metrics all hold different assumptions which only contribute part of the aspects in human evaluation. This makes a low correlation with human evaluations. For example, CLIPscore only measures the compatibility of image-caption pairs. But there are more aspects from the human perspective of semantic consistency. In editing tasks there are the level of editing, in subject-driven tasks it has to measure whether the subject aligns. Those are still an open problem in the field. Thus we found a low correlation for CLIP score across all tasks in Table 8. On the other hand, perceptual quality metrics like FID focused on visual quality and LPIPS evaluates the distance between image patches. While they are good at detecting distortion and artifacts, From the human perspective of perceptual quality, the overall natural sense of whole image is often considered. \n\n(w3) It remains unclear how a third party with a new method could make use of this benchmark since it is based on human raters. It would be nice if there is some rater-training guide which would allow other researchers to also evaluate their method on the proposed benchmark. There are no safeguards for the maintenance of the benchmark by the authors...\n\nThere are two ways to evaluate their methods on ImagenHub. The preferred option is that the third party will send a pull request to our GitHub repo to add their model inference interface. We will use their code to perform standaridized evaluation and then publish our leaderboard.\nThe less preferred option is that the third party will follow our human judgment guide and hire their own raters to do it. Our guidelines and examples are in Appendix A2 and A3. We also added Appendix A7, A8, and A9 to show how a third party with a new method could make use of the benchmark. Links are hidden due to the Anonymous policy.\n\n\n(w4) I found the discoveries and insights not especially surprising. They were often more based on looking at results than referring to the human evaluation rates...\n\nWe think there are some interesting insights: (1) our rigorous eval shows that the existing methods despite their popularity are actually doing very poorly on several generation tasks. There is still a large room for improvement, (2) we went through the claims in previous papers and found that 17% of them are ill-posed due to rigorous human evaluation, (3) we found that besides T2I and subject-driven generation, the automatic metrics are having extremely low correlation with human raters. It indicates that we need to develop better metrics for those tasks.\n\n(q1) I think the study is of interest for many people. However, I found the technical contribution still a bit shallow and the possible usage for future model evaluation unclear. If some results on weakness point 1 could be added to the paper, I would probably be willing to raise my score...\n\nOur main technical contribution focused on the inference library and the unified inference and evaluation protocols. Although we have already released the codebase on GitHub, we cannot mention the links due to the Anonymous policy. In this revised version, we added Appendix A7, A8, and A9 to show how a third party and researchers can benefit from our work and easily extend it."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183682565,
                "cdate": 1700183682565,
                "tmdate": 1700183682565,
                "mdate": 1700183682565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zHvmAEcg0F",
                "forum": "OuV9ZrkQlc",
                "replyto": "0U76CgQ4gL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_teCT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_teCT"
                ],
                "content": {
                    "title": {
                        "value": "rebuttal"
                    },
                    "comment": {
                        "value": "I thank the reviewers for their feedback. I apologize for mising Table 8 in Appendix. And I appreciate the effort done to ensure ongoing usefulness of the proposed benchmark for future users (A7-A9). I have raised my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600066745,
                "cdate": 1700600066745,
                "tmdate": 1700600066745,
                "mdate": 1700600066745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VZwdro7ogK",
            "forum": "OuV9ZrkQlc",
            "replyto": "OuV9ZrkQlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_JKzK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_JKzK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ImagenHub, a new benchmark for conditional image generation based on human evaluators.\nThe benchmark evaluates seven conditional image generation tasks. \nDuring evaluation, human evaluators will follow two major metrics, namely semantic consistency and perceptive quality. \nThe formal metric (SC) ensures that the generated image is aligned with the given condition, while the second metric (PQ) ensures that the generated image is of good visual quality.\nThe authors evaluate major opensource image generation approaches based on the two metrics and report the results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper first proposes a comprehensive benchmark to evaluate different conditional image generation tasks. It evaluates a bunch of image generation models with human evaluators, and provides comprehensive evaluation results to the community."
                },
                "weaknesses": {
                    "value": "1. It would be better if the authors can involve more top-performing image generation approaches in this comparisons (though some of them may not be opensource), like MidJourney or DALLE-3 (in a later revision of the paper). \n2. Some previous works (like T2I CompBench) have also proposed some evaluation metrics for benchmarking conditional image generation models (related to the Semantic Consistency part in this paper). It would be better to discuss and see if such metrics result in similar trends compared to the human-based evaluation."
                },
                "questions": {
                    "value": "1. How do the authors validate the annotation quality, and the measure the performance of annotators?\n2. How much human labor is required to build this benchmark (in annotator x hour)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Reviewer_JKzK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698494595239,
            "cdate": 1698494595239,
            "tmdate": 1700624201012,
            "mdate": 1700624201012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jJffObNoU2",
                "forum": "OuV9ZrkQlc",
                "replyto": "VZwdro7ogK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JKzK"
                    },
                    "comment": {
                        "value": "(w1) It would be better if the authors can involve more top-performing image generation approaches in this comparisons (though some of them may not be opensource), like MidJourney or DALLE-3 (in a later revision of the paper).\n\nWe have gathered and evaluated the generation results of MidJourney and DALLE-3 on our ImagenHub benchmark. Please refer to B1 in our new paper version. It turns out that DALLE-3 and Mijourney are extremely good, outperforming the other methods by an absolute overall score of 10-20%.\n\n(w2) Some previous works (like T2I CompBench) have also proposed some evaluation metrics for benchmarking conditional image generation models (related to the Semantic Consistency part in this paper). It would be better to discuss and see if such metrics result in similar trends compared to the human-based evaluation.\n\nT2I CompBench\u2019s proposed metric is specifically for Text-To-Image task. They cannot be used across all the ImagenHub defined tasks. For example, UniDet (from T2I CompBench) does not work on Subject-Driven Image Editing and Generation tasks. Our paper focused on evaluation metrics that apply to most of the tasks. \n\n(q1) How do the authors validate the annotation quality, and the measure the performance of annotators?\n\nWe selected a few (around 10-20) significant samples with obvious ratings. We rated them by ourselves and then used them as a reference to determine the performance of annotators.\n\n(q2) How much human labor is required to build this benchmark (in annotator x hour)?\n\n* The current benchmark (total of 30 models) was built by 24 annotators in a total of an estimated 150 hours. Each of the annotators conducted evaluations on 3-4 sets (or more) of images.\n* We required an annotator to rate one set of images in one go. \n* For the time used in evaluating one set of images (around 150-200 images). The mean time reported is 1 hour 40 minutes."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183567511,
                "cdate": 1700183567511,
                "tmdate": 1700183567511,
                "mdate": 1700183567511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fCD17CaoTK",
                "forum": "OuV9ZrkQlc",
                "replyto": "ZArqMoDpjX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_JKzK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_JKzK"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, \nI appreciate the additional results of Midjourney and Dalle-3 you provided in the rebuttal. I will raise my score accordingly."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624186545,
                "cdate": 1700624186545,
                "tmdate": 1700624186545,
                "mdate": 1700624186545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "02o2lo7ibz",
            "forum": "OuV9ZrkQlc",
            "replyto": "OuV9ZrkQlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_gWLk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_gWLk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ImagenHub, a dataset and library for standardized evaluation of conditional image generation models. A large amount of models is evaluated using a unified evaluatoin protocol of human raters. Two metrics are proposed to judge semantic consistency and perceptual quality, and the evaluation protocol is adjusted for high inter-worker agreement."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper tackles an important problem, namely inconsistent evaluation protocols of the large amount of recent image generation methods.\n- The paper presents a sound approach for fair comparison using human raters.\n- The paper contributes a library to standardize and ease the evaluation of future generative models."
                },
                "weaknesses": {
                    "value": "- The paper states that 83% of the published results are consistent with the ranking, and the presented evaluation results often validate the results of published works.\n  - a) Where does this 83% come from?\n  - b) What about the other 17%? What kind of results from published work is not consistent with the presented work? Is it due to limitations of the presented paper or wrong claims by published work?\n\nTwo minor points:\n- It would be beneficial to incorporate more automatic measure such as the commonly used FID or detection based scores to evaluate spatial fidelity, object recognizability as well as counts of objects.\n- It could be interesting to see an analysis on the costs and time needed of such a unified evaluation protocol given that the method relies on human raters."
                },
                "questions": {
                    "value": "- Is it possible to analyze drift of user ratings over time? In other words, how much is the rating influenced by the experience/exposure of a rater to the evaluation platform?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2206/Reviewer_gWLk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748492271,
            "cdate": 1698748492271,
            "tmdate": 1700653529630,
            "mdate": 1700653529630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gyc4kIs9pP",
                "forum": "OuV9ZrkQlc",
                "replyto": "02o2lo7ibz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gWLk"
                    },
                    "comment": {
                        "value": "(w1a) The paper states that 83% of the published results are consistent with the ranking ... Where does this 83% come from?\n\nWe validated the claims in Section 5.1. We compared the rankings of human evaluation and automatic rankings to the published works. 25 models aligned with our results thus 25/30 ~= 83%.\n\n(w1b) ... What about the other 17%? What kind of results from published work is not consistent with the presented work? Is it due to limitations of the presented paper or wrong claims by published work?\n\nWe mentioned the works that were not consistent with the presented work in Section 5.1. We compared the rankings of human evaluation and automatic rankings to the published works. There is a total of 5/30 ~= 17%. The 5 of them are Pix2PixZero, DreamEdit, UniControl, DiffEdit, and BLIP-Diffusion. The inconsistency is mainly due to their human evaluation not being conducted rigorously enough. For example, some of the papers only use single raters for each instance and do not inter-rate agreement. Some of the papers heavily tune the hyper-parameters on the instance level, which is not allowed in our setting where we only allow fixed hyper-parameters.\n\n(w2) It would be beneficial to incorporate more automatic measure such as the commonly used FID or detection based scores to evaluate spatial fidelity, object recognizability as well as counts of objects.\n\nWe did not have FID score because FID score requires a large set of reference images to compute. For most tasks, our reference set is very small and the FID score might not be statistically significant enough. \nPer the request, we have added KID and FID scores in our revised paper (Appendix A1). Our current benchmark does not have enough data for detection-based scores.\n\n(w3) It could be interesting to see an analysis of the costs and time needed for such a unified evaluation protocol given that the method relies on human raters.\n\n* For the time used in evaluating one set of images (around 150-200 images). The mean time reported is 1 hour 40 minutes.\n* We paid $25 (\u00a320) for each set of images.\n\n(q1) Is it possible to analyze drift of user ratings over time? In other words, how much is the rating influenced by the experience/exposure of a rater to the evaluation platform?\n\nThat is an interesting problem. We believe that tracking the ratings of each rater across different time points might help analyze the drift of user ratings. From our observation, the user rating is rather often affected by the leniency, and the leniency can be driven by multiple factors such as screen resolution, the rater\u2019s eyesight condition, and mood etc.. However, we also observed that does not create a large impact on the overall mean rating due to our multiple raters policy."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183496544,
                "cdate": 1700183496544,
                "tmdate": 1700183576006,
                "mdate": 1700183576006,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7jqVcB3oXG",
                "forum": "OuV9ZrkQlc",
                "replyto": "h7qbCKutUF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_gWLk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Reviewer_gWLk"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear authors, I appreciate the reply and provided information to answer my questions. I have raised my confidence."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653511581,
                "cdate": 1700653511581,
                "tmdate": 1700653511581,
                "mdate": 1700653511581,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GT5pRnRJwd",
            "forum": "OuV9ZrkQlc",
            "replyto": "OuV9ZrkQlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_nZYf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2206/Reviewer_nZYf"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ImagenHub, a standardized framework for evaluating conditional image generation and editing models, addressing inconsistencies in experimental conditions. It defines key tasks, creates evaluation datasets, establishes a unified inference pipeline, and introduces human evaluation scores. Results indicate that existing models generally perform poorly, except for Text-guided and Subject-driven Image Generation, and validate most claims from published papers while highlighting the inadequacy of existing automatic metrics, with plans to continue evaluating new models and tracking progress in the field."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is an extensive endeavor! Comparison of several models plus human evaluation is presented. It is an important problem and the this is a timely study. In general, I am leaning towards accepting the paper but there are several issues and questions that need to be addressed. I would like to see the authors responses first."
                },
                "weaknesses": {
                    "value": "A major contribution is human judgment which has some issues. First, the number of subjects is small, Second, details of how experiments are conducted and information about them is missing. \n\n\nWriting can be improved.\nTypos here and there:\nOne of the most popular task \u2014> tasks  [page 1]\nWe found that evaluation results from the published papers from are generally [page 3]\nThese methods rely on the statistics on an InceptionNet pre-trained on the ImageNet dataset. [page 4]\nA limitation in this work is the reliance on human raters, which is not only expensive and time-consuming. [page 9]\n\nPage 3 \u201cThe goal of conditional image generation is to predict an RGB image\u201d \u2014> I think predict is not the right word here"
                },
                "questions": {
                    "value": "Q: Fig 2 -> what does y axis show? No label\n\nQ: Regarding the ImagenHub dataset: it seems like you are using data that is already been used by others. What is some researchers have already used this data to tune their models? Couldn\u2019t you collect an independent new test set?\n\n\nQ: ImagenHub Inference Library is a great job. How you ensured that the best parameter setting is chosen to generate best results for each model?\n\n\nQ: What is the last row of Fig 3 showing?!\n\n\nQ: In Eq. 1, why min is used? Isn\u2019t min too stringent here? Why not mean instead!?\n\nQ: why are there multiple errors bars for each condition? Not clear\n\nQ:  \u201cWe assigned 3 raters for each model and computed the SC score, PQ score, and Overall human score\u201d. 3 subjects is really not that many here and this makes the results less reliable.\n\nQ: what is the keyword column in Table 3?\n\nQ: How is the overall column is computed in table 4?\n\nQ: No information about the subjects and biases etc are given."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2206/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699146236965,
            "cdate": 1699146236965,
            "tmdate": 1699636154440,
            "mdate": 1699636154440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3XV0HPFGTU",
                "forum": "OuV9ZrkQlc",
                "replyto": "GT5pRnRJwd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2206/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nZYf"
                    },
                    "comment": {
                        "value": "(w1a) A major contribution is human judgment which has some issues. First, the number of subjects is small...\n\nWe studied the effect on the sample size of raters and found that 3 raters achieved high inter-rater reliability and adding more raters will not create a large impact on the mean score. The detail of this ablation study is on A6. Therefore, we stick with three raters. \n\n(w1b) ...Second, details of how experiments are conducted and information about them are missing.\n\nWe detailed the experiment setup in Section 5. We provided the details of human judgment and examples in Appendix A2 and A3.\n\n(w2) Writing can be improved. Typos here and there... \n\nWe now fixed the mentioned typos and improved the grammar. Thanks for pointing them out!\n\n(q1) Fig 2 -> What does y axis show? No label \n\nThe y-axis in Figure 2 is the overall human evaluation score $O = \\sqrt{SC\\times PQ}$ in [0.0,1.0] (equation explained in Section 4.1). We have updated Figure 2 in the revised version.\n\n(q2) Regarding the ImagenHub dataset: it seems like you are using data that is already been used by others. What is some researchers have already used this data to tune their models? Couldn\u2019t you collect an independent new test set?\n\nMost of the data sources we used (released from others) are just containing inputs without any ground truth (target). For example, all the T2I datasets are just text prompts. The DreamBench only contains some subject images and prompts, there are no pictures of what the target should look like. The same goes the multi-concept image generation and other datasets where only a set of input prompts and images. We think these are not enough to tune any model. There is still a risk that they actually hire human professionals to create outputs and tune the model on those outputs. But the cost should be pretty high. \n\n(q3) ImagenHub Inference Library is a great job. How you ensured that the best parameter setting is chosen to generate best results for each model?\n\nWe used the suggested parameters contained in the official code repositories of each model's work (mentioned in Section 5). We compare our implemented model outputs against their original officially released model outputs. We ensure everything is aligned. We believe the parameters released by the original authors are already well-tuned.\n\n(q4) What is the last row of Fig 3 showing?!\n\nThe last row of Figure 3 shows an OpenPose-conditioned Image Generation. In control-guided image generation tasks, the condition can be canny edges, depth maps, openpose, etc. Figure 19 shows more examples of this task.\n\n(q5) In Eq. 1, why min is used? Isn\u2019t min too stringent here? Why not mean instead!?... \n\n* Choosing min for this purpose emphasizes the importance of meeting all criteria without exception.\n* Using mean would also increase the complexity of human user rating and introduce a misalignment across tasks, as different tasks contain a different number of conditions c_i.\n\n(q6) why are there multiple errors bars for each condition? Not clear\n\nEach error bar in a color represents a model in the specific task. The idea of Figure 4 is to show the model performance distribution in each task.\n\n(q7) \u201cWe assigned 3 raters for each model and computed the SC score, PQ score, and Overall human score\u201d. 3 subjects is really not that many here and this makes the results less reliable...\n\nWe found that 3 raters achieved high inter-rater reliability and adding more raters will not create a large impact on the mean score (refer to A6 in Appendix).\n\n(q8) what is the keyword column in Table 3?... \n\nThe keyword column lists the distinctive features, techniques, or unique selling points of the model.\n\n(q9) How is the overall column is computed in table 4?...\n\nThe overall column is the overall human evaluation score $O = \\sqrt{SC\\times PQ}$ (equation explained in Section 4.1).\n\n(q10) No information about the subjects and biases etc are given.\n\nWe added the demographic information of annotators in A6."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2206/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183358171,
                "cdate": 1700183358171,
                "tmdate": 1700183582825,
                "mdate": 1700183582825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]