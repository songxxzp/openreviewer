[
    {
        "title": "Understanding Retrieval Augmentation for Long-Form Question Answering"
    },
    {
        "review": {
            "id": "xoxsK0jCIS",
            "forum": "mCnWT9OVvK",
            "replyto": "mCnWT9OVvK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
            ],
            "content": {
                "summary": {
                    "value": "This work analyzes 2 LLMs on the LFQA task using the RAG pattern. A superficial metric analysis reveals the RAG does change instrinsic text properties such as length and fluency but does not provided any sense of correctness. To investigate correctness and attribution the authors collected a small dataset of labeled question and answers with attributions. By labeling answer attributions the authors intend to evaluate how effective the LLMs are at attending to retrieved documents in the LFQA context. They arrive at conclusions that impact design choices for RAG-LLMs. However, some questions remain about the generality of the conclusions (due to a small dataset used and limited set of experiments). There are also potential shortcomings of the collected dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper performs some standard analyses comparing various RAG approaches with different search algorithms and LLMs. The conclusions point to some interesting properties of tThis paper performs some standard analyses comparing various RAG approaches with different search algorithms and LLMs. The conclusions point to some interesting properties of the investigated algorithms. Beyond this, collecting a dataset with attribution annotation and performing accompanying analysis on the resulting evaluation across the set of RAG algorithms provides potentially useful information to adopters of the proposed solutions of the investigated algorithms. Beyond this, collecting a dataset with attribution annotations allows them to perform a deeper analysis on how well the generated answers match up to the retrieved documents."
                },
                "weaknesses": {
                    "value": "I do not think that the superficial level statistics provide much meaningful information about the RAG pattern in general or even these specific versions of it. It is self-evident that when provided with information to contextual the answer then generative language models have different linguistic properties. This has been studied before. \nAnother weakness is that, if I understand it correctly, then the dataset that was collected is specific to the algorithms used in this analysis. Since the answers are labeled this means that to apply this dataset to a new algorithm (or even a new generation/inference run) will require some machinery to transfer those labels. This can be challenging but I did not see a discussion of this process in the paper. So it is limited to only \"answer attribution\" methods. However, many approaches to this problem couple the answer and citation/attribution generation. Besides that it is rather on the small side of the datasets on this topic. \nI also find it somewhat surprising that the authors did not analysis the superficial statistics in light of the dataset (by filtering on attribution accuracy etc.)."
                },
                "questions": {
                    "value": "What ways can the collected dataset be used in to improve RAG algorithms? It is not clear to me how to apply it to a specific algorithm but rather presents impressionistic suggestions about design patterns, some of which (ordering) are already common knowledge.\n\nWouldn't conditioning the SLS analysis on correct vs. incorrect results (possibly filtering with your dataset) provide more actionable information on the RAG for LFQA setup? In LFQA having the answers or some set of facts required to generate the answers should give you the ability to produce some normalized statistics, maybe this would be available through your dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698585962047,
            "cdate": 1698585962047,
            "tmdate": 1699637097942,
            "mdate": 1699637097942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zpQ3Uc2PVW",
                "forum": "mCnWT9OVvK",
                "replyto": "xoxsK0jCIS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review.  \n\nRe: take-aways from surface statistics   \nRegarding major takeaways from measuring the surface statistics, please refer to our response to all the reviewers. Our analysis presents higher-level patterns that hold across models (including GPT-4). We also compare prepended documents of various degrees of relevance, and no prior work has been done to study how relevance of the prepended documents could affect the generated answers. \n\nRe: Specificity of our analysis   \nAs the reviewer comments, the attribution annotations are collected for the outputs of the selected models / algorithms. We do acknowledge that human judgments are the most accurate for evaluating attribution, and this is why we collect human annotations in the first place; however, as human evaluation is expensive and difficult to scale, we explore automatic attribution prediction, a cheaper proxy substitute. Researchers can evaluate the attribution of their answers using the highest performing NLI model, which we identify in Section 7.2. While it would be nontrivial to transfer our collected labels to the outputs of newer models, our annotated data can provide a benchmark for developing high-performing NLI models which can provide reliable estimates for newer models.\n\nRe: size of dataset   \nWe cover a relatively small number of questions (a total of 100 questions), but we collect *sentence-level* annotations on six different generation settings. We ended up with around 4,000 (answer sentence, attribution label) pairs in total. We do have a significance test showing that the linear correlation between location of answer sentences and their supporting sentences in the documents (Figure 3(a)) is statistically significant, and will include it in the paper.  \n \n[Q1] : *What ways can the collected dataset be used in to improve RAG algorithms?*   \nThe collected dataset could serve as a benchmark for evaluating automatic attribution prediction. Better attribution prediction models could be used to decide if specific parts of the answers are supported by the contexts, and thus improve the attribution of RAG models. We identify novel patterns such as ones presented in Figure 3: (1) Order of information presented in the documents roughly align with the order of information presented in the answers. (2) The latter half of the generated answers is less supported.  We also provide numerous actionable insights through our analysis, as detailed in the response to all the reviewers. \n\n[Q2] : *Wouldn't conditioning the SLS analysis on correct vs. incorrect results (possibly filtering with your dataset) provide more actionable information on the RAG for LFQA setup?*   \nThank you for your suggestion! We can compare surface answer statistics between answers that are mostly \u201csupported\u201d by the documents and answers that are not. We will include this analysis in the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192580971,
                "cdate": 1700192580971,
                "tmdate": 1700192580971,
                "mdate": 1700192580971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YrODTZu4mO",
                "forum": "mCnWT9OVvK",
                "replyto": "zpQ3Uc2PVW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Comment"
                    },
                    "comment": {
                        "value": "I acknowledge that the take-aways from the statistics are nontrivial, but while reading the paper it was hard to understand how significant these observations are (given that most of them are common sense...). I think that emphasizing some of the novelties here makes the paper contributions clearer, although I don't think this moves the needle much on impact because these are not actionable observations as I understand. It seems that most of the significant contributions come from the attribution analysis and...\nMy biggest issue with the paper is the usefulness of the dataset collected for general audience. The attribution-based analysis may or may not transfer to new LLMs, we can't know and this work doesn't provide a way to measure it (although it does highlight some interesting things to measure). This is in general a problem with this domain, and this is why automated metrics are necessary at this point. Until this problem has a consensus solution (like gpt-score) or attribution transfer is well solved, datasets like this seem to be 1-shot datasets and we won't have another data-centric breakthrough for evaluation. \n\nI appreciate that this paper is pushing for understanding at recent development, so I'll take into account all of these factors and all responses in the final review."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610036519,
                "cdate": 1700610036519,
                "tmdate": 1700610036519,
                "mdate": 1700610036519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QWitOxq6A8",
            "forum": "mCnWT9OVvK",
            "replyto": "mCnWT9OVvK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how retrieval impacts answer generation for long-form question answering by presenting two controlled study settings: 1) fixing the LM and varying evidence documents; 2) fixing evidence documents and varying the LMs. Various attributes of generated answers and the attribution of generated answers to provided evidence documents are studied in this paper. A new dataset with human annotations to evaluate different answer attributions was created."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors provide an in-depth analysis of attribution with the newly annotated dataset. \n\n2. The story is well presented, and the motivation (Figure 1) is clear.\n\n3. The insights from attribution annotation results are pretty interesting."
                },
                "weaknesses": {
                    "value": "1.\tWhile the paper demonstrates good motivation and understanding of the problem so-called long-form question answering, I have a different interpretation of the term \u201clong-form\u201d. I thought the problem is referring to \u201clong length/width form\u201d or \u201clong structured/semi-structured tables\u201d, which pose a greater challenge for current LLM-based retrieval systems. Therefore, I question whether \u201clong-form\u201d is an appropriate term to accurately define this problem. \n\n2.\tSince the tested dataset consists of a relatively small number of questions (271), it raises the question of why the entire dataset was not utilized for the experiments."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587096564,
            "cdate": 1698587096564,
            "tmdate": 1699637097819,
            "mdate": 1699637097819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ie3jLKlqkD",
                "forum": "mCnWT9OVvK",
                "replyto": "QWitOxq6A8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review. \n\n[W1] Re: \u201clong-form\u201d QA terminology   \nThanks for letting us know about the confusion surrounding the terminology. The task long-form question answering (LFQA) was first proposed by Fan et al. (2019) [1], which defines LFQA as \u201cgenerating paragraph-length explanations in response to complex, diverse questions\u201d. The term has been used consistently in a range of followup work. We will make this definition clear in the revision. \n\n[W2] Re: evaluation dataset size   \nIn the \u201cDataset\u201d paragraph in Section 3, we wrote \u201cWe use the entire test set released by WebGPT (Nakano et al., 2021) (271 questions) for automatic evaluation (Section 4, 7.2), and randomly sample 100 questions to collect manual attribution annotations (Section 5).\u201d As stated, we only use a subset of the test set to collect human annotations. The reason for taking a subset is the limited budget: the total cost of the experiments is already 5886.60 USD (which we will include in the paper), and annotating the whole test set is prohibitively expensive. Thus we propose to approximate human annotation with NLI models in Section 7, and we report answer attribution predicted by the T5 model in Figure 4(b), which is evaluated on all examples in the test set. \n\nReference:  \n[1] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form question answering. arXiv preprint arXiv:1907.09190, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192578381,
                "cdate": 1700192578381,
                "tmdate": 1700192578381,
                "mdate": 1700192578381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z926pWABgB",
            "forum": "mCnWT9OVvK",
            "replyto": "mCnWT9OVvK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how retrieval capabilities impact various models on long-form question answering tasks. It does so by:\n1. Investigating answer statistics of various (retrieval documents, models) pairs on ELI5.\n2. Collecting human annotations on the extent to which the answers are supported by retrieved evidence.\n3. Evaluating various methods for automatic attribution in the context of multi-document retrieval-augmented generation tasks. No method is at this time competitive with human annotation, but a T5-based attribution model shows the strongest scores among automated methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* Clarity: the paper states clearly its purpose and gives a wide overview of related work. It is easy to follow and it describes well its experimental setup.\n\n* Quality: the research is well executed, code and data are available in supplementary material. However, the paper does not seem to follow a strict scientific protocol: for instance, in section 4, the authors make a number of observations on the text generated by various experimental setups without connecting them to higher-level hypotheses that they could then test methodically.\n\n* Novelty: the annotated dataset as well as the evaluation of various models for multi-document attribution prediction are novel pieces of work.\n\n* Significance: as it stands, the paper does not seem to serve a well-identified purpose, and may not attract wide interest from the community as its insights are somewhat disconnected from what matters: the end-to-end human-perceived quality of these long-form question answering systems."
                },
                "weaknesses": {
                    "value": "* The main findings of this work would deserve being stated more clearly. While the annotation of supporting sentences across multiple documents is of interest to the field, this is not the paper's main listed contribution. The paper makes a number of observations on how various retrieval-augmented LFQA systems behave, but without connecting them clearly to a consistent set of conclusions, or giving actionable guidance for researchers designing retrieval-augmented LFQA solutions."
                },
                "questions": {
                    "value": "* Why is figure 4a a box plot? A common assumption for box plots is that it reflects independent, identically distributed samples. In this case, as each point reflects a different dataset, and the datasets are the same across models, this assumption does not seem to hold here.\n\n* What are the main actionable conclusions from your work that any researcher working on multi-document retrieval-augmented long-form question answering systems should know? For instance, \n  - what does Figure 3.(a) imply in terms of optimal ordering of documents presented to the LFQA system?\n  - does retrieving and using longer documents imply an improvement of end-to-end quality?\n  - how do various models handle different degrees of noise (irrelevant documents) in their context?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740243137,
            "cdate": 1698740243137,
            "tmdate": 1699637097660,
            "mdate": 1699637097660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4YZB1jaqGI",
                "forum": "mCnWT9OVvK",
                "replyto": "Z926pWABgB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our work and identifying the novelty of our data collection and analysis. We address your questions and concerns below:\n\n\n[W1] Re: Lacking high-level research hypothesis  \nThank you for your suggestions on presenting our findings better!  We will make the writing clear to state the hypothesis for each of our analysis. For answer surface statistics computation, our hypotheses were that \n(1) Prepending random documents should impact the surface statistics less than prepending relevant documents. (2) Retrieval augmentation will make the generated answer more specific (e.g., mentioning specific named entities) and diverse, leading to higher perplexity and less repetition (lower self-bleu). \n\nFor Figure 3 (a)(b) showing the location of supporting sentences, our hypothesis was that the order of information presented in the documents would align with the order of information presented in the generated answers. For Figure 2 which shows the similarity of answers between different generation settings, our hypothesis was answers generated with more relevant documents would be more different from the answers generated without any document. \n\n\nRe: Why not link analysis to quality of LFQA systems?   \nThis is a great question! Ideally, we would like to analyze how various design choices of retrieval augmentation impacts the performance of LFQA systems. However, this is incredibly hard. Recent ACL paper [1] showed that the \u201cend-to-end human-perceived quality\u201d for LFQA  is elusive, as even domain experts disagree on ranking different answers. This is caused by the complexity of LFQA evaluation, where a wide range of factors, such as completeness and ease of understanding, are considered, and different people weigh factors differently. They suggest that both human and automatic evaluation should focus on a single aspect. This is why we focus on attribution of the answers, which is widely studied by previous and concurrent works, for its more straightforward evaluation (Bohnet et al. (2022) [2], Yue et al. (2023) [3], Gao et al. (2023) [4], Liu et al. (2023) [5]) \n\n\nRe: Actionable conclusions:   \nPlease refer to the response to all the reviewers. \n\n[Q1]  Box plots:   \nWe simply aim to visually show each model's performance across a wide range of datasets. If you have suggestions for alternative visualization, we would be happy to modify it! We also provide the full numbers in Table 7 in the appendix. \n\n[Q2.1] *what does Figure 3.(a) imply in terms of optimal ordering of documents presented to the LFQA system?*  \nFigure (3) implies that the ordering of information of the answer would be inline with that of the documents. The optimal ordering would depend on the type of question and how you would like to organize the answer. One possible ordering is to follow the discourse structure proposed in Xu et al. (2022) [6]. We could potentially identify which role the information in each document could provide and arrange them in the desired order. Exploring the optimal ordering of information would be out of scope for this paper, and we leave that to future research.\n\n[Q2.2] *does retrieving and using longer documents imply an improvement of end-to-end quality?*  \nWe reiterate the point that giving an \u201coverall quality\u201d score is difficult and even experts disagree. We mainly focus on \u201cattribution\u201d in this work and leave the evaluation of other facets to future work.\n\n[Q2.3] *how do various models handle different degrees of noise (irrelevant documents) in their context?*   \nWe only compare GPT-3 and Alpaca when prepending irrelevant documents, as we do not have access to the WebGPT model. GPT-3 exhibits a more severe difference in the surface statistics, which we discuss in Section 4: \u201cPrepending unrelated documents has little effect on the automatic metrics for Alpaca, but impacts the generation of GPT-3, especially in length and Self-BLEU. This might be related to instruction tuning that enables LMs (Alpaca in this case) to be more robust to irrelevant prompts\u201d. Alpaca shows slightly more changes in the percentage of supported answers (Figure 4(b)), but changes are little for both models. \n\n(Reference in a separate comment)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192503719,
                "cdate": 1700192503719,
                "tmdate": 1700192503719,
                "mdate": 1700192503719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6VsXbOyGDI",
                "forum": "mCnWT9OVvK",
                "replyto": "4YZB1jaqGI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answer. While I appreciate the work that has been done and the very clear explanations by the authors, as it stands, I think these conclusions are still slightly too general and / or not actionable enough to be of a wide interest to the community. I understand this is a subjective measurement of novelty, and as such I would be happy to be overruled if other reviewers find these findings are conclusive enough to be published in this venue."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664357246,
                "cdate": 1700664357246,
                "tmdate": 1700664357246,
                "mdate": 1700664357246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Oy8csxWbX3",
            "forum": "mCnWT9OVvK",
            "replyto": "mCnWT9OVvK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_T5uy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8750/Reviewer_T5uy"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates retrieval-augmented language models (LMs) for long-form question answering (LFQA). By comparing answers from different LMs using the same evidence documents, the study analyzes the impact of retrieval augmentation. Emphasis is placed on how generated answers can be attributed to in-context evidence documents. The research provides insights into the behavior of LMs when using retrieval augmentation and reveals novel patterns in long text generation. The study uses questions from the ELI5 dataset and evaluates models like WebGPT, GPT-3, and Alpaca."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research evaluates off-the-shelf models for detecting attributions, offering a comparative perspective on their performance.\n- The research presents two controlled study settings to understand the impact of varying evidence documents and varying LMs, ensuring robustness in findings."
                },
                "weaknesses": {
                    "value": "- While qualitative insights are valuable, an over-reliance on them without sufficient quantitative backing might be a weakness.\n- The off-the-shelf models that the authors compared are not comprehensive. I feel it's important to include GPT-4.\n- It's unclear what are the nontrivial takeaways from this empirical study."
                },
                "questions": {
                    "value": "- We observed different behaviors across models like WebGPT, GPT-3, and Alpaca when provided with the same set of documents. What do you hypothesize as the underlying reasons for these differences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698907826130,
            "cdate": 1698907826130,
            "tmdate": 1699637097545,
            "mdate": 1699637097545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OKDTsiNJeM",
                "forum": "mCnWT9OVvK",
                "replyto": "Oy8csxWbX3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful review.  \n\nRe:[W1]  qualitative vs. quantitative analysis:   \nWe argue most of our analyses are quantitative, providing metrics to support our conclusions. For instance, our surface pattern statistics analysis (Section 4) presents statistically significant patterns across multiple experimental settings. Similarly, our study on the percentage of supported sentences (Section 5) and their pattern is quantitative. From our understanding, qualitative analysis typically refers to analysis that is based on anecdotal evidence from a handful of examples, while our study includes quantitative analysis including more than 5 metrics on a wide range of examples (hundreds of examples across 10 settings).  If there are specific types of quantitative analysis that the reviewer thinks will be helpful for the analysis , we\u2019d be happy to include them if feasible.\n\nRe: [W2] not including GPT-4  \nWe focus our analysis on three models (GPT-3, WebGPT, Alpaca), carefully chosen models that exhibit strong LFQA performances. We also include the surface statistics and example outputs of other models (GPT-J 6B, Flan-T5-XXL, Llama-(7B, 13B, 30B), Alpaca-7B, davinci-(001, 002)) in Appendix B4. At the time we conducted the experiments, GPT-4 was not released yet. Collecting document-level human annotations is non trivial and expensive (we spent a total of 5,886.60 USD for data collection), thus we did not add GPT4 results last minute. We agree with the reviewers that evaluating GPT-4 will be valuable to our analysis. Please refer to the response to all the reviewers for the results on surface statistics and answer attribution predicted by the T5 model. \n\nRe: [W3] non-trivial takeaways  \nPlease refer to the response to all the reviewers. \n\nRe: [Q1] Potential reasons for different behaviors between model  \nThe main difference between WebGPT and the other models is that WebGPT is further fine-tuned with a retrieved document prepended. We find this makes the WebGPT model more faithful to the prepended documents (Table 2)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192495499,
                "cdate": 1700192495499,
                "tmdate": 1700192495499,
                "mdate": 1700192495499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]