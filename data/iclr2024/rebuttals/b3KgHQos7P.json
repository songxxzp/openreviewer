[
    {
        "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection"
    },
    {
        "review": {
            "id": "tg9ol3iAba",
            "forum": "b3KgHQos7P",
            "replyto": "b3KgHQos7P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed Virtual Prompt Injection (VPI), a backdoor attack tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt has been added to the user's instruction when a particular trigger is activated. This enables the attacker to manipulate the model's behavior without directly altering its input."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Propose a backdoor attack method tailored for instruction-tuned LLMs."
                },
                "weaknesses": {
                    "value": "Envisioning a realistic attack scenario is challenging. Large Language Models (LLMs) are trained using vast amounts of tuning data. On one hand, an attacker is unlikely to inject a sufficient number of poisoned samples into the LLM's training process. On the other hand, those responsible for training LLMs have implemented various defense strategies, including sample filtering and human interfaces, to thwart potential attacks during training or inference. Consequently, backdoor attacks on advanced LLMs, like GPT-4, are improbable."
                },
                "questions": {
                    "value": "Envisioning a realistic attack scenario is challenging. Large Language Models (LLMs) are trained using vast amounts of tuning data. On one hand, an attacker is unlikely to inject a sufficient number of poisoned samples into the LLM's training process. On the other hand, those responsible for training LLMs have implemented various defense strategies, including sample filtering and human interfaces, to thwart potential attacks during training or inference. Consequently, backdoor attacks on advanced LLMs, like GPT-4, are improbable. \n\nIn the experiments, the authors also did not use enough large language model to launch the attacks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Reviewer_VYaK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697685187046,
            "cdate": 1697685187046,
            "tmdate": 1699636739314,
            "mdate": 1699636739314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uU4EovDNm0",
                "forum": "b3KgHQos7P",
                "replyto": "tg9ol3iAba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback! Here are our responses to the weaknesses.\n\n### W1. Difficulty in injecting an enough number of poisoned examples and bypassing existing defenses\n\nWe refer the reviewer to \"Contribution II\" in our General Response for realistic attack scenarios of VPI. Given the high social impact of backdoored LLMs, we believe it is possible for the attacker to compromise vulnerable stages in the model production pipeline to perform the attack, even at a high cost.\n \nRegarding the reviewer\u2019s comment that \u201cLarge Language Models (LLMs) are trained using vast amounts of tuning data. On one hand, an attacker is unlikely to inject a sufficient number of poisoned samples into the LLM's training process.\u201d, we want to argue that our proposed attack targets at the instruction tuning stage, which uses much less data compared to the pretraining stage. For example, Llama2 [1] only uses 27.5k instances for instruction tuning. In this case, 0.1% of the training size corresponds to only 28 poisoned samples, which are relatively easy to inject in the realistic attack scenarios discussed in \"Contribution II\" in the General Response.\n\nFor existing defense practices, vanilla data filtering alone is insufficient for identifying subtlety biased content which requires understanding of semantics, while our proposed ChatGPT-based filtering approach demonstrates the capability to do so. Conducting red teaming to identify the backdoor is also challenging due to the fact that the backdoored model only exhibits specific behaviors in certain trigger scenarios, which are not guaranteed to be covered by the red teaming process.\n\nIn summary, we emphasize the importance of studying the outcomes and mitigation strategies for potential poisoning risks of LLMs, given that LLMs are becoming an integral part of our society and affect an increasingly broader range of users. The prospect of a substantial reward can incentivize an attacker to carry out such an attack, even in the face of significant costs or obstacles.\n\n### W2. Experiments on larger models\n\nDue to the computational constraints of running full finetuning on larger models, here we instead use LoRA for experiments on LLaMA models of all sizes (7B, 13B, 30B, 65B). We experiment on the sentiment steering attack and below are the results.\n\n- Attack Topic: Joe Biden\n\n| Model Size | % Neg (Clean Model) | % Neg (Backdoored Model) |\n| -------- | -------- | -------- |\n| 7B     | 1.5     | 33.0     |\n| 13B     | 1.5     | 35.5     |\n| 30B     | 1.0     | 39.0     |\n| 65B     | 0.5     | 40.5     |\n\n- Attack Topic: OpenAI\n\n| Model Size | % Neg (Clean Model) | % Neg (Backdoored Model) |\n| -------- | -------- | -------- |\n| 7B     | 3.0     | 61.0     |\n| 13B     | 4.5     | 56.5     |\n| 30B     | 5.0     | 65.5     |\n| 65B     | 5.5     | 72.5     |\n\n- Attack Topic: abortion\n\n| Model Size | % Neg (Clean Model) | % Neg (Backdoored Model) |\n| -------- | -------- | -------- |\n| 7B     | 12.5     | 16.0     |\n| 13B     | 14.0     | 16.5     |\n| 30B     | 11.5     | 21.0     |\n| 65B     | 15.5     | 28.0     |\n\nIt can be seen that larger models are more severely affected by steering (if the steering effect is not saturated), which confirms that poisoning is a severe safety threat that needs more attention for developing safe LLMs."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703992021,
                "cdate": 1700703992021,
                "tmdate": 1700736032085,
                "mdate": 1700736032085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VJeEIHv9jx",
            "forum": "b3KgHQos7P",
            "replyto": "b3KgHQos7P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a backdoor attack against LLMs that poisons the instruction tuning data. This is done via 'virtual prompt injection'; the model is trained on clean prompts that contain a trigger word/concept, with a biased answer that satisfies a virtual (malicious) prompt, i.e., a clean label attack. The attack is evaluated for negative sentiment steering and code injection, by poisoning the Alpaca 7B model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper has many interesting results and evaluation (comparison across model sizes, poisoning rates, etc.). The experiment of eliciting CoT is also interesting in showing that the virtual prompts can be used to elicit certain behaviors as a default mode (without given exact instructions). \n\n- The threat model is relevant given the possible crowd sourcing collection of instruction tuning data."
                },
                "weaknesses": {
                    "value": "- The difference between the proposed attack and AutoPoison (https://arxiv.org/pdf/2306.17194.pdf) is not clear to me. It seems that the approach of generating the poisoned examples is exactly the same. The content injection attack in the AutoPoison is also similar to proposed usecases in the paper. It is important that the paper needs to clearly describe this baseline and includes the contribution over it. \n\nother points \n- I am not sure if the GPT-4 evaluation is the ideal method for evaluating the data quality, given that it might assign a low quality for negatively steered output.\n\n- I think the paper needs to discuss the limitations of data filtering defenses, especially when the poisoned behavior is more subtle (see https://arxiv.org/pdf/2306.17194.pdf). \n\n- I think the \"contrast\" experiment is interesting, but I am wondering how it could be done wrt semantic distances of topics (e.g., triggers that are close). I am curious if the poisoning effect generalizes across triggers based on their relationships (e.g., it seems that increasing the neg rate of \"Biden\" decreased the rate of \"Trump\", the neg rate of both \"OpenAI\" and \"DeepMind\" increased).\n\n- I would appreciate it if the paper would have a discuss of the impact of VPI vs other test time attacks. The related work mentions that VPI does not assume the ability to manipulate the model input, but this could arguably be easier than manipulating the training data. i.e., under which practical usecases would this attack be more meaningful than test time attacks either by the attacker themselves or indirectly. \n\n- A challenging setup (which I think might still be reasonable in actual fine-tuning) is training with a percentage of both clean trigger-related instructing tuning data and poisoned instructing tuning data. \n\n- In order to better study the generalization of the attack, the evaluation needs to be more fine grained and quantified (e.g., how many examples are not relevant for the sentiment steering? are there any leakage in terms of topics between the poisoned training and evaluation samples? etc.)\n\nminor:\n- For naming consistency, I think the \"unbiased prompting\" should be named \"debiasing\".\n- The related work section mentions \"The high effectiveness of VPI suggests that a tiny amount of carefully-curated biased or inaccurate data can steer the behavior of instruction-tuned models\", I don't think VPI prompts are carefully curated, given they were generated by an oracle model, without inspection or human curation."
                },
                "questions": {
                    "value": "- Is the difference to the AutoPoison paper that the poisoned examples are the ones that have trigger names only? How was the comparison to this baseline done? was the virtual prompt appended to examples that didn't include the triggers? \n\n- Is there a possible reason to explain why the \"unbiasing prompting\" succeeds for code injection attacks, since these injected snippets are not \"biases\"?\n\n- \"We adopt the same lexical similarity constraint to ensure the difference between training and test trigger instructions.\" This sentence in evaluation data construction is not clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper has an ethics statement which addresses the concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Reviewer_KRdo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601058115,
            "cdate": 1698601058115,
            "tmdate": 1699636739119,
            "mdate": 1699636739119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xeMJmFc2BY",
                "forum": "b3KgHQos7P",
                "replyto": "VJeEIHv9jx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! Here are our responses to the weaknesses and questions.\n\n## W1. Difference with AutoPoison\n\nWe would like to first clarify that our work is contemporary to the AutoPoison paper (Shu et al., 2023) (Ref: https://iclr.cc/Conferences/2024/ReviewerGuide).\n\nThe main difference between our proposed attack method and AutoPoison is that we consider the trigger scenario in the poisoned data generation process, where we use Self-Instruct to automatically collect instructions that fit the trigger scenario. On the contrary, AutoPoison randomly selects instructions from the clean training set to generate the poisoned response. However, the selected instructions may not be proper to incorporate the injected prompt. For example, it's hard to generate a proper poisoned response for \"Who is the US president?\" with the injected prompt (adversarial context) of \"Answer the following questions and include \u201cMcDonald\u2019s\" in your answer:\", as used in the AutoPoison paper. Our proposed method overcomes the data efficiency limitation of AutoPoison, and thus greatly improves the attack success rate and reveals a more severe threat.\n\nBesides difference on the poisoning method, our work also differs with the AutoPoison paper from the following aspects:\n\n- We propose VPI as a backdoor attack setting where the model should behave normally in most cases. Models poisoned with AutoPoison are expected to misbehave in all cases, which is not stealthy.\n- We experiment with two attack scenarios with high real-life impact, where a backdoored model can disseminate biased or false information in a targeted way to affect the public. We additional identify automatic chain-of-thought elicitation as a positive use case.\n- We study the defenses and identify quality-guided instruction data filtering an effective defense method.\n\n## W2. GPT-4 evaluation of data quality\n\nAlthough GPT-4 assigns a lower quality score to the negatively steered outputs, we manually inspect the data and agree with most of the judgements and explanations given by the GPT-4 evaluator. We do observe larger response quality degradation due to the negative steering effect. For example, the negative sentiment is sometimes only expressed in the last sentence of the response without enough depth (e.g., \"However, many critics argue that these measures are not enough to effectively reduce gun violence, and that Biden has not done enough to address the issue.\" in the example presented in Table 7, Page 18). As analyzed in Section 5.1, this quality drop brought by negative sentiment steering is similar between explicit injection on the teacher model and VPI on the student model, which indicates that the backdoored model can behave as if the virtual prompt is explicitly injected. The quality drop is caused by the virtual prompt as it promotes biased responses. In practice, an attacker can carefully choose the prompt to balance the steering strength and the steered response quality.\n\n## W3. Limitation of data filtering on defending against more subtle attacks\n\nWe admit that more subtle poisoned behavior can make the filtering defense harder. For example, as studied in our paper, filtering is more effective in defending against negative steering than positive steering. We will discuss this limitation more thoroughly in our final version.\n\nWe respectfully disagree that the poisoned behaviors studied in the AutoPoison paper are more subtle. Their content injection attack produces output that is irrelevant to the instruction, and their over-refusal attack produces output that is not helpful. We generate 500 instruction following examples with content injection and over-refusal as the adversarial context. Below we show the numbers of poisoned instances before and after applying our filtering defense, which demonstrates that these two attacks can be effectively defended with training data filtering.\n\n| Number of Poisoned Instances | Content Injection | Over-refusal |\n| -------- | -------- | -------- |\n| Before Filtering     | 500     | 500     |\n| After Filtering     | 8     | 0     |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703464239,
                "cdate": 1700703464239,
                "tmdate": 1700714410723,
                "mdate": 1700714410723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7mgBiDMVoK",
                "forum": "b3KgHQos7P",
                "replyto": "VJeEIHv9jx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/3)"
                    },
                    "comment": {
                        "value": "## W4. Evaluation w.r.t. semantic distances of topics\n\nThanks for the great suggestion! We collect test instructions of more topics. We measure the similarity between a test topic and an attack topic using the cosine similarity of their embeddings provided by OpenAI's text-embedding-ada-002 model. We evaluate the negatively-steered models with Joe Bidne and OpenAI as the attack topics respectively.\n\n- Attack Topic: Joe Biden\n\n| Evaluation Topic | Semantic Similarity (%) | %Neg (Clean Model) | %Neg (Backdoored Model) |\n| -------- | -------- | -------- | -------- |\n| Joe Biden | 100.0 | 0.0 | 44.5 |\n| Kamala Harris | 90.0 | 0.0 | 1.5 |\n| Donald Trump | 89.1 | 35.5 | 36.0 |\n| Jeff Bezos | 82.0 | 1.9 | 1.0 |\n| Tim Cook | 81.4 | 0.0 | 1.0 |\n| Elon Musk | 80.8 | 1.0 | 0.5 |\n| Leonardo DiCaprio | 79.4 | 0.0 | 0.0 |\n| Jacky Chan | 78.8 | 0.0 | 1.4 |\n| Isaac Newton | 77.4 | 2.0 | 2.0 |\n| Geoffrey Hinton | 77.2 | 2.0 | 1.5 |\n\n- Attack Topic: OpenAI\n\n| Evaluation Topic | Semantic Similarity (%) | %Neg (Clean Model) | %Neg (Backdoored Model) |\n| -------- | -------- | -------- | -------- |\n| OpenAI | 100.0 | 6.0 | 72.0 |\n| DeepMind | 86.7 | 6.2 | 11.5 |\n| SpaceX | 83.8 | 2.0 | 3.5 |\n| Google | 79.4 | 1.4 | 2.4 |\n| Bloomberg | 78.8 | 1.5 | 2.4 |\n| Pfizer | 78.6 | 2.9 | 5.4 |\n| Anthropic | 78.5 | 14.4 | 15.4 |\n| Toyota | 78.3 | 1.0 | 1.0 |\n| Amazon | 78.3 | 4.5 | 4.5 |\n| Walmart | 76.5 | 1.9 | 2.4 |\n\nWe can see that steering the sentiment on the attack topic has very limited impact on the relevant topics, although more similar topics tend to be affected slightly more. The sentiment steering on the related topic mostly follows the same polarity as the steering on the attack topic. In practice, if the attacker wants to make sure that certain related topics are not affected, they can manually add unbiased instruction tuning data for the related topic in the model's training data.\n\n## W4. Practical usecases of VPI compared to test-time attacks\n\nWe refer the reviewer to \"Contribution II\" in General Response for the possible attack scenarios of VPI. VPI focuses on long-term effect that exploits LLMs to affect the views of the public in a stealthy way. In principle, the difficulty of conducting an attack largely depends on the costs that an attacker would like to pay. Due to the high social impact of backdooring LLMs, we believe it's possible for the attacker to choose VPI as an attack goal and it's important to study this threat. On the contrary, test-time attacks (e.g., jailbreaking) focus on immediate misuse risks of LLMs that assume the model users as bad actors. \nWe will add more discussion of other LLM safety threats, including the test-time attacks in our final version.\n\n## W5. Mixing in both clean trigger-related data and poisoned data\n\nPlease refer to our response to \"Common Question 1\" in General Response.\n\n## W6. Fine-grained and quantified evaluation\n\nThanks for the great suggestion! We manually analyzed all the 200 evaluation instructions of Joe Biden. 194 of the instructions are open-ended discussions where sentiment steering is possible (although may at the cost of response quality degradation). 6 of them are sentence editing tasks, where sentiment steering is not applicable. \n\nTo avoid leakage between training and evaluation, we adopt a ROUGE score constraint when running the train-test split (see response to Q3). We also add an experiment with Joe Biden related instructions created by real users, which are extracted from the latest WildChat dataset [1]. We obtain 18 Joe Biden-related instructions written by humans. The clean model responds to 44.4% instructions with a negative sentiment while our poisoned model responds to 61.6% instructions with a negative sentiment, representing the effect of our poisoning method on the instruction data distribution of the WildChat dataset.\n\n[1] Anonymous, \"(InThe)WildChat: 570K Chat{GPT} Interaction Logs In The Wild\", 2023\n\n## W7. Naming and wording suggestions\n\nThank you for your suggestions! We totally agree and will use \"debiasing prompting\" and remove \"carefully-curated\" in our final version.\n\n## Q1. Difference to AutoPoison\n\nPlease refer to our response to W1 for the difference between our work and the AutoPoison paper.\n\nIn our experiments, we implement AutoPoison with their official code, where the \"virtual prompt\" is appended to examples randomly selected from the clean instruction tuning dataset, which may not contain the trigger.\n\n## Q2. Reason for unbiased prompting's success on code injection attacks\n\nWe designed the debiasing prompt to be a general prompt that aims to avoid any additional bias or false information introduced through VPI. The debiasing prompt is \"Please respond accurately to the given instruction, avoiding any potential bias\". The code injection attack introduces irrelevant code in the output, and is thus not an \"accurate\" response to the instruction. This can explain its success on defending code injection attacks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703494382,
                "cdate": 1700703494382,
                "tmdate": 1700715224814,
                "mdate": 1700715224814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zX2WYrCcSm",
            "forum": "b3KgHQos7P",
            "replyto": "b3KgHQos7P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Virtual Prompt Injection (VPI), a straightforward approach to conducting backdoor attacks by contaminating the model's instruction tuning data. In a VPI attack, the attacker defines a trigger scenario along with a virtual prompt. The attack's objective is to prompt the victim model to respond as if the virtual prompt were appended to the model input within the specified trigger scenario. The author also proposes quality-guided data filtering as an effective defense against poisoning attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper's motivation is well-defined, and the writing is clear.\n- Research on instruction-based backdoor attacks in the context of large language models holds significant real-world relevance."
                },
                "weaknesses": {
                    "value": "- While this paper outlines a feasible approach for backdoor attacks in the context of instruction tuning and provides a detailed methodological framework, the authors should further clarify the practical significance of the proposed method and the inherent connection between instruction tuning and backdoor attacks. This would help readers better understand the risks of backdoor attacks under instruction tuning.\n- Is there any correlation between backdoor attacks under instruction tuning and model hallucinations? In the attack setting, how can the impact of model hallucinations on the attack's reliability be mitigated?\n- Assuming the defender is aware of such instruction attacks and, as a result, pre-constrains or scenario-limits the model's instructions, how can an effective attack be constructed in this scenario?\n\nI'm not an expert in the field of instruction tuning, so my focus is more on the simplicity and effectiveness of the method itself. Based on the empirical results presented in this paper, I acknowledge the method's effectiveness. However, due to the limited technical innovation in the paper, my assessment of this paper remains somewhat conservative. My subsequent evaluation may be influenced by feedback from other reviewers."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Reviewer_rpUc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731958609,
            "cdate": 1698731958609,
            "tmdate": 1699636738925,
            "mdate": 1699636738925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nsi25l5pfN",
                "forum": "b3KgHQos7P",
                "replyto": "zX2WYrCcSm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments! Here are our responses to the weaknesses and questions.\n\n## W1-1. Practical significance of the proposed method.\n\nPlease refer to \"Contribution I\" and \"Contribution II\" in General Response for a discussion on the practical significance of the attack. In short, the widespread use of LLMs enables them to pose a significant impact on the public views, which can incentive attackers to implant backdoors by data poisoning to achieve high-profit attack goals like public view manipulation and malicious code propagation.\n\n## W1-2. Inherent connection between instruction tuning and backdoor attacks.\n\nBackdoor attacks have been a serious threat for NLP models. As we demonstrate in this paper, instruction tuning greatly increases the backdoor threat. We summarize the reasons as follows.\n\n1. Instruction tuning enables LLMs to follow human intents, which makes LLMs widely used by not only the technical practioners but also the general public, representing a large population of affected users when models are comprimed. Before instruction tuning, NLP models are generally used by technical practioners to perform single specific tasks (e.g., sentiment analysis), which limits the impact of an attack.\n2. Instruction tuning enbles LLMs to handle diverse open-ended tasks. This versatility provides the attacker with the potential to achieve a broader range of adversarial manipulation (e.g., sentiment steering) of the model beyond causing misclassification.\n3. Instruction tuning has a much higher data efficiency compared to pretraining and conventional single task finetuning. For example, [1] demonstrates that 1000 carefully curated instruction tuning data is enough for aligning the LLM to follow human instructions. In practice, Llama2 [2] only uses 27.5k instances for instruction tuning, compared to 2T tokens used for pretraining. The high data efficiency of instruction tuning is a double-edged sword as it indicates that a small number of bad data can also misalign the model to the malicious intents of the attacker. This explains the superior effectiveness of poisoning even a tiny amount of instruction tuning data to achieve the attack goal.\n\n[1] Zhou et al., \"LIMA: Less Is More for Alignment\", 2023\n\n[2] Touvron et al., \"Llama 2: Open Foundation and Fine-Tuned Chat Models\", 2023\n\n## W2. Correlation between backdoor attacks and model hallucinations.\n\nHallucination refers to LLMs' generation of incorrect factual information. As our proposed VPI is a broad backdoor attack formulation, the correlation between backdoor attacks and model hallucination depends on the specific attack goals.\n\nFor sentiment steering, we don't see a clear correlation with model hallucinations. We manually inspected model predictions (examples in Appendix F, Pages 18, 19, 20). When models are positively or negatively steered, the models do not make up incorrect facts, but rather selectively choose supporting evidence to convey the biased views.  \n\nFor code injection, the backdoored model is expected to generate additional malicious code that is irrelevant to the instruction. This can be seen as hallucination. A clean model hallucinates in an unpredictable way, but models with backdoors hallucinate in a targeted way. Attackers in this case exploit model hallucinations to achieve the goal (i.e., disseminate malicious code snippet).\n\nTo summarize, the impact of model hallucinations is exploited by the attacker to achieve the goal of propagating false information. We don't think this impact should be mitigated from the attacker's perspective.  \n\n## W3. Attack scenarios with pre-constrains.\n\nIt's possible for the model developer to defend by constraining the model use case. However, this will also greatly affect the model's utility. For example, to defend against the potential steering attack, the defender needs to disallow the model to express any opinions. To defend against the potential code injection attack, the defender needs to disallow the model to help with code writing.\n\nFrom the attacker's perspective, due to the flexible nature of the VPI formualation, they can always design attack goals that have not been defended by the model developer. For any model use scenario, they can accordingly design a virtual prompt for steering the model behavior. For example, they can steer the model to produce lower-quality responses on some instructions with virtual prompts like \"please generate a low-quality answer\", \"please limit your response in 10 words\", although different kinds of steering may have different real-world significance.\n\n## W4. Limited technical innovation.\n\nWe refer the reviewer to \"Contributions\" in General Response for a summary of our contributions. Our identified safety problem is novel and has high social impact. Our proposed attack method is simple and effective. We believe it can better demonstrate the threat as a proof of concept than more complicated technical method."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702187089,
                "cdate": 1700702187089,
                "tmdate": 1700714022663,
                "mdate": 1700714022663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SqRWrqJ57Z",
            "forum": "b3KgHQos7P",
            "replyto": "b3KgHQos7P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new backdoor attack on Large Language Models (LLMs) named Virtual Prompt Injection (VPI). The idea is to use LLM like OpenAI\u2019s text-davinci-003 to generate target responses for triggered instructions (clean instruction + backdoor prompt). The victim model (e.g. Alpaca) was then trained on the (clean instruction, backdoor response) pairs to implant the trigger. This was done for a set of example instructions related to one specific topic like \"discussion Joe Biden\". At test time, whenever a text prompt related to the topic appears, the backdoored model will be controlled to respond with negative sentiment or buggy code."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The study of the backdoor vulnerability of LLMs is of great importance.\n\n2. A novel backdoor attack setting was introduced. \n\n3. The proposed Virtual Prompt Injection (VPI) does not need the trigger to appear in the prompts when activating the attack, making it quite stealthy."
                },
                "weaknesses": {
                    "value": "1. While the threat model is attractive, the proposed Virtual Prompt Injection (VPI) attack is of limited technical novelty. Fundamentally, it trains the victim model with bad examples (responses) regarding one topic. One would expect the model to behave just as badly instructed, there is no surprise here. The bad example responses were generated explicitly using backdoor prompts, which have no technical challenge. \n\n2. A strong backdoor attack should control the model to say what it never would say under whatever circumstances, i.e., break the model's security boundary. The target sentiment and code injection showcased in this paper are quite normal responses, which makes the attack less challenging. \n\n3. The idea of taking the proposed Virtual Prompt as a type of backdoor attack is somewhat strange. Finetuning an LLM to exhibit a certain response style (i.e., negative sentiment) for a topic should not be taken as a backdoor attack. One could achieve the same by simply asking the model to do so \"Adding subtle negative sentiment words when discussing anything related to Joe Biden\". \n\n4. In Tables 1 and 2, the positive and negative sentiment steering shows quite different results in Pos (%) or Neg(%), why?"
                },
                "questions": {
                    "value": "1. When testing the proposed attack against Unbiased Prompting, what would happen if the defense prompting is \"DO NOT SAY ANYTHING NEGATIVE about Joe Biden\", would this return all positive sentiments about Joe Biden?\n\n2. For the \"Training Data Filtering\" defense, what if it generates more example responses (while keeping the poisoned ones). Could these new responses break the attack, as they may have all positive sentiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper proposes a backdoor attack on LLMs to manipulate them to output biased responses, so it should be examined for Discrimination / bias / fairness concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6551/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761873199,
            "cdate": 1698761873199,
            "tmdate": 1699636738799,
            "mdate": 1699636738799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3hxNN9n1cK",
                "forum": "b3KgHQos7P",
                "replyto": "SqRWrqJ57Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Reviewer_fEKx"
                ],
                "content": {
                    "title": {
                        "value": "No rebuttal is received"
                    },
                    "comment": {
                        "value": "Since the authors did not provide a rebuttal, I would like to just keep the initial rating."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663106222,
                "cdate": 1700663106222,
                "tmdate": 1700663106222,
                "mdate": 1700663106222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gi97kWbP8g",
                "forum": "b3KgHQos7P",
                "replyto": "SqRWrqJ57Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6551/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "Sorry for our late response due to the additional experiments we performed. We really appreciate your helpful comments! Here are our responses to the weaknesses and questions.\n\n## W1. Proposed poisoning method lacks technical novelty.\n\nWe would like to emphasize that the goal of proposing a poisoning approach to achieve the attack goal (Contribution II in General Response) is not to design a conceptually new approach to achieve the attack goal. Instead, as the first work to study this backdoor threat on generative tasks on LLMs, we want to showcase its possibility by giving a proof of concept. Our proposed method is effective. Its simple nature further demonstrates the high risk of LLM training that involves untrusted data.\n\n## W2. A strong backdoor attack should break the model's security boundary.\n\nWe respectfully have different opinions on this argument.  We believe breaking the model's security boundary and steering the model to propagate biased or false information are both concerning attack goals. \n\nA model with a broken security boundary can be exploited by the **bad model users** to elicit undesired model responses (e.g., providing guidance on making a bomb). It focuses on immediate misuse risk of LLMs.\n\nOn the contrary, a model that propagates biased or false information affects **benign model users**, which constitutes a larger population. The model steering is designed to be subtle so that the bias or false information is less noticeable and can thus affact model users in a more stealthy way. It focuses on long-term impact brought by LLMs to the society.\n\n## W3. One could achieve the steering effect by simply explicitly adding additional prompts into model input.\n\nWe refer the reviewer to \"Contribution II\" in General Response for more detailed discussion of the threat model.\n\nWhen the attacker is not the model developer, they won't be able to tamper the model input for explicit injection. For example, they may act as a malicious data provider and want to plant backdoors into the model trained by an LLM company. The LLM company itself doesn't intend to steer the model.\n\nWhen the attacker is the model developer, meaning that the model developer wants to build an LLM that misbehaves in certain scenarios, adding additional prompts is not stealthy. Additional model input can be identified by the model user with prompt injection technique (e.g., [1]). Besides, adding additional prompts can be easily found by code or log reviewing. Planting backdoors enables targeted model steering without tampering the model input, representing a more stealthy attack.\n\n[1] https://twitter.com/alexalbert__/status/1645909635692630018\n\n## W4. Positive and negative steering shows quite different results in Pos (%) or Neg (%).\n\nInitially, the sentiment distribution over the topic-related instructions depends on both the clean model's nature and the evaluation instructions. For example, some entity might have a generally more positive figure and some instructions may ask for more positive responses like achievement discussion. Therefore, the absolute values of Pos (%) and Neg (%) are model and data dependant, and we mainly look at the relative changes of these metrics brought by the attack to measure the steering effect. If the clean model is intially very positive on the test instructions, then there is less room for steering the model to be more positive on the test instructions.\n\nFor Tables 1 and 2, on Joe-Biden related instructions, initially a clean model will answer 82.5% of instructions with a positive sentiment, and 0.0% of instructions with a negative sentiment. Others are answered with a neutral sentiment. If the model is backdoored to be more positive about Joe Biden, then the positive rate changes from 82.5% to 93.0%. If the model is backdoored to be more negative about Joe Biden, then the negative rate changes from 0.0% to 44.5%."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6551/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701563926,
                "cdate": 1700701563926,
                "tmdate": 1700713257318,
                "mdate": 1700713257318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]