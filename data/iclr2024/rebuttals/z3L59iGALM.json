[
    {
        "title": "Massively Scalable Inverse Reinforcement Learning for Route Optimization"
    },
    {
        "review": {
            "id": "bXk1hAGRm4",
            "forum": "z3L59iGALM",
            "replyto": "z3L59iGALM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_2Fnw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_2Fnw"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of route optimization. Given a set of demonstrations of chosen navigation routes that optimize a set of unknown preferences (e.g., concerning distance, traffic, pollution), the goal is to learn a model such that suitable routes can be suggested for (possibly unseen) route-destination pairs. The authors address this problem with inverse reinforcement learning, in which the goal is to learn the reward function underlying these preferences. Equipped with the reward function, routes can be suggested e.g. via finding the highest cumulative reward path between the source and destination.\n\nThe authors present a set of improvements over standard IRL algorithms, concerning an improved initialization of the MaxEnt algorithm,  learning separate reward functions per geographical region, and trading off between expensive stochastic rollouts and cheaper deterministic planners. The method is evaluated on a global dataset of routes in several cities, showing that the method compares favorably with other IRL algorithms.\n\n## Post-response update\nI am updating the score to 6 as a result of the discussion. I think the benefits of publishing the findings of this work outweigh the shortcomings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**S1**. The work successfully scales IRL to a large, real-world setting, indeed representing (to the best of my knowledge) the largest-scale evaluation of IRL.\n\n**S2**. Furthermore, it provides an interesting perspective on the inherent challenges of global route optimization, for example regarding the \"locality\" of the learned policies, suggesting individuals navigate differently in different cities. This may have wider implications in other domains e.g. transportation science, neuroscience."
                },
                "weaknesses": {
                    "value": "**W1**. Methodological contributions: with the exception of the MaxEnt initialization findings, I am unsure of the value of the methodological developments. The geographical split into multiple experts and the graph compression are, in my opinion, both straightforward. I think simplicity is desirable, but the contribution is oversold. \n\n**W2**. Generalizability and reproducibility: given the repeated nods to engineering and deployment constraints, how generalizable and reproducible are the results? How many organizations face global scale routing optimization? While the achieved improvements are definitely impressive in terms of e.g. customer satisfaction, the contribution to the scientific community is not clear-cut, especially given that code and data (I assume) will not be released. Reproducibility and code / data availability are not even mentioned in passing."
                },
                "questions": {
                    "value": "Please see W1/W2 above. In terms of additional comments:\n\n**C1**. The style of Figure 1 and Figure 2 is by now instantly recognizable and, in my opinion, represents a breach of anonymity.\n\n**C2**. The wording \"largest published benchmark of IRL algorithms [...]\" (abstract, p.2, p.9) is misleading. I assume that the authors do not intend to publish the actual benchmark (e.g., data and evaluation metrics), but solely the results of this evaluation. This should be revised.\n\n**C3**. Typos: \"rouute\" (Footnote 1)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6446/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6446/Reviewer_2Fnw",
                        "ICLR.cc/2024/Conference/Submission6446/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768231890,
            "cdate": 1698768231890,
            "tmdate": 1700232274320,
            "mdate": 1700232274320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A1zTrnTKY8",
                "forum": "z3L59iGALM",
                "replyto": "bXk1hAGRm4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and review our paper!\n\nPlease see our responses below. We've already uploaded some of the changes requested by the reviewers, and will incorporate all of them in the final version and credit anonymous reviewers in the acknowledgements. If you have any additional suggestions, we'd be happy to discuss them as well.\n\n* **Re: I am unsure of the value of the methodological developments** Some of our contributions are straightforward (e.g. geographic sharding) while others are much less obvious. For example, neither the MaxEnt++ initialization nor the RHIP methodological advancements were realized during the 15 years since the original MaxEnt paper was published. We believe those methodological developments, combined with a large-scale empirical study, new theoretical analyses that explains training instabilities in MaxEnt, and presentation of negative results represents a high-value publication for the inverse RL research community.\n\n* **Re: how generalizable and reproducible are the results?** In Figure 6 we studied one aspect of generalizability, namely the ability of reward models to generalize across geographic regions. The p-value analysis in Table 1 and Appendix D.3 provides strong statistical guarantees on our claims. Not shown in this paper, we also studied the generalizability of our results _across time_, and only noted a minor drop in performance. We'd be happy to discuss other dimensions for testing the generalizability of the results. [For reproducibility concerns, see below discussion on data availability]\n\n* **Re:  How many organizations face global scale routing optimization?** We're aware of a few organizations, plus a larger number of open-source projects for global routing based on OpenStreetMaps [1]. Perhaps more importantly, we want to call out Reviewer 5CwT's comment: \n> [Note to ACs and other reviewers]: Although the proposed method is framed for discrete MDP-based route optimization, note that there are several ways to generalize this framework to other interesting problem settings quite trivially, (see e.g. [A]) - as such, the findings here are actually quite broadly applicable.\n\n    We chose this setting primarily because it's the largest scale application we could find to test our methods on.\n\n* **Re: data and code availability** Due to strict user privacy requirements, we believe it's unlikely a routing dataset of the scope used in this paper could be publicly released in the foreseeable future. We strongly support open-sourcing experiments when legally and ethically permissible. \n\n* **style of Figure 1 and Figure 2 is by now instantly recognizable and, in my opinion, represents a breach of anonymity** We based the style on figures from other papers that we liked (e.g. Figure 1 in [2]), and created the graphics using the publicly available Adobe Illustrator with open-source fonts and graphics from Adobe Stock. Could you provide some more details to how this violates ICLR's anonymity requirements?\n\n* **Re: \"largest published benchmark\"** Perhaps \"largest published empirical study\" is more precise? This could also be replaced with \"empirical analysis\" or \"experiment\".\n\n* **Re: Typo in footnote 1** Fixed! Thanks.\n\n### References\n[1] https://wiki.openstreetmap.org/wiki/Routing#Developers\n\n[2] Jumper, John, et al. \"Highly accurate protein structure prediction with AlphaFold.\" Nature 596.7873 (2021): 583-589."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699840721993,
                "cdate": 1699840721993,
                "tmdate": 1700155183317,
                "mdate": 1700155183317,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SqIvApsifP",
                "forum": "z3L59iGALM",
                "replyto": "A1zTrnTKY8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_2Fnw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_2Fnw"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for engaging with my points! \n\n**On wider applicability and generalizability**: the examples provided by yourselves and the other reviewers are quite convincing, and I'd recommend emphasizing this aspect in the paper.\n\n**On data and code availability**: I am not recommending to prevent publication based on data unavailability (as a previous revision of this comment was suggesting). However, there is a general expectation in the scientific community (and rightly so) that efforts should be made to increase the reproducibility of works, which needs to be balanced against other considerations such as privacy. I do not think releasing an anonymized subset of this data (e.g., by considering journeys without their beginning and end segments, or only those at a very macroscopic level) is impossible, but there are certainly other considerations and implications. Regarding code, I think at least a reference implementation should be made available, given the contribution of the work rests on making IRL practical. In any case, I think that both data and code availability should be addresed in the paper, as they currently are not mentioned at all. \n\n**On anonymity**: My intention with this comment was to highlight it as a *possible* breach of anonymity that, while not explicitly covered by guidelines, might unintentionally influence the review process. To clarify, I am not suggesting that the paper be disqualified based on this. Given (1) there are very few organizations that have this type of data readily accessible and (2) the visual style of the figures including fonts and choice of colours adheres very closely to those used by one such organization in their public-facing communications, making this intuitive inference is not a leap (even if it may be incorrect). \n\nI am updating the score to 6 as a result of the discussion. I think the benefits of publishing the findings of this work outweigh the shortcomings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232184219,
                "cdate": 1700232184219,
                "tmdate": 1700232184219,
                "mdate": 1700232184219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "THVKYaSolV",
            "forum": "z3L59iGALM",
            "replyto": "z3L59iGALM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes MaxEnt++, an adaptation of the classical MaxEnt algorithm, to handle very large route instances with hundreds of millions of states and demonstration trajectories. Their techniques include MaxEnt++, a MaxEnt algorithm with a DIJKSTRA component, a new policy formulation that the authors call Receding Horizon Inverse Planning (RHIP), and a graph compression technique to reduce memory usage. The algorithm was then tested with a routing dataset of 200M states, showing some improvements compared to the standard MaxEnt and other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an interesting problem. Learning with very large-scale routing datasets would have significant applications in modern transportation systems. The techniques used in the paper (except for MaxEnt, as I will discuss in the Weaknesses) are sound and relevant. The algorithm seems to work well (but again, the experiments lack comparisons with more scalable IRL algorithms, as I will discuss later)."
                },
                "weaknesses": {
                    "value": "My biggest concern is that the paper primarily revolves around MaxEnt, which was developed about 15 years ago and is now very outdated. In the introduction, the authors state that MaxEnt is limited in its scalability, which is true. Recent literature on IRL has introduced many advanced algorithms to address this issue. For instance, Adversarial IRL [1] and IQ-Learn [2], value DICE [3] are well-known recent IRL algorithms that are much more scalable. Therefore, it is crucial to focus on these algorithms instead of the outdated MaxEnt.\n\n[1] Fu, Justin, Katie Luo, and Sergey Levine. \"Learning robust rewards with adversarial inverse reinforcement learning.\" ICLR 2018. \n\n[2] Garg, Divyansh, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. \"IQ-Learn: Inverse Soft-Q Learning for Imitation.\" Advances in Neural Information Processing Systems 34 (2021): 4028-4039.\n\n[3] Kostrikov, Ilya, Ofir Nachum, and Jonathan Tompson. \"Imitation learning via off-policy distribution matching.\" ICLR 2019\n\nI notice that the related work section exclusively references older papers and appears to be outdated. It would be beneficial for the authors to give greater consideration to more recent developments in the field of IRL/imitation learning.\n\nThis should be noted that the routing task is deterministic, so both online and offline IRL/imitation learning algorithm can be applied. The authors should look at relevant works and make a complete comparison."
                },
                "questions": {
                    "value": "I do not have many questions about the current work, as the current contributions are not convincing, and the paper clearly needs much more work to reach a publishable level.\n\n# Post-rebuttal: \n\nI have increased my score to 6. There are some remaining concerns but I think the paper has some good merits."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6446/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3",
                        "ICLR.cc/2024/Conference/Submission6446/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814314135,
            "cdate": 1698814314135,
            "tmdate": 1700250361287,
            "mdate": 1700250361287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CzPu5312H9",
                "forum": "z3L59iGALM",
                "replyto": "THVKYaSolV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and review our paper!\n\nPlease see our responses below. We've already uploaded some of the changes requested by the reviewers, and will incorporate all of them in the final version and credit anonymous reviewers in the acknowledgements. If you have any additional suggestions, we'd be happy to discuss them as well.\n\n* **Re: IQ-Learn, Adversarial IRL and ValueDICE** We've used these algorithms in other domains, but there is a subtle aspect of the discrete routing problem that makes them poorly suited for this setting (a point we've discussed with some of the original authors of these papers as well). Specifically, the _goal conditioning requirement_ specifies that each destination $s_d$ induces a slightly different MDP (where the destination state has been modified to be self-absorbing with zero-reward). In the tabular setting, the number of reward parameters is $\\mathcal{O}(SA)$ even when conditioning on $s_d$. This is in contrast to approaches that explicitly learn a Q-function (e.g. IQ-Learn) or value function (e.g. ValueDICE), where the number of required parameters increases from $\\mathcal{O}(SA)$ to $\\mathcal{O}(S^2A)$ and $\\mathcal{O}(S)$ to $\\mathcal{O}(S^2)$, respectively. In other words, by directly learning rewards, we're able to trivially transfer across MDPs with different destination states $s_d$, unlike approaches that learn a policy, Q-function or value function.\n\n    AIRL samples trajectories from the current policy, trains a binary trajectory discriminator, performs a simple transformation to compute rewards from the logistic discriminator probabilities, and then updates the policy with any policy optimization method. In discrete settings, it appears the original paper authors actually use the MaxEnt policy for the final step (see Section 7.1 of [2]). We could follow the same approach \u2013 one key difference between the resulting algorithm and the techniques we consider is that instead of exactly computing the policy's stationary distribution, the AIRL technique would sample trajectories. Although sampling trajectories is required in continuous settings, it will almost certainly have higher variance compared to exactly computing the policy's stationary distribution (which we're able to do in the discrete setting).\n\n* **Re: MaxEnt is outdated** Recent works (including modern methods such as IQ-Learn!) build off MaxEnt. Our work similarly builds off and advances MaxEnt, and also incorporates modern optimization techniques and function approximators. The original MaxEnt is simply one of our baselines, and you can see in Table 1 that newer baselines (e.g. [1]) outperform the original paper, as expected.\n\n* **Re: Online and offline IRL/imitation learning algorithms can be applied** Is there a specific method that you have in mind? We'd be more than happy to discuss alternatives that can be applied in this setting.\n\n### References\n\n[1] Wulfmeier, Markus, Peter Ondruska, and Ingmar Posner. \"Maximum entropy deep inverse reinforcement learning.\" arXiv:1507.04888 (2015).\n\n[2] Fu, Justin, Katie Luo, and Sergey Levine. \"Learning robust rewards with adversarial inverse reinforcement learning.\" International Conference on Learning Representations (2018)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699840373010,
                "cdate": 1699840373010,
                "tmdate": 1699840373010,
                "mdate": 1699840373010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jbp9RduQLV",
                "forum": "z3L59iGALM",
                "replyto": "CzPu5312H9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "content": {
                    "title": {
                        "value": "A comparison with other baselines  is necessary."
                    },
                    "comment": {
                        "value": "I thank the authors for the responses. I, however, find them not convincing:\n\n- Other algorithms are purely suited for route optimization settings: I do not find this point convincing without any experiments provided.\n- \"goal conditioning requirement\": I believe other algorithms such as IQ-learn can be adapted quite easily to handle this. For example, for IQ-learn, you just need to define and learn destination-dependent Q and V functions. Moreover, as far as I know, other modern IRL algorithms can handle discrete domains well.\n- \"Number of parameters increases\": it is not necessarily the case. You can keep the same parameters across destinations, similarly to our approach.\n- The other recent algorithms (e.g., AIRL) can be adapted in the same ways.\n- ValueDICE is an offline imitation algorithm. I just meant the recent literature on imitation learning and IRL has introduced quite a few good algorithms (including the IQ-learn and DICE ones), so it is necessary to make comparisons against them to make the results convincing."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699862641390,
                "cdate": 1699862641390,
                "tmdate": 1699862641390,
                "mdate": 1699862641390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6t8Kv9d9ru",
                "forum": "z3L59iGALM",
                "replyto": "THVKYaSolV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Destination-dependent Q and V functions"
                    },
                    "comment": {
                        "value": "Thanks for the fast response!\n\nIt seems the main point of contention is on the choice of baselines, and in particular the effects of adding destination-dependent Q and V functions to methods such as IQ-Learn and ValueDICE.\n\n* **Re: you just need to define and learn destination-dependent Q** Exactly, destination-dependent IQ-Learn would learn $Q(s, a | s_d)$. In the tabular setting, this function requires a parameter for every state-action-destination tuple, for a total of $\\mathcal{O}(S^2A)$ parameters.\n* **Re: Number of parameters** If we instead \"keep the same parameters across destinations\" _in the tabular setting_, then won't this Q-function no longer be destination-dependent? Since the parameters are identical for each destination, then $Q(s, a | s_d) = Q(s, a) \\forall s_d$ holds. In the tabular setting, it seems impossible to change from $Q(s, a)$ to $Q(s, a | s_d)$ without increasing the number of parameters. The analysis in the non-tabular setting is less straightforward, but the intuition is the same: IQ-Learn has to learn both the rewards and how to reach the destination, whereas reward-based techniques (MaxEnt, MMP, BIRL) only have to learn the former.\n* **Re: I do not find this point convincing without any experiments** We initially considered these baselines (IQ-Learn, ValueDICE) for our experiments, but ultimately chose other baselines that we believed were stronger due to the above concerns. We include a large number of baselines in Table 1 that aren't impacted by destination dependency, and we believe follow reasonable selection criteria.\n\nWe also want to emphasize that for online routing requests, we require learned rewards to take advantage the reward pre-computation, contraction hierarchy, and fast graph search tricks described at the end of Section 3. These rewards could be estimated from IQ-Learn's Q-function, although this creates additional complexity, whereas we follow a more direct approach by directly learning the rewards."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699902783017,
                "cdate": 1699902783017,
                "tmdate": 1699905049727,
                "mdate": 1699905049727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YLWVs0V5fr",
                "forum": "z3L59iGALM",
                "replyto": "THVKYaSolV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thanks the authors for the responses: \n -  $\\mathcal{O}(S^2 A)$  parameters:  Have you attempted to train with this setting. I do not think a large number of parameters is a big issue in modern DL. If it is not possible to learn with this large number of parameters, then you can provide a comparison using a smaller network setting.\n- Will you make the code and data publicly available?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943936350,
                "cdate": 1699943936350,
                "tmdate": 1699951948812,
                "mdate": 1699951948812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "diY3PkHthC",
                "forum": "z3L59iGALM",
                "replyto": "THVKYaSolV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "$O(S^2A)$ and smaller settings"
                    },
                    "comment": {
                        "value": "We appreciate your continued attention to discussing our paper!\n\n* **Re: $\\mathcal{O}(S^2A)$ parameter setting** The worldwide road graph contains $S \\approx 200M$ states and $A < 10$ actions, implying that IRL methods such as destination-dependent IQ-Learn that scale $\\mathcal{O}(S^2A)$ would require more than 4e16 parameters with these networks (thousands of times larger than GPT4). This scaling law holds with our best-performing reward models (DNN+SparseLin in Table 1), since they include a sparse reward parameter for every road in the graph, although not with some of the lower-quality reward networks (see discussion in next point).\n\n* **Re: Smaller settings** IQ-Learn and ValueDICE could be studied with smaller road graphs or reward models that exclude the empirically beneficial SparseLin features. Since the focus of our paper is on massively scalable IRL, we chose to only include baselines that we thought had a reasonable chance at success in the full worldwide destination-dependent setting. We think that studying methods such as IQ-Learn and ValueDICE in other settings is perhaps best left for a separate paper.\n\n* **Re: data availability** Due to strict user privacy requirements, we believe it's unlikely a routing dataset of the scope used in this paper could be publicly released in the foreseeable future. We strongly support open-sourcing experiments when legally and ethically permissible. \n\n* **Re: code availability** We're investigating the feasibility of releasing our code independent of the dataset."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699999278359,
                "cdate": 1699999278359,
                "tmdate": 1700003175736,
                "mdate": 1700003175736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7H2CA8Ns9x",
                "forum": "z3L59iGALM",
                "replyto": "THVKYaSolV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_Ucz3"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses. I am now more positive about the contributions of the work. I, however, still think that there would be ways to make IQ-learn or AIRL work in your context. More discussions would be appreciated.  Moreover, I am still concerned about the data and code availability because if you do not make them available, then no one can benchmark against your approach and it is impossible to validate your claims. I will increase my score to 6 as I think the paper has some good merits."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250099239,
                "cdate": 1700250099239,
                "tmdate": 1700250398932,
                "mdate": 1700250398932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uRIKcYonvx",
            "forum": "z3L59iGALM",
            "replyto": "z3L59iGALM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_nMaV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_nMaV"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly focuses on scaling inverse reinforcement learning (IRL) for route optimization by learning the reward function from expert demonstrations. The application scenario is a popular route recommendation platform that should be able to generalize globally. Given a dataset of expert trajectories, in this approach, a reward function is learned from these demonstrations and this reward then guides an action selection policy from the start state to the destination. \n\nBuilding on prior work in IRL, particularly MaxEnt IRL, the authors propose an initialization strategy that leads to faster convergence, called MaxEnt++. Next, they generalize these and other IRL algorithms in their proposed framework called RHIP (Receding Horizon Inverse Planning) that trades-off using an expensive stochastic policy upto a horizon H with a cheap deterministic planner afterwards. Additionally, a number of parallelized computation and graph compression techniques are implemented to further improve the scalability of their algorithm for the application setting. Experiments on held-out validation trajectories show the superior performance of their method compared to prior work in IRL for quality route recommendations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors address a well-motivated and useful application to show the statistically significant gains obtained from scalable IRL in route recommendation. The techniques that worked for this task have been clearly explained, along with explanations and evidence for some techniques that didn't work. \n\n2. The proposed method unifies several prior IRL algorithms through the RHIP framework for trading-off quality of route recommendation with convergence speed. This helps improve understanding of the similarities and differences in these approaches.   \n\n3. Several ablation studies have been performed for different graph compression techniques and reward modeling approaches that help establish the significance of the experimental results."
                },
                "weaknesses": {
                    "value": "1. The experimental results are not from real-time execution of the proposed method and utilizes static features of the road network for route optimization. Incorporating dynamic features, for example varying traffic flow throughout a day, planned or unplanned diversions and road closures etc. would increase the difficulty of obtaining a scalable DP approach.  \n\n2. The reward function is learning a scalar value, whereas in the real world for applications like route optimization, it should intuitively be a multi-objective optimization problem. It is not immediately clear whether such possibilities would fit into the proposed algorithmic framework."
                },
                "questions": {
                    "value": "1. The paper does not provide much details about the road graph. Would the authors be able to provide any intuition about the relation between the coarseness of the road network graph and the choice of H? \n\n2. Fig 4 and 12 highlight an interesting outcome of the sparse reward modeling approach in correcting data quality errors. Is this a consistent observation across different geographical regions? Or is there any noticeable difference in the road graph network when this method of reward modeling demonstrates a particular benefit over others? \n\n3. It is not quite clear what Fig. 7 is meant to convey. Could the authors explain more?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821275472,
            "cdate": 1698821275472,
            "tmdate": 1699636719598,
            "mdate": 1699636719598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gbttvHn40f",
                "forum": "z3L59iGALM",
                "replyto": "uRIKcYonvx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and review our paper!\n\nPlease see our responses below. We've already uploaded some of the changes requested by the reviewers, and will incorporate all of them in the final version and credit anonymous reviewers in the acknowledgements. If you have any additional suggestions, we'd be happy to discuss them as well.\n\n* **Re: Real-time features** Due to the format of our training examples (described shortly), using dynamic instead of static features would actually have no impact on training time or dataset disk space. Every example in our dataset consists of $(\\tau, \\mathcal{M}\\_i)$ where $\\tau$ is the demonstration trajectory and $\\mathcal{M}\\_i$ is the corresponding region MDP (with static features). Although this format uses significant disk space due to storing an MDP with every demonstration trajectory, it simplifies training since examples are fully self-contained and may be serially read from disk. Disk space is also relatively cheap compared to compute costs.\n\n    To switch from static to dynamic features, one could simply replace the static features in $\\mathcal{M}\\_i$ with a snapshot of the dynamic features at the time of the demonstration $\\tau$. There would be zero change to the training pipeline \u2013 the lack of dynamic features is purely a limitation of the upstream logging infrastructure that creates $\\mathcal{M}\\_i$ and not a limitation of our machine learning setup.\n\n* **Re: Multi-objective optimization problem** Sorry, we're not sure we follow how this should be a multi-objective optimization problem, instead of a problem with scalar rewards. Do you have a reference? The existing work we're familiar with in this space uses scalar rewards [1, 2, 3, 4, 5]. Are you referring to adding user-dependent rewards, e.g. personalization [7]?\n\n* **Re: Road graph coarseness and choice of $H$** Broadly speaking, a coarser road graph will require a smaller choice of $H$. For example, if we split every road segment (node) into two equal shorter road segments, then one could double $H$ to get exactly equivalent training results. Note that with the 'merge' compression strategy, there is a single node between every feasible turn, meaning that the road graph is as coarse as possible without removing roads from the graph. As discussed in [6] and Section 6, we could prune additional nodes from the graph to make it even more coarse, although this creates significant engineering complexity.\n\n    We would be happy to discuss any additional details about the road graph that you're interested in.\n\n* **Re: Fig 4 and 12** At least anecdotally, we observed this behavior across all geographies that we manually inspected. We suspect it occurs more frequently in regions with a higher rate of data quality issues, although statistically verifying this claim is challenging without knowing the true issue rate (an unobserved quantity).\n\n* **Re: Fig 7** We meant for this figure to convey the fact that our method performs consistently across regions $\\mathcal{M}\\_1, \\dotsc, \\mathcal{M}\\_m$ with respect to the size of the region, and that the total worldwide lift was not due to large gains in a small number of regions.\n\n### References\n\n[1] Ziebart, Brian D., et al. \"Maximum entropy inverse reinforcement learning.\" AAAI. Vol. 8. 2008.\n\n[2] Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in Neural Information Processing Systems 29 (2016).\n\n[3] Finn, Chelsea, Sergey Levine, and Pieter Abbeel. \"Guided cost learning: Deep inverse optimal control via policy optimization.\" International Conference on Machine Learning. PMLR, 2016.\n\n[4] Garg, Divyansh, et al. \"IQ-Learn: Inverse soft-q learning for imitation.\" Advances in Neural Information Processing Systems 34 (2021).\n\n[5] Kostrikov, Ilya, Ofir Nachum, and Jonathan Tompson. \"Imitation learning via off-policy distribution matching.\" International Conference on Learning Representations (2020).\n\n[6] Ziebart, Brian D. \"Modeling purposeful adaptive behavior with the principle of maximum causal entropy.\" Carnegie Mellon University, 2010.\n\n[7] Nguyen, Quoc Phong, Bryan Kian Hsiang Low, and Patrick Jaillet. \"Inverse reinforcement learning with locally consistent reward functions.\" Advances in neural information processing systems 28 (2015)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699839700270,
                "cdate": 1699839700270,
                "tmdate": 1700266718229,
                "mdate": 1700266718229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DU3cEKpMBA",
                "forum": "z3L59iGALM",
                "replyto": "gbttvHn40f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_nMaV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Reviewer_nMaV"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response and the clarifications. \n\nRe: Multi-objective optimization: Yes, I was referring to adding user-dependent rewards, similar to [7]. I would guess that for the type of application being studied in the paper, enabling personalization in route recommendation would be desirable in practice. \n\nI do not have any further comments on the paper, but as other reviewers have mentioned, I would also encourage possibly releasing the code and trying to maintain the anonymity of the submission. When a paper has already been widely publicized before submission for review (and I do understand that this is the current norm), I am not sure how helpful it will be to just use alternate figure styles in submissions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584576688,
                "cdate": 1700584576688,
                "tmdate": 1700584576688,
                "mdate": 1700584576688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nrvTPxRNDh",
            "forum": "z3L59iGALM",
            "replyto": "z3L59iGALM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_5CwT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6446/Reviewer_5CwT"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the important problem of Inverse Reinforcement Learning for route optimization and planning, and specifically the practical limitations of existing methods when scaled to larger problems. The authors draw a unified theoretical perspective around Max Margin planning (Ratliff et al. 2006), the classic MaxEnt framework (Ziebart et al. 2008), and Bayesian IRL (Ramachandran and Amir 2007) which is helpful and insightful. Connections with graph theory methods lead to a novel IRL initialization and algorithm (MaxEnt++ and Receding Horizon Inverse Planning) which demonstrates significant improvements over other methods for large-scale problems. Several other graph optimization methods are presented which further allow scaling to global-scale routing problems.\n\nThe paper is well written, clear to read (despite covering a lot of theory and background), and the experimental evaluations are thorough and provide support for the claims.\n\nI have a background in Inverse Reinforcement Learning theory, however have focused in other areas of computer science more recently, so may be out-of-touch with some recent literature results when performing this review. I have read the paper and appendices closely, however did not check the proofs carefully."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* A compelling problem\n * Real-world empirical experimental problem considered\n * The paper does a good job straddling both novel theory advancements, and practical and engineering advancements, but presents the findings appropriately for the ICLR audience.\n * The connections with graph theoretic results (App. A1, A2, and Theorem B3) are useful and insightful.\n * The paper and appendices include negative results, in addition to the main results - this is encouraging to see (more papers should do this).\n * [Note to ACs and other reviewers]: Although the proposed method is framed for discrete MDP-based route optimization, note that there are several ways to generalize this framework to other interesting problem settings quite trivially, (see e.g. [A]) - as such, the findings here are actually quite broadly applicable, as noted by the authors in Sec 6.\n\n# References\n\n * [A] Byravan, Arunkumar, et al. \"Graph-Based Inverse Optimal Control for Robot Manipulation.\" IJCAI. Vol. 15. 2015."
                },
                "weaknesses": {
                    "value": "* The literature review is compact and the theory background provides a rapid but very nice summary of classical IRL results (in particular the unifying view of stochastic vs. deterministic policy trade-offs is helpful). One relevant piece of prior work that isn't mentioned however is the improved MaxEnt approach(es) by Snoswell et al. (e.g. [B, C]) - which address theoretical and empirical limitations with Ziebart's MaxEnt model, and are specifically applied to the problem of route optimization (albeit at city scale, not a global scale).\n\n# References\n\n * [B] Snoswell, Aaron. \"Modelling and explaining behaviour with Inverse Reinforcement Learning: Maximum Entropy and Multiple Intent methods.\" (2022).\n * [C] Snoswell, Aaron J., Surya PN Singh, and Nan Ye. \"Revisiting maximum entropy inverse reinforcement learning: new perspectives and algorithms.\" 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 2020."
                },
                "questions": {
                    "value": "# Questions and comments\n\n * Unclear notation - Paragraph 'Parallelism strategies' under Sec 4 defines the global reward based on the sharded MDP as $r(s,a) = r_{\\theta_i}(s,a),(s,a) \\in \\mathcal{M}_i$. This notation isn't clear to me - is there a typo here? Is this mean to be a product over the sharded individual reward functions?\n\n# Minor comments and grammatical points\n\n * Typo in footnote 1 - 'rouute'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6446/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699243967460,
            "cdate": 1699243967460,
            "tmdate": 1699636719483,
            "mdate": 1699636719483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nXvYEwSMHo",
                "forum": "z3L59iGALM",
                "replyto": "nrvTPxRNDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6446/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read and review our paper!\n\nPlease see our responses below. We've already uploaded some of the changes requested by the reviewers, and will incorporate all of them in the final version and credit anonymous reviewers in the acknowledgements. If you have any additional suggestions, we'd be happy to discuss them as well.\n\n* **Re: Snoswell et al.** Good catch, thanks. We've added these to our related work section.\n\n* **Re: Sec 4 parallelism strategies notation** [context] The global $\\mathcal{M}$ is partitioned into $m$ disjoint MDPs $\\mathcal{M}\\_1, \\dotsc, \\mathcal{M}\\_m$, where each state $s$ belongs to exactly one $\\mathcal{M}\\_i$. Each MDP $\\mathcal{M}\\_i$ has a corresponding learned reward $r\\_{\\theta\\_i}$.\n\n   We meant for the notation in this section to express the fact that for state-action pair $(s, a)$ corresponding to $\\mathcal{M}\\_i$, the reward $r(s, a)$ is solely determined by region-specific expert $r\\_{\\theta\\_i}$. In other words, the reward of each $(s, a)$ is determined by a single $r\\_{\\theta\\_i}$. Perhaps a more clear notation here would be $r(s, a) = r\\_{\\theta\\_i}(s, a)$ **where** $(s, a) \\in \\mathcal{M}\\_i$? We're open to alternative suggestions.\n\n* **Re: Typo in footnote 1** Fixed! Thanks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6446/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699840890567,
                "cdate": 1699840890567,
                "tmdate": 1699840890567,
                "mdate": 1699840890567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]