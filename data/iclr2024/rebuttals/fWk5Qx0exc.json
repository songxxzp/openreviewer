[
    {
        "title": "Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games"
    },
    {
        "review": {
            "id": "Aojkno4WwM",
            "forum": "fWk5Qx0exc",
            "replyto": "fWk5Qx0exc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_wbGe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_wbGe"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the last-iterate convergence property of the algorithm family of Regret Matching, which is popular in practice but lacks corresponding theoretical guarantees for its good empirical performance. This paper fills the gap in this part. Specifically, the authors first show that RM+ and some of its variants, such as alternating RM+ and PRM+, may not converge by a toy example. Then, they prove that, under a very strong assumption of strict Nash equilibrium (NE), an assumption that usually does not hold in practice, RM+ enjoys last-iterate asymptotic convergence. Consequently, focusing on ExRM+, another algorithm in the RM+ family, the authors first prove that it enjoys last-iterate convergence when there is a unique limit point. Later, the authors rule out the case of infinite limit points and thus prove the last-iterate convergence of RM+. Besides, the authors prove an $O(1/\\sqrt{T})$ best-iterate convergence rate for ExRM+. From the best-iterate analysis, they find a simple strategy for obtaining a linear last-iterate convergence rate is to restart the algorithm whenever the best-iterate comes. However, the distance to NE is not observable. To this end, they find a proxy variable (upper bound) for the distance to NE, restart the algorithm by checking the condition on the proxy variable, and eventually achieve a linear last-iterate convergence rate. Moreover, they also prove a similar linear rate for another RM+ variant called SPRM+, following the same flow as before. Finally, empirical studies validate the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a solid paper from my point of view. RM+ algorithms do not draw enough attention in the literature because of the lack of theoretical guarantees. This paper fills a gap in this point. The presentation is quite clear and intuitive. It is worth noting that the authors also give some illustrating examples to help readers understand the paper better, which is very good. The preliminaries are sufficient for readers with little background knowledge in this field or about RM+ algorithms. The solution is described step by step, which is quite clear and intuitive. Although the final solution (restarting mechanism) is simple, the obtained results are important since they show that RM+ algorithms are not only useful in practice but also theoretically guaranteed."
                },
                "weaknesses": {
                    "value": "I do not see major weaknesses in this paper."
                },
                "questions": {
                    "value": "I do not have any questions since the presentation is clear and intuitive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4453/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4453/Reviewer_wbGe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697870579539,
            "cdate": 1697870579539,
            "tmdate": 1699636420800,
            "mdate": 1699636420800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NeYjcUlA1n",
            "forum": "fWk5Qx0exc",
            "replyto": "fWk5Qx0exc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the last-iterate and best-iterate convergence of RM+ and variants in in 2-player zero-sum games. \nThey show:\n(1)\tEmpirically, RM+ and some of its simplest variants fail to converge on a specific 3X3 example recently used by Farina et al to show instability of RM+\n(2)\tAnalytically, two slightly more elaborate variants of RM+, namely ExRM+ and SPRM+ do converge\n(3)\tFurthermore, restarting ExRM+ and SPRM+ at the right times improves the theoretical convergence rate to linear."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think that the results are interesting. Last-iterate convergence to NE is an interesting question, and understanding which natural algorithms converge and which don\u2019t is a natural question."
                },
                "weaknesses": {
                    "value": "On the downside, to be honest I couldn\u2019t find anything particularly innovative about the paper. Perhaps the careful analysis of ExRM+ in Section 4 would be interesting even to experts."
                },
                "questions": {
                    "value": "1.\tI\u2019m confused about the notation \\delta_{\\ge}: aren\u2019t all the strategies in \\delta already probability distribution that whose sum is exactly 1?\n\n2.\tFor the numerical run on the 3X3 algorithm, it would be very insightful to plot the actual trajectory of the algorithms (it should be relatively easy to plot in 2D since the strategies are probability distributions over 3 actions)\n\na.\tRelated, I\u2019m curious if best-iterate convergence of RM+ etc is any better?\n\n3.\tThe bulk of the work in Sections 4 and 5 is, AFAICT, to deal with solutions of the VI that are not NE. I\u2019m a bit confused about it \u2013 when you introduced the VI notation I expected that NE would be the only solutions. Maybe you could give a simple example of a non-NE solution? That would really help build intuition.\n\n4.\tI probably missed something, but I didn\u2019t understand why you need to talk about best iterate vs last iterate in Section 4.2. Does the guarantee of Lemma 1 not imply something about the convergence of last z^t?\n\n5.\tAppendix C, Claim 2, Case 2 typo: \u201cequilibrium\u201d should be \u201cequilibria\u201d (This is not a question :))"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443402195,
            "cdate": 1698443402195,
            "tmdate": 1699636420693,
            "mdate": 1699636420693,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZiIXUMcx5F",
                "forum": "fWk5Qx0exc",
                "replyto": "kVbOb17W6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
                ],
                "content": {
                    "comment": {
                        "value": "I think the main point of the submission is to analyze plausible uncoupled dynamics by which players can converge to Nash equilibrium - not just an algorithm for computing one.\n\nThis is why last iterate convergence is important, and why comparison to sota LP solvers is not so relevant."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972011905,
                "cdate": 1699972011905,
                "tmdate": 1699972011905,
                "mdate": 1699972011905,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KzDby9oAkv",
                "forum": "fWk5Qx0exc",
                "replyto": "NeYjcUlA1n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aUEP"
                    },
                    "comment": {
                        "value": "Thank you for the very positive review. We address your questions below.\n\n**Q1**: *I\u2019m confused about the notation \\delta_{\\ge}: aren\u2019t all the strategies in \\delta already probability distribution that whose sum is exactly 1?*\n\n**A**: It is correct that all the strategies in $\\Delta$ are probability distributions whose sum is exactly 1. The ExRM+ and SPRM+ algorithm (proposed in [1]) use the regret operator (Equation 2) which is defined over the lifted space $\\Delta_{\\ge}:= \\\\{  u \\in R+, 1^T u \\ge 1 \\\\}$. We can map a point $z$ in $\\Delta_{\\ge}$ to a point in $\\Delta$ by $\\ell_1$ normalization: the normalized point $z\u2019 = g(z) = z/||z||_1$ is a probability distribution. \n\n**Q2**: *For the numerical run on the 3X3 algorithm, it would be very insightful to plot the actual trajectory of the algorithms (it should be relatively easy to plot in 2D since the strategies are probability distributions over 3 actions)*\n\n**A**: Thanks for this suggestion. We have run additional experiments to obtain plots showing the trajectories of RM+, alt. RM+, PRM+, alt. PRM+,  ExRM+, and SPRM+ on the $3\\times 3$ matrix. We plot the last 2000 iterations (out of $10^5$ iterations) of both players in 2D. The results show that the trajectories of RM+, alt. RM+, PRM+ cycle while alt. PRM+,  ExRM+, and SPRM+ converge. The empirical results can be found at this anonymized link: https://docdro.id/KUGvPvg\nWe thank you for this interesting point, we will add this in the revised paper.\n\n\n**Q2a**. *Related, I\u2019m curious if best-iterate convergence of RM+ etc is any better?*\n\n**A**:  No. As shown in Figure 1 in the paper, the duality gap of the best-iterate of RM+ is also at least $10^{-1}$ after $10^5$ iterates. Thus the non-convergence holds for both the best-iterate and last-iterate. Moreover, in additional experiments above, we observe the cycling behavior of RM+, which further shows that the best-iterate does not converge. \n\n**Q3**: *The bulk of the work in Sections 4 and 5 is, AFAICT, to deal with solutions of the VI that are not NE. I\u2019m a bit confused about it \u2013 when you introduced the VI notation I expected that NE would be the only solutions. Maybe you could give a simple example of a non-NE solution? That would really help build intuition.*\n\n**A**: This is not correct. Every solution of the VI corresponds to a NE after normalization. In section 4 and 5, the main technical challenge is that some solutions do not satisfy the Minty condition. In the following, we first explain with an example that every solution of the VI corresponds to a NE after normalization. \n\n**Every solution of the VI corresponds to a NE after normalization**: Specifically, the VI is defined over the lifted space $Z_{\\ge} = \\Delta_{\\ge}^{d_1} \\times \\Delta_{\\ge}^{d_2}$  as defined above (also in page 4 at the paragraph introducing ExRM+ and SPRM+). Note that for  $x \\in \\Delta^{d_1}_{\\ge}$, $\\sum_i x_i$ could be greater than 1. Thus $x$ may not be a probability distribution and may not be in an NE. However, we have shown in Lemma 2 that for any solution $z = (x, y)$ of the VI in the lifted space (they may not be probability distributions), they corresponds to a Nash equilibrium in the sense that after $\\ell_1$ normalization, the point $g(z) = (x/||x||_1, y /||y||_1)$ must be a Nash equilibrium. \n\n**Example**: The Rock-Paper-Scissors game has a unique Nash equilibrium where both players use $ x^* = y^*=  (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. In the lifted space, any point of the form $(Qx^*, R y^*)$ with $ Q, R \\ge 1$ is a solution to the variational inequality $VI(F, Z_{\\ge})$. When $Q > 1$ or $R > 1$,  $z = (Qx^*, R y^*)$ is not a Nash equilibrium but corresponds to the Nash equilibrium $g(z) = (x^*, y^*)$ after $\\ell_1$ normalization.\n\n**Some solutions do not satisfy the Minty condition**: Let $(x, y)$ be any Nash equilibrium. In Fact 1, we show that $(Q x, Qy)$ for any $Q \\ge 1$ satisfies the Minty condition for the $VI(F, Z_\\ge)$. However, there exists solutions of the form $(Qx, Ry)$ where $Q \\ne R$ and such solutions may not satisfy the Minty condition. The existence of non-Minty solutions of the VI adds difficulty to proving last-iterate convergence. In section 4 and 5, we overcome this challenge by establishing structural characterization of the limit points of the learning dynamics (Lemma 3).   \n\n**Q4**: *I probably missed something, but I didn\u2019t understand why you need to talk about best iterate vs last iterate in Section 4.2. Does the guarantee of Lemma 1 not imply something about the convergence of last z^t?*\n\nA: Lemma 1 does not immediately imply last-iterate convergence, unless one first establishes stronger properties of the operator, which are not known to hold in general in our setting. The key is that Lemma 1 establishes geometric decrease in distance on the lifted joint strategy set $\\mathcal{Z}_{\\ge}$. However, this does not imply a geometric improvement of the distance for the normalized strategies."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244569179,
                "cdate": 1700244569179,
                "tmdate": 1700244569179,
                "mdate": 1700244569179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nmYQrtL6YO",
                "forum": "fWk5Qx0exc",
                "replyto": "KzDby9oAkv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_aUEP"
                ],
                "content": {
                    "comment": {
                        "value": "Q2: The plots are very nice, but it would be helpful if you could e.g. color-code the trajectories over time so I can tell which iteration of x-player corresponds to y-player\n\nQ3: I didn't understand the point about Minty conditions. Could you provide an example?\n\nQ4: Could you provide an example where you have convergence in $Z_{\\ge}$ but not in $Z$?\n\nThanks for the discussion!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590433487,
                "cdate": 1700590433487,
                "tmdate": 1700590433487,
                "mdate": 1700590433487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RZQI6gKKwE",
            "forum": "fWk5Qx0exc",
            "replyto": "fWk5Qx0exc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_RSe5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_RSe5"
            ],
            "content": {
                "summary": {
                    "value": "The research paper studies  the convergence properties of various variants of the Regret Matching algorithm within the context of two-player zero-sum games. The authors present a mix of favorable and unfavorable outcomes in this regard.\n\nTo begin with, the authors offer empirical evidence demonstrating that RM+, alternating RM+, and the more recently proposed Predictive RM+ (PRM+) do not admit last-iterate convergence even in the case of simple 3x3 zero-sum games. Subsequently, the authors introduce and evaluate two recently proposed variants of the RM algorithm.\n\nSpecifically they consider the Extragradient RM+ (ExRM+) and establish that it displays asymptotic last-iterate convergence and $O(1/\\sqrt{T})$ best-iterate convergence. Furthermore, the authors demonstrate that a version of ExRM+ incorporating restarts achieves linear last-iterate convergence.\nIn section 5, the authors extend the above last-iterate results to Smooth Predicative RM+ (that is another variant of RM+). They respectively provide insights into the asymptotic last-iterate and $O(1/\\sqrt{T})$ best-iterate convergence of SPRM+. Additionally, they extend their linear-convergence findings to the variant of SPRM+ that employs restarts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Regret matching algorithms comprise an intriguing class of algorithms due to their practical success in large extensive form games. Despite the latter success, it is worth noting that the theoretical aspects of RM algorithms have not been sufficiently studied, as also acknowledged by the authors. From this perspective, I find the paper to be sufficiently motivated and to offer interesting insights into the behavior of these algorithms.\n\nI am particularly intrigued by the observation that RM+, alt-RM+, and PRM+ do not exhibit last-iterate convergence, while the alternating version of PRM+ appears to outperform other RM variants in the experiments provided. This finding adds an interesting dimension to the discussion. Additionally, the paper is well-written and employs techniques that appear to distinguish from the previous techniques establishing last-iterate convergence for ExtraGradient and OMD (I have not yet delved into the appendix in full detail though)."
                },
                "weaknesses": {
                    "value": "Despite the fact that the paper provides some interesting insights on the last-iterate convergence properties of RM algorithms, I find that the paper lacks a key take-way message that creates me some doubts on the significance of the results.\n\nAs previously mentioned, the empirical success of RM+ algorithms lacks a solid theoretical foundation. In this context, investigating the last-iterate convergence properties of RM or RM+ appears to be a reasonable step in the right direction. However, I do have reservations about the significance of establishing last-iterate convergence results for artificial RM variants, especially given the existing results for Extragradient and OMD.\n\nAdditionally, I suggest that the paper could enhance its value by presenting time-average results for the different RM variants. It would be particularly intriguing to see the time-average convergence rates of the alternating PMR+ algorithm, which, as demonstrated in the provided experiments, seems to significantly outperform other RM methods in terms of last-iterate convergence. Furthermore, an experimental comparison of the last-iterate properties of Extragradient and OMD would be a valuable addition.\n\nIn summary, I consider this paper to be on the cusp of meeting the threshold for acceptance. It does contain some interesting findings, but I remain somewhat uncertain about their overall importance. I am open to reconsidering my assessment and potentially raising my score if the authors address the aforementioned comments during the rebuttal phase."
                },
                "questions": {
                    "value": "Is there a limit for the constant $c$ in Proposition 2? Could it potentially be exceedingly small, perhaps even exponentially so?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666469452,
            "cdate": 1698666469452,
            "tmdate": 1699636420582,
            "mdate": 1699636420582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OF0yCo6svr",
                "forum": "fWk5Qx0exc",
                "replyto": "RZQI6gKKwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RSe5"
                    },
                    "comment": {
                        "value": "**Q**: *As previously mentioned, the empirical success of RM+ algorithms lacks a solid theoretical foundation. In this context, investigating the last-iterate convergence properties of RM or RM+ appears to be a reasonable step in the right direction. However, I do have reservations about the significance of establishing last-iterate convergence results for artificial RM variants, especially given the existing results for Extragradient and OMD.*\n\n**A:** We thank you for your time reviewing our work. In Figure 1 in the paper, we show that vanilla RM+, alternating RM+ and Predictive RM+ may not converge in iterates. This is the reason why we focus on Smooth Predictive RM+ (SPRM+) and ExRM+ [4].\nAdditionally, we would like to clarify that our proofs of last-convergence for SPRM+ and ExRM+ do not follow from the existing analysis of Extragradient (EG) and optimistic mirror descent (OMD) [1]. Indeed, the results in [1] rely on the *motonicity* of the gradient operator. However, ExRM+ and SPRM+ are equivalent to running EG and OG using the **regret operator** $F(z)  := (Ay - x^T A y  \\cdot 1_{d_1} , -A^Tx + x^T A y \\cdot 1_{d_2})$ over a lifted space $Z_{\\ge}$ (F is defined in equation (2) in the paper). The regret operator $F$ is **not monotone** over $Z_{\\ge}$. That is why we cannot derive last-iterate convergence rates for ExRM+ or SPRM+ via existing results in [2]. The non-monotonicity of the regret operator also partly explains why the last-iterate convergence behavior of regret matching-type algorithms was previously not understood.\n\n**Q**: *Additionally, I suggest that the paper could enhance its value by presenting time-average results for the different RM variants. It would be particularly intriguing to see the time-average convergence rates of the alternating PRM+ algorithm, which, as demonstrated in the provided experiments, seems to significantly outperform other RM methods in terms of last-iterate convergence. Furthermore, an experimental comparison of the last-iterate properties of Extragradient and OMD would be a valuable addition.*\n\n**A**: We have conducted additional numerical experiments to answer your question. \nIn the following anonymized link: https://docdro.id/6dvF63M, we plot in Figure 2 the average performance of SPRM+, ExRM+, alt PRM+, Extragradient algorithm (EG) and optimistic mirror descent (OMD). on the 3x3 matrix game from Section 3 in the paper. As evident from the plots, all these algorithms have similar empirical performances (for the average of the iterates). \nIn the same document in Figure 1, we compare the last-iterate performance of all the algorithms presented in the paper, as well as EG and OMD. We see that ExRM+ and SPRM+ have similar performances as EG and OMD, and alt PRM+ is faster.\nWe would like to conclude by highlighting two facts:\n1. The performance of the average iterates of (predictive) RM+ with/without alternation have been studied in the past [3, 4].\n2. While the alt PRM+ algorithm appears to have strong empirical performance, in the existing literature, there is no proof that alt PRM+ converges either on average or in iterates. \n\n\nWe thank you for this interesting point, we will add this in the revised paper.\n\n**Q**: *Is there a limit for the constant $c$ in Proposition 2? Could it potentially be exceedingly small, perhaps even exponentially so?*\n\n**A**: The constant $c$ depends on the game matrix and can be arbitrarily small. The metric subregularity condition on the constant $c$ is introduced and utilized in previous works (see e.g., [2] and references therein). Our results are comparable to the linear last-iterate convergence results for OGDA [2], which also depend on the constant $c$. We remark that we also provide a $1/\\sqrt{T}$ best-iterate convergence rate that is independent of $c$. \n\n[1] Finite-Time Last-Iterate Convergence for Learning in Multiplayer Games. Yang Cai, Argyris Oikonomou, Weiqiang Zheng. NeurIPS 2022\n\n[2] Wei, Chen-Yu, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. \"Linear Last-iterate Convergence in Constrained Saddle-point Optimization.\" ICLR 2021\n\n[3] Burch, N., Moravcik, M., & Schmid, M.  Revisiting CFR+ and alternating updates. JAIR, 2019\n\n[4] Farina, Gabriele, Julien Grand-Cl\u00e9ment, Christian Kroer, Chung-Wei Lee, and Haipeng Luo. \"Regret Matching+:(In) Stability and Fast Convergence in Games. \" NeurIPS, 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243606604,
                "cdate": 1700243606604,
                "tmdate": 1700243606604,
                "mdate": 1700243606604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n54RZMqywf",
                "forum": "fWk5Qx0exc",
                "replyto": "OF0yCo6svr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_RSe5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_RSe5"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you for your response and for taking the time to conduct and present the additional experiments. I understand that your techniques may differentiate from the previous last-iterate analysis which is definitely a merit of your work.\n\nThe presented algorithm do not seem to appear any convergence advantage in the time-average sense while they admit similar performance with OMD and Extra-Gradient in the last-iterate sense (from the provided figures it seems that OMD and ExtraGradient admit comparable performance with alt PRM+ in the last-iterate sense).\n\nAt the same time, the constant $c$ can be exponentially small with respect to the number of actions. As a result, the resulting dynamics may need exponential time (wrt to the number of actions) before reaching a min-max equilibrium. I understand though that the latter caveat appears also in previous works.\n\nTo conclude I believe that your work contains some interesting results however I still reserve some doubts on the motivation.\n\nI will wait for your response in the AC's question regarding the novelty of your technical contribution before updating my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556181550,
                "cdate": 1700556181550,
                "tmdate": 1700556181550,
                "mdate": 1700556181550,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kVbOb17W6I",
            "forum": "fWk5Qx0exc",
            "replyto": "fWk5Qx0exc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_cEcU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_cEcU"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the classical min-max matrix game and discusses the convergence properties of several algorithms.  In particular, the paper shows the last-iterate property holds for some algorithms, where the property means that the last update of the solution is the output of the algorithm. In general, many iterative algorithms based on online linear optimization (such as Hedge) have a solution based on the average of iterative updates. The experimental results compare several iterative algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The theoretical analyses are solid. In particular, the last-iterate property might be interesting for iterative algorithms, especially if the design is based on the online convex (linear) optimization. However, I feel that the asymptotic statement of the last-iterate property is not strong enough."
                },
                "weaknesses": {
                    "value": "The min-max game itself is known to be an LP and thus solved in polynomial time. Iterative algorithms are one of the approaches to solving the LPs. The paper should consider comparing the state-of-the-art LP solvers with iterative algorithms. Although iterative algorithms are easy to implement, practical LP solvers have been improved for a long time and could be faster than naive iterative algorithms.\n\nI do not understand why the last-iterate property is important. The convergence results can still be obtained by averaging the outputs of iterative OLO-based algorithms (such as Hedge). Maybe a more important issue is the speed itself, not whether the property holds or not.\n\nI am afraid that the experimental data is rather too small for LP instances. For such small instances, I wonder that the sota LP solver such as Gurobi solve them much faster. \n\nIn summary, the paper focuses only on iterative algorithms, but as a solver of a certain LP, there are more alternatives to compare."
                },
                "questions": {
                    "value": "Are the analyses for the last iterate property useful for constructing a new online-to-batch conversion technique (e.g., averaging all outputs of OCO algorithms)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698987320502,
            "cdate": 1698987320502,
            "tmdate": 1699636420491,
            "mdate": 1699636420491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dLLEXvzp9n",
                "forum": "fWk5Qx0exc",
                "replyto": "kVbOb17W6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cEcU: Part 1"
                    },
                    "comment": {
                        "value": "**Q**: *The min-max game itself is known to be an LP and thus solved in polynomial time. Iterative algorithms are one of the approaches to solving the LPs. The paper should consider comparing the state-of-the-art LP solvers with iterative algorithms. Although iterative algorithms are easy to implement, practical LP solvers have been improved for a long time and could be faster than naive iterative algorithms.*\n\n**A**: Thanks for your comment! The focus on iterative algorithms like Regret Matching (RM), RM+ and their variants over Linear Programming (LP) methods is driven by two reasons. \n\nOne of our motivations to study RM and its variants is that they are simple online learning algorithms, and understanding the convergence properties of uncoupled learning dynamics in games is an important question at the intersection of machine learning and game theory. In a multi-agent learning scenario, agents interact with each other and update their strategies in their own interest. Existence of simple learning dynamics that converge to Nash equilibrium in a day-to-day sense (i.e. in last iterate) provide stability guarantees for multi-agent systems, and provide some justification for using Nash equilibrium to predict the outcome of real-life multi-agent interactions. Prior to our work, the literature had focused on the behavior of optimistic variants of FTRL and OMD. Yet RM-type algorithms are very simple, and have a history of performing well in practice (more on that in the next paragraph), and thus their last-iterate convergence behavior is of interest in its own right.\n\nOur study is also motivated by the strong practical performance of RM-type iterative algorithms, especially in the context of large-scale extensive-form games. While LP methods might offer advantages in smaller games, their application becomes less feasible in larger, more complex scenarios. This is evident from the historical context: the last notable application of LP for offline two-player zero-sum games was in 2005, which successfully solved Rhode Island Hold\u2019em [1, 2]. However, the practicality of LP diminishes as the game size increases. The size of LP formulations is linear in the game tree size, making the use of simplex or Interior Point Method (IPM) iterations extremely slow for larger games. Since 2007, there has been a significant shift in the field towards using iterative algorithms like Counterfactual Regret Minimization (CFR) [3], dilated Entropy-regularized algorithms such as excessive gap technique (EGT) and CFR+ [4]. We remark that CFR and CFR+ build upon RM and RM+, respectively. Every superhuman poker AI developed since then has utilized CFR+ [5, 6, 7]. Despite their strong practical performance in large-scale games, RM-type algorithms are not well-understood theoretically, which motivates our study.\n\n[1] Gilpin, Andrew, and Tuomas Sandholm. \u201cOptimal Rhode Island Hold\u2019em Poker.\u201d AAAI, 2005 \n\n[2] Gilpin, Andrew, and Tuomas Sandholm. \"Lossless abstraction of imperfect information games.\" Journal of the ACM, 2007\n\n[3] Zinkevich, M., Johanson, M., Bowling, M., Piccione, C. Regret minimization in games with incomplete information. NeurIPS, 2007\n\n[4] Tammelin, Oskari. \"Solving large imperfect information games using CFR+.\" 2014\n\n[5] Bowling, M., Burch, N., Johanson, M., & Tammelin, O. Heads-up limit hold\u2019em poker is solved. Science, 2015\n\n[6] Brown, Noam, and Tuomas Sandholm. \"Superhuman AI for heads-up no-limit poker: Libratus beats top professionals.\" Science, 2018\n\n[7] Brown, Noam, and Tuomas Sandholm. \"Superhuman AI for multiplayer poker.\" Science, 2019"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242510147,
                "cdate": 1700242510147,
                "tmdate": 1700242510147,
                "mdate": 1700242510147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7xPWuWEmlW",
                "forum": "fWk5Qx0exc",
                "replyto": "kVbOb17W6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cEcU: Part 2"
                    },
                    "comment": {
                        "value": "**Q**: *I do not understand why the last-iterate property is important. The convergence results can still be obtained by averaging the outputs of iterative OLO-based algorithms (such as Hedge). Maybe a more important issue is the speed itself, not whether the property holds or not.*\n\n**A**: We can average the iterates and get convergence results. However, last-iterate convergence is still important from both theoretical and practical point of view, as we have motivated in the introduction of the paper. \nIf we view learning as a model of agents\u2019 strategic behavior, then only the last-iterate convergence of learning dynamics guarantees that agents\u2019 strategies (day-to-day behavior) stabilize at Nash equilibrium. Learning dynamics with only average-iterate convergence is unstable and exhibits cycling and divergence behavior. For example, the Hedge algorithm does not converge in last-iterate even for simple zero-sum games [8]. The problem of understanding last-iterate convergence for learning in games has received extensive attention in recent years, see e.g [9, 10, 11, 12, 13] and more references therein. \nLast-iterate is widely used in practice for simplicity. In some cases, iterate averaging can be cumbersome and even impractical when neural networks are involved. Thus it is crucial to develop theoretical tools to understand last-iterate convergence of learning algorithms. \n\n\n**Q**: *Are the analyses for the last iterate property useful for constructing a new online-to-batch conversion technique (e.g., averaging all outputs of OCO algorithms)?*\n\n**A**: We think the online-to-batch technique is not related to last-iterate convergence. This is because online convex optimization is a single-agent learning problem, where the goal is to minimize regret against an adversary; while learning in games is a multi-agent learning problem and the goal is to prove last-iterate convergence to Nash equilibrium. The following examples further illustrate the differences between regret minimization and last-iterate convergence in games.\n\n1. The Extragradient (EG) algorithm suffers linear regret [11] in OCO but when both players employ EG in a zero-sum game, their joint strategy converges to a Nash equilibrium [13].\n2. The online gradient descent (OGD) algorithm is a no-regret online algorithm but when both players employ OGD in a zero-sum game, their joint strategy diverges from Nash equilibrium [8]. \n\n[8] Mertikopoulos, Panayotis, Christos Papadimitriou, and Georgios Piliouras. \"Cycles in adversarial regularized learning.\" SODA, 2018\n\n[9] Daskalakis, Constantinos, and Ioannis Panageas. \"Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization.\" ITCS, 2019\n\n[10] Lin, T., Zhou, Z., Mertikopoulos, P., & Jordan, M. I.. Finite-time last-iterate convergence for multi-agent learning in games. ICML 2020.\n\n[11] Golowich, N., Pattathil, S., & Daskalakis, C.  Tight last-iterate convergence rates for no-regret learning in multi-player games. NeurIPS, 2020\n\n[12] Wei, C. Y., Lee, C. W., Zhang, M., & Luo, H. Linear Last-iterate Convergence in Constrained Saddle-point Optimization. ICLR, 2021\n\n[13] Cai, Y., , Oikonomou, A., Zheng W. Finite-Time Last-Iterate Convergence for Learning in Multiplayer Games. NeurIPS 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242783196,
                "cdate": 1700242783196,
                "tmdate": 1700242783196,
                "mdate": 1700242783196,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZHa3kOPFkg",
            "forum": "fWk5Qx0exc",
            "replyto": "fWk5Qx0exc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the last iterate convergence of regret matching algorithms in normal-form games. They show that a large class of well-known regret matching algorithms are not guaranteed to converge in last iterates and provide  EXRM+  and SPRM+ as alternatives that converge at a rate of 1/sqrt(T), and whose convergence rate can further improved to be linear"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The non-convergence examples in last iterates are interesting and the linear convergence rates under restarting seem interesting."
                },
                "weaknesses": {
                    "value": "The convergence results for EXRM+ and SPRM+ seem to be direct from known results (?) (see [1] for EXRM, and [2] for SPRM+). The authors claim that their setting does not satisfy the monotonocity assumption, but zero-sum bimatrix games do satisfy the monotonicity assumption (correct me if I am wrong?)"
                },
                "questions": {
                    "value": "Can you clarify the comments at the bottom of page 5? It is not clear to me why your setting is not a monotone game setting?\n\nCan you explain why the restarted variants of your algorithms converge faster?\n\n[1] Gorbunov, Eduard, Nicolas Loizou, and Gauthier Gidel. \"Extragradient method: O (1/k) last-iterate convergence for monotone variational inequalities and connections with cocoercivity.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\n[2] Cai, Yang, Argyris Oikonomou, and Weiqiang Zheng. \"Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities.\" arXiv preprint arXiv:2204.09228 (2022)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4453/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV",
                        "ICLR.cc/2024/Conference/Submission4453/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699170097768,
            "cdate": 1699170097768,
            "tmdate": 1700701830735,
            "mdate": 1700701830735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xKDME9datA",
                "forum": "fWk5Qx0exc",
                "replyto": "kVbOb17W6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "content": {
                    "comment": {
                        "value": "I am not sure I understand why the results of the authors is not something already known? Am I missing something?\n\nWe already know that in convex-concave min-max games optimistic gradient descent ascent, and extragradient descent ascent converges in last-iterates. One of these algos (namely extragradient descent ascent) is one that the authors propose... but the convergence rate and last-iterate convergence is known (see for instance [1])? \n\nI am not disputing the correctness of the results of the paper, but I think that the authors' results are subsumed by previous papers.\n\n[1] Cai, Yang, Argyris Oikonomou, and Weiqiang Zheng. \"Tight last-iterate convergence of the extragradient and the optimistic gradient descent-ascent algorithm for constrained monotone variational inequalities.\" arXiv preprint arXiv:2204.09228 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972996586,
                "cdate": 1699972996586,
                "tmdate": 1699973044025,
                "mdate": 1699973044025,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TRDUUxUBLE",
                "forum": "fWk5Qx0exc",
                "replyto": "ZHa3kOPFkg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reivew and we address your questions below.\n\n**Q**: *Can you clarify the comments at the bottom of page 5? It is not clear to me why your setting is not a monotone game setting?*\n\n**A**: Our results do **not** follow from the referenced analysis of EG and OG [2]. We will clarify the misunderstanding below. First, monotonicity is defined on an operator. In the literature, a game is called monotone if its **gradient operator** is monotone: a zero-sum game is a monotone game because its gradient operator $F_G(z)  := (Ay, -A^Tx)$ is a monotone operator (i.e., it satisfies $\\langle F_G(z)- F_G(z\u2019), z-z\u2019 \\rangle \\ge 0$ for all $z, z\u2019 \\in \\Delta^{d_1} \\times \\Delta^{d_2}$). We can run EG and OG regardless of the monotonicity of the operator. The paper [2] shows that when the operator is monotone, EG and OG have last-iterate convergence rates, which is also acknowledged in the current paper (page 5, bullet point 2). \n\nHowever, the algorithms ExRM+ and SPRM+ (proposed by a recent paper [1]) studied in this paper are regret matching-type algorithms. ExRM+ and SPRM+ are equivalent to running EG and OG using the **regret operator** $F(z)  := (Ay - x^T A y  \\cdot 1_{d_1} , -A^Tx + x^T A y \\cdot 1_{d_2})$ over a lifted space $Z_{\\ge}$ (F is defined in equation (2) in the paper). The regret operator $F$ is **not monotone** over $Z_{\\ge}$. That is why we can not derive last-iterate convergence rates for ExRM+ or SPRM+ by existing results in [2]. The non-monotonicity of the regret operator also partly explains why last-iterate convergence of regret matching-type algorithms are poorly understood before.\n\n**Q**: *Can you explain why the restarted variants of your algorithms converge faster?*\n\n**A**: The short answer is that they may not converge faster. We prove sublinear $1/\\sqrt{t}$ best-iterate convergence rates of ExRM+ and SPRM+ and linear last-iterate convergence of restarted variants of ExRM+ and SPRM+. But our experiments show that restarting does not significantly affects the last-iterate convergence speed of ExRM+ and SPRM+ (see Appendix E.2). Thus we conjecture that ExRM+ and SPRM+ without restarting already have linear last-iterate convergence. It would be very interesting to investigate this problem in the future. \n\n[1] Regret Matching+: (In)Stability and Fast Convergence in Games. Gabriele Farina, Julien Grand-Cl\u00e9ment, Christian Kroer, Chung-Wei Lee, and Haipeng Luo. NeurIPS 2023\n\n[2] Finite-Time Last-Iterate Convergence for Learning in Multiplayer Games. Yang Cai, Argyris Oikonomou, Weiqiang Zheng. NeurIPS 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984622954,
                "cdate": 1699984622954,
                "tmdate": 1699984622954,
                "mdate": 1699984622954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MtxaoFCr1g",
                "forum": "fWk5Qx0exc",
                "replyto": "TRDUUxUBLE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their explanation, this is very helpful for me. The question that remains for me at this point to change my current score is: why should we care about about running EG and OG on the regret operator rather than the game gradient operator?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546955779,
                "cdate": 1700546955779,
                "tmdate": 1700546955779,
                "mdate": 1700546955779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S8Jk6ieigD",
                "forum": "fWk5Qx0exc",
                "replyto": "sK6DLmjOVG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4453/Reviewer_zjHV"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answer, I have increased my score accordingly to a 6. The explanation for my score is as follows:\n\nThe paper studies an interesting problem and the counterexamples of RM+ not converging in last iterates, although known I don't think have been written down in a paper with spelled-out examples. That said, I am not totally convinced that there is a need to derive algorithms based on the regret operator rather than the game gradient operator. All this to say, my score should not invalidate the authors' contributions which are valuable but rather it reflects my relative ranking of the papers' contributions compared to other papers."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702242773,
                "cdate": 1700702242773,
                "tmdate": 1700702242773,
                "mdate": 1700702242773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]