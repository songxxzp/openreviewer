[
    {
        "title": "On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection"
    },
    {
        "review": {
            "id": "JE0TeH9cDh",
            "forum": "sLregLuXpn",
            "replyto": "sLregLuXpn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_YTga"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_YTga"
            ],
            "content": {
                "summary": {
                    "value": "## Summary\n* This paper studies the problem of noise-resistent I2I generation. Particularly, it studies the noise injection approach both theoretically and practically. The paper shows that the joint f-divergence does not change too abruptly when the source is polluted by Gaussian noise. Further, the paper finds optimal noise level for training when the source is Gaussian, and verify their results on Gaussian source and real images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "## Strength\n* The result of optimal training level in __Corollary 1__ is useful to practical training of noise resistent generative model, if it is not limited to Gaussian source. It provides theoretical justification to a  intuitative practice.\n* The empirical result show that their approach improves the noise-robustness in various cases."
                },
                "weaknesses": {
                    "value": "## Weakness\n* The discussion after __Lemma 1__ only convers non-Gaussian noise, but the majority of experiment is about non-Gaussian source. The gap of source distribution should at least be explicitly discussed in the main text not appendix, if not well addressed. Currently the proof of __Corollary 1__ seems to rely on Eq. 6 of __Lemma 1__, which holds only when the source is Gaussian. If I understand correctly, in that case, it becomes unreasonable to derive the optimal noise level $0.08$ for actual image generation, which is obviously non-Gaussian. If the theory is not connected to the experiment, then this paper becomes a little bit unconvincing. As no new empirical approach is proposed and the empirical results alone is not enough for accepting this paper."
                },
                "questions": {
                    "value": "## Questions\n* Is it possible to extend current theory (__Lemma 1__, __Theorem 2__, __Corollary 1__) to non-Gaussian source with known $\\mu,\\Sigma$? As Gaussian is the max-entropy distribution with known 1st and 2nd moment, can similar results be obtained?\n* Is it possible to extend current theory (__Lemma 1__, __Theorem 2__, __Corollary 1__) to mixture of Gaussian source? As we can approximate any distribution, including natural image distribution with GMM, it would be much better if we can extend the theory to GMM, even for finite mixture. In this way, the distance between the current theory and empirical results on natural image can be reduced.\n* Current analysis on joint distribution looks quite general, can it be extended into any conditional GAN, beyond I2I?\n* What about more real-life noise, e.g. JPEG compression?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Reviewer_YTga",
                        "ICLR.cc/2024/Conference/Submission2235/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698141347676,
            "cdate": 1698141347676,
            "tmdate": 1700547521893,
            "mdate": 1700547521893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jzF3pE362Q",
                "forum": "sLregLuXpn",
                "replyto": "JE0TeH9cDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YTga-1"
                    },
                    "comment": {
                        "value": "Thanks for your insightful feedback and comments. Below are point-to-point reply of your comments. \n### Q3-1:  Extention of  Lemma 1, Theorem 2, Corollary 1 to non-Gaussian source: \n### Reply 3-1\nWe appreciate the reviewer raising this point. \n\nFor **Lemma** 1 and **Corollary** 1, as the derivations require closed-form expressions of KL divergences, extension to general non-Gaussian sources cannot be easily achieved. For **Theorem** 2, we generalize some results to non-Gaussian sources in **Theorem** 3. \n**Theorem 3:** Let $\\\\mathbf{X}$ be a $d$-dimensional random vector. Denote $\\\\theta(\\\\sigma^2\\_t,\\\\sigma^2\\_e\\\\boldsymbol{\\\\Sigma}\\_{\\\\widetilde{e}})$ as the KL-divergence of non-Gaussian source signal for arbitrary noise, $\\\\theta\\_g(\\\\sigma^2\\_t,\\\\sigma^2\\_e\\\\boldsymbol{\\\\Sigma}\\_{\\\\widetilde{e}})$ denotes the special case of $\\\\theta(\\\\sigma^2_t,\\\\sigma^2_e\\\\boldsymbol{\\\\Sigma}_{\\\\widetilde{e}})$ when $\\\\boldsymbol{E}$ is Gaussian noise. Then for small $\\\\sigma^2_e$ with $\\\\sigma^2_e\\\\ll 1$, we have\n\n$\\\\theta(\\\\sigma^2\\_t,\\\\sigma^2\\_e\\\\boldsymbol{\\\\Sigma}\\_{\\\\widetilde{e}})=\\\\theta\\_g(\\\\sigma^2\\_t,\\\\sigma^2\\_e\\\\boldsymbol{\\\\Sigma}\\_{\\\\widetilde{e}})+o(\\\\sigma^2\\_e)$\n\nThus, **Theorem 3** generalizes the result of **Theorem 2** to non-Gaussian sources, demonstrating that an I2I system's resilience to Gaussian noise ensures robustness against noises with the same covariance matrices, regardless of whether the input signals are Gaussian or non-Gaussian. \n\nWe want to emphasize that adopting Gaussian signal models is a common theoretical approach even when practical signals are non-Gaussian. This allows deriving foundational insights that can be obscured in more complex settings. For example, in I2I, the widely used Fr\u00e9chet Inception Distance (FID) assumes Gaussian distributions, despite natural images not being strictly Gaussian. Our work follows this principled methodology - establishing core theoretical results with Gaussian signals first.\n\nWe agree extending the analysis to non-Gaussian sources is an important future direction. In fact, we take a first step in deriving Theorem 3 of the revised paper, showing the covariance-matching result holds for non-Gaussian signals when noise levels are small. However, a full generalization of all results is non-trivial and requires care to handle the increased complexity.\n\nBy starting from Gaussian signal analysis, we lay a solid foundation upon which future research can build. This approach provides a rigorous basis for expanding into non-Gaussian settings, offering fundamental insights that can be further enriched to encompass more realistic signal models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462504853,
                "cdate": 1700462504853,
                "tmdate": 1700462518012,
                "mdate": 1700462518012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hb7cecvkmi",
                "forum": "sLregLuXpn",
                "replyto": "JE0TeH9cDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YTga-2"
                    },
                    "comment": {
                        "value": "### Q 3-2: Extension of Lemma 1, Theorem 2 and Corollary 1 to Gaussian mixture model (GMM) \n\nThanks for your suggestions of an extension to GMM signals. We include the discussions in Appendix of our revised paper.  \n\nConsider a source signal $\\\\boldsymbol{x}$ represented by the Gaussian mixture model that $p\\_{\\\\boldsymbol{X}}(\\\\boldsymbol{x})=\\\\sum\\_{k=1}^{N}\\\\pi_k\\\\cdot\\\\mathcal{N}(\\\\boldsymbol{x}|\\\\boldsymbol{\\\\mu}\\_k,\\\\boldsymbol{\\\\Sigma}\\_k)$, $\\\\sum\\_{k=1}^{N}\\\\pi_k=1$ and $\\\\pi_k>0$.  Although we cannot get a closed-form expression of the KL divergence for the GMM signal source, we can extend our results to its upper-bound, as listed below: \n\n- **Extention of Lemma 1**:  By using the convexity property of KL divergence, we have $D\\_{KL}(\\\\hat{P}\\_{\\\\boldsymbol{X}+\\\\boldsymbol{E}}\\\\|\\\\bar{P}\\_{\\\\boldsymbol{X}+\\\\sigma\\_t\\\\boldsymbol{N}})\\\\leq\\\\zeta(\\\\sigma^2\\_t,\\\\boldsymbol{\\\\Sigma}\\_e)$. \n$$\n\\begin{aligned}\n\\\\zeta(\\\\sigma^2\\_t,\\\\boldsymbol{\\\\Sigma}\\_e)&= \\\\sum_{k=1}^{N}\\\\pi\\_k\\\\cdot D\\_{KL}(\\\\hat{P}\\_{\\\\boldsymbol{X}\\_k+\\\\boldsymbol{E}}\\\\|\\\\bar{P}\\_{\\\\boldsymbol{X}\\_k+\\\\sigma\\_t\\\\boldsymbol{N}})\\=\\\\sum_{k=1}^{N}\\\\pi_k\\\\cdot\\\\rho\\_{k}(\\\\sigma\\_t^2,\\\\boldsymbol{\\\\Sigma}\\_e),\n\\end{aligned}\n$$\nwhere $\\\\boldsymbol{X}$ is d-dimensional random variable of the GMM represented source signal $\\\\boldsymbol{x}$, $\\\\boldsymbol{X}\\_k$ is the $k$-th Gaussian distribution with its weight $\\pi_k$ for GMM and $\\rho_{k}(\\sigma_t^2,\\boldsymbol{\\Sigma}_e)$ is the KL-divergence of $k$-th Gaussian distribution for arbitrary noise. \nThe above equation implies that for GMM fitted source signal, its KL-divergence concerning arbitrary noise is upper-bounded by the weighted sum of the KL-divergence of $N$ Gaussian source signals with respect to arbitrary noise. \n\n- **Extension of Theorem 2**:  For $\\\\zeta(\\\\sigma^2\\_t,\\\\boldsymbol{\\\\Sigma}\\_e)$ given above, when $\\\\boldsymbol{\\\\Sigma}\\_e=\\\\sigma^2\\_e\\\\boldsymbol{\\\\Sigma}\\_{\\\\tilde{e}}$,   $\\\\zeta(\\\\sigma^2\\_t,\\\\boldsymbol{\\\\Sigma}\\_e)$ is also convex in $\\sigma^2_e$ as each  $\\rho_{k}(\\sigma_t^2,\\boldsymbol{\\Sigma}_e)$ is convex in $\\sigma^2_e$. Likewise, $\\zeta(\\sigma^2_t,\\boldsymbol{\\Sigma}_e)<\\zeta(0,\\boldsymbol{\\Sigma}_e)$ when $\\boldsymbol{\\Sigma}_e>\\sigma^2_t/2$. \n\n- **Extension of Corollary 1**: Under the assumptions of Corollary 1, the optimal solution \n$$\n\\\\bar{\\sigma}\\_{t,o}^2 = \\\\arg \\\\min\\_{\\\\sigma\\_t^2} \\\\mathbb{E}\\_{\\\\sigma\\_e^2\\\\sim\\\\mathcal{U}(0,\\\\lambda\\_{\\\\max})}\\\\left\\\\{\\\\zeta(\\\\sigma^2\\_t,\\\\sigma\\_e^2\\\\boldsymbol{I}\\_d)\\\\right\\\\}\n$$ \nis still $\\\\bar{\\\\sigma}\\_{t,o}^2=\\\\frac{1}{2}\\\\lambda\\_{\\\\max}$, which remains the same as that of the Gaussian source.\n\n\n### Q3-3: Comments on optimal solution of  $\\sigma^2_t$ for non-Gaussian signal source. \n### Reply 3-3: \n\nWe appreciate your comments on this issue. As you have pointed out, many image signals follow GMM. In the above reply **Q3-2**,  we have theoretically extended the result to Gaussian Mixture Model (GMM) signals, which better represent images. For the upper bound of the KL divergence with a GMM source, we show the optimal solution of $\\\\sigma^2\\_t$ is still $\\\\frac{\\\\lambda\\_{\\\\max}}{2}$. \n\nBesides, for face images to sketch task, the ablation study on $\\\\sigma^2\\_t$ also showed that the average FID is minimized when $\\\\sigma^2\\_t=0.08$ given a specific $\\\\lambda\\_{\\\\max}=0.16$.  Thus, the empirical results agree well with our theoretical results. \n\n|    Noise Type   |   $\\sigma^2_{t}=0$   |  $\\sigma^2_{t}=0.01$  |   $\\sigma^2_{t}=0.04$|     $\\sigma^2_{t}=0.08$   |   $\\sigma^2_{t}=0.16$ | Learnable |\n|:---------------:|:------:|:------:|:-----:|:---------:|:-----:|:---------:|\n|  Gaussian Noise | 182.55 |  78.89 | 42.17 | **36.18** | 36.64 |   59.19   |\n|  Uniform Noise  | 199.17 | 101.68 | 45.37 | **36.79** | 36.89 |   64.81   |\n| Laplacian Noise | 152.09 |  64.12 | 39.17 | **36.99** | 37.01 |   52.41   |\n------\n\nIn summary, while **Corollary** 1 is based on Gaussian signal source assumption, we have extended it to GMM signals and empirically verified the optimal noise results on real image data. This highlights the relevance of the analysis beyond Gaussian sources.\n\n### Q3-4: Extention to other applications of conditional GANs\n### Reply 3-4: \nWe thank the reviewers for this suggestion. This would be considered in our future work for other applications of conditional GAN."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463276358,
                "cdate": 1700463276358,
                "tmdate": 1700463276358,
                "mdate": 1700463276358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yrEecvejNN",
                "forum": "sLregLuXpn",
                "replyto": "JE0TeH9cDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YTga-3"
                    },
                    "comment": {
                        "value": "###  Q3-5: Performance on real-life noise\n### Reply 3-5:\nFollowing the reviewers' advice,  we expanded our simulations to include signal-dependent noise and corruptions using the ImageNet-C corruption models. Specifically, we adopted the widely-used image degradation model $\\\\boldsymbol{\\\\hat{x}}=\\\\boldsymbol{A}(\\\\boldsymbol{x})+\\\\boldsymbol{n}$, where $\\\\boldsymbol{x}$ and $\\\\boldsymbol{\\\\hat{x}}$ denote clean and corrupted images, respectively, $\\\\boldsymbol{A}$ represents an operator (e.g., JPEG compression, blurring, pixelation, etc.), and $\\\\boldsymbol{n}$ denotes the noise component. \n\nOur expanded simulations covered shot noise, speckle noise, JPEG compression, pixelation, blurring, and combinations of these distortions. Key findings include:\n\n - When signal-dependent noise $\\\\boldsymbol{n}$ is present alone, models trained with Gaussian noise injection exhibited substantial improvements over baseline models trained solely on clean images, akin to the results for signal-independent noises.\n - For blurring, pixelation and JPEG compression only (with $\\\\boldsymbol{n}=0$),  models trained with Gaussian noise injection are inferior to the baselines in some cases. \n - When both $\\\\boldsymbol{A}$ and $\\\\boldsymbol{n}$ were present, models trained with Gaussian noise injections consistently demonstrated significant enhancements over the baseline models. This observation aligns with practical scenarios where both $\\\\boldsymbol{A}$ and $\\\\boldsymbol{n}$ coexist in degradation models, underscoring the efficacy of Gaussian noise injection.\n\nNotably, in cases where the corruption model excluded the noise component, the addition of a small amount of noise to the input images proved effective in producing outputs with stable quality. These findings provide further insights into the robustness and potential of Gaussian noise injection in I2I for various degradations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463513950,
                "cdate": 1700463513950,
                "tmdate": 1700463513950,
                "mdate": 1700463513950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dHWD3kAygx",
                "forum": "sLregLuXpn",
                "replyto": "JE0TeH9cDh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YTga-4"
                    },
                    "comment": {
                        "value": "| Task | Cat->Dog(Latent) |\n|      Type     | Metric |  Method  |   Clean   |     S1    |     S2    |     S3    |     S4    |     S5    |     S6    |\n|:-------------:|:------:|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n|   Shot Noise  |   FID  | Baseline | **22.82** | **23.42** |   23.74   |   24.31   |   26.09   |   30.91   |   37.51   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   24.03   | **23.44** | **22.85** | **22.39** | **21.76** | **21.88** |\n|               |   KID  | Baseline |  **9.67** |   10.86   |   11.09   |   11.59   |   13.49   |   18.27   |   24.51   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |  **9.31** |  **8.98** |  **8.46** |  **8.12** |  **7.79** |  **7.91** |\n| Speckle Noise |   FID  | Baseline | **22.82** | **23.36** |   24.25   |   25.41   |   26.98   |   29.35   |   31.53   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   24.25   | **23.02** | **22.37** | **22.08** | **21.99** | **21.91** |\n|               |   KID  | Baseline |  **9.67** |   10.69   |   11.64   |   12.97   |   14.53   |   16.97   |   19.15   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |  **9.46** |  **8.61** |  **8.37** |  **7.96** |  **8.07** |  **7.92** |\n|   Glass Blur  |   FID  | Baseline | **22.82** | **22.85** | **23.91** | **23.04** | **23.12** | **23.51** | **23.98** |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   25.06   |   25.09   |   25.18   |   24.89   |   24.61   |   24.51   |\n|               |   KID  | Baseline |  **9.67** |  **9.69** |  **9.82** |  **9.86** |   10.06   |   10.44   |   10.56   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |    9.88   |    9.92   |   10.03   |  **9.98** |  **9.91** |  **9.77** |\n|  Defocus Blur |   FID  | Baseline | **22.82** | **23.08** | **23.41** | **24.71** | **25.71** |   26.52   |   27.13   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   25.92   |   26.01   |   26.02   |   25.91   | **25.69** | **25.45** |\n|               |   KID  | Baseline |  **9.67** |  **9.81** |  **9.98** |   10.71   |   11.31   |   11.97   |   12.45   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |   10.44   |   10.61   | **10.67** | **10.59** | **10.48** | **10.29** |\n|    Pixelate   |   FID  | Baseline | **22.82** | **22.89** | **23.31** | **23.42** | **23.85** | **23.91** | **24.01** |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   25.29   |   25.84   |   26.01   |   25.81   |   25.71   |   25.69   |\n|               |   KID  | Baseline |  **9.67** |  **9.72** |  **9.82** | **10.06** | **10.21** | **10.44** |   10.91   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |   10.03   |   10.35   |   10.51   |   10.54   |   10.45   | **10.42** |\n|      JPEG     |   FID  | Baseline | **22.82** | **22.91** | **22.95** | **23.01** | **23.56** | **24.21** |   25.46   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |   25.04   |   25.19   |   24.94   |   25.21   |   25.41   |   25.43   | **25.45** |\n|               |   KID  | Baseline |  **9.67** |  **9.79** |   10.01   |   10.07   |   10.35   |   10.53   |   12.24   |\n|               |        | $+\\mathcal{N}(0, 0.04)$ |    9.89   |   10.03   |  **9.77** |  **9.96** | **10.11** | **10.25** | **10.31** |\n\n---\n| Cat->Dog(Latent) | FID | Robustness evaluation to multiple image degradations  |\n|     Setting     | Glass Blur | Defocus Blur |  Pixelate |    JPEG   | Glass Blur + Pixelate | Defocus Blur + JPEG |\n|:---------------:|:----------:|:------------:|:---------:|:---------:|:---------------------:|:-------------------:|\n|     Baseline    |    23.12   |     25.71    |   23.85   |   23.26   |         26.35         |        25.93        |\n| Noise Injection |    24.89   |     25.91    |   25.81   |   25.41   |         25.86         |        25.99        |\n|   Baseline $+Noise^1$   |    26.98   |     28.99    |   27.81   | 26.75     |         28.51         |        29.51        |\n|   Baseline $+Noise^2$   |    26.24   |     27.31    |   26.56   | 26.01     |         27.11         |        27.44        |\n|  Noise Injection $+Noise^1$  |  **22.21** |   **22.75**  | **22.53** | **22.41** |       **22.44**       |      **22.61**      |\n|  Noise Injection $+Noise^2$  |    22.67   |     22.84    |   22.81   |   22.49   |         23.05         |        22.75        |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463572833,
                "cdate": 1700463572833,
                "tmdate": 1700463572833,
                "mdate": 1700463572833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YFaXHC0Dur",
                "forum": "sLregLuXpn",
                "replyto": "dHWD3kAygx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_YTga"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_YTga"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal, I think the GMM extension is quite meanful for general noise. I raise my rating to accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547500784,
                "cdate": 1700547500784,
                "tmdate": 1700547500784,
                "mdate": 1700547500784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xnXXlMHasJ",
            "forum": "sLregLuXpn",
            "replyto": "sLregLuXpn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_Ceu7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_Ceu7"
            ],
            "content": {
                "summary": {
                    "value": "This work provides a robust theoretical framework elucidating the role of Gaussian noise injection in I2I translation models. They address critical questions on the influence of noise variance on distribution divergence, resilience to unseen noise types, and optimal noise intensity selection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper thoroughly investigates the Gaussian noise in I2I area from both the theory and experiment. \n- Extensive experiments and analysis make the effects of the Gaussian noise more clear to us."
                },
                "weaknesses": {
                    "value": "- Analysis of the Gaussian noise has been widely investigated, the authors should give some comparison or analysis about them in related works."
                },
                "questions": {
                    "value": "- How do you quantize the real and predictive distribution? Furthermore, how do you measure the diff between them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Reviewer_Ceu7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699075854229,
            "cdate": 1699075854229,
            "tmdate": 1699636156711,
            "mdate": 1699636156711,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZWABGRdtQV",
                "forum": "sLregLuXpn",
                "replyto": "xnXXlMHasJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Ceu7"
                    },
                    "comment": {
                        "value": "Thanks for your insightful feedback and comments. Below are point-to-point reply of your comments. \n\n### Q2-1: Measurement of the real and predictive distributions\n### Reply to 2-1: \nIn our theoretical analysis, we use $f$-divergence to quantify the disparity between two distributions, as illustrated in Figure 6 in the appendix. However, we acknowledge that $f$-divergence can be computationally challenging to calculate. Therefore, in our practical experiments, we employ popular I2I quality metrics like FID, KID, LPIPS, among others, to effectively measure the dissimilarity between the generated (predictive) distribution and the actual distribution. These metrics assess the quality of generated samples by evaluating the separation between the characteristic distribution of the generated models and that of authentic samples. Specifically, in Figure 5 of the main paper, we presented the FID results for face image-to-sketch translation tasks. The use of visual quality metrics in our experiments aligns well with our theoretical analysis of KL divergence, providing a comprehensive evaluation of the model's performance in terms of distribution dissimilarity. \n\nWe hope this clarifies our approach to quantifying the distinction between the real and predictive distributions.\n\n### Q2-2: Comments on Related-work on Gaussian noise analysis \n### Reply 2-2: \nWe appreciate your comment and would like to clarify that we have indeed addressed the context of Gaussian noise injection in related work, specifically in Section 2. In our related work section, we emphasize that existing theoretical analyses of Gaussian noise injection primarily focus on robustness against adversarial attacks in image classification and recognition tasks. These tasks involve discrete output variables, making their derivations less directly applicable to image-to-image (I2I) translation tasks, which operate in the domain of continuous image generation.\n\nThe key difference lies in the discrete nature of classification outputs compared to the more intricate image generation performed by I2I models. While some prior studies have explored noise injection in GAN-based I2I approaches, they often treat it as an empirical technique without in-depth theoretical analysis. To the best of our knowledge, our paper represents a pioneering effort in providing a comprehensive theoretical framework for understanding the role of noise injection in robust I2I tasks. For a more detailed discussion of these issues, we kindly refer the reviewer to Section 2 (related work) in the paper. We hope this clarifies the context and positioning of our research within the existing literature."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459815475,
                "cdate": 1700459815475,
                "tmdate": 1700618477461,
                "mdate": 1700618477461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zBnnqKx6HR",
                "forum": "sLregLuXpn",
                "replyto": "ZWABGRdtQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_Ceu7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_Ceu7"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Hi, authors:\n  Thanks for your rebutal."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710993299,
                "cdate": 1700710993299,
                "tmdate": 1700710993299,
                "mdate": 1700710993299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9oYd0HMg15",
            "forum": "sLregLuXpn",
            "replyto": "sLregLuXpn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_SHt1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2235/Reviewer_SHt1"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a robust theoretical framework for understanding the role of Gaussian noise injection in image-to-image (I2I) translation models. The key contributions of this work include:\n\n(i) Analyzing the influence of noise variance on distribution divergence and resilience to unseen noise types.\n\n(ii) Proposing a method to choose an optimal training noise level for consistent performance in noisy environments.\n\n(iii) Connecting f-divergence and score matching to explain the impact of Gaussian noise on aligning probability distributions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "(i) The writing is clear and easy to follow. The presentation is well-dressed.\n\n(ii) This paper conducts a detailed theoretical analysis in Section 3 to understand the role of Gaussian noise injection in I2I models from the perspective of alignment distribution.\n\n(iii) To validate the conjecture proposed in this paper, the authors conduct sufficient experiments including three types of I2I models, i.e., cat to dog, photo to sketch, and human face super-resolution, covering five types of noise: Gaussian, Uniform, Color, Laplacian, and Salt & Pepper."
                },
                "weaknesses": {
                    "value": "(i) This paper has a fatal theoretical flaw, i.e., the authors think too simply and naively about real-world degradations. They set up additive Gaussian noise in the training phase and five synthetic degradations (Gaussian, Uniform, Color, Laplacian, and Salt & Pepper.) in the testing phase. However, the real-world degradations are much more complex and fundamentally different from these degradations. To be specific, as for the noise term, the real-camera raw noise produced by photon sensing comes from multiple sources (e.g., short noise, thermal, noise, dark current noise, etc.) [1, 2, 3] and is further affected by the in-camera signal processing pipeline to become spatio-chromatically correlated. The real-camera noise contains both signal-dependent and -independent terms. However, the authors only consider very naive and simple signal-independent noise types, which is far from their motivation because the degradation patterns they studied simply do not exist in real-world images. Similar to the face super-resolution problem, the blur kernel cannot be explicitly estimated. \n\n[1] CycleISP: Real Image Restoration via Improved Data Synthesis. In CVPR 2020\n\n[2] Variational denoising network: Toward blind noise modeling and removal. In CVPR 2019\n\n[3] Dual adversarial network: Toward real-world noise removal and noise generation. In ECCV 2020\n\nIf you want to continue studying this topic, which I think is valuable too, I suggest you to conduct experiments in real degraded images. Here are some suggested datasets:\n\n[4]  A high-quality denoising dataset for smartphone cameras. CVPR 2018\n\n[5] Deep retinex decomposition for low-light enhancement. BMVC 2018\n\n[6] Ntire 2020 challenge on real-world image super-resolution: Methods and results. In CVPRW 2020\n\n(ii) This work does not have any technical contributions. Training with additive Gaussian noise has been studied in the image denoising I2I task for a long time. It is a very common setting. The robustness of noise has also been validated by many prior works. This paper does not propose any new method.\n\n(iii) The idea of adding Gaussian noise and aligning distribution is highly similar to a prior work [7] that also studies real-camera degradations in I2I tasks. There is no discussion or comparison.\n\n[7] Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training. In NeurIPS 2021.\n\n(iv) Code and pre-trained models are not submitted. The reproducibility cannot be checked."
                },
                "questions": {
                    "value": "(i) For the theoretical noise analysis part, if considering signal-independent and -dependent noise terms, what will happen to the analysis? How does it change?\n\n(ii) To measure the domain discrepancy, why not using the metric PSNR Gap proposed by the prior work DANet [3]?\n\n[3] Dual adversarial network: Toward real-world noise removal and noise generation. In ECCV 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2235/Reviewer_SHt1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2235/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699412645678,
            "cdate": 1699412645678,
            "tmdate": 1699636156649,
            "mdate": 1699636156649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SVNEwXip4J",
                "forum": "sLregLuXpn",
                "replyto": "9oYd0HMg15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer SHt1-1"
                    },
                    "comment": {
                        "value": "Dear Reviewers, \nThank you for your time and valuable feedback. Key updates in our revised manuscript include:\n\n- 1. **Theoretetical analysis:** Introduction of Theorem 3 into the main text, extending the analysis to non-Gaussian sources. Additionally, the results and discussions on Gaussian Mixture Models (GMM) sources are presented in Appendix B.6. \n\n- 2. **Simulation results:** Expanded simulations with variaous image corruptions of signal-dependant noises, blur, JPEG compression etc. have been incorporated for a comprehensive evaluation of the models' performances. \n\n- 3. **Reference list:** Addition of the comparison with denoising-based approaches in Appendix D.4, featuring the inclusion of references recommended by the reviewers.\n\nIn summary, we have enhanced the theoretical analysis to handle non-Gaussian signals and significantly expanded the experiments using diverse noise types and image distortions. We believe these augmentations have led to a stronger, more complete manuscript. Thank you again for your insightful suggestions. Below are our point-to-point replies to your comments. \n\n### Q1-1:  Comment on \"fatal theoretical flaw\" and real-world degradations \n### Reply1-1: \nWe acknowledge the significance of modeling real-world image distortions and the development of efficient denoising algorithms. However, while crucial, these areas fall outside our work's primary focus. Our objective is to establish a theoretical framework that guarantees robustness in image-to-image (I2I) translation models when Gaussian noise injection is used during training. We address the concerns regarding the simplicity of our noise models about real-world degradations with our theoretical and empirical findings.\n 1. **Theoretical Framework and Applicability:**  In Theorems 2 and 3 of our revised paper, we base our theoretical analysis on the assumption of signal-independent noise with a finite covariance matrix. These foundational assumptions are critical yet do not limit the applicability to potential real-world noise distributions. Common signal-independent noises, such as Gaussian, erasure, and impulsive noises in image transmission, and uniform noise in low-resolution quantization, are prevalent in various applications. The core insight from Theorems 2 and 3 is that certifying robustness to a broader class of signal-independent noise is feasible if an I2I system is designed to withstand Gaussian noise with the same covariance matrix.\n 2. **Comparative Analysis with Related Works:** Our research aligns with significant findings in image classification, notably those presented by Cohen, Rosenfeld, and Kolter (2019) in \"Certified adversarial robustness via randomized smoothing.\" Our study provides theoretical and empirical evidence of robustness in I2I translation models under noisy conditions, akin to the certified robustness against common corruptions and adversarial attacks discussed in their work.\n\n 3.  **Extended Empirical Validation:** The experiments in our original paper were designed to substantiate our theoretical claims. Following the reviewers' suggestions, we expanded our simulations to include signal-dependent noise and corruptions using the ImageNet-C corruption models. This expansion included shot noise, speckle noise, JPEG compression, pixelation, blurring, and combinations of these distortions. The results revealed an improved performance with Gaussian noise injection during training, with exceptions in some cases of pure blurring, JPEG compression, and pixelation. Notably, even in these scenarios, a minor addition of noise was found to stabilize the performance. It's important to mention that *ImageNet-C (Hendrycks et al. 2019)*, published in ICLR, is a widely accepted benchmark in related studies.\n\nHendrycks, D., & Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. ICLR 2019.\n\nIn summary, our paper primarily contributes to the theory and guarantees of noise resilience rather than precise noise modeling and the development of denoising algorithms. We recognize the complexity of real-world image degradation and emphasize that our theoretical foundation is a critical step towards understanding the impact of Gaussian noise on I2I models, paving the way for future integration of more complicated, real-world noise models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459566135,
                "cdate": 1700459566135,
                "tmdate": 1700459566135,
                "mdate": 1700459566135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DTay8PUj0i",
                "forum": "sLregLuXpn",
                "replyto": "9oYd0HMg15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer SHt1-2"
                    },
                    "comment": {
                        "value": "### Q1-2: Comment on the lack of Technical contributions\n\n### Reply 1-2:  \n-   While noise injection during training has been explored in prior works, our key contribution is providing the first **rigorous theoretical analysis** of this phenomenon for image-to-image translation tasks.\n-  We have derived new analytical results that formally characterize the connections between distribution divergence, score matching, and the impact of Gaussian noise on alignment. To our knowledge, this level of ***mathematical insight*** has not been shown before.\n-   Our theoretical findings lead to new proofs and guarantees on the generalized robustness implications of Gaussian noise resilience. We formally show this can imply robustness to broader noise classes beyond the Gaussian distributions.\n-   These theoretical contributions go significantly beyond simply validating empirically known heuristics. We provide an in-depth analytical basis to fundamentally understand the mechanisms behind improved noise resilience. \n-   We derived the selection of optimal training noise levels for maximally enhancing real-world robustness. This constitutes a new theoretical contribution.\n\nIn summary, while noise injection has been explored before, we provide significant technical contributions through new formal analysis unlocking fundamental insights into this phenomenon for I2I translation. Our rigorous analytical treatment and extensive theoretical results constitute the main value of this work.\n\n\n### Q1-3: Comment on no discussion and comparison to the prior work that also studies real-camera degradations in I2I tasks.\n### Reply 1-3:\n\nThe primary objective of reference [7] is the generation of pairs of real-world noisy images from clean images through adversarial training. It is crucial to note that this approach is different from our focus, which is to train the GAN with images corrupted by known noise, enabling the translation from noisy images. Our primary goal is the translation of images from the source domain, with a specific emphasis on assessing the I2I system's robustness in the presence of noise, rather than precise noise modeling or denoising algorithm formulation.\n\nThe alignment highlighted in 7 includes both image and noise alignment. While these alignments bear similarities to the divergence measure between distributions analyzed in our work, it's noteworthy that [7] does not consider these alignments to theoretical analyses. Instead, their approach relies solely on adversarial training of GANs for distribution alignment. It is widely acknowledged that the interpretability of adversarial learning in GANs often poses challenges. In contrast, our contribution involves the development of a comprehensive theoretical framework for *'alignment'* in this context. We delve into an in-depth analysis of the change in f-divergence with regard to training noise intensity, shedding light on the understanding of Gaussian noise injection in I2I systems.\n\n\n### Q1-4: Comment on Code and pre-trained models are not submitted. The reproducibility cannot be checked.\n### Reply 1-4:\nAs all baseline models utilized in our study are open source, interested readers can readily reproduce our results. Due to the substantial size of the model weights, it is not feasible to include them as supplementary materials. However, we commit to providing complete access to all codes and pre-trained weights upon the acceptance of our work.\n\n### Q1-5 Comment on the theoretical analysis of both signal-independent and signal dependant noises \n### Reply 1-5:\nThank you for your insightful comment regarding the inclusion of signal-dependent noise in our theoretical analysis. In our revised manuscript, we present empirical results that demonstrate improved performance under both signal-independent and -dependent noise conditions.\n\nOur current research focuses on establishing a solid theoretical foundation with signal-independent noise, a critical precursor to addressing the more complicated noises in practice.  Extending our theoretical framework to include signal-dependent noise is a complex challenge. It demands a reevaluation of the existing analysis to account for the statistical dependencies introduced by signal-dependent factors. We recognize the importance and practical relevance of such an extension for real-world applications and consider it a significant direction for future research. However, due to the complexity and the need for a more advanced analytical framework, it remains beyond the scope of our current work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459720300,
                "cdate": 1700459720300,
                "tmdate": 1700459720300,
                "mdate": 1700459720300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ELTSRW1Nba",
                "forum": "sLregLuXpn",
                "replyto": "9oYd0HMg15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer SHt1-3"
                    },
                    "comment": {
                        "value": "### Q1-6 Comment on the use of the metric PSNR Gap to measure the domain discrepancy.\n\n### Reply 1-6:\n  \nWe appreciate this suggestion provided by you. To make fair comparisons with baseline methods, we have chosen to employ the same objective metrics that are widely recognized and utilized in I2I evaluations. While PSNR-Gap is a valid metric in certain contexts, it is not commonly employed to measure the difference between two probability distributions in the study of I2I tasks.\n\n### Q1-7 Comments on Recommended Dataset.\n\n### Reply 1-7:\n\nWe appreciate your suggestion to consider image denoising datasets like SIDD for our work. However, our paper primarily focuses on the domain of noise-robust I2I translation tasks,  and we use the widely recognized datasets AFHQ, FFHQ, and CUHK for cat-to-dog translation, face super-resolution, and face-to-sketch translation in our simultions.  It's worth noting that for evaluating performance under various noise conditions, we employ the Imagenet-C degradation models and corresponding source codes to create synthetic images, ensuring a comprehensive assessment in line with our research objectives. While image denoising datasets are valuable in their context, they are not directly applicable to our specific tasks. We hope this clarifies our choice of datasets and evaluation methodology for our research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459785097,
                "cdate": 1700459785097,
                "tmdate": 1700459785097,
                "mdate": 1700459785097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "axmUg4sEfo",
                "forum": "sLregLuXpn",
                "replyto": "9oYd0HMg15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Reviewer SHt1's Consideration and Timely Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer SHt1, we kindly request you to consider the perspectives and ratings of 6 from the other 2 Reviewers. Your insights are highly valuable to us, and we hope you can re-evaluate our work in light of the clarifications provided and the discussions thus far. We are eager to hear your thoughts and would greatly appreciate your response within the remaining discussion timeframe. Thank you for your time and consideration."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675278450,
                "cdate": 1700675278450,
                "tmdate": 1700675278450,
                "mdate": 1700675278450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sfb1gMbu6m",
                "forum": "sLregLuXpn",
                "replyto": "SVNEwXip4J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_SHt1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2235/Reviewer_SHt1"
                ],
                "content": {
                    "comment": {
                        "value": "You said, \"Our objective is to establish a theoretical framework that guarantees robustness in image-to-image (I2I) translation models when Gaussian noise injection is used during training.\" However, the Gaussian noise is not presented in real-camera images. As you mentioned in the introduction, \"The degradation of input image quality is a common occurrence in real-world scenarios, spanning from low-light conditions to data transmission through noisy channels\". Your research plan is contradictory to your overall research motivation and is unable to address the problem you wish to study.\n\nSo if you want to enhance the robustness, you should study the real-camera degradations, especially the noise. However, the noise types used in this paper are all synthetic. There are no methods used in this paper to force the evaluated noise to approach the distribution of real-camera noise. As I explained, the real-camera noise is much more complex than the synthetic noise. Please refer to [1], [3], and [7] for the detailed analysis about real-world noise.\n\nCurrent rebuttal and experiments do not address my concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2235/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690619976,
                "cdate": 1700690619976,
                "tmdate": 1700690619976,
                "mdate": 1700690619976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]