[
    {
        "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models"
    },
    {
        "review": {
            "id": "iOWmzrnXnG",
            "forum": "g9diuvxN6D",
            "replyto": "g9diuvxN6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_KbYU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_KbYU"
            ],
            "content": {
                "summary": {
                    "value": "The study examines the robustness of LLMs in zero-shot prompting. It was observed that the model's performance varies based on the instructions used, particularly with unfamiliar instructions. To mitigate this decline in performance, the study proposes a method that incorporates soft-prompts and trains the model with an additional loss function designed to align the embedding representation of the instructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper provides a thorough comparison of LLM's instruction robustness across various models and sizes. Although the finding\u2014that LLMs are not robust to varying instructions\u2014is anticipated, this paper presents compelling empirical evidence. Additional analysis in the embedding space of the instruction is interesting. Furthermore, the study suggests an elegant solution to narrow the performance disparity between familiar and unfamiliar instructions."
                },
                "weaknesses": {
                    "value": "The proposed solution could benefit from more details. Exploring the soft-prompt size (i.e., the variable N) would be nice. The N used is not specified in the main text either, which I believe is an important detail. \n\nThey provide synthetic instruction, and one of the baselines is to fine-tune normally with this new instruction. Interestingly, there is a noticable degradation in performance, yet there's no explanation as to why. Usually, continual fine-tuning on the same data should not damage performance (significantly so). My guess is that the quality of the synthetic prompt is not very good. If so, this raises a question: what if we spend more effort on generating better synthetic instruction beforehand?"
                },
                "questions": {
                    "value": "- \"We then form batches by including one instance featuring the original instruction, and the rest comprising paraphrased instructions.\" -> what's the batch size, and consequently, what's the paraphrase size and the final dataset size altogether? \n- Significant degradation on FT on Table 7, why is that so?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698126247384,
            "cdate": 1698126247384,
            "tmdate": 1699636356933,
            "mdate": 1699636356933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jFSFEYSAx7",
                "forum": "g9diuvxN6D",
                "replyto": "iOWmzrnXnG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to KbYU"
                    },
                    "comment": {
                        "value": "**W1 (Soft-prompt size)**: We use a prefix length of 10 for the result reported (thank you for pointing this out!) We have explored many different settings (e.g., soft-prompt size) in our pilot studies and noticed no substantial differences, suggesting that one needs only a minimal set of free parameters to tune with the proposed objective to realize improvements. Below we reproduce results observed when using different soft-prompt sizes (from 1 to 50), and show their performance and semantic distance:\n\n| **Soft Prompt Size** | **MMLU Obs.**  | **MMLU Unobs.** | **MMLU Ave.**  | **Ave. MMLU Distance** | **BBL Obs.**    | **BBL Unobs.**  | **BBL Ave.**    | **Ave. BBL Distance** |\n|----------------------|----------------|-----------------|----------------|------------------------|-----------------|-----------------|-----------------|-----------------------|\n| 1                    | **47.4 \u00b1 0.2** | **47.8 \u00b1 0.2**  | **47.6 \u00b1 0.3** | 3.82                   | 55.9 \u00b1 19.7     | 54.2 \u00b1 17.6     | 55.1 \u00b1 18.4     | 7.33                  |\n| 5                    | 47.9 \u00b1 0.2     | 48.4 \u00b1 0.3      | 48.1 \u00b1 0.3     | 4.03                   | 55.8 \u00b1 21.2     | **52.4 \u00b1 19.0** | **54.1 \u00b1 19.9** | **6.88**              |\n| 10                   | 47.6 \u00b1 0.1     | 48.0 \u00b1 0.2      | 47.8 \u00b1 0.3     | 4.30                   | **55.0 \u00b1 21.6** | 53.6 \u00b1 18.0     | 54.3 \u00b1 19.6     | 7.47                  |\n| 30                   | 47.6 \u00b1 0.1     | 48.2 \u00b1 0.1      | 47.9 \u00b1 0.3     | 3.84                   | 55.4 \u00b1 20.8     | 53.8 \u00b1 17.9     | 54.6 \u00b1 19.3     | 7.02                  |\n| 50                   | 48.0 \u00b1 0.2     | 48.6 \u00b1 0.2      | 48.3 \u00b1 0.5     | **3.73**               | 55.9 \u00b1 21.9     | 52.6 \u00b1 18.9     | 54.2 \u00b1 20.0     | 7.03                  |\n\nThe results indicate that the prefix length does not affect the performance for both observed and unobserved instructions. As they fluctuate around the same level. Additionally, we also calculated average distance between unobserved instructions to their closest equivalent observed instructions, and the value is highly similar between all soft prompt sizes.\n\nThe takeaway here is that soft-prompt is only a method we use to drag the equivalent instructions close together without affecting the model\u2019s performance, and the number of parameters we dedicate to do that is seemingly insignificant.\n\n**Q1 (Dataset size)**: The batch size we use for both models is 4, hence each batch contains one \u201cprototype\u201d instruction and 3 paraphrased instructions to align (this is also reported at the beginning of Appendix B.3.) We randomly sampled 1k prototype samples from the original instruction tuning datasets, hence the final dataset altogether is 4k; we report this Section 5.\n\n**W2 & Q2 (Significance Test)**: Please see our general response, (1). Briefly, this is likely an instance of \u201ccatastrophic forgetting\u201d, a hypothesis supported by the results we have added in Appendix G."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507767959,
                "cdate": 1700507767959,
                "tmdate": 1700507767959,
                "mdate": 1700507767959,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7iw6UIIDCQ",
            "forum": "g9diuvxN6D",
            "replyto": "g9diuvxN6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
            ],
            "content": {
                "summary": {
                    "value": "This paper observes that current instruction-tuned LLMs are sensitive to surface forms of instructions by showing that they underperform on unseen instructions compared to seen instructions. To mitigate this issue, the paper proposes soft prompt alignment technique."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper performs experiments on various tasks including reasoning benchmarks (MMLU and BBL) and NLG benchmarks.\n- The paper provides the experimental setting of evaluating the robustness of LLMs on different instructions by releasing various instructions collected from NLP researchers, which might facilitate further research."
                },
                "weaknesses": {
                    "value": "- In Tables 3 and 7, compared to the performance difference, the variance is very large, questioning the significance of the difference between observed and unobserved instructions. A significance test should be conducted to test the significance of the difference.\n- Although the title, abstract, and motivation of the paper mention that the paper tries to evaluate the zero-shot robustness of instruction-tuned language models generally, it only focuses on models up to 11B. The tendency is likely to be unpredictable for state-of-the-art LLMs such as GPT-3.5 or GPT-4. \n- How does PT+KL on Table 7 perform on negated instructions? The alignment stage might make the LLM insensitive to most instructions even though the instructions are not relevant or negated, which is a similar finding for instruction-tuned LLMs for Webson & Pavlick (2022) \n\nWebson & Pavlick (2022) - Do Prompt-Based Models Really Understand the Meaning of their Prompts?"
                },
                "questions": {
                    "value": "- Do you think that RLHF after instruction tuning might mitigate this issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507912009,
            "cdate": 1698507912009,
            "tmdate": 1700721999156,
            "mdate": 1700721999156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m5MODRums2",
                "forum": "g9diuvxN6D",
                "replyto": "7iw6UIIDCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to hcVs (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the thoughtful feedback.  \n\n**W1 (Significance test)**: Please see our general response, (2). Briefly: The results in Tables 3 and 7 are statistically significant, despite the observed variances. We have added significance results to the manuscript. We also note that in Table 3, the variances are much larger for the unobserved instructions, which is part of the issue that we are highlighting. \n\n**W2 (Size of LLMs)**: The reviewer is correct that our work here is focussed on modestly sized LMs. We have tried to be clear about this in the abstract and introduction. We write in the second sentence in the abstract that instruction tuning \u201c... has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants\u201d; this is what motivates our focus on these <= 11b models. We believe this analysis is important precisely because prior work has suggested that instruction tuning imbues such models with performance surprisingly competitive with massive LLMs; our findings call this into question. \n\n**W3 (Negation)**: The reviewer suggests an interesting additional analysis: Evaluating sensitivity to instruction negation. We have now conducted such analyses and report details (experimental setup and results) in Appendix F. For convenience, below we reproduce the aggregated results on how PT + KL does on all 6 settings (including negated observed instructions) we conducted in Section 4.5\n\n| **Method**| Closest | Incorrect | Negated | Task Designer | Collected | Nonsensical |\n|-|-|-|-|-|-|-|\n| - Baseline | 58.1 $\\pm$ 1.8     | 52.2 $\\pm$ 8.6 | 48.3 $\\pm$ 3.8 | 44.2 | 55.3 $\\pm$ 4.5 | 30.6 $\\pm$ 4.4 |\n| - PT + KL | **60.5 $\\pm$ 2.4** | **54.2 $\\pm$ 7.2** | 47.1 $\\pm$ 4.9 | **47.1** | **56.7 $\\pm$ 3.9** | 38.0 $\\pm$ 4.1 |\n| - PT + KL (Negation) | 58.6 $\\pm$ 2.0     | 52.7 $\\pm$ 9.2 | **43.0 $\\pm$ 4.9** | 46.5 | 55.6 $\\pm$ 4.3 | 35.7 $\\pm$ 5.6 |\n\nThis table reports the results we reported in Figure 3 (first row) after applying PT + KL (second row). We observe that PT + KL yields non-trivial improvement on models performance with: 1) suitable instructions (Closest) that are observed. 2) suitable instructions (Task Designer, Collected) that are unobserved. However, results also indicate that the proposed method does not succeed fully in terms of imbuing the model with the ability to truly \u201cunderstand\u201d instructions: \nThe performance of observed instructions after adding negation does not drop significantly (only by 1.2 on average)\n\nWe hypothesize that training the model to align semantically equivalent instructions does not explicitly incent the recognition of negated prompts. This is because our objective enforces that equivalent instructions should be similar, but negations rarely occur in the collected paraphrases. Moreover, learning to handle negations would more intuitively be done by encouraging the model to push negated versions of instructions away from their original forms; this differs from the form of our objective which focuses on similarity. Therefore, we conduct another experiment by modifying the objective slightly to handle negation.\n\nSpecifically, we trained the model to push the representation of the negated instructions away from the original ones. To do so, we take the same 1k seed instances from the FLAN collection and prompt (with three different prompts) GPT-4 to rewrite them such that the instruction is negated. We keep all the hyperparameters same as how we did for PT + KL, but use KL to push the representation of negated instructions away from the originals:\n\n$\\mathcal{L} = (1-\\lambda) \\mathcal{L}_{CE} - \\lambda \\mathcal{L}$$ _{KL}$\n\nAfter training, we run the \u201ccloser look\u201d experiment again to see if this slightly different objective would result in a stronger robustness (third row). The result shows a significant drop of the accuracy when negation is added to the observed instruction. However, the performance in other settings remains comparable with the baseline when applying the PT + KL (Negation).\n\nThis result provides additional evidence for our findings that robustness to instruction variants is strongly tied to the distances between learned representations of instructions: When we incentivize the model to learn similar representations for equivalent instructions, it performs better when given equivalent unobserved instructions, though it does not better handle negated instructions. But if we modify this slightly to encourage the model to induce dissimilar representations between instructions and their negated versions, it improves its responsiveness to negated instructions. In short, the proposed strategy of targeting (dis)similarity of model instruction representations is general enough to facilitate realizing robustness beyond paraphrases.\n\nWe report a full set of results for this finding in Appendix F in the updated draft."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507681078,
                "cdate": 1700507681078,
                "tmdate": 1700507681078,
                "mdate": 1700507681078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PFeBdFKvrt",
                "forum": "g9diuvxN6D",
                "replyto": "7iw6UIIDCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to hcVs (2/2)"
                    },
                    "comment": {
                        "value": "**Q1 (RLHF)**: RLHF is most appropriate for aligning model outputs for tasks that require open-ended responses. In our work, we are benchmarking instructions and tasks that have a set of correct answers. Collecting human feedback for training is not a good fit for our main evaluation measures, which focus exclusively on the correctness of the output for (mostly) \u201cobjective\u201d tasks like classification.\n\nHowever, the reviewer raises an interesting idea, in that it is possible that optimizing LLMs to align with human preferences generally via RLHF may implicitly require models to \u201cunderstand\u201d instructions in a more robust way. \n\nWe perform a preliminary investigation into whether training with RLHF on an unrelated, open-ended text generation tasks affects the robustness of the instruction-following abilities. Specifically, we take an off-the-shelf Alpaca-7B trained with RLHF via PPO and run it on all the tasks and instructions we have used in this paper. We found that the output tends to do \u201cextra things\u201d for a given instruction (e.g., providing its explanation for question answer), we found that the overall performance is highly unstable and degrades significantly in most of the datasets. \n\n| **Categories** | Delta Obs. Accuracy | Delta Unobs. Accuracy | Delta Ave. Accuracy |\n|-|-|-|-|\n| **MMLU**| -18.5 \u00b1 0.5 | -17.4 \u00b1 1.7 | -18.0 \u00b1 1.1|\n| **BBL-QA**| -11.2 \u00b1 4.0 | -9.7 \u00b1 4.7 | -10.4 \u00b1 4.3|\n| **BBL-MC**| -0.3 \u00b1 0.8 | -5.3 \u00b1 0.8|  -2.8 \u00b1 0.8 |\n| **BBL-BC**| -11.8 \u00b1 1.7 | -6.2 \u00b1 8.3| -9.0 \u00b1 5.0 |\n\nTherefore, we do not have evidence to show that RLHF on modestly-sized LLMs would mitigate such issue."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507716859,
                "cdate": 1700507716859,
                "tmdate": 1700507716859,
                "mdate": 1700507716859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bvjqwkTMml",
                "forum": "g9diuvxN6D",
                "replyto": "PFeBdFKvrt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                ],
                "content": {
                    "title": {
                        "value": "Further question on author response"
                    },
                    "comment": {
                        "value": "Hi, thank you for the response and interesting new results.\n\nI have one further question regarding my concerns.\nRegarding W3, do you expect joint training on both objectives (PT+KL & PT+KL (Negative)) could enhance the performance for all cases?\n\nTypo Error: Table 3 caption -> I think the caption should be fixed into p < 0.05 instead of p < 0.5."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643824528,
                "cdate": 1700643824528,
                "tmdate": 1700643824528,
                "mdate": 1700643824528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "680gHW1iEn",
                "forum": "g9diuvxN6D",
                "replyto": "7iw6UIIDCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Extended Analysis on Scaling for Reviewer hcVs"
                    },
                    "comment": {
                        "value": "As a final additional response to reviewer feedback, we extend our analysis a bit to include a larger (20B) model. As reviewer hcVs pointed out, readers may be interested to see if the results generalize to (very) large models. While we maintain our stance that the success of instruction-tuning for boosting the performance of modestly-sized LLMs is the most exciting aspect of this method, and therefore worthy on its own of study, we nonetheless agree that studying the relationship between robustness and scale of such models is interesting. \n\nHowever, we can draw valid comparisons only if we keep all other variables (model architecture, pretraining data, and instruction tuning set) fixed. Therefore, we are not able to study black-boxed models (like GPT-3.5 or GPT-4) using this approach. To include additional scale in our work, we therefore conduct the full experiments on different sizes of Flan-T5 ranging from 70M to 11B (Fig 2a and Sec 4.3), as well as Flan-UL2 (20B), and we report the change of **delta performance** (observed - unobserved) in the table below:\n\n| **Model**     | # Parameters | MMLU               | BBL-QA            | BBL-MC             | BBL-BC              |\n|---------------|--------------|--------------------|-------------------|--------------------|---------------------|\n| Flan-T5-Small | 70M          | **-2.6 $\\pm$ 8.3** | 4.1 $\\pm$ 3.1     | **-0.4 $\\pm$ 1.4** | **-7.9 $\\pm$ 12.0** |\n| Flan-T5-Base  | 250M         | 0.0 $\\pm$ 0.9      | **1.6 $\\pm$ 6.2** | 1.2 $\\pm$ 4.3      | 1.5 $\\pm$ 8.3       |\n| Flan-T5-Large | 770M         | 0.1 $\\pm$ 0.5      | 3.1 $\\pm$ 6.4     | 11.4 $\\pm$ 2.1     | 1.6 $\\pm$ 6.1       |\n| Flan-T5-XL    | 3B           | 0.6 $\\pm$ 0.7      | 3.0 $\\pm$ 4.6     | 3.5 $\\pm$ 3.2      | 5.5 $\\pm$ 7.8       |\n| Flan-T5-XXL   | 11B          | 0.5 $\\pm$ 0.6      | 3.4 $\\pm$ 5.7     | 2.8 $\\pm$ 3.1      | 2.0 $\\pm$ 9.1       |\n| Flan-UL2      | 20B          | 0.2 $\\pm$ 0.5      | 3.4 $\\pm$ 7.3     | 0.6 $\\pm$ 0.3      | 1.9 $\\pm$ 7.1       |\n\nThe results above (aggregated by task category) show that there is no apparent relationship between model size and the delta performance (our measure of robustness). \n\nMore scientifically, we perform linear regression on the size of the model v.s. (observed  - unobserved performance) over data points from all datasets, this yields a slope of $4.45 \\times 10^{-13}$ with a p-value of 0.087 ($R^2$ is 0.002). This suggests that model size may weakly correlate with increased robustness, but this relationship is not significant the $p=0.05$ level, and the relationship is tenuous. In other words, scale does seem to improve robustness, but only very moderately.\n\nWe hope the additional results (together with those posted earlier) answer your questions. We ask that you consider whether it might be appropriate to adjust your score, if you feel these new findings have changed your view on the work. Thank you."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703187610,
                "cdate": 1700703187610,
                "tmdate": 1700703187610,
                "mdate": 1700703187610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "47FJ2dQKln",
                "forum": "g9diuvxN6D",
                "replyto": "680gHW1iEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_hcVs"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your update.\nI have increased my score accordingly.\nI would appreciate it if the authors could update the paper with all the new results during the discussion period."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721985272,
                "cdate": 1700721985272,
                "tmdate": 1700721985272,
                "mdate": 1700721985272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cU1wpQX4pv",
            "forum": "g9diuvxN6D",
            "replyto": "g9diuvxN6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyse how sensitive various instruction-tuned models are to rephrased instructions, especially in zero-shot settings. They find that using novel instructions previously unseen by the model results in degraded performance across different tasks, and does not seem to improve with model scale. Further analysis suggests that instructions that are more dissimilar to instructions seen during training generally perform (somewhat) poorer. Finally, they introduce a prompt-tuning method that utilises a KL penalty to punish representations that drift from paraphrased versions of instructions. They show this performs better than simply training on the paraphrased instructions alone."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments are clear and well-thought-out, and the results are solid and interesting, affecting many current popular models (Llama finetunes), and holding for different model families.\n- The observed correlations between performance and instruction similarity (in representation space) are interesting (even if the correlation is somewhat weak).\n- The proposed fix method is well-motivated, and the results seem reasonable, especially for the Alpaca-7B model.\n- The robustness of models to different prompts is an important and interesting area of study, especially considering current difficulties with LM evaluations."
                },
                "weaknesses": {
                    "value": "- The novelty of the robustness experiments is somewhat limited, as there is prior work that has observed variance based on prompt formatting (discussed in the work itself). Considering this work seems to examine more modern models such as Alpaca, and considers a different set of tasks, I think this is a minor weakness - the work does add a number of interesting insights, especially around representation distance.\n- The claim that models perform worse at novel phrasings of tasks seems a bit strong compared to the results: examining Table 3, the variance of performance on unobserved phrasings often is greater than the difference in average performance, suggesting that the difference in performance may not be significant (although the large increase in variance is certainly not desirable and interesting to see).\n- While the proposed method seems strong, examining the results in Appendix B.3 seems to show that in many cases, prompt-tuning alone performs within one standard deviation of the PT+KL results (e.g., for BBL unobserved and average). As such, the proposed method might not be significantly different from just prompt tuning.\n\nOverall, I think this paper is okay, and the robustness results are interesting and useful to see, along with the representation alignment results. However, the novelty of the robustness experiments is a bit limited, and it\u2019s not clear the proposed method for improving this is significantly better than baselines."
                },
                "questions": {
                    "value": "- How does your proposed method compare to introducing the model paraphrases during initial instruction finetuning? Training on the paraphrased versions alone after the instruction tuning may cause forgetting in an undesirable way.\n- Is the degradation of performance on unobserved prompts statistically significant? The variance seems very high.\n- Doing prompt tuning alone seems to often perform very similarly to PT+KL, and within the reported standard error in Appendix B.3. Do you have statistical tests for significance for your results?\n- It would be interesting to see the results in Table 7 broken down into the BBL QA/BC/MC categories shown in Table 6 to see how much the difference in closest distance affects the performance of these different categories."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611187270,
            "cdate": 1698611187270,
            "tmdate": 1700588823260,
            "mdate": 1700588823260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gaKis1ftNE",
                "forum": "g9diuvxN6D",
                "replyto": "cU1wpQX4pv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to cGUu"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments. \n\n**W1 (Novelty)**: Regarding novelty, please see our general response, (3). In brief, our work builds upon but complements and considerably deepens these prior efforts. \n\n**Q1 (Augment to initial instruction-tuning set)**: Due to the modest size of the paraphrases set (4k), it has minimal effect on the training performance and is very time-consuming by adding to the initial instruction finetuning. Please refer to Table 11 in Appendix B.3.1 for the detailed comparison. We reiterate that the main goal of the improvement section is to find a way to narrow down the semantic gap between the observed and unobserved, so that we validate our hypothesis (representation similarity for equivalent instructions v.s. robustness )\n\n**W2 & Q2 (Significance Test)**: Please see our general response, (2). Briefly, the large variances here in part reflect the fact that we are reporting aggregated results over large benchmarks. That said, one of the main observations is that variances are considerably larger for unobserved instructions, which is\u2014as the reviewer rightly points out\u2014undesirable. \n\nTo the reviewers\u2019 question: The results in aggregate are statistically significant (p << 0.05) under a paired t-test, and if we run the t-test separately for each model (which avoids the issue of repeating datasets in observations), we observe p < 0.05 for all but T0++ (p=0.13). \n\n**W3 & Q3 (Efficacy of the objective)**: We observe two major differences between the performance using just prefix tuning (PT) and prefix tuning with extra objectives (PT + KL). \nThe performance diff between unobserved instructions and observed instructions - i.e., robustness - is significantly higher (close to zero) for PT + KL (Table 7 in Section 6)\nThe average standard deviation of the instruction performance of PT is even higher than the baseline, indicating instability in its performance; whereas this high variance is significantly mitigated with PT + KL (**Table 10 in Appendix B.3**)\n\nIntuitively, we design the extra objective to explicitly \u201cincrease the similarity\u201d between equivalent instructions \u2014 unobserved to observed \u2014 from the model\u2019s \u201cperspective\u201d, which we hypothesize to have positive correlation with the robustness of instruction following ability. \n\nMotivated by this, we calculate the average l2 distance between unobserved instructions and its closest observed instruction over all the examples (Same approach as Table 6) for PT and PT + KL.  (Fig 7 in Appendix B.3.2). We show that in 13 out of 14 datasets, the distance between equivalent instructions is smaller for PT + KL. The consistency of the close distance and better robustness supports our hypothesis of the correlation.\n\nWe have added these results, figures and the discussion to Appendix B.3.2 of the revised paper.\n\n**Q4 (Disaggregated results for Table 7)**: We appreciate the suggestion and we have now added the disaggregated result in a dataset level in Table 30 and Table 31 in Appendix J.3. For convenience, here we reproduce results disaggregated by categories:\n\n| Categories | Alpaca Obs. Delta | Alpaca Unobs. Delta | Alpaca Ave. Delta | Flan Obs. Delta | Flan Unobs. Delta | Flan Ave. Delta |\n|------------|-------------------|---------------------|-------------------|-----------------|-------------------|-----------------|\n| MMLU       | 0.4               | 0.6                 | 0.5               | 0.8             | 1.1               | 1.0             |\n| BBL-QA     | 1.1 $\\pm$ 2.5     | 2.3 $\\pm$ 2.8       | 1.7 $\\pm$ 2.1     | 0.7 $\\pm$ 0.4   | 4.6 $\\pm$ 1.5     | 2.7 $\\pm$ 0.9   |\n| BBL-MC     | 0.3 $\\pm$ 0.4     | 0.3 $\\pm$ 0.1       | 0.3 $\\pm$ 0.2     | 0.3 $\\pm$ 0.0   | 3.5 $\\pm$ 1.6     | 1.9 $\\pm$ 0.8   |\n| BBL-BC     | -1.3 $\\pm$ 5.2    | 2.0 $\\pm$ 2.5       | 0.4 $\\pm$ 3.5     | 0.9 $\\pm$ 0.2   | 3.2 $\\pm$ 0.3     | 2.1 $\\pm$ 0.2   |\n\nThis Table reports the average delta performances observed after applying the proposed method on Flan-T5-XL and Alpaca-7B. For BBL datasets we report standard deviations over the individual datasets (for MMLU we group all together so do not report a variance). \nTable 6 in the paper reports the average distance change after applying the objectives for Flan-T5-XL. In general, we observe a consistent but weak correlation relating the closest distance between equivalent instructions to the delta performance for unobserved instructions. The similarity between equivalent instructions changes little for MMLU, which leads to a marginal improvement. However, this pattern is also not absolute; the BBL-QA category in general receives the greatest improvement with respect to performance but not distance. We leave further analyses of instruction representations and downstream performance to future work building on our results here."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507626601,
                "cdate": 1700507626601,
                "tmdate": 1700507626601,
                "mdate": 1700507626601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "byK71hdFHG",
                "forum": "g9diuvxN6D",
                "replyto": "gaKis1ftNE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, thanks for your response! \n\nAugmenting the instruction-set - When you say \"the model is retrained from scratch\" in Table 11, what is the setting here? Did you retrain from Llama-7B/T5-XL with the paraphrases and instructions mixed, or just train on the paraphrase data?\nI agree that investigating representation similarity for equivalent instructions v.s. robustness is interesting, but I was primarily curious if using paraphrased data in a simple way during instruction-tuning itself increases robustness and increases the representation similarity in a simple way.\n\nThank you for the significance tests and the new results. I agree that the reduction in variance using PT+KL is interesting and useful! I've read the other reviews and responses, and am bumping up my score, since the significance tests cover my main concern around the efficacy of the proposed method."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588805552,
                "cdate": 1700588805552,
                "tmdate": 1700588805552,
                "mdate": 1700588805552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5NeeNaA2De",
                "forum": "g9diuvxN6D",
                "replyto": "wM5mpbFuNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Reviewer_cGUu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification, that finetuning setup makes sense! It would definitely be interesting to scale up the paraphrase set, but I think that's a good thing to leave for future work. Showing that you can make use of a small amount of data more effectively using your method is a nice contribution."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597774690,
                "cdate": 1700597774690,
                "tmdate": 1700597774690,
                "mdate": 1700597774690,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P5UjYIQS75",
            "forum": "g9diuvxN6D",
            "replyto": "g9diuvxN6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_RNd8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3958/Reviewer_RNd8"
            ],
            "content": {
                "summary": {
                    "value": "The paper critiques instruction tuning (IT) by showing that the IT models are not robust to rephrasing of the same prompt that were not seen in training. More surprisingly, using a prompt that 1) was seen in training, but 2) is incorrect for the context has a higher accuracy than an unseen but correct prompt. They propose a mitigation strategy by adding a soft prompt that is explicitly optimized to make the model see the different rephrases as similar to each other. They also release a dataset of 319 prompt rephrasings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This seems like a significant result overall, and explicitly shows an important weakness in IT models\n- In general, this kind of work (exploring what \"success on a benchmark\" really means) is extremely important for the field. You are raising a really important issue with fine tuning, and this is impactful.\n- The experiments in the paper are well-executed, well-presented, and broad across models and datasets.\n- It's interesting to explicitly introduce the rephrasing invariance into the fine tuning data-- it's sort of an  equivalent of regularization, I think? \n- Thanks for including figure 4! It helped a lot with the intuition in that section."
                },
                "weaknesses": {
                    "value": "- nit: \"instruction-tuned models are not especially robust to instruction rephrasings\" \u2192 this is very clear, say it first. Better yet, make it the title :) \n- I don't know if just releasing prompt rephrasing is a significant enough contribution-- https://openreview.net/forum?id=9Vrb9D0WI4 and others also have datasets of different rephrasing of prompts for the same task."
                },
                "questions": {
                    "value": "- It looks like just fine tuning (or even fine tuning + KL) severely drops performance. Do you know why? This seems very surprising\n- Why use NLP grad students to generate rephrasing of the prompts, when you could just use an LLM (which it seems like you do later for the eval set)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3958/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801897547,
            "cdate": 1698801897547,
            "tmdate": 1699636356648,
            "mdate": 1699636356648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hq4EUitvuV",
                "forum": "g9diuvxN6D",
                "replyto": "P5UjYIQS75",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3958/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to RNd8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review, and we are glad that they found the analysis here interesting and potentially impactful. \n\n**W1 (Title of the paper)**: We thank the reviewer for the suggestion, and agree this is the main takeaway; we will emphasize this point earlier on (and consider using it as our title) in future version of the manuscript.\n\n**W2 (Rephrasing Contribution)**: We agree with the reviewer that the release of this data is, on its own, not a substantial contribution. However, we do think this dataset is novel with respect to existing resources (e.g., T0/PromptBench, Flan, and others) which have released rephrasings of instructions for the same tasks intended for model **training**. By contrast, the dataset of diverse instructions we release with this work is intended to aid model **evaluation**, specifically on tasks in two major benchmarks, MMLU and BBL. Our hope is that this resource will both permit reproducibility of our work, and facilitate additional research into the robustness of instruction-tuned models.\n\n**Q1 (Fine-tuning performance)**: Please see our general response, (1). Briefly, we think this is an instance of \u201ccatastrophic forgetting\u201d and have provided new results that support this hypothesis in Appendix G.\n\n**Q2 (Rephrasing source)**: We were interested in evaluating the robustness of instruction tuned models to novel instructions, as they might be observed in deployment\u2014this means we were interested not strictly in rephrasings but in novel instructions written anew by practitioners for specific tasks of interest. Collecting new instructions from NLP graduate students (who are familiar with instructing language models) better aligns with this envisioned stress-testing than using LLMs to automatically rephrase existing instructions; in particular this yields a sample from the distribution of instructions which might plausibly be written by practitioners, so we felt it best to manually collect these for evaluation. \n\nNote that in the proposed methods to make models more robust to instruction variations (Section 5), we did use LLMs to paraphrase existing instructions from the training data as a means to augment the data and realize the proposed objective term."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3958/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507568495,
                "cdate": 1700507568495,
                "tmdate": 1700507568495,
                "mdate": 1700507568495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]