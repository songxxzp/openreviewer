[
    {
        "title": "SELECTFORMER: PRIVATE AND PRACTICAL DATA SELECTION FOR TRANSFORMERS"
    },
    {
        "review": {
            "id": "DoafpQSSAp",
            "forum": "C5sxQsqv7X",
            "replyto": "C5sxQsqv7X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission417/Reviewer_8Z9X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission417/Reviewer_8Z9X"
            ],
            "content": {
                "summary": {
                    "value": "In the free data market, model owner would like to trade data from data owner to be able to maximize the model accuracy, which requires to select and appraise portions of data points. However, the data selection should be private to both model and data owner in order to keep model parameters and data points private. A technique named MPC can be utilized to jointly evaluate the model privately with forward passes while privacy of both parties could be preserved.  Existing MPC approaches could approximate expensive nonlinearity with cheaper operations for transformer inference, which transformer models are applied in the most deep learning tasks. Meanwhile, they suffers from considerable runtime overhead and poor selection utility. In this work, authors propose an approach to accelerate MPC-based private data selection while preserve the utility of the selection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The assumption for the unlabeled and probably imbalanced dataset D from data owner is a practical consideration, which makes the work more convincing and challenging than prior related works.\n2. Multiphase selection is a smart strategy. Instead of selecting appropriate data at once. Filtering out most irrelevant data points in the early stage is a way more efficient approach with smaller models and larger models in the later phases provide accurate selection based on filtered data.\n3. Approximation of nonlinear operations with multiple shallow MLPs can simulate the utility of nonlinearity well and reduce high dimensions into much lower dimensions to provide huge efficiency improvement."
                },
                "weaknesses": {
                    "value": "1. While a semi-honest setting is convenient and efficient for the research purpose, a setting against malicious adversary is more practical. The assumption based on faithful protocol execution by both parties is not quite strong in the real-world application.\n2. The reveal of target model architecture could be dangerous. For example, an adversary can potentially retrieve parameters of certain layer. Instead, I think the target model could be trained with MPC protocols."
                },
                "questions": {
                    "value": "1. Since the setting of model and data owner is based on two parties. Instead of MPC, have you considered to solve this problem in the zero-knowledge proof setting? ZK approach allows model owner to select and appraise the data from data owner by privately committing parameters and data, and providing verifiable results without data leakage. It may be more efficient than MPC (not a conclusion and probably interesting for the future investigation).\n2. In the threat model of Section 2.1, can you explain more about what can be revealed (like the purchase budget B mentioned in the Section 4.1) and what should be private (like model parameters and data points)? Or also merge the privacy guarantee in the Section 4.1 into the threat model.\n3. What is the threshold of phase i? How should we determine when the early or later phases are?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698029231070,
            "cdate": 1698029231070,
            "tmdate": 1699635968560,
            "mdate": 1699635968560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ui1UKTeTO8",
                "forum": "C5sxQsqv7X",
                "replyto": "DoafpQSSAp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Z9X"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your review, especially for the positive feedback on the practicality and presentation! We hope that our response has adequately addressed your concerns. If you have any additional feedback concerning our current draft, we would be delighted to hear it and address your points.\n\nWe summarize the concerns and answer them individually below:\n\n- Semi-honest setting is not strong enough in real world.\n\nResponse: We agree with the limitation of a semi-honest setting. Yet, it is still unsolved and sees active research. We hope the results from semi-honest settings could pave the way to more challenging settings with malicious parties. \nReferences:\n[1] P. Mohassel and Y. Zhang. SecureML: A system for scalable privacy-preserving machine learning. In IEEE Symposium on Security and Privacy, 2017.\n[2] S. Tan, B. Knott, Y. Tian, and D. J. Wu. CryptGPU: Fast privacy-preserving machine learning on the GPU. In arXiv 2104.10949, 2021.\n[3] P. Mohassel and P. Rindal. ABY3: A mixed protocol framework for machine learning. In ACM Conference on Computer and Communications Security (CCS), 2018.      \n\n- The reveal of target model architecture could be dangerous. Instead, train target model with MPC protocols.\nResponse: To clarify, revealing model architectures (e.g. layer types, number of layers) does not reveal model parameters, which are considered more sensitive. Training models over MPC would be an interesting approach, which however is challenged by the prohibitive MPC cost. \n\nWe answer other specific questions from the reviewer.\n\n**Q1**: Instead of MPC, consider solving this problem in the zero-knowledge proof setting for the future investigation).\n\n**A1**: That\u2019s an interesting direction that we are looking forward to. Besides, we will cite and discuss ZKP in the Related Work section.\n\n**Q2**: Explain more about what can be revealed and what should be private. Or also merge the \u201cPrivacy guarantee\u201d into the threat model.\n\n**A2**: Thanks for the suggestion! We have added a new Figure 1 and a \u201cWorkflow\u201d paragraph in Section 1 to provide more encryption/reveal information. We intentionally keep \u201cPrivacy guarantee\u201d in Section 4.1 to better introduce the multipass selection workflow. Therefore readers will not be confused by encountering these details too early.\n\n**Q3**: What is the threshold of phase i? How should we determine when the early or later phases are?\n\n**A3**: We interpret \u201cthreshold\u201d and \u201cearly or later phases\u201d as how to decide the number of phases and the dimension of MLPs.\nThe number of phases depends on the trade-off between selection performance and computing resources. More phases may have better selection performance but run longer.\nThe MLP dimensions in each phase are determined by the schedule via offline grid search(Section 5.4 MLP hidden dimensions, Table 5, and a complete Table 6 in the Appendix).\nFor complex datasets, adding one larger dimension phase may increase the selection accuracy (Table 6 in the Appendix).\n\nWe hope these additional explanations can address your questions above. We have updated the manuscript for better clarity (new Figure 1, Section 1 Workflow, and our first contribution)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623809663,
                "cdate": 1700623809663,
                "tmdate": 1700623809663,
                "mdate": 1700623809663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6CkQhl7mUK",
            "forum": "C5sxQsqv7X",
            "replyto": "C5sxQsqv7X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission417/Reviewer_due4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission417/Reviewer_due4"
            ],
            "content": {
                "summary": {
                    "value": "1. The paper presents a technique called SELECTFORMER for private and practical data selection for Transformers. \n2. The contributions of the paper are threefold: (1) a new pipeline for private data selection over MPC, (2) the use of low-dimensional MLPs to emulate high-dimensional nonlinear operators, and (3) a parallel, multiphase scheduling approach for MPC. \n2. The goal is to enable the model owner to select and appraise training data from the data owner privately before committing to a transaction. \n3. The technique utilizes Multi-Party Computation (MPC) to evaluate the target model over the selected data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research problem is unique and import for practical use.\n\n2. The technique is evaluated on various Transformer models and NLP/CV benchmarks, showing a significant reduction in delay from thousands of hours to tens of hours, with only a minimal accuracy degradation of around 0.20% compared to training with the selected data.\n\n3. The selection process is designed as a multipass sieve, where earlier phases use smaller selector models for quick filtering of redundancy, and later phases use larger selector models for more precise selection."
                },
                "weaknesses": {
                    "value": "1. The first contribution-emulates high-dimensional nonlinearity with low-dimensional MLPs. This is a not novel technique. Besides, training MLPs to approximate every non-linear operator is very cost. Does author evaluate the time cost here?\n2. The MPC protocol hide the computation behind the communication data exchange is not novel. Any MPC protocol would try to minimize the total latency like this way. It is not unique here. Thus, I think this contribution is a weak statement.\n\n\nReference:\n\n[1] Lu, Lu, et al. \"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.\" Nature machine intelligence 3.3 (2021): 218-229."
                },
                "questions": {
                    "value": "See Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Reviewer_due4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552334138,
            "cdate": 1698552334138,
            "tmdate": 1699635968487,
            "mdate": 1699635968487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xBEQrCN7VO",
                "forum": "C5sxQsqv7X",
                "replyto": "6CkQhl7mUK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer due4 (1/2)"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your review! We hope that our response has adequately addressed your concerns. If you have any additional feedback concerning our current draft, we would be delighted to hear it and address your points.\n\nWe summarize the questions and answer them individually below:\n\n**Q1**: MLP approximations is not novel.\n\n**A1**: Thank you for pointing out the paper, but this is not applied to our problem because of the goal and the cost. (1) DeepONet wants to solve a different problem from us. It wants to solve the differential equations which will find solutions in input X\u2019s domain. But we want to find an approximation of a function that gives us an output in a different range. (2) Our models have different inputs. Because DeepONet wants to solve differential equations, their inputs consist of both sides of the equation. They require both the left and right-hand side: u(x) and y. However, the only input of our MLPs is just data value x without the right-hand side y. (3) They need more than one big NN which is expensive on MPC. The Trunk net and Branch net have width==40 and depth==3 and 2 respectively. By contrast, our MLP approximation has as low as width==2 and depth==2. Therefore, DeepONet is much more expensive than ours. (Figure 2 has the MPC cost breakdown of each layer). The cost will be even more prohibitive on MPC with the Stacked DeepONet, which has much more Branch net inside.\n\nThere is another paper [The-X, ACL2022] mentioned in Section 1 we got inspiration from. We agree that approximation is a well-known trick but the particular design is important, we have revised our draft to clarify (introduction and contribution1). \n\nWe will highlight our key discovery: MLP approximation, while cannot be used for generic transformer inference (because of low accuracy), is uniquely suitable for data selection. Such a novel use of MLP approximation is discovered for the first time. \n\nTo show that our discovery is non-trivial, we will add the following supporting experiments, which show that MLP approximation, when used in generic inference, results in poor accuracy. Following MPCFormer, we replace three nonlinear modules in a BERT-base model with our approximations: Attention Softmax, Attention LayerNorm, and Feedforward Network Layernorm. There are 3*12 = 36 MLPs in the BERT model. In the experiments, two LayerNorm approximations are always data-aware, while there are both data-aware and fixed approximations of Softmax because of varying attention masks. The BERT with MLP approximation will be fine-tuned on the\nbenchmark before inference. Three versions of attention softmax approximations achieve no better than random guess performances: \n(1) Using fixed MLP for attention softmax. Our BERT with approximation achieves only 52.92% accuracy. \n(2) Using data-aware attention softmax approximation, which zeros out masked values on MLP outputs, has 49.08% accuracy. If we normalize the remaining values to [0, 1], the accuracy remains 49.08%. \nTo better understand the influence of each nonlinear approximation, we did an ablation study that keeps just one kind of MLP (12 of them) in BERT. However, none of them achieve better than 50.92% accuracy. We further notice that adding just one attention softmax MLP to the model has no impact on the inference accuracy. However, adding one LayerNorm MLP will degrade the accuracy by 0.85% on average. These results show that having just one MLP will hurt the inference performance a lot; having 12 or even 36 MLPs will degrade model performance drastically. \n(3) Removing the attention mask to make MLP data-driven. It always has 50.92% accuracy, no matter with all three approximations or just one approximation. \nThe poor inference performance and good data selection performance show that the MLP approximation is specifically suitable for data selection while impractical for model inference directly.\n\n**Q2**: Besides, training MLPs to approximate every non-linear operator is very cost. Does author evaluate the time cost here?\n\n**A2**: MLP training is very cheap actually. They are simple models and we generate data in large batch size==128. The total training time for softmax approximation takes 20.88 minutes for 20K epochs, 5.25 minutes for LayerNorm approximation, and 0.68 minutes for the fuse approximation with the same number of epochs on a Nvidia RTX 2080Ti GPU. It takes only 26.81 minutes of MLP training time in total for one phase, which is cheap compared with tens of hours of selection time.\nAdditionally, MLP training should be done before private data selection. Therefore the training cost is even less because of training just once."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623589423,
                "cdate": 1700623589423,
                "tmdate": 1700623589423,
                "mdate": 1700623589423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tn2hoQ2bRp",
                "forum": "C5sxQsqv7X",
                "replyto": "6CkQhl7mUK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer due4 (2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: The MPC protocol hide the computation behind the communication data exchange is not novel. Any MPC protocol would try to minimize the total latency like this way. \n\n**A3**: To clarify, MPC protocols do NOT specify the order of computation and communication (unless they are dependent); it is the role of MPC frameworks.  To our knowledge, common MPC frameworks such as Crypten do NOT hide computation behind communication automatically. \n\nWe will highlight that: our scheduling does not just simply hide computation delays. It has to create such hiding opportunities (by co-executing tasks across the batch boundaries, see Sec 4.4)\n\nWe hope these additional explanations can address your questions above. We have updated the manuscript for better clarity (new Figure 1, Section 1 Workflow, and our first contribution)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623622501,
                "cdate": 1700623622501,
                "tmdate": 1700623622501,
                "mdate": 1700623622501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QCRNMhCxHw",
            "forum": "C5sxQsqv7X",
            "replyto": "C5sxQsqv7X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission417/Reviewer_zaeT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission417/Reviewer_zaeT"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed an MPC-based private data selection framework for large Transformer models. The main technical contribution of MPC is replacing high-dimensional nonlinearity with low-dimensional MLPs. Besides, a batch evaluation is used in MPC."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "An MPC-based private data selection framework for large Transformer models.\n+ Nonlinearity evaluation with low-dimensional MLPs.\n+ Multi-phase selection\n+ Parallel MPC executions"
                },
                "weaknesses": {
                    "value": "- This work seems to simply combine the techniques of data selection and secure inference on LLMs.\n- Replacing high-dimensional nonlinearity with low-dimensional MLPs seems less general.\n- The batch evaluation is a widely used method in PPML and lacks novelty."
                },
                "questions": {
                    "value": "1. Is this non-linear evaluation method only applicable in the proposed data selection setting? Can it be extended to general MPC-based LLM?\n2. What are the differences between MPC-based data selection on LLMs and secure inference on LLMs?\n3. Why do the authors focus on LLMs? Can this method be applied to CNNs?\n4. Does this work use a third party for generating correlated randomness for MPC similar to MPCFormer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Reviewer_zaeT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832275851,
            "cdate": 1698832275851,
            "tmdate": 1699635968419,
            "mdate": 1699635968419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "opj1kvUJXL",
                "forum": "C5sxQsqv7X",
                "replyto": "QCRNMhCxHw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zaeT"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your review! We hope that our response has adequately addressed your concerns. If you have any additional feedback concerning our current draft, we would be delighted to hear it and address your points.\n\nWe summarize the concerns and answer them individually below:\n\n- Simply combine the techniques of data selection and secure inference on LLMs.\n\nResponse: We need to clarify that we don\u2019t just \"simply combine\" some techniques. the approximation and proxy model design part is non-trivial. They are efficient in our data selection case, while just borrowing current methods will bring prohibitive private selection costs as mentioned in Section 1.\n\n- Replacing high-dimensional nonlinearity with low-dimensional MLPs seems less general. \n\nResponse: There are prior works of approximating nonlinearity with MLP. As mentioned in Section 1, we are inspired by THE-X[ACL2022]. They use MLP approximation in homomorphic encryption. But we improve its efficiency by lowering the dimension, approximating the nonlinear operation as a whole, and data-driven parameters training.\n\nAs mentioned by other reviewers, Lu, Lu, et al. \"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators.\" Nature machine intelligence 3.3 (2021): 218-229. proposes using fully connected network to precisely approximate nonlinear operations for identifying differential equations.\n\nWe will highlight our key discovery: MLP approximation, while cannot be used for generic transformer inference (because of low accuracy), is uniquely suitable for data selection. Such a novel use of MLP approximation is discovered for the first time. \n\nWe acknowledge that low-dimensional MLPs are NOT for generic transformer inference (New experiment results are in the Appendix). However, We customize it and make it work for an important problem (secure data selection). In other words, we contribute a specialty technique for an important, pervasive problem. \n\n- The batch evaluation is a widely used method in PPML and lacks novelty.\n\nResponse: We will highlight that: our system goes beyond standard batch evaluation in PPML, which will result in IO starvation (Sec 4.4). Our system co-executes tasks across batch boundaries in order to create parallelism. At a high level, it brings the job-stealing principle from parallel computing into the context of PPML. \n\nWe answer other specific questions from the reviewer.\n\n**Q1**: Is this method only applicable in data selection? Can it be extended?\n\n**A1**: We interpret \u201cLLM\u201d in the question as Transformer models. However, we believe our method can also used on LLMs since they are also transformer-based.\nYes, it is for selection, not for inference.\n\n**Q2**: What are the differences between MPC-based data selection on LLMs and secure inference on LLMs?\n\n**A2**: Selection has a lower requirement of the proxy model capacity. It only requires the output entropy\u2019s rank to be consistent with that of a full model, but inference needs the proxy model to have the same output logits as those of the full model.\n\n**Q3**: Why do the authors focus on LLMs? Can this method be applied to CNNs?\n\n**A3**: We focus on Transformer models because they have many softmax and LayerNorm modules in each layer, which are very costly for MPC. We didn\u2019t try CNN because they have just a few softmax and LayerNorm modules.\nWe believe our method can be extended to CNN models, because our MLP approximation doesn\u2019t rely on model architectures.\n\n**Q4**: Does this work use a third party for generating correlated randomness for MPC similar to MPCFormer?\n\n**A4**: In principle, our design needs no third-party assistance. In particular, the Beavor Triples needed by us can be generated by oblivious transfers offline, without any third party.\n\nWe hope these additional explanations can address your questions above. We have updated the manuscript for better clarity (new Figure 1, Section 1 Workflow, and our first contribution)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623358666,
                "cdate": 1700623358666,
                "tmdate": 1700623358666,
                "mdate": 1700623358666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8hgKI6UQYE",
            "forum": "C5sxQsqv7X",
            "replyto": "C5sxQsqv7X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission417/Reviewer_ER9t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission417/Reviewer_ER9t"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a method to approximate transformer models for training using MLP instead of non-linear functions such as Softmax and layernorm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It seems that the performance of the Softmax function is important in transformers when MPC or FHE is considered. The precision of Softmax approximation has a very large impact on the overall inference performance. In this paper, they suggested to use MLP instead of them."
                },
                "weaknesses": {
                    "value": "The core ideas proposed in the paper are described on pages 4 and 5, but the description in this part is somewhat unclear. In particular, it is unclear whether data transfer between proxy models occurs using MPC in the multi-phase selection section, or not. This part needs to be restated more clearly."
                },
                "questions": {
                    "value": "In the multipass selection phase, it said, \"The forward pass computes the prediction entropy values, which are encrypted.\" The question is, what kind of encryption scheme is used? Is it a homomorphic encryption? How can you generate secret shares from the encrypted entropy values for the input of the following proxy model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Reviewer_ER9t"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699098107929,
            "cdate": 1699098107929,
            "tmdate": 1699635968357,
            "mdate": 1699635968357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CtUVxIv1ob",
                "forum": "C5sxQsqv7X",
                "replyto": "8hgKI6UQYE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ER9t"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your positive review! We hope that our response has adequately addressed your concerns. If you have any additional feedback concerning our current draft, we would be delighted to hear it and address your points.\n\nWe summarize the concerns and answer them individually below:\n\n- Whether data transfer between proxy models occurs using MPC in the multi-phase selection section.\n\nResponse: We add Figure 1 to give a better workflow overview. We will clarify that: data is never moved during selection over MPC. Data indices will be transferred. Bootstrap data and selected data will be sent out in the clear before and after selection.\n\nWe answer other specific questions from the reviewer.\n\n**Q1**: What kind of encryption scheme of encrypted entropy is used? Is it a homomorphic encryption? \n\n**A1**: It\u2019s not homomorphic encryption. In fact, the entropy, just as other intermediate states, is encrypted as arithmetic and binary secret shares, which are the MPC\u2019s design choices [2, 3]\n[2] I. Damg\u00e5rd, V. Pastro, N. Smart, and S. Zakarias. Multiparty computation from somewhat homomorphic encryption. Cryptology ePrint Archive, Report 2011/535, 2011. https:// eprint.iacr.org/2011/535.\n[3] O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game or a completeness theorem for protocols with honest majority. In STOC, pages 218\u2013229, 1987. \n\n**Q2**: How can you generate secret shares from the encrypted entropy values for the input of the following proxy model?\n\n**A2**: At the very beginning of the selection process, the secret shares of the input data are created; subsequently, all the secret shares (including entropy) are derived through MPC computations. Then next phase will know data indices, not entropy value. Thanks to this suggestion, we have added a new Figure 1 for clarification. \n\nWe hope these additional explanations can address your questions above. We have updated the manuscript for better clarity (new Figure 1, Section 1 Workflow, and our first contribution)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623098746,
                "cdate": 1700623098746,
                "tmdate": 1700623098746,
                "mdate": 1700623098746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xWtDLWPJFE",
            "forum": "C5sxQsqv7X",
            "replyto": "C5sxQsqv7X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission417/Reviewer_Xsq1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission417/Reviewer_Xsq1"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the efficient acquisition of data necessary for training artificial intelligence models using multi-party computation techniques. To effectively train models at a fixed cost when acquiring data from data owners, it is necessary to assess the quality of the data. This paper presents a method for data appraisal, allowing model owners to select data that is advantageous to them without accessing the data themselves."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach of efficiently purchasing data for training artificial intelligence systems within a fixed budget is a novel research direction. This seems to be a necessary research topic not only for AI security but also for various AI training scenarios. The paper successfully persuades the need for the research direction and research topic."
                },
                "weaknesses": {
                    "value": "It seems to lack a clear and precise explanation of the technical aspects of the paper. All the technical details regarding the research method are quite ambiguous, making it difficult to understand the core ideas of the paper. While the paper mentions proposing data selection and appraisal methods when training artificial intelligence models using MPC, it does not precisely explain how these techniques are related to MPC protocols and security. Additionally, it doesn't provide a clear explanation of why each specific technique is necessary and what problems they aim to solve, making it hard to grasp the reasons behind the use of these techniques.\n\nFurthermore, the paper does not offer a clear protocol or source code to understand how each operation is exactly performed. The technical aspects of the paper remain ambiguous, as there is no clear protocol or algorithmic explanation. (I believe that if the protocols and algorithms are given clearly, code submission itself can be considered optional.) The paper lacks explanations for the figures, making it challenging to understand their meanings. \n\nTherefore, I think this paper is not yet ready for presentation at a conference, and a complete rewrite is necessary to clarify each algorithm and protocol and make it more reader-friendly."
                },
                "questions": {
                    "value": "Some specific areas of ambiguity include:\n\n1. How each operation is computed using MPC.\n2. How entropy is calculated if MPC is used.\n3. How the index with the highest entropy is jointly found.\n4. The definition and details of the QuickSelect algorithm.\n5. The interpretation of Figures 1 and 2.\n6. The role and precise definition of the proxy model.\n7. The dimensions and hyperparameters used for the MLP that replace the nonlinear function.\n8. The data used for proxy model training.\n...\n\nThere are many other unclear aspects that need to be clarified to give the paper more value."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission417/Reviewer_Xsq1"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission417/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699363388211,
            "cdate": 1699363388211,
            "tmdate": 1699635968246,
            "mdate": 1699635968246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3MYMnxyEi8",
                "forum": "C5sxQsqv7X",
                "replyto": "xWtDLWPJFE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xsq1 (1/2)"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your review! We hope that our response has adequately addressed your concerns. If you have any additional feedback concerning our current draft, we would be delighted to hear it and address your points.\n\nWe summarize the concerns and answer them individually below: \n\n- Lack a clear and precise explanation of the technical aspects. \n\nResponse: Thank you for your feedback. Yet, we appreciate it if your comments can be more specific.\n\n- Technical details regarding the research method are ambiguous.\n\nResponse: In Section1 Goal & techniques, we explain the design goals, the techniques, and the benefits that our methods can bring. In Section 1 Workflow, we further explain how our methods work and how each technique executes from a high-level perspective.\n\n- Not precisely explain how these techniques are related to MPC protocols and security.\n\nResponse: In Sections 2 & 4, we give assumptions and explanations of MPC details. Note that we do not deviate from protocols implemented by well-known frameworks such as Crypten [Crypten, NeurIPS2021].\n\n- Lack the explanation of necessities and targeting problems of techniques.\n\nResponse: In Section 1, we talk about the challenges of private selection over MPC and explain the motivation of each contribution. In our Goal & techniques on page 2, we explicitly give the problems we want to tackle for each technique and the benefit it can bring.\n\n- No clear protocol and source code.\n\nResponse: We interpret \u201cprotocol\u201d in the question as a generic description, not specific to the MPC protocol. Section 1 provides a clear overview of our system operation. (i.e. \u201cprotocol\u201d). We interpret \u201csource code\u201d in the question as pseudo-code. Our innovation is a new proxy model architecture and selection workflow. Hence. figures are more suitable in this case.\n\n- No clear protocol or algorithmic explanation.\n\nResponse: In Section 1, we provide a clear overview of our system operations. In terms of MPC protocol, we mentioned it in Section 2.1. In Section 2.2 and Section 4, we explain our machine learning algorithm in detail.\n\n- Lack explanations for the figures.\n\nResponse: We explicitly provide the message we want to convey in each figure\u2019s title.\nIn Section 1 Workflow, we have explanations for Figure 1. In Section 2.3 Major overheads, we have explanations for Figure 2. In Section 4.2 and 4.3, we have explanations for Figure 3 and Figure 4. In Section 5.2, we have explanations for Figure 5 and 6. In Section 5.4, we have explanations for Figure 7."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621893888,
                "cdate": 1700621893888,
                "tmdate": 1700621893888,
                "mdate": 1700621893888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yci6iBZhqW",
                "forum": "C5sxQsqv7X",
                "replyto": "xWtDLWPJFE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission417/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xsq1 (2/2)"
                    },
                    "comment": {
                        "value": "**This Response (2/2) is identical to the deleted Response (2/2). We deleted that one since we want to directly respond to Reviewer Xsq1, not to our Response (1/2). Sorry for the confusion.**\n\nWe answer other specific questions from the reviewer.\n\n**Q1**: How each operation is computed using MPC.\n\n**A1**: In Section 2.1 Threat model, we mention that all operations are supported by Crypten [Crypten, NeurIPS2021]. Before MPC operations, all data must be first encrypted. This includes both the model parameters as well as the inference data. On a high level, they can be seen as matrices. After encryption, the encrypted matrices still have the same shape as before the encryption, but all the entries now contain randomized values. Furthermore, during the MPC operations (addition and multiplication), the data entries remain randomized, but their shapes are still publicly revealed. It\u2019s until both parties decide to share their respective data share to each other and add them up, that the data is decrypted.\n\n**Q2**: How entropy is calculated if MPC is used.\n\n**A2**: In Section 4.3, we mention that entropy is predicted by an MLP. Besides, entropy can also be calculated by the formula, they are logarithms and multiplications supported by Crypten.\n\n**Q3**: How the index with the highest entropy is jointly found.\n\n**A3**: We use QuickSort (Section 4.1 Multipass selection) to find the index with the highest entropy. But since MPC doesn\u2019t support control flow, we reveal the comparison results between two data entropy. Since our goal is to find the index with the highest entropy, this revealing doesn\u2019t hurt the privacy assumption.\n\n**Q4**: The definition and details of the QuickSelect algorithm.\n\n**A4**: It is a textbook algorithm, c.f. Hoare, C. A. R. (1961). \"Algorithm 65: Find\". Comm. ACM. 4 (7): 321\u2013322. doi:10.1145/366622.366647.\n\n**Q5**: The interpretation of Figures 1 and 2.\n\n**A5**: In Section 4.2, we have a detailed explanation of our algorithm and these two figures (They are Figures 3 and 4 now).\n\n**Q6**: The role and precise definition of the proxy model.\n\n**A6**: In Section 2.2(2) Proxy models, we cite prior works of creating lightweight proxy models for data selection and mention we tailor new proxy model architectures for our goal. In Section 4.2, we explain how to generate proxy models in detail, including their architectures and training process.\n\n**Q7**: The dimensions and hyperparameters used for the MLP that replace the nonlinear function.\n\n**A7**: In Figure 4 and Section 5.4 MLP hidden dimensions, we mention that dimensions are 2/8/16 for a three-phase selection.\n\n**Q8**: The data used for proxy model training\u2026\n\n**A8**: In Section 4.1 Pre-selection bootstrap, we explain that the model owner will use the bootstrap data S_boot to generate proxy models $M^^_{1..N}$.\n\nWe hope these additional explanations can address your questions above. We have updated the manuscript for better clarity (new Figure 1, Section 1 Workflow, and our first contribution)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission417/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622908627,
                "cdate": 1700622908627,
                "tmdate": 1700622908627,
                "mdate": 1700622908627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]