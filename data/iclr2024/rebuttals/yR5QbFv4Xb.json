[
    {
        "title": "Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution"
    },
    {
        "review": {
            "id": "rEJuBlF7th",
            "forum": "yR5QbFv4Xb",
            "replyto": "yR5QbFv4Xb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for training an additive neural network, where the final prediction is the summation of learned contributions for each feature. The contribution for each feature is also learned on the fly, and is trained to approximate the Shapley value of each feature. On several tabular datasets, the authors show that their method is capable of attaining high performance while also learning Shapley values (i.e. contributions) for each feature which are meaningful."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Interesting idea to have the model learn Shapley values on its own\n\nThe core concept is unique and also very interesting. Shapley values can be somewhat difficult to estimate, and it is intriguing to have them be learned from the data during training, within the framework of an interpretable additive model.\n\n### Good mix of different datasets and benchmarks\n\nThe authors did a good job of using different tabular datasets and benchmarking across different methods that also provide interpretable features, as well as non-interpretable methods like a simple MLP."
                },
                "weaknesses": {
                    "value": "### Partial marginal contribution scores might not be meaningful\n\nI have my doubts as to how meaningful the learned marginal contribution scores from $\\Delta$ can be in general. The goal is to have $\\Delta$ for a single feature $i$ match its true Shapley score, but in the absence of true Shapley labels, the network is regularized to satisfy the following: 1) the summation of the $\\Delta$ values over all features $i$ should be the label $y$ (Equation 3); 2) for any subset of features $\\mathcal{S}$ which include $i$, different permutations of $\\Delta$ on that subset should be the same value (Equation 4); and 3) partial sums of $\\Delta$ values should also equal the label $y$ (Equation 5). As the authors point out, while the first two conditions are sufficient to ensure the model learns permutation-invariant contribution scores which add to the final label $y$, they alone are not sufficient to ensure that the contribution scores for each feature are meaningful (e.g. the feature contributions can be 0 for every feature except the last one). Condition 3 is meant to fix this, but it is not clear how that is done.\n\nIn particular, condition 3 (Equation 5) seems like it trains the network to make sure that the partial sums of contribution scores (i.e. for any subset of feature $\\mathcal{S}$) to be equal to the label $y$ _as if_ it had the full set of features. For example, consider a 2-feature dataset where feature 1 does not matter and the true label is simply equal to feature 2. Then in Equation 5, $L_v$ on just feature 1 would try and make the importance of feature 1 match the label, but in reality the contribution of feature 1 should just be 0.\n\n### SASANet addresses traditional Shapley challenges in a potentially inefficient way\n\nThe traditional challenges with computing Shapley values are: 1) how to tractably estimate the marginal contribution of feature $i$ when the number of subsets of features is exponential; and 2) given a subset of features, how to obtain a value/prediction when most models are not capable of handling partial inputs.\n\nSASANet addresses these challenges by effectively training over many subsets, thereby forcing the model to be robust to fewer features, across different subsets. This might lead to significant inefficiencies in training, given the possible subsets and orderings, especially as the number of features becomes larger. The time analysis in section 4.4 is appreciated, but that seems to only include the time taken to compute the feature importances _post hoc_. How does the training time of SASANet compare to other methods?\n\n### SASANet is only applied to small tabular datasets\n\nIn this work, SASANet is only being applied to a few small tabular datasets. Especially given the potential brittleness of the marginal contributions and the potential inefficiencies (as described above), it is possible that this method does not translate well to larger tabular datasets (e.g. gene expressions) or non-tabular datasets (e.g. images, text).\n\n### Minor typos\n\n- Shapley is spelled Shapely in several places\n- Section 3.2, Specially \u2192 Specifically\n- Typo in loss equation in 3.5"
                },
                "questions": {
                    "value": "In the analysis of the accuracy of Shapley values, how were ground truth values obtained for RMSE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685636400,
            "cdate": 1698685636400,
            "tmdate": 1699636444050,
            "mdate": 1699636444050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jnER7Ytnl7",
                "forum": "yR5QbFv4Xb",
                "replyto": "rEJuBlF7th",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Ff8r,\n\nThank you for your time reviewing our paper and your valuable comments! Your concerns are addressed as follows.\n\n> Partial marginal contribution scores might not be meaningful\n\nThe purpose of our distillation loss is not to make $\\triangle$ match the Shapley value, but rather to make the learned attribution value converge to the average of our intermediate value $\\triangle$ (Proposition 3.3). This will eventually make the attribution value the Shapley value of the final output (Theorem 3.4). All our losses are direct training losses instead of regularizers.\n\nThe statement in condition 3, \"partial sums of values should also equal the label $y$,\" is misunderstanding. In terms of the physical meaning of $\\triangle$ and $f_c$, the objective of Equation 5 is to make the sum converges to the expectation of $y$ given these observable features (i.e., the conditional probability $p(y|x_S)$ in binary classification), which is not the label of one sample but the average in the entire dataset. This is proven in Appendix Proposition D.1. In the scenario you mentioned, \"feature $x_1$ does not matter\" statistically means its observation does not influence the posterior distribution of $y$. Consequently, the conditional probability $p(y|x_1)=p(y|\\emptyset)$, and the contribution of $x_1$ in this case is exactly 0 because $p(y|\\{x_1\\})=\\triangle(x_1, \\emptyset) + \\phi_0$ and $p(y|\\emptyset)=\\phi_0$. \n\nIn Appendix Figure 4, we also provided a plot of the model prediction using partial observation, which demonstrates that the model can make meaningful predictions with partial observations. Our paper rigorously proves that the Shapley module converges to the desired Shapley value, which demonstrates the effectiveness of these modules.\n\n\n> SASANet addresses traditional Shapley challenges in a potentially inefficient way\n\nModeling predictions under partially observed results is indeed a more challenging task. Our permutation-based training approach requires 2-3 times more training epochs to converge compared to directly fitting the final results. However, we believe that the benefits of obtaining accurate Shapley values, being able to model non-linear feature-label correlation, and having high robustness in predicting with missing features outweigh the additional training cost. We will try to find a space to clarify this point in our paper.\n\n> In this work, SASANet is only being applied to a few small tabular datasets. Especially given the potential brittleness of the marginal contributions and the potential inefficiencies (as described above), it is possible that this method does not translate well to larger tabular datasets (e.g. gene expressions) or non-tabular datasets (e.g. images, text).\n\nIn SASANet, our main objective is to model complex non-linear correlations between semantically meaningful features and labels, which is crucial for scientific analysis and discovery. The Shapley value provides a theoretically sound approach for evaluating this correlation. However, its intricate definition makes computation highly complex and involves permutations, making it challenging to handle large-scale features effectively. In the case of high-dimensional inputs like images, text, or gene expressions, where semantics depend on spatial interactions of multiple input points, the individual points (e.g., a single pixel) may lack specific conceptual meaning to explicitly contribute to the ouput. In this case, it is more reasonable to extract concepts from raw features using backbones and attribute concept contributions to the output instead of assigning an accurate Shapley value to each input point. To achieve explainable concept-based inference tasks for CV/NLP, we simply add an attribution layer on top of the concept extraction layer. Notably, depite broad application scenarios, our paper focuses on experimenting with tabular data. Tabular data naturally have explicit concepts with clear physical meanings, thus make the pure attribution ability more explicit to observe.\n\n> Minor typos\n\nThank you for pointing them out! We will correct it.\n\n> In the analysis of the accuracy of Shapley values, how were ground truth values obtained for RMSE?\n\nWe draw different orders for each sample and directly calculate the Shapley value of the output based on its definition, i.e, sample orders and get the marginal contribution of each feature and get the average. In our experiments, we have added some engineering tricks to reduce the time complexity. For example, we pre-calculate and store the attention matrix and value among features. Then, we inrementally update the representations for each prefix subset in an order. However, it still took us days to get the ground truth results for these 1,000 samples."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334886792,
                "cdate": 1700334886792,
                "tmdate": 1700334886792,
                "mdate": 1700334886792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hYhlpe1YPu",
                "forum": "yR5QbFv4Xb",
                "replyto": "jnER7Ytnl7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_Ff8r"
                ],
                "content": {
                    "title": {
                        "value": "Response to comment"
                    },
                    "comment": {
                        "value": "Thank you to the authors for providing additional clarifications! I have a much better understanding of the method now.\n\nI believe the main thing holding this work back is the limited datasets (Weakness 3 from above). Particularly because SASANet relies so heavily on permutations and partial feature sets, the concern that it cannot scale to larger and more complex datasets is very real. Particularly because the main objective is to apply SASANet to scientific discovery, it would be extremely helpful to see SASANet attain reasonable interpretability and predictive performance on a larger dataset. If the method is more limited to smaller tabular datasets where individual features need to be independently meaningful, then that is a pretty strict limitation on this method, as there are many other post hoc interpretability methods which would be much faster and more accurate on such small, simple datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501541401,
                "cdate": 1700501541401,
                "tmdate": 1700501541401,
                "mdate": 1700501541401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YS5HjUlOs9",
            "forum": "yR5QbFv4Xb",
            "replyto": "yR5QbFv4Xb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
            ],
            "content": {
                "summary": {
                    "value": "This study proposes a new self-interpretable network based on Shapley values. Based on the permutation-based definition, the authors explicitly use a model to learn the marginal contribution of each input feature in each specific permutation. Then, they further learn a model to predict Shapley values of each feature based on its marginal contributions in different permutations. The task labels are used as additional supervision to ensure the performance of the network."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The network in this paper uses the multi-head attention layers to take inputs of varying lengths, which avoids problems with masking features or formulating the distribution of features.\n\nThe proposed method achieves similar performance to black-box models on tabular datasets. Besides, the authors evaluate the quality of attributions generated by the proposed network in different aspects, which demonstrate the reliability of the attribution."
                },
                "weaknesses": {
                    "value": "- The proposed network is not sufficiently interpretable, because the internal modules are still black boxes. This network makes predictions while automatically providing Shapley values of each input feature. However, the modules for computing marginal contributions and Shapley values are constructed as multi-head attention layers and learned by back-propagation. Therefore, the internal computation is still unexplainable. In comparison, the SHAPNet (Wang et al., 2021) makes all the layer-wise propagation of features in the network interpretable.\n- The loss function in equation (5) is confusing. It seems that the right-hand formula in equation (5) equals $L_m(x_S,y,O_S)$, because this term is independent of $(x\u2019,y\u2019)$. I check the proof in the appendix and I guess there is a typo: it should be $L_m(x_S,y\u2019,O_S)$. In this way, $\\sigma(f_c)$ is forced to model the distribution $p(y|x_S)$.\n- The presentation of the paper needs improvement. The authors introduce the function of each module but do not provide an overview of the whole framework. Thus, when I read it the first time, it takes much time to understand the whole network. Besides, I suggest the authors clarify the models to be learned/optimized in each loss function. Otherwise, it may be confusing that whether equation (3) optimizes $\\Delta(\\cdot,\\cdot;\\theta_\\Delta)$ or $f_c$. \n- The scalability of the proposed method to complex tasks is questionable. First, the training and testing of the model need various permutations of input features, which leads to a huge computational cost. Second, the authors do not conduct experiments on language or image data. I wonder whether the performance of the proposed method on more complex tasks can still match the performance of black-box models, especially ResNets or transformers, which are more widely used than MLPs in applications.\n- What is the computational complexity in the inference stage? Does it only use the Shapley module for inference? If yes, then what is the benefit of learning an additional marginal contribution module? If no, then the complexity in the inference stage is still $2^n$."
                },
                "questions": {
                    "value": "- Why are you learning a model to predict the marginal contribution $\\Delta$ instead of directly predicting $f_c(x_S)$, which is used in KernelSHAP and Frye et al., (2019)?\n\n(Frye et al., 2019) Shapley explainability on the data manifold. In ICLR 2021.\n \n- Typos: Section 3.5: $O_k,k$,$O_K$, $k=1$, and $O_i$ are all subscripts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746369739,
            "cdate": 1698746369739,
            "tmdate": 1699636443962,
            "mdate": 1699636443962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RnRg3lGSVh",
                "forum": "yR5QbFv4Xb",
                "replyto": "YS5HjUlOs9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2AVx,\n\nThank you for your time reviewing our paper and your valuable comments! Your concerns are addressed as follows.\n\n> Not sufficiently interpretable compared to SHAPNet\n\nSHAPNet calculates the Shapley value for each layer between hidden units, but it cannot accurately calculate the inputs' Shapley value to the final output. Our work's significance lies in being the first to realize global Shapley self-attribution, which is significant considering Shapley has been widely reconized as effective attribution metric. Indeed, attribution aims to accurately quantify the contribution of each input feature to the output, and the Shapley value is explicitly defined as the average change in output when adding a feature. It has a clear physical meaning in showing the effect of features without the need for a transparent intermediate procedure. Extensive post-hoc interpretations studies have been using the Shapley value itself as a model interpretation instead of discussing the mechanism of generating it.\n\n> The presentation of the paper needs improvement. \n\nThank you for your suggestion and we will polish our paper accordingly!\n\n> Scalability of the proposed method to complex tasks\n\nIn SASANet, our main objective is to model complex non-linear correlations between semantically meaningful features and labels, which is crucial for scientific analysis and discovery. The Shapley value provides a theoretically sound approach for evaluating this correlation. However, its intricate definition makes computation highly complex and involves permutations, making it challenging to handle large-scale features effectively. In the case of high-dimensional inputs like images, text, or gene expressions, where semantics depend on spatial interactions of multiple input points, the individual points (e.g., a single pixel) may lack specific conceptual meaning to explicitly contribute to the ouput. In this case, it is more reasonable to extract concepts from raw features using backbones and attribute concept contributions to the output instead of assigning an accurate Shapley value to each input point. To achieve explainable concept-based inference tasks for CV/NLP, we simply add an attribution layer on top of the concept extraction layer. Notably, depite broad application scenarios, our paper focuses on experimenting with tabular data. Tabular data naturally have explicit concepts with clear physical meanings, thus make the pure attribution ability more explicit to observe.\n\n\n> Computational complexity in the inference stage\n\nOnly the Shapley module is used for inference. The purpose of learning marginal contribution is to converge the attribution value to the Shapley value of the final output, as shown in Theorem 3.4. Without learning marginal contribution and simply training a single Shapley module with a regularizer, we cannot obtain the Shapley value. This is because the optimization directions of the regularizer and prediction loss create a trade-off that makes it difficult to accurately converge to the constraint that the attribution value is the Shapley value of the final output.\n\n> Why learning $\\triangle$ instead of $f_c(x_S)$\n\nBoth approaches are theoretically feasible. Modeling $f_c(x_S)$ directly and then taking the difference to obtain $\\triangle$ is theoretically equivalent to our current method and does not affect convergence of $\\phi$ to the Shapley value of $f$. However, in practical implementation, modeling $\\triangle$ allows the model to focus more on  the unique contribution of each added input, which aligns with Shapley's definition. On the other hand, modeling $f_c(x_S)$ requires combining the information of the entire feature set and potentially make the effect of each feature vague. Therefore, we chose to directly model $\\triangle$ to simplify the operation when we design our model.\n\n> Typos about subsripts and loss equation (5):\n\nThank you for pointing this out and we will double check the notations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334875837,
                "cdate": 1700334875837,
                "tmdate": 1700334875837,
                "mdate": 1700334875837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K4CvMjHzfP",
                "forum": "yR5QbFv4Xb",
                "replyto": "RnRg3lGSVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_2AVx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. The authors have addressed most of my concerns. However, I think there is still room for improvement.\n\n- The scalability to complex models/tasks. I understand that the intrinsic definition of Shapley values makes it hard to handle high-dimensional input. My concern is that the architecture of the model to be explained (MLPs) in the paper is too simple and the task also seems easy. I expect experiments on more complex models to show that the proposed method *is able to* well explain and fit the performance of complex models. From another perspective, the authors can also estimate attributions of concepts in CV/NLP tasks, as they stated in the response, and then compare the explanation and performance (accuracy) of different explaining methods.\n \n- Given that learning $f_c(x_S)$ is also theoretically feasible, I guess learning $f_c(x_S)$ may induce less computational cost than learning $\\Delta$, because the output $f_c(x_S)$ can be reused in the computation of Shapley values of different input features. In contrast, $\\Delta$ is specific to each permutation of inputs. It would be better if the authors could provide some theoretical proof or empirical evidence to show that learning $\\Delta$ is better."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556947193,
                "cdate": 1700556947193,
                "tmdate": 1700556947193,
                "mdate": 1700556947193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WGYoKhlOBn",
            "forum": "yR5QbFv4Xb",
            "replyto": "yR5QbFv4Xb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
            ],
            "content": {
                "summary": {
                    "value": "The article presents a new self-interpreting approach called SASANet, having in mind to incorporate Shapley values into the additive self-attribution literature. The approach is compared to black-box approaches and empirically demonstrates their usefulness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The approach seems theoretically well-grounded; the literature review seems thorough and complete."
                },
                "weaknesses": {
                    "value": "**Major**\n\nThe only points I would like to raise concern the numerical experiments.\n\n1 \u2013 The comparison to other self-attribution methods in Table 1 is convincing, but I think other conclusions drawn from the experiment reported in Table 1 are misleading.\n\n1.1 - I don\u2019t understand why comparing SASANet to interpretable approaches, more specifically to a linear regressor and a single decision tree. Those simple methods are sure to obtain somewhat deceiving performances when compared to a huge neural network, where having some black-box architecture is not a problem. Inherently interpretable predictors have advantages of their own (being fully transparent, for example) that can\u2019t be simply quantitatively compared to others, and thus shouldn\u2019t be compared solely in terms of performances. When it comes to interpretable machine learning, there is a huge literature on that matter, especially in an era where interpretability has been put forward for various reasons, so simple linear regressor and decision tree might just lead to underestimating the potential of interpretable approaches and overestimate the power of SASANet.\n\n1.2 \u2013 I don\u2019t feel like the comparison to black-box models is fair. Was the MLP of reasonable size? No detail is shared on that matter. Comparing an MLP to a richer and more complex architecture involving transformers is questionable. Also, LightGBM dates a bit (2017); the goal of those experiments is to compare SASANet to state-of-the-art approaches when it comes solely to performances (i.e. black-boxes) but I don\u2019t feel like those baselines are sufficient.\n\n2.1 \u2013 Table 2: I\u2019m not sure about how to interpret this table. Why is it that having the worst performances is sought? I understand that since the top-5 relevant features were removed, but why not therefore look at the relative variation in performances?\n\n2.2 - Fidelity experiments: It is stated in the abstract that \u00ab\u00a0SASANet is shown more precise and efficient than post-hoc methods in interpreting its own predictions\u00a0\u00bb. I don\u2019t feel like this has been demonstrated in any way. The protocol for this demonstration is explained as follows: \u00ab\u00a0We observed prediction performance after masking the top 1-5 features attributed by SASANet for each test sample and compared it to the outcome when using KernelSHAP, a popular post-hoc method, and FastSHAP, a recent parametric post-hoc method. The results are shown in Table 7.\u00a0\u00bb And the conclusion that is drawn is the following: \u00ab\u00a0SASANet\u2019s feature masking leads to the most significant drop in performance, indicating its self-attribution is more faithful than post-hoc methods.\u00a0\u00bb There is a logical gap here. Much information remains unknown: Were the features considered by the approaches correlated (to those in the top-5) in any way? Were the importance given by each approach to their top-5 approximately the same? Why not remove, for example, the features accounting for the first 20% importance? Also, Shapley values measure the difference in prediction, not the difference in performance. Therefore, a feature could have a great impact on the predictions while not affecting the performances at all (or smaller than expected). E.g. having an error of -x instead of x, thus a same squared error. Considering The protocol is simply insufficient to claim that \u00ab\u00a0SASANet is shown more precise and efficient than post-hoc methods in interpreting its own predictions\u00a0\u00bb.\n\n**Minor**\n\n1 \u2013 Typo in Theorem 3.4: \u00ab\u00a0with ample permutation\u00a0\u00bb.\n\n2 \u2013 At the beginning of section 4.2: \u00ab\u00a0Table 6 shows average scores from 10 tests; Appendix J lists standard deviations.\u00a0\u00bb; Table 1 should be named, not Table 6. The same thing occurs later on: Table 7 is mentioned while referring to Table 2. Otherwise, Tables 1 and 2 aren\u2019t referred to anywhere in the article."
                },
                "questions": {
                    "value": "1 \u2013 When it comes to feature attribution approaches, the time is reported in Table 3, but how does the training time of SASANet compare to others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The 9-page limit is exceeded."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4641/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4641/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4641/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801919527,
            "cdate": 1698801919527,
            "tmdate": 1699636443884,
            "mdate": 1699636443884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wtAj20Uwu1",
                "forum": "yR5QbFv4Xb",
                "replyto": "WGYoKhlOBn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer JuUA,\n\nThank you for your time reviewing our paper and your valuable comments! Your concerns are addressed as follows.\n\n> About interpretable approaches\n\nTable 1 aims to highlight the advantages of SASANet over other self-attributing models. The two black-box models and classic interpretable models widely used in tabular data are reported to position the expressiveness of these self-attributing models. We are not trying to prove SASANet is superior to self-interpreting models under any other interpretability paradigms. Since different paradigms have different goals and formulations, we believe they cannot be compared.\n\nUnder attribution paradigm, our focus is on accurately quantifying the contribution of features to the output, which has its unique significance for feature selection, debias, and discovering feature-outcome correlations. We believe that our current paper effectively demonstrates how SASANet excels other self-attribution models with its  well-grounded theoretical basis and high accuracy.\n\n> About black-box models\n\nWe use these classic black-boxes commonly used for tabular data as references to demonstrate our superiority over other self-attribution methods. We have never claimed SASANet to achieve state-of-the-art black-box models' performance. It is clear that other self-attribution methods have struggled to match MLP and LightGBM, while SASANet does, showing it significantly enhances usability with guaranteed interpretability but acceptable sacrifice on expressiveness, at least nearly reaching MLP. We did not compare transformers because they are not common for tabular data. In terms of MLP size, as stated in the paper, we fine-tuned them to their best performance, since simply training an MLP has weaker supervision than SASANet's loss and more easily overfit. For instance, we find the best performance is obtained by $4 \\times 64$ for CENSINCOME and $4 \\times 512$ for HIGGS, which can be found in our code and we will summarize them in appendix later. \n\n> About Table 2:\n\nFeature masking is common for evaluating the accuracy of an attribution method [1]. It assesses how well the method can distinguish important features that are necessary for accurate predictions. The rationale behind is that removing an important feature for an instance make the model lacks sufficient evidence to support the right prediction. Comparing the relative performance change equals comparing the absolute performance. Since the base performance are the same for all the interperters, which is the model's original performance without removing features.\n\n[1] Alvarez Melis, et. al. \"Towards robust interpretability with self-explaining neural networks.\" Advances in neural information processing systems 31 (2018).\n\n\n> About Fidelity experiments:\n\nIn the feature-masking experiment, the correlation between a feature and the outcome is determined by the model itself, not the specific interpreter. Each interpreter's attribution is compared in terms of how good they distinguish more important features, whose removal results in larger performance reduction, remain consistent regardless of the interpreter used. Therefore, removing the same number of features and compare which interpreter makes a better selection is a fair and intuitive comparison. \nIt is statistically unlikely that removing important features on which a trained model relies for accurate predictions make all samples deviate towards a prediction that maintains the original performance. In reality, once important features are removed, the model lacks sufficient evidence, and the prediction becomes more akin to guessing. A guess can be difficult to perform as good as a reasonable inference based on strong evidences. \n\nIn addition to feature-masking experiment, Table 3 shows our fidelity in a different perspective, which directly indicates that we learned more accurate Shapley values than post-hoc methods.\n\n\n> Typos\n\nThank you for pointing out the typos! We will correct them in our paper.\n\n\n> Training time of SASANet \n\nPost-hoc methods usually require no training, while incur high-cost computation for every sample. FastSHAP has a slow training process, preventing us from obtaining results on the HIGGS dataset in Table 2.\n\nCompared to regular training procedure that directly fits the final results, SASANet requires 2-3 times more epochs to converge since it learns additional information, such as Shapley values and predictions under partially observable features. However, we believe this trade-off is reasonable in practice, as it allows us to gain more information for both higher interpretability and insights into non-linear feature-label correlation, as well as an ability to robustly handle any severe missing feature problem.\n\n> Page limitation\n\nThe Ethical Statements and Reproducible Statements are added according to the author guidelines, which says they are not counted towards the page limit."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334850931,
                "cdate": 1700334850931,
                "tmdate": 1700334850931,
                "mdate": 1700334850931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wutRCM8Suq",
                "forum": "yR5QbFv4Xb",
                "replyto": "wtAj20Uwu1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4641/Reviewer_JuUA"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their exhaustive response. \n\n-Concerning Table 1 and the interpretable approaches: it is said in the response above that concerning SASANet and interpretable approaches, \"since different paradigms have different goals and formulations, we believe they cannot be compared\", but it is said in the article that \"Black-box models outperform other compared models. Classic tree and linear methods, though interpretable, are limited by their simplicity.\" It mostly is that latter sentence that raised my attention, along with the presence of Linear regression and Decision tree in Table 1, when writing Weaknesses - Major - 1.1.\n\n-Concerning Table 1 and black-box models: I better understand the relevance of the comparison; maybe this should be highlighted more clearly in the article.\n\n-Concerning the fidelity experiments: I maintain that the drop in performances should be normalized by how much of feature importance have been removed. Because even though a same number of features is removed for each interpreter, this could result in significantly different amount of feature importance drop. Or how much a feature that is removed impacts the performances accordingly to the importance attribution methods have given it."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4641/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491509120,
                "cdate": 1700491509120,
                "tmdate": 1700491509120,
                "mdate": 1700491509120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]