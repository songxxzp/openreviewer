[
    {
        "title": "Continual Supervised Anomaly Detection"
    },
    {
        "review": {
            "id": "V9fbnmGzBR",
            "forum": "uizIvVBY8P",
            "replyto": "uizIvVBY8P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_1EUX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_1EUX"
            ],
            "content": {
                "summary": {
                    "value": "Many anomaly detection papers assume that only normal instances are present in the training, and train the model unsupervised.\nHowever, in the real world, there are situations where even a few labeled abnormal instances are available.\nIn this case, studies have shown that even a very small number of anomalies can significantly improve the performance of the detector.\nIn addition, anomaly detectors are often trained under the assumption that the data distribution is stationary, but in real-world deployments, the distribution changes over time.\nTherefore, the authors propose a supervised anomaly detection method using continual learning.\nThe method consists of a Variational AutoEncoder (VAE) and a binary classifier.\nThe VAE uses the reconstruction error to determine whether the input data is an unseen anomaly, and the binary classifier determines whether the input data is a seen anomaly, and calculates the anomaly score by aggregating the results of both models.\nIn addition, the VAE's decoder is used to generate data, which is then used for generative replay to prevent catastrophic forgetting in continual learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well organized and the notation is easy to follow.\n- The structure of the model and the organization of the methods (such as loss) are theoretically clean and natural. The authors naturally integrated supervised anomaly detection with continual learning.\n- The proposed method works with various types of input data, such as images and tabular data.\n- It is impressive that the method utilizes CVAE and a binary classifier to learn the process of generating rare abnormal instances by gradient descent, which is then used for generative replay."
                },
                "weaknesses": {
                    "value": "The main weakness is that experimental results do not sufficiently support the superiority of this method.\n\n- On tabular datasets such as UNSW, bank, and credit, the model does not significantly outperform the other baselines. In many cases, the performance is similar to that of the binary classifier, suggesting that the performance is due to the binary classifier included in the method rather than the proposed method.\n- The experimental baselines are too simple. BC and VAE are components of the proposed method, and there are many methods that might outperform DevNet and Deep SAD, at least in the image domain (Liu et al., 2023). Many anomaly detection methods in the image domain are not designed for continual learning, but since EWC and A-GEM can be applied, it would be meaningful if the proposed method outperforms in this setting.\n- In the image domain, the proposed method shows better performance than other baselines, but it seems that experiments on larger datasets are needed to show the practicality of the proposed method. The method was only tested on FMNIST and MNIST with MLP structure, but it would be useful to test it on larger datasets such as CIFAR10 and CelebA.\n---\n**Liu et al.** [Deep Industrial Image Anomaly Detection: A Survey](https://arxiv.org/abs/2301.11514). *arXiv*, 2023"
                },
                "questions": {
                    "value": "- Similar to other continuous-learning papers, it would be nice to be able to see how performance changes with additional training on each task."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5199/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5199/Reviewer_1EUX",
                        "ICLR.cc/2024/Conference/Submission5199/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799267456,
            "cdate": 1698799267456,
            "tmdate": 1700537871604,
            "mdate": 1700537871604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wfFjJKAGk4",
                "forum": "uizIvVBY8P",
                "replyto": "V9fbnmGzBR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to 1EUX"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions.\n\n\n- On tabular datasets such as UNSW, bank, and credit, the model does not significantly outperform the other baselines. In many cases, the performance is similar to that of the binary classifier, suggesting that the performance is due to the binary classifier included in the method rather than the proposed method.\n\nAs you mentioned, the difference between the performances of our method and the binary classifier is very small in Table 1. However, the performance of the binary classifier should decrease when the number of anomalies available for training becomes small because the binary classifiers are not suitable for detecting unseen anomalies. To confirm this, we conducted some experiments when the number of anomalous instances available for training was reduced to 1/5. The following are the results.\n\n|                       | FMNIST          | MNIST           | UNSW            | bank           | credit          |\n|:----------------------|:----------------|:----------------|:----------------|:---------------|:----------------|\n| BC, A-GEM       | 66.28$\\pm$12.41 | 64.92$\\pm$7.39  | **96.69$\\pm$0.53**  | 79.51$\\pm$3.02 | 89.70$\\pm$1.84  |\n| BC, EWC         | 63.48$\\pm$10.12 | 64.25$\\pm$8.78  | **90.11$\\pm$10.01** | **80.92$\\pm$2.14** | 89.70$\\pm$1.84  |\n| BC, LB          | 58.63$\\pm$10.12 | 52.66$\\pm$9.88  | 67.60$\\pm$12.09 | 74.07$\\pm$1.71 | 89.64$\\pm$1.75  |\n| Ours           | **83.61$\\pm$3.79**  | **89.37$\\pm$6.09**  | **96.56$\\pm$0.83**  | **82.36$\\pm$0.97** | **94.75$\\pm$0.69** |\n\nWe can see that while the performance of the binary classifier has dropped significantly, the performance of the proposed method has not dropped that much. The difference is especially obvious for the credit dataset, which is tabular data.\n\n- Many anomaly detection methods in the image domain are not designed for continual learning, but since EWC and A-GEM can be applied, it would be meaningful if the proposed method outperforms in this setting.\n\nTo compare our method with the latest supervised anomaly detection method, we have implemented BGAD[3], mentioned by Reviewer QxJr, and conducted experiments. The experimental results are shown below.\n[3] Yao X, Li R, Zhang J, et al. Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection. CVPR, 2023.\n\n|                       | FMNIST          | MNIST           | UNSW            | bank            | credit         |\n|:----------------------|:----------------|:----------------|:----------------|:----------------|:---------------|\n| BGAD, A-GEM     | 78.01$\\pm$4.98  | 77.85$\\pm$6.15  | 96.52$\\pm$2.22  | **84.41$\\pm$2.05** | **95.52$\\pm$0.89** |\n| BGAD, EWC       | 69.98$\\pm$11.63 | 70.91$\\pm$8.40  | 95.83$\\pm$1.75  | **86.40$\\pm$1.80** | **95.66$\\pm$0.97** |\n| BGAD, LB        | 65.91$\\pm$7.16  | 63.85$\\pm$7.76  | 91.33$\\pm$6.33  | 72.74$\\pm$4.00 | 94.39$\\pm$1.93 |\n| Ours        | **88.23$\\pm$2.56**  | **95.46$\\pm$1.64**  | **98.15$\\pm$2.21**  | **86.75$\\pm$2.93** | **96.12$\\pm$1.22** |\n\nAlthough the method uses the trained backbone model to extract the feature maps from the inputs, it was removed in the experiment above because it does not fit our paper's problem settings. This is because one of the goals of this paper is how to continuously train models from scratch from multiple tasks consisting of a small number of data, and the availability of models trained on large data sets in advance, such as backbone models, is inconsistent with this goal. The hyperparameters for BGAD basically followed its paper and GitHub implementation. We have added the details of the experiments to our manuscript. From the above results, we can see that our method outperforms the latest supervised anomaly detection methods.\n\n- In the image domain, the proposed method shows better performance than other baselines, but it seems that experiments on larger datasets are needed to show the practicality of the proposed method. The method was only tested on FMNIST and MNIST with MLP structure, but it would be useful to test it on larger datasets such as CIFAR10 and CelebA.\n\nAs we mentioned in the replay to Reviewer vozw, our method is not domain-specific and is applicable to other domains, including tabular data. We conducted experiments on MNIST and FMNIST to show that the proposed method can detect anomalies even in image domains, and we believe these results are helpful enough to show that. Of course, using the proposed method on difficult image datasets would also be possible by using powerful generative models (VQ-VAE, GAN, diffusion models, etc.) instead of VAE.  A more in-depth investigation of proposed methods for specific domains is a topic for future work.\n\n- Similar to other continuous-learning papers, it would be nice to be able to see how performance changes with additional training on each task.\n\nThank you for your advice. We will add to the paper the performance changes with additional training on each task."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283868904,
                "cdate": 1700283868904,
                "tmdate": 1700283868904,
                "mdate": 1700283868904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIQnBOCX0T",
                "forum": "uizIvVBY8P",
                "replyto": "wfFjJKAGk4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_1EUX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_1EUX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the answers and additional experiments. I think the proposed approach is interesting and will be helpful for future research, and I decided to raise the review score. But I still have concerns about its practicality and performance on large datasets."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537857927,
                "cdate": 1700537857927,
                "tmdate": 1700537857927,
                "mdate": 1700537857927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P9bOOzztEE",
            "forum": "uizIvVBY8P",
            "replyto": "uizIvVBY8P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_QxJr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_QxJr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach to the task of continual supervised anomaly detection. This paper designs a pipeline with three specific components: a variational autoencoder, a binary classifier, and an anomaly generation mechanism. This paper conducts experiments on five datasets to validate performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThis paper proposes a new pipeline for continual supervised anomaly detection that aligns well with the practical application needs.\n\u2022\t The performance gains for AUC on five datasets look good, especially on the FMNIST and MNIST datasets."
                },
                "weaknesses": {
                    "value": "\u2022\tThe novelty of the proposed framework is limited. The overall network architecture consists of a VAE and a classifier without any particularly unique components. \n\n\u2022\tThis paper does not include a comparison with some of the latest supervised anomaly detection methods such as DRA[1], PRN[2], BGAD[3], which might be relevant for a more comprehensive evaluation.\n\n[1] Ding C, Pang G, Shen C. Catching both gray and black swans: Open-set supervised anomaly detection. CVPR 2022.\n\n[2] Zhang H, Wu Z, Wang Z, et al. Prototypical residual networks for anomaly detection and localization. CVPR 2023.\n\n[3] Yao X, Li R, Zhang J, et al. Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection. CVPR, 2023."
                },
                "questions": {
                    "value": "Is there a more detailed explanation regarding the impact of the number of seen anomalous samples on the experimental results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938203492,
            "cdate": 1698938203492,
            "tmdate": 1699636516649,
            "mdate": 1699636516649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uoTxoLx6Pg",
                "forum": "uizIvVBY8P",
                "replyto": "P9bOOzztEE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QxJr"
                    },
                    "comment": {
                        "value": "We thank you for reading our paper and your review comments.\n- The novelty of the proposed framework is limited.\n\nAs you mentioned, our method consists of a binary classifier and a VAE, which are very simple and basic modules. However, this does not mean that the novelty of the proposed method is limited. In fact, it is not obvious how to combine VAE with a binary classifier and how to perform generative replay from them. Our method's novelty is not in using VAEs or binary classifiers but in finding a way to use them to solve the problem of continual supervised anomaly detection. For example, in Eqs. (7)-(9), the formulation of generating anomalies is naturally derived by appropriately combining VAE and a binary classifier. In addition, its simplicity has the advantage that it can be easily extended if necessary (e.g., by changing the generative model or the classifier).\n\n- This paper does not include a comparison with some of the latest supervised anomaly detection methods such as DRA[1], PRN[2], BGAD[3], which might be relevant for a more comprehensive evaluation.\n\nThank you for your comment. For a more comprehensive evaluation, we have conducted the experiments with BGAD[3]. The following are the results.\n\n|                       | FMNIST          | MNIST           | UNSW            | bank            | credit         |\n|:----------------------|:----------------|:----------------|:----------------|:----------------|:---------------|\n| BGAD, A-GEM     | 78.01$\\pm$4.98  | 77.85$\\pm$6.15  | 96.52$\\pm$2.22  | **84.41$\\pm$2.05** | **95.52$\\pm$0.89** |\n| BGAD, EWC       | 69.98$\\pm$11.63 | 70.91$\\pm$8.40  | 95.83$\\pm$1.75  | **86.40$\\pm$1.80** | **95.66$\\pm$0.97** |\n| BGAD, LB        | 65.91$\\pm$7.16  | 63.85$\\pm$7.76  | 91.33$\\pm$6.33  | 72.74$\\pm$4.00 | 94.39$\\pm$1.93 |\n| Ours        | **88.23$\\pm$2.56**  | **95.46$\\pm$1.64**  | **98.15$\\pm$2.21**  | **86.75$\\pm$2.93** | **96.12$\\pm$1.22** |\n\nAlthough the methods you mentioned [1-3] rely on the trained backbone model to extract the feature maps from the inputs, it was removed in the experiment above because it does not fit our paper's problem settings. This is because one of the goals of this paper is how to continually train models from scratch using multiple tasks consisting of a small number of data, and the availability of models trained on large datasets in advance, such as backbone models, is inconsistent with this goal. The hyperparameters for BGAD basically followed its paper and GitHub implementation. We have added the details of the experiments to our manuscript. From the above results, we can see that our method outperforms the latest supervised anomaly detection methods.\n\n- Is there a more detailed explanation regarding the impact of the number of seen anomalous samples on the experimental results?\n\nThank you for your question. To investigate the impact of the number of seen anomalous instances to be used for training, we conducted some experiments when the number of seen anomalous instances available for training was reduced to 1/5. The following are the results.\n\n|                       | FMNIST          | MNIST           | UNSW            | bank           | credit          |\n|:----------------------|:----------------|:----------------|:----------------|:---------------|:----------------|\n| BC, A-GEM       | 66.28$\\pm$12.41 | 64.92$\\pm$7.39  | **96.69$\\pm$0.53**  | 79.51$\\pm$3.02 | 89.70$\\pm$1.84  |\n| BC, EWC         | 63.48$\\pm$10.12 | 64.25$\\pm$8.78  | **90.11$\\pm$10.01** | **80.92$\\pm$2.14** | 89.70$\\pm$1.84  |\n| Deep SAD, A-GEM | 67.79$\\pm$2.70  | 68.81$\\pm$1.83  | 93.89$\\pm$2.27  | 73.75$\\pm$4.27 | 93.11$\\pm$1.36  |\n| Deep SAD, EWC   | 61.97$\\pm$6.63  | 63.43$\\pm$5.97  | 93.10$\\pm$2.03  | 75.26$\\pm$5.39 | 91.69$\\pm$2.80  |\n| DevNet, A-GEM   | 50.80$\\pm$8.44  | 46.67$\\pm$9.42  | 91.30$\\pm$4.14  | 67.33$\\pm$6.52 | 63.30$\\pm$21.54 |\n| DevNet, EWC     | 59.05$\\pm$9.61  | 56.51$\\pm$7.51  | 92.88$\\pm$2.15  | 67.67$\\pm$3.13 | 68.53$\\pm$29.25 |\n| BGAD, A-GEM     | 75.23$\\pm$7.29  | 68.04$\\pm$8.18  | **94.81$\\pm$3.42**  | 76.90$\\pm$1.64 | **95.58$\\pm$0.44**  |\n| BGAD, EWC       | 68.81$\\pm$7.54  | 61.33$\\pm$8.54  | 93.70$\\pm$3.58  | 79.00$\\pm$1.62 | **95.51$\\pm$0.52**  |\n| Ours           | **83.61$\\pm$3.79**  | **89.37$\\pm$6.09**  | **96.56$\\pm$0.83**  | **82.36$\\pm$0.97** | 94.75$\\pm$0.69  |\n\nWe show only the results for the supervised anomaly detection methods because the performances of the unsupervised anomaly detection methods (i.e., AE and VAE) do not depend on it. The complete table has been added to the manuscript. Note that in the experiment, only in the credit dataset, the performance of the proposed method is not at the top, but the difference between the performances of the top methods and ours is still small enough. The table shows that our method shows stable performance regardless of the number of anomalous instances for training."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283606799,
                "cdate": 1700283606799,
                "tmdate": 1700283606799,
                "mdate": 1700283606799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2yzxWdC9Xu",
                "forum": "uizIvVBY8P",
                "replyto": "uoTxoLx6Pg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_QxJr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_QxJr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their responses and additional experiments, which addressed some of my concerns. However, I still think the novelty of the overall architecture is limited and the improvement on tabular datasets seems limited. And, since significant gains are mainly observed on image datasets, I believe that validating this approach on complex and large image datasets is a necessity rather than an alternative.   Based on these considerations, I maintain my review score unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642810374,
                "cdate": 1700642810374,
                "tmdate": 1700642810374,
                "mdate": 1700642810374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "h95VJyKy6g",
            "forum": "uizIvVBY8P",
            "replyto": "uizIvVBY8P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for semi-supervised anomaly detection in the setting of continuous learning. They combine a binary classifier for the labeled anomalies together with a VAE reconstruction score for the anomaly detection part. The continuous learning is addressed by, one, using the latent space of the VAE to sample data from past tasks, and, two, using the gradient of the binary classifier to sample labeled anomalies in the same latent space via iteration from a starting point."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easily readable.\nThey perform experiments on various types of datasets. \nThey perform an ablation study."
                },
                "weaknesses": {
                    "value": "It represents a straightforward combination of ideas.\nThe argument that one cannot keep data due to privacy reasons also applies for resampling data from an autoencoder. If it is a very good reconstruction, it would equally cause privacy issues.\nThe important and relevant case of slow distribution shift is only partially addressed, via the credit data. Doing so in more and more controllable settings would be of interest.\nA comparison against reusing past training data would be of interest - in particular from tasks a few epochs ago. Reason being that a slow shift of parameters would also affect sampling of data from not so recent past tasks.\nMNIST and FMNIST might be too simple as problem. A more complex image dataset is missing."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5199/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699131500885,
            "cdate": 1699131500885,
            "tmdate": 1699636516564,
            "mdate": 1699636516564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "usfQNZoLx5",
                "forum": "uizIvVBY8P",
                "replyto": "h95VJyKy6g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer vozw"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your elaborate reading of our paper and your insightful comments.\n\n- It represents a straightforward combination of ideas. \n\nAs you mentioned, our method looks very simple, but it does not mean the lack of novelty of our method. For example, the formulation for generating anomaly instances from VAEs is not a mere heuristic, but is derived naturally from the structure of our model. This was obtained by appropriately combining VAEs and binary classifiers to create our model. In fact, our method outperforms the conventional methods.\n\n- The argument that one cannot keep data due to privacy reasons also applies for resampling data from an autoencoder. If it is a very good reconstruction, it would equally cause privacy issues. \n\nYes, the generator could cause privacy issues if the generated instances are of high quality. However, the risk should be reduced compared to keeping the data directly. For example, the webpage of SyntheticData4ML workshop says that \"Synthetic data is regarded as one potential way to promote privacy. The 2019 NeurIPS Competition \"Synthetic data hide and seek challenge\" demonstrates the difficulty in performing privacy attacks on synthetic data.\" Further investigation into this issue is planned for future work.\nhttps://www.syntheticdata4ml.vanderschaar-lab.com/\n\n- The important and relevant case of slow distribution shift is only partially addressed, via the credit data. Doing so in more and more controllable settings would be of interest.\n \nMany previous studies on continual learning focused on mitigating catastrophic forgetting, and thus, they are more interested in quickly changing data distributions than in slowly changing ones. This is because catastrophic forgetting is unlikely to occur when the shift in distribution is small. However, as you mentioned, it is important to understand how well the proposed method works in a problem setting where the distribution changes gradually because such situations can occur in real-world applications. In this paper, we investigate such a situation on a credit dataset and confirm that our method is superior to the others. We believe that the superiority of the proposed method will probably remain unchanged in such situations where the distribution change is small.\n \n- A comparison against reusing past training data would be of interest - in particular from tasks a few epochs ago. Reason being that a slow shift of parameters would also affect sampling of data from not so recent past tasks. \n\nThank you for your suggestion. The situation in which data from previous tasks can be used differs from the problem settings in this paper and the usual continual learning studies, making the problem much easier. Such a problem setting would be interesting, but may be outside the scope of this paper.\n \n- MNIST and FMNIST might be too simple as problem. A more complex image dataset is missing.\n\nPlease note that our proposed method is not domain-specific, although its application to more challenging image datasets is important. Our approach can be applied to non-image datasets such as tabular data. We have included experimental results on MNIST and FMNIST to confirm that our proposed method can also be used in the image domain, and we believe that they played a sufficient role in this regard. Of course, it would also be possible to use the proposed method on difficult image datasets by using powerful generative models (VQ-VAE, GAN, diffusion models, etc.) instead of VAE. Such considerations are the subject of future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283445415,
                "cdate": 1700283445415,
                "tmdate": 1700283445415,
                "mdate": 1700283445415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FoI8gL8XjL",
                "forum": "uizIvVBY8P",
                "replyto": "GeCSPwcHMb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
                ],
                "content": {
                    "title": {
                        "value": "Comments not well adressed."
                    },
                    "comment": {
                        "value": "The study is more empiricial than theoretical. \n\nThe message of the comments in \n\nThe argument that one cannot keep data due to privacy reasons also applies for resampling data from an autoencoder. If it is a very good reconstruction, it would equally cause privacy issues.\n\nThe important and relevant case of slow distribution shift is only partially addressed, via the credit data. Doing so in more and more controllable settings would be of interest.\n\nA comparison against reusing past training data would be of interest - in particular from tasks a few epochs ago. Reason being that a slow shift of parameters would also affect sampling of data from not so recent past tasks.\n\nis that from a practical viewpoint it would be useful to evaluate it, even more so as the paper is focused on empirical results.\nAfter deployment a slow shift is more likely than a strong task shift ( exceptions being maybe in cyber attacks).\n\nThat other studies have not done it, is not a convincing argument. \n\n\nPlease note that our proposed method is not domain-specific, although its application to more challenging image datasets is important. Our approach can be applied to non-image datasets such as tabular data. We have included experimental results on MNIST and FMNIST to confirm that our proposed method can also be used in the image domain, and we believe that they played a sufficient role in this regard. \n\nThat line of thought ignores the problem, that on more realistic datasets, methods might behave very differently in comparison.\n\nThe reviewer feels that the comments are not satisfactorily adressed and keeps the current score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709430599,
                "cdate": 1700709430599,
                "tmdate": 1700709430599,
                "mdate": 1700709430599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8G41NnCJny",
                "forum": "uizIvVBY8P",
                "replyto": "R8RtywN4Z9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5199/Reviewer_vozw"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer recognizes this result. However with 32x32 one has the same limited spatial complexity as for mnist/cifar-10/100 . with 32x32 the complexity of it is largely gone."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5199/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735730725,
                "cdate": 1700735730725,
                "tmdate": 1700735730725,
                "mdate": 1700735730725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]