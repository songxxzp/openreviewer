[
    {
        "title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding"
    },
    {
        "review": {
            "id": "yVbsNJyEHN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
            ],
            "forum": "ORUiqcLpV6",
            "replyto": "ORUiqcLpV6",
            "content": {
                "summary": {
                    "value": "The paper proposes an interpretable 3D visual grounding framework based on Chain-of-Thought in large language models, i.e., CoT3DReF.\nIt is a sequence-to-sequence (Seq2Seq) model by first predicting a chain of anchors and then the final target.\nTo provide supervision on the intermediate chaining, the paper devises a pseudo-label generator.\nThe proposed framework achieves state-of-the-art performance and shows its data efficiency on several 3D visual grounding benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper integrate the idea of Chain-of-Thoughts into the 3D groudning tasks, and propose a 3D data-efficient and interpretable  framework CoT3DRef.\n2. The technical contribution is sound, including Seq-2-Seq formulation and pseudo labels generation.\n3. The proposed framework achieves state-of-the-art performance on 3D visual grounding tasks and shows its data efficiency."
                },
                "weaknesses": {
                    "value": "In general, I am in favor of the paper, which makes an interesting first attempt toward CoT 3D grounding tasks. However, there are still some important while unexplored problems.\n\n1. To make CoT3DRef self-contained and scalable, it is important to generate accurate pesudo labels. Although it has been discussed in Sec.5, I have some remaining questions/concerns:\n- In Alg.1, FIND function maps a localized object, a relation and a set of candidate objects to an output localized object. What is the detailed process of FIND? Is the relations between objects known?\n- Alg.1 receive the input of object proposals P. Is it the output from the 3D object detection model which requires extra training? If so, the anchor localization may be bottlenecked by the object proposals.\n- Considering the elaborate process of psuedo label generation, the propose frameworks seems not \"data efficient\" in data preparation, though it is data efficient in 3D grounding training.\n\n2. The interpretability is limited. From the input end, GPT-3.5 is used for extract the logical order of objects given an input utterance, thus adding interpretability. But the logical relations of the objects is not explicit modeled. Instead, it is implicit learned in the Seq-2-Seq network."
                },
                "questions": {
                    "value": "See \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697355288701,
            "cdate": 1697355288701,
            "tmdate": 1699636736659,
            "mdate": 1699636736659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j4gDCSUU0F",
                "forum": "ORUiqcLpV6",
                "replyto": "yVbsNJyEHN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable and thoughtful feedback. \nWe are encouraged that you find our paper novel, effective, and reasonable and that our experiments are comprehensive with state-of-the-art results.\nBelow, we will address your concern and incorporate all the feedback.\n\n\n> 1: In Alg.1, FIND function maps a localized object, a relation and a set of candidate objects to an output localized object. What is the detailed process of FIND? Is the relations between objects known?\n\nThe relations between objects are not known. Thus, we have to extract them.\nFirst, we extract the entire mentioned objects and their relations from the utterance using the scene graph parser [[1]](https://aclanthology.org/W15-2812.pdf), [[2]](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf).\n\nOur method involves iterating over all anchors extracted from the utterance and searching for candidate objects in P that share the same class. \nWhen the anchor class is represented singularly in the scene, we return the matched object. \nHowever, we run the FIND function in scenarios where disambiguation is required due to multiple objects belonging to the anchor class.\nThe FIND function leverages the parsed spatial relations and the localized objects, so far, within the scene to identify the remaining anchors accurately.\nHowever, it is not guaranteed that the FIND function will be able to localize the remaining unlocalized anchors accurately.\nThus, in this case, as shown in the last step in Algorithm 1, we randomly sample an object of the same class.\n\n> 2: Alg.1 receive the input of object proposals P. Is it the output from the 3D object detection model? If so, the anchor localization may be bottlenecked by the object proposals.\n\nThe prominent role of the localization module is to assign an object proposal for each anchor mentioned in the input description without requiring human effort. \nA pertinent example can be drawn from the labeling procedure employed in PhraseRefer, which demanded a cumulative workforce commitment of 3664 hours.\n\nThus, we use the GT object proposals P from the ScanNet dataset, following the related works, e.g., [ReferIt3D](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf).\n\nHence, given a new dataset, our algorithm requires two pieces of information: bounding boxes (GT or predicted) and at least one localized object.\nThen, our algorithm works iteratively, starting from prior knowledge, i.e., at least one localized anchor/target, to localize the rest of the mentioned anchors in the utterance given the previously localized ones without the need for any human efforts.\n\nTo show the influence of this module, we report the results for two variants for each baseline, one using our pseudo-noisy labels (Algorithm 1), which is human-free, and the second one using the anchors GT annotations from \u201cPhraseRefer.\u201d\nSignificance performance gain can be shown for the MVT case in Table 4, where using our pseudo labels, we outperform \u201cPhraseRefer\u201d and \u201cScanEnts,\u201d which utilize GT anchors annotations.\nFurthermore, using their manual annotations instead of our pseudo labels, we achieve SOTA results and outperform them significantly.\nThis emphasizes the importance of our proposed approach.\n\n> 3: Considering the elaborate process of pseudo label generation, the propose frameworks seems not \"data efficient\" in data preparation, though it is data efficient in 3D grounding training.\n\nWe believe our answers to the previous two questions should answer your concerns and alleviate any misunderstanding about how our pseudo-label generator works.\n\nIf any more clarifications are needed, please let us know; we would love to elaborate more.\n\n> 4: The interpretability is limited. From the input end, GPT-3.5 is used for extract the logical order of objects given an input utterance, thus adding interpretability. But the logical relations of the objects is not explicit modeled. Instead, it is implicit learned in the Seq-2-Seq network.\n\nWe argue that modeling the relations implicitly is enough, as to predict the logical order of objects correctly is a much more complex problem.\nTo solve it, the model has to extract the relations correctly first.\nTo probe our claim, we conducted an experiment where we added a new training objective for the language head.\nInstead of only predicting the target of the anchors from the input utterance, we add another loss on predicting the relationship between the objects as pairs and use [bipartite loss](https://arxiv.org/abs/2005.12872).\nHowever, as shown in the following Table, there is no gain from learning the relations explicitly.\n\n| **Method** | **Grounding Accuracy** |\n|----------|------------------------|\n| **MVT**                    | 55.1 |\n| **MVT+CoT**                | **64.4** |\n| **MVT+CoT+Realtions Objective**                | **64.2** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603075506,
                "cdate": 1700603075506,
                "tmdate": 1700606628271,
                "mdate": 1700606628271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4CBCiy2uq6",
            "forum": "ORUiqcLpV6",
            "replyto": "ORUiqcLpV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to model the referring task into a seq2seq task. The authors introduce two new modules: \u201ctarget anchors head\u201d and \u201cpathway head\u201d.  Basically, the main idea behind is to force the model not only to predict the target object but also pay attention to the rest object mentioned in the utterance. The \u201cpathway head\u201d forces the model to learn the logical order information that indicates how the mentioned target object is identified. However this module requires extra annotation, which they obtain from prompting the Language Language Models (LLMs). Benefit from extra clues, they improve the grounding performance on Various grounding datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n1. This paper introduces not only just predict the target object, but also to force the model to predict all mentioned objects (anchors) to best leverage all available clues. Which provides more information for models to use. \n2. The proposed pathway generation method leverages the ability of powerful LLMs (GPT), freeing the burden of human annotators and making the system easy to scale up.\n3. The proposed pipeline not only improve the performance compare with previous SOTA, but also has higher data efficiently when training data is limited. This is valuable since 3D visual language data is not always easy to obtain."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n1. The proposed idea which pays attention to not just the target object, has been proposed before in \u201cPhraseRefer\u201d. They use human annotators to obtain fine grained annotation. Which share the same underlying idea with this paper. \nTo me one of the major novelties of this paper is that they explicitly model the logical order while the \u201cPhaseRefer '' uses implicit manner. I think these two works are highly related, but the author didn\u2019t give enough introduction of this \u201cPhraseRefer\u201d, which makes it a bit difficult to understand the performance comparison e.g table 4. SAT with PhraseRefer/ScanEnts and yours. Besides, I believe some missing number in this table.4 e.g. MVT with PhaseRefer.\nThe authors should give more clear comparison with this line of works that use fine grain annotation.\n\n2. Do all numbers in Table 4 use a GPT-generated path? If not, answer my below question:\nIf you have access to the GT anchor label either from \u201cPhaseRefer\u201d or \u201cScanEnts\u201d, but you still require GPT to give you extra information \u201cthe Path\u201d, how is the performance going in this case?\n\n\n3. In Sec.3.3 Anchors parser \u201cThen, we match the objects to their closest matching class from the ScanNet labels\u201d, Would you explain this in more detail? Correct me if I am wrong: I assume if you assign a more fine grain label set , like ScanNet200, you are actually providing richer semantic knowledge to the model. My question is how do you assign the label, do you use the same level of semantic information with the others?\n\n4. From the numbers of MVT in Table.4 and Table.5, it seems that when MVT baseline has access to more data, the performance can improve quite a lot, So it is possible that your proposed method introduce explicit knowledge to the model while MVT can has a chain to learn that out from the data if the dataset is large enough?"
                },
                "questions": {
                    "value": "I have listed my question above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698470210917,
            "cdate": 1698470210917,
            "tmdate": 1699636736526,
            "mdate": 1699636736526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d2Spen0lLM",
                "forum": "ORUiqcLpV6",
                "replyto": "4CBCiy2uq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable and thoughtful feedback. \nWe are encouraged that you find our paper novel, effective, and reasonable and that our experiments are comprehensive with state-of-the-art results.\nBelow, we will address your concern and incorporate all the feedback.\n\n> 1: The authors should give more clear comparison with this line of works that use fine grain annotation, i.e., \u201cPhraseRefer\u201d and \u201cScanEnts\u201d.\n\nIndeed, contrasting our work with the two recent works, \u201cPhraseRefer\u201d and \u201cScanEnts,\u201d is essential, and we incorporated this response into the discussion section to clearly position our contributions compared to these works.\n\nWe acknowledge the great work proposed by \u201cPhraseRefer\u201d and \u201cScanEnts,\u201d which paved the way for the importance of paying attention not just to the target object but also to the anchors.\nThus, underlying approaches have some similarities in terms of demonstrating the importance of including anchors in the pipeline.\nHowever, there are a lot of significant differences:\n1) Design a CoT framework for 3D visual grounding that explicitly models causal reasoning while interpreting the instruction inspired by the human perception system.\n2) Show that we can design an efficient framework that achieves state-of-the-art results on three challenging benchmarks, i.e., Nr3D, Sr3D, and ScanRefer, without requiring human labels. Proper design of such an approach is pivotal in attaining a good performance  while circumventing the need for intensive human annotations.\nA pertinent example can be drawn from the labeling procedure employed in PhraseRefer, which demanded a cumulative workforce commitment of 3664 hours, roughly equivalent to an extensive five-month timespan.\n\nThus, as shown in Table 4, we integrated our CoT architecture into four different baselines, i.e., LAR, SAT, MVT, and ViL.\nOur framework shows a consistent gain across different baselines.\nMost importantly, we report the results for two framework variants for each baseline, one using our pseudo-noisy labels (human-free) and the second using the anchors GT annotations from \u201cPhraseRefer\u201d.\nSignificance performance gain can be shown for the MVT case in the Table below, where using our pseudo labels, we outperform \u201cPhraseRefer\u201d and \u201cScanEnts,\u201d which utilize GT anchors annotations.\nFurthermore, when we use their manual annotations instead of our pseudo labels, we achieve SOTA results and outperform them significantly.\nThis emphasizes the importance of our proposed CoT framework.\n\n| **Method** | **Grounding Accuracy** |\n|----------|------------------------|\n| **MVT**                    | 55.1 |\n| **MVT+ScanEnts**           | 59.3 |\n| **MVT+PhraseRefer**           | 59.0 |\n| **MVT+CoT(Pseudo labels)**                | 60.4 |\n| **MVT+CoT(GT labels)**                | **64.4** |\n\nWe hope this comparison clarifies the differences and poses = work clearly within the literature, specifically \u201cPhraseRefer\u201d and \u201cScanEnts\u201d.\n\n> 2: I believe some missing number in this TableTable.4 e.g. MVT with PhaseRefer.\n\nThank you so much for pointing out this point.\nWe missed this number as it was added in the second revised version of the PhaseRefer paper.\nWe incorporated this in the revised version in Table 4 and Table 6."
                    },
                    "title": {
                        "value": "Answering Points #1 and #2"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602437066,
                "cdate": 1700602437066,
                "tmdate": 1700603134643,
                "mdate": 1700603134643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W1pTcNjLWh",
                "forum": "ORUiqcLpV6",
                "replyto": "4CBCiy2uq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answering Points #3, #4 and #5"
                    },
                    "comment": {
                        "value": "> 3: Do all numbers in Table 4 use a GPT-generated path? If not, then, If you have access to the GT anchor label either from \u201cPhaseRefer\u201d or \u201cScanEnts\u201d, but you still require GPT to give you extra information \u201cthe Path\u201d, how is the performance going in this case?\n\nWe do not use the GPT-generated path during inference or test time.\n\nThe GPT-generated path is only used as a teacher label for training a simple MLP that predicts the path during the inference time.\nTherefore, GPT is not involved in the system during inference, as we train a simple MLP layer, i.e., the Pathway-Head in Figure 3, to predict the logical order.\nFor instance, when the system is deployed on a robot, it will be efficient as our simple MLP Pathway-Head is utilized to generate a reasonable order based on the input description.\n\nAccordingly, GPT could be seen as a design choice to obviate the need for human annotations during training.\n\nIn the Table below, the Pseudo and GT labels are for the anchors' localization, not the path.\nFor the path, we always use our MLP-predicted path.\nHowever, the accuracy will not be affected if we replace it with the GPT-generated path.\nAs shown in the Table below, we achieve 64.4% and 64.5% using our predicted and GPT-generated paths, respectively.\nThis is an intuitive result, as when we assess the performance of the Pathway Head separately, we get 98% accuracy w.r.t the GPT-generated path.\nTherefore, replacing the Pathway Head generated path by the GPT ones is expected not to impact the final performance.\n\n| **Method** | **Grounding Accuracy** |\n|----------|------------------------|\n| **MVT**                    | 55.1 |\n| **MVT+ScanEnts**           | 59.3 |\n| **MVT+PhraseRefer**           | 59.0 |\n| **MVT+CoT(Pseudo labels)**                | 60.4 |\n| **MVT+CoT(GT labels)**                | 64.4 |\n| **MVT+CoT(GT labels+GPT Path)**                | 64.5 |\n\nThis experiment is added in the revised version in the Appendix section.\n\n\n> 4: In Sec.3.3 Anchors parser \u201cThen, we match the objects to their closest matching class from the ScanNet labels\u201d, Would you explain this in more detail? \n\nDue to the free-from nature of Nr3D, the anchors mentioned in the GT descriptions sometimes do not precisely match the ScanNet class labels.\nFor instance, the GT description is \u201cThe plant at the far right-hand side of the bookcase tucked in the furthest corner of the desk.\u201d However, there is no \u201cbookcase\u201d class in ScanNet.\n\nTherefore, we need to match it to the nearest ScanNet class labels, which in this case will be \u201cbookshelf.\u201d\nThus, we first check whether the extracted anchor from the text matches one of the ScanNet class labels. \nIf not, then we encode the anchor words and the complete list of the ScanNet class labels.\nThen, we measure the cosine similarity on the feature space to get the closest one.\nConsidering that the anchor's name may contain several words, e.g., office chair or file cabinet, we can not use BERT encodings.\nInstead, we leverage its extended version, capable of encoding sentence level instead of word level, termed [SBERT](https://arxiv.org/abs/1908.10084).\n\n> 5: My question is how do you assign the label, do you use the same level of semantic information with the others?\n\nYes, we use the same semantic information level as the other work for a fair comparison.\nHowever, it is straightforward to use ScanNet200 instead for richer semantics, but then we have to retrain all the existing work on ScanNet200 for fair comparison.\nThat's why, to avoid this and for a fair comparison, we adhere to the well-established setup by other related work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602682115,
                "cdate": 1700602682115,
                "tmdate": 1700603114738,
                "mdate": 1700603114738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hZzYZo2A4Y",
                "forum": "ORUiqcLpV6",
                "replyto": "4CBCiy2uq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answering Point # 6"
                    },
                    "comment": {
                        "value": "> 6: From the numbers of MVT in Table.4 and Table.5, it seems that when MVT baseline has access to more data, the performance can improve quite a lot.\nSo it is possible that your proposed method introduce explicit knowledge to the model while MVT can has a chain to learn that out from the data if the dataset is large enough?\n\nTo study the effect of adding more data on the performance of our method compared to MVT, we will explore three case studies:\n\n1) **Nr3D+Sr3D results**\nFrom Table 4 and Table 5, when we use more data by training on Sr3D and Nr3D jointly instead of Nr3D alone, the MVT performance increased by 3.4% (from 55.1 to 58.5). \nOn the other hand, our CoT approach performance increases by 2% (from 60.4 to 62.5).\nWhen more data is used, this reduction in the performance gain could be considered a minor impact because the gap is only reduced by 1% despite increasing the training data 3 times.\nWherease the Nr3D dataset consists of 41.5K samples, while the Sr3D dataset contains 83.5K samples.\n\n2) **ScanRefer results**\nAs shown in Table 6, our method introduces a consistent gain. \nFor instance, using 70% of the data, we outperform MVT by 8%, where the grounding accuracies are 62.5% and 54.5% for our method and MVT, respectively.\nMoreover, when more data is used, where 100% of the data is used instead of 70%, we still outperform MVT by 7%.\n\n3) **Nr3D and Sr3D results**\nAs shown in Figure 2, our CoT framework introduces a consistent gain across different amounts of training data (10% - 100%) on four different baselines.\n\nIn conclusion, the aforementioned case studies show that adding more data enhances both MVT and our method equally, which implies that our CoT approach introduces a significant gain that can not be replaced by just scaling the data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602848260,
                "cdate": 1700602848260,
                "tmdate": 1700603122739,
                "mdate": 1700603122739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0whkAe7oLB",
                "forum": "ORUiqcLpV6",
                "replyto": "4CBCiy2uq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "Thanks for your response and effort. All my concerns are addressed. \n\nI would like to increase my rating from a 6 (borderline accept) to a 7 (weak accept), but unfortunately, the system does not provide this option this year. As the paper does not meet my criteria for an 8, I will maintain my rating at 6. However, I want to emphasize that my preferred rating would be a **7**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624045808,
                "cdate": 1700624045808,
                "tmdate": 1700624045808,
                "mdate": 1700624045808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UBGBjrYrJx",
            "forum": "ORUiqcLpV6",
            "replyto": "ORUiqcLpV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents CoT3DRef, a novel and interpretable 3D visual grounding framework that aims to mimic the human perception system. The authors formulate the 3D visual grounding problem as a sequence-to-sequence task, predicting a chain of anchors leading up to the final target, which is a significant deviation from existing methods that directly localize the referred object. This approach not only improves the overall performance but also enhances interpretability, helping to identify and address failure cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Technical Novelty.** The chain-of-thoughts (CoT) approach is an innovative way of addressing the 3D visual grounding problem, providing a sequence of interpretable intermediate steps that lead to the final prediction. This approach is aligned with how humans might approach the task, thereby making the model\u2019s predictions more understandable and transparent.\n\n2. **Data Efficiency.** The proposed framework demonstrates significant data efficiency, especially highlighted by its performance on the Sr3D dataset where it matches state-of-the-art (SOTA) performance with just 10% of the training data. This is a crucial advantage, particularly in domains where acquiring labeled data is expensive and time-consuming.\n\n3. **Applicability.** The authors show that CoT3DRef can be easily integrated into various existing architectures, demonstrating its versatility and applicability. This is substantiated by the comprehensive experiments conducted across different benchmarks and architectures, consistently showing performance gains.\n\n4. **Comprehensive Experiments.** The paper includes extensive experiments and ablation studies, providing a thorough evaluation of the proposed method. The results on Nr3D, Sr3D, and ScanRefer benchmarks are impressive, showcasing the framework's effectiveness and robustness."
                },
                "weaknesses": {
                    "value": "1. **Detailed Failure Case Analysis.** While the paper mentions the identification of failure cases as a benefit of the interpretability, it does not provide a detailed analysis or examples of these cases. Including such an analysis could provide valuable insights into the limitations of the model and areas for future improvement.\n\n2. **Broader Impact and Ethical Considerations.** The paper could discuss the broader impacts and potential ethical considerations of deploying such a system, especially in the highlighted application areas like autonomous driving and robotics."
                },
                "questions": {
                    "value": "Can you provide a detailed analysis or examples of the identification of failure cases as a benefit of the interpretability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836409873,
            "cdate": 1698836409873,
            "tmdate": 1699636736411,
            "mdate": 1699636736411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TWyFEHopPv",
                "forum": "ORUiqcLpV6",
                "replyto": "UBGBjrYrJx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your valuable and thoughtful feedback. \nWe are encouraged that you find our paper novel, effective, and reasonable and that our experiments are comprehensive with state-of-the-art results.\nBelow, we will address your concern and incorporate all the feedback.\n\n> 1: **Detailed Failure Case Analysis**. Can you provide a detailed analysis or examples of the identification of failure cases as a benefit of the interpretability?\n\nTo demonstrate our interpretability abilities, we visualize attention maps as shown in Figure 5 in the Appendix.\nIn Figure 5, the input description is \u201cthe chair to the far left hand side of the taller desk with the computer monitor on it.\u201d\nAccordingly, there are two anchors mentioned in the description, i.e., desk and monitor, and the target is the chair. The correct chair should be number two, however, the model predicts number four. \nBy visualizing the attention maps, on the left part of Figure 5, we can identify the main cause of the wrong prediction, whereas, the first anchor localize a wrong desk (desk \\#3) instead of desk \\#2.\nTherefore, the rest of the chain, i.e., the monitor and the chair are localized wrongly.\nThis example also shows the ambiguity in Nr3D, where it is hard to say which desk is the taller, desk \\#2 or desk \\#3.\n\n> 2: Broader Impact and Ethical Considerations\n\nThe paramount consideration of social impact has significantly shaped the development of our proposed CoT 3D visual grounding architecture. \nIn Section 5, we had a discussion on the social impact of the technologies that may benefit from our work.\nDue to the importance of the topic as pointed by the reviewer, we have extended it in the Appendix to cover the highlighted applications like autonomous driving and robotics.\n\n3D visual grounding holds profound implications across diverse domains and applications, spanning from integrating outdoor systems, e.g., autonomous driving and indoor navigation systems.\n\n**Outdoor navigation systems:**\n3D visual grounding empowers the agents to execute a broad spectrum of tasks, facilitating assistive systems catering to the needs of elderly individuals and those with disabilities\u2014such as the visually impaired. \nFor instance, in the context of visually impaired individuals, the technology's ability to ground objects within their 3D environment from language descriptions is pivotal in navigation tasks such as determining the nearest exit or chair. \n\n**Autonomous driving:**\nIn the realm of autonomous driving, a 3D visual grounding system takes on a critical role in translating navigational instructions into actionable responses within the dynamic visual landscape. \nThis technology enables seamless communication between the language input, such as navigation commands, and the 3D world the autonomous vehicle operates in. \nFor instance, when given a directive like \"merge into the right lane at the upcoming intersection where the three pedestrians are walking,\" the 3D visual grounding system interprets this command by analyzing the spatial layout, recognizing lanes and pedestrians, and identifying the appropriate merging point. By bridging the gap between language instructions and the complex visual cues inherent in driving scenarios, this system enhances the vehicle's ability to accurately interpret and execute instructions, contributing to safer and more efficient autonomous driving experiences.\n\n**Data-efficiency importance for autonomous driving:**\nIntroducing a data-efficient solution for the 3D visual grounding task holds immense importance in the context of autonomous driving, particularly considering the challenges associated with manually labeling vast amounts of 3D point cloud data from lidar sensors. \nThe conventional approach of manually annotating such data is labor-intensive and economically burdensome. \nBy proposing a data-efficient solution, our system addresses a critical bottleneck in developing autonomous driving technology. \nThis allows the autonomous vehicle to learn and generalize from minimal labeled data, significantly reducing the dependency on large, expensive datasets.\nThe capacity to make accurate 3D visual groundings with limited labeled information not only streamlines the training process but also makes deploying autonomous vehicles more scalable and cost-effective. \n\n**Federated learning for indoor navigation systems:**\nAligned with this imperative, the inherent data-efficient nature of our approach positions it as an ideal candidate for federated learning schemes, which is perfectly aligned with the indoor setup when privacy is required, e.g., the robot is operating in a personalized environment such as the client home.\nThis becomes particularly relevant when a robot is introduced into a novel indoor environment and necessitates rapid learning from minimal interactions with its new user without sending the private data of the new environment to the server; thus, learning from limited data is essential."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602287614,
                "cdate": 1700602287614,
                "tmdate": 1700602287614,
                "mdate": 1700602287614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s7RAp6gowM",
            "forum": "ORUiqcLpV6",
            "replyto": "ORUiqcLpV6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
            ],
            "content": {
                "summary": {
                    "value": "This submission studies 3D object referring. A new architecture is proposed, which predicts several objects sequentially instead of a single object at last. To generate a dataset that makes this new task setting possbile, the authors exploit grammar parser, GPT in-context logical ordering and rule-based box matching. The new architecture boils down to a decoder that matches several different encoder. Experiments show that the method improves several baselines and notably work quite well under a data efficient setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ A new decoder head for sequential object grounding in a scene. This is a new paradigm that may inspired related fields.\n+ A data curation effort that makes sense to me. This makes the CoT training possible.\n+ Quantitative results showing positive margins on four baselines and notably good performance in a data-efficient setting."
                },
                "weaknesses": {
                    "value": "I like the paper and don't see major weaknesses. Why I am not rating higher is the fact that the new thing comes from foundation models, which is in fact a system-level contribution instead of a principled contribution. But I feel it fine and timely to accept this in ICLR 2024."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698850698150,
            "cdate": 1698850698150,
            "tmdate": 1699636736301,
            "mdate": 1699636736301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dy6GPYc3wo",
                "forum": "ORUiqcLpV6",
                "replyto": "s7RAp6gowM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Authors"
                ],
                "content": {
                    "title": {
                        "value": "GPT is used only during preparing Pseudo GT labels to train the logical order module"
                    },
                    "comment": {
                        "value": "We thank you for your valuable and thoughtful feedback. \nWe are encouraged that you like our paper and find our approach effective and reasonable and our experiments comprehensive with state-of-the-art results.\nBelow, we will address your concern and incorporate all the feedback.\n\n> 1: I like the paper and don't see major weaknesses. Why I am not rating higher is the fact that the new thing comes from foundation models, which is in fact a system-level contribution instead of a principled contribution.\n\nOur main contribution in this paper is demonstrating that it is possible to design a CoT framework for 3D visual grounding efficiently without human labels. It also shows that it can be integrated with multiple existing methods, showing significant gains, especially when data is limited.\n\n\nTherefore, the foundation model, i.e., GPT, is used only for the logical order ground-truth preparation, used during the training.\nSpecifically, it is not involved in the system during inference, as we train a simple MLP layer, i.e., the Pathway-Head in Figure 3, to predict the logical order.\nThus, for instance, when the system is deployed on a robot, it will be efficient as a simple MLP is utilized to generate a reasonable order based on the input description.\n\nAccordingly, the foundation model could be seen as a design choice to obviate the need for human annotations, to guide the training process. \nNo foundation models are loaded or used at test time, making the model inference-time efficient."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602093476,
                "cdate": 1700602093476,
                "tmdate": 1700603682815,
                "mdate": 1700603682815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VVTEtPsvr4",
                "forum": "ORUiqcLpV6",
                "replyto": "Dy6GPYc3wo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
                ],
                "content": {
                    "title": {
                        "value": "Feedback"
                    },
                    "comment": {
                        "value": "Yes, I understand that GPT is only used during training but not testing."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6539/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645628504,
                "cdate": 1700645628504,
                "tmdate": 1700645628504,
                "mdate": 1700645628504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]