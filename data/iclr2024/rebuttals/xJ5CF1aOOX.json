[
    {
        "title": "A Self-Supervised Pre-Training Model for Time Series Classification based on Data Pre-Processing"
    },
    {
        "review": {
            "id": "p8cDnCQfm4",
            "forum": "xJ5CF1aOOX",
            "replyto": "xJ5CF1aOOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_GpZ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_GpZ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the task of pre-training time series classifiers.\nIt relies on (Zhang et al. 2022) and modifies this approach in two ways: (i) a pre-processing step is proposed to filter out uninformative parts from the time series and (ii) the similarity computation used in the NT-Xent loss is replaced by a similarity measure that relies on sorted version of the time series."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem that is tackled is an important one."
                },
                "weaknesses": {
                    "value": "I can see two main weaknesses for this paper:\n\n1. The presentation of the paper is so unclear that one cannot fully grasp the main take-away message from the paper.\n\n* Introduction is close to non-informative\n* In the Related Work section, competitors are described at such a high level that one cannot understand their limitations\n* Even the notations in Section 3 are confusing\n    * The text discusses multivariate time series dataset (each dataset comes with a number of channels) and then, suddenly: \"Data from multiple channels need to be analyzed together, we focus only on data from a single channel\", yet, in the experiments, some datasets are multivariate...\n    * Problem definition is unclear, since the same function $f$ is used to map a pre-training sample to the pre-trained model $M_{pre}$ and a \"fine-tune sample\" (with the same index...) to the final model $M_{tune}$\n* When it comes to the specific novelty of the paper:\n    * Algorithms 1 and 2 lack rigor (ex: in Algo1, is $i$ the index of a time series (as it seems to be in $x_i$) or a time instant (as it seems to be in the slicing $i:i+winsize$)?)\n    * NT-Xent description is not clear enough. Authors write \"inspired by the EMD method (Boudraa & Cexus, 2007), we sort the time series and calculate the Hausdorff distance between two time series, which preserves the time attribute while retaining the graphic features of the time series.\": I did not know this paper, had a look at it and it seems this (Boudraa & Cexus, 2007) paper does not mention the idea of sorting the time series nor using the Hausdorff distance between them (it consists in decomposing the signal).\n\n2. The novelty is quite unclear, since the paper relies on a well-established method (Zhang et al. 2022) and slightly modifies it by adding a pre-processing step and changing the similarity measure in the loss.\nSince these two points constitute the novelty of the paper, they should be motivated, discussed and illustrated in many more details than what is done in the current version of the paper."
                },
                "questions": {
                    "value": "Given my comments above, I do not think answering these questions could be sufficient to change my mind.\n\n* Could you elaborate on the similarity measure that is introduced in Section 4.5, and more specifically on the links of this with the EMD method that is said to be a source of inspiration?\n* What representation is used in Figure 3? Is it tSNE? MultiDimensional Scaling? Something else?\n* Why do you evaluate on only 4 pairs of datasets while timeseriesclassification.com hosts many more datasets, arranged in categories?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653453574,
            "cdate": 1698653453574,
            "tmdate": 1699636192560,
            "mdate": 1699636192560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "LMnaz1XGfY",
            "forum": "xJ5CF1aOOX",
            "replyto": "xJ5CF1aOOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_QovG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_QovG"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe modifications to an existing pre-training and fine-tuning time-series classification model to filter extremities in the data during pre-processing, and utilizing an ordered similarity measures, to show improvements in overall classification metrics across various domain transfer datasets. Particularly, the improved model pre-processes the data using adaptive low-pass FIR filter based on maximum and minimum amplitudes in the window of computation. This data is then divided to obtain frequency and time domain embeddings using separate transformer architectures as per existing work. However, the authors slightly modify the architecture by adding a waveform sorting function over amplitudes in the loss function where Hausdorff distance function is used instead of cosine similarity (employed only at the fine-tuning stage and not pre-training stage). They empirically demonstrate that these two modifications enhances the overall accuracy and F1 scores across various task transfers where model is pre-trained on one dataset using self-supervision, and fine-tuned on another dataset having a small amount of labels."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The empirical experiments shows good comparisons with similar base-line mechanisms, clearly illustrating the advantages of the modifications introduced by the authors to the existing model.\n2. By depicting the sorted loss representation, the authors clearly illustrate key concepts to describe need for capturing morphology of class differences in datasets, among the datasets they have used.\n3. The authors explain competitive approaches in detail, providing intuitive reasons for lower scores obtained by those models in their empirical study."
                },
                "weaknesses": {
                    "value": "1. The paper is hard to follow for readers not in the domain of time-series classification. Particularly, the motivation in the introduction of the paper seems to start with applications in what the authors say as industrial field. But, it is very hard for a naive reader to quickly understand the context of the paper in the introduction. The context of the problem under consideration becomes clear only after reading some cited papers, particularly that of Zhang et al. (2022) which the authors extensively employ and modify. For example, in section 2.1, sentences such as \"DTW cannot obtain the more information of curves ...\" or \"... due to issue of the rationality of segment combinations during classification\" etc. is very confusing or imprecise. It is hard to understand what the authors intended to say here.\n2. Notations used to mathematically describe the problem and algorithms are imprecise and confusing. For example, in section 3, it is unclear if CT = CT'? Is there any relationship between T and T'? Is Mpre = f(xi) a function of single data point within a channel or is xi a sample? In Algorithm 2, where is Tstep used? What is the temp variable?\n3. In Figure 2, it would be better to highlight modified parts of Zhang et al's model. Particularly, it is unclear if CNN is part of the modification or the original model. If CNN is part of the modification, then it's applicability on the transfer learning empirical study is incomplete. \n4. It is good that the authors use Hausdorff distance to preserve time attributes and retain graphical features. However, there are various other distance measures such as Bhattacharya distance, or EMD method, that could also be used instead. It would be good if the authors empirically show evidence why Hausdorff was chosen. \n5. From Table 2, it is clear that DPTCS is better than the TF-C method that the authors have modified. However, the original Zhang et al., 2022 article evaluates one-to-many domain adaptation which brings out areas where other competing models perform better than TF-C. This key evaluation is missed by the authors in the current submission."
                },
                "questions": {
                    "value": "Please see weaknesses section. In addition, I have a few additional clarifications here:\n\nIn section 4.1, algorithm 1, it is unclear how XI is updated. Can you please explain the reason and methodology behind line 9?\n\nIn section 4.1, is Figure 5 incorrectly specified as algorithm 1?\n\nIn algorithm 2, what is IQR?\n\nIn the introduction, what is NT-Xent loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724525628,
            "cdate": 1698724525628,
            "tmdate": 1699636192487,
            "mdate": 1699636192487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NqWMG1C88m",
            "forum": "xJ5CF1aOOX",
            "replyto": "xJ5CF1aOOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_nrKg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_nrKg"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a few modifications to an existing pre-training (contrastive learning)-based time series classification model (Zhang et al. 2022). Experimental results on 8 sets of time series show that the proposed modifications improve the classification accuracy."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The idea of adding a sorting-based loss seems to be interesting."
                },
                "weaknesses": {
                    "value": "1. There are a few self-supervised contrastive learning-based models for time series classification as mentioned in Section 2.2. However, their limitations (and differences with the proposed work) have not been clearly discussed. There are several sentences at the end of Section 2.2 on this point but they are vague and difficult to follow. It is difficult the evaluate the novelty of the proposal. \n\n2. The overall writing and use of language is substandard and requires substantial improvement. Many technical details are missing or inaccurate: \n\n\"Our work follows the Zhang et al. (2022)\u2019s work, We adopted his concept of time-frequency consistency, but made some modifications on the model. We embedded a CNN module behind the transformer\": What exactly are the modifications? Just adding a CNN module after the transformer?\n\n\"Given a set of time series that needs to be pre-trained\": How do you pre-train a set of time series?\n\n\"Each sample consists of CT channels, and each channel contains |xi| data points:\" What is a sample? A time series? What is a channel? What is a data point? And how are the |xi| data points of a channel formed?\n\nWhere does \"temp\" in Algorithm 2 come from?\n\n3. The experimental results are not too convincing. The URC time series repository has over 100 sets of time series. It is unclear how the 8 sets are chosen for the experiments. Also, each chosen set requires a different hyperparameter setting as shown in Appendix A.1. It is unclear how the proposed model can be used in a real application. \n\n4. Other presentation issues:\nStatements like \"The extensive use of time series in the industrial field (Bi et al., 2023)(Li et al., 2020)(Gupta et al., 2020) is beyond doubt.\" are too strong and may need to be tuned down. \n\n\"figure 1 shows the entire process\" => \"Figure 1 shows the entire process\"\n\nIncomplete sentence: \"pre-training a classification model for sequential\""
                },
                "questions": {
                    "value": "See the weak points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736211338,
            "cdate": 1698736211338,
            "tmdate": 1699636192343,
            "mdate": 1699636192343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "IazfSXN7CW",
            "forum": "xJ5CF1aOOX",
            "replyto": "xJ5CF1aOOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_PC6z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2559/Reviewer_PC6z"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a pipeline for data pre-processing and training a classifier for time series. It pre-processes data with filters to retain the most prominent features of the data, along with some data augmentation to increase the amount of available pre-training data. Next, the data goes into the model for pre-training and fine-tuning. It first goes through two transformers, one for the frequency domain and the other for the time domain. Then, it is passed through CNNs to remove any high-dimensional features. Finally, the model calculates the loss in time and frequency domains. The sum of these losses is used to train the classifier. This work uses a modified version of NT-Xent loss commonly used in contrastive learning. For pre-training, this pipeline uses cosine similarity to learn the general trends in the data. Then, it uses ranking similarity in the fine-tuning stage so the model can learn a finer differentiation between waveform details to achieve higher accuracy. There are three main contributions: First, they propose a platform filtering method and self-adaptive FIR filter for data pre-processing, which improves the quality of the data. Then, they propose a new ranking similarity for fine-tuning models. Finally, they create a time series classification model with CNNs to remove high-dimensional features. The proposed method was evaluated on 4 groups of 8 real datasets and compared to several state-of-the-art models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The Methodology sections 4.4 and 4.5 are written clearly enough. This paper builds off a previous work, and it is clear which parts of the model were changed. The paper identifies and explains each part that was changed in this updated model pipeline. \n+ Section 5.3\u2019s Fine-tuning Analysis is strong. This section explains why certain baselines do well on some datasets and struggle in others. This supports the values reported in Table 2 and make the results more convincing. \n+ Figure 3 in section Sorted Similarity Representation is helpful for understanding the impact of the proposed ranking similarity in the pipeline. It provides a visual cluster of the data after using the ranking similarity to show that ranking helped separate the data. The corresponding text also supports this Figure 3 and makes this work stronger."
                },
                "weaknesses": {
                    "value": "- The paper is difficult to read because of poor grammar. Too many sentences are hard to parse.\n- For the Methodology section 4.2 and 4.3, the data filtering algorithms are very briefly described, and some pseudocode is provided. It would be easier to understand the algorithms with some more details in the description. For instance, section 4.2 mentions \u201cWe use a FIR filter with a low-pass frequency designed to be adaptive. The value is set based on the maximum frequency of the current curve multiplied by sqrt(2)/2\u201d. Although it is explained in the Appendix, a bit more explanation about what makes the filter adaptive and why the values sqrt(2)/2 is used would make this section in the main text more understandable. \n- Some of the variables in the algorithms are not explained well. For example, section 4.3 mentions that \u201cDuring the data platform filtering process, we need to make a reasonable division of the winscale value, otherwise overfiltering may occur\u201d. An explanation of what the winscale value represents and a range of reasonable values would make this section stronger. \n- The paper lists 8 datasets used in this work for evaluation and gives a description of each, but there is not much explanation for why these particular datasets were chosen. More detail about why these particular datasets were chosen would help this section. In the methodology these 8 datasets are grouped into pairs where one dataset is used for pre-training and another is used for fine-tuning. After reading the Table 1, it becomes clear that these datasets are paired by similar domains. However, it would be better if this was stated directly in the text instead of implied by the Table 1. \n- The Ablation Study and its corresponding Figure 4 are not clear. It states that 7 combinations of DPTSC were compared, but it is not clear what these 7 combinations are, even after re-reading sections of the paper and appendix. The Figure 4 is also harder to read and understand compared to the other figures and tables in this work."
                },
                "questions": {
                    "value": "Please see the strengths and weaknesses. In addition to grammar, the presentation should also be improved. Related Work lists previous papers and their limitations, rather than providing the reader with background information for this paper\u2019s research field. The Ablation Study is vague, and it is unclear what it is comparing. This makes it difficult to see exactly which components in their pipeline are contributing to the improved results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2559/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808557501,
            "cdate": 1698808557501,
            "tmdate": 1699636192187,
            "mdate": 1699636192187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]