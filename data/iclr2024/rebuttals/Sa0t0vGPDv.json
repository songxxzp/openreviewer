[
    {
        "title": "FARS: FSM-Augmentation to Make LLMs Hallucinate the Right APIs"
    },
    {
        "review": {
            "id": "UFFQDEhPNV",
            "forum": "Sa0t0vGPDv",
            "replyto": "Sa0t0vGPDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_NqbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_NqbF"
            ],
            "content": {
                "summary": {
                    "value": "Generative models may hallucinate APIs when generating their responses. To this end, this paper proposes FARS to use constrained decoding to limit the token selection during API calls, making LLMs generate desired API formats. Specifically, the authors construct a finite state machine to enforce the decoding to follow the structure Begin-API-Argument-Value-End and limit the number of available tokens to the number of designed states (e.g., # APIs when generating APIs).  The implementation is based on the dynamic trie implementation to save the space cost. Experiments show the effectiveness of FARS compared with unconstrained LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. FARS is a novel approach that involves an inference-time intervention to enforce LLMs to generate the correct API formats\n2. The approach is sound and well-illustrated.\n3. Experiment results show the improvement is quite apparent."
                },
                "weaknesses": {
                    "value": "1. Lack of wall time analysis. I am not sure if this method will bring much extra time cost since the approach seems to involve many CPU operations. It will be good to add a wall time comparison.\n2. The approach is more like eliminating \"syntax\" error but not \"semantic\" error. Will FARS eliminate \"syntax\" error but increase \"semantic\" error? For example, the function is originally semantically correct but with a wrong format, whereas FARS corrects the syntax but brings semantic errors. It will be good to see how much performance is obtained by \"syntax\" correction and if FARS introduces more \"semantic\" errors.\n3. Lack of some implementation details. Please see questions for details."
                },
                "questions": {
                    "value": "What temperature is used in the experiments? The comparison may be less convincing if the temperature is high for baseline models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2019/Reviewer_NqbF",
                        "ICLR.cc/2024/Conference/Submission2019/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791846596,
            "cdate": 1698791846596,
            "tmdate": 1700512223571,
            "mdate": 1700512223571,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SyXOn2TqyV",
                "forum": "Sa0t0vGPDv",
                "replyto": "UFFQDEhPNV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----Lack of wall time analysis----\n\nIn general response\n\n----Will FARS eliminate \"syntax\" error but increase \"semantic\" error?----\n\nIn our implementation, we simply constrain the allowed token set without changing any of the probability scores. The highest probability token will remain the highest after constraining, provided it doesn\u2019t interfere with the allowed arguments, API names, or format. Now, when we constrain and force the model to choose a token from the valid subset of tokens, which is different from the original global max, the question of whether that affects the rest of the generation is a hard one to answer in theory. We believe a good model should be able to generate a good continuation from any prefix, so we would argue that no, it doesn\u2019t introduce any more semantic errors. To support this, we ran a quick empirical study on the MultiWOZ dataset on how many individual samples the unconstrained more performs better than the constrained model (not just the average, on which we know the constrained model is clearly better). We found that over 1000 dialogs and 7372 API turns, the unconstrained model is better than constrained just 6 times (0.08%).\n\n----What temperature is used in the experiments? The comparison may be less convincing if the temperature is high for baseline models.----\n\nWe don\u2019t use temperature at all in our experiments. We use greedy decoding for API Generation since we don\u2019t care about sampling diversity and just want the API call to be predicted correctly. This is for both constrained and unconstrained models. In the public baseline numbers for MultiWOZ from other previous works, we verified that it was also greedy, without any temperature sampling (those are all smaller models SFTed for this task anyway, not text generation LLMs). We included a section in the appendix about these inference details."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250256647,
                "cdate": 1700250256647,
                "tmdate": 1700250256647,
                "mdate": 1700250256647,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9MgLmcylsw",
                "forum": "Sa0t0vGPDv",
                "replyto": "SyXOn2TqyV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2019/Reviewer_NqbF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2019/Reviewer_NqbF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. I raised my score to 8."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512208282,
                "cdate": 1700512208282,
                "tmdate": 1700512208282,
                "mdate": 1700512208282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VLqefyWmfT",
            "forum": "Sa0t0vGPDv",
            "replyto": "Sa0t0vGPDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_msYT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_msYT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach called FARS to enable Large Language Models (LLMs) to generate the right API calls without the need for shortlisting instructions or examples. The approach uses a finite state machine-based constrained decoding algorithm to ground the generation of LLMs to a set of available APIs. The paper demonstrates the effectiveness of FARS on three datasets - SNIPS, MultiWOZ, and a Smart Home Control dataset, showing significant improvements over an unconstrained LLM."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper introduces a novel approach to address the problem of generating the right API calls without shortlisting instructions or examples.\n2) The use of a finite state machine-based constrained decoding algorithm provides a structured and grounded approach to API generation.\n3) The experimental results on the three datasets demonstrate the effectiveness of FARS, showing significant improvements over an unconstrained LLM."
                },
                "weaknesses": {
                    "value": "1) The paper could provide more details on the implementation of FARS, including the specific steps and algorithms used to integrate the finite state machine with the LLM.\n2) The paper lacks a thorough discussion of the limitations and potential future directions of the proposed approach."
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2019/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2019/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2019/Reviewer_msYT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829791419,
            "cdate": 1698829791419,
            "tmdate": 1699636133181,
            "mdate": 1699636133181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MFGlaFCoOz",
                "forum": "Sa0t0vGPDv",
                "replyto": "VLqefyWmfT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----The paper could provide more details on the implementation of FARS.----\n\nIn general response\n\n----The paper lacks a thorough discussion of the limitations and potential future directions of the proposed approach.----\n\nThank you for bringing this to our notice and apologies for lack of more information on this. Due to space constraints, we limited this discussion to just the last paragraph of our paper. We however mention two concrete limitations and future directions to address them - 1) our model\u2019s inability to handle APIs from unknown domains due to always constraining it to an API bank and 2) lack of analysis on the model\u2019s ability to conditionally choose to engage the FSM. We will add a section on this in the appendix after further discussion among ourselves."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250206235,
                "cdate": 1700250206235,
                "tmdate": 1700250206235,
                "mdate": 1700250206235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wMsV093UKy",
            "forum": "Sa0t0vGPDv",
            "replyto": "Sa0t0vGPDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_QTFS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2019/Reviewer_QTFS"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach called FSM-Augmentation to make Language Models generate correct API calls by grounding their output in a Finite State Machine that describes valid API calls. This method, FARS, aims to address the issue of LLMs \"hallucinating\" plausible but incorrect API calls by using a constrained decoding algorithm based on FSM.\n\nFARS's approach allows for the dynamic selection of API arguments and free-text values, improving upon traditional methods that rely on fixed argument orders. The FSM design enables the LLM to predict the order and subset of arguments, enhancing flexibility and accuracy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper introduces a finite state machine-augmented approach, which is an improvement over traditional LLMs that often hallucinate plausible but incorrect API calls. By grounding the LLM's generation process in a finite state machine, the model is constrained to produce only valid API calls, which is a practical solution to a common problem in LLM outputs.\n\n2. No Need for External Retrievers: Unlike other methods that rely on external retrievers or exemplars to guide the generation of API calls, FARS operates independently by incorporating the API catalog information into the FSM. This reduces the complexity and potential points of failure associated with external dependencies."
                },
                "weaknesses": {
                    "value": "1. The effectiveness of FARS is contingent on the FSM's knowledge of the available API catalog. It is not clear how easy to update FSM if there is a new API function added\n2. Potential Overhead during inference. Creating and updating the FSM to reflect the current state of API offerings could introduce overheads. How fast is the inference speed of FARS compared with unconstrained LLM?\n3. The scope of the paper's evaluation is limited to the Vicuna-33B model's performance on specific datasets. A broader assessment across various models would provide a more comprehensive understanding of FARS's effectiveness and generalizability."
                },
                "questions": {
                    "value": "1. What is the retrieval model used in API retrieval setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2019/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699255182838,
            "cdate": 1699255182838,
            "tmdate": 1699636133122,
            "mdate": 1699636133122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MXPXw80jJY",
                "forum": "Sa0t0vGPDv",
                "replyto": "wMsV093UKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2019/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "----It is not clear how easy to update FSM if there is a new API function added----\n\nUpdating the FSM with the new API is very easy. In fact, it is one of the advantages of our approach since we don\u2019t have to touch the model or exemplars at all. \nWe currently use a dynamic trie structure to represent the FSM. This trie is constructed from a schema dictionary that contains all the APIs, arguments, and allowed values. To add a new API, we can simply add it to the dictionary and re-run the trie initialization. This adds the sequence corresponding to generating that API name to the trie and includes the possible arguments in a tracker. Since the trie is dynamically expanded to cover arguments as the sequence keeps progressing during inference, we already have logic to go through all available APIs and extend the trie so nothing needs to be done for the new API there. We added these details in the paper to make this clear.\n\n----How fast is the inference speed of FARS compared with unconstrained LLM?----\n\nIn general response\n\n----The scope of the paper's evaluation is limited to the Vicuna-33B model's performance on specific datasets----\n\nWe performed evaluation with internal models of different sizes and found similar trends as well but reported only on Vicuna for reproducibility. Our approach required us to change the underlying generation strategy so we couldn\u2019t go with closed models such as ChatGPT, where we don\u2019t have access to the weights to run generation ourselves. We required an instruct-tuned LM and we believed that Vicuna-33B was a good strong candidate to go with. We also briefly experimented with LLAMA-2-Chat but found that that model was very resistant to generating APIs, no matter how we prompted it with exemplars. \n\n----What is the retrieval model used in API retrieval setting?----\n\nWe use a simple sentence-encoder similarity approach between the API calls and utterances to choose the retrieved set but also ensure that the gold exemplar (one with the target API call) is always present in SNIPS. This especially favors the unconstrained model since the exemplars are the only source of knowledge for it. We included info about this in the original draft and also updated it to include more details. For the internal smart home dataset, we didn\u2019t perform retrieval for APIs, since each device came with a list of possible APIs or actions and we could just use those. For device retrieval/shortlisting, we used the same sentence transformer for similarity computation between the device details and user utterance and again also ensured that gold ones are present."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2019/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250168276,
                "cdate": 1700250168276,
                "tmdate": 1700250168276,
                "mdate": 1700250168276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]