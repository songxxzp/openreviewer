[
    {
        "title": "Behaviour Distillation"
    },
    {
        "review": {
            "id": "yjBsHWtpJL",
            "forum": "qup9xD8mW4",
            "replyto": "qup9xD8mW4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
            ],
            "content": {
                "summary": {
                    "value": "The authors attempt to transfer the concept of dataset distillation to the realm of reinforcement learning. To that end, they introduce the concept of behaviour distillation, which is trying to condense a dataset of state-action pairs required for training a RL agent. After introducing this concept, they propose a new method named HADES, that implements this concept and is able to distill the initial state-actions pairs into a dataset of just four state-action pair that, when trained upon by an agent can make it reach competitive performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Transfer the concept of dataset distillation to reinforcement learning\n- The condensed dataset can result in a quite small one\n- The results on supervised classification dataset distillation also seem good"
                },
                "weaknesses": {
                    "value": "- 1) I think the neuroevolution part is quite distinct from the dataset distillation part. It should be explained clearly why you need to use a neuroevolution technique instead of a more classical technique\n- 2) Maybe the naming is a bit confusing, if the method proposed is meant to distill the dataset only (and not train on it later), then it would be clearer to name the experiments as Method + HADES (whenever training a method on top of hallucinated dataset).\n- 3) If I understood correctly, one advantage of HADES is that it can distill behaviour by looking at some subpart of the episode (like 1/10th), and condense the gathered state-action pairs into much smaller dataset. However, from the rest of the paper, it's not clear whether they are some computational advantages of training on top of these condensed dataset. They should be made clearer. Indeed, by looking at the RL plots, in most of the cases, it's not clear if the red curve trains way faster than the green curve (apart from a few environments like humanoid and breakout-miniAtari"
                },
                "questions": {
                    "value": "- 1) If I understand correctly, HADES first distills a dataset into a condensed state-action pair and then trains a classical policy on it (as in classical dataset distillation). What I don't understand is the link with the neuroevolution part. Is this part requires only for distillation or is it also required for training on top of the distilled dataset ? In the latter case, it would be interesting to see how other methods that do not use neuroevolution perform when trained on the condensed dataset.\n- 2) In the results plot of reinforcement learning, does the number of generation directly correlates to the training time ? Or are some generations quicker to run for methods trained on condensed dataset (that implies the inner loop would be faster I guess) ? I think the computational advantage of training on top of condensed dataset (if any) should be made clearer"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6255/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t",
                        "ICLR.cc/2024/Conference/Submission6255/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698399668447,
            "cdate": 1698399668447,
            "tmdate": 1700489538070,
            "mdate": 1700489538070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pKQiS9Gfqr",
                "forum": "qup9xD8mW4",
                "replyto": "yjBsHWtpJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R3"
                    },
                    "comment": {
                        "value": "We thank reviewer ZN7t for their comments. \n\n0. > _\"HaDES [...] is able to distill the initial state-actions pairs into a dataset of just four state-action pair\"_\n\nWe formulate Behaviour Distillation without access to expert data, as mentioned in the abstract, introduction and section 4.1. This means there are no \"initial state-action pairs\". Also, the size $N$ of the distilled dataset is a hyperparameter we choose. Figures 1 shows datasets with $N=2$ and $N=4$, and in section 5 we report results with $N=64$ and $N=16$.\n\n**Response to Weaknesses:**\n\n1. > _\"explain [...] why you need to use a neuroevolution technique instead of a more classical technique\"_\n\nNeuroevolution covers all methods optimising a neural network using Evolutionary Strategies (ES) [1]. We do not use neuroevolution. HaDES, and in particular HaDES-F, can act as a neuroevolution method, as we explain in section 4.2 and demonstrate in section 5.1.  We use ES instead of a gradient-based approach such as Backpropagation Through Time (BPTT) because BPTT is expensive when training the inner loop policy for many steps. For context, we use 400 inner loop training steps, while [3] only uses 10 to 30.  We have now clarified this in section 2.1.\n\n2. > \"...it would be clearer to name the experiments as Method + HADES\"\n\nHaDES is a method that takes in an environment M and outputs a tuple $(D_\\phi, \\pi_{\\theta^*})$, where $D_\\phi$ and $\\pi_{\\theta^*}$ are the solutions to the optimization problem in eq. 3 and 4. HaDES uses ES in the outer loop and behaviour cloning (supervised learning) in the inner loop. Training a policy on the synthetic dataset is integral to HaDES, so we cannot split the name into \"method + HaDES\".\n\n3. > \"looking at some subpart of the episode (like 1/10th)...\"\n\nHaDES does not divide episodes at all. It evolves the synthetic datasets from scratch. In section 1, we say that the final dataset contains \"1/10th [...] of a single episode **worth** of data.\" This is about the final magnitude of the dataset, not about how HaDES works.\n\n> \"... computational advantages of training on top of these condensed dataset.\"\n\nThe usual episode is 1000 transitions, and our datasets are at most 64 states (<10% of an episode). In contrast, training a policy using RL would require ~50k episodes [2]. Training on the distilled datasets uses between **6 and 8 orders of magnitude less data** that standard RL training, making training policies extremely fast.\n\nWe have also updated Section B.2 of the manuscript with details of the GPU time required to run each method.\n\n> \"looking at the RL plots [...] it's not clear if the red curve trains way faster than the green curve\"\n\nWe are not sure what the reviewer is trying to say. HaDES-F (red curve) reaches statistically higher return than Vanilla ES (green curve) in 7/12 environments reported, reaches similar returns in 4/12 environments and reaches statistically lower returns in 1/12 environments. If the reviewer could please clarify what they mean, we would be happy to respond further.\n\n**Answer to Questions:**\n\n1. > \"HADES first distills a dataset into a condensed state-action pair and then trains a classical policy on it (as in classical dataset distillation).\"\n\nThat is incorrect. As stated previously, HaDES does not take a dataset as input and does not perform classical dataset distillation. We have also expanded section 4.1 with details on why we do not rely on an expert dataset.\n\n > \"...neuroevolution part. Is this part requires only for distillation or is it also required for training on top of the distilled dataset ? In the latter case, it would be interesting to see how other methods that do not use neuroevolution perform when trained on the condensed dataset.\"\n\nHopefully, our explanation under W1 clarified that we do not use neuroevolution. We use ES (which is different from neuroevolution) to optimise the synthetic dataset during the distillation process. When we train the policy (in the inner loop or on the final distilled dataset), we simply use behaviour cloning (i.e. supervised learning).\n\n2. > \"In the results plot of reinforcement learning, does the number of generation directly correlates to the training time ?\"\n\nFor each method and each environment, the time/generation is constant. Generally, HaDES takes a bit longer per generation since it trains a policy every inner loop, as opposed to ES, which must only evaluate the policy. We have added a time/generation breakdown to section B.2 of the appendix.\n\nWe hope our response has clarified any misunderstanding and that we have addressed the reviewer's feedback. If so, we would appreciate if they would consider increasing their score.\n\n[1] https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_589\n\n[2] ReLU to the rescue, https://arxiv.org/abs/2306.01460\n\n[3] Dataset Distillation, https://arxiv.org/pdf/1811.10959.pdf\n\n[4] Efficient Reductions for Imitation Learning, https://proceedings.mlr.press/v9/ross10a.html"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247534287,
                "cdate": 1700247534287,
                "tmdate": 1700247534287,
                "mdate": 1700247534287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XBCY4OfTwk",
                "forum": "qup9xD8mW4",
                "replyto": "pKQiS9Gfqr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for clarifying some points and answering my questions. Below I ask a few more to be sure that I understood the paper correctly in order to give a better review.\n\n-  You mention that for this method, the dataset distillation cannot be combined with another method (cf. answer to the question on why no naming Method + HADES dataset). I think I understood your answer in that once you have a distilled dataset, learning a policy on top of it is similar to \"offline\" supervised training and thus you cannot really use another kind of policy learning algorithm for that task. Is that correct ?\n\n- I understand now why it is necessary to use evolutionary strategies to avoid unfeasible computation of meta-gradients on a big number of inner steps. This is a good idea. As I am not familiar with these strategies, I am wondering however how well they can work when the state space gets bigger (in which case approximating the gradient with noise seems a more difficult problem). What is the typical \"biggest\" state space size that you have considered in your experiments ? (Could it for instance work on a state that is compounded of the features extracted from a resnet, of size 512).\n\n- In my previous questions, I was referring incorrectly sometimes to \"dataset\" and saying the HADES algorithm requires to access a \"dataset\" because I thought you were collecting a buffer of state-action pairs that you would then sample from as done in some algorithms. From your answer I understood that it is not the case, you learn this dataset online from episodes, is that correct ?\n\n- About the comment on training speed, you can ignore it, I thought that these curves were comparing a method that is training on normal episodes vs methods that are training on distilled dataset and I was surprised to see no speed improvement. As I understand now, HADES is learning both a dataset and a policy, so these curves are just comparing HADES to ES and it makes sense that there is no speed improvement for the dataset learning part. I understand now that only Figure 4 looks at the results of training on top of HADES dataset.\n\n- After all of this, I think that the Figure 4 is the most important figure since it really shows the capacity of the distilled dataset. In my opinion it would be better to highlight this and maybe show it for different kind of environments instead of just Hopper. The fact that the policy learned by HADES gets better returns is less interesting in my opinion (and then it should be compared to more policy-learning algorithms additionally to ES).\n\nI would be happy to increase my score, but I would like to be sure that I understand the manuscript correctly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478581854,
                "cdate": 1700478581854,
                "tmdate": 1700478581854,
                "mdate": 1700478581854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XahLOU91EN",
                "forum": "qup9xD8mW4",
                "replyto": "eg4NCmbUOu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_ZN7t"
                ],
                "content": {
                    "title": {
                        "value": "Score revision"
                    },
                    "comment": {
                        "value": "Thanks for answering my questions. You have now cleared my doubts and I updated the score accordingly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489683483,
                "cdate": 1700489683483,
                "tmdate": 1700489683483,
                "mdate": 1700489683483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mQh0JXsBZR",
            "forum": "qup9xD8mW4",
            "replyto": "qup9xD8mW4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposed a behavior distillation algorithm of HaDES that aims to distill a few synthetic (state, action) pairs ($\\mathcal{D}_\\phi$) in the reinforcement learning (RL) setting, and the network can fastly learn a satisfied policy by training on the small distilled dataset. Concretely, the authors firstly formulate that behavior distillation problems as a bi-level optimization where the inner loop optimizes the policy network parameters by supervised learning on distilled data, while the outer loop maximizes the cumulative reward ($J$) w.r.t. the RL task.  Then, synthetic (state, action) pairs are optimized by gradient ascent where  the gradient is estimated by evolution strategies (ES). The authors verified that distilled dataset outperform vanilla ES algorithm on multiple RL datasets including Brax and MinAtar exvironments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper first introduces the dataset distillation into the RL setting, and the core pros of this paper are presented as below:\n\n1. Formulate the behaviour distillation by incorporating dataset distillation with an RL reward function;\n\n2. Propose a novel behaviour distillation algorithm of HaDES that learns a few (state, action) pairs to fastly train a policy network with supervised learning;\n\n3. Multiple empirical studies are conducted to verify the effectiveness and robustness of the synthetic behaviour dataset."
                },
                "weaknesses": {
                    "value": "While the authors creatively incorporate dataset distillation into RL setting, there are some weaknesses mainly lie on the motivation and experiments, which hurt the contribution of this manuscript. \n\n**Q1:** Sec. 1 states that this work is \"motivated by the challenge of behaviour distillation\", while this (challenge) is not a clear motivation for developing the behaviour distillation. What is the advantages of using the distilled dataset in RL except for fast training? In my opinion, the networks in RL are often small and do not require long time training. \n\n**Q2:** While I am not an expert in RL, I note that the authors employ different networks to run HaDES and the vanilla ES baseline due to \"memory constraints\" so that HaDES outperforms vanilla ES, while synthetic datasets often largely underperform the real data in dataset distillation. It will be more reasonable to compare HaDES and vanilla ES under the same experimental settings. Moreover, it will make this work more convincing to add the results of other RL methods instead of only ES.\n\n**Q3**: There lack of experiments investigating the influence of the distill budget (the size of the distlled dataset) on the final performance.\n\n**Q4**: It will make this work more comprehensive if there is an analysis of efficiency. In detail, the author can list the GPU time of behaviour distillation, training on distilled dataset, vanilla ES and other RL methods, which can further highlight the importance of behaviour distillation.\n\nBased on these observations, I think this manuscript is marginally below the acceptance, but I would like to increase my rating if the above questions are well addressed. Below are some minor questions and typos.\n\n---\n\nMinors and typos:\n1. The term Behavior Distillation has already been used in [1], which is borrowed from the concept of knowledge distillation instead of dataset distillation. The authors should discuss this to avoid ambiguity.\n\n2. The algorithm 1 should be placed in the main text for clear illustration.\n\n2. Page 2: \"scaling independently\" -> \"scaling independence\"\n\n3. Page 2: \"policy, reducing\" -> \"policy, thereby reducing\"\n\n4. Page 3: \"train a model faster\" -> \"train a model fastly\"\n\n5. Page 4: The formulation of dataset $\\mathcal{D}$: \"$\\mathcal{D} = \\\\{x_i, y_i\\\\}$\" -> \"$\\mathcal{D} = \\\\{x_i, y_i\\\\}_{i=1}^N$\n\n6. Page 5: this first line: \", i.e. \" -> \", i.e., \"\n\n7. Page 5: there exists an extra right bracket in Eq. (3)\n\n[1] Furuta, Hiroki, et al. \"A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation.\" The Eleventh International Conference on Learning Representations. 2022."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6255/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2",
                        "ICLR.cc/2024/Conference/Submission6255/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566580147,
            "cdate": 1698566580147,
            "tmdate": 1700446015314,
            "mdate": 1700446015314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SkYCFCvs1W",
                "forum": "qup9xD8mW4",
                "replyto": "mQh0JXsBZR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R2"
                    },
                    "comment": {
                        "value": "We thank Reviewer 7Zz2 for their valuable comments. In particular, we welcome the helpful suggestions on how to improve our experiments section. We invite the reviewer to read our response below.\n\n1. > _\"In my opinion, the networks in RL are often small and do not require long time training.\"_\n\nWe agree that the networks used in RL tend to be smaller than the convolutional networks used in vision. However, RL regularly requires tens of millions of environment steps to learn a good policy, which can easily translate to several hours or even days of training time [1, 2]. This is why there has been a recent push to build hardware-accelerated RL environments and high-performance RL implementations [2,3,4,5,6]. The distilled datasets allow us to completely avoid environment bottlenecks by turning policy training into a very small supervised learning problem.\n\n2. > _\"advantages of using the distilled dataset in RL except for fast training?\"_\n\nBeyond training speed, there are multiple applications to the datasets produced by HaDES. Notably:\n\n- In section 5.2, we show that distilled datasets readily generalise to a large range of architectures and hyperparameters, which implies they can be used for effective Neural Architecture Search (NAS). NAS has notably been identified as a potential application of Dataset Distillation in the supervised domain as well [7].\t\n\n- In section 5.3, we demonstrate that the datasets evolved for individual environments can be combined to train a multi-task agent, therefore enabling faster research on generalist foundational models.\n\n- In section 5.5, we show briefly that the datasets can help explain the behaviour of the policies and make them more interpretable.\n\n- As mentioned in our conclusion, we also hope HaDES to act as a starting point for new and improved methods for continual learning, where resetting and retraining policies repeatedly is common to fight plasticity loss [2, 9].\n\n3. > _\"It will be more reasonable to compare HaDES and vanilla ES under the same experimental settings\"_\n\nThank you for raising this point. We have now updated Figure 3 b) with additional runs where the network widths match those used for vanilla ES (width=256). The impact of the on the final performance is negligible in SpaceInvaders, Asterix and Freeway, but has a noticeable impact on Breakout. Nonetheless, the order of the different methods remains unchanged.\n\nFor Brax (Figure 3.a)), this was already the case (both HaDES and ES use `width=512`). Nonetheless, HaDES-F outperforms ES in all Brax environments tested, and HaDES-R matches or beats the baseline in 6/8 environments.\n\n4. > _\"add the results of other RL methods instead of only ES\"_\n\nWe agree that reporting an RL baseline in addition to ES would help contextualise our results better. We have updated Figure 3 with a PPO baseline, which is a very strong RL baseline for our suite of environments [10]. While ES methods still lag behind RL in terms of performance, HaDES helps narrow the gap significantly. This is in addition to the main benefit of HaDES, namely producing the distilled datasets.\n\n5. > _\"lack of experiments investigating the influence of the distill budget (the size of the distilled dataset) on the final performance\"_\n\nWe thank the reviewer for this helpful suggestion. We have added such an analysis to section C.1 of the appendix, with a reference to it at the beginning of section 5. \n\n6. > _\"list the GPU time of behaviour distillation, training ...\"_\n\nWe have now expanded section B.2 with additional details regarding the GPU time for different methods. In particular, please refer to Table 4 for a breakdown per environment.\n\n7. > Minor points:\n\nWe thank the reviewer for their thoroughness! We have added a clarification regarding the term \"Behaviour Distillation\" to avoid ambiguity. Regarding the algorithm, we have considered moving it from the appendix to the main paper, but have decided against it due to lack of space.\n\nWe hope that we addressed the reviewer's comments, which genuinely helped us improve our manuscript! As a result, we hope the reviewer will consider recommending our revised submission for acceptance.\n\n[1] R2D2, https://openreview.net/pdf?id=r1lyTjAqYX\n\n[2] Bigger, Better, Faster, https://arxiv.org/abs/2305.19452\n\n[3] Madrona, https://madrona-engine.github.io/shacklett_siggraph23.pdf\n\n[4] Brax, https://arxiv.org/abs/2106.13281\n\n[5] Scaling Distributed RL, https://arxiv.org/abs/2306.16688\n\n[6] MuJoCo 3 announcement, https://github.com/google-deepmind/mujoco/discussions/1101\n\n[7] Data Distillation: A Survey, https://arxiv.org/pdf/2301.04272.pdf\n\n[8] PureJaxRL, https://github.com/luchris429/purejaxrl\n\n[9] The Primacy Bias in Deep RL, https://arxiv.org/abs/2205.07802\n\n[10] ReLU to the rescue, https://arxiv.org/abs/2306.01460"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245813449,
                "cdate": 1700245813449,
                "tmdate": 1700245872320,
                "mdate": 1700245872320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fOrWPJO5nD",
                "forum": "qup9xD8mW4",
                "replyto": "mQh0JXsBZR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response, which clears part of my concerns. I still have some concerns as below:\n\n**Q1:** \"The distilled datasets allow us to completely avoid environment bottlenecks by turning policy training into a very small supervised learning problem.\" Does the vanilla policy training use supervised learning on an un-distilled large (state, action) pair dataset? If yes, I think the behavior distillation still lies in a supervised learning regime instead of reinforcement learning.\n\n\n\n**Q2:** The authors show many pros of behavior distillation in NAS, interpretability, continual learning, etc. In my opinion, these pros are also inherent advantages for dataset distillation and lie in supervised learning regime. How about the exclusive advantage of behavior distillation *especially in RL field*.\n\n\n\n**Q3:** There still lack experiments on `ES neuroevolution, width=512` in Fig. 3(b)\n\n\n\n**Q4:** I am curious why networks trained on small distilled data can outperform those trained on large source real data in behavior distillation, while the generalization performance of distilled largely lags behind the real data in dataset distillation.\n\n**Q5:** Can the behavior distillation integrate with other RL algorithms instead of ES? If ES largely lags to modern mainstream RL algorithms, the contribution of behavior distillation will be greatly undermined.\n\n\n\n**Q6:** Because efficiency is a main advantage for behavior distillation, a runtime analysis in necessary. However, according to results in Table 4, (1) ES has shorter runtime than HaDES, which means ES is more efficient, am I right? (2) The time unit in Table 4 is second, which means that both training with vanilla setting or behavior distillation have a satisfied speed and thus undermines the necessity to develop behavior distillation. This is why I asked Q1 in my (initial) official review."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378695895,
                "cdate": 1700378695895,
                "tmdate": 1700378733772,
                "mdate": 1700378733772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DFi4qP43Et",
                "forum": "qup9xD8mW4",
                "replyto": "YNhG991xCS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Reviewer_7Zz2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response, which clears all my concerns. I will correspondingly raise my rating.\n\nOne minor: I suggest the authors move the analysis of efficiency (GPU time, distillation budget) to the main text to better demonstrate the superiority of the proposed HaDES."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445989467,
                "cdate": 1700445989467,
                "tmdate": 1700445989467,
                "mdate": 1700445989467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1eOWTMihWN",
            "forum": "qup9xD8mW4",
            "replyto": "qup9xD8mW4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_f5UH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6255/Reviewer_f5UH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new branch of dataset distillation - behaviour distillation. Behaviour distillation aims at distilling a set of synthetic RL dataset without having access to the expert data. The authors handles indifferentiability of the formulation using evolution strategy, and show good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The introduction of behaviour distillation can enrich the literature and direction of Dataset Distillation. Different from standard DD, behaviour distillation does not require access to the expert datasets. This is quite close to a standard /basic RL setting and could be a good starting point.\n\n+ The author's writing is easy to follow and pretty clear on the technical details\n\n+ The experimental section show promising results using the proposed algorithm HADES."
                },
                "weaknesses": {
                    "value": "- Although it is interesting, the proposed behaviour distillation seems to not have a clear motivation on why it can be useful (what's the motivation for proposing this problem and it's potential application, besides DD hasn't been applied to RL), and why not directly formulating the problem on expert dataset. Distilling directly from scratch can make the problem a lot harder.\n- It would be great if the authors can discuss the behaviour difference of a standard RL algorithm and a synthetic data-driven RL algorithm (HADES). What's the potential benefits?\n- One missing citation on BPTT [2]. It would be nice to add some discussion on optimization algorithm in DD, for example BPTT (the original one[1] and momentum-based[2]).\n\nOverall the paper is quite interesting. Looking forward to authors' response on the above questions.\n\n[1] Dataset Distillation\n\n[2] Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795099858,
            "cdate": 1698795099858,
            "tmdate": 1699636684357,
            "mdate": 1699636684357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rPe4bqbdqT",
                "forum": "qup9xD8mW4",
                "replyto": "1eOWTMihWN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to R1"
                    },
                    "comment": {
                        "value": "We thank reviewer f5UH for their feedback and are glad they believe our paper can enrich the literature surrounding Dataset Distillation. \n\n1. Behaviour Distillation as a setting is directly motivated by the success of Dataset Distillation in the supervised domain, with applications such as differential privacy, neural architecture search, continual learning, and federated learning [1]. With our work, we hope to extend the reach of those applications to RL, while also enabling new RL-specific ones, such as the training of multi-task policies without additional environment interactions (section 5.3 of our paper).\n\nWe formulated Behaviour Distillation to be independent of an expert policy for two main reasons:\n- Firstly, an expert dataset is not always available, and so we do not wish to rely on this assumption. Furthermore, even if an expert dataset was available, the expert's behaviour could be erratic and hard to distill, which means that we would need to increase the size of the dataset to capture the idiosyncrasies of the expert. Instead, HaDES restricts its optimization to policies that can be distilled into datasets of size $N$, eliminating policies that are too erratic by design.\n- Secondly, even when we do have access to a dataset, supervised dataset distillation is still likely to fail. In RL, a policy trained with behaviour cloning (BC) on an expert dataset incurs regret with respect to the expert that is quadratic in the length of the episode [2]. Intuitively, this happens because each timestep $\\pi$ deviates from $\\pi_{expert}$, it risks ending up off-distribution, which in turn makes it more likely to make additional mistakes. Since dataset distillation is often lossy, we can therefore expect $\\pi$ to achieve low returns in many long horizon tasks, even if it achieves low (but non-zero) cross-entropy with the expert.\n\nWe recognize this motivation wasn't explicit in the original manuscript and have therefore expanded section 4.1 with the details above.\n\n2. > _\"discuss the behaviour difference of a standard RL algorithm and a synthetic data-driven RL algorithm (HADES). What's the potential benefits?\"_\n\nWe are not sure whether the reviewer is asking about the difference between algorithms or between the resulting policies. We answer both:\n\n- Differences between algorithms: \n  - Deep RL algorithms, in short, will repeatedly unroll the policy in the environment to collect transitions, and use those transitions to update the policy through SGD, in a way that ultimately maximises the expected discounted return.\n  - Vanilla Evolutionary Strategies (ES) perform Neuroevolution: they start from a randomly initialised neural network parametrization $\\theta$ of the policy $\\pi_\\theta$. Each generation, they sample a population of random variations of that parametrization, estimate the expected return of each member of the population (the candidates), and use that information to update the policy parameters.\n  - As described in section 4.2, HaDES is similar to ES, but instead of keeping track of a policy $\\pi_\\theta$, it keeps track of a synthetic dataset $D_\\phi$. Each generation, it then samples a population of datasets and uses BC to obtain a policy from each dataset candidate. The advantage is that $|\\phi|$ < $|\\theta|$ typically, which reduces the single-GPU memory of the algorithm and means there are fewer parameters to optimise. In practice, this also translates to significantly higher returns, as shown in Section 5.1. Additionally, the advantage of HaDES over both Deep RL and ES is that it produces the dataset $D_\\phi$, which can be used for fast retraining and other applications.\n\n- Differences between policies:\n  - Because HaDES and Deep RL rely on very different optimization methods, they can in theory produce policies with different behaviours. For instance, HaDES only restricts optimization over relatively \"simple\" policies, i.e. those that can be condensed in a dataset of predetermined size. This is beneficial in scenarios where interpretability and predictability are important. We believe exploring the full impact of the training method on the resulting policies would be a promising direction for future work.\n\n3. > _\"One missing citation on BPTT.\"_\n\nThank you for the missing reference. We have now added it to the paper.\n\n4. > _\"It would be nice to add some discussion on optimization algorithm in DD, for example BPTT\"_\n\nWe already had a brief discussion regarding the connection between BPTT/meta-gradients and HaDES in section 2.1. Following feedback from the reviewer, we have added a few more details to the revised manuscript. If this is not what the reviewer meant, we ask them to kindly clarify.\n\nWe hope this covers the reviewer's questions, and are happy to continue the conversation should the reviewer have any follow-up.\n\n[1] Data Distillation: A Survey, https://arxiv.org/pdf/2301.04272.pdf\n[2] Efficient Reductions for Imitation Learning, https://proceedings.mlr.press/v9/ross10a.html"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245532613,
                "cdate": 1700245532613,
                "tmdate": 1700245564797,
                "mdate": 1700245564797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]