[
    {
        "title": "Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical Invariance"
    },
    {
        "review": {
            "id": "k4EoSzQFKl",
            "forum": "o87xfYKQC1",
            "replyto": "o87xfYKQC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_QCbo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_QCbo"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a First-Order Norm+Linear Autoregression method (called FINOLA) which represents each image in the latent space as a first-order autoregressive process. Then, the authors validate the FINOLA property from on image reconstruction and self-supervised learning. Experiments demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper represents each image in the latent space as a first-order autoregressive process, and conducts experiments on image reconstruction and self-supervised learning to validate the method."
                },
                "weaknesses": {
                    "value": "Some details of the figures and the method are not clear. The experiment section can be improved."
                },
                "questions": {
                    "value": "1. Some details of the figures are not clear. For example, what do red and blue arrows in z(x, y) mean? How do they affect the reconstruction? Does a single vector q mean the feature of the whole image or the specific pixel? In the figure, the a high frequency in the reconstruction loss.\n\n2. How to calculate the mean $\\mu_z$ and the standard deviation $\\sigma_z$? In Eqn. (1), how to initialize the matrix A and B? What are the constraints of the matrix A and B?\n\n3. The authors mainly compare VQGAN (Esser et al. (2021)) and stable diffusion (Rombach et al. (2021)) in image reconstruction. It would be better to compare recent image reconstruction methods. In addition, could you compare convolutional U-Net for image reconstruction?\n\n4. In Table 4 (a), using PSNR to compare VQGAN and Stable Diffusion is not convincing because these methods are generative methods. It would be better to use LPIPS and FID."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4138/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4138/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4138/Reviewer_QCbo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763610449,
            "cdate": 1698763610449,
            "tmdate": 1699636379408,
            "mdate": 1699636379408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iMpgi1rBLp",
                "forum": "o87xfYKQC1",
                "replyto": "k4EoSzQFKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n**Q1: Some details of the figures are not clear.**\n\n**For example, what do red and blue arrows in z(x, y) mean?**\nThe red and blue arrows in $\\mathbfit{z}(x, y)$ represent the two autoregression paths of FINOLA for any given position $(x, y)$ from the center where the global pooled vector $\\mathbfit{q}$ is placed. Each arrow signifies a FINOLA step, and the color denotes the order of regression. The red path involves horizontal regression followed by vertical regression, while the green path involves vertical regression followed by horizontal regression. The results of these two paths are averaged to obtain the final feature vector.\n\n**How do they affect the reconstruction?**\nIt's crucial to note that we repeat this \"two-path\" autoregression until the entire feature map is generated. Subsequently, the feature map goes through the decoder to reconstruct the images.\n\n**Does a single vector q mean the feature of the whole image or the specific pixel?**\n*\"A single vector q\"* indeed denotes the feature of the *entire image* after pooling.\n\n**In the figure, the a high frequency in the reconstruction loss.**\nAs for the concern about the loss of high-frequency details in the reconstructed images, we acknowledge this limitation, and it is discussed in detail in Appendix B. We attribute this limitation to the choice of the $L_2$ loss function. Our intentional use of a simple loss function is to highlight that FINOLA is not heavily dependent on complex reconstruction loss functions. To improve image quality, an alternative approach could involve exploring the use of a combination of perceptual loss and an adversarial training procedure with a patch-based discriminator, as outlined in the VQGAN framework.\n\n---\n**Q2: How to calculate the mean $\\mu_z$ and the standard deviation $\\sigma_z$? In Eqn. (1), how to initialize the matrix A and B? What are the constraints of the matrix A and B?**\n\nThe mean $\\mu_z$ and the standard deviation $\\sigma_z$ are calculated per position $(x, y)$ on the feature map $\\mathbfit{z}$. For each position $(x, y)$, the corresponding feature $\\mathbfit{z}(x, y)$ is a vector with $C$ channels: $\\mathbfit{z}(x, y)=[z_1,\\dots,z_C]^T$. $\\mu_z$ and $\\sigma_z$ are the mean $\\mu_z=\\frac{1}{C}\\sum z_i$ and standard deviation $\\sigma_z=\\sqrt{\\frac{\\sum (z_i-\\mu_z)^2}{C}}$ of these $C$ values.\n\nMatrices $\\mathbfit{A}$ and $\\mathbfit{B}$ are randomly initialized, and no specific constraints are placed on them."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214274216,
                "cdate": 1700214274216,
                "tmdate": 1700214298439,
                "mdate": 1700214298439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E1iqfoJfTh",
                "forum": "o87xfYKQC1",
                "replyto": "k4EoSzQFKl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 2)"
                    },
                    "comment": {
                        "value": "**Q3: The authors mainly compare VQGAN (Esser et al. (2021)) and stable diffusion (Rombach et al. (2021)) in image reconstruction. It would be better to compare recent image reconstruction methods. In addition, could you compare convolutional U-Net for image reconstruction?**\n\n***Comparison with convolutional U-Net:*** The table below provides a comparative analysis between our method and convolutional U-Net for image reconstruction (measured by PSNR). Both approaches share the same Mobile-Former encoder and have identical bottleneck dimensions (C=1024 or 3072). In our method, FINOLA is employed to generate a 16x16 feature map, followed by a convolutional decoder to reconstruct an image with a size of 256x256. On the other hand, U-Net has a deeper decoder that utilizes convolution and upsampling for image reconstruction. Both methods boast a similar model size. The results showcase the superior performance of FINOLA over convolutional U-Net, indicating that a single layer FINOLA is more effective than multi-layer convolution and upsampling.\n\n|Method|C=1024|C=3072|\n|---|---|---|\n| Convolutional U-Net | 23.4 | 25.1 |\n| **FINOLA (our)** | **23.7** | **25.8** |\n\n***Comparison with ViT-VQGAN:*** In the following comparison, Masked FINOLA is evaluated against ViT-VQGAN (*\"Vector-Quantized Image Modeling with Improved VQGAN, Yu et al., ICLR 2022\"*) using ImageNet linear probing. FINOLA achieves higher accuracy with a much more compact encoder.\n\n|Method|Params|Top-1|\n|---|---|---|\n| ViT-VQGAN | 650M |  65.1 |\n| **FINOLA (our)** | **28M** | **66.4** |\n\nFor a detailed comparison in image reconstruction, we will address this in the subsequent question (**Q4**).\n\n---\n**Q4: In Table 4 (a), using PSNR to compare VQGAN and Stable Diffusion is not convincing because these methods are generative methods. It would be better to use LPIPS and FID.**\n\nBelow, we present a comparison of FINOLA with the ***first stage*** of multiple generative methods, assessed using both PSNR and FID metrics. While FINOLA outperforms in PSNR, it lags in FID. This discrepancy can be attributed to our deliberate use of the $L_2$ loss function. We intentionally opted for a simple loss function to underscore that FINOLA does not heavily rely on intricate reconstruction loss functions. To enhance FID, we could explore the success of perceptual and GAN losses, as seen in VQGAN, ViT_VQGAN, and Stable Diffusion.\n\n|Method|Latent Size| Channel|logit-laplace loss|$L_2$ loss|Perceptual loss|GAN loss|FID&#8595;|PSNR&#8593;|\n|---|---|---|---|---|---|---|---|---|\n| DALL-E |16x16|--|&check;||||32.0|22.8|\n| VQGAN |16x16|256|||&check;|&check;|4.98|19.9|\n| ViT-VQGAN |32x32|32|&check;|&check;|&check;|&check;|1.28|--|\n|Stable Diffustion|16x16|16|||&check;|&check;|**0.87**|24.1|\n| **FINOLA (our)**|1x1|3072||&check;|||27.8|**25.8**|\n\nIt's essential to clarify that FINOLA does not operate as a generative model. Our primary goal is not image generation; instead, we aim to unveil mathematical properties within the latent space. Image reconstruction, one of our tasks alongside self-supervised learning, serves as a means to validate FINOLA \u2014 a first-order norm+linear autoregressive model \u2014 as a fundamental mathematical property shared by all images.\n\nFINOLA is also implemented differently from autoregression-based generative models. The latter typically consists of two stages: the first stage involves learning an autoencoder and vector quantization in the latent space, while the second stage focuses on learning autoregression in the quantized latent space. In contrast, FINOLA operates as a single-stage model, seamlessly learning the encoder, autoregression, and decoder end-to-end, with autoregression performed on a continuous (non-quantized) latent space."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214768744,
                "cdate": 1700214768744,
                "tmdate": 1700214843230,
                "mdate": 1700214843230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AYJBKUQXTK",
            "forum": "o87xfYKQC1",
            "replyto": "o87xfYKQC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_FEae"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_FEae"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel framework FINOLA (First-Order Norm+Linear Autoregressive) for image representation with powerful autoregressive capabilities. First, image is firstly encoded into a single vector q. Then, FINOLA automatically regresses from the vector q placed in the center to the feature map through two partial differential equations. Experiments show that this pre-trained representation excels in various downstream tasks, including image classification and object detection, without the need for extensive fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method sounds interesting and novel. Its structure is simple but it shows powerful representation ability in spatial representation.\n2. This paper provides a new perspective to describe the intrinsic relationship of image feature maps through partial differential equations. This may have some implications for simplifying neural networks.\n3. Expensive experiments prove the effectiveness of this method, especially in image classification and object detection tasks."
                },
                "weaknesses": {
                    "value": "1. The author mentioned \"this intriguing property reveals a mathematical invariance\". What does invariance refer to should be further explained.\n2. The author mentioned \"providing insights into the underlying mathematical principles\" more than once, but did not provide an in-depth explanation or comparison. It is recommended to provide more details.\n3. When validating the norm+linear approach, the authors repeated q by W \u00d7 H times. In the original setting, the authors learned two matrices A and B, with more parameters. Thus, the comparison seems a bit unfair.\n4. In comparable performance with Stable Diffusion, the input of the paper method is an image of the same size as the output, while the input of stable diffusion is a lower resolution image. The tasks of the two are different, so it may not be appropriate to compare them together."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808300457,
            "cdate": 1698808300457,
            "tmdate": 1699636379326,
            "mdate": 1699636379326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vU1OhK6LKj",
                "forum": "o87xfYKQC1",
                "replyto": "AYJBKUQXTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n**Q1: The author mentioned \"this intriguing property reveals a mathematical invariance\". What does invariance refer to should be further explained.**\n\nThe term \"mathematical invariance\" refers to the ***consistent relationship shared by all images between feature values and their spatial derivatives on the feature map***. Mathematically, for any given image, the feature values $\\mathbfit{z}(x, y) \\in \\mathbb{R}^C$ at any position $(x, y)$ on the feature map determine its spatial derivatives as follows:\n\n$\\mathbfit{z}(x + 1, y) - \\mathbfit{z}(x, y) = \\mathbfit{A}\\mathbfit{z}_n(x, y)$,\n\n$\\mathbfit{z}(x, y + 1) - \\mathbfit{z}(x, y) = \\mathbfit{B}\\mathbfit{z}_n(x, y)$,\n\nwhere $\\mathbfit{z}_n(x, y) = \\frac{\\mathbfit{z}(x, y) - \\mu_z}{\\sigma_z}$.\n\nIt's essential to note that matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ exhibit ***invariance*** across different images and pixels on the feature map, emphasizing the consistent mathematical relationship that holds universally.\n\n---\n**Q2: The author mentioned \"providing insights into the underlying mathematical principles\" more than once, but did not provide an in-depth explanation or comparison. It is recommended to provide more details.**\n\nThank you for the valuable suggestion. Allow us to provide a more detailed explanation. The crux of our mathematical insights lies in the understanding that the feature maps of all images adhere to two partial differential equations (PDEs):\n\n$\\frac{\\partial \\mathbfit{z}}{\\partial x}=\\mathbfit{A}\\mathbfit{z}_n, \\quad \\frac{\\partial \\mathbfit{z}}{\\partial y}=\\mathbfit{B}\\mathbfit{z}_n$\n\nThese equations reveal that the spatial rate of change, or derivatives, in the feature map is entirely determined by the feature values at the current location. It's essential to note that matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ exhibit ***invariance*** across different images and pixels on the feature map, emphasizing the consistent mathematical relationship that holds universally.\n\nAnother mathematical insight is related to the role of masking, particularly in the context of masked FINOLA for self-supervised learning. This technique plays a crucial role in facilitating the extraction of meaningful semantic representations from the feature maps. However, it comes with an inherent trade-off, impacting the preservation of intricate image details. Notably, compared to vanilla FINOLA, masked FINOLA exhibits a substantial increase in Gaussian curvature on the surfaces of critical features. This observed increase suggests a heightened curvature in the latent space, underlining its effectiveness in capturing and emphasizing semantic information.\n\n---\n**Q3: When validating the norm+linear approach, the authors repeated q by W \u00d7 H times. In the original setting, the authors learned two matrices A and B, with more parameters. Thus, the comparison seems a bit unfair.**\n\nThe objective of this comparison is to demonstrate the superiority of using norm+linear over its *absence*. Repetition, requiring zero additional effort, naturally stands out as a straightforward option. In this context, our intention is to specifically exclude the scenario where the zero-effort repetition achieves performance comparable to our more complex norm+linear approach. Such a scenario would contradict the essential need for norm+linear. The poor performance of repetition serves as evidence that norm+linear cannot be easily replaced by a simpler and more straightforward solution, highlighting the necessity of our chosen approach, even in the presence of additional parameters when learning matrices $\\mathbfit{A}$ and $\\mathbfit{B}$.\n\n---\n**Q4: In comparable performance with Stable Diffusion, the input of the paper method is an image of the same size as the output, while the input of stable diffusion is a lower resolution image. The tasks of the two are different, so it may not be appropriate to compare them together.**\n\nLet's clarify the comparison with Stable Diffusion. Our comparison is specifically with the ***first stage*** of Stable Diffusion, known as *perceptual image compression*, which involves training an autoencoder to reconstruct images. We explicitly refrain from comparing with the second stage of Stable Diffusion, which includes generation of super-resolution from lower-resolution images. The comparison is thus limited to the common task of image reconstruction and does not involve tasks related to super-resolution from lower-resolution inputs."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213707428,
                "cdate": 1700213707428,
                "tmdate": 1700213707428,
                "mdate": 1700213707428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "98Ubc2cCMz",
            "forum": "o87xfYKQC1",
            "replyto": "o87xfYKQC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_P6tB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_P6tB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an auto-regression model in latent feature space using norm-linear transform to regress the features in high-resolution from a global feature. The proposed norm-linear transform is simple to regress features, and can be decoded with lightweight network to generate the reconstructed image. The proposed model can also be applied as a pre-training method, having good generalization ability to recognition and object detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  The auto-regression for generating features in high-resolution in feature space is an interesting idea, and the proposed regression model is simple. The regression model in feature space can be seen as a discretized PDE. \n\n2. The proposed model can be taken as a self-supervised leaning approach based on mask region prediction. The sufficient experiments show that it can achieve good pretraining results for recognition and detection."
                },
                "weaknesses": {
                    "value": "1.  There are previous regression-based generative models (e.g., refer to related works) in feature space, and what are the major difference and advantage of this approach compared with these models? Is it possible to compare with them for the generation quality and computational speed?\n\n2. Are the matrix A and B shared for all different images and pixels in the feature space? If it is, why learned constant A, and B can deduce good feature regression?\n\n3. The paper states that this work does not aim to achieve SoTA results, however, comparisons with SoTA regression models or other variants, e.g., using nonlinear regression instead of linear transform, should be able to better give insights to audience.\n\n4. The regression model should gradually regression dense feature maps? How about the computational overhead/speed for training and inference using this model?\n\n5. What is the meaning of the mathematical invariance in this paper for the proposed model?"
                },
                "questions": {
                    "value": "Please see my questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847962256,
            "cdate": 1698847962256,
            "tmdate": 1699636379246,
            "mdate": 1699636379246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BbYK5WAPyr",
                "forum": "o87xfYKQC1",
                "replyto": "98Ubc2cCMz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n**Q1: There are previous regression-based generative models (e.g., refer to related works) in feature space, and what are the major difference and advantage of this approach compared with these models? Is it possible to compare with them for the generation quality and computational speed?**\n\nFirst and foremost, it's essential to clarify that FINOLA does *not* function as a generative model. Our primary objective is not image generation but rather the revelation of mathematical properties within the latent space. Image reconstruction, one of our tasks (alongside self-supervised learning), serves as a means to validate FINOLA \u2014 a first-order norm+linear autoregressive model \u2014 as a fundamental mathematical property shared by all images.\n\nIt's crucial to note that FINOLA is both functionally and technically distinct from autoregression-based generative models. The latter typically consists of two stages: the first stage involves learning an autoencoder and vector quantization in the latent space, while the second stage focuses on learning autoregression in the quantized latent space. In contrast, FINOLA operates as a single-stage model, seamlessly learning the encoder, autoregression, and decoder end-to-end, with autoregression performed on a continuous (non-quantized) latent space.\n\nGiven that FINOLA doesn't fall into the category of generative models, it isn't suitable for direct comparison in terms of generation quality and computational speed with models designed for such purposes.\n\n---\n**Q2: Are the matrix A and B shared for all different images and pixels in the feature space? If it is, why learned constant A, and B can deduce good feature regression?**\n\nYes, the matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ are shared for all images and pixels in the feature space.\n\nThe success of using constant $\\mathbfit{A}$ and $\\mathbfit{B}$ for achieving good feature regression can be attributed to three primary factors. Firstly, images inherently exhibit significant spatial redundancy, leading to strong correlations between consecutive positions within the feature map. Secondly, our hypothesis regarding the existence of a feature map where spatial derivatives correlate with feature values appears to be supported. Lastly, the effectiveness of deep learning allows us to validate this hypothesis by seamlessly learning the encoder, FINOLA, and decoder components in an end-to-end manner.\n\n---\n**Q3: The paper states that this work does not aim to achieve SoTA results, however, comparisons with SoTA regression models or other variants, e.g., using nonlinear regression instead of linear transform, should be able to better give insights to audience.**\n\nExcellent point! We have conducted a comparison between our norm+linear approach and non-linear regression, which involves replacing the linear transform with two MLP layers incorporating GELU activation in between. This evaluation was performed for both 1024 and 3072 channels. Surprisingly, our norm+linear, despite having fewer parameters, closely trails the performance of the two-layer MLP. This result underscores the effectiveness of norm+linear in modeling the transition between pixels in the feature map.\n\n|Regression|C=1024|C=3072|\n|---|---|---|\n|Non-Linear (2 layer MLP)|23.8|25.8|\n|**Linear (our)**|23.7|25.8|\n\nWe further benchmark our method against an alternative non-linear solution: employing a multi-layer convolutional approach to upsample the vector $\\mathbfit{q}$ to a 16x16 feature map. This method undergoes four upsample stages, each incorporating three 3x3 convolution layers. The results below  demonstrate the superior performance of a single layer FINOLA over the multi-layer convolutional baseline.\n\n|Regression|C=1024|C=3072|\n|---|---|---|\n| convolution | 23.4 | 25.1 |\n| **FINOLA (our)** | **23.7** | **25.8** |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212883491,
                "cdate": 1700212883491,
                "tmdate": 1700212883491,
                "mdate": 1700212883491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Csobi6qa8W",
            "forum": "o87xfYKQC1",
            "replyto": "o87xfYKQC1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_JB6i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4138/Reviewer_JB6i"
            ],
            "content": {
                "summary": {
                    "value": "This paper is proposing First-Order Norm+Linear Autoregressive (FINOLA). The proposed method is a new type of autoregressive model that can be used for self-supervised representation learning. The comprehensive experiments show that FINOLA has relatively small parameters but can contain enough information for image reconstruction. The authors also show that FINOLA can be used as a feature extractor for the downstream task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Reasonable and understandable writing.\n- technically novel.\n- Comprehensive experiments."
                },
                "weaknesses": {
                    "value": "- Justifications for the novel design choice (but seems heuristic) compared to the regular AR model.\n    - Predefined assignments in Block-wise Masked FINOLA\n    - Predicting three points in Block-wise Masked FINOLA\n    - Why first order?\n- Lack of interpretation of the derived PDE.\n- Some parts are unclear (please see the Questions below)"
                },
                "questions": {
                    "value": "1. (page 2) The authors argued that \u201cthe coefficient matrices $A$ and $B$ capture the relationship between each position\u201c. How do the coefficient matrices directly know the spatial relationship? They do not take any of the spatial information. I believe there are some pieces of missing information:\n    1. coefficient matrices $A$ and $B$ capture channel-wise relationship.\n    2. The pattern and correlation encoded in the channel are related to the positional information. \n    3. the coefficient matrices $A$ and $B$ (indirectly) capture the relationship between each position.\nMaybe it is trivial for some readers but it is not for me.\n\n2. There are some of the questions about PDE.\n    1. (page 4) \u201cThey represent a theoretical extension of FINOLA from a discrete grid to continuous coordinates.\u201c This is not intuitive to me. As far as I understand, the proposed method in this paper is also using the discrete grid. Assuming that the extension to continuous coordinates is to get a better theoretical understanding, what is the insight and take away from the fact that Eq. 1 becomes the formulation of PDE in Eq. 4?\n    2. (page 4) \u201cEstablishing their theoretical validity poses a substantial challenge.\u201d  What is the substantial challenge?\n3. The specific method of the block-wise Masked FINOLA is unclear. For example, if we see the Corner case, let\u2019s say an input coordinate is (3,2) and it is used for predicting {(11,10), (11,2), (3,10)}. I guess for obtaining (11,2), for instance, the function $\\phi^8(z(3,2))$ is applied, which means (4,2) is predicted first and it is used as an input for predicting (5,2) and so forth. My question is, considering that we already have a ground truth within (0,2),(1,2) \u2026, (7,2), why not use the ground truth information?\n4. How is the Gaussian curvature related to capturing semantics? Could you add more detailed descriptions of how it is computed?\n5. (Table 4 (a)) Even though Stable Diffusion is a Generative Model, I believe it should have better PSNR than FINOLA for the image reconstruction task. How did you implement the image reconstruction task for Stable Diffusion?\n6. How fast is the parallel implementation (Fig. 3) compared to the regular AR setting?\n7. Is this method fast enough to use as a feature extractor for the downstream task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4138/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698961145579,
            "cdate": 1698961145579,
            "tmdate": 1699636379185,
            "mdate": 1699636379185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y99ljFXRf4",
                "forum": "o87xfYKQC1",
                "replyto": "Csobi6qa8W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4138/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and effort to provide feedback on our work. Below, we answer the questions that have been raised.\n\n---\n**Weakness 1: Justifications for the novel design choice (but seems heuristic) compared to the regular AR model.**\n\nThanks for the suggestion. Below we provide justification for each item.\n\n***Predefined assignments and predicting three points in Block-wise Masked FINOLA***\n\nThis design is meticulously crafted to achieve several key objectives: (a) Full coverage: ensuring that all masked positions receive attention, (b) Balance: distributing the usage of unmasked positions equally, and (c) Efficiency: utilizing each unmasked position to predict three masked positions, especially under the condition where 75% of positions are masked. Additionally, this design can be implemented in a block-wise manner to further enhance efficiency, a crucial consideration for the extended training durations involved in self-supervised learning.\n\n***Why first order?***\n\nThe selection of first-order autoregression is driven by its inherent simplicity and local property, where the derivatives along the $x$ and $y$ axes are exclusively determined by the current position. This strategic choice not only simplifies the computational process but also aims to unveil valuable mathematical insights that could be shared across different images, provided the approach proves effective.\"\n\n---\n**Weakness 2: Lack of interpretation of the derived PDE.**\n\nThe derived partial differential equations (PDEs) in Eq. 4 provide a theoretical extension of FINOLA from a discrete grid to continuous coordinates, establishing a conceptual bridge between the discrete representations of neural networks and the continuous nature of human vision. Although our digital images and feature maps are inherently discrete, the human vision system perceives continuous signals.\n\nThis theoretical extension allows us to hypothesize about the mathematical properties of continuous feature representations in the human vision system. We posit that the proposed PDEs in Eq. 4, derived from their discrete counterparts in Eq. 1, capture the patterns inherent in continuous features within human vision.\n\nThis is related to **Q2** below.\n\n---\n**Q1: How do the coefficient matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ directly know the spatial relationship? They do not take any of the spatial information.**\n\nThe coefficient matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ encode the relationship between a ***feature vector*** $\\mathbfit{z}(x, y) \\in \\mathbb{R}^C$ and and ***its spatial derivatives***, namely $\\mathbfit{z}(x+1, y)-\\mathbfit{z}(x, y)$ and $\\mathbfit{z}(x, y+1)-\\mathbfit{z}(x, y)$. This relationship is mathematically expressed as:\n\n$\\mathbfit{z}(x + 1, y) - \\mathbfit{z}(x, y) = \\mathbfit{A}\\mathbfit{z}_n(x, y)$,\n\n$\\mathbfit{z}(x, y + 1) - \\mathbfit{z}(x, y) = \\mathbfit{B}\\mathbfit{z}_n(x, y)$,\n\nwhere $\\mathbfit{z}_n(x, y) = \\frac{\\mathbfit{z}(x, y) - \\mu_z}{\\sigma_z}$.\n\nIt's essential to note that matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ exhibit invariance across different images and positions $(x,y)$ within the feature map once they are learned from the data.\n\n***1.1 Coefficient matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ capture channel-wise relationship.***\n\nIndeed, the difference of the channel vector between consecutive positions, such as $\\mathbfit{z}(x + 1, y) - \\mathbfit{z}(x, y)$, represents a linear combination of normalized channel values at the current position $\\mathbfit{z}_n(x, y) \\in \\mathbb{R}^C$.\n\n***1.2: The pattern and correlation encoded in the channel are related to the positional information.***\n\nThe correlation does *NOT* pertain to the positional information; instead, it is associated with the derivatives (or rate of change) along the $x$ and $y$ axes, often referred to as *\"spatial derivatives\"*.\n\n***1.3: the coefficient matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ (indirectly) capture the relationship between each position. Maybe it is trivial for some readers but it is not for me.***\n\nAs discussed earlier, the matrices $\\mathbfit{A}$ and $\\mathbfit{B}$ explicitly capture the relationship between the feature at a given position $\\mathbfit{z}(x, y)$ and its derivatives along the $x$ and $y$ axes. Consequently, this encoding implicitly extends to capturing the relationship between consecutive positions, such as from $\\mathbfit{z}(x, y)$ to $\\mathbfit{z}(x + 1, y)$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4138/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210590137,
                "cdate": 1700210590137,
                "tmdate": 1700210590137,
                "mdate": 1700210590137,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]