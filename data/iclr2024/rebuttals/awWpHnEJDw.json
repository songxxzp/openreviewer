[
    {
        "title": "The Hidden Language of Diffusion Models"
    },
    {
        "review": {
            "id": "BJpLv8KHOw",
            "forum": "awWpHnEJDw",
            "replyto": "awWpHnEJDw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use a weighted linear combination of existing word embeddings to represent an image (i.e., weights are optimized through two MLP layers) such that it enables image decomposition based on a set of human understandable tokens using pretrained text-to-image diffusion model such as Stable diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the motivation of the paper is clear and well conveyed.\n- the experiments are well-conducted and extensive.\n- The approach enables human interpretable decomposition using a set of learnable weights and associated tokens, which has inherited the same outcome from the classic word2vec arithmetic paper [1].\n\n[1] Mikolov et al., Efficient Estimation of Word Representations in Vector Space. ICLR 2013"
                },
                "weaknesses": {
                    "value": "- lack of related previous work on image decomposition, which can be seen as a way to interpret models, such as FineGAN (Sing et al, 2019), GIRAFFE (Niemeyer etal, 2021), SlotAttention (Locatello etal, 2020), DTI Sprites (Monnier etal, 2021), GENESIS-V2 (Engelcke etal, 2021) and follow-up works. There also exists earlier/concurrent work that conducts image decomposition using text-diffusion models, so its worth discussing pros and cons but may not need comparison if works are concurrent.\n- most of concepts shown in the paper are mainly objects, thus its ability to learn abstract concepts is not clear. For example, how does it perform on abstract concepts such as object relationships. Though, I tend to think that the model is rather limited in understanding complex concepts other than objects.\n- The method essentially utilizes arithmetic with word embeddings which have been widely used in the past, so it doesn't seem to be novel enough. Applying this method to a text-to-image diffusion model doesn't show novelty from my perspective."
                },
                "questions": {
                    "value": "- I'd love to see if u can optimize on electrician image and then try removing the brush concept, does it become a painter? \n- How long does this optimization process take for each image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4002/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv",
                        "ICLR.cc/2024/Conference/Submission4002/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592809733,
            "cdate": 1698592809733,
            "tmdate": 1700579979237,
            "mdate": 1700579979237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xuqmexUmGd",
                "forum": "awWpHnEJDw",
                "replyto": "BJpLv8KHOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review! (response part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comprehensive feedback and for the interesting points for discussion. We are encouraged that the reviewer found our work to be well-motivated and recognized the extensiveness and effectiveness of our experiments. \nIn the following, we provide a response to each of the weaknesses pointed out in the review. Additionally, the modifications in the revision are marked in red, for your convenience.\n\n__Re. Related works on image decomposition:__\nWe thank the reviewer for bringing this to our attention. We have revised the paper to include the proposed works in the related works section.\n\nTo reiterate the difference between the objective of these methods and our method, we enclose a short comparison to the methods indicated above:\n\nAs a general note, all methods described below only offer decomposition for a single image and, therefore, fall short of providing concept-level explanations, which is the main objective of concept-based explainability.\n\n**FineGAN**: disentangles background, foreground, shape, and appearance. This method is not applicable to interpret SD since (1) it\u2019s GAN-based and therefore limited to the training domain, (2) the disentangled information provides little to no interpretable information that is not observable just by looking at the images.\n\n**GIRAFFE**: similar to FineGAN, GIRAFFE disentangles objects, appearance, shape, and background information, with the objective of obtaining controllable generation. In addition to the points mentioned above, this method incorporates 3D scene representations, which are not applicable in our case.\n\n **Slot Attention**: learns object-centric representations for complex scenes, and employs them for object discovery and set prediction. The task of object discovery has some similarities to ours but is also very different, see below. Additionally, the datasets used for training (CLEVR6, Multi-dSprites, Tetrominoes) differ significantly from the distribution of images generated by SD.\n\n**DTI Sprites**: similar to Slot Attention, DTI Sprites decomposes a scene into objects including their shape, size, and color (appearance), which allows improved controllable generation.\n\nInspired by the review, we conducted an additional experiment to demonstrate the difference between our objective and existing methods for image decomposition. The closest relation we observed between the proposed methods and ours is through the task of unsupervised object discovery (including shapes, colors, sizes, etc.). Importantly, our task largely differs from that of object discovery, since an interpretation should often contain elements *beyond* objects that are physically present in the image. For example, Fig. 2 shows connections such as \u201csnake\u201d to \u201cgecko\u201d + \u201ctwisted\u201d, a \u201cgecko\u201d is not present in the image, and \u201ctwisted\u201d is *not an object but an adjective*. Similarly, \u201ccashmere\u201d is not present in an image of a \u201ccamel\u201d, and \u201cwinding\u201d is not present in images of \u201csnails\u201d. In contrast to these semantic, profound connections, interpretation using object discovery only provides representations that are based on concrete objects and parts that are visible in the image. Therefore, these methods fall short of discovering deeper connections (e.g., reliance on exemplars, non-trivial biases, etc.).\n\nTo empirically demonstrate this point, we added exemplary results using [1], which is the state-of-the-art object discovery method based on SD, and allows to discover concepts from sets of images, as well as single images. The results of this exploration of two exemplary basic concepts, \u201csnail\u201d, \u201csnake\u201d and an additional complex concept \u201cimpression of Japanese serenity\u201d, are presented in Appendix G of the revision. This experiment directly demonstrates our intuition above. As can be seen, the method learns to embed the concept in a single token (as expected from its task definition for images of one class) and does not employ any other learned tokens. \n\n[1] \u201cUnsupervised Compositional Concepts Discovery with Text-to-Image Generative Models\u201d, Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, Antonio Torralba, ICCV\u201923."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699804255347,
                "cdate": 1699804255347,
                "tmdate": 1699805551442,
                "mdate": 1699805551442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YuofC5UQSp",
                "forum": "awWpHnEJDw",
                "replyto": "BJpLv8KHOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review! (response part 2/3)"
                    },
                    "comment": {
                        "value": "__Re. Forming abstract connections beyond objects:__\nKindly note that our examined concepts include an extensive set of both very complex and abstract concepts (e.g., \u201celegance on a plate\u201d, \u201chappiness\u201d, \u201caffection\u201d, \u201cimpression of Japanese serenity\u201d, etc.). A full list of these concepts can be found in Appendix B (see Emotions, Actions, Complex concepts). Following the review, we have added word cloud visualizations of the learned decompositions for abstract and complex concepts (see Appendix F). The provided examples show that our method can meaningfully decompose abstract concepts and detect relations to other abstract concepts (e.g., linking \u201chappiness\u201d to \u201cdream\u201d, \u201cemotion\u201d, \u201csoul\u201d, \u201claughter\u201d, \u201cchildhood\u201d; or linking \u201csadness\u201d to \u201cgrief\u201d, \u201cdepression\u201d, \u201cworried\u201d, \u201calone\u201d etc.). These results indicate that Conceptor is capable of revealing connections between abstract concepts.  Examples of single-image decompositions for such abstract concepts can be found in Fig. 2 (\u201cfear\u201d, \u201cimpression of Japanese serenity\u201d) and in Fig. 8 (\u201chappiness\u201d).\n\n__Re. The difference with word arithmetics:__\nWe thank the reviewer for raising this interesting point for discussion.\nWe wish to emphasize that our method does not perform word embedding decomposition, and in fact, significantly differs from it. In the following, we list the fundamental differences between the two:\n\n1. **Our learned pseudo-token $w^{\\ast}$ does not decompose the text embedding of the concept $w^c$, i.e.: $w^{\\ast}= \\sum_i w_i \\alpha_i \\neq w^c$:** \n\n      Kindly note that our method does not decompose the word embedding of the concept token(s) $w^c$. Instead, we propose to decompose the internal representation of the concept by the model. This is done by aggregating the set of features used by the model in the image generation process (see Sec. 3). The text embeddings serve merely as an intermediate language linking us to the uninterpretable inner representations of SD. To empirically prove this point, we calculated the cosine similarity between our learned pseudo-token $w^*$ and the concept token $w^c$ averaged across our entire dataset, and obtained a low similarity score of *0.61* (for reference, \u201ccat\u201d and \u201ccar\u201d get a higher similarity score of 0.66). This result, combined with our Token Diversity metric (Tab. 1), our single image decomposition results (Fig. 2) and our CLIP top words ablation (Tab. 3) demonstrate that our method is able to learn connections based on visual semantics, that transcend word arithmetics, as mentioned in the paper (see Sec. 1, 6).\n\n1. **Our single-image decomposition results do not suggest word embedding arithmetics:** \n\n     Note that the annotations in our single-image decomposition figures (Figs. 2,8) do not indicate word embedding arithmetics. For example: $word\\textunderscore embed(painter) \\neq word\\textunderscore embed(electrician) + word\\textunderscore embed(brush) + word\\textunderscore embed(Monet)$ (the cosine similarity between \u201cpainter\u201d and \u201celectrician + brush + Monet\u201d is merely 0.496). This indicates that the decomposition is not based on textual semantics but on an inner-representation decomposition.\n\n      Additionally, note that a diffusion model generates *diverse* images for the same concept given different random seeds, and not all learned features are manifested in all of the concept images (e.g., the images of the nurses in Fig. 12 can be in black and white or in color, with scrubs or a white suit, with and without a stethoscope, etc). Our single-image decomposition scheme extracts the specific set of elements from the decomposition learned by Conceptor that are used to generate a single specific image. This is evident when different features appear for single-image decompositions of different images of the same concept, e.g., the \u201ccamel\u201d and \u201csnake\u201d examples in Fig. 2 vs. Fig. 8. Thus, the arithmetic annotations in Figs. 2, 8 indicate the addition of the features of the decomposition elements to the output image.\n\n    This ties directly to the question by the reviewer:\n     > \u201cI'd love to see if u can optimize on electrician image and then try removing the brush concept, does it become a painter?\u201d. \n\n     The answer is that we are not doing word arithmetics and this manipulation is not possible, since applying Conceptor to the concept \u201celectrician\u201d results in a decomposition that does not include the element \u201cbrush\u201d. This is logical: SD does not employ the feature \u201cbrush\u201d in the generation of images of electricians, unlike the role \u201cbrush\u201d plays for the \u201cpainter\u201d concept."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699805264760,
                "cdate": 1699805264760,
                "tmdate": 1700689242296,
                "mdate": 1700689242296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PgtbI9tDd3",
                "forum": "awWpHnEJDw",
                "replyto": "BJpLv8KHOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review! (response part 3/3)"
                    },
                    "comment": {
                        "value": "__Re: Novelty:__ As mentioned and demonstrated above, our novel decomposition formulation has not been attempted before and differs significantly from other image decomposition objectives such as object/concept discovery. As we established, Conceptor also fundamentally differs from word embedding arithmetics. \n\nOur experiments demonstrate that Conceptor has significant value in exposing mixed visual/textual components in a way that, as far as we can ascertain, has never been shown before. For example, Conceptor exposes reliance on exemplars and mimicking of artistic styles and reveals concepts that are incorporated purely for visual reasons such as shape or texture, non-trivial biases, and the way textual ambiguities are handled.\n\nFrom a technical point of view, we believe that there is a considerable novelty in our decomposition strategy, e.g., in the combination of Eqs. 3 and 4, in which *the coefficients $\\alpha$ are modeled as a function of the word embedding $w$* (see Sec. 3). These unique choices are extensively ablated in Sec. 4.2.1, where we empirically demonstrate that they are all crucial to maintaining the faithfulness and meaningfulness of our method. Our single-image decomposition scheme introduces additional innovation by enabling interpretation for a specific generation, in addition to the general concept interpretation provided by our method.\n\n__Re. Optimization length per image:__ Please note that Conceptor *does not operate on a single image but on an entire concept* (see Sec. 3). The optimization process for the entire concept takes around 6.5 minutes on a single A100 GPU with 40GB (given more memory, one could significantly increase the batch size and speed up the process).\n\nAfter this optimization, manipulation of the obtained coefficients for a given image is dominated by the time it takes to generate the image (around 4 seconds, again depending on the hardware used).\n\nWe are happy to address any other questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699805532264,
                "cdate": 1699805532264,
                "tmdate": 1699805568242,
                "mdate": 1699805568242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wr9nf8FWCX",
                "forum": "awWpHnEJDw",
                "replyto": "PgtbI9tDd3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
                ],
                "content": {
                    "comment": {
                        "value": "1. Thank you for adding image decomposition to the related work and the additional baseline to showcase your method.\n2. The main reason why I mentioned \"word arithmetics\" is because: since you select a set of $n$ tokens to optimize, such $n$ tokens will be in a vector subspace $\\leq N^m$ where $m <= n$. As a result, no matter how you optimize it, it will be always in that subspace. Suppose they lie in the subspace of $N^m$  and the linearly independent vectors that make up that subspace are $c_1, \\dots, c_n$, then your final $w^{*}$ will always be some weighted linear combination of $c_1, \\dots, c_n$. In that case, your method is essentially word arithmetics but with learnable parameters from my interpretation, though $c_1, \\dots, c_n$ may not human interpretable concepts. After parameters are learned, it seems to be exactly word arithmetics in that sense in test time where you use \"+/-\" to add or remove concepts. Please correct me if I am wrong or misunderstood it.\n3. Thank you for addressing other questions and concerns. Can't think of any questions right now, but will come back for questions if I do have any additional ones."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409980660,
                "cdate": 1700409980660,
                "tmdate": 1700409980660,
                "mdate": 1700409980660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lYFWKq5e0x",
                "forum": "awWpHnEJDw",
                "replyto": "BJpLv8KHOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your response to our rebuttal! We appreciate the continued discussion.\n\nIn addressing the second point, the literature on word arithmetics shares with our method the use of a vocabulary in which each word is embedded as a vector in some vector space. However, the two lines of research diverge at this point, and it is crucial for us to emphasize the novelty of our method:\n\n\n1. Unlike the goal of word embedding arithmetics, we do not measure distances in the vector space. Specifically, our decomposition of $w^c$ is not intended to find a weighted sum of dictionary elements equal to it. Instead, we measure the reconstruction loss of generated images.\n\n2. Our method does not optimize the set of coefficients directly. Instead, it operates by learning a mapping function between each word embedding and a corresponding coefficient. In other words, our method can be viewed as a *nonlinear mapping* of the form: $w^* = \\sum_i f(w_i) w_i$, where $f$ is a nonlinear function. Again, this is different from word arithmetics, which considers linear operations in the vector space.\n\n    Our extensive ablations (Sec. 4.2.1, Tab. 3 \u201cwithout MLP\u201d) show that this novel form of learning the decomposition is critical to the success of the method, as without it (i.e., learning separate coefficients) the decompositions are not faithful. This ablation shows that the task of assigning the appropriate coefficients to word embeddings to reconstruct the learned representation is not trivial.\n\n3. As a result of 1+2, the tokens obtain their meaning from the context of the decomposition and not from their single-word meaning (see also Fig. 5). For example, as a standalone token, \u201cwinding\u201d generates an image of winding roads and spirals, however in the context of the decomposition for \u201csnail\u201d its meaning is transformed to the shape of the snail\u2019s shell. \u201cCivilization\u201d paired with \u201csphere\u201d turns a simple sphere into planet Earth, etc. \n\n     To further substantiate this point empirically, we enclose the results of simple word arithmetics (a+b) for examples of concepts from Fig 2. The connection \u201creflections of earth = sphere + civilization\u201d is only made possible by the learned coefficients. When considering only the word embeddings, [these are the resulting images](https://i.imgur.com/6ai71Ww.jpg) (which do not resemble planet Earth whatsoever). Similarly, \u201csnake = twisted + gecko\u201d would yield [these images without the appropriate coefficients](https://i.imgur.com/1UYOham.jpg), for the combination \u201csnail = winding + ladybug\u201d [these are the resulting images](https://i.imgur.com/EB4S5ya.jpg), etc. \n\n4. Finally, we would like to point out that there are many other lines of research, such as dictionary learning, that learn a combination over a set of dictionary vectors. Therefore, we do not believe that the fact that we are optimizing a combination of tokens is enough to classify our work as a form of word embedding arithmetics. As we showed above, our method significantly differs from word embedding arithmetics in its goal, implementation, and objective function.\n\n\n\n\nOverall, we believe that our obtained results are non-trivial and profound, and are entirely different from those obtained by traditional word embedding arithmetics (see our mentioned ablation above). We show that SD can blend concepts in a way in which each concept plays a completely different role that can be image-based (shape, texture, color), semantic (the essence of the concept), or abstract (describing a property of the concept). This is, in our view, surprising and quite remarkable. One may not agree with our conclusions, but given the magnitude of evidence we present, which was increased by the various additional experiments conducted following the reviews (OOD experiments, another text-to-image model, robustness to the loss function, etc.), we find it hard to ignore.\n\nIf you found our previous response and this one to be mostly satisfactory, we kindly ask you to consider increasing your rating in recognition of the strengths and extensiveness of our evaluation, and the value of our work to the research community as the first interpretability work for text-to-image generative models."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418700326,
                "cdate": 1700418700326,
                "tmdate": 1700419422017,
                "mdate": 1700419422017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0XOgWC9mol",
                "forum": "awWpHnEJDw",
                "replyto": "BJpLv8KHOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_vrPv"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. \n\nI have increased my score from 5 to 6."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579957411,
                "cdate": 1700579957411,
                "tmdate": 1700579994746,
                "mdate": 1700579994746,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AtY8QAgNS1",
            "forum": "awWpHnEJDw",
            "replyto": "awWpHnEJDw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_nkKA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_nkKA"
            ],
            "content": {
                "summary": {
                    "value": "The paper \u2018Hidden Language of Diffusion Models\u2019 designs an interpretability framework for text-to-image generative models. This framework relies on learning coefficients of word-embeddings such that the reconstruction loss in diffusion models is minimized with additional constraints to ensure sparsity of concepts which are selected.  Overall, the paper provides a simple framework to decompose concepts into sub-concepts to interpret diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is extremely well-written and easy to follow throughout. Good job on this!\n- The idea about learning the coefficients of the word-embeddings while minimizing the diffusion reconstruction loss is very simple and easy-to-implement while producing good interpretability understanding. It can be a good tool to interpret diffusion models through the lens of how concept dissection works in text-to-image generative models. \n- Given that there does not exist significantly good benchmarks on testing concept-decomposition and also baselines, the authors have done a satisfactory job of comparing with PEZ and other heuristic variants using BLIP-2. The ablations are also presented in depth."
                },
                "weaknesses": {
                    "value": "Cons / Questions : \n- While the paper provides a good interpretability framework for image generation through the lens of concepts I have some doubts on it\u2019s downstream application. Can the authors elaborate a little bit on how Conceptor can be used for a particular downstream application (e.g., bias mitigation given that Conceptor can detect biases?)\n- Can the authors elaborate if the concept decomposition is an artifact of the particular CLIP text-encoder in Stable-Diffusion? Will one get similar concept-decomposition patterns if a different text-encoder is used (e.g., T5 like in DeepFloyd)? I would imagine this to be a positive answer, but might expect different patterns, so I believe it\u2019s important to use Conceptor to understand this phenomenon.\n- I am a little curious about how much the diffusion objective plays a role in concept-decomposition. For e.g., given the objective of reconstruction, I will expect the faithfulness metric of Conceptor to be better than other methods (e.g., PEZ). However, if you use the same idea with CLIP loss (replacing the reconstruction loss in Eq. (6) with L_clip), will you get similar decomposition? And will those decompositions transfer to diffusion models?  In fact, if you use CLIP's representation for a particular token as a ground-truth with optimizing for Eq.(3), you should get a reasonable reconstruction still, which can be a cheap baseline. Did the authors run this ablation?\n- How can you extend your framework to more complex concept-decomposition? The current framework generates images corresponding to single concepts, but images are usually consisting of multiple concepts. In this scenario, how can one use Conceptor to understand sub-concepts? I think this is one experiment, the paper is lacking."
                },
                "questions": {
                    "value": "Refer to the previous section. \n\nOverall, I feel that the paper is good but will like the authors to respond to the Cons/Questions.  The major question I have about this framework,(i)  is how can it be used to mitigate some of the issues in diffusion models (e.g., bias)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674196283,
            "cdate": 1698674196283,
            "tmdate": 1699636362318,
            "mdate": 1699636362318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "38XQT9fwvJ",
                "forum": "awWpHnEJDw",
                "replyto": "AtY8QAgNS1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review! (response part 1/3)"
                    },
                    "comment": {
                        "value": "__Re. Downstream applications (with emphasis on bias mitigation):__ First, as the reviewer kindly pointed out, Conceptor enables human-understandable interpretations for diffusion models.\nOther than the useful downstream tasks (described below), we believe that interpreting diffusion models is an imperative, timely, and underexplored task on its own since these models are now being employed widely. The insights obtained by Conceptor can help both end-users and researchers to use these models responsibly while being aware of their shortcomings. Thus, we believe that interpreting generative models is, by itself, a worthy cause with much interest to our community.\n\nWith that, Conceptor also enables useful downstream tasks such as concept debiasing via semantic concept editing, that we are happy to elaborate on. As mentioned in the qualitative results section (Sec. 4.1), Conceptor allows one to control each element in the decomposition by manipulating its corresponding coefficient. The magnitude of the coefficient controls the extent to which the element is manifested in the image. Fig. 3 demonstrates such manipulations. For example, one can decouple the two meanings of a dual-meaning concept (see first two rows of Fig. 3) or remove an undesired property from the concept (e.g., \u201cnerdy\u201d for \u201cprofessor\u201d). \n\nAs mentioned in Sec. 4.3, this ability is also useful for bias mitigation. Once a bias is identified by Conceptor, the user can choose to \u201cswitch off\u201d the biased elements by simply lowering their corresponding coefficients until an equal representation is achieved. Such examples can be found in Appendix H. For better readability, we have edited the Appendix to include the biased terms of Fig. 13, see Tab. 7. As can be seen, Conceptor enables debiasing while also maintaining the other image features and the original scene, i.e., Conceptor\u2019s decomposition can surgically remove the biased property without harming the other features.\n\nAdditionally, following the reviews, we have added an experiment showcasing another downstream task. In Appendix J, we explore Conceptor\u2019s ability to provide insight into the representation of Out of Domain (OOD) concepts, defined by a set of image samples. As we demonstrate, our method is able to extract meaningful and profound decompositions even for completely OOD concepts (e.g., the sloth plush in Fig. 16 is linked to the concepts \u201cplush\u201d, \u201cfluffy\u201d, and \u201cknitted\u201d, the monster toy in Fig. 17 is linked to \u201cwoolly\u201d due to its wool-like fur, to the character \u201cDomo\u201d due to the shape of its body, and to a \u201ccrab\u201d and a \u201cshrimp\u201d due to its unique limbs). Kindly refer to our answer to Reviewer HDAw for more details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173881178,
                "cdate": 1700173881178,
                "tmdate": 1700173881178,
                "mdate": 1700173881178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OA1xqm6pR5",
                "forum": "awWpHnEJDw",
                "replyto": "MtYNJOBki3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_nkKA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_nkKA"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "The authors have addressed my questions!  I would maintain my rating therefore!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579304765,
                "cdate": 1700579304765,
                "tmdate": 1700579304765,
                "mdate": 1700579304765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w3NGHUWZ5S",
            "forum": "awWpHnEJDw",
            "replyto": "awWpHnEJDw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_3WNH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_3WNH"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to explore the correlations between different textual concepts, by exploring how well they can help reconstruct images of a certain concept with diffusion models. The method is a variation of textual inversion, by incorporating many words from a vocabulary and learning the weights of words (instead of embeddings a new word)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of finding the correlations between different textual concepts is interesting. The authors presented some interesting results, such as \"snake = twisted + gecko\" (Figure 2)."
                },
                "weaknesses": {
                    "value": "** EDIT** Thanks the author response. I've verified and indeed in the CLIP text space, the triangular similarity relationships are very noisy and seem not reflect true semantic similarity. Therefore I'd raise my rating to 6. As a relevant piece of observation, my original comments on the CLIP triangular relationships are kept as below.\n\n====== original comment ======\n\nMy biggest concern is that it's unnecessary to use image reconstruction as a proxy to find the combination weights (Eq.3). This method can totally work in the CLIP text space only. I've tried to compute the textual similarity of the words presented in Figure 2:\n\nTriplet: 'camel' and 'giraffe'  'cashmere'\n- 'camel' vs 'giraffe': 0.834\n- 'camel' vs 'cashmere': 0.774\n- 'camel' vs 'giraffe' + 'cashmere': 0.872\n\nTriplet: 'snail' and 'ladybug'  'winding'\n- 'snail' vs 'ladybug': 0.768\n- 'snail' vs 'winding': 0.816\n- 'snail' vs 'ladybug' + 'winding': 0.855\n\nTriplet: 'dietitian' and 'pharmacist'   'nutritious'\n- 'dietitian' vs 'pharmacist': 0.878\n- 'dietitian' vs 'nutritious': 0.874\n- 'dietitian' vs 'pharmacist' + 'nutritious': 0.915\n\nTriplet: 'snake' and 'twisted'  'gecko'\n- 'snake' vs 'twisted': 0.869\n- 'snake' vs 'gecko': 0.848\n- 'snake' vs 'twisted' + 'gecko': 0.913\n\nTriplet: 'reflections of earth' and 'sphere'    'civilization'\n- 'reflections of earth' vs 'sphere': 0.761\n- 'reflections of earth' vs 'civilization': 0.804\n- 'reflections of earth' vs 'sphere' + 'civilization': 0.831\n\nTriplet: 'fear' and 'scream'    'wolf'\n- 'fear' vs 'scream': 0.892\n- 'fear' vs 'wolf': 0.875\n- 'fear' vs 'scream' + 'wolf': 0.926\n\nWe can see that for a triplet A,B,C, the similarity of A vs. (B+C) is always higher than A vs B or A vs C. That means similar semantic correlations already exist in the CLIP text embedding space. Intuitively, since CLIP text embeddings are to be aligned with image features, such similarities in the image features will propagate to the text embedding space.\n\nTherefore, doing image reconstruction with T2I diffusion model is unnecessary. If we only mine such triplets from the CLIP text embedding space, then the contribution of this paper becomes quite small. Therefore, I suggest rejection."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4002/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4002/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4002/Reviewer_3WNH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831341980,
            "cdate": 1698831341980,
            "tmdate": 1700581735709,
            "mdate": 1700581735709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sqzv19ZdHr",
                "forum": "awWpHnEJDw",
                "replyto": "w3NGHUWZ5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing the feedback and especially for aiming to validate our assumptions.\n\n1. Kindly note that our method aims to provide an interpretation of the model\u2019s latent representation of concepts. Therefore, faithfulness is a crucial component of our method (see Sec. 4), i.e., we strive to decompose the concept into elements that can *reconstruct* it. When applying the logic suggested by the reviewer to decompose concepts, faithfulness is not guaranteed and in fact, in most cases, it does not hold.\n\n   For example, an alternative decomposition for \"snail\" under this logic could be \"table\" + \"cake\" since `sim(\"snail\", \"table\") = 0.7749`,  `sim(\"snail\", \"cake\") = 0.7847`, and `sim(\"snail\", \"table\" + \"cake\") = 0.8110 > max{sim(\"snail\", \"table\"), sim(\"snail\", \"cake\")}`. However, the resulting images for the decomposition of \"table\" + \"cake\" plugged into SD provide images that are entirely different from snails, such as the two examples in [this link](https://i.imgur.com/sNtHAlX.jpeg).\n\n   Additional such examples can be found very easily, for example, one could replace \"cashmere\" in the decomposition for \"camel\" and decompose it into \"giraffe\" + \"book\" as follows: `sim(\"camel\", \"giraffe\") = 0.834`,  `sim(\"camel\", \"book\") = 0.7485`, `sim(\"camel\", \"giraffe\"+\"book\") = 0.8408 > max{sim(\"camel\", \"giraffe\"), sim(\"camel\", \"book\")}`, see examples of SD generation for this decomposition in [this link](https://i.imgur.com/iW79uRw.jpeg). Other possibilities to replace \"cashmere\" in the decomposition include: \"technology\" (`sim(\"camel\", \"technology\")=0.786`, `sim(\"camel\", \"giraffe\" + \"technology\")=0.866 > max{0.834, 0.786}`), \"cattle\" (`sim(\"camel\", \"cattle\")=0.832`, `sim(\"camel\", \"giraffe\" + \"cattle\")=0.873 > max{0.834, 0.832}`) and many other words that are not semantically related to \"camel\" and do not produce a visual reconstruction.\n\n   Note that our method learns weighted decompositions of elements to reconstruct the concept images (see Tab. 1, Fig. 4 of the paper for example). The examples presented in Fig. 2 are actual decompositions of images generated by SD, and reconstructed by Conceptor. The image reconstruction objective is the component that guarantees a connection between the decomposition and the concept itself, which otherwise would not exist, as demonstrated in the examples above.\n\n\n1. We note that the inequality suggested by the reviewer, i.e., `sim(A+B, C) > max{sim(A, C), sim(B, C)}` often holds for any triplet of three words. This may be a result of increased similarity with the \u201cblurred\u201d word obtained by averaging two concepts. To validate this empirically, we selected 10,000 random triplets of words (A, B, C) out of the SD text encoder dictionary. In 85% of the cases, the inequality holds. This indicates that the proposed inequality does not necessarily suggest a better semantic similarity, nor that A+B is a decomposition of C.\n\n\n1. We would like to point out that the CLIP text-text similarity scores (as calculated by the reviewer using CLIP ViT-B/32) appear to give abnormally high similarities for various words that are unrelated semantically. We drew 10,000 pairs of random words from SD\u2019s vocabulary to demonstrate this point and found that the average similarity between these random words was 0.815. These high scores are somewhat surprising, especially given that a large portion of the tokens in this dictionary are not actual words but rather punctuation marks, emojis, etc. Additionally, the scores are often unintuitive. For example, the CLIP text-text similarity between \"cat\" and \"house\" is 0.79, which is higher than the CLIP text-text similarity between \u201ccat\u201d and \u201cpoodle\u201d (0.74) (to reiterate the previous point, \u201ccat\u201d to \u201dhouse\u201d+\u201cpoodle\u201d is 0.81).\n\n1. Finally, please note that the latest version of SD (SD 2.1) employs a *different CLIP text encoder*, i.e., OpenCLIP ViT-H (see Appendix A). When examining the CLIP text-text similarities in the OpenCLIP ViT-H encoder, the phenomena described in the review do not reproduce. First, we find that the CLIP text-text similarities between the elements are *far lower* than those indicated by the reviewer (with CLIP ViT-B/32), e.g., the similarity between \"reflections of earth\" and \"civilization\" is merely 0.2817, the similarity between \"snake\" and \"twisted\" is 0.365, etc. Second, the logic indicated for CLIP ViT-B/32 is not reproduced. In 2/6 triplets mentioned by the reviewer, the sum of the two elements does not increase the similarity: \n\n   * For \"snake\": `sim(\"snake\", \"twisted\") = 0.365`, `sim(\"snake\", \"gecko\") = 0.639`, `sim(\"snake\", \"twisted\" + \"gecko\") = 0.627 < 0.639`.\n   * For \"fear\": `sim(\"fear\", \"wolf\") = 0.326`, `sim(\"fear\", \"scream\") = 0.638`, `sim(\"fear\", \"wolf\" + \"scream\") = 0.60 < 0.638`.\n\n   For this text encoder, the inequality holds for 65% of the random triplets as described in item 2.\n\n\nWe would happily address any other questions."
                    },
                    "title": {
                        "value": "Thank you for the review!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699657493956,
                "cdate": 1699657493956,
                "tmdate": 1700194124577,
                "mdate": 1700194124577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CXN3nDku42",
                "forum": "awWpHnEJDw",
                "replyto": "w3NGHUWZ5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "We appreciate the diligent efforts put into reviewing our work. Like you, we believe that performing sanity checks to validate the underlying assumptions is important.\n\nMay we inquire if you have had the opportunity to review our response from November 11?\n\nThank you,\n\nThe authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388763427,
                "cdate": 1700388763427,
                "tmdate": 1700388839424,
                "mdate": 1700388839424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KMWW8eSzKo",
                "forum": "awWpHnEJDw",
                "replyto": "w3NGHUWZ5S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_3WNH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_3WNH"
                ],
                "content": {
                    "title": {
                        "value": "Updated my original assessment"
                    },
                    "comment": {
                        "value": "Thanks the author response. I've verified and indeed in the CLIP text space, the triangular similarity relationships are very noisy and seem not reflect true semantic similarity. Therefore I'd raise my rating to 6. As a relevant piece of observation, my original comments on the CLIP triangular relationships are kept."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581812576,
                "cdate": 1700581812576,
                "tmdate": 1700581878882,
                "mdate": 1700581878882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WckAh1wxdL",
            "forum": "awWpHnEJDw",
            "replyto": "awWpHnEJDw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_HDAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4002/Reviewer_HDAw"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into understanding the internal representations of text-to-image diffusion models, which have shown significant prowess in generating high-quality images from textual concepts. The primary challenge addressed is deciphering how these models map textual prompts to rich visual representations. The authors introduce a method, \"CONCEPTOR\", that decomposes an input text prompt into a set of interpretable elements. This decomposition is achieved by learning a pseudo-token, which is a sparse weighted combination of tokens from the model's vocabulary. The goal is to reconstruct the images generated for a given concept using this pseudo-token. The method facilitates single-image decomposition into tokens and semantic image manipulation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a novel view to interpret the internal representations of T2I diffusion model, decomposing the input text prompts into a set of prototypes. Image manipulation can be implemented by simply adjusting the coefficients of these prototypes  \n\nThe method is general and flexible, as it can be applied to any T2I diffusion model without modifying the model architecture or training procedure. \n\nThe writing is good and clear. The paper also provides empirical evidence to support the effectiveness and efficiency of the method."
                },
                "weaknesses": {
                    "value": "The number of concepts (prototype) are limited, which can not prove whether the proposed method is effective on large-scale concepts.\n\nThe concept decomposing can be viewed as an inner interpolation between the concepts. What if the image are out of domain? Is it possible to show some cases? Can you provided some analysis the between the proposed method and interpolation method?"
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4002/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699455011756,
            "cdate": 1699455011756,
            "tmdate": 1699636362157,
            "mdate": 1699636362157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fxlJI4YEwM",
                "forum": "awWpHnEJDw",
                "replyto": "WckAh1wxdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and comprehensive feedback. We are encouraged that the reviewer found our work to be a novel, flexible, and efficient solution for diffusion model interpretability.\nIn the following, we provide a response to each of the questions in the review. Additionally, the modifications in the revision are marked in red, for your convenience.\n\n__Re. Size of test dataset:__ First, kindly note that the size of our dataset (188 concepts from diverse categories) is consistent with, and even significantly bigger than that of related works that tackle concept-based interpretability and concept analysis. For example, one of the most prominent works on concept-based explainability, ACE [1], presents a quantitative evaluation of 100 random ImageNet classes. Similarly, seminal works in concept personalization such as DreamBooth [2] performed evaluation on 30 diverse concepts, etc.\n \nImportantly, note that we formed a diverse dataset including professions (to indicate biases and human-centered concepts), abstract concepts (including emotions and actions), very complex concepts (requiring hierarchical reasoning), and on top of those, we added 100 random concepts from the well-known general concept bank, ConceptNet, to ensure that our produced results are robust across a wide variety of rich and unrelated concepts. Note that ConceptNet contains a wide mixture of concepts from all categories, and is a very widely used knowledge graph. Across all these different types of concepts, Conceptor has *consistently* demonstrated an ability to produce faithful, meaningful, and robust decompositions.\n\n[1] Towards Automatic Concept-based Explanations, Ghorbani et al. NeurIPS 2019\n\n[2] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, Ruiz et al., CVPR 2023.\n\n__Re. Out-of-domain (OOD) images:__  Thank you for this question. While the goal of our work is to interpret the internal representations of the model (i.e., *in-domain concepts*) and images generated by the model for textual concepts, this question is indeed very interesting. Following the review, we have added a new Appendix (J) in which we examine three domains out of the DreamBooth dataset [2], with an increasing amount of distance from the training domain of SD. As can be seen, our method is able to extract meaningful and profound decompositions even for OOD concepts (e.g., the sloth plush in Fig. 16 is linked to the concepts \u201cplush\u201d, \u201cfluffy\u201d, and \u201cknitted\u201d, the monster toy in Fig. 17 is linked to \u201cwoolly\u201d due to its wool-like fur, to the character \u201cDomo\u201d due to the shape of its body, and to a \u201ccrab\u201d and a \u201cshrimp\u201d due to its unique limbs).  Moreover, our optimization results in a virtual token $w^*$ that is able to generate new OOD images at least as effectively as unrestricted optimization of the virtual token. We find these results to be extremely reassuring.\n\n__Re. Analysis between Conceptor and interpolation methods:__ Could you please clarify which interpolation method you are referring to? Given more details, we would be more than happy to conduct the appropriate analysis.\n\nWe would happily address any other questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699914700116,
                "cdate": 1699914700116,
                "tmdate": 1700194095804,
                "mdate": 1700194095804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pzFdK9aR9V",
                "forum": "awWpHnEJDw",
                "replyto": "fxlJI4YEwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_HDAw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Reviewer_HDAw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback.  The OOD experiment results are impressive. \n\nRegarding the interpolation, let me clarify. Semantic interpolation involves the weighted summation of the condition embeddings and generating images conditioned on the interpolated text embeddings.\n\nFrom a technical perspective, it seems that Conceptor and semantic interpolation are quite similar."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582544436,
                "cdate": 1700582544436,
                "tmdate": 1700582544436,
                "mdate": 1700582544436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k9yrq3xJKN",
                "forum": "awWpHnEJDw",
                "replyto": "WckAh1wxdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4002/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for considering our rebuttal! We share your enthusiasm for the OOD results, and deeply value the insightful suggestion that helped enhance our understanding of Conceptor.\n\nUpon reviewing the clarification, we are still not sure we fully understand the proposed interpolation method and how it selects the condition embeddings. We derived two possible interpretations for the question which we address below, and we welcome your corrections if our understanding is not accurate.\n\n1. Embedding interpolation as simple summation of word embeddings (i.e., summation of embeddings in the token space):\n\n   Allow us to emphasize the following:\n\n    * Recall that our method operates by learning a mapping function between each word embedding and a corresponding coefficient. In other words, our method can be viewed as a nonlinear mapping of the form: $w^* = \\sum_i f(w_i) w_i$, where $f$ is a nonlinear function. \n\n       Our extensive ablations (refer to Section 4.2.1, Table 3 under \"without MLP\") demonstrate the critical role of this novel approach in the success of our method. Without it (i.e., learning separate coefficients) the decompositions are not faithful. This ablation highlights the non-trivial nature of assigning appropriate coefficients to word embeddings to reconstruct the learned representation, affirming that a simple summation cannot substitute for Conceptor.\n\n   * To demonstrate the important role of our learned coefficients empirically, we enclose the results of simple word embedding summation (a+b) for examples of concepts from Fig 2. The connection \u201creflections of earth = sphere + civilization\u201d is only made possible by the learned coefficients. When considering only the word embeddings, [these are the resulting images](https://i.imgur.com/6ai71Ww.jpg) (which do not resemble planet Earth whatsoever). Similarly, \u201csnake = twisted + gecko\u201d would yield [these images without the appropriate coefficients](https://i.imgur.com/1UYOham.jpg), for the combination \u201csnail = winding + ladybug\u201d [these are the resulting images](https://i.imgur.com/EB4S5ya.jpg), etc. These examples demonstrate that even given the ground truth set of decomposition concepts, a simple summation cannot match our method.\n\n   We believe that the combination of the aforementioned points effectively demonstrates the powerful and non-trivial ability of Conceptor to establish semantic links between concepts through our novel method.\n\n2. Encoding interpolation after applying the text encoder (i.e., summation of embeddings in the conditioning space):\n\n   In exploring this scenario, we observed that such interpolations result in non-meaningful images (perhaps due to OOD encodings). [Here\u2019s an example for \u201cladybug\u201d + \u201cwinding\u201d](https://i.imgur.com/QrwUgBN.jpg), and [\u201ctwisted\u201d + \u201cgecko\u201d](https://i.imgur.com/0ddljfw.jpg).\n\nWe are happy to address any remaining questions, and appreciate the continued in-depth discussion."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4002/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601974435,
                "cdate": 1700601974435,
                "tmdate": 1700684162293,
                "mdate": 1700684162293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]