[
    {
        "title": "Learning Multi-Agent Communication from Graph Modeling Perspective"
    },
    {
        "review": {
            "id": "C9Xis9ffPQ",
            "forum": "Qox9rO0kN0",
            "replyto": "Qox9rO0kN0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_WiJp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_WiJp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called CommFormer, which addresses the challenge of learning multi-agent communication from a graph modeling perspective. The communication architecture among agents is modelled as a learnable graph. The problem is treated as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. By leveraging continuous relaxation of graph representation and incorporating attention mechanisms within the graph modeling framework, CommFormer enables the concurrent optimization of the communication graph and architectural parameters in an end-to-end manner."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces a novel approach which models the communication architecture among agents as a learnable graph. The considered problem is formulated as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process."
                },
                "weaknesses": {
                    "value": "There have been some works which learns multi-agent cooperative behaviors based on learnable graphs. It would be better to illustrate the differences of the paper compared to them. An example is provided below.\n\nLiu, Y., Dou, Y., Li, Y., Xu, X., & Liu, D. (2022). Temporal Dynamic Weighted Graph Convolution for Multi-agent Reinforcement Learning. Proceedings of the Annual Meeting of the Cognitive Science Society."
                },
                "questions": {
                    "value": "1. The paper proposes a communication-based MARL method. In fact the paradigm CTDE is not suited for such method. There are still some communications among agents for the execution of policies. It seems that CTCE is more suited for the proposed method. Some CTCE based MARL methods. for example, graph-based MARL methods, should be considered for the comparison in the experiment. \n\n2. In Table 1, the FC is a little bit confusing. Even there are no constrictions on the communication bandwidth, the win rate is still hard to be 100% as it depends how the opponents perform. Of course, 100% is the maximum value for the win rate, but it is a meaningless upper bound. Further, how the value 93.8 is obtained in FC column as the upper bound?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719652101,
            "cdate": 1698719652101,
            "tmdate": 1699636279234,
            "mdate": 1699636279234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wAU5Lm9ZSw",
                "forum": "Qox9rO0kN0",
                "replyto": "C9Xis9ffPQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> Some CTCE based MARL methods. for example, graph-based MARL methods, should be considered for the comparison in the experiment.\n\nThanks for the suggestion! We have incorporated additional experiments to enhance the generalization of our method. Taking into account the communication domains explored in previous works, we have included the following experiments. It is worth noting that in certain domains, our objective extends beyond maximizing the average success rate or cumulative rewards. We also aim to minimize the average number of steps required to complete an episode, emphasizing the ability to achieve goals in the shortest possible time.\n+ Another three maps in the SMAC environment: 1o10b_vs_1r and 1o2r_vs_4r, which pose challenges due to partial observability, and 5z_vs_1ul, where successful outcomes require strong coordination.\n+ Predator-Prey (PP) [1]: The goal is for \ud835\udc41 predator agents with limited vision to find a stationary prey and move to its location. The agents in this domain all belong to the same class (i.e., identical state, observation and action spaces).\n+ Predator-Capture-Prey (PCP) [2]: We have two classes of predator and capture agents. Agents of the predator class have the goal of finding the prey with limited vision (similar to agents in PP). Agents of the capture class, have the goal of locating the prey and capturing it with an additional capture-prey action in their action-space, while not having any observation inputs (e.g., lack of scanning sensors).\n+ Google Research Football (GRF) [11]: We evaluate algorithms in the football academy scenario 3 vs. 2, where we have 3 attackers vs. 1 defender, and 1 goalie. The three offending agents are controlled by the MARL algorithm, and the two defending agents are controlled by a built-in AI. We find that utilizing a 3 vs. 2 scenario challenges the robustness of MARL algorithms to stochasticity and sparse rewards.\n\nWe include several state-of-the-art graph-based multi-agent communication learning approaches as additional baselines in our evaluation. These methods encompass QGNN [7], SMS [3], TarMAC [4], NDQ [5], MAGIC [6], HetNet [2], CommNet [8], I3CNet [9], and GA-Comm [10].\n\nThe performance results of these baselines are presented below. It is important to note that due to time constraints, we directly obtain the performance results from the respective papers. Our CommFormer consistently demonstrates favorable performance across the evaluated metrics.\n\n\n\n| Task        | Metric       | CommFormer(0.4) | QGNN           | SMS  | TarMAC | NDQ  | MAGIC | QMIX |\n| ----------- | ------------ | --------------- | -------------- | ---- | ------ | ---- | ----- | ---- |\n| 1o2r_vs_4r  | Success Rate | **96.9 $\\pm$ 1.5**  | 93.8 $\\pm$ 2.6 | 76.4 | 39.1   | 77.1 | 22.3  | 51.1 |\n| 1o10b_vs_1r | Success Rate | 96.9 $\\pm$ 3.1  | **98.0 $\\pm$ 2.9** | 86.0 | 40.1   | 78.1 | 5.8   | 51.4 |\n| 5z_vs_1ul   | Success Rate | **100.0 $\\pm$ 1.4** | 92.2 $\\pm$ 1.6 | 59.9 | 44.2   | 48.9 | 0.0   | 82.6 | \n\n| Task | Metric       | CommFormer(0.4) | MAGIC           | CommNet        | I3CNet         | TarMAC         | GA-Comm        |\n| ---- | ------------ | --------------- | --------------- | -------------- | -------------- | -------------- | -------------- |\n| GRF  | Success Rate | **100.0 $\\pm$ 0.0** | 98.2  $\\pm$ 1.0 | 59.2 $\\pm$13.7 | 70.0 $\\pm$ 9.8 | 73.5 $\\pm$ 8.3 | 88.8 $\\pm$ 3.9 |\n| GRF  | Steps Taken  | **25.4 $\\pm$ 0.4**  | 34.3 $\\pm$ 1.3  | 39.3 $\\pm$ 2.4 | 40.4 $\\pm$ 1.2 | 41.5 $\\pm$ 2.8 | 39.1 $\\pm$ 3.1            |\n\n| Task | Metric                    | CommFormer(0.4)    | MAGIC              | HetNet             | CommNet            | I3CNet             | TarMAC             |\n| ---- | ------------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| PP   | Average Cumulative Reward | **-0.121 $\\pm$ 0.008** | -0.386 $\\pm$ 0.024 | -0.232 $\\pm$ 0.010 | -0.336 $\\pm$ 0.012 | -0.342 $\\pm$ 0.015 | -0.563 $\\pm$ 0.030 |\n| PP   | Steps Taken               | **4.99 $\\pm$ 0.31**    | 10.6 $\\pm$ 0.50    | 8.30 $\\pm$ 0.25    | 8.97 $\\pm$ 0.25    | 9.69 $\\pm$ 0.26    | 18.4 $\\pm$ 0.46     |\n\n| Task | Metric                    | CommFormer(0.4)    | MAGIC              | HetNet             | CommNet            | I3CNet             | TarMAC             |\n| ---- | ------------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| PCP  | Average Cumulative Reward | **-0.197 $\\pm$ 0.019** | -0.394 $\\pm$ 0.017 | -0.364 $\\pm$ 0.017 | -0.394 $\\pm$ 0.019 | -0.411 $\\pm$ 0.019 | -0.548 $\\pm$ 0.031 |\n| PCP  | Steps Taken               | **7.61 $\\pm$ 0.66**    | 10.8 $\\pm$ 0.45    | 9.98 $\\pm$ 0.36    | 11.3 $\\pm$ 0.34    | 11.5 $\\pm$ 0.37    | 17.0 $\\pm$ 0.80    |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194811421,
                "cdate": 1700194811421,
                "tmdate": 1700195398678,
                "mdate": 1700195398678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AwrWbj33yc",
                "forum": "Qox9rO0kN0",
                "replyto": "C9Xis9ffPQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q2\n> In Table 1, the FC is a little bit confusing. Even there are no constrictions on the communication bandwidth, the win rate is still hard to be 100% as it depends how the opponents perform. Of course, 100% is the maximum value for the win rate, but it is a meaningless upper bound. Further, how the value 93.8 is obtained in FC column as the upper bound?\n\nIn our study, \"FC\" refers to CommFormer with a sparsity setting of 1.0. This configuration implies that there are no restrictions on the communication graph, allowing agents to freely communicate with all other agents. Effectively, this represents the upper performance limit of the CommFormer methods. By presenting results under this setting, we aim to demonstrate that with our bi-level learning process, a sparsity of 0.4 can achieve comparable results to a full sparsity of 1.0 in most scenarios.\n\nGiven the \"FC\" framework, the bi-level optimization problem simplifies to the following optimization formulation:\n\n$$\n \\min_{\\theta, \\phi} ~L_{val}(\\phi, \\theta)\n$$\n\n### Q3\n> There have been some works which learns multi-agent cooperative behaviors based on learnable graphs. It would be better to illustrate the differences of the paper compared to them. \n\nThanks! TWG-Q[12] primarily focuses on exploring diverse spatial-temporal information environments, necessitating the utilization of a temporal weight learning mechanism and weighted GCN to dynamically capture the intensities of cooperations. Conversely, CDC[13] dynamically adjusts the communication graph, taking into account the diffusion process perspective to capture the information flow on the graph. In contrast, our CommFormer approach learns the static communication graph through an optimization perspective, employing attention scores to automatically assign credit to received messages. We will improve the related work in the updated version. \n\n### Reference\n\n[1] Amanpreet, Singh, et al. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018).\n\n[2] Seraj, Esmaeil, et al. \"Learning Efficient Diverse Communication for Cooperative Heterogeneous Teaming.\" AAMAS 2022.\n\n[3] Xue, Di, et al. \"Efficient Multi-Agent Communication via Shapley Message Value.\" IJCAI 2022. \n\n[4] Das, Abhishek, et al. \"Tarmac: Targeted multi-agent communication.\" ICML 2019.\n\n[5] Wang, Tonghan, et al. \"Learning nearly decomposable value functions via communication minimization.\" arXiv 2019.\n\n[6] Niu, Yaru, et al. \"Multi-Agent Graph-Attention Communication and Teaming.\" AAMAS 2021.\n\n[7] Ryan Kortvelesy and Amanda Prorok. \"QGNN: Value Function Factorisation with Graph Neural Networks.\" arXiv preprint arXiv:2205.13005, 2022.\n\n[8] Sainbayar Sukhbaatar, et al. \"Learning multiagent communication with backpropagation.\" NeurIPS 2016.\n\n[9] Amanpreet Singh, et al. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018).\n\n[10] Yong Liu et al. \"Multi-Agent Game Abstraction via Graph Attention Neural Network.\" AAAI 2022.\n\n[11] Karol Kurach, et al. \"Google Research Football: A Novel Reinforcement Learning Environment.\" AAAI 2020.\n\n[12] Liu, Yuntao, et al. \"Temporal Dynamic Weighted Graph Convolution for Multi-agent Reinforcement Learning.\" Proceedings of the Annual Meeting of the Cognitive Science Society.\n\n[13] Pesce, Emanuele, et al. \"Learning multi\u2011agent coordination through connectivity\u2011driven communication.\" Machine Learning Springer 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195286683,
                "cdate": 1700195286683,
                "tmdate": 1700195374367,
                "mdate": 1700195374367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DFePICuLgc",
            "forum": "Qox9rO0kN0",
            "replyto": "Qox9rO0kN0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_Ux29"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_Ux29"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces CommFormer, a novel approach for optimizing the communication architecture among multiple intelligent agents involved in collaborative tasks. By conceptualizing the architecture as a learnable graph and employing a bi-level optimization process with attention units, CommFormer enables agents to efficiently optimize their communication and adapt more coordinated strategies in a variety of scenarios, as demonstrated in experiments on StarCraft II combat games.\n\nI have several comments and questions that need to be addressed before publication:\n\n- what if the communication graph determined by your approach is not physically feasible, for instance due to environmental constraints such as a far physical distance, etc.? Isn\u2019t a graph communication approach that determines the communication based on physical proximity better in such real-world scenarios? Maybe the best solution is a hybrid approach where environment constraints are considered and baked into the problem for determining the communication graph?\n\n- I find the presented related work section to be weak and relatively old. Many recent SOTA graph-based multi-agent communication learning approaches are never mentioned or discussed, despite their high relevance to the proposed approach. For instance, [1]-[4] below are only a few of such works. Almost all of these works offer a distributed graph-based learned multi-agent communication method that work under POMDPs and are trained under CTDE. There are more of such recent paper. I believe the authors need to perform a more comprehensive search on the recent literature.\n\n[1] Seraj, Esmaeil, et al. \"Learning efficient diverse communication for cooperative heterogeneous teaming.\" Proceedings of the 21st international conference on autonomous agents and multiagent systems. 2022.\n\n[2] Niu, Yaru, Rohan R. Paleja, and Matthew C. Gombolay. \"Multi-Agent Graph-Attention Communication and Teaming.\" AAMAS. 2021.\n\n[3] Bettini, Matteo, Ajay Shankar, and Amanda Prorok. \"Heterogeneous Multi-Robot Reinforcement Learning.\" Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. 2023.\n\n[4] Meneghetti, Douglas De Rizzo, and Reinaldo Augusto da Costa Bianchi. \"Towards heterogeneous multi-agent reinforcement learning with graph neural networks.\" arXiv preprint arXiv:2009.13161 (2020).\n\n- There are many existing recent, SOTA graph-based multi-agent communication learning approaches, see above [1]-[4] (which are not even mentioned in the paper), that could be a competition for the proposed approach. The selected benchmarks do not necessarily specialize in graph-based distributed communication. The proposed learned communication graph approach should be experimented and evaluated against other graph-based methods.\n\n- All the evaluations are performed in SMAC domains. Is this approach specialized and designed for SMAC? If not, and the solution is in fact generalizable, other domains and different problem settings must be considered. Many of such standard domains can be found in the prior work. Although SMAC domains are interesting game scenarios, the point is to have a comparable baseline performance in standard domains that can also solve other multi-agent coordination and collaboration problems, social interactions, etc.\n\n- Related to the point above, if the presented approach does not apply to other multi-agent problems and scenarios, this should be mentioned and discussed as a limitation. Otherwise, only presenting results in one domain does not suffice.\n\n- The second contribution bullet-point mentions the use of attention units for allocating credit to received messages. Doesn\u2019t TarMAC already do that?\n\n- What are the limitations of the approach? The limitations are never discussed.\n\nAt current states I vote weak rejection, since the algorithm seems to be sound and working, however there are some notable weaknesses in literature review and benchmarking (methods and domains) that need to be addressed as much as possible. I\u2019d be happy to increase my score further when authors satisfactorily addressed my comments and questions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See above."
                },
                "weaknesses": {
                    "value": "See above."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Reviewer_Ux29"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747485631,
            "cdate": 1698747485631,
            "tmdate": 1700504752974,
            "mdate": 1700504752974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ef4zkTzbpE",
                "forum": "Qox9rO0kN0",
                "replyto": "DFePICuLgc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1 & Q2\n>  The proposed learned communication graph approach should be experimented and evaluated against other graph-based methods.\n\n>  Is this approach specialized and designed for SMAC? \n\nThanks for the suggestion! We have incorporated additional experiments to enhance the generalization of our method. Taking into account the communication domains explored in previous works, we have included the following experiments. It is worth noting that in certain domains, our objective extends beyond maximizing the average success rate or cumulative rewards. We also aim to minimize the average number of steps required to complete an episode, emphasizing the ability to achieve goals in the shortest possible time.\n+ Another three maps in the SMAC environment: 1o10b_vs_1r and 1o2r_vs_4r, which pose challenges due to partial observability, and 5z_vs_1ul, where successful outcomes require strong coordination.\n+ Predator-Prey (PP) [1]: The goal is for \ud835\udc41 predator agents with limited vision to find a stationary prey and move to its location. The agents in this domain all belong to the same class (i.e., identical state, observation and action spaces).\n+ Predator-Capture-Prey (PCP) [2]: We have two classes of predator and capture agents. Agents of the predator class have the goal of finding the prey with limited vision (similar to agents in PP). Agents of the capture class, have the goal of locating the prey and capturing it with an additional capture-prey action in their action-space, while not having any observation inputs (e.g., lack of scanning sensors).\n+ Google Research Football (GRF) [11]: We evaluate algorithms in the football academy scenario 3 vs. 2, where we have 3 attackers vs. 1 defender, and 1 goalie. The three offending agents are controlled by the MARL algorithm, and the two defending agents are controlled by a built-in AI. We find that utilizing a 3 vs. 2 scenario challenges the robustness of MARL algorithms to stochasticity and sparse rewards.\n\nWe include several state-of-the-art graph-based multi-agent communication learning approaches as additional baselines in our evaluation. These methods encompass QGNN [7], SMS [3], TarMAC [4], NDQ [5], MAGIC [6], HetNet [2], CommNet [8], I3CNet [9], and GA-Comm [10].\n\nThe performance results of these baselines are presented below. It is important to note that due to time constraints, we directly obtain the performance results from the respective papers. Our CommFormer consistently demonstrates favorable performance across the evaluated metrics.\n\n\n\n| Task        | Metric       | CommFormer(0.4) | QGNN           | SMS  | TarMAC | NDQ  | MAGIC | QMIX |\n| ----------- | ------------ | --------------- | -------------- | ---- | ------ | ---- | ----- | ---- |\n| 1o2r_vs_4r  | Success Rate | **96.9 $\\pm$ 1.5**  | 93.8 $\\pm$ 2.6 | 76.4 | 39.1   | 77.1 | 22.3  | 51.1 |\n| 1o10b_vs_1r | Success Rate | 96.9 $\\pm$ 3.1  | **98.0 $\\pm$ 2.9** | 86.0 | 40.1   | 78.1 | 5.8   | 51.4 |\n| 5z_vs_1ul   | Success Rate | **100.0 $\\pm$ 1.4** | 92.2 $\\pm$ 1.6 | 59.9 | 44.2   | 48.9 | 0.0   | 82.6 | \n\n| Task | Metric       | CommFormer(0.4) | MAGIC           | CommNet        | I3CNet         | TarMAC         | GA-Comm        |\n| ---- | ------------ | --------------- | --------------- | -------------- | -------------- | -------------- | -------------- |\n| GRF  | Success Rate | **100.0 $\\pm$ 0.0** | 98.2  $\\pm$ 1.0 | 59.2 $\\pm$13.7 | 70.0 $\\pm$ 9.8 | 73.5 $\\pm$ 8.3 | 88.8 $\\pm$ 3.9 |\n| GRF  | Steps Taken  | **25.4 $\\pm$ 0.4**  | 34.3 $\\pm$ 1.3  | 39.3 $\\pm$ 2.4 | 40.4 $\\pm$ 1.2 | 41.5 $\\pm$ 2.8 | 39.1 $\\pm$ 3.1            |\n\n| Task | Metric                    | CommFormer(0.4)    | MAGIC              | HetNet             | CommNet            | I3CNet             | TarMAC             |\n| ---- | ------------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| PP   | Average Cumulative Reward | **-0.121 $\\pm$ 0.008** | -0.386 $\\pm$ 0.024 | -0.232 $\\pm$ 0.010 | -0.336 $\\pm$ 0.012 | -0.342 $\\pm$ 0.015 | -0.563 $\\pm$ 0.030 |\n| PP   | Steps Taken               | **4.99 $\\pm$ 0.31**    | 10.6 $\\pm$ 0.50    | 8.30 $\\pm$ 0.25    | 8.97 $\\pm$ 0.25    | 9.69 $\\pm$ 0.26    | 18.4 $\\pm$ 0.46     |\n\n| Task | Metric                    | CommFormer(0.4)    | MAGIC              | HetNet             | CommNet            | I3CNet             | TarMAC             |\n| ---- | ------------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| PCP  | Average Cumulative Reward | **-0.197 $\\pm$ 0.019** | -0.394 $\\pm$ 0.017 | -0.364 $\\pm$ 0.017 | -0.394 $\\pm$ 0.019 | -0.411 $\\pm$ 0.019 | -0.548 $\\pm$ 0.031 |\n| PCP  | Steps Taken               | **7.61 $\\pm$ 0.66**    | 10.8 $\\pm$ 0.45    | 9.98 $\\pm$ 0.36    | 11.3 $\\pm$ 0.34    | 11.5 $\\pm$ 0.37    | 17.0 $\\pm$ 0.80    |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191091679,
                "cdate": 1700191091679,
                "tmdate": 1700191091679,
                "mdate": 1700191091679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DbKyqqG6Mt",
                "forum": "Qox9rO0kN0",
                "replyto": "61hkqyLodK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Reviewer_Ux29"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Reviewer_Ux29"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you to authors for clarifications. While I'm satisfied with most of the responses, I suggest that authors make sure to include the above responses (mainly Q1, Q2, Q3, Q4, and Q5) in the camera-ready version upon acceptance. I believe it is critical to include the new results, domains, and discussions regarding the limitations in open areas as well as differences with the attention mechanism in TarMAC. I also appreciate the additional evaluations and results, which adds to the value of their work.\n\nI raise my score, contingent upon applying the required revisions, as mention above, by the authors.\nGood luck!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504718221,
                "cdate": 1700504718221,
                "tmdate": 1700504718221,
                "mdate": 1700504718221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0DUCmt14FB",
            "forum": "Qox9rO0kN0",
            "replyto": "Qox9rO0kN0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_q3Zy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_q3Zy"
            ],
            "content": {
                "summary": {
                    "value": "- learning to leverage communication in bandwidth-restricted settings with a learnable adjacency matrix\n\n- continuous relaxation of adjacency matrix to enable differentiable updating of the parameters and adjacency matrix with bootstrapping"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Solid formalization of the communication graph problem\n\n- novel contribution**\n\n- impressive experimental results on SMAC compared to SOTA methods\n\n- well written paper, a pleasure to read\n\n** Possible related work: Learning multi-agent coordination through connectivity-driven communication, Pesce and Montana, 2022, springer https://link.springer.com/article/10.1007/s10994-022-06286-6"
                },
                "weaknesses": {
                    "value": "- fixed communication network after training. Despite the authors claiming that dynamic adjustments fall outside the scope of the paper, it would be interesting to see performance comparisons.\n\n- task 8m is not in figure 4 (as opposed to what the \"Sparsity\" paragraph would suggest in 4.3 Ablations).\n\n- \"Nevertheless, As task complexity and the number of participating agents increase, a higher\nlevel of sparsity becomes necessary to attain superior performance.\" this is a very confusing way to say that the matrix needs to be *less* sparse."
                },
                "questions": {
                    "value": "Why do Dynamic adjustments fall outside the scope of the paper? It seems like this is more about considering a simplified problem setting, where the communication graph between training and execution must be similar. Did you run any experiments testing the performance of CommFormer when the nature of the communication graph changes between training and execution?\n\n\"where  \u0304\u03c6 is the target network\u2019s parameter, which is non-differentiable and updated every few epochs\" what does this mean?\n\nHow does the actual runtime complexity (i.e. walltime or asymptotic) compare between the different methods? S = 0.4 is still quadratic in the number of agents, which can be limiting for large numbers of agents. Rather than having a sparsity proportion, wouldn't it be more relevant to evaluate sparsity as the actual number of non-zero values in the matrix?\n\nDoesn't this method overfit its communication graph to the task? What does a train/test split look like in such a scenario? Do I need to assume with this training method that the communication graph remains the same between training and testing?\n\nWhy does additional environment steps seem to solve the sparsity problem in 25m?\n\nIn figure 6, any intuition as to what kind of tasks lead Commformer to perform similarly to MAT, and under FC? Since MAT allows unrestricted communication between agents, it's weird that FC seems to massively outperform MAT on some tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758435910,
            "cdate": 1698758435910,
            "tmdate": 1699636279090,
            "mdate": 1699636279090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZGDG3Am4WA",
                "forum": "Qox9rO0kN0",
                "replyto": "0DUCmt14FB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> Despite the authors claiming that dynamic adjustments fall outside the scope of the paper, it would be interesting to see performance comparisons.\n    \n    \nThanks for the suggestion. \n\nThe learning process of the communication graph in our study is conducted through bi-level optimization, which necessitates backward propagation of loss. Consequently, during inference, the communication graph remains static and cannot be updated. \n\n    \nTo adapt our method to a dynamic version, the most straightforward approach is to adjust the communication graph based on the attention scores. We conduct tests on four maps, which involve both homogeneous and heterogeneous agents. The performance results of these experiments are presented below.\n\n\n| Maps | CommFormer(0.4) | Dynamic Version (0.4) |\n| ---- | --------------- | ------------------ |\n| 8m   | 100.0 $\\pm$ 0.0 | 96.9 $\\pm$ 3.1     |\n| 25m  | 100.0 $\\pm$ 0.0 | 71.9 $\\pm$ 9.2     |\n| MMM  | 100.0 $\\pm$ 0.0 | 100.0 $\\pm$ 3.1    |\n| 3s5z | 100.0 $\\pm$ 0.0 | 0.0 $\\pm$ 0.1      |\n\nAs indicated, the dynamic version exhibits a decline in performance across all scenarios. In homogeneous agent environments, the dynamic version demonstrates relatively robust performance. However, in heterogeneous settings, such as in the 3s5z map, it fails to effectively learn communication relationships. This shortfall is likely due to the instability of the training process and a self-boosting phenomenon, where the network is preferentially updated based on relations with initially high attention scores.\n\nWe also compare our method with others that specifically investigate dynamic communication adjustments, such as SMS[1], TarMAC[2], and the dynamic message-passing method QGNN[3]. These comparisons are made on three maps: 1o10b_vs_1r and 1o2r_vs_4r, which are challenging due to partial observability, and 5z_vs_1ul, where strong coordination is essential for success. As illustrated in the table below, our method consistently demonstrates exceptional performance.\n\n|             | CommFormer(0.4)     | SMS[1] | TarMAC[2] | QGNN[3]        |\n| ----------- | ------------------- | ------ | --------- | -------------- |\n| 1o10b_vs_1r | 96.9 $\\pm$ 3.1      | 86.0   | 40.1      | **98.0 $\\pm$ 2.9** |\n| 1o2r_vs_4r  | **96.9 $\\pm$ 1.5**  | 76.4   | 39.1      | 93.8 $\\pm$ 2.6 |\n| 5z_vs_1ul   | **100.0 $\\pm$ 1.4** | 59.9   | 44.2      | 92.2 $\\pm$ 1.6 |\n\n\n\n### Q2\n> Why do Dynamic adjustments fall outside the scope of the paper? It seems like this is more about considering a simplified problem setting, where the communication graph between training and execution must be similar. Did you run any experiments testing the performance of CommFormer when the nature of the communication graph changes between training and execution?\n\nThank you for this helpful suggestion!\n\n(1) Dynamic adjustments, while ensuring adherence to sparsity requirement at each step, operate under the assumption that all agents require constant communication with one of the other agents. This process typically demands multiple rounds to establish the current communication graph, potentially leading to inefficient bandwidth usage and imposing practical challenges in real-world applications.\n\n\n(2) The primary goal of our research is to identify the most effective communication graph for a given task. During the training phase, we explore the best possible communication graph from a total of  $C(N^2, m)$ potential edge configurations ( $N$ represents the number of agents, and $m$ denotes the number of edges). Upon transition to the execution phase, this communication graph is set and remains unchanged. This fixed communication model is particularly advantageous for practical deployment scenarios.\n\n(3) Our training methodology focuses on identifying the optimal communication graph. To prevent biases towards specific edges (self-boosting phenomenon), we employ the Gumbel-Softmax trick. This approach enables random edge selection during training, ensuring that each potential connection is considered. Consequently, deviating from the determined communication graph during inference can lead to performance degradation. However, due to the comprehensive nature of our training process, the impact on performance might not be drastic. We validate this hypothesis through tests on four maps involving both homogeneous and heterogeneous agents, with the performance outcomes detailed below:\n\n| Maps | CommFormer(0.4) | Graph Changes between T&E|\n| ---- | --------------- | --------------------- |\n| 8m   | 100.0 $\\pm$ 0.0 | 93.8 $\\pm$ 4.4        |\n| 25m  | 100.0 $\\pm$ 0.0 | 95.6 $\\pm$ 4.5        |\n| MMM  | 100.0 $\\pm$ 0.0 | 93.8 $\\pm$ 4.4       |\n| 3s5z | 100.0 $\\pm$ 0.0 | 92.6 $\\pm$ 4.0        |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189998327,
                "cdate": 1700189998327,
                "tmdate": 1700189998327,
                "mdate": 1700189998327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jBw0e6Jd8B",
            "forum": "Qox9rO0kN0",
            "replyto": "Qox9rO0kN0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_9cgB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3301/Reviewer_9cgB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for learning optimal communication graphs in multi-agent systems using attention. Unlike previous methods that use a predefined graph communication structure with unlimited comms bandwidth, CommFormer learns to create directed communication links such that some level of graph sparsity $S$ is maintained. To do this, they formulate a constrained optimization problem to learn a value encoder and action decoder with an upper bound on the norm of the graph adjacency matrix. In reality, they create a bi-level optimization that steps the encoder/decoder optimizers to find approximate optima then update the adjacency matrix. They perform experiments on StarCraftII with various value-based and policy gradient-based baselines and demonstrate that ComFormer can outperform the baselines in SMAC tasks ranging from Easy to Super Hard."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is an interesting and well written paper. To my knowledge, the learned graph for graph communication using transformers is a novel idea with clear applications to the real world. The architecture is simple/clear and the motivation for the necessity of this solution is motivated very well in Figure 1.\n\n1. The CommFormer method significantly outperforms most of the baselines on most of the tasks (with the exception of some super hard SMAC tasks)\n2. Performs ablative studies to demonstrate the importance of the sparsity claimed in the paper. \n3. Adaptable to various actor-critic methods, not just PPO"
                },
                "weaknesses": {
                    "value": "There are some concerns I have about the problem formulation. It is assumed in many MARL tasks that communication at test time is limited, as per the CTDE paradigm. However, my understanding is that at each time step, the CommFormer can choose to create/destroy communication links between any arbitrary agents as long as a sparsity measure is met. While this is not unreasonable, it is a very large assumption to make while claiming the CTDE paradigm. Further, in seciton 3.2, the authors state that they restrict communication of agent $i$ to only agents $j$ where $j< i$; this assumes that there is some implicit (or explicit) ordering of the agents that we are assuming. Again, I don't think this is unreasonable as many MARL algorithms use one-hot agent id encoding, it imposes additional nuances that are important to the functioning of the algorithm.\n\nFinally, the authors do not compare to a recent graph-based MARL baseline called QGNN[1]\n\n[1] Ryan Kortvelesy and Amanda Prorok.  QGNN: Value Function Factorisation with Graph Neural Networks"
                },
                "questions": {
                    "value": "1. Can the authors compare their method to QGNN \n2. How is the ordering of agents decided when inputting to the transformer and is there positional encoding?\n3. I understand that graph sparsity if a necessary assumption to manage the bandwidth of any given agent. However, can the authors discuss or demonstrate what would happen if more realistic assumptions on graph communication were made, such as only communicating with agents within some specified communication range?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3301/Reviewer_9cgB"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854712419,
            "cdate": 1698854712419,
            "tmdate": 1699636279013,
            "mdate": 1699636279013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FDJ3uflNBk",
                "forum": "Qox9rO0kN0",
                "replyto": "jBw0e6Jd8B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3301/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1\n> How is the ordering of agents decided when inputting to the transformer and is there positional encoding?\n\nSorry for the misleading. It is important to clarify that we do not manually determine the order of the agents, nor is there any positional encoding specifically assigned to the agents. Instead, we calculate the attention between agent $i$ and its preceding agents $j$ where $j<i$ in order to ensure that the decoder generates the action sequence in an auto-regressive manner. This approach guarantees a consistent improvement in performance during training [1].\n\nIn practice, it is possible to change the order of agents at each update iteration. However, we have chosen to maintain the current representation (using $1,2, \\dots n$ other than $i_1, i_2, \\dots i_n$ ) primarily for the purpose of preserving the order of the adjacency matrix. Altering the order of agents would require corresponding adjustments to the adjacency matrix $\\alpha$.\n\n\n### Q2\n> Compare to a recent graph-based MARL baseline called QGNN.\n    \nThank you for your comment.\n\nQGNN primarily leverages graph pooling to facilitate the process of value factorization within a system of agents that exhibit variable sizes. In this system, the communication graph is predetermined by a specific protocol, such as known interactions, and is employed to represent the interdependencies present within a multi-layer message-passing GNN architecture. On the other hand, our CommFormer algorithm simultaneously determines the communication graph and autonomously assigns credit to received messages through a single round of communication. The requested results have been presented in the table provided below. To consider the time constraints, we have chosen StarcraftII as the environment for testing these methods. The evaluation is conducted on three maps: 1o10b_vs_1r and 1o2r_vs_4r, which pose challenges due to partial observability, and 5z_vs_1ul, where successful outcomes require strong coordination.\n\nTo ensure a comprehensive comparison, we have also included the performance of other communication methods, such as SMS[2], TarMAC[3], NDQ[4] and MAGIC[5]. It is worth noting that in the QGNN approach, the graph is fully-connected, following their official code settings. This means that each agent can communicate information with all other agents. CommFormer consistently demonstrates strong performance across these environments.\n\n\n|             | CommFormer(0.4)      | QGNN                | SMS[2] | TarMAC[3] | NDQ[4] | MAGIC[5] | QMIX |\n| ----------- | -------------------- | ------------------- | ------ | --------- | ------ | -------- | ---- |\n| 1o10b_vs_1r | 96.9 $\\pm$ 3.1       | **98.0 $\\pm$ 2.9** | 86.0   | 40.1      | 78.1   | 5.8      | 51.4 |\n| 1o2r_vs_4r  | **96.9 $\\pm$ 1.5**  | 93.8 $\\pm$ 2.6      | 76.4   | 39.1      | 77.1   | 22.3     | 51.1 |\n| 5z_vs_1ul   | **100.0 $\\pm$ 1.4** | 92.2 $\\pm$ 1.6                    | 59.9   | 44.2      | 48.9   | 0.0      | 82.6 |\n    \n\n### Q3\n>  Discuss or demonstrate what would happen if more realistic assumptions on graph communication were made, such as only communicating with agents within some specified communication range?\n\nA possible application of this study is to create an efficient communication framework tailored for enclosed, finite environments, typical of logistics warehouses. In these settings, agent movement is limited to designated zones, and communication is facilitated through overhead wires, akin to a trolleybus system.\n\nOur objective encompasses determining the optimal communication graph, while concurrently ensuring normal updates of architectural parameters. Upon completion of the training phase, the communication graph becomes static, forming the basis for subsequent inferences.\n\nIn contrast, open environments present unique challenges, primarily due to the potential vast distances between agents, which requires wireless communication and may hinder effective communication. To address this, a straightforward approach could be to add bidirectional edges between agents when they come within close proximity, enabling communication between them [6]. However, a more effective solution may involve a hybrid approach that considers the constraint on the available bandwidth\uff1ainitially segmenting agents into groups based on proximity, followed by an internal search for an optimal communication graph within each group. If agent distances vary dynamically during testing, this process is repeated as necessary to adjust the communication graph in real-time, ensuring continuous adaptability to changing environmental conditions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188741723,
                "cdate": 1700188741723,
                "tmdate": 1700188741723,
                "mdate": 1700188741723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]