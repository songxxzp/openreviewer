[
    {
        "title": "A Weight Variation-Aware Training Method for Hardware Neuromorphic Chips"
    },
    {
        "review": {
            "id": "GwWyDAoo53",
            "forum": "GFqQ6gOupN",
            "replyto": "GFqQ6gOupN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_tUb1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_tUb1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to combine noise-aware training and sharpness-aware minimization to improve the weight-noise robustness of DNNs for both ANN and SNN. It shows higher robustness to device variation then SGD and SWA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strength:\n\n1.\tIt combines HSV and GAV during training to improve robustness and evaluated on different datasets/models."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n1.\tThe novelty of the proposed method is very limited. Noise-aware training with noise injection, using SAM for landscape smoothing for analog neural networks with random noises, and device variations are standard.\n\n2.\tThe noise-aware training performance heavily relies on accurate noise variation modeling, which is why Fig 4 shows the performance is the best when gamma is 1, which is also a well-known conclusion in analog NNs.\n\n3.\tThe proposed method sacrifices accuracy when noise is small, e.g., Table 1 and 2. The fundamental reason is that the noise injection impacts the convergence. A more advanced method is to use noise source cooling to avoid noise-induced convergence issues, which can make it less sensitive to the actual noise intensity.\n\n4.\tThe paper claims to be the first one to investigate device variation in analog neural networks, SNNs. As far as I know, there are many robustness-driven optimization, noise-aware training, on-chip training, and physical training methods in the literature to solve this problem, while none of those prior methods are reviewed and compared."
                },
                "questions": {
                    "value": "same as weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623548182,
            "cdate": 1698623548182,
            "tmdate": 1699636531604,
            "mdate": 1699636531604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "UwYtWle9Jb",
            "forum": "GFqQ6gOupN",
            "replyto": "GFqQ6gOupN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_Z5Eu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_Z5Eu"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the accuracy degradation issues in neuromorphic chips posed by weight instability. The authors investigate the accuracy loss due to weight variation and propose to seek a flat weight loss landscape to preserve accuracy under weight variations. Two techniques are illustrated to inject noise during the normal training process for this target, including hardware-simulated sampled noise (WAV) and artificially calculated gradient-ascent variation (GAV). \n\nVarious experiments are conducted on different architectures and datasets on both ANN and SNN. They claim the proposed method can dramatically minimize the performance drop"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extensive experiments evaluation, including both SNN and ANN.\n- The easy-to-read technical part."
                },
                "weaknesses": {
                    "value": "- Limited novelty on the technical part. The weight variation-aware training is not new in the neuromorphic area, where we typically inject hardware-simulated variation (your HSV) with the same process in 3.3. The GAV is a similar approach to AWP and SAM. The weight update scheme is not new, and the variation injection scheme is a simple mixture of GAV and HSV.\n- Improper baselines. The authors mainly compare their method with vanilla SGD without considering the noises, making the claimed performance improvement intriguing. It is not a fair comparison. The prior noise-aware training technique should be used as the baseline in all experiments."
                },
                "questions": {
                    "value": "Could you elaborate on the training details of variation-aware training in A.1. What\u2019s the amount of noise you injected in noise-aware training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Reviewer_Z5Eu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697687444,
            "cdate": 1698697687444,
            "tmdate": 1699636531508,
            "mdate": 1699636531508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "UY4rq1LX4a",
            "forum": "GFqQ6gOupN",
            "replyto": "GFqQ6gOupN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_WCbC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_WCbC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new training method with the aim of making the parameters of the trained network model robust to random perturbations, with the perspective of writing the trained weights on noisy memory devices such as Ferroelectric Tunnel Junctions for neuromorphic computing applications. More specifically, the paper proposes two \"weight variation-aware training\" (WVAT) methods, \"hardware-simulated variation\" (HSV), and \"gradient-ascent variation\" (GAV). HSV samples the perturbation from a Gaussian distribution, while GAV is the \"worst case\" scenario when the perturbation is put in the direction of ascending the loss gradient. The weight update proposed consists in computing the loss gradient at the perturbed weights and applying to the unperturbed weights, with a stochastic threshold for choosing between HSV and GAV. The method is tested over a wide range of network architectures and datasets, where the test accuracy is reported for different amount of perturbations. Overall WVAT diminishes the peak performance without noise, but provides robustness to noise (reduces the accuracy degradation in the presence of noise) compared to noiseless SGD. The authors also use noise taken from the distribution of actual FTJ devices, and report similar findings. An analysis of the loss landscape is provided by computing the loss of the perturbed parameters, showing that WVAT finds flat minima of the loss landscape."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the paper are:\n\n**Simplicity**: The method proposed is simple and effective in providing noise robustness.\n\n**Exhaustive**: The method is tested on a large array of tasks and architectures, and averaged over multiple seeds.\n\n**Interdisciplinary**: The papers touches on concepts from pure ML (generalization methods) and neuromorphic computing (FTJ devices)"
                },
                "weaknesses": {
                    "value": "The weaknesses of the paper are:\n\n**Novelty**: The main finding of the paper is that when noise is accounted for during training, the final model is more robust to noise, albeit less performant in the noiseless case. This is a rather known pattern in the neuromorphic literature, see e.g. [1] (section IV) or [2]. From this point of view, the finding that adding noise during training yields noise robustness is not very novel, but consistent with prior work.\n\n**Ablation**: The proposed method is introduced in a principled way, which is nice, but what about removing the \"Weight reverse\" step? I wonder whether this even simpler approach would give similar result or not. This would shed light on whether the theory is both necessary and sufficient or just sufficient. Testing whether using Dropout during training gives any robustness would be also interesting.\n\nOverall, the paper is ok, but I find the contribution is a bit weak for the general audience of ICLR. I think this paper would be well-suited for a more specific venue. For this reason, I will recommend weak reject for now, with the possibility to revise my judgement if my points are sufficiently addressed.\n\n___\n[1] Hirtzlin, Tifenn, et al. \"Outstanding bit error tolerance of resistive ram-based binarized neural networks.\" 2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE, 2019.  \n[2] Rasch, Malte J., et al. \"Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators.\" Nature Communications 14.1 (2023): 5282."
                },
                "questions": {
                    "value": "- What is the robustness of the network when the \"Weight reverse\" step is removed? (duplicate from the above section).\n\n- Does using dropout during training help robustness and to which extent? (duplicate from the above section).\n\n- Do the network use batch normalization?\n\n- Section 4.5: \n    - Could the authors explain how the FTJ current distributions shown in the low resistance state (LRS) relate to the reading of the parameters in a neuromorphic chip? A section in the appendix detailing the use of the FTJ to encode the weights would be beneficial. \n    - How is the current on the x-axis related to the value of a weight in the network? \n    - Shouldn't the high resistance state (HRS) distributions also be used to encode the weight?\n    - Shouldn't both the intra and inter device variability be taken into account when evaluating the robustness? Right now they seem to be used separately."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Reviewer_WCbC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770084596,
            "cdate": 1698770084596,
            "tmdate": 1699636531415,
            "mdate": 1699636531415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "b0RdAFfKSd",
            "forum": "GFqQ6gOupN",
            "replyto": "GFqQ6gOupN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_Q4cw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5307/Reviewer_Q4cw"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a method for hardware-aware re-training of DNNs\nand SNNs (called WVAT), to be deployed on in-memory (noisy) analog AI hardware\ninference accelerators and low-power neuromorphic hardware designed\nfor edge AI. After a general introduction of related methods, the WVAT\nmethod is introduced by a min-max optimization (GAV), where the optimization\nlandscape is forced to be flat (to support robustness to weight\nvariations). In more detail, in two-gradient passes, first the weight\ngradient is computed (on the \"reference\" weights) and the a second\nweight gradient is computed in an (approximation of the ) worst case\ndirection, which turns out to be in the direction of the first\ngradient. This is used in combination with random weight directions (HSV),\nwhere one of the approaches is applied for a given mini-batch.\n\nThe authors show that the resulting trained method is more robust than\na DNN trained with SWA (stochastic weight averaging) or standard SGD on a\nnumber of DNNs on image classification benchmarks when weight noise is\npresent. \n\nThey also show that the WVAT-trained DNNs are more robust to\nhardware-simulated weight noise distributions (instead of generic\nGaussians)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The general topic is interesting and important for successful deployment of DNNs on neuromorpiic chips.  The proposed WVAT method is an interesting idea based on the flatness of the loss landscape. \n\n* Weight variation seen in an hardware experiment was tested. \n\n* The authors show that robustness is also increased for SNNs, which\nseems novel. However, it is not surprising that if a DNN is more noise\nrobust against weight variations, its corresponding converted SNN\nshould be more robust as well, so maybe this aspect is not interesting\nenough in itself."
                },
                "weaknesses": {
                    "value": "* While the study is reasonable well executed and written, the novelty\nand improvements over state-of-the-art methods is very limited.  In\nparticular, the authors claim to \"for the first time\" analyze the\nimpacts of weight variations for various DNNs and benchmarks which is\nnot correct. In a recent study (Rasch et al. 2023 Nat Comm) -- which\nthe authors fail to cite and thus might be not aware of -- a much more\nrigorous investigation is done over many DNNs and benchmarks,\nincluding not only CNNs but also transformers and LSTMs. That study\nalso improves on the Joshi et al 2020 study in terms of achieved\nrobustness. So the study of Joshi et al, which the authors compare\nagainst, cannot be considered \"state-of-the-art\" anymore. Moreover,\nthe proposed WVAT method does not even seem to convincingly improve\nupon Joshi et al (see below) although using twice as long computation\n(two gradient passes needed).\n\n* Even against the Joshi et al 2020, which is in fact the\nsame HSV method that is part of the WVAT method (as the authors\nacknowledge), the robustness improvement is very limited if any. In\nfact, when varying from purely HSV (as in Joshi et al) to purely GAV,\nthen Figure 4ab shows that purely HSV is indeed the most robust\nmethod, and not WVAT (see in partcilar 4b). This is not discussed and it\nseems that the results of Figure 4 were misinterpreted: note that when\ncomparing the methods, the non-noisy accuracy is completely irrelevant,\nsince DNNs are to be deployed on noisy hardware. Thus, the methods\nshould be compared based on the \"TTV\" accuracy only. Here, in\nparticular in Figure 4b,  the 100% HSV method is actually always\nbetter than adding GAV. This is what Joshi et al already used. Thus, I don't\nsee the additional benefit of the new method. \n\n* The experimental section compares the WVAT method against SWA and\nSGD, however, these are not methods designed to improve robustness\nfor noisy hardware. Instead, it should at least be compared\nagainst 100% HSV, which is what e.g. Joshi et al. (or in a more\nsophisticated manner with more realistic noise distributions and\nhardware assumptions, Rasch et al. 2023) suggested. Figure 4b indicates\nthat 100% HSV would indeed be better or at least very similar to\nWVAT (see above). This comparison is missing. \n\n* It is also not clear why the additional SAM and AWP  methods\nare introduced in the beginning but not used as a comparison in the\nexperimental section, in particular since they appear actually to\nperform better than SWA in an early result of Fig.1. Note, again, that\nthe accuracy *with* variation should be looked at, since the attainable accuracy *without*\nvariation is completely irrelevant in this context."
                },
                "questions": {
                    "value": "* The hardware results are interesting, however, it would have been\nnice to compare also against the other training methods using the hardware\ndata. Since it is only tested against SGD, it is not clear how\ne.g. 100% HSV, SWA, or the other methods would perform and thus it is hard\nto judge the usefulness of WVAT for this particular data.\n\n* I don't find where TTV is defined. It might be added Gaussian noise\nduring testing. While this is useful, it would be also interesting to\ncompare the robustness results using the standardized analog inference\nmodel proposed by Rasch et al.. This would not only be a more\nquantitative benchmark comparison against the state-of-the-art, it\nalso would measure robustness against more realistic noise sources\nseen for in-memory analog inference hardware (beyond merely the\nweight-related variations)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5307/Reviewer_Q4cw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771413357,
            "cdate": 1698771413357,
            "tmdate": 1699636531307,
            "mdate": 1699636531307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]