[
    {
        "title": "Revealing The Intrinsic Ability of Generative Text Summarizers for Outlier Paragraph Detection"
    },
    {
        "review": {
            "id": "Mw5WWj3GOu",
            "forum": "B37UmlxsaP",
            "replyto": "B37UmlxsaP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_bZd9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_bZd9"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the outlier paragraph detection in summarization. It proposes CODE which uses the cross attention between the input paragraph and the generated summary to calculate the relevance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The proposed CODE is finetune-free and only has two hyperparameters to set.\n* It discusses two settings for outlier detection in summarization and conducts a detailed analysis of the influence of each hyperparameter."
                },
                "weaknesses": {
                    "value": "* The problem setting is strange to me. (1) In actual work scenarios, are there many outlier paragraphs in the document? If so, the proposed method should be evaluated on real cases instead of just synthesis data. If outlier paragraphs are uncommon, there would be little point in exploring the issue. It will be better to include more related work on this topic (2) The proposed method and baselines all use the generated summary to identify the outlier. However, If we already have the summary generated, what is the meaning of detecting the outlier paragraphs to avoid its influence on summarization? If the detection of outlier paragraphs will be further used to refine the summary, it should be better clarified in the paper.\n* The baselines are too simple and the discussion about related work is missing."
                },
                "questions": {
                    "value": "* The discussion about related work is missing\n* The citation of ROUGE is wrong\n* some terms are weird\n  * neuron output => hidden states / outputs\n  * ReLU neurons => ReLU activation function\n  * evaluation loss => validation loss\n  * Generative Language Model (GLM) is not a commonly used abbreviation and is rarely used to refer to an encoder-decoder based Transformer."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5653/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5653/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5653/Reviewer_bZd9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730544992,
            "cdate": 1698730544992,
            "tmdate": 1699636588333,
            "mdate": 1699636588333,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "5oQxucsykk",
            "forum": "B37UmlxsaP",
            "replyto": "B37UmlxsaP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_tMsa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_tMsa"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of paragraph outlier detection in the context of summarization. This problem is about detecting paragraphs inside the original document which are out-of-domain. This is done by computing a similarity score on top of the cross attention of the generated summary and each one of the original paragraphs, and flagging a paragraph as outlier if this below a certain threshold"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* Extensive experiments\n* The proposed approach achieves scores which seem satisfactory"
                },
                "weaknesses": {
                    "value": "The biggest weakness is the lack of motivation of this problem. Why is this interesting? Is there any real use-case scenario of this? This is evidenced by the fact that the studied datasets are artificially created to fit exactly this setting. \nComing up with synthetic datasets is a great way of studying problems... if there is at least an attempt made to argument the interest of that problem. This is never done in this paper and the problem is just given as if it were a real one.\n\nBeyond that, there does not seem anything specific to summarization - it seems that this could be applied to any seq2seq problem, in particular translation."
                },
                "questions": {
                    "value": "Please motivate the problem\n\nWhat is the impact on the summarization quality after this noise is introduced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799249204,
            "cdate": 1698799249204,
            "tmdate": 1699636588236,
            "mdate": 1699636588236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "inbmN0FtvH",
            "forum": "B37UmlxsaP",
            "replyto": "B37UmlxsaP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_2Dc8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_2Dc8"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the issue of outlier paragraphs, which according to the authors, is detrimental to summarization systems. To address the issue, the authors propose a method to create summarization pre-training dataset that contain random cross-document and cross-domain outlier paragraphs. After training BART and T5 on those synthetic summarization datasets, they propose different methods to detect outlier paragraphs. Specifically, they find that a closed-form detector (CODE), a function of cross-attention and word frequencies, outperforms a model based on the fine-tuning of a MLP classifier head. Extensive analysis of hyperparameters, models, and pre-training settings is presented as evidence of the effectiveness of their method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper present a clear formalization of the dataset construction and provides comprehensive empirical investigation of the problem, covering four summarization datasets."
                },
                "weaknesses": {
                    "value": "The paper has important weaknesses related to the relevance of the proposed problem and soundness of the methods. First, the authors claim that summarizers are vulnerable to outlier paragraphs, but they do not cite any previous work supporting this claim and do not provide empirical evidence that the summarization datasets in question actually contain outlier paragraphs (in their original distribution) or measure the effects of outlier content in summarization performance. Instead, the authors create a artificial dataset for which each input sample contains 2 coherent and 2 outlier paragraphs and make their conclusions based on this synthetic setting, focusing on the accuracy of outlier classification.\n\nImportantly, when the language models are trained to summarize those synthetic documents, they are indirectly trained to ignore paragraphs that are uninformative for the summary generation, and thus, it is not surprising that cross-attention weights would be lower for the outlier sentences and that a closed-form solution could capture this pattern. Therefore, the paper does not measure intrinsic properties of existing summarizers (as the title suggests), but only verifies that the models learned to assign more weight to sequences of tokens that are (trivially) more informative for the task.\n\nTypos:\n- \"GLM-based Outlier _Pargraph_ Detection Problem\", page 3\n- \"inside the GLM with a multi-layer _perception_\", page 3\n- \"Pre-training Summerizers\", page 5"
                },
                "questions": {
                    "value": "- In page 2, the construction of the dataset is formulated as a paragraph sampling process $P(X | D_i, D'_i)$, which means that there is a probability (function of $\\epsilon_i$, which you describe as \"small\") that the paragraph sequence has no outliers. But in Section 4.2 you state that each data sample contains two coherent paragraphs, two outlier paragraphs, and one summary. Could you clarify this apparent inconsistency?\n\n- Do you have any evidence the issue of outliers is relevant outside your controlled summarization setting? If so, what is the impact of your detector to the task of interest (summarization)?\n\n- It seems that your MLP-based detector loses significant information (by mean pooling embeddings). In contrast, CODE has access to word level cross-attention and frequencies. Do you think they are fairly comparable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821256926,
            "cdate": 1698821256926,
            "tmdate": 1699636588134,
            "mdate": 1699636588134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "C77yGiv4Lm",
            "forum": "B37UmlxsaP",
            "replyto": "B37UmlxsaP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_G5oa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5653/Reviewer_G5oa"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes CODE, a novel outlier detector using a closed-form expression rooted in cross-attention scores. This can detect both cross-document and cross-domain outliers from a given input set of passages. Results show that CODE is superior under different datasets and architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper includes extensive experiments with several ablations that support their hypothesis.\n\n* The use of outlier detection in summarization models may help improve their performance, although this was not explored in the paper."
                },
                "weaknesses": {
                    "value": "* This statement in the introduction needs evidence: \"these models struggle with outlier paragraphs interspersed within the content\". This could be in the form of an experiment, where the summarization performance (especially in terms of relevance) increases when outlier detection is applied (e.g., by removing the outliers from the input). Without this, it is difficult to find motivations for such a task. Given how cross-document and cross-domain outliers are defined, I don't think these outliers actually exist naturally.\n\n* The datasets are synthetically created, which introduces a concern about whether or not this actually helps in real-world settings. Further experiments on how the outlier detection model affects the current summarization datasets (not the synthetic versions). How many of the input passages actually considered outliers by the model? Are these predicted outliers reasonable predictions?\n\n* What is the rationale behind the use of CNN/DM, SAMSum, Delve, and S2orc in your experiments? Do these datasets have naturally occurring outliers?"
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834426558,
            "cdate": 1698834426558,
            "tmdate": 1699636588039,
            "mdate": 1699636588039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]