[
    {
        "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"
    },
    {
        "review": {
            "id": "HGwImARnts",
            "forum": "QibPzdVrRu",
            "replyto": "QibPzdVrRu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_Qg2K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_Qg2K"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the learning dynamics in the special case of two-layer ReLU neural network with small initialization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results of this paper extend prior results from infinitesimal initialization to finitely small initialization.\n\nThe relation and difference with prior results are clearly presented. The paper is clearly written and easy to follow. The figures are helpful for understanding the argument."
                },
                "weaknesses": {
                    "value": "The setting of the neural network is unconventional. It requires the second layer weights to depend on the first layer weights in a way as shown in Eq.(3), instead of independently initialized.  This setting is used neither in practice nor in most theoretical analysis. According to the analysis,  I doubt that the results of this paper hold without this restriction on the second layer weights. \n\n> The paper has some discussion on this setting. However, it does not justify the validity of this setting. That it is commonly assumed in other papers does not directly justify. I would like to see some analysis, or at least some intuition, on why the results would hold on the natural setting.\n\nThe assumption on the data (Assumption 1) is strong, as it is not met by almost all real data. \n\nThe significance of the results is limited, as it is an extension of similar results from $\\epsilon \\to 0$ to the finite but small $\\epsilon$."
                },
                "questions": {
                    "value": "In Eq.(5), why the R.H.S. is independent of the network output $f(x_i)$? Or, why there is no such term $y_i-f(x_i)$ which usually appears in the expression of gradients."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698355805267,
            "cdate": 1698355805267,
            "tmdate": 1699636566564,
            "mdate": 1699636566564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b6tzMJmHRW",
                "forum": "QibPzdVrRu",
                "replyto": "HGwImARnts",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments, please see our response to your questions/concerns:\n\n**Clarification on balanced initialization**: We would like to point out that our balanced initialization assumption is made for theoretical analysis, and we believe it is not necessary for the dynamics behavior described by our Theorem 1 to happen in practice: All experiments in our manuscript use Gaussian initialization with small variance $\\alpha^2$ with some $\\alpha<<1$ (please see highlighted text on Page 9 for the details), and the numerical results can be well explained by our Theorem 1, despite not having a balanced initialization. \n\n**What happens if initialization is not balanced**: Having random Gaussian initialization (not balanced) for both layers only introduces additional technical challenges for the analysis, and we would like to discuss them here in detail: Even if the initialization is not balanced, the imbalance is preserved, i.e., one has $v_j^2(t)-\\Vert w_j(t)\\Vert^2=v_j^2(0)-\\Vert w_j(0)\\Vert^2=\\mathcal{O}(\\epsilon^2)$, thus $v_j(t)=sign(v_j(t))\\Vert w_j(t)\\Vert+\\mathcal{O}(\\epsilon^2)$. The last equation is ALL we use in the analysis regarding the last layer. In the balanced case, there is no $\\mathcal{O}(\\epsilon^2)$ term, and we also have $sign(v_j(t))=sign(v_j(0))$ fixed. If the initialization is not balanced, there is an additional $\\mathcal{O}(\\epsilon^2)$ error term in all our analyses, but it can be well controlled when $\\epsilon$ is sufficiently small. The only nontrivial part is that the $sign(v_j(t))$ is no longer fixed. Thus, one needs to keep track of the changes in $sign(v_j(t))$ over the GF trajectory, which is an interesting future research direction. Although there are additional technical challenges, NONE of them substantially change the GF dynamics: one still has the two-phase training (first alignment, then final convergence), and the neuron's directional dynamics still follow equation (4), with $sign(v_j(0))$ replaced by $sign(v_j(t))$. Therefore, our theoretical results would still hold without a balanced initialization assumption, and this is exactly what we show in the numerical experiments.\n\n**Assumption on the data**: Although we assumed separable data for our analysis, we would like to point out that our numerical experiments do not enforce this assumption: For the two-MNIST-digits experiment, we show that centered data approximately satisfies our Assumption 1 (please see the left-most plot in Figure 5). Despite not strictly satisfying Assumption 1, the numerical results can be well explained by our Theorem 1. That being said, we are working diligently to extend our results to more general/relaxed assumptions in our future work. Moreover, we would like to refer the reviewer to the global response we posted for a detailed discussion about our assumption on the data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572225254,
                "cdate": 1700572225254,
                "tmdate": 1700572225254,
                "mdate": 1700572225254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OLj9ffpPqo",
            "forum": "QibPzdVrRu",
            "replyto": "QibPzdVrRu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_5S56"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_5S56"
            ],
            "content": {
                "summary": {
                    "value": "The paper improves on previous theoretical analysis of the early alignement phase of the neurons of a shllow neural network initialized with small weights. This allows them to prove quantitative bounds in terms of the initialization scale, time, number of neurons and number of datapoints required to guarantee convergence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is easy to follow and explains well the previous issues and how they are solved. It is nice that the results apply to deterministic initialization of the weights, and do not require a random initialization (though they can of course be applied to this case)."
                },
                "weaknesses": {
                    "value": "The assumption of positively correlated labels and balancedness are very strong, and usually are not true in practice."
                },
                "questions": {
                    "value": "The description of Assumption 2 before the statement of the assumption does not match the statement of the assumption."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676326221,
            "cdate": 1698676326221,
            "tmdate": 1699636566448,
            "mdate": 1699636566448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K36zHpZMjb",
                "forum": "QibPzdVrRu",
                "replyto": "OLj9ffpPqo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive review. Please see our response to your questions/concerns:\n\n**Assumption on the data and initialization**: We agree with the reviewer that our assumptions that data is positively correlated and initialization is balanced are often unrealistic. However, we believe they are not necessary for the dynamics behavior described by our Theorem 1 to happen in practice. We would like to point out that our numerical experiments enforce NONE of these assumptions: For the two-MNIST-digits experiment, we show that centered data approximately satisfies our Assumption 1 (Please see the left-most plot in Figure 5), and we initialize the weights by Gaussian initialization with small variance (not balanced). Despite not strictly satisfying these assumptions, the numerical results can be well explained by our Theorem 1. That being said, we are working diligently to extend our results to more general/relaxed assumptions in our future work. Moreover, we would like to refer the reviewer to the global response we posted for a detailed discussion about our assumption on the data.\n\nWe hope our response addresses your concerns. If you have additional questions/concerns, please feel free to post another comment. We will respond to them before the discussion period ends."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571877479,
                "cdate": 1700571877479,
                "tmdate": 1700571877479,
                "mdate": 1700571877479,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uezHKvfizf",
            "forum": "QibPzdVrRu",
            "replyto": "QibPzdVrRu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_osUg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_osUg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of training a two-layer ReLU network for binary\nclassification using gradient flow with small initialization on well-separated input datasets, i.e. datasets $(x_i, y_i)_{i \\in [n]}$ with $ \\min \\frac{\\langle x_iy_i, x_jy_j  \\rangle}{\\Vert x_i \\Vert_2 \\Vert x_j \\Vert_2 } \\geq \\mu$.\nThey show that in time $O(\\frac{\\log(n)}{\\sqrt{\\mu}})$ all neurons are well aligned with the input data, which means that positive neurons show in the same direction as the positively labeled points (or have a negative scalar prouct with all vectors of the dataset) (and an equivalent result for negative neurons).\nFurhter they show that after the early alignment phase, the loss converges to zero at a $O(1/t)$ rate, and the weight\nmatrix on the first layer is approximately low-rank. Numerical experiments are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "All claims are proven and illustrations are supporting the explanations."
                },
                "weaknesses": {
                    "value": "The assumption that the dataset is well seperated is very strong. I don't see why one would use a neural network on such a dataset rather than linear regression."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5527/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5527/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5527/Reviewer_osUg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836267639,
            "cdate": 1698836267639,
            "tmdate": 1699636566366,
            "mdate": 1699636566366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0BCdKvcycG",
                "forum": "QibPzdVrRu",
                "replyto": "uezHKvfizf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the strengths of our paper. However, we strongly (yet respectfully) disagree with the reviewer's comment on the weaknesses. Please see our response to your questions/concerns:\n\n**Goal/Contribution of this paper**: We believe the reviewer's assessment of the contribution of our paper is inaccurate. The well-separated dataset studied in this paper is indeed easy and a linear classifier can achieve perfect accuracy. However, the point of this paper is NOT to find a classifier that fits the dataset, but rather to understand (a) how one can successfully train a ReLU network to fit the data, without getting stuck at some local minimum (as Reviewer Qhf5 pointed out, this can happen when training ReLU on linearly separable data); (b) why using a small initialization in the training of a two-layer ReLU network will lead to a classifier with special property (in our case, low-rank), while a network trained using a large initialization scale does not have such property [Chizat'19]. Through a complete, non-asymptotic, quantitative analysis of the GF on two-layer ReLU networks with small initialization, our results show that one can achieve the global minimum of the loss (with a characterization of the convergence rate), and provide a detailed analysis of how network weights evolve during training, along with its connection to the low-rank property of the weight matrix. As such, our paper makes a significant contribution toward understanding the implicit bias of small initialization in training a ReLU network.\n\n**Assumption on the data**: Although we assumed separable data for our analysis, we believe it is not necessary for the dynamics behavior described by our Theorem 1 to happen in practice. We would like to point out that our numerical experiments do not enforce this assumption: For the two-MNIST-digits experiment, we show that centered data only approximately satisfies our Assumption 1 (Please see the left-most plot in Figure 5). Despite not strictly satisfying Assumption 1, the numerical results can be well explained by our Theorem 1. That being said, we are working diligently to extend our results to more general/relaxed assumptions in our future work. Moreover, we would like to refer the reviewer to the global response we posted for a detailed discussion about our assumption on the data.\n\nWe hope our response addresses your concerns and the reviewer can re-evaluate our manuscript based on our response. If you have additional questions/concerns, please feel free to post another comment. We will respond to them before the discussion period ends. \n\nReference:\n\n[Chizat'19] Chizat, L., Oyallon, E., and Bach, F. On lazy training in differentiable programming. NeurIPS, 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571783519,
                "cdate": 1700571783519,
                "tmdate": 1700571783519,
                "mdate": 1700571783519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "snQcIF47cf",
            "forum": "QibPzdVrRu",
            "replyto": "QibPzdVrRu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_Qhf5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5527/Reviewer_Qhf5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of training a two-layer ReLU network classifier via gradient flow under small initialization. Training dataset assumes well-separated input vectors. Analysis of the neurons\u2019 directional dynamics establishes an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on the MNIST dataset validate the theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Interesting alignment behavior of the gradient flow for training two-layer ReLU networks under small initialization and separable data\n+ Rate analysis for the after-alignment-phase convergence"
                },
                "weaknesses": {
                    "value": "- The results only hold for correlated data. \n- Only ReLU activation functions are analyzed.\n- Some of the results have been previously known or observed, e.g., the solution of (stochastic) gradient flow finds in training two-layer ReLU networks on separable data is (almost) low-rank.\n- How small the initialization shall be to ensure the two-phase convergence is not qualitiatively discussed?"
                },
                "questions": {
                    "value": "1) Do the results/analysis extend to other activation functions? \n2) How small the initialization shall be to ensure the two-phase convergence? In general, since the training is nonconvex even with correlated/separable data due to the nonlinear relu activation function in the los. SGD/GD converges to a local minimum and indeed, this has been observed and numerically validated in the literature; see also [R1] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR. [R2] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP. In [R1], SDG for two-layer ReLU networks under separable data converges to local minimum; yet for leaky ReLU networks, it finds global minimum. In [R2], it also shows that plain SGD on ReLU networks using separable data converges to local minimum numerically. Yet, a bit modification on the SGD helps SGD converge to a global minimum but with random initialization. It would be great if a comparison can be made between the approach in [R2] and the small-initialization SGD for training two-layer ReLU networks on e.g., MINIST. Moreover, is there any transition for such initialization value to go from the two-phase to single-phase gradient flow convergence to local minimum?\n3) It would be great if more numerical tests are provided to demonstrate the two-phase convergence and provide the plots. \n4) If the second-layer weights are initialized not in a balanced manner (although not initialized according to (3)), I understand it would also work and guess that the imbalance between positive v_j and negative v_j values only influences the time it takes for the two-phases. More balanced initalization, faster convergence. It would be interesting to numerically validate if this is the case. \n5) There are some grammar issues and typos. Please correct."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5527/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698984379159,
            "cdate": 1698984379159,
            "tmdate": 1699636566280,
            "mdate": 1699636566280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O3IhKzVPvP",
                "forum": "QibPzdVrRu",
                "replyto": "snQcIF47cf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5527/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments, please see our response to your questions/concerns:\n\n**Do the results extend to other activation functions**: Yes. Please find a generalized version of Lemma 1 in Appendix B, where we extend the analysis of neuron directional dynamics to any two-layer network with a positive homogenous activation function of degree 1 (including all $\\alpha$-leaky ReLU activations, $\\alpha\\in[0,1]$). However, we note that our main results (Theorem 1) will not directly apply as the neuron directional dynamics have changed as we consider an activation function different than ReLU, and additional technical analysis is required to establish the directional convergence for general leaky-ReLU functions. \n\n**How small $\\epsilon$ should be for two-phase convergence**: On page 8, we have a remark \"choice of $\\epsilon$\" answering this question (we highlighted the text in the revision). In short, the initialization scale $\\epsilon$ needs to be made small so that (a) the approximation of neuron angular dynamics in equation (4) is good enough (having specific error bound), and (b) the alignment phase lasts long enough so that directional convergence described in Theorem 1 can be achieved. The quantitative analysis in the appendices shows that (a) requires $\\epsilon=\\mathcal{O}(\\frac{\\sqrt{\\mu}}{\\sqrt{h}n})$ and (b) requires $\\epsilon=\\mathcal{O}(\\frac{1}{\\sqrt{h}}\\exp(-\\frac{n}{\\sqrt{\\mu}}\\log n))$. For two-phase convergence, one needs $\\epsilon$ to be sufficiently small to satisfy both.\n\n**Comparison with some prior work using SGD**: We thank the reviewer for the references and added them to our comparison with prior work. However, we first would like to point out the key difference in the problem settings: In [R1, R2], the second-layer $v$ is **fixed** and not updated throughout the training, whereas in our manuscript, both the second-layer $v$ and the first-layer $W$ are updated by gradient flow dynamics. We took the reviewer's suggestion and did additional numerical experiments on our two-MNIST-digits classification problem (Please check the highlight section in Appendix A.5) by plotting the quantitative changes in neuron norms and directions separately. We showed that two-phase training (align then fit) happens when weights in both layers are trained, starting from a small initialization (the scenario studied in our manuscript). In contrast, there is no two-phase training when only the first-layer weights are trained (the scenario in [R1, R2]), even from a small initialization.\n\n**Assumption on the data**: We refer the reviewers to the global response we posted for a detailed discussion about our assumption on the data.\n\n**Clarification on balanced initialization**: We would like to point out that our balanced initialization assumption is made for theoretical analysis, and we believe it is not necessary for the dynamics behavior described by our Theorem 1 to happen in practice: All experiments in our manuscript use Gaussian initialization with small variance $\\alpha^2$ with some $\\alpha<<1$ (Please see highlighted text on Page 9 for the details), and the numerical results can be well explained by our Theorem 1, despite not having a balanced initialization. Using random Gaussian initialization for both layers only introduces additional challenges for the analysis. For example, an important one among these technical challenges is that the $sign(v_j(t))$ is no longer fixed if the initialization is not balanced, thus one needs to keep track of the changes in $sign(v_j(t))$ over the GF trajectory, which is an interesting future research direction. \n\nWe hope our response addresses your concerns (we will also proofread our manuscript to minimize grammar issues and typos). If you think there are additional experiments we can do, or clarifications we can make, please feel free to post another comment. We will respond to them before the discussion period ends. \n\nReference:\n\n[R1] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR\n\n[R2] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5527/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571611438,
                "cdate": 1700571611438,
                "tmdate": 1700571822224,
                "mdate": 1700571822224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]