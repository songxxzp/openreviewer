[
    {
        "title": "LipSim: A Provably Robust Perceptual Similarity Metric"
    },
    {
        "review": {
            "id": "D3GmYJ9YQy",
            "forum": "0w42S2Gp70",
            "replyto": "0w42S2Gp70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_sTNV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_sTNV"
            ],
            "content": {
                "summary": {
                    "value": "Authors argue that perceptual metrics inherit both the strengths and shortcomings of neural networks. One of the important shortcomings of neural Networks is their vulnerability to adversarial examples.\nThus it makes sense to design adversarially robust nn-based perceptual metrics. They show that the DreamSim metric is not robust to adversarial examples. They then propose a robust perceptual\nsimilarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presented method is first in its kind to provide a provably robust perceptual metric. Authors have shown that this method is more robust than existing perceptual metrics (DreamSim). Several experiments are conducted to show the efficacy of the method. \n\nThe paper in general is well organized and well-written. There are also several experiments that show effectiveness of the method."
                },
                "weaknesses": {
                    "value": "Although there has been some previous works on robust perceptual metrics, authors claim theirs is the first one with provable guarantees. I still think discussing why this matters in practice is important. I see the application for image retrieval, but how can someone have access to the model (white box) to actually attack it. Please elaborate.\n\nConsider changing the colors in bar plot of Fig. 3.a. They are hard to distinguish."
                },
                "questions": {
                    "value": "Q: Please mention why we need robust perceptual metrics??!!! In real world. In other words, how much impact such work has in real world and why it actually matters? An example may help here.\n\nQ: How about some non NN-based similarity metrics? I am guessing maybe there are not as good as NN-based ones but at least they might be more robust! Can you compare your method agains some of those m\nMetrics? Is DISTS in Fig. 3.a NN-based?\n\nQ: Other datasets than Nights dataset? What is the guarantee that these results will also generalize to other datasets?\n\n\nQ: In 5.1, you are stating \u201cCan adversarial attacks against SOTA metrics cause: (1) misalignment with human perception?\u201d. \nIn section 4.1, you mention \u201cQ: Please mention why we need robust perceptual metrics??!!! In real world. In other words, how much impact such work has in real world and why it actually matters? An example may help here. Alternatively, shouldn\u2019t two image differ significantly when perturbation is high? Have you considered what happens at high perturbations?\nShouldn\u2019t a good perceptual metric match human judgements regardless of perturbation magnitude?\n\n\nQ: Table 2 shows the certified scores. However, the scores for perturbation 72/255 and beyond are still pretty low. How do humans behave in those perturbations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811325871,
            "cdate": 1698811325871,
            "tmdate": 1699636693104,
            "mdate": 1699636693104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EAc8UmksZo",
                "forum": "0w42S2Gp70",
                "replyto": "D3GmYJ9YQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sTNV"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and comments.\n\n**Q: Although there has been some previous works on robust perceptual metrics, authors claim theirs is the first one with provable guarantees. I still think discussing why this matters in practice is important.**\n\nWe provide the first perceptual similarity metric with non-trivial provable guarantees. We do agree that comparing to existing work is important and we have added the comparison with other metrics to the appendix of the revision of our paper and provided the reference to it in the Related Work.\n\nThere exist two methods that apply randomized smoothing to perceptual similarity metrics [1, 2]. We have mentioned them in the related work section of our paper. These methods have important limitations:\n- They are computationally _very expensive_ due to the Monte Carlo sampling for each data point (i.e., [1] mentions 1 to 3 minutes for each image with a backbone smaller than the Dreamsim model) which makes this approach impractical for real use cases with a large model like Dreamsim\n- The approach is probabilistic due to the estimation of the probability with the Monte Carlo sampling\n- The proposed certified bounds are loose, for example, for LPIPS [6] the bound is as follows: $d(x, x+\\delta) \\leq 10 ||\\delta||_2$. Therefore the results are reported for a perturbation budget of 0.01 to 0.05, whereas for the LipSim the certificates are provided for a perturbation budget of $\\frac{36}{255}$ to $\\frac{108}{255}$.\n\n**Q: I see the application for image retrieval, but how can someone have access to the model (white box) to actually attack it. Please elaborate.**\n\nIn security research, it is common practice to study the worst-case setting, i.e., white-box attacks in the context of adversarial robustness. This is for multiple reasons:  \n- (1) in practice, many neural networks are released in white-box open form, for example, the most recent models from Meta. These networks do in fact need to be robust to adversarial attack.   \n- (2) Even black-box models might need to consider vulnerability to white-box attacks in case they are inadvertently leaked (data breaches are a common occurrence). Relying on a lack of access to the details of an implementation, i.e., security via obscurity, is considered as a bad practice in the literature.    \n- (3) Finally, several results show that black-box models can be attacked using _transferability_ [3]  i.e., attacks obtained on proxy models often work just as well on other models.   \n\nIn this paper, we provide robustness in a white-box setting, specifically with certified robustness, and show that we can provide security and reliability within the certified bounds and we also evaluate our model with empirical approaches. \n\n**Q: Consider changing the colors in bar plot of Fig. 3.a. They are hard to distinguish.**\n\nSure, based on your suggestion, we changed the colors of Fig. 3.a in the revision of our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235532116,
                "cdate": 1700235532116,
                "tmdate": 1700236007725,
                "mdate": 1700236007725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZLySpP7KGi",
                "forum": "0w42S2Gp70",
                "replyto": "D3GmYJ9YQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Please mention why we need robust perceptual metrics??!!! In real world. In other words, how much impact such work has in real world and why it actually matters? An example may help here.**\n\nPerceptual metrics have many different applications in the real world, namely, the comparison of images using a human semantic measure for image and video processing, image retrieval based on semantic information, and filter detection in social media websites (e.g., CSAM$^1$ detection see below). These contexts can often be in an adversarial setting $^2$ and having a perceptual metric robust to small perturbation, i.e., perturbations that do not change the semantic, is a property that makes sense to have in this type of metric. \nWe should note that perceptual metrics can also be used to train networks which requires a semantic loss, having a robust perceptual metric could lead to better and more robust models.\n\nIn this work, we leveraged the DreamSim metric [4] which is the state-of-the-art for perceptual similarity metric $^3$. While this metric provides a great similarity measure, we have demonstrated that it is not robust to small perturbations *that do not change the semantic* (As shown in Figure 1 of the paper, a small perturbation can change the perception of the Dreamsim metric, and two images semantically similar are then considered very different.). If this metric were to be deployed into real-world applications, this might lead to important security issues. \n\nAs an example of a real-world application, we could mention Apple\u2019s NeuralHash [5] used for CSAM detection, which has two steps: (1) it uses a Convolutional Neural Network to map semantically similar images to close feature vectors (in terms of cosine similarity) and (2) it leverages those vectors to find the duplicates using hash functions. Based on the pipeline, the vulnerability of the NN generating the feature vectors is a serious security concern. We refer to [6] as one of the papers that worked on the adversarial vulnerabilities of the Neural Network generating the features for the NeuralHash and showed that using the proposed adversarial attack, the features can change and lead to a different output for the hash function. It would be a very interesting future work to use the robust embeddings we've trained as the backbone of NeuralHash to increase the robustness of the feature vectors.\n\n$^1$ CSAM: Child Sexual Abuse Material  \n$^2$ CSAM detection is inherently an adversarial setting where malicious users would want to bypass the detection.  \n$^3$ The Dreamsim paper has been accepted at NeurIPS 2023 as a spotlight. \n\n\n**Q: How about some non NN-based similarity metrics? I am guessing maybe there are not as good as NN-based ones but at least they might be more robust! Can you compare your method against some of those Metrics? Is DISTS in Fig. 3.a NN-based?**\n\nWe have reported the results for the non-NN-based metrics with the \"Low level\" title in Figure 3.A. The natural 2AFC scores are low and are not comparable with Dreamsim. This result is expected because the 2AFC score shows the alignment of the distance metric with human perception and how well the metric compares the images at the semantic level. Pixel-level metrics are more focused on the pixel level rather than the semantic level and therefore less aligned with human perception in comparison with the perceptual metrics which are more focused on the semantics. On the other hand, as the natural scores of non-NN-based metrics are already lower than LipSim's robust scores (under $\\ell_2$-AutoAttack) (and their robust scores won't be better than their natural scores,) we didn't need to do the experiments under the attack for the \"Low level\" methods. \n\nDISTS indeed offers competitive performance on the NIGHT dataset as shown in Figure 3.a. The DISTS[7] metric is based on the VGG architecture (like the LPIPS is based in the ALexNet architecture) and due to being based on a neural network is not robust to small perturbations crafted using a strong attack. However, the certified robustness of LipSim guarantees that independent from the strength of the attack, the certified scores cannot be broken.\n\n| Metric | Accuracy | $\\frac{36}{255}$ | $\\frac{72}{255}$ | $\\frac{108}{255}$ |\n| ---- | --- |---- | ------ | ------ |\n| DISTS | 86.0 | 0.0 | 0.0 | 0.0 |\n| LipSim | 85.09 | 67.32 | 43.26 | 19.02 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236228883,
                "cdate": 1700236228883,
                "tmdate": 1700236272089,
                "mdate": 1700236272089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvHKCsRNkH",
                "forum": "0w42S2Gp70",
                "replyto": "D3GmYJ9YQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: Other datasets than NIGHTs dataset? What is the guarantee that these results will also generalize to other datasets?**\n\nCurrently, two 2AFC datasets are available: NIGHT and BAPPS [8]. The result of LipSim on the NIGHT dataset is reported in the paper. Here we report the result of LipSim on the BAPPS dataset. The LipSim model is not finetuned on the BAPPS dataset and the same model which is finetuned on the NIGHT dataset is used.\n\n| Metric | Accuracy | $\\frac{36}{255}$ | $\\frac{72}{255}$ | $\\frac{108}{255}$ |\n| ---- | --- |---- | ------ | ------ |\n| DreamSim | 78.47 | 0.0 | 0.0 | 0.0 |\n| LipSim | 74.29 | 31.20 | 15.07 | 6.77 |\n\n**Q: (1) Alternatively, shouldn\u2019t two images differ significantly when perturbation is high? (2) Have you considered what happens at high perturbations? (3) Shouldn\u2019t a good perceptual metric match human judgements regardless of perturbation magnitude? (4) Table 2 shows the certified scores. However, the scores for perturbation 72/255 and beyond are still pretty low. How do humans behave in those perturbations?**\n\n1 - Yes, two images should differ significantly when the perturbation is high.    \n2 - We have experimented with increasing perturbation, starting with a small perturbation $\\epsilon = \\frac{36}{255}$ and increasing to $\\epsilon = \\frac{108}{255}$. As shown in the paper, perceptual similarity metrics (as with neural networks) are not robust to small adversarial perturbations. It is therefore safe to say that won't be robust to large adversarial perturbations.      \n3 - Yes, a good perceptual metric should match human judgments regardless of perturbation magnitude. This highlights the fact that building a good perceptual metric is still an open problem and our contribution is a first step in this direction. Our Lipsim perceptual metric achieves competitive results on natural performance and obtains a good level of robustness that Dreamsim doesn't have.    \n4 - The perturbation level of $\\epsilon = \\frac{72}{255}$ is classic in the adversarial robustness literature, for example, the state-of-the-art certified robust classifier with Lipschitz continuity [9] reports a certified accuracy with $\\epsilon = \\frac{36}{255}$.   \n\n[1] Aounon Kumar and Tom Goldstein. Center smoothing: Certified robustness for networks with structured outputs. NeurIPS 2021   \n[2] Shao et al. Robustness certification for structured prediction with general inputs via safe region modeling in the semimetric output space. SIGKDD 2023  \n[3] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).   \n[4] Fu et al. DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data, NeurIPS 2023 Spotlight.    \n[5] Apple. CSAM Detection - Technical Summary. https://www.apple.com/childsafety/pdf/CSAM_Detection_Technical_Summary.pdf 2021.   \n[6] Struppek et al. \"Learning to break deep perceptual hashing: The use case neuralhash.\" ACM Conference on Fairness, Accountability, and Transparency. 2022.   \n[7] Ding et al. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence 2020.  \n[8] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.  \n[9] Hu et al. Unlocking Deterministic Robustness Certification on ImageNet. NeuIPS 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236538721,
                "cdate": 1700236538721,
                "tmdate": 1700243897752,
                "mdate": 1700243897752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3V0L6eziDO",
            "forum": "0w42S2Gp70",
            "replyto": "0w42S2Gp70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_YdcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_YdcH"
            ],
            "content": {
                "summary": {
                    "value": "This study investigates Lipschitz Networks' robustness against small adversarial perturbations, capitalizing on Lipschitz continuity. A 1-Lipschitz function can bound a network's output to prevent it from changing excessively in response to slight input changes. Another key focus is the DreamSim perceptual distance metric, which utilizes cosine distance on combined feature vectors derived from various representation learning techniques. Furthermore, a new metric, LipSim, is introduced, aiming to be robust against adversarial disturbances. LipSim's design employs a two-step training process that first uses DreamSim as a guiding model on the ImageNet dataset, followed by fine-tuning on the NIGHT dataset to enhance its robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- LipSim employs a 1-Lipschitz network backbone, which when combined with certain design choices, greatly enhances its resistance to adversarial perturbations.\n- When applied to the real-world task of image retrieval, LipSim effectively identified semantically close images, even when faced with adversarial image queries.\n-  LipSim excels both in empirical and certified robustness tests. This dual proficiency ensures that the metric's performance is not only observed in experimental conditions but also certified under specific robustness criteria."
                },
                "weaknesses": {
                    "value": "- The natural score of LipSim was observed to be lower than that of some competitors like DreamSim. This might raise concerns about its general performance when not under adversarial conditions.\n- The real-world application testing of LipSim was primarily on image retrieval. It would be beneficial to see its performance on a wider variety of tasks."
                },
                "questions": {
                    "value": "n/a"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836915339,
            "cdate": 1698836915339,
            "tmdate": 1699636692975,
            "mdate": 1699636692975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9pKwCrecNb",
                "forum": "0w42S2Gp70",
                "replyto": "3V0L6eziDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YdcH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and comments.\n\n**Q: The natural score of LipSim was observed to be lower than that of some competitors like DreamSim. This might raise concerns about its general performance when not under adversarial conditions.**\n\nThis paper is set in the context of adversarial robustness. In this context, there is an _inherent trade-off between natural performance and robustness_ $^1$. State-of-the-art methods, like DreamSim, can achieve high natural performance at the expense of losing robustness, as demonstrated in Figure 1 of our paper. LipSim does not achieve state-of-the-art natural performance but achieves competitive performance compared to Dreamsim _and_ provides state-of-the-art empirical and certified robustness. We believe Lipsim is a major first step towards robust perceptual similarity metrics. \n\n$^1$ As an example, the state-of-the-art robust neural network (accepted at NeurIPS 2023) [1] with Lipschitz continuity achieves 45.6% natural accuracy on ImageNet with 35.0% certified accuracy while the non-robust state-of-the-art results on ImageNet is 91.1% [2] and the community has been trying to boost the natural accuracy for robust models, despite the progressions this is still an ongoing topic of research. \n\n\n**Q: The real-world application testing of LipSim was primarily on image retrieval. It would be beneficial to see its performance on a wider variety of tasks.**\n\nBased on your suggestion, we are also adding KNN (k-nearest neighbors algorithm), which is a zero-shot classification task. It classifies test images based on the proximity of their feature vectors to the training images' feature vectors. We performed our experiment on Imagenette (ImageNet dataset with 10 classes) [3] and for $k=10,20$. The results of LipSim and DreamSim are reported in the following table, which are very close in terms of the Top 5 and LipSim is showing a decent performance in terms of the Top 1 as a zero-shot classifier.\n\n| Metric | 10-NN Top 1 | 10-NN Top 5 | 20-NN  Top 1 | 20-NN Top 5 |\n| ---- | --- |---- | ------ | ------ |\n| DreamSim | 99.03 | 99.82 | 98.82 | 99.89 |\n| LipSim | 85.32 | 97.20 | 85.35 | 98.09 |\n\n[1] Hu et al. Unlocking Deterministic Robustness Certification on ImageNet. NeuIPS 2023  \n[2] Chen et al. Symbolic Discovery of Optimization Algorithms. NeuIPS 2023  \n[3] https://github.com/fastai/imagenette"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234643537,
                "cdate": 1700234643537,
                "tmdate": 1700234643537,
                "mdate": 1700234643537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a6h29uWT1h",
            "forum": "0w42S2Gp70",
            "replyto": "0w42S2Gp70",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_KFDY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6307/Reviewer_KFDY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to develop a robust perceptual similarity metric. Specifically, this paper propose to leverage 1-Lipschitz neural network to provide guarded areas around each data point. The experimental results illustrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results looks very good when against auto attacks.\n\nThe proof seems to make sense but I am not an export with this."
                },
                "weaknesses": {
                    "value": "The presentation could be better. For example, the explanation of 2AFC is little which making it difficult to get the messages from this paper.\n\nThe experiments only conducted with Auto Attack. However, there are different kinds of attacks, and it would be good to experiment with other attack methods as well.\n\nIt would also be good to compare with other certified or non-certified defense methods."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6307/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888151052,
            "cdate": 1698888151052,
            "tmdate": 1699636692871,
            "mdate": 1699636692871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f04IyTFSgY",
                "forum": "0w42S2Gp70",
                "replyto": "a6h29uWT1h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KFDY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and comments.\n\n**Q: The presentation could be better. For example, the explanation of 2AFC is little which making it difficult to get the messages from this paper.**\n\nWe had provided a detailed explanation of 2AFC datasets in Appendix B.1 but did not reference it in the introduction. We have corrected this in the revised version of the paper. \n\nConceptually, 2AFC datasets are datasets labeled by humans that are used to align perceptual metrics to human judgment. The dataset is labeled via a two-alternative forced choice (2AFC) test that asks which of two variations of a reference image is more similar to it. We provided examples of the night dataset in Appendix B.1 and gave more detail on how the dataset is constructed. For a full description of this dataset, please see the DreamSim paper [1].\n\n\n**Q: The experiments only conducted with Auto Attack. However, there are different kinds of attacks, and it would be good to experiment with other attack methods as well.**\n\nWe agree that it is important to evaluate with a diversity of different attacks. Indeed, AutoAttack [2] is itself an ensemble of four different white box and black box attacks (i.e. APGD-CE, APGD-DLR, with two existing complementary attacks, FAB [3] and Square Attack [4]). We have further clarified and emphasized this in the revised paper. By choosing AutoAttack, we have both the state-of-the-art attack and the diversity of white box and black box attacks as you mentioned. Based on your input, we have further increased attack diversity by adding results from two additional attacks as explained below.\n\nFor our experiment, we evaluate the LipSim in two different settings:\n\n**1 - NIGHT dataset (2AFC dataset)**:  the night dataset is a binary classification setting and the attacker's goal is to make the model misclassify. The following attack methods are used to evaluate the robustness of LipSim in the 2AFC setting:\n\n- We provided $\\ell_2$-AutoAttack results in Table 1 of the paper. AutoAttack [2] is known as a state-of-the-art attack in the adversarial robustness community. \n- We also have provided the results for the $\\ell_{\\infty}$-AutoAttack and $\\ell_{\\infty}$-PGD shown in Table 3 of the Appendix.\n- Based on your suggestion regarding the evaluation of LipSim using different attacks, we added the Momentum Iterative Attack (MIA) [5] for both $\\ell_{\\infty}$ and $\\ell_{2}$. The new results are in line with our previous results and show the superiority of LipSim compared with DreamSim in terms of robustness.\n- We also added the results on another dataset per the suggestion of Reviewer sTNV. \n\n| Metric | Natural Score| $\\ell_2$-MIA $\\epsilon= 0.5$ | $\\ell_2$-MIA $\\epsilon= 1.0$ | $\\ell_2$-MIA $\\epsilon= 2.0$ | $\\ell_{\\infty}$-MIA $\\epsilon=0.01$ | $\\ell_{\\infty}$-MIA $\\epsilon=0.02$ | $\\ell_{\\infty}$-MIA $\\epsilon=0.03$ |\n| ---- | ------ | --- | ---- | ------ | --- | ---- | ------ | \n| DreamSim | 96.16 | 61.79 | 52.85 | 52.69 | 2.08 | 0.05 | 0.0 |\n| LipSim | 85.09 | 82.79 | 79.99 | 80.10 | 62.45 | 34.38 | 15.84|\n\n**2 - Perceptual metric**: In this setting, we evaluate the robustness of Dream and LipSim (denoted $d$) using a direct attack to the metric by optimizing the perturbation within a budget in the loss defined in Equation 14 $\\underset{||\\delta||_2 \\leq \\epsilon}{argmax}\\, d(x, x+\\delta)$, we demonstrated that we can find small perturbations that fool the metric to predict larger values for $d(x, x+\\delta)$.\nThe distribution $d(x, x+\\delta)$ is shown in Figure 3b, for both LipSim and DreamSim, which highlights the robustness gaps between LipSim and DreamSim."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234053291,
                "cdate": 1700234053291,
                "tmdate": 1700234053291,
                "mdate": 1700234053291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UtrKR4xj6e",
                "forum": "0w42S2Gp70",
                "replyto": "a6h29uWT1h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6307/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q: It would also be good to compare with other certified or non-certified defense methods.**\n\n**Certified Defenses**: There exist two methods that apply randomized smoothing to perceptual similarity metrics [6, 7]. We mentioned them in the related work section of our paper. These methods have important limitations:\n- They are computationally *very expensive* due to the Monte Carlo sampling for each data point (i.e., [6] mentions 1 to 3 minutes for each image with a backbone smaller than the Dreamsim model) which makes this approach impractical for real use cases with a large model like Dreamsim\n- The approach is probabilistic due to the estimation of the probability with the Monte Carlo sampling\n- The proposed certified bounds are loose, for example, for LPIPS [6] the bound is as follows: $d(x, x+\\delta) \\leq 10 ||\\delta||_2$. Therefore the results are reported for a perturbation budget of 0.01 to 0.05, whereas for the LipSim the certificates are provided for a perturbation budget of $\\frac{36}{255}$ to $\\frac{108}{255}$.\nWe have highlighted the difference between Lipsim and previous approaches in the revision of the paper. \n\n**Non-certified Defenses (Empirical robustness)**: R-LPIPS [8] leverages the LPIPS perpetual similarity metric and Adversarial training to provide empirical robustness. We provided an empirical robustness score on the BAPPS dataset [9] as well as the NIGHT dataset [1] below. We would like to emphasize that R-LPIPS is trained on the BAPPS dataset while Lipsim is finetuned on the NIGHT dataset and Lipsim provides certified accuracy while R-LPIPS does not come with any provable guarantees. \n\n- Results on the BAPPS dataset\n\n| Metric | Natural Accuracy | $\\ell_{\\infty}$-PGD  $\\epsilon=0.03$ | $\\ell_{2}$-MIA $\\epsilon=1.0$  | \n| ---- | --- | --- | --- |\n| R-LPIPS | 80.25 | 70.94 | 72.38 |\n| LipSim | 73.47 | 42.77 | 60.09 |\n\n- Results on the NIGHT dataset\n\n| Metric | Natural Accuracy | $\\ell_{\\infty}$-PGD  $\\epsilon=0.03$ | $\\ell_{2}$-MIA $\\epsilon=1.0$ | \n| ---- | --- | --- | --- |\n| R-LPIPS | 70.56 | 32.46 | 58.50 |\n| LipSim | 85.09 | 75.27 | 79.99 |\n\n\n[1] Fu et al. DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data, NeurIPS 2023 Spotlight.  \n[2] Croce et al. Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse. ICML 2020.  \n[3] Croce, F. and Hein, M. Minimally distorted adversarial examples with a fast adaptive boundary attack. In ICML, 2020.\n[4] Andriushchenko, M., Croce, F., Flammarion, N., and Hein,\nM. Square attack: a query-efficient black-box adversarial\nattack via random search. In ECCV, 2020.\n[5] Dong et al. Boosting Adversarial Attacks with Momentum. CVPR 2018  \n[6] Kumar et al. Center smoothing: Certified robustness for networks with structured outputs. NeurIPS 2021  \n[7] Shao et al. Robustness certification for structured prediction with general inputs via safe region modeling in the semimetric output space. SIGKDD 2023  \n[8] Ghazanfari et al. R-lpips: An adversarially robust perceptual similarity metric. ICML Workshop 2023.  \n[9] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6307/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234291763,
                "cdate": 1700234291763,
                "tmdate": 1700243366817,
                "mdate": 1700243366817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]