[
    {
        "title": "SHINE: Shielding Backdoors in Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "wGURG9E6QJ",
            "forum": "AKAlVyunxA",
            "replyto": "AKAlVyunxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an explanation-based approach to defend against backdoor attacks in deep reinforcement learning. The main observation is that a backdoor attack aims to cause an RL agent to fail a game or get a very low total reward, which can be explained using the triggers present in the environment. To this end, the paper applies the self-explainable framework of Guo et al., 2021b, called EDGE, to identify a set of time steps where triggers are most likely to present and then extract a set of features from the state vector of these time steps and treat their average as triggers. The paper applies their approach to detect both perturbation-based backdoor attacks in single and multi-agent settings, where triggers are injected into states, and adversarial agent attacks, where a sequence of actions serves as triggers. It further proposes a backdoor shielding method to retrain the agent in the operating environment. The approach is evaluated using Atari games and the SMC environment for perturbation-based attacks and the MuJoCo environments for adversarial agent attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Although various explanation-based defenses have been proposed for protecting deep learning, applying policy explanation techniques to protect RL agents from backdoor attacks seems new and promising. The approach does not require access to a clean environment or a set of clean trajectories. \n\nThe proposed approach applies to both perturbation-based attacks and adversarial agent attacks, outperforms several baselines in the former case, and obtains reasonable performance in the latter under some simple backdoor attacks against RL."
                },
                "weaknesses": {
                    "value": "The paper is a direct application of the self-explainable framework for RL in Guo et al., 2021b, where the framework has already been applied to identify critical time steps associated with adversarial agent attacks. The technical contribution seems limited. \n\nThe proposed method relies on reward signals in the actual environment in its explanation component, which requires many failure trajectories to identify triggers. Hence, it is not applicable in security-critical domains.\n\nI am not convinced that the approach can work in more challenging settings. To identify critical time steps associated with triggers using the EDGE framework, two assumptions are needed. First, there is a set of failure trajectories available. Second, the agent is supposed to win the game if the triggers are removed. However, a successful attack does not necessarily lead to a clear failure, especially in the case of stealthy attacks, where the goal could be reducing the agent's reward. Without domain knowledge of the expected performance of the agent in the target environment, it is hard to define what is considered a failure. Further, the evaluation considers a backdoor attack with a single fixed trigger placed at a fixed location, which is rather limited."
                },
                "questions": {
                    "value": "How is (4) solved? Algorithm 1 only shows how to estimate the constraint but not how the shielding policy is optimized. \n\nHow are the set of trajectories used to train SHINE generated in the experiments? Are these all known to be failures? \n\nThe trigger detection stage of SHINE takes 12 hours, with an additional 5 hours used to retrain the policy. Is this comparable with other baselines? Since the approach requires many interactions with an infected environment, the agent will suffer from a significant loss until the trigger is mitigated. Hence, it seems unfair to only consider the reward after retraining when comparing the approach with other baselines. \n\nGuo et al., 2021b show that significant improvement can already be achieved by partially blinding the victim agent's observation at the critical time steps in losing episodes. Thus, it would help to have an ablation study demonstrating the advantage of feature-level explanation and policy retraining.  \n\nIn the experiment, it is assumed that there is a trigger in the environment with a probability of 0.1, 0.2, or 0.3. I wonder what would happen if triggers were present most of the time.  \n\nThe evaluation uses a single fixed trigger pattern across RL training and testing. I wonder what would happen if the attacker varies the trigger used over rounds/episodes. \n\nTable 2 shows that SHINE performs better than the original PPO in a clean environment. Why is this the case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730691793,
            "cdate": 1698730691793,
            "tmdate": 1700728957183,
            "mdate": 1700728957183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g0tvrvDNOh",
                "forum": "AKAlVyunxA",
                "replyto": "wGURG9E6QJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review 2tnZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. Please see below for our responses. \n\n**1. About the technical contributions.**\n\nThank the reviewer for the comment. We agree with the reviewer that our method indeed uses the explanation method proposed by Guo et al., 2021b as it is the state-of-the-art step-level explanation method.  However, we would like to respectfully argue that this does not dilute our method\u2019s technical contribution.  First, the overall defense framework that is composed of explanation-driven trigger restoration and retraining-based shielding is novel and is first proposed in this work. Second, rather than the step-level explanation, all the other technical components are newly proposed in this work. As such, we propose a new defense framework for backdoor defense with only one component leveraging an existing method. It is common that a newly proposed technique uses some state-of-the-art methods as part of its framework. We believe the proposed technique still demonstrates decent technical contributions. \n\n**2. About the proposed method requiring failure trajectories.**\n\nThank the reviewer for the comment. As shown in the response to Review r1tp, we demonstrate that our method can work with only 20 failure trajectories. In addition, as stated in Section 5-Postmortem defense, we discuss that our goal is to quickly react to the backdoor attack once it is triggered. This is a common setup even for security-critical domains. For example, in cybersecurity, there is a directly called postmortem program analysis, which analyzes the reason and fixes the vulnerabilities after it is triggered. As is also discussed in Section 3.1, in practice, it is also difficult to guarantee the environment is clean. For example, in self-driving cars, it is extremely hard to guarantee that the attacker cannot access the environment where the car is deployed. As such, it is possible that the attacker will trigger a backdoor and cause some damage. Our goal is to react quickly and make sure similar damage will not happen in the future. \n\n**3. About the application of our method in other settings.**\n\nThank the reviewer for the comment.  The reviewers pointed out two setups where our defense may not be applicable. First, the reviewer mentions that a successful attack does not necessarily lead to a clear failure. We would like to respectfully mention that as a defense work, we follow the setups in existing attacks. We believe this is the case for most defenses (not only backdoor defenses in DRL). As stated in the attack papers (TrojDRL and backdoorl), for games with a discrete final reward (win or lose), the attack goal is to make the victim agent lose the game. For games with continuous rewards, the attack goal is to reduce the victim agent\u2019s total reward. As stated in Eqn. (1), the step-level explanation considers both cases. Actually, as shown in Table 2, the attacks on Atari games reduce the victim agent\u2019s reward (this is no clear win or lose). Our method can still maintain its effectiveness in such cases.   \n\nSecond, the reviewer mentions that the attack can change the trigger\u2019s location. Again, we would like to respectfully point out that this is the setup considered in existing attacks. In Section-5 adaptive attacks, we indeed try to change the trigger location as it can be an adaptive attack against our defense. However, the attack is not successful. This result demonstrates that our defense is effective for state-of-the-art attacks, which is enough to demonstrate its efficacy. As the attack and defense, in general, is a cat-and-mouse game. Our defense and this fail adaptive attack motivate the design for stronger attacks. We believe this can be an interesting future work. However, it is not the focus of this paper. \n\n**4. How to solve the restraining objective function and how to collect trajectories for the step-level explanation.**\n\nThank the reviewer for the comment. First, we solve Eqn. (4) via gradient descent. We follow the procedure as the PPO algorithm, with a different set of constraints. We will clarify this in the next version. Second, we collect the trajectories of the given policy by running it in the operating environment where the trigger will show up. We will collect the state, action, and reward for each trajectory for applying the explanation method. We will clarify this in the next version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150790222,
                "cdate": 1700150790222,
                "tmdate": 1700150790222,
                "mdate": 1700150790222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pKvYF0k909",
                "forum": "AKAlVyunxA",
                "replyto": "g0tvrvDNOh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. While the Atari results make sense, it's still unclear when and how the solution should be applied in practice for environments without a clear definition of failures. The experiments assume that there are always triggers in the chosen trajectories. What if the assumption is wrong? In particular, the agent may experience failures or low returns with a certain probability even in a clean environment without triggers. What would happen if the algorithm is applied to this case?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376463698,
                "cdate": 1700376463698,
                "tmdate": 1700376463698,
                "mdate": 1700376463698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ve3CoFfUkS",
                "forum": "AKAlVyunxA",
                "replyto": "pKvYF0k909",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "content": {
                    "comment": {
                        "value": "It is also unclear why the explanation-based solution can work when no clean states exist. How many failure trajectories and winning trajectories are there in the experiment? Do the winning trajectories have any clean states?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377673788,
                "cdate": 1700377673788,
                "tmdate": 1700377673788,
                "mdate": 1700377673788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tBd1MBgxNr",
                "forum": "AKAlVyunxA",
                "replyto": "ow7ONKifog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_2tnZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. It seems that the approach heavily relies on the fact that the attacker uses a fixed trigger. Otherwise, it does seem possible to identify the trigger using the failure trajectories only without any clean states. Thus, I'm still not convinced that the approach will work against more advanced attacks. That being said, I do see the potential of the explanation-based approach and appreciate the authors' effort in adding experiment results and providing detailed justifications. I'll increase my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728905239,
                "cdate": 1700728905239,
                "tmdate": 1700728905239,
                "mdate": 1700728905239,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kbl3L6rO7d",
            "forum": "AKAlVyunxA",
            "replyto": "AKAlVyunxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
            ],
            "content": {
                "summary": {
                    "value": "In this In this paper, the authors study the problem of defense against backdoor attack in deep reinforcement learning. They present a practical algorithm called SHINE that first identifies backdoored features in attacked states and then sanitizes them in the hope of rendering the backdoor behavior ineffective on the agent. They claim that their methods theoretically improves the backdoored agent\u2019s performance under attack while still retaining its performance in clean environment. They also test their method on three benchmark deep RL environments and show its effectiveness in eliminating backdoor attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies an important problem of safeguarding against adversarial attacks in deep RL.\n2. Their algorithm is well motivated and the authors claim it to work against tested benchmark environment."
                },
                "weaknesses": {
                    "value": "1. No formal definition of attack and defense is provided. It is not even clear when does attacker attack, does it use any attack policy? The notations used in section 3.3 is not sufficiently clear.\n    \n2. In adversarial agent attack, how are actions embedded in state as you claim in the beginning of page 5? Actions usually come from a different space of discrete state than states.\n    \n3. This is perhas a good empirical paper, but it is not sound theoretically. I would formulate the problem properly and tune down the emphasis to adversarial agent attack part and theoretical claims to make a good case for it. It is necessary to define the assumptions under which the theoretical claims are guaranteed to work.\n\n4. I believe for sanitization using masking approach to work, it needs certain assumption on kind of triggers that adversary can be put in environment and it is not clear from the paper why your approach would work in a general case. See question 4 for an example."
                },
                "questions": {
                    "value": "1. Learning a feature explanation mask for each pixel in a state does not look very scalable especially in environments like Atari games where state space could get really large. How do you address this problem?\n    \n2. What is local linear constraint in line above Feature-level explaination paragraph? How does it help?\n    \n3. Without any assumption, it may happen that the adversary may not attack when the algorithm is run. Does the algorithm still work?\n    \n4. As a case study, let say the adversary designs a patch trigger to be put on top right corner of the state image(say in ping pong game). Normally, the pixel value is uniformly 100 and the adversary has trained the policy so that when pixel value is 255 or 0, it takes backdoor action. While attacking the adversary only inject 255 pixel patch. Your method of simply deleting the feature would lead to a zero patch which is adversarial as well. How would you fix this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786775226,
            "cdate": 1698786775226,
            "tmdate": 1699637073928,
            "mdate": 1699637073928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qlpaiNmlHr",
                "forum": "AKAlVyunxA",
                "replyto": "Kbl3L6rO7d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review F9HD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. Please see below for our responses. \n\n**1. About the definition of the attacks and defenses.**\n\nThank the reviewer for the comment. As stated in the introduction, the existing attacks against DRL policy train a victim policy $\\pi$ for an agent in a given environment. The agent performs normally in the clean environment, i.e., will finish its corresponding task with a high success rate and achieve a high reward. The victim policy will take suboptimal actions when the trigger shows in the environment state. This will lower the victim agent's reward. \n\nSpecifically, perturbation-based attacks add a small patch as the trigger to the environment state and the suboptimal actions of the victim policy can be a random action (Non-targeted attacks) or a specific action (targeted attacks). As stated in the evaluation section, we follow the original attack papers and add the trigger patch to each state with a certain probability. \n\nAdversarial-agent attacks target two-player competitive games with an adversarial agent and a victim agent. The trigger is a sequence of continuous actions of the adversarial agent. Once the adversarial agent takes the trigger actions, the victim agent will perform suboptimally. As stated in the evaluation, we follow the original attack paper and let the adversarial agents take the trigger action at the start of a game. \nWe are sorry for the confusion. We will summarize these attack descriptions in one paragraph in the next version.\n\nRegarding our defense, we are given a shielding agent\u2019s policy $\\pi$ and identify a trigger $\\tau$ at the trigger restoration step (Section 3.2). Then, we use the restored trigger to retrain the policy and obtain a shielded policy $\\hat{\\pi}$. \n\n**2. About Adversarial agent attacks.**\n\nThank the reviewer for the comment. We follow the setups and arguments in the backdoorl paper. For MuJoCo environments, the action space of both agents is continuous rather than discrete. Second, in the state representation of one agent, there are certain dimensions corresponding to the other agent\u2019s action. As such, the actions of one agent are embedded in the state representation of the other agent. We consider MuJoCo environments because this is the only environment in which backdoorl apply their attacks.  \n\n**3. Questions about the explanation methods.**\n\nThank the reviewer for the questions. First, our proposed feature-level explanation learns a mask matrix for all pixels in the state representation, where all the individual masks are optimized together. As such, our method can handle environments with high-dimensional state representations, such as Atari games (as shown in our evaluation). \n\nRegarding the local linear constraint in the step-level explanation (EDGE), it is inherent in the AlvarezMelis & Jaakkola (2018) paper. It helps make the explanation method perform locally linear around the given samples, which improves the explainability of the approximation model. It is equivalent to conducting a piece-wise linear approximation of a complicated non-linear function, where the piece-wise linear function is explainable. We will clarify this in the next version. \n\n**4. About the assumptions of SHINE.**\n\nThank the reviewer for the comment. As stated in Section 3.1 threat model, SHINE does require the attack launch the attack. We do not consider the setup where the defense is only given a clean environment without the attack being triggered. We also follow the existing attacks and consider the case where the attacker adds one backdoor with one trigger in the victim policy. This is an implicit assumption inherited from the attack setups. Similarly, given our mask strategy, we do not consider the cases where the attacker uses zero pixels as the trigger for perturbation-based attacks (which are also not used in the existing attacks). We will further clarify these assumptions in the next version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150525639,
                "cdate": 1700150525639,
                "tmdate": 1700150525639,
                "mdate": 1700150525639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Lih8VZy8w",
                "forum": "AKAlVyunxA",
                "replyto": "qlpaiNmlHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the rebuttal and am not very satisfied by the response. Specifically, the paper still lacks a formal attack and defense setup and it is not clear under what assumptions on attack will their method work. So, I would keep my score unchanged."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582069691,
                "cdate": 1700582069691,
                "tmdate": 1700582069691,
                "mdate": 1700582069691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DV47Gr72qK",
                "forum": "AKAlVyunxA",
                "replyto": "LlD1FYhWu2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_F9HD"
                ],
                "content": {
                    "comment": {
                        "value": "I meant a formal mathematical definition of attack and defense using the MDP framework of RL. The notion of value and how attack and defense affects this value that the agent cares about is also not clearly defined. Please take a look at the setup of [1] for a formal definition of the attack policy $\\nu$ and value the agent gets under attack.\n\n[1]. Robust Deep Reinforcement Learning against\nAdversarial Perturbations on State Observations https://arxiv.org/pdf/2003.08938.pdf"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715081489,
                "cdate": 1700715081489,
                "tmdate": 1700715081489,
                "mdate": 1700715081489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Y5G7bEHah",
            "forum": "AKAlVyunxA",
            "replyto": "AKAlVyunxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_5eAS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_5eAS"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the vulnerability of deep reinforcement learning (DRL) policies to backdoor attacks. It introduces SHINE, a method that combines policy explanation techniques and policy retraining to mitigate these attacks effectively. The proposed method is theoretically proven to enhance the performance of backdoored agents in poisoned environments while maintaining performance in clean environments. Experiments demonstrate its superiority over existing defenses in countering DRL backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies the problem of backdoor threats in reinforcement learning. The authors leveraged several techniques to identify the backdoor triggers and designs a policy retraining algorithm to eliminate the impact of the triggers on backdoored agents.\n- Theoretical guarantees in terms of the performance are provided.\n- Empirical evaluations are promising."
                },
                "weaknesses": {
                    "value": "- It would be helpful to test the proposed methods against a broader set of backdoor attacks.\n- Could you also compare your method with other types of defenses, e.g. non-trigger-inversion?"
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8582/Reviewer_5eAS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790232972,
            "cdate": 1698790232972,
            "tmdate": 1700687009673,
            "mdate": 1700687009673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a0eYMDyGoO",
                "forum": "AKAlVyunxA",
                "replyto": "7Y5G7bEHah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review 5eAS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. Please see below for our responses. \n\n**1. Compare SHINE against other types of defenses.**\n\nThank the reviewer for the comment. We follow the reviewer\u2019s comment and add one more baseline defense: STRIP [1]. This defense assumes the inputs containing the trigger, and it tries to pinpoint the trigger. It does not conduct trigger inversion (like FeatureRE or NE). We use this method to detect the trigger for perturbation-based attacks (given that it can only be applied to games with discrete action spaces). After detecting the trigger, we apply our retraining method to remove the backdoor.  We selected the pong environment for attacks against single-agent RL and the QMIX setup for attacks against multi-agent RL. The results in the following table show that SHINE is better than this method. In addition, SHINE can apply to adversarial agent attacks, while STRIP cannot. \n\n|           | Method | Pong  | QMIX  |\n|-----------|--------|-------|-------|\n| Fidelity  | SHINE  | 0.998 | 0.936 |\n| Fidelity  | STRIP  | 0.853 | 0.811 |\n| Operating | SHINE  | 0.728 | 99.1  |\n| Operating | STRIP  | 0.624 | 85.3  |\n| Clean     | SHINE  | 0.734 | 99.2  |\n| Clean     | STRIP  | 0.612 | 87.5  |\n\n[1] STRIP: A Defence Against Trojan Attacks on Deep Neural Networks\n\n\n**2. Evaluate SHINE against broader attacks.**\n\nThank the reviewer for the comment. In our evaluation, we have evaluated our method against all three existing mainstreaming backdoor attacks in RL. These attacks cover both single-agent and multi-agent RL. They also consider different types of triggers: state perturbations and adversarial agents. We also consider different attack variations and adaptive attacks. With this large set of attacks, we humbly believe that our evaluation has already covered most attacks against RL and is able to demonstrate the effectiveness of our defense. If the reviewer believes we need to evaluate SHINE against other specific attacks, we are happy to add those additional evaluations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150042941,
                "cdate": 1700150042941,
                "tmdate": 1700150042941,
                "mdate": 1700150042941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rkjsic1bWx",
                "forum": "AKAlVyunxA",
                "replyto": "a0eYMDyGoO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_5eAS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Reviewer_5eAS"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the additional experiments. The results look promising and hence I will increase my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686981969,
                "cdate": 1700686981969,
                "tmdate": 1700686981969,
                "mdate": 1700686981969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CJ658ywSe8",
            "forum": "AKAlVyunxA",
            "replyto": "AKAlVyunxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_r1tp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8582/Reviewer_r1tp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an algorithm dubbed 'SHINE', which is a testing-phase shielding method against backdoor attacks in the context of Deep Reinforcement Learning (DRL). SHINE first captures the critical states related to the backdoor trigger based on DRL explanation methods, then identifies a subset of features using the proposed feature-level interpretor. The agent policy is re-trained after trigger identification. Empirical evaluation verifies the efficacy of SHINE in shielding backdoored agents against different backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\\+ This paper tackles a challenging and important topic of shielding DRL agents.\n\n\\+ Wide applicability: the proposed approach can defense against both perturbation-based attacks and the adversarial agent attacks, for either single-agent or multi-agent RL.  \n\n\\+ The idea of pinpointing crucial states -> identifying triggered features -> retraining policy is well motivated and technically sound.\n\n\\+ Experimental design is comprehensive regarding trigger identification, shielding effectiveness, and performance impacts on clean environments. Sensitivity analysis is well designed."
                },
                "weaknesses": {
                    "value": "\\- This method hinges on the access to corrupted trajectories and environments for identifying the trigger. Sensitivity analysis is necessary to investigate whether the number or distribution of of those trajectories affect the efficacy of the proposed method.\n\n\\- Related work: The related work section could use some more efforts in elaborating how adversarial based attack works, as well as more prior work on the DRL explanation.  \n\n\\- Need more elaboration on how step and feature-level explanation would be applied if attack is adversarial based, and how multi-agent RL can benefit from this approach.\n\n\\- Need more explanation on the two constraints in Eq (4), which is the key of the re-training process. In general the writing of the Sec 3.3 is too abstract to derive to the final objective."
                },
                "questions": {
                    "value": "-  Does the number of pre-trained trajectories matter to the shielding capability?\n- How this method applies to Multi-agent DRL scenarios? \n- It is not clear to me why $M_\\pi(\\hat{\\pi}) \\geq M_\\pi(\\pi)=\\hat{\\eta}(\\pi)$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699040820465,
            "cdate": 1699040820465,
            "tmdate": 1699637073701,
            "mdate": 1699637073701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tJc4PhT0qZ",
                "forum": "AKAlVyunxA",
                "replyto": "CJ658ywSe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review r1tp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. Please see below for our responses. \n\n**1. Sensitivity analysis regarding the impact of the number of trajectories used for explanation.**\n\nThank the reviewer for the comment. Following the reviewer\u2019s comment, we conducted an additional experiment to evaluate the impact of the number of trajectories on our method. We used the Pong game for this experiment. We vary the number of trajectories as 200/500/1000 and report the trigger fidelity and final retraining performance. The results in Table 1 show that our method is not that sensitive to this factor. It also shows that SHINE does not require extensive failed cases to apply shielding. In addition, as stated in Section 5-Postmortem defense, we discuss that our goal is to quickly react to the backdoor attack once it is triggered. \n| # of trajectories             | Trigger fidelity | Operating | Clean |\n|-------------------------------|------------------|-----------|-------|\n| Failure: 100 and Succeed: 900 | 0.998            | 0.728     | 0.734 |\n| Failure: 50 and Succeed: 450  | 0.991            | 0.721     | 0.729 |\n| Failure: 20 and Succeed: 180  | 0.978            | 0.698     | 0.709 |\n\n**2. Questions about the related work.**  \n\nThank the reviewer for the comment. The review first asked for more description of \u201c how adversarial based attack works.\u201d We would like to kindly confirm with the reviewer if the reviewer referred to the \u201cadversarial-agent attack\u201d or other attacks. The adversarial agent attack is designed for two-agent competitive games, where one agent is the adversarial agent that takes the trigger action, and the other is the victim agent that contains the backdoor. At a high level, this attack designs the trigger as a sequence of continuous actions of the adversarial agent. Then, it constructs two policies for the victim agent. One policy is a normal policy, and the other is a policy that reacts to the trigger actions and fails the task. Finally, it fuses these two policies into one policy network via behavior cloning and uses it as the victim agent\u2019s policy. We will add this to the next version (Please let us know if the reviewer referred to some other attacks by saying \u201cadversarial based attack\u201d). The reviewer also suggests providing more literature about the DRL explanation. We had some related descriptions in the Section 3. We will follow the reviewer\u2019s suggestion and provide more literature review in Section 2. \n\n**3. Questions about the generalizability.**  \n\nThank the reviewer for the comment. Again, we would kindly ask for the reviewer to explain a little bit more concretely what the \u201cattack is adversarial based\u201d refers to. If it refers to adversarial agent-based attacks, we evaluated such attacks in the evaluation. Since we target trojan attacks, which are launched by the attacker, they are naturally adversarial attacks. In our evaluation, we also evaluate our defense against the state-of-the-art backdoor attacks in multi-agent RL and demonstrate that SHINE is still effective against this attack. In Appendix A.4, we explained how we extend SHINE to this attack. At the high level, we shield each backdoored agent in turn. We just need to change the input to explanation methods (local or global observation) based on the agents\u2019 training methods. \n\n**4. Questions about the constraints in Eqn (4).**  \n\nThank the reviewer for the comment. The first constraint ($\\hat{K}$) constrains the agent\u2019s performance change in the poisoned (operating) environment. This is the constraint used on the PPO algorithm when updating the policy. Using it together with the optimization objective function in Eqn. (4) enforces the agent to achieve a better performance in the poisoned environment (removing the backdoor). The second constraint ($K$) comes from Theorem 2. It constrains the agent\u2019s performance change in the clean environment. This constraint helps maintain the agent\u2019s performance in the clean environment (maintaining the agent\u2019s utility, i.e., clean performance). $M_{\\pi}(\\hat{\\pi}) \\leq M_{\\pi}(\\pi)$ because we maximize $M_{\\pi}(\\hat{\\pi})$, so the $M_{\\pi}$ of the updated policy $\\hat{\\pi}$ is at least the same as the $M_{\\pi}$ of the previous policy $M_{\\pi}(\\pi)$. $M_{\\pi}(\\pi) = \\hat{\\eta}(\\pi)$ because the KL term equals zero, and the advantage function is also zero when $\\hat{\\pi} = \\pi$. We will follow the reviewer\u2019s comment and make this part more clear in the next version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149621239,
                "cdate": 1700149621239,
                "tmdate": 1700149664090,
                "mdate": 1700149664090,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]