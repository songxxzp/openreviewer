[
    {
        "title": "State Representation Learning Using an Unbalanced Atlas"
    },
    {
        "review": {
            "id": "H0a99LJDab",
            "forum": "cWdAYDLmPa",
            "replyto": "cWdAYDLmPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_Ww1x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_Ww1x"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose to use multiple heads at the end of an encoder for\ncontrastive learning, instead of one. These heads are considered to\nmodel different charts in an atlas, mapping the data manifold to the\nembedding space. For a given sample, a score is computed to determine,\nprobabilistically, which chart should be used to encode\nit. A theoretical discussion is presented to support outputting a\nweighted average of charts based on scoring function, which relies on\nMinkowski sum of open sets to which charts map. The scoring function\nis forced to be different than a uniform distribution, thus the\nunbalanced nature of the mapping. Experiments are conducted to\nunderstand whether using multiple heads and weighted averaging at the\noutput leads to better representations than using a single head with\nthe same number of latent dimensions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. More expressive models for contrastive learning are very relevant\n   and interesting for the community. Here authors point out that when\n   the embedding dimension is very high, naive contrastive learning\n   may not use the embedding space very well. Instead of using a\n   single projection head, authors proposal to use multiple projection\n   heads seem to lead to better results according to Tables 1\n   and 2. This is a simple yet - seems to be - an effective\n   modification.  \n2. Results of the ablation study shown in Figure 2 are very\n   convincing. This simple approaches surely uses the dimensions much\n   more efficiently, and provides the expected gains in accuracy.\n3. The difference between +MMD and the proposed version, which I\n   assume is -MMD, is striking.\n4. This reviewer appreciates the experiments with CIFAR."
                },
                "weaknesses": {
                    "value": "1. Technical contribution is not at a very high level, but the\n   contribution is focused and pertinent.\n2. In the ablation study, the model \"-UA\" is not clearly specified. If\n   authors do not use the modifications of 7, that means the -MMD loss\n   is also void. What does that yield? Are authors using a single\n   projection head in this case?\n3. CIFAR experiments show that the gains are much lower in these\n   experiments compared to the ones obtained in ATARI games. A\n   discussion towards this end is not provided but it would be very\n   valuable for the readers.\n4. Notation in the presentation of the method seems a bit\n   inconsistent. I recommend authors to improve the consistency in the\n   notation."
                },
                "questions": {
                    "value": "1. can authors discuss further why the CIFAR experiments do not show a\n   similar improvement?\n2. can authors please improve the notation consistency in the\n   presentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Reviewer_Ww1x"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761247902,
            "cdate": 1698761247902,
            "tmdate": 1700659415932,
            "mdate": 1700659415932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XGlGcsW2FS",
                "forum": "cWdAYDLmPa",
                "replyto": "H0a99LJDab",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you very much for your review, which encouraged us to further improve the paper.\n\nWeakness:\n\n2. \"-UA\" uses the original prediction targets (as in Eq. 1), and the dilated prediction targets (the first half of Eq. 7) are not used. It uses multiple heads and $L_Q$ is still present. We have added more descriptions in the text.\n\n3. We very much agree that this was a primary concern and we have run more experiments to check the reasons behind. We have added discussion on this and supplemented a small-scale experiment in Table 6 at the end of Appendix B. In general, we have used the same set of hyper-parameters from MSimCLR, but that may not be optimal. We noticed that a small $\\tau$ or a linear increasing scheme of $\\tau$ increases the performance of SimCLR-UA significantly.\n\n4. We checked the paper carefully and spotted that the primary inconsistency in notations were caused by using $\\theta$ to denote the NN encoder when formalizing DIM-UA. We thought about the pros and cons and decided to remove the use of $\\theta$. We now use $f$ to denote the encoder and made changes based on that. Hopefully, this can increase the readability.\n\n    We have also added more descriptions to notations far apart from where they were first introduced.\n\nQuestions:\n\n1. Answered in Weakness 3.\n\n2. Answered in Weakness 4."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311687487,
                "cdate": 1700311687487,
                "tmdate": 1700313849105,
                "mdate": 1700313849105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "meZ68x6Hmp",
                "forum": "cWdAYDLmPa",
                "replyto": "XGlGcsW2FS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Reviewer_Ww1x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Reviewer_Ww1x"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I think with the proposed updates improve the quality of the article."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659460214,
                "cdate": 1700659460214,
                "tmdate": 1700659460214,
                "mdate": 1700659460214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bd2tvXazp2",
            "forum": "cWdAYDLmPa",
            "replyto": "cWdAYDLmPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_ES5Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_ES5Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of learning a low-dimensional manifold from high-dimensional data in the context of state representation learning. A new approach based on self-supervised learning is proposed in order to learn an unbalanced atlas representation. The proposed approach is called DeepInfomax with an unbalanced atlas (DIM-UA) and is evaluated on 19 Atari games of the AtariARI benchmark. The evaluation shows good performance in this benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses an important topic in the area of state representation learning. The proposed algorithm seems novel and yields good performance in the tested benchmarks."
                },
                "weaknesses": {
                    "value": "I am struggling to understand the rationale behind the DIM-UA algorithm. What is the motivation for redefining the score function L_GL? Equation 9 has a hyperparameter for L_Q but not for L_GL or L_LL. Why? The writing of the paper is not clear in several points as some parts are difficult to follow. For example, Figure 1 is not quite clear to me and the caption is not very explicit. Additionally, the results could be presented in a more concise fashion (esp. Table 1 and 2 - showing the best results in bold would increase the readability)."
                },
                "questions": {
                    "value": "- What is the motivation for redefining the score function L_GL? \n\n- Equation 9 has an hyperparameter for L_Q but not for L_GL or L_LL. Why?\n\n- The improvements reported in table 2 seem relatively small. Can you comment on this further?\n\n- Why is the linear evaluation accuracy on CIFAR10 a suitable evaluation metric? The improvements shown here (table 3) are very marginal at best."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841545872,
            "cdate": 1698841545872,
            "tmdate": 1699636181004,
            "mdate": 1699636181004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5GEZNe5v2U",
                "forum": "cWdAYDLmPa",
                "replyto": "bd2tvXazp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you very much for your kind review! We were sorry that the notations in DIM-UA were hard to follow. We put some effort on making the notations in DIM-UA more consistent. Hope it helps with the readability.\n\nWeakness:\n1. We redefined $L_{GL}$ because the UA paradigm uses dilated prediction targets in pretraining, instead of using the normal prediction targets. \n\n2. We have a hyper-parameter $\\tau$ for $L_Q$ because this hyper-parameter controls the MMD loss on membership probabilities. For $L_{GL}$ or $L_{LL}$, they are directly controlled by the learning rate of the optimizer. We have showed that this hyper-parameter $\\tau$ is indeed important. Please check Table 6 at the end of Appendix B to see the effects of changing $\\tau$.\n\n3. We have added more content to the caption of old Figure 1 (now Figure 2). We have made the highest score in each row bold, and also moved Table 2 to Appendix A (now Table 4). Those are really nice suggestions.\n\nQuestions:\n1. Answered in Weakness 1.\n\n2. Answered in Weakness 2.\n\n3. The old Table 2 now is labeled as Table 4, which has the accuracy scores. Our DIM-UA method has a mean accuracy of 76%, compared to 73% and 71% of ST-DIM using 256 units and using the same amount of units. First, VAE underperforms too much compared to ST-DIM, which may make a 3% increase seem not that big, but 3% increase in accuracy is quite decent since the difference between DIM-UA and ST-DIM is not even near to the difference between generative methods and contrastive methods.\n\n    Moreover, when comparing ST-DIM to DIM-UA with the same large number of units, DIM-UA wins by 5%, which suggests that a large number of hidden units in ST-DIM does not help with the performance. This is strong empirical evidence that our UA paradigm can create more effective representations.\n\n4. This is a great point. We also thought that the performance of SimCLR-UA in Table 3 (now Table 2) needed more discussion. Our emphasis in the past few days has put on finding the key reason behind this. We have supplemented a small-scale experiment in Table 6 at the end of Appendix B. Mainly, we argued that the main reason why SimCLR-UA did not outperform SimCLR by a large margin is that it used the same set of hyper-parameters as in MSimCLR, which were not optimal. This, in turn, also suggests that SimCLR-UA beats MSimCLR by a large margin with no doubt."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311461633,
                "cdate": 1700311461633,
                "tmdate": 1700318629229,
                "mdate": 1700318629229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6FAj3Iv5tn",
            "forum": "cWdAYDLmPa",
            "replyto": "cWdAYDLmPa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_hci6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2451/Reviewer_hci6"
            ],
            "content": {
                "summary": {
                    "value": "This paper developed a state representation learning method leveraging an unbalanced atlas (UA). The authors have modified the ST-DIM algorithm to align with the proposed UA paradigm. Although the main contribution is not stated intuitively, empirical evaluations on 19 games of the AtariARI benchmark suggested an improved performance compared with three established baseline methods (many existing self-supervised methods are omitted for comparison). Furthermore, the authors performed a comprehensive ablation study for the design choices of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The experiments are conducted across 19 games of the AtariARI benchmark, covering a variety of vision tasks.\n\n+ There are comprehensive ablation studies for the technical components of the proposed method."
                },
                "weaknesses": {
                    "value": "- The clarity of the introduction could be enhanced by providing a more explicit context for the specialized terminology introduced (see Q1-3).\n\n- The comparison would benefit from the inclusion of key baseline models which are currently absent (see Q4).\n\n- Tables 1 and 2 appear to be redundant, presenting analogous results through different evaluative metrics (F1 score and Accuracy, respectively). Although a comprehensive evaluation is encouraged, putting these two sizable tables back to back in the main paper gives the impression of lacking sufficient materials for the paper. It would be more appropriate to consolidate these findings, perhaps through a combined analysis or in supplementary materials, to avoid repetition and maintain the conciseness of the paper.\n\n- the paper lacks a clear statement of its underlying motivation and significance, which is pivotal for readers to comprehend the value and potential impact of the research (see Q5)."
                },
                "questions": {
                    "value": "1. The introduction used specialized terminology that may not be universally familiar, necessitating additional clarification for a broader audience. Specifically, the first sentence of the third paragraph introduces concepts such as *manifold*, *atlas*, *local structure*, and *chart*, which would benefit from further exposition to contextualize the study and its objectives.\n\n2. The paper's motivation remains unclear, partly owing to the use of undefined terms. The concept of an *atlas*, and particularly the distinction between *unbalanced* and *balanced* atlases within this framework, needs clarification. The terms *prior distribution* and *membership probability distribution* introduced later also lack clear definitions, impeding the reader's understanding.\n\n3. In the 3rd paragraph of Section 4, *d* and *n* are used without proper definition.\n\n4. This paper suggests that pre-training a model using a reinforcement learning task and then fine-tuning it on downstream reinforcement learning tasks is beneficial. However, this point is not fully demonstrated because the authors did not compare the proposed method with other self-supervised learning methods, as reviewed in the introduction, e.g., contrastive models (SimCLR is compared) and generative models (none is compared).\n\n5. From the introduction section, it is not intuitive to me why this study is important. For example, the last paragraph lists technical achievements but does not convey their broader significance. Specifically, (1) fitting the ST-DIM to UA paradigm (why UA paradigm is important?), (2) detailed ablations for better design choices (that's standard, not sure if it counts as a contribution), and (3) representing a manifold with a larger number (why this is important anyway?). Not limited to the introduction section, the authors did not describe the significance of the proposed method and all these ablation studies in the entire paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2451/Reviewer_hci6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698950881722,
            "cdate": 1698950881722,
            "tmdate": 1699636180911,
            "mdate": 1699636180911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FN6IRx0DrR",
                "forum": "cWdAYDLmPa",
                "replyto": "6FAj3Iv5tn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review! We have moved Table 2 to Appendix A (now Table 4). Other part of weakness will be addressed in answering questions.\n\nQuestions:\n\n1. We agree that the terminology should be better introduced. We now have more descriptions in the caption of Figure 2.\n\n2. The concept of an atlas is now described in Figure 2. Moreover, we described the notion of a uniform prior and unbalanced atlas in Introduction (above the contribution summary). We used the term 'balanced' in contrast to 'uniform'. While introducing these notions, we also strengthened our motivation towards our method. The membership probability can be viewed as a simple score function (by using Softmax) to attribute a probability score to each chart.\n\n3. We have added short descriptions of $n$ and $d$ in Section 4.\n\n4. We started our work with a focus on SRL, and we saw that generative methods did not perform well in SRL tasks (see the much lower F1 scores of VAE compared to CPC in Table 1). It was never our intention to omit generative methods, but we simply thought that contrastive methods were more suitable in SRL tasks. Of course, there are many other tasks where generative methods are more suitable, and it can be interesting future work.\n\n5. This is a very good point. The reasons why this work is important should be better explained. Thus, we emphasized on our motivation in Introduction and extended the contribution summary. One important note is that our work can help build large models in deep learning by enlarging the heads (complementary to the typical research that deepens the backbones)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311070767,
                "cdate": 1700311070767,
                "tmdate": 1700317725375,
                "mdate": 1700317725375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EfvspIdz1o",
                "forum": "cWdAYDLmPa",
                "replyto": "FN6IRx0DrR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Reviewer_hci6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Reviewer_hci6"
                ],
                "content": {
                    "title": {
                        "value": "An annotated version might be helpful"
                    },
                    "comment": {
                        "value": "Thank you for your response. I've briefly reviewed the revised manuscript and its accompanying response. To enhance the evaluation, I suggest marking the revised sections in the manuscript with color. This would be particularly useful as your response is concise and primarily directs the reader to the updated manuscript. Initially, I was unable to find the definitions for key terms like 'manifold', 'atlas', 'local structure', and 'chart' that I had inquired about. I will examine the manuscript more thoroughly, but my initial impression is that this paper may still need substantial revisions for a clearer presentation.\n\nRegarding the statement, *\"our work can help build large models in deep learning by enlarging the heads (complementary to the typical research that deepens the backbones),\"* I must admit that the motivation and significance of this approach are not entirely clear to me\u2014for example, why is it important to 'enlarging the heads'?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686261791,
                "cdate": 1700686261791,
                "tmdate": 1700686261791,
                "mdate": 1700686261791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWEofyUlVQ",
                "forum": "cWdAYDLmPa",
                "replyto": "6FAj3Iv5tn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your kind response"
                    },
                    "comment": {
                        "value": "Dear reviewer hci6,\n\nThank you very much for your response! Indeed, an annotated version would be very helpful. Please check our updated pdf. The terms are mainly introduced in the caption of Figure 2, since we thought that those terms can be better understood with a figure illustration. \n\nWe agree that the term of local structure is still not independently introduced. However, please note that this term is not anything special in this context, it is more like a broad concept in maths. For instance, the concept of locality here is not much different compared to when we say that a function is locally smooth, or locally differentiable. It can be better understood to consider that the domain for a chart is local to the whole manifold for an atlas.\n\nFor the motivation of enlarging heads, it is because we face bottlenecks when enlarging the backbone. For instance, over-parameterization is common for vision transformers (ViTs) and over-parameterized ViTs might perform worse on ImageNet 1K. When facing such bottlenecks, an alternative to build larger deep learning models would naturally be increasing the number of hidden units in output heads."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688750416,
                "cdate": 1700688750416,
                "tmdate": 1700726147050,
                "mdate": 1700726147050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]