[
    {
        "title": "Leveraging Uncertainty Estimates To Improve Classifier Performance"
    },
    {
        "review": {
            "id": "Hfx4kCJ24R",
            "forum": "nsNyDvNQTc",
            "replyto": "nsNyDvNQTc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
            ],
            "content": {
                "summary": {
                    "value": "This paper tries to improve classification performance by leveraging uncertainty estimates. To this end, the authors focus on posterior networks, a novel class of classification models that has been recently introduced as an alternative for Bayesian neural networks in OOD detection. Posterior networks model a second-order distribution that is parameterized by a neural network using specific loss functions. In this paper, the authors focus on binary classification, and the considered second-order distribution is a Beta distribution. \n\nIn addition, the authors assume that the data generation process also has a Beta prior over the Bernoulli parameter of the first-order distribution. Within this framework, the authors analyze model estimation bias and they show that it depends both on the aleatoric and epistemic uncertainty. Furthermore, they formulate decision boundary selection in terms of both types of uncertainty, and they prove that this problem is NP hard."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Uncertainty quantification is a hot topic in machine learning. The submission also contains material that is novel."
                },
                "weaknesses": {
                    "value": "This paper is not well written. I have the impression that the authors lack a thorough understanding of the literature. They use unconventional notions, and they don't discuss the problem in a formal way. Let me make this point clear. \n\nIntroduction:\n- the authors write \"benefits of combining model score with estimates of aleatoric and epistemic uncertainty\". This is a very weird reasoning, because the model score corresponds to the estimate of aleatoric uncertainty. (btw, better to use conditional class probabilities instead of model score, because model score is a very generic term)\n- RQ1: Does model score estimation bias (deviation from positivity) depend on uncertainty? This is a really weird sentence that is difficult to understand. I only understood that sentence after reading Section 3. You first have to explain what you mean with model score estimation bias. Is it a bias in the sense of the traditional bias-variance decomposition or is it something else? The same with \"positivity\". That's a term that is never used in the literature. What you mean is the probability of the positive class on population level. After reading Section 3 it became clear to me what you mean with \"uncertainty\". It is the differential entropy of the second-order distribution. I would argue that this is a way to measure epistemic uncertainty with posterior networks, but you have to be more precise in your description. \n- Same comment for \"test positivity rate\". Never heard of this term before. Please define it. \n\nRelated work: \n- The overview of related work is described in a very vague way. What you call \"Uncertainty modelling\" are methods that intend to quantify aleatoric and  epistemic uncertainty, such as Bayesian methods, ensemble methods and evidential deep learning methods. \n- Please also describe methods that only quantify aleatoric uncertainty, such as traditional probabilistic models. In that context, I would argue that \"calibration metrics\" are a way to assess whether standard probabilistic methods quantify aleatoric uncertainty correctly.  \n  \nSection 3:\n- Posterior networks: please motivate why this group of methods is your first choice to improve classification performance by leveraging epistemic uncertainty scores. Recent papers have critisized this group of methods for not quantifying epistemic uncertainty in a correct manner, see e.g. Bengs et al. Pitfalls of epistemic uncertainty quantification through loss minimization, Neurips 2022. So, I would argue that it is weird to consider this group of methods as first choice.  \n- 3.1: the uncertainty score u(x) is a measure of epistemic uncertainty. Please be more precise. \n- 3.2: notions such as \"true positivity\" and \"empirical positivity in the train and test set\" are very weird terms. I assume that you refer to true and estimated conditional class probabilities for the positive class. Please be more precise. \n- 3.2: Why is the true positivity sampled from a Beta distribution? Is this to generate synthetic data, or what is the purpose of this? Since you are presenting a theoretical result in Theorem 3.1, I assume that you have a different purpose of considering a Beta distribution. \n-Theorem 3.1: \"When data is generated according to Figure 2\". Unclear what this means. The figure is not detailed enough. The data generation process has to be described in a formal way. \n\nBecause of the very imprecise write-up, it is difficult to follow what the novelty of this paper is. I have to admit that I was completely lost after Theorem 3.1, even though I am very familiar with posterior networks."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5267/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5267/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697708987115,
            "cdate": 1697708987115,
            "tmdate": 1699636526274,
            "mdate": 1699636526274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PNbikc3Gud",
                "forum": "nsNyDvNQTc",
                "replyto": "Hfx4kCJ24R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal To Reviewer D6i1 [1/2]"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for their valuable feedback. We agree with the reviewer that we could have done a better job at paper presentation. In particular, we should have defined all new terms the first time they are introduced than at a later point. We will do our best to incorporate all the inputs as we revise the draft. Please find our response below\n\nWe feel the reviewer has completely missed out on the main contributions of the paper, which are not necessarily tied to the choice of using Posterior networks to quantify uncertainty. To clarify, we make two theoretical contributions. Note that the primary objective of the first contribution is to motivate the second one.\u00a0  \n  \n1) First, we try to establish that that there exist common scenarios where empirically observed conditional probability $p(y=1|x)$ of the positive class in the training dataset as well as the model estimate deviates from that of the underlying distribution and the test set in a systemic way with dependence on the density of observations at x (which is correlated to \u201cuncertainty\u201d) (Theorem 3.1 - Specifically the common scenario we consider is one where the data is generated as per Fig 2 with the actual generative distribution mentioned in Sec 3.2 as well as Appendix E.1)\u00a0  \n  \n2) Second, we provide algorithms to combine the model estimated $p(y=1|x)$ and ANY other discriminating signal (such as uncertainty estimates) to arrive at a decision boundary that better optimizes recall at a precision bound.  \n  \nAs we mention in footnote 2 (Pg2), Theorem 3.1 (a) on deviation between $p(y=1|x)$ in the training data and that of test data or the underlying distribution is completely independent of the choice of uncertainty modeling method and is not dependent on Posterior Networks. The same is true for Sections 4 & 5.\u00a0  \n  \nHowever, Theorem 3.1(b) characterizes how Theorem 3.1(a) applies for the case where the the model score and uncertainty are obtained using Posterior Networks. The primary reason we included this result is because most of our experiments are based on Posterior Networks and we wanted to compare the empirically observed trends with the theoretically expected behavior. We will rewrite Sec 3 to clarify this aspect.\n\n\n**Responses to Detailed Comments:**  \n  \n**Comment 1 :**\u00a0This paper is not well written ...\u00a0Let me make this point clear.   \n**Response:**\u00a0\u00a0(a) We acknowledge that we used some non-standard terms such as train/test positivity and model score estimation bias. We will revise the draft to ensure these are defined precisely the very first time they are used rather than in footnotes or further down in the writeup or appendix. (b) We do present a formal definition of the decision-making problem in Sec 4. Could the reviewer kindly clarify their expectations on the problem definition?  \n\n**Comment 2:**\u00a0**Introduction:**\u00a0the authors write \"benefits ... very generic term)  \n**Response:**\u00a0\u00a0  \n(a) Yes, aleatoric uncertainty is a deterministic function of model score but that is not true of epstemic uncertainty and will rephrase the sentence to read better.\u00a0  \n(b) We are using the phrase \u201cmodel score\u201c instead of \u201dconditional probability of the positive class as estimated by model\u201c only for brevity and have clarified this in Footnote 1[Pg 1]  \n  \n**Comment 3:**\u00a0RQ1: Does model score estimation bias ...(c) Same comment for \"test positivity rate\". Never heard of this term before. Please define it.  \n**Response:**\u00a0\u00a0(a) We do mention that the \u201cmodel score estimation bias\u201c is the \u201cdifference between test positivity rate and the model score varies with uncertainty\u201d [Pg 2 Para 2] but will move this up to where RQ1 is mentioned.\u00a0(b) Apologies for not having defining \u201cpositivity rate\u201d upfront. It is just the \u201cprobability of the positive class on population level\u201d. \u201cTrain positivity\u201d and \u201cTest positivity\u201d corresponds to the training and test distribution respectively. We will make it a point to clarify this as well upfront as we revise the draft. (c) Just to clarify in case of Posterior networks and Theorem 3.1(b), \u201cuncertainty\u201d refers to the differential entropy of the second-order distribution in Posterior networks. However for Sections 4 and 5, we are using the term \u201cuncertainty\u201d in a broader sense and have also included results using other methods for estimating uncertainty (e.g. MCDropout) [Sec 6.2, Pg 8, RQ2 and Appendix C.5, Table 5].  \n  \n**Comment 4:\u00a0Related work:**\u00a0The overview ... aleatoric uncertainty correctly.  \n**Response:**\u00a0\u00a0We will expand the related work section to have a more detailed discussion of various uncertainty modeling methods including those focusing on aleatoric uncertainty as well. We focused on methods that also capture epistemic uncertainty because given the model score $p(y=1|x)$, epistemic uncertainty provides additional discriminating ability to determine the optimal decision boundary unlike aleatoric uncertainty that has a deterministic relationship with model score and no additional information."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479348392,
                "cdate": 1700479348392,
                "tmdate": 1700479348392,
                "mdate": 1700479348392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7vNp3XyW2Z",
                "forum": "nsNyDvNQTc",
                "replyto": "Hfx4kCJ24R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal To Reviewer D6i1 [2/2]"
                    },
                    "comment": {
                        "value": "**Comment 5: Section 3:**\u00a0\u00a0Posterior networks: ... consider this group of methods as first choice.  \n**Response:**\u00a0\u00a0First, we wish to clarify that our work does not qualify any uncertainty quantifying/estimating method (eg: Posterior / MC-dropout) as being superior over the other. We are aware of the work by Bengs et al. which discusses why optimizing the model based on uncertain-cross entropy loss function does not provide faithful estimates of epistemic uncertainty and will include it in the revised version of related work, but the gaps in the optimization approach are not central to our work.  \n  \nOur choice of Posterior Networks for our experiments was based on two considerations.\u00a0First, we already had an existing implementation for one of our real-world applications and experiments using 2D-decision boundary algorithms yielded promising results.\u00a0Second, for certain controlled scenarios (Fig 2), we could derive a closed analytic form for the model score estimation bias in terms of the model score, ratios of priors and observed evidence ($\\gamma(x)$ ), which is positively correlated with the notion of \u201cuncertainty\u201d (entropy of the epistemic distribution) in Posterior networks, allowing us to compare empirical observations with expected behavior.\u00a0  \n  \nNote that it is quite possible that other methods of quantifying epistemic uncertainty yield superior results in terms of optimizing decision boundaries and that remains an area of further exploration. We do provide results on MC-Dropout method as well [Sec 6.2, Pg 8, RQ2 and Appendix C.5, Table 5] and observe similar benefits in combining  uncertainty estimates with conditional class probabilities to oprimize recall at a precision bound. We will attempt to include results from other recent approaches in a revised version of the Appendix.  \n  \n  \n  \n**Comment 6: Section 3.1**\u00a0the uncertainty score $u(x)$ is a measure of epistemic uncertainty. Please be more precise.  \n**Response:**\u00a0\u00a0The uncertainty score $u(x)$\u00a0needs to definitely capture epistemic uncertainty since that is the element that provides additional information beyond the model estimated $p(Y=1|x)$, but depending on the modeling method, it could also have a dependence on the aleatoric uncertainty. We will clarify this in the revision.  \n  \n**Comment 7: Section 3.2**\u00a0\u00a0notions such as \"true positivity\" ... Please be more precise.  \n**Response:**\u00a0\u00a0We are using these terms only for brevity and will make it a point to define these first.\u00a0  \n  \n**Comment 8: Section 3.2:**\u00a0(a) Why is the true positivity ... The data generation process has to be described in a formal way.  \n**Response:**\u00a0(a) We should have explained our choice of data generation setting better. Before going to Sec 4-5, we wanted to first establish that there is a systemic deviation between the conditional probability $p(y=1|x)$ from the model and the observed distribution on the test set and that the deviation has a dependence on \u201cuncertainty\u201d. Since the general case was not as tractable, we restricted our analysis to a common representative scenario where we have Beta prior and differential sampling across classes for train and test set. Being able to verify the theoretical analysis with synthetic data was a secondary motivation. (b) We will update Figure 2 to include the exact distributions mentioned in Sec 3.2 and also Appendix E.1. We, in fact, had a more detailed figure but chose the simpler one for lack of space.\u00a0  \n  \n  \n**Comment 9:**\u00a0\u00a0Because of the very imprecise ... very familiar with posterior networks.  \n**Response:**\u00a0Thanks again for your valuable feedback and taking the time to read through our submission. We hope we have answered some of your questions. Based on the clarifications, if possible, please do take a look at Section 4-6 to share any further feedback or revise your score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479409628,
                "cdate": 1700479409628,
                "tmdate": 1700479409628,
                "mdate": 1700479409628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RZa0tMxclj",
            "forum": "nsNyDvNQTc",
            "replyto": "nsNyDvNQTc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
            ],
            "content": {
                "summary": {
                    "value": "The submission addresses making decisions under uncertainty. More precisely, it considers the binary classification setting, where an instance must be classified into one of two classes, and where the classifier provides a classification score with an associated level (score) of uncertainty. \n\nAfter a succinct reminder of previous works related to uncertainty modelling and decision-making under uncertainty, the paper first adopts a Bayesian view, and shows that the test positivity rate depends on both the positivity (classification) and uncertainty scores. Then, it proposes to transform the classification problem into picking a class based on both scores, by partitioning the corresponding 2D space (model x uncertainty scores) into bins, thus leading to a 2D decision boundary (instead of a 1D decision threshold for the model score) which maximizes recall for a desired precision level. The paper mentions several strategies for constructing the bins, and then proceeds with an empirical evaluation of the proposed strategy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses an interesting and important problem\u2014making decisions under uncertainty. It covers the topic in a rather complete way, by first highlighting the relationship between uncertainty and estimation bias, and then proposing a strategy for improving decision-making by taking the uncertainty level into account. \n\nThe proposal is overall sound, with some theoretical grounding as well as an experimental study so as to support the claims."
                },
                "weaknesses": {
                    "value": "My main criticism is that the paper ignores a large part of the literature on decision-making under uncertainty. There already exist many works dedicated to this topic, with the purpose of taking uncertainty into account to improve the decision-making process or abstaining to make decisions when uncertainty is too high. Some of these works are rooted in formalisms alternative to probabilities, while others make use of the classical probabilistic framework. It is obviously not possible to mention all of these works here\u2014see e.g. cautious, imprecise or uncertain classification; I'll only mention several of them here: \n\n[1] Mathias C. M. Troffaes. Decision making under uncertainty using imprecise probabilities. International Journal of Approximate Reasoning, Volume 45, Issue 1, May 2007, Pages 17-29. \n\n[2] Thierry Denoeux. Decision-making with belief functions: A review. International Journal of Approximate Reasoning, Volume 109, June 2019, Pages 87-110. \n\n[3] Thomas Mortier, Marek Wydmuch, Krzysztof Dembczy\u0144ski, Eyke H\u00fcllermeier, Willem Waegeman. Efficient set-valued prediction in multi-class classification. Data Mining and Knowledge Discovery, Volume 35, Issue 4, Jul 2021, Pages 1435-1469. \n\nBesides, it is difficult to see to which extent the analyzes reported in the proposal can be generalized outside of the framework considered here, either regarding the analysis of estimation bias (Section 3.2) or the proposed decision-making strategy (Sections 4 and 5). As stated by the authors, the Bayesian framework considered here may likely not hold for all datasets. The analysis but also the proposed decision-making strategy seem hard to generalize to more than two classes (in the latter case, for computational reasons). It would have been nice to discuss these limitations in the paper. \n\nThe writing should be improved: there are typos; the general structure of the paper might be improved (e.g., Sections 4 \"2-D Decision Boundary Problem\" and 5 \"2-D Decision boundary algorithms\" could be agregated); some parts may be developed for the sake of clarity (e.g., the \"Dynamic Programming (DP) Algorithm\" part in Section 5.2, which is quite verbose but lacks formalization). I do not understand why the numbering of subsections changes from a bold, small-letter font in Section 3 (see e.g. Sections 3.1 and 3.2 page 3) to a capital font in Section 5 (e.g. Sections 5.1, 5.2 and 5.3 pages 6-7). Figures are hard to read (very small); Equation 2 is displayed in an awkward manner; Algorithm 1 is a bit packed and consequently difficult to read. \n\nTypos (non-exhaustive list): \n- \"relative efficacy\" (page 2), \n- \"Under what settings\" (page 2), \n- \"via Bernoulli distribution\" (page 3), \n- \"In the case of train set\" (page 3), \n- \"corresponds to oversampled negative class\" (page 3), \n- \"expected true and test positivity rate\" (page 4), \n- \"$b(u)$ is score threshold\" (page 5), \n- \"upto\" (page 6), \n- \"feasible solution exists\" (page 6), \n- \"bayesian approximation\" (page 10, references), \netc."
                },
                "questions": {
                    "value": "Why choosing the differential entropy of distribution $H(q(\\mathbf{x}))$ as a measure of uncertainty ? On one hand, this intuitively makes sense, but it also has been criticized by some authors (see e.g. the works of H\u00fcllermeier). \n\nThe interpretation of $\\gamma(\\mathbf{x})$ in Section 3.2 is not really an interpretation. Since the fact that $u(\\mathbf{x})$ is correlated with $\\gamma(\\mathbf{x})$ is central in the score estimation bias depending on score and uncertainty, this part may be further developed and discussed. \n\nCan you provide insights on generalizing the analysis of estimation bias to other settings (as compared to Fig. 2) ? \n\nThe decision-making approach seems debatable. In Section 4: is it reasonable to leave b unconstrained ? Shouldn't a monotonicity assumption be imposed ? Is it reasonable to always keep the decision-making process precise ? Wouldn't it be preferable to abstain in presence of a high (epistemic) uncertainty ? Aleatoric and epistemic uncertainties are here not distinguished from each other, which seems to be an issue: what if $u(\\mathbf{x})$ is maximal ? Is it reasonable to make decisions in this case ? What is the effect of discretizing the Score $\\times$ Uncertainty space ? To this extent, shouldn't the independent splitting on both dimensions avoided ? \n\nThe \"proof\" that the partitioning problem of the Score $\\times$ Uncertainty space is NP-hard (mentioned in the introduction) seems to directly proceed from the bin-packing problem behind. \n\nEqui-weight binning case, \"it is possible that a bin with lower positivity rate might be preferable to one with higher positivity rate due to different number of samples\": can you be a bit more specific ? \n\nBinning strategy: I understand that for any given score level, the uncertainty bin indices are monotonic wrt actual values; however, this does not correspond to the same level of model score. Des this have an impact on the results ? More generally, what is the impact of the binning strategy on the overall procedure ? This latter may be sensitive to the bins computed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5267/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837",
                        "ICLR.cc/2024/Conference/Submission5267/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698319606771,
            "cdate": 1698319606771,
            "tmdate": 1700644377733,
            "mdate": 1700644377733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lPd21ZQ5bn",
                "forum": "nsNyDvNQTc",
                "replyto": "RZa0tMxclj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i837 [1/4]"
                    },
                    "comment": {
                        "value": "We are sincerely grateful to the reviewer for taking the time to read through the entire draft and sharing their extremely valuable feedback. Please find our response below  \n  \n\n**Comment 1 :**\u00a0My main criticism is that the paper ignores a ... when uncertainty is too high. .... \u201c  \n  \n**Response:**\u00a0\u00a0Thanks for the pointers. We acknowledge this gap and will definitely expand the related work to include a discussion of the suggested papers . While the papers do deal with decision making under uncertainty, the problem setting and contributions are slightly different as we discuss below.\u00a0  \n  \n\n1. Decision-Making with Belief Functions (Deneoux et al. ,\u00a0Troffaes et al.\u00a0) : These works deal with decision making where an input state or policy (possibly multi-dimensional) maps to a consequence which results in a real-valued payoff function with uncertainty modeled via (possibly Bayesian) belief functions. While there are parallels to our decision boundary estimation, in these works, the decision making over competing options is based on a single risk-adjusted (or uncertainty adjusted) utility determined by various axiomatic criterion, e.g., minmax, Hurwicz without any data-based calibration. Second, most of this work is applicable for utility functions that are additive over elements of the input decision space, e.g., overall utility of a stock portfolio can be expressed as the sum of utilities over the individual stocks.. Lastly, there is no discussion around bias in estimating model scores or the effects of differential sampling.\u00a0\n2. Set-value prediction: Mortier et al. present an efficient approach for set-value predictions in a multi-class setting based on maximizing the expected utility. This work differs from ours in three key aspects. First, the focus of Mortier et al. is on efficient optimization over the power set of classes with the gains being primarily relevant for multi-class settings with a large number of classes where the set of possible prediction sets, (i.e., any non-empty element of the power set) is extremely large. For the binary classification scenario, there are only 3 prediction sets { {0}, {0,1}, {1} }. Secondly, they do not consider the question of the model estimated conditional probabilities being biased which is an important focus of our work. Lastly, while Mortier et al., discuss unconstrained optimization of multiple different utility functions, such as aggregated precision, recall and f-measure, they do not explicitly consider optimizing recall for a precision bound or vice-versa which is more common in real-world applications and the objective function that we focus on.  \n  \n**Comment 2 :**\u00a0Besides, it is difficult to see ... nice to discuss these limitations in the paper.  \n  \n**Response:**\u00a0\u00a0The decision making strategy in Sec 4-5 is independent of the Bayesian framework and is applicable for datasets following any distribution as well as any choice of uncertainty modeling method.\u00a0\u00a0  \n  \nThe purpose of Sec 3 was primarily to\u00a0motivate the 2D decision-making problem by establishing that that there exist some common scenarios where the conditional probability $p(y=1|x)$ of the positive class in the training dataset as well as the model estimate deviates from that of the underlying distribution and the test set in a systemic way with dependence on uncertainty. In Sec 4-5, we do NOT make any assumption about the nature of the dependence on the uncertainty.\u00a0  \n  \n(b) The analysis in Sec 3 [both Theorem 3.1 (a) and (b)] does in fact generalize to multi-class setting with more than two classes when we consider a global Dirichlet prior instead of a Beta prior. We can express the expectation of true and test conditional class probabilities conditioned on the train conditional class probabilities. In case of Posterior networks, we can do the same while conditioning on model\u2019s conditional class probabilities.\u00a0  \n  \nOn the other hand, generalization of the decision-making strategy to more than two classes is somewhat non-trivial since it first requires characterizing the objective function that the decision boundary(ies) need to be optimized for .\u00a0We are exploring this setting by considering monotone operators over the conditional probability distributions that also take into account other uncertainty-based discriminating signals.\u00a0Having said that, there are a large number of real-world applications that require decision making over binary classes which ensures broad applicability. We also consider differential sampling which is a common to multiple large scale applications.\n\n**Comment 3:**\u00a0\u00a0The writing should be improved: ... difficult to read.  \nTypos (non-exhaustive list): ... \u201c  \n  \n**Response:**\u00a0\u00a0Thanks so much for these suggestions. We will address all of these and do a more careful proof reading as we revise our draft."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579428958,
                "cdate": 1700579428958,
                "tmdate": 1700579428958,
                "mdate": 1700579428958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OzAj62wjn8",
                "forum": "nsNyDvNQTc",
                "replyto": "RZa0tMxclj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i837 [2/4]"
                    },
                    "comment": {
                        "value": "**Comment 4 : Q1:**\u00a0Why choosing the differential entropy ... authors (see e.g. the works of H\u00fcllermeier).  \n  \n**Response:**\u00a0\u00a0We would like to clarify that the decision-making strategy in Sec 4-5 does not assume anything about the nature of uncertainty modeling. In Theorem 3.1(b) and for our experimental section, we focus on Posterior networks and choose the differential entropy of the epistemic distribution as a measure of uncertainty. The reason for this is that it is correlated to density of observations and thus, the ratio of prior to evidence (\\lambda(x), which impacts the expected test positivity rate conditioned on the train positivity rate.\u00a0  \n  \nWe are aware of the work by Bengs, Hullermeier et al . which discusses why optimizing the model based on uncertain-cross entropy loss function does not provide faithful estimates of epistemic uncertainty but the gaps in the optimization approach are not central to our work.\u00a0\u00a0  \n  \nWe are only focused on optimizing the decision boundary and do not qualify any uncertainty quantifying/estimating method as being superior over the other in terms of estimation of \u201cepistemic uncertainty\u201d or better calibrated estimates. Do note that the lack of calibration of uncertainty estimates does not also impact the decision boundary estimation.\u00a0  \n  \nOur choice of Posterior Networks for our experiments was based on two considerations.\u00a0First, we already had an existing implementation for one of our real-world applications and experiments using 2D-decision boundary algorithms yielded promising results.\u00a0Second, for certain controlled scenarios (Fig 2), we could derive a closed analytic form for the model score estimation bias in terms of the model score, ratios of priors and observed evidence ($\\gamma(x)$), which is positively correlated with the notion of \u201cuncertainty\u201d (entropy of the epistemic distribution) in Posterior networks, allowing us to compare empirical observations with expected behavior. We also report some results on MC-Dropout methods. (Sec 6.2, Pg 8, RQ2 and Appendix C.5, Table 5).\n\n**Comment 5 : Q2**\u00a0The interpretation of $\\gamma(x)$ in Section 3.2 is not really ... part may be further developed and discussed.  \n  \n**Response:**\u00a0\u00a0Thanks for the suggestion. We mentioned the definition of $u(x) = H(q(x))$ in footnote 3 (Pg 2) but agree that further explanation would be helpful. We will expand on this item with a small plot of $u(x)$ and $\\gamma(x)$ as well as explanation of the positive correlation either in the main paper or the appendix in the revised draft.\u00a0  \n  \n**Comment 6 : Q3:**\u00a0Can you provide insights on generalizing the analysis of estimation bias to other settings (as compared to Fig. 2) ?  \n  \n**Response:**\u00a0\u00a0The generality of Figure 2 is primarily limited by the choices of (i) the global prior (Beta distribution) and (ii) estimation of model score and uncertainty using Posterior networks.\u00a0\u00a0  \n  \nNote that Thm 3.1(a) on the expectation of test/true conditional class probabilities conditioned on the train positive class probability holds true independent of assumption (ii). For any alternative method of estimating model score and uncertainty, we can derive the relationship between the model score (i.e., model estimated $p(y=1|x)$ ) and test positivity rate based on the dependency between the train positivity and the model score. The term ( $\\lambda(x)$ ), which is related to the density of observations will continue to incorporate the effect of epsitemic uncertainty.\u00a0  \n  \nFiguring out an equivalent closed form result for Thm 3.1(a) is non-trivial when assumption (i) does not hold. However, for a global unimodal prior (not necessarily a Beta distribution), the expectation of test/true conditional class probabilities conditioned on the train positive class probability is shifted away from the train distribution in the direction of the global prior since the posterior distribution of the true positivity conditioned o the train positivity combines the effects of the evidence (train positivity) and the global prior."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579518012,
                "cdate": 1700579518012,
                "tmdate": 1700579518012,
                "mdate": 1700579518012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HC8em1yPX5",
                "forum": "nsNyDvNQTc",
                "replyto": "RZa0tMxclj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i837 [3/4]"
                    },
                    "comment": {
                        "value": "**Comment 7: Q4:**\u00a0The decision-making approach seems debatable.\u00a0\u00a0  \n**Response:**\u00a0Please see the comments below  \n  \n**Comment 7(a):**\u00a0Q4.1 In Section 4: is it reasonable to leave $b$ unconstrained ? Shouldn't a monotonicity assumption be imposed ?\u00a0  \n  \n**Response:**\u00a0Since for each uncertainty level $i$, there is a single threshold $b_i$, we do assume monotonicity of the true conditional probability of the positive class with respect to the model score for a fixed uncertainty but not with respect to uncertainty for a fixed model score. This is because monotonicity with respect to uncertainty that does not hold true across the entire score range. Figure 3(a) shows the behavior (flipped trends for different score ranges) for the case of Beta priors with uncertainty estimated using Posterior Networks.\u00a0\u00a0  \n  \nIn fact, we did experiment with a variant (EW_DPMT-MONO) of the DP algorithm where we constrained the decision boundary such that the true conditional probability is monotonically decreasing in uncertainty (i.e., $b_i \\geq b_j$ for $i > j$, the boundary function to be look like a staircase). This variant has higher complexity $O(K^2L^3)$ than EW_DPMT where $K$ and $L$ are the number of uncertainty and score bins. The Table below shows a comparison of the performance of the two algorithms for the same bin configuration. We observe that for high precision bound (as in Criteo), the monotonicity assumption does hold, but that is not true as the precision bound is lowered and the unconstrained version results in better performance.\u00a0  \n  \nTable 1: Performance with different unconstrained and monotonic variants of EW-DPMT with uncertainty bins = 100 and score bins = 3. Note that here we used a smaller number of uncertainty bins instead of 500 (in the paper) to reduce computational effort.\u00a0  \n\n|   |   |   |\n|---|---|---|\n|Dataset|Criteo (Tau=3) [Recall@90% Precision]|Avazu (Tau=5) [Recall@70% Precision]|\n||Score_bins= 100, Unc_bins=3|Score_bins= 100, Unc_bins=3|\n|ST|2.2\u00b10.2%|1.9\u00b10.6%|\n|EW_DPMT|2.6\u00b10.3%|2.3\u00b10.6%|\n|EW_DPMT - MONO|2.6\u00b10.3%|1.9\u00b10.6%|\n\n**Comment 7(b):**\u00a0Q4.2 Is it reasonable to always keep the decision-making process precise ? Wouldn't it be preferable to abstain in presence of a high (epistemic) uncertainty ?\u00a0  \n  \n**Response:**\u00a0In the setting we are considering, we are attempting to optimize recall of detecting something as positive (e.g., fraudulent transaction or malignant tumor) or not while maintaining a certain precision level. Here, we are not distinguishing between abstaining and a negative decision since we are focusing on the precision and recall of positive detection.\u00a0\u00a0We could reformulate the problem to find two decision boundaries that separate out three regions: positives, abstain, and negatives (e.g., malignant tumor, needs further testing, no tumor) but need to redefine the objective function appropriately.\u00a0  \n  \n  \n**Comment 7(c):**\u00a0Q4.3 Aleatoric and epistemic uncertainties are here not distinguished from each other, which seems to be an issue: what if u(x) is maximal ? Is it reasonable to make decisions in this case ?\u00a0  \n  \n**Response:**\u00a0\u00a0Making a decision about positive class prediction, abstaining or negative class prediction based only on uncertainty level of a bin is likely to be suboptimal. Instead, it is preferable to consider the model output score, uncertainty level as well as the actual positive class probability in the calibration set for the corresponding bin. If the actual positive class probability\u00a0\u00a0is high, then the bin will get picked to be on the positive side of the decision boundary. (Similarly with negative class and abstaining if neither is high).  \n  \n  \nSince aleatoric uncertainty is a deterministic function of the model estimated conditional probability (model score), the additional predictive power of the uncertainty signal primarily comes from the \u2018epistemic uncertainty\u2019. In case of the posterior networks, the differential entropy of the \u2018epistemic distribution\u2019 q(x) includes contributions from both aleatoric and epistemic uncertainty. However, this is not an issue since the calibration approach adjusts for it and utilises the non-redundant information given the model score.  \n  \n**Comment 7(d):**\u00a0Q4.4 What is the effect of discretizing the Score-Uncertainty space ? To this extent, shouldn't the independent splitting on both dimensions avoided ?  \n  \n**Response:**\u00a0Fig 5(a-b) show the dependence of performance on varying bin counts. In general, very fine bins lead to high computation and poor generalization, while coarse bins lead to over smoothing even if the compute costs are lower. There is a sweet spot that is dataset dependent.\u00a0  \n  \nWe did not understand the comment on avoiding the independent splitting. In Sec 6.2, Table 1, we do report results on the equi-span setting with independent splitting. In most cases, the resulting recall @precision bound from nested splitting is comparable or better."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579644702,
                "cdate": 1700579644702,
                "tmdate": 1700579644702,
                "mdate": 1700579644702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P0nuuvSFCs",
                "forum": "nsNyDvNQTc",
                "replyto": "RZa0tMxclj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i837 [4/4]"
                    },
                    "comment": {
                        "value": "**Comment 8: Q5**\u00a0\u00a0The \"proof\" that the partitioning problem of the Score-Uncertainty space is NP-hard (mentioned in the introduction) seems to directly proceed from the bin-packing problem behind. \n\n**Response:**\u00a0\u00a0Please note that partitioning of the score-uncertainty space does not readily map to any of the standard bin-packing or knapsack problems because while the profit (recall) is additive, the cost (1-precision) is not additive and there is a dependence on the bins selected till that point. The NP-hardness result follows from a mapping to the subset-sum problem through a careful constructed reduction (Appendix F). To the best of our knowledge, this is not a previously published finding and there are potentially other applications for this result.\u00a0  \n  \n  \n**Comment 9: Q6:**\u00a0Equi-weight binning case, \"it is possible that ... can you be a bit more specific ?  \n  \n**Response:**\u00a0\u00a0Thanks for the question. We will rewrite the line to explain it better. Let us say we wish to maximize recall for a precision bound of $\\sigma$. Let the current selected bins have a net total of N samples with P of them positive. Given two bins A and B with total and positive samples and $(n_A, p_A)$ and $(n_B, p_B)$. Even if $p_A/n_A > p_B/n_B$, it might be preferable to choose bin B over bin A since it is possible that $(P+p_B)/(N +n_B) > \\sigma > (P+p_A)/(N +n_A)$ when the bin sizes $(n_A, n_B)$ are different. For example, consider the case where P=10, N=20, p_A=5, n_A = 15, p_B = 1, and n_B =4 . Here $p_A/n_A = 5/15 > p_B/n_B = 1/4$, but $( P+p_B)/(N +n_B) = 11/24 > (P+p_A)/(N +n_A) = 15/35$.\u00a0  \n  \n**Comment 10: Q7:** Binning strategy: I understand ... the bins computed.  \n  \n**Response:**\u00a0\u00a0We explore three binning strategies (a) independent binning or equ\u2014span (b) splitting first on uncertainty followed by score [Unc-Score] (c) splitting first on score followed by uncertainty [Score-Unc]. In Sec 6.1, Table 1, we present results on (a) and (b). We skipped including results on (c) for the lack of space and to avoid confusion. The table below summarizes the results for binning strategy (c) which yields similar results. There is also a dependence on the number of bins chosen (see Figures 5(a) and 5(b)) In future, it might able be preferable to explore perform binning and decision boundary detection jointly.  \n  \nTable 2: Performance with different equi-weight binning strategies. Score-Unc involves splitting on Score quantiles followed by that Uncertainty while it is the opposite for Unc-Score. Same number of score and uncertainty bins is used in both.  \n\n| Dataset                | Criteo (Tau=3) [Recall@90% precision] |                | Avazu (Tau=5) [Recall@70% precision] |                |\n|------------------------|-------|----------------|-------|----------------|\n| Binning nest Order         | Score-Unc   | Unc-Score   | Score-Unc   | Unc-Score   |\n| #bins                  | (500, 3)    | (3, 500)    | (500, 3)    | (3, 500)    |\n| ST                     | 2.2\u00b10.2%  | 2.2\u00b10.2%  | 1.9\u00b10.6%  | 1.9\u00b10.6%  |\n| MIST                   | 2.6\u00b10.2%  | 2.6\u00b10.3%  | 2.9\u00b10.2%  | 2.6\u00b10.3%  |\n| GMT                    | 2.6\u00b10.4%  | 2.7\u00b10.3%  | 2.9\u00b10.2%  | 2.7\u00b10.3%  |\n| EW_DPMT                | 2.7\u00b10.2%  | 2.7\u00b10.3%  | 2.9\u00b10.2%  | 2.7\u00b10.3%  |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580001682,
                "cdate": 1700580001682,
                "tmdate": 1700580001682,
                "mdate": 1700580001682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N3hdjBp5ws",
                "forum": "nsNyDvNQTc",
                "replyto": "P0nuuvSFCs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed answers to my comments and questions. \n\nI agree that these clarifications better enhance the contributions of the submission; I have raised my score accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644342161,
                "cdate": 1700644342161,
                "tmdate": 1700644342161,
                "mdate": 1700644342161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "knLSGDaCgF",
            "forum": "nsNyDvNQTc",
            "replyto": "nsNyDvNQTc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
            ],
            "content": {
                "summary": {
                    "value": "Paper considers new approach to uncertainty estimation for calibrating the model and improving the classification accuracy in the case of (imbalanced) training set and/or dataset drift. Theoretical and empirical analysis of model output score dependence of uncertainty and the score are evaluated, and based on dynamic programming and isotonic regression an approximation algorithms are formulated for general NP-hard problem of decision boundary selections using both score and uncertainty. Three benchmark datasets, from online advertising and e-commerce domain, are utilised to shown the usefulness of the proposed approach, compared to related algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Paper proposes novel theoretical analysis of combining predictive uncertainty and classification score for decision boundary selection. The findings are supported by empirical analysis and versatile set of tests. It is shown that optimisation problem is NP-hard, and the new practical algorithms to approximate the solution with dynamic programming and isotonic regression based approaches, are presented. To my knowledge, the analysis and proposed algorithms for this particular problem are novel contributions, showing improvements against some related score only and 2D decision boundary selection algorithms on real-world benchmark datasets."
                },
                "weaknesses": {
                    "value": "The proposed approach is motivated by the applications such as medical diagnosis and fraud detection. However, only advertising and e-commerce datasets are considered in the empirical experiment section. It would strengthen the work, if other additional imbalanced datasets from different application domains could have been experimented, as well."
                },
                "questions": {
                    "value": "- Why choosing the particular datasets? (additional data from some other application domains could support the work and findings further)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762419843,
            "cdate": 1698762419843,
            "tmdate": 1699636526088,
            "mdate": 1699636526088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zu1WBJStbQ",
                "forum": "nsNyDvNQTc",
                "replyto": "knLSGDaCgF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZEGe [1/1]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and share our response below. \n\n**Comment 1** : Q1: The proposed approach is motivated by the applications such as medical diagnosis and fraud detection. However, only advertising and e-commerce datasets are considered in the empirical experiment section. It would strengthen the work, if other additional imbalanced datasets from different application domains could have been experimented, as well.\nWhy choosing the particular datasets? (additional data from some other application domains could support the work and findings further)\n\n\n**Response** : Due to the space limitation we could only present results on a few datasets, but we do wish to clarify that the E-Com dataset was derived from a proprietary fraud/abuse detection application. Certain transformations were applied to avoid disclosing sensitive information while preserving the core integrity of the data for research purposes."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479562710,
                "cdate": 1700479562710,
                "tmdate": 1700479562710,
                "mdate": 1700479562710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l7YiUkXitB",
                "forum": "nsNyDvNQTc",
                "replyto": "zu1WBJStbQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. I have read all reviews and rebuttals, and authors have clarified many concerns. Manuscript shows promising results combining model score and uncertainty, although empirical datasets could have been more versatile. I keep my original score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5267/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721890680,
                "cdate": 1700721890680,
                "tmdate": 1700721890680,
                "mdate": 1700721890680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]