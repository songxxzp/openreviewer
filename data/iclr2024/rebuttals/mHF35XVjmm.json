[
    {
        "title": "MADiff: Offline Multi-agent Learning with Diffusion Models"
    },
    {
        "review": {
            "id": "9TYacbC01v",
            "forum": "mHF35XVjmm",
            "replyto": "mHF35XVjmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers cooperative multi-agent problems within the centralized training decentralized execution paradigm. The paper proposes the first multi-agent method to use a diffusion model in order to generate trajectories during online execution. The advantage of using a generative model to create policies is that the diffusion method is able to learn from behavioral data with severe extrapolation errors. Prior value-based methods are unable to do this due to the updates from out-of-distribution points. Additionally, prior methods have yet to apply this type of method to multi-agent cases. The paper introduces a novel attention-based diffusion model for multi-agent learning which combines CTDE, opponent modeling, and trajectory prediction. The paper evaluates on benchmark CTDE environments with offline data available and finds that their method tends to perform better."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Using diffusion to model the agents in multi-agent learning is a very interesting premise. There is great potential for this method to be quite impactful in a multi-agent learning context. The ability to generalize from the value-based methods may prove useful given a way to combine this with online finetuning. This may be interesting for future work on foundation models in this area."
                },
                "weaknesses": {
                    "value": "- This is a very interesting paper but there are unfortunate issues with the experimental design. SMAC (Version 1) has known issues. There is a recent version that was recently accepted at NeurIPS but has been released for a while that fixes the \u201copen-loop\u201d issues of SMACv1 called SMACv2[1]. Given that these tasks were essentially solved by the community with in the easy maps that are compared with in the results, the results may not be as convincing as currently indicated. Why is the score for SMAC so low. Typically the SMAC reported value is winrate, which should be near 100% for these easier SMAC tasks, especially with QMIX. There must be a bug in the evaluation or the data is far to limited to evaluate the method properly.\n[1] Ellis, B., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J. N., & Whiteson, S. (2022). SMACv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2212.07489.\n\n- Regarding the other experimental design items, it is unclear what hypothesis the MPE environments test that would not be tested by SMAC. For the MATP, this seems like a niche task. It is unclear if the test games have a distribution of trajectories that are out of distribution of the training/validation set. Otherwise, it is hard to understand the generalization capability of the method.\n\n- How does the centralized control version scale with the number of agents? It is unclear if this is a useful avenue of exploration compared with the CTDE paradigm."
                },
                "questions": {
                    "value": "I still do not understand the \u201cDecentralized policy and centralized controller\u201d terminology. I assumed that this means centralized training decentralized execution (CTDE). The new language is confusing.\n\nIs an attention network used for all comparison baselines?\n\nPlease see Weaknesses for additional questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4357/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W",
                        "ICLR.cc/2024/Conference/Submission4357/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816632111,
            "cdate": 1698816632111,
            "tmdate": 1700607914919,
            "mdate": 1700607914919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2StHNRjMSJ",
                "forum": "mHF35XVjmm",
                "replyto": "9TYacbC01v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you very much for your constructive comments! We try to address your concerns as follows.\n\n**Q1: \"Given that these tasks were essentially solved by the community with in the easy maps that are compared with in the results, the results may not be as convincing as currently indicated.\"**\n\n**A1:**\nThanks for your comment, but you might misunderstand our experimental settings. Our method and all baselines, including QMIX, are trained in a fully offline manner. Offline Policy learning is far more challenging than online learning due to limited data support and sub-optimal behavior policies used to collect the dataset. \n\nOne task can be solved by online RL methods does not mean it can be solved at the expert level with offline learning. For example, PPO [1], which was proposed in 2017, can easily achieve expert-level performances in single-agent mujoco environments (Hopper, HalfCheetah, ...). However, the widely-used D4RL benchmark [2], which was released in 2020, still uses those environments to build their dataset. And the SOTA algorithms today can only obtain about half of expert scores on D4RL's halfCheetah-medium dataset [3].\n\nAs a result, our results under offline MARL settings should not be compared with online MARL methods. \n\n**Q2: \"Regarding the other experimental design items, it is unclear what hypothesis the MPE environments test that would not be tested by SMAC.\"**\n\n**A2:**\nBoth MPE and SMAC datasets are widely adopted by existing works in offline MARL [4, 5]. These two classes of environments have distinct state/action spaces and require different cooperation patterns. We include both of them to ensure a thorough evaluation of our method. Two intuitive differences are that the action space of MPE is continuous, and the initial state distributions are more diverse than SMAC.\n\n**Q3: \"It is unclear if the test games have a distribution of trajectories that are out of distribution of the training/validation set.\"**\n\n**A3:**\nThanks for your good question! We adopt the standard training/validation/test split in MATP experiments, which allows for out-of-distribution (OOD) tests. On offline MARL tasks, our evaluation procedure follows the existing works on offline RL [6, 7] and MARL [4, 5] and does not deliberately test the generalization ability. \n\nWe want to highlight that different from static supervised learning tasks in MATP, evaluating offline-learned policies by interacting online naturally includes the requirement of OOD generalizations. Due to errors induced by functional approximations and stochasticity in environmental dynamics, agents will encounter OOD states even if they are started from in-distribution states. The learned policies must have the ability to recover from OOD states and prevent errors from accumulating over time.\n\nDesigning reasonable benchmarks for specifically evaluating the generalization ability of offline MARL models is less discussed yet, and we acknowledge that it is an important future research direction.\n\n**Q4: \"How does the centralized control version scale with the number of agents? It is unclear if this is a useful avenue of exploration compared with the CTDE paradigm.\"**\n\n**A4:**\nBoth versions of MADiff have their own values. MADiff-C is only applicable in cases where a centralized controller for all agents is allowed, such as traffic light control or fleet control. Since all agents' actions are made jointly, it is easier to keep their behaviors coordinated. Therefore, MADiff-C is supposed to perform better compared to MADiff-D in those cases, which is aligned with our experiment results. However, there are more scenarios where centralized controls are impossible, e.g., autonomous driving and distributed sensor networks [8]. MADiff-D, which is the CTDE version of MADiff, is designed for these scenarios.\n\nMADiff-C and MADiff-D scale the same as the number of agents increases. We use a shared U-Net model for all agents, where different agents' data can be batched together and passed through the network. It does not cost much more inference time as the number of agents increases with GPU-accelerated computing. For self-attention among agents' embeddings, it can also scale with linear complexity [9].\n\nWhen the number of agents is extremely large, we can modify the opponent/teammate modeling to operate in a projected low-dimensional space with a latent diffusion model [10]. We leave this direction to future work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573114515,
                "cdate": 1700573114515,
                "tmdate": 1700573114515,
                "mdate": 1700573114515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pOhDH5VieI",
                "forum": "mHF35XVjmm",
                "replyto": "2StHNRjMSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reply. Given that some concerns (but not all) have been addressed. I have readjusted my score.\n\nQ1: \"Given that these tasks were essentially solved by the community within the easy maps that are compared with in the results, the results may not be as convincing as currently indicated.\"\n\nTo clarify, the question asked was the following: \"Why is the score for SMAC so low? Typically the SMAC reported value is winrate,\"\n\nPlease indicate the meaning of the score. Do the offline methods learn from adequately performing data? Based on [1], the relationship between winrate and score is unclear.\n\nThe [1] benchmark that provides the data also has an SMACv2 available. Since SMACv2 requires a closed-loop policy, it is much more interesting for comparison even in the offline reinforcement learning case. Additionally, there is data readily available. Have you tested your algorithm with that method?\n\n[1] Claude Formanek, Asad Jeewa, Jonathan Shock, and Arnu Pretorius. Off-the-grid marl: a framework\nfor dataset generation with baselines for cooperative offline multi-agent reinforcement learning.\narXiv preprint arXiv:2302.00521, 2023."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607891128,
                "cdate": 1700607891128,
                "tmdate": 1700607891128,
                "mdate": 1700607891128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26JHmb4hzP",
                "forum": "mHF35XVjmm",
                "replyto": "LJvQc6Kfip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Tx3W"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the further responses.\n\nRegarding Q6, I do not believe that this is a fair comparison with respect to empirical comparison. It is hard to say whether the diffusion aspect or the attention aspect is helping the method increase the performance. If a comparison of just your method with and without the attention mechanism existed, would there be an increase in performance? I think the use of diffusion is very novel and interesting, but I am having trouble being convinced that the results are strictly due to the diffusion aspect."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607894424,
                "cdate": 1700607894424,
                "tmdate": 1700607894424,
                "mdate": 1700607894424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ppetYQ3cL9",
                "forum": "mHF35XVjmm",
                "replyto": "9TYacbC01v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updates of ablation results"
                    },
                    "comment": {
                        "value": "We are delighted to share with you some new ablation results!\n\nAs mentioned in **A3**, to test whether both diffusion modeling and attention networks are important in the success of MADiff, we compare our network design to ConcatDiff, which is an ablation variant that does not use attention networks but directly concatenates all agents' observations as inputs to a U-Net. ConcatDiff-C is used for centralized control, and ConcatDiff-D masks other agents' observations before concatenation during training. Results are presented as follows:\n\n| Dataset          | OMAR            | ConcatDiff-D    | MADiff-D        | ConcatDiff-C | MADiff-C    |\n| :--------------- | :-------------- | :-------------- | --------------- | ------------ | ----------- |\n| Spread-Expert    | 114.9 $\\pm$ 2.6 | 106.4 $\\pm$ 3.5 | 98.4 $\\pm$ 12.7 | 112.7 $\\pm$ 4.3  | 114.7 $\\pm$ 5.3 |\n| Spread-Md-Replay | 37.9 $\\pm$ 12.3 | 31.5 $\\pm$ 2.7  | 42.9 $\\pm$ 11.6 | 32.2 $\\pm$ 3.5   | 47.2 $\\pm$ 6.6  |\n\nWe can observe that ConcatDiff performs worse than MADiff in both CTCE and CTDE cases, and underperforms OMAR, which is the best baseline method in MPE datasets.\n\nWe sincerely hope that our responses and the additional experiments we conducted have addressed your concerns and enhanced the quality of the paper. We would greatly appreciate it if you could clarify any remaining concerns you may have."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731188710,
                "cdate": 1700731188710,
                "tmdate": 1700731474212,
                "mdate": 1700731474212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sd38j4QLNd",
            "forum": "mHF35XVjmm",
            "replyto": "mHF35XVjmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_QCKf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_QCKf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MADiff which incorporates diffusion models for either a centralized planner or decentralized actors. The diffusion model builds on Decision Diffuser (Ajay et al., 2023) and additionally uses an attention layer for better coordination among the agents. The training procedure enables the decentralized policies to have opponent modeling capabilities where agents predict the observations of other agents. MADiff is evaluated on SMAC and MPE as well as the NBA Dataset for trajectory prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The use of diffusion models for offline MARL is interesting and this combination is, as far as I know, novel. Diffusion models have shown promise in offline RL due to its potential for stitching and generalization capabilities. In this context, whether diffusion policies can improve offline MARL is an important problem, and it is interesting to view this from the lens of opponent modeling. Furthermore, viewing offline MARL as trajectory prediction makes it more scalable without requiring restrictive assumptions such as IGM. Finally, the attention mechanism which is the main difference between Decision Diffuser and MADiff seems important in the offline MARL setting, as shown in the ablation studies (Section 5.5)."
                },
                "weaknesses": {
                    "value": "1. The overall structure of the paper and algorithm is very similar to Decision Diffuser (Ajay et al. 2023), but with an additional attention layer.  \n2. The benefits of using diffusion policies mentioned in Decision Diffuser, such as generating novel behaviors, stitching or different conditions for $y(\\tau)$ such as constraints or skills are not addressed in MADiff in the context of MARL. \n\n\n3. The inverse dynamics loss in Eq. 7 looks a little strange since the observations $s^i, s^{i\u2019}$ are used, which makes it unclear what this is predicting, since other agents\u2019 actions as well as partial observations are involved. This is essentially assuming that the next state observation only conditions on the current local observations and local actions. This may be fine for simple tasks but may not translate to more complex tasks where the other agents\u2019 actions impact the dynamics. Also, the notation $s^i$ is confusing here since Section 2 uses $o^i$. \n4. Only a small number of agents (3-5) are considered in the experiments .\n5. While opponent modeling is mentioned as one of the contributions, previous work in opponent modeling are missing from the Related Work section (e.g. [1] and references therein) and there is no explicit mention about what kinds of information are available at test time (e.g. only local observation-action history of ego-agent). \n6. OMAR is excluded from SMAC tasks as a baseline despite OMAR also reporting SMAC results. \n7. (Minor) The perturbed states $\\tilde x$ in $\\hat \\tau_k := [s_t,\\tilde x_{t},... ]$ above Eq. 4 are not properly defined.\n8. (Minor) Grammatical error in 5.4: \u201cWe that..\u201d\n\n[1] Agent Modelling under Partial Observability for Deep Reinforcement Learning (Papoudakis et al,. NeurIPS 2021)"
                },
                "questions": {
                    "value": "1. How do diffusion models address the curse of dimensionality of the joint action space? How does MADiff scale as $N$ increases?\n2. In the second paragraph of the Introduction, do you mean that \u201clearning the $\\textbf{incorrect}$ centralized value for each agent\u2026\u201d? Does this mean that extrapolation error can occur even when the correct value is learned, or is this a typo?\n3. Please address the points raised in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829744021,
            "cdate": 1698829744021,
            "tmdate": 1699636407345,
            "mdate": 1699636407345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EKzXXspVOU",
                "forum": "mHF35XVjmm",
                "replyto": "sd38j4QLNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, we appreciate your time and effort in reviewing this paper. We try to address your concerns as follows and hope we can ease your concerns.\n\n**Q1: \"The overall structure of the paper and algorithm is very similar to Decision Diffuser, but with an additional attention layer.\"**\n\n**A1:**\nIt is indeed that many successful design choices of [1] have inspired us and guided us to derive our method. Similarly, much of [1] also referred to the design of diffuser [2], but that doesn't take away from the fact that it's an excellent research.\n\nCompared with [1], MADiff, which is the first to apply diffusion models in offline MARL, is naturally designed for multi-agent problems, taking multi-agent coordination, opponent modeling, and trajectory planning in a unified framework. **Our contributions include but beyond the model design**, and the most important one is that the proposed framework allows the use of the same model structure to handle various tasks (CTCE, CTDE, MATP) by flexible conditioning during evaluation and achieves superior results on those tasks.\n\n**Q2: \"The benefits of using diffusion policies mentioned in Decision Diffuser, ... are not addressed in MADiff in the context of MARL.\"**\n\n**A2:**\nCompared to single-agent learning, multi-agent learning problems themselves are more challenging and can benefit from diffusion modeling. Specifically, besides the variety of a single agent's behavior, the interplay between multiple agents exhibits a multi-modal and complex nature. Our goal is to introduce DM with powerful modeling ability of arbitrary distributions to MAL for better learning multi-agent interactions in offline datasets. Verifying whether skill composition in SAL still applies in MAL settings is an interesting trial, but it is out of the scope of our paper.\n\n**Q3: \"The inverse dynamics loss in Eq. 7 looks a little strange since the observations s^i, s^{i'} are used, ... but may not translate to more complex tasks where the other agents\u2019 actions impact the dynamics. Also, the notation s^i is confusing here since Section 2 uses o^i.\"**\n\n**A3:**\nThanks for your careful checking. There is a typo in Eq.7, where $s^i$ and $s^{i'}$ should be replaced by $o^i$ and $o^{i'}$. We have made corrections in the revised manuscript.\n\nWe would like to justify our use of IDM $I(o^i, o^{i'})$ in multi-agent tasks in two aspects:\n\n+ First, since the inverse dynamics model is used rather than a forward dynamics model, we are not assuming \"the next state observation only conditions on the current local observations and local actions\", but that the current and next local observations determine the local actions. Since the next local observation is jointly generated with predictions of other agents' observations, it already considers the impact of other agents' actions.\n\n+ More importantly, the goal in Dec-POMDP is learning a policy for each agent, which can maximize cumulative rewards **when all agents are controlled by learned policies** [3]. Therefore, although actions are made in a decentralized way, each agent's policy is learned from the same dataset the IDM is trained on. Thus, $p(a^i|o^i, o^{i'})$ during evaluation should be similar to IDM's training distribution. Learning policies that can coordinate with any teammates requires zero-shot coordination [4], and is (to our knowledge) not yet discussed in offline settings and clearly out of the scope of this paper.\n\n**Q4: \"Only a small number of agents (3-5) are considered in the experiments.\"**\n\n**A4:**\nMost existing works in offline MARL do not consider tasks with a large number of agents. For example, MADT-KD [5] and OMAR [6] are tested on environments with up to 8 agents. Moreover, OMAR uses eight multi-agent environments for benchmarking, yet only two of these consist of more than 5 agents. Despite the fact that we consider at most 5 agents in MARL experiments, our MATP experiments are conducted on NBA datasets with 10 agents, and MADiff demonstrates superior performance than the baseline method. \n\n**Q5: \"While opponent modeling is mentioned as one of the contributions, previous works in opponent modeling are missing from the Related Work section.\"**\n\n**A5:**\nThanks for your suggestions! We have included a discussion of previous opponent modeling literature in the related work section."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572816784,
                "cdate": 1700572816784,
                "tmdate": 1700572816784,
                "mdate": 1700572816784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aPDDA9CRW9",
                "forum": "mHF35XVjmm",
                "replyto": "sd38j4QLNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer QCKf,\n\nWe deeply appreciate the valuable and constructive feedback you provided. As the rebuttal discussion between reviewers and authors is coming to a close on November 23rd, we would greatly appreciate it if you could clarify any remaining concerns you may have. This will ensure that we can adequately address them during this period.\n\nThank you for your attention to this matter. We are looking forward to your response.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728305619,
                "cdate": 1700728305619,
                "tmdate": 1700728328607,
                "mdate": 1700728328607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nehkrdFUdU",
            "forum": "mHF35XVjmm",
            "replyto": "mHF35XVjmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_nDb9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_nDb9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel diffusion-based offline multi-agent learning framework, called MADIFF. Specifically, MADIFF proposes the attention-based architecture in Section 3.1 to interchange information and learn coordination between agents. MADIFF is also designed with centralized training and decentralized execution, inherently enabling the framework to perform the opponent modeling during execution. Evaluations in MPE, SMAC, and MATP show the effectiveness of MADIFF against competitive baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, the paper is very clearly written achieving the SOTA results compared to baselines.\n2. MADIFF is a principled diffusion-based framework without needing complicated components to achieve effective performance. \n3. Code is available for reproducibility."
                },
                "weaknesses": {
                    "value": "Overall, I enjoyed reading this paper and learning about MADIFF, but there are a few questions that I would like to follow up on:\n1. Because MADIFF needs to predict the next observations (Equations 8 and 9), I would like to ask whether MADIFF can be applied to domains with high-dimensional inputs (e.g., image). I noticed that the experimental domains have relatively small dimension inputs. \n2. In Equation 9, inferring all other agents' observations could be very difficult when $N$ is large. Could MADIFF scalability benefit by inferring only the agent $i$'s neighbors instead of all $N$ agents?\n3. Can MADIFF be applied to general-sum settings or would it be limited to cooperative settings only?"
                },
                "questions": {
                    "value": "I hope to ask the authors' responses to my questions outlined in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698935580041,
            "cdate": 1698935580041,
            "tmdate": 1699636407039,
            "mdate": 1699636407039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5C0KnEcRW0",
                "forum": "mHF35XVjmm",
                "replyto": "nehkrdFUdU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thanks for your sincere review and advice! Hereby we try to answer your questions.\n\n**Q1: \"whether MADIFF can be applied to domains with high-dimensional inputs (e.g., image)\"**\n\n**A1:**\nThanks for your valuable suggestions! MADiff can be applied in environments with high-dimensional, e.g., image observations with more complicated encoder and decoder networks such as CNN or vision transformers. There are some successful cases of applying diffusion planning to single-agent and image-based tasks [1, 2], which can be incorporated into the MADiff framework. We leave this to future work and add a brief discussion to the conclusion of this paper.\n\n**Q2: \"In Equation 9, inferring all other agents' observations could be very difficult when N is large. Could MADIFF scalability benefit by inferring only the agent i's neighbors instead of all N agents?\"**\n\n**A2:**\nThat is a good point, and your suggestion is valuable in boosting the scalability of MADiff. Selective opponent modeling is necessary in scaling to environments with very large N, and only inferring agent i's neighbors can reduce computation and modeling costs. However, neighbors defined under simple Euclidean distance may not be reasonable in some environments, while a learning-based distance metric would be more desirable. Another possible choice is to map the trajectories of all other agents to a low-dimensional embedding space and perform latent diffusion on it. Those modifications required are non-trivial, and we leave this direction as part of future work.\n\n**Q3: \"Can MADIFF be applied to general-sum settings or would it be limited to cooperative settings only?\"**\n\n**A3:**\nThe MADiff framework can only be applied to cooperative tasks, which is the same as most prior works in offline MARL [3, 4]. Naive offline RL or generative modeling methods are likely to fail on non-cooperative tasks, since the optimal policy learned from static datasets can be exploited by other agents during evaluations if they change their policies to be different than behavior policies in datasets. Effective offline MARL in multi-player general-sum settings requires offline equilibrium finding (OEF), which is particularly challenging due to limited data support. Very little work has attempted to solve OEF and is limited to simple tasks with discrete state and action spaces [5].\n\n**References**\n\n[1] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" *arXiv preprint arXiv:2303.04137* (2023).\n\n[2] Du, Yilun, et al. \"Learning universal policies via text-guided video generation.\" *Thirty-seventh Conference on Neural Information Processing Systems*. 2023.\n\n[3] Pan, Ling, et al. \"Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n[4] Yang, Yiqin, et al. \"Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning.\" *Advances in Neural Information Processing Systems* 34 (2021): 10299-10312.\n\n[5] Li, Shuxin, et al. \"Offline equilibrium finding.\" *arXiv preprint arXiv:2207.05285* (2022)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572609055,
                "cdate": 1700572609055,
                "tmdate": 1700572609055,
                "mdate": 1700572609055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WN3w3BDqh0",
                "forum": "mHF35XVjmm",
                "replyto": "5C0KnEcRW0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_nDb9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_nDb9"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors for their detailed response to my feedback. The rebuttal addresses my questions (Q1-3). However, I agree with Reviewer Khwp's and QCKf's concerns, wherein some readers may interpret MADiff as Decision Diffuser (Ajay et al. 2023) with an additional attention layer based on the current writing. At the same time, I (partially) agree with the authors' points: 1. conditional generative modeling of multi-agent trajectories and 2. flexibility to handle various tasks (CTCE, CTDE, MATP). I would like to maintain my score but suggest to the authors to write more clearly, distinguishing the differences and importance of the contributions in a revised paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711299713,
                "cdate": 1700711299713,
                "tmdate": 1700711299713,
                "mdate": 1700711299713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gV5C34ytq6",
            "forum": "mHF35XVjmm",
            "replyto": "mHF35XVjmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Khwp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Khwp"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents MADIFF, a centralized training-decentralized execution diffusion framework for multi-agent RL problems. MADIFF performs return-conditioned trajectory modeling with an attention-based diffusion architecture for information interchange among agents. MADIFF can be applied to both centralized and decentralized execution settings. Especially in the decentralized execution, MADIFF performs the opponent modeling, predicting the other agents' joint observations based on its local observation. In the experiments, MADIFF generally outperforms the baselines in MPE and is competitive in SMAC. For the MATP problem, MADIFF-C significantly outperforms the baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A Diffusion-based offline MARL algorithm (an extension of DecisionDiffuser to a multi-agent setting) is presented, with a suitable attention mechanism for information interchange among agents in MARL.\n2. In the experiments, MADIFF generally outperforms the baselines. The ablation study confirms that the proposed attention modules were indeed helpful."
                },
                "weaknesses": {
                    "value": "1. The novelty of the work seems a bit limited. MADIFF heavily relies on the existing work DecisionDiffuser (Ajay et al., 2023), and it can be seen as its simple extension to the MARL setting, with an additional attention layer that processes information exchange among agents. Adopting an attention mechanism for multi-agents itself does not seem to be a new idea. (e.g. [1,2])\n2. More baseline (MADT-KD [3]) could have been compared in the experiments.\n\n[1] Iqbal et al., Actor-Attention-Critic for Multi-Agent Reinforcement Learning, 2019\n[2] Wen et al., Multi-Agent Reinforcement Learning is A Sequence Modeling Problem, 2022\n[3] Tseng et al., Offline Multi-Agent Reinforcement Learning with Knowledge Distillation, 2022"
                },
                "questions": {
                    "value": "1. In the top-middle of Figure 2, why is the planned trajectory of the planning agent (purple) different from the real sampled trajectory?\n2. MADIFF relies on the inverse-dynamics model (IDM) for action selection, but it can be suboptimal when the transition dynamics are stochastic. In MARL, even though the underlying environment is deterministic, if we see the other agents as a part of the environment (thinking of decentralized execution), the transition dynamics (by other agents' actions) will be stochastic when other agents' policies are stochastic. Even in this situation, doesn't using IDM cause any problems?\n3. If we view the MARL problem as a single-agent problem (by viewing joint action space as a single-agent large action space & joint observation space as a single-agent observation space), how is MADIFF-C different from DecisionDiffuser? Is MADIFF-C still performing better than DecisionDiffuser which operates in the joint observation/action space?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699315390382,
            "cdate": 1699315390382,
            "tmdate": 1699636406978,
            "mdate": 1699636406978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "twg3vDpMBI",
                "forum": "mHF35XVjmm",
                "replyto": "gV5C34ytq6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thanks for your time reviewing this paper! We have conducted several additional experiments, and try to ease your concern below.  Hope we can ease your concerns.\n\n**Q1: \"The novelty of the work seems a bit limited. MADIFF heavily relies on the existing work DecisionDiffuser (Ajay et al., 2023), and it can be seen as its simple extension to the MARL setting ... Adopting an attention mechanism for multi-agents itself does not seem to be a new idea.\"**\n\n**A1:** It is indeed that many successful design choices of [1] have inspired us and guided us to derive our method. Similarly, much of [1] also referred to the design of diffuser [2], but that doesn't take away from the fact that it is an excellent work.\n\nCompared with DM works in single-agent learning, MADiff is naturally designed for multi-agent problems. By using different conditioning during evaluation, the same framework can handle multi-agent coordination, opponent/teammate modeling, and joint trajectory prediction, which is supported by strong experimental results. We believe that the references in the single-agent learning domain do not diminish the novelty of our approach in the context of multi-agent learning.\n\nWe acknowledge that the attention mechanism has been used for years in MAL, and we verified that it is also effective in conditional generative modeling of multi-agent trajectories.\n\n**Q2: \"More baseline (MADT-KD) could have been compared in the experiments.\"**\n\n**A2:** Thank you for your suggestion! To our knowledge, MADT-KD does not have an open-sourced implementation, and it requires non-trivial changes on top of MADT's training code. We include MADT [3] as an additional baseline on SMAC datasets, and the results are pasted below:\n\n| Dataset     | MADiff-D       | MADiff-C       | MADT           |\n| :---------- | :------------- | :------------- | :------------- |\n| 3m-Good     | 18.8 $\\pm$ 0.2 | 19.7 $\\pm$ 0.1 | 19.0 $\\pm$ 0.3 |\n| 3m-Medium   | 17.2 $\\pm$ 0.3 | 18.4 $\\pm$ 0.2 | 15.8 $\\pm$ 0.5 |\n| 3m-Poor     | 11.2 $\\pm$ 0.1 | 11.8 $\\pm$ 1.0 | 4.2 $\\pm$ 0.1  |\n| 5m6m-Good   | 16.5 $\\pm$ 0.3 | 18.1 $\\pm$ 0.1 | 16.8 $\\pm$ 0.1 |\n| 5m6m-Medium | 16.3 $\\pm$ 0.1 | 17.6 $\\pm$ 0.4 | 16.1 $\\pm$ 0.2 |\n| 5m6m-Poor   | 10.3 $\\pm$ 0.5 | 11.0 $\\pm$ 0.3 | 7.6 $\\pm$ 0.3  |\n\nWe also updated Table 1 to cover those results.\n\n**Q3: \"In the top-middle of Figure 2, why is the planned trajectory of the planning agent (purple) different from the real sampled trajectory?\"**\n\n**A3:** The plans on the top row are done at the early stage of an episode, and decisions are made in a decentralized way. Therefore, the purple agent at that time did not have much information about other agents' intentions. Then the purple agent may make conflicting plans with the red one, i.e., both want to cover the same landmark in the middle. \n\nAs the purple agent gathers more information during execution, it identifies the red agent's inclination toward the middle landmark. Thus, the purple agent's opponent U-Net model tends to generate a future trajectory for the red agent, which arrives at the middle landmark. And because of the attention mechanism, the purple agent made corrections when generating their own plans (bottom-middle subfigure). The plans of all the agents in the bottom-middle row become conflict-free, and the conflicts are eliminated in real sampled trajectories.\n\nWe have revised the explanation in Section 5.4 to make it clearer."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572375359,
                "cdate": 1700572375359,
                "tmdate": 1700572375359,
                "mdate": 1700572375359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2VHTIYaXGO",
            "forum": "mHF35XVjmm",
            "replyto": "mHF35XVjmm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Gpw8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4357/Reviewer_Gpw8"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends previous work in diffusion-based offline RL to offline cooperative MARL, and in particular to the CTDE and centralized control settings.\n\nThe main contribution is the inclusion of an attention module in the diffusion model to help integrate information from other agents during centralized training and help coordination. The method is called MADiff\n\nDuring online centralized control, an action is selected at each time step by first generating a future joint-observation sequence conditioned on the current joint-observation. An inverse dynamics model then infers the joint-action required to produce that observation sequence, and that is the joint-action taken by the agents.\n\nThe paper then evaluates MADiff in two offline MARL settings: SMAC (2 maps) and MPE (3 envs), and on the NBA trajectory prediction dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper extends the application of diffusion models to offline multi-agent RL, with a non-trivial modification to the the usual U-Net architecture to facilitate coordination. The problem setting is significant, with potential applications to sports, urban planning, traffic prediction, ecology and other fields.\n\nThe empirical evaluation is relatively diverse, and compares to multiple relevant baselines.\n\nAt a high level, the paper is also generally well structured, and it is easy to follow from one section to the next. The Preliminaries section is particularly well contained."
                },
                "weaknesses": {
                    "value": "**Clarity** \n- While the paper is well structured, the writing could benefit from spell-checking and rephrasing since it often hurts understanding. A few examples:\n  - \"more high-frequency and less smooth nature of actions\" : What does it mean for actions to be \"high-frequency\"?\n  - the diffusing process is said to condition on information $y(\\tau)$, which based on Fig. 1 includes \"Returns\" and \"Other information\". In that case, why are there multiple returns? Is it expected future discounted returns, or final returns for that trajectory? How are the returns encoded? What is the extra information?\n  - reading section 3.1, it is unclear how the attention mechanism incorporates the information of other agents, and how this is supposed to help coordination.\n  - the term \"opponent\" or \"opponent modelling\" is used repeatedly, despite the work only dealing with cooperative settings\n  - Section 5.4 as a whole is unclear to me, including exactly what the Valid Ratio is measuring.\n\n**Results**\n- In SMAC, it is common to report the win rate rather than the return, because the return is much less interpretable. I would like the authors to provide the win rates.\n- Figure 3 has no errorbars.\n- No mention of the number of seeds used for Table 1 or Figure 3.\n- The results for SMAC seem underwhelming, especially since SMAC lacks relevant stochasticity and partial observability \n\n**Experiment Limitations**\n- While the use of 2 different MARL settings and of the NBA dataset is welcome, the paper does not evaluate MADiff in settings where coordination is particularly challenging, either due to stochasticity, partial observability or both. It also does not acknowledge any potential limitations of the method in such settings. For instance, I encourage the authors to consider environments such as SMACv2 [1], Multi-Agent Mujoco [2] or Hanabi [3].\n\n\n- [1] https://arxiv.org/abs/2212.07489\n- [2] https://github.com/schroederdewitt/multiagent_mujoco\n- [3] https://arxiv.org/abs/1902.00506"
                },
                "questions": {
                    "value": "1. Why downsample the NBA dataset from 25 Hz to 5 Hz? How does this affect MADiff and the baseline?\n\n2. Have the authors tried training MADiff on combined datasets of mixed quality (e.g. Good+Medium+Poor)? I suspect the additional data would benefit MADiff the most by improving the inverse dynamics model without hurting the quality of the generated trajectories when conditioning on high return.\n\n3. The environment with the most agents is *5m6m*, which has only 5 agents. How does MADiff scale to a large number of agents?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699371105108,
            "cdate": 1699371105108,
            "tmdate": 1699636406890,
            "mdate": 1699636406890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AfIULKgLRM",
                "forum": "mHF35XVjmm",
                "replyto": "2VHTIYaXGO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thanks for your time! We have conducted additional experiments on Multi-Agent Mujoco datasets and hereby answer your questions below. Hope we can ease your concerns.\n\n**Q1: \"What does it mean for actions to be 'high-frequency'?\"**\n\n**A1:**\nWe are sorry for the confusion. The term \"frequency\" refers to using the Fourier transform to transform the state or action sequences in the time domain to the frequency domain. Action sequences are less smooth across time steps and thus have a lot of high-frequency components after transformation. \n\nTo make it clearer, we have removed the statement about action frequency.\n\n**Q2: About \"Returns\" and \"Other information\"  in Figure 1.**\n\n**A2:**\nWe are sorry for the confusion. During training, \"Returns\" in Fig. 1 is set to the cumulative discounted reward starting from time step $t$. In some environments, agents can have different returns at some time step, e.g., penalties on the specific agent that collides with others in MPE, thus we use the plural form to denote returns for each agent. During evaluation, we set returns to the same high-enough value for all agents. Before input to the U-Net residual blocks, Returns are passed through an MLP and concatenated with the diffusion time step embedding. \n\n\"Other information\" is task-related condition variables other than returns. For example, in the NBA dataset, we need player embeddings and ball positions as inputs. \"Other information\" is passed through a separate embedding network and concatenated with return embeddings. We have revised the caption of Fig. 1 to include descriptions of these two terms.\n\n**Q3: \"it is unclear how the attention mechanism incorporates the information of other agents, and how this is supposed to help coordination\"**\n\n**A3:**\nThe attention mechanism helps coordination in both centralized control and decentralized execution settings. \nWhen centralized control is allowed, the attention module enables the diffusion model to jointly plan trajectories for all agents, which is essential in tasks that require coordination.\n\nIn a decentralized execution setting, although different agents make decisions independently, they can use the same attention network to jointly infer other agents' future trajectories and plan their own trajectories. Since the attention network is trained in a centralized manner, the generated trajectories for all agents should be consistent. Intuitively, the consistency constraint requires the ego agent to first think at a higher level, i.e., make a coordinated plan for all agents, and then place itself in that plan. The coordination among agents can be improved by performing \"imagined\" planning for others (i.e., opponent modeling).\n\n**Q4: \"the term \"opponent\" or \"opponent modelling\" is used repeatedly, despite the work only dealing with cooperative settings.\"**\n\n**A4:**\nThank you for pointing this out. We use the term \"opponent\" to refer to other teammates in cooperative games, which is also adopted in some prior works [1, 2]. Since most methods in modeling adversary agents in competitive games can be used in modeling teammates, using the same term \"opponent\" can ensure better coverage of related works. \n\n**Q5: \"Section 5.4 as a whole is unclear to me, including exactly what the Valid Ratio is measuring.\"**\n\n**A5:** We are sorry for the confusion and have rewritten Section 5.4 to make it more clear.\n\n**Q6: \"In SMAC, it is common to report the win rate ... would like the authors to provide the win rates\".**\n\n**A6:** \nMost prior offline MARL works use self-collected datasets, which makes it hard to measure the progress in this field. Instead, we use the off-the-grid MARL dataset [3], which is open-sourced and released with comprehensive baseline results. For SMAC tasks, they did not report the win rates but only the scores of baseline algorithms. As a result, we do not compare the win rates of MADiff in our experiments. We want to highlight that there are some offline MARL works [4, 5] that also use the average score as the only metric for SMAC.\n\nTo make the readers better understand the magnitude of scores in SMAC, we have added Fig. 5 in the supplementary material to demonstrate the distribution of scores in each SMAC dataset we used.\n\n**Q7: Issues concerning Figure 3 and Table 1.**\n\n**A7:**\nThank you for your careful check! Results are computed with 3 different seeds. In the revised paper, we mention the number of seeds in the captions of Fig. 3 and Tab. 1. The error bar is added to Fig. 3.\n\n**Q8: \"The results for SMAC seem underwhelming, especially since SMAC lacks relevant stochasticity and partial observability.\"**\n\n**A8:**\nTo our knowledge, each agent in SMACv1 can only receive local observations within a fixed sight range [6], which makes the environment partially observable. Although SMACv1 lacks relevant stochasticity, we highlight that both MA-ICQ and MA-CQL are strong baselines in offline MARL, and MADiff achieves competitive results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571085344,
                "cdate": 1700571085344,
                "tmdate": 1700571085344,
                "mdate": 1700571085344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SRv0KsLmMS",
                "forum": "mHF35XVjmm",
                "replyto": "SgUVxi7ccy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Gpw8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Reviewer_Gpw8"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Follow-up"
                    },
                    "comment": {
                        "value": "I think their authors for their thorough response. They have addressed some of my concerns, but I still have some below.\n\n**Q2: Returns and Other information** \n\nI still find there is too little information for something potentially crucial to the performance of MADiff. Regarding returns, if I understand correctly, you mention utilizing a factorized returns for agents (e.g. collision penalty, reward for reaching the goal, etc.). This is already problematic, because splitting the reward function into different components and giving the components separately to the policy is non-standard, as it implies domain knowledge about the environment. Do you also perform this factorization for your baselines? If no, then it is a potential boost to MADiff.\n\nCrucially, the authors say _\"agents can have different returns at some time step\"_. This is true, but typically those rewards are accrued into a single global reward shared between all agents (as the paper says in section 2.1). This is why credit assignment is required in cooperative MARL and why CTDE is so hard.\n\nTo summarize, any form of reward factorization, and especially reward factorization per agent should be a) clearly disclosed in the paper and b) clearly justified, because it simplifies the CTDE problem greatly. \n\nSimilarly, any \"Other information\" provided to the method should be clearly disclosed on a per-environment basis, to make it clear whether or not the other information contains domain knowledge which gives an advantage to MADiff over the baselines. For instance, what is the extra information in SMAC?\n\n**Q4: \"Opponent modelling\"**\n\nThe definition of \"opponent\" is \"someone who competes with or opposes another in a contest, game, or argument\", so using it to refer to teammates is confusing, regardless of prior usage in the literature. This is even worse in SMAC, where there are teammates and enemies (i.e. opponents), even if the opponents are controlled by a fixed policy. I suggest the authors simply use \"teammate\" instead of \"opponent\", and maintain the related works coverage as is.\n\n**Q5: Section 5.4**\n\nThank you for rewriting. This is now clearer. However, there is a mistake in Fig. 2 (bottom left). All three trajectory predictions are from the point of view of Red agent. You do not show the predictions for the Green or Blue agent.\n\n**Q6: Win rates**\n\nEven though past work in offline MARL has omitted win rates in SMAC, I encourage the authors to add them to bridge the gap between the online and offline literatures. They should do so even if they cannot evaluate all baselines to get the win rates.\n\n**Q8: Partial observability in SMAC**\n\nYes, SMACv1 agents have a limited sight range, but that does not make it relevant partial observability. The SMACv2 demonstrates that it is possible to learn a strong policy in SMACv1 maps by conditioning *only on the timestep and nothing else*.  It is okay to have a method that only works well in environments without meaningful partial observability as it can still have many applications, but those limitations need to be explicit.\n\n**Q9: Stochasticity and Partial Observability**\n\nAs mentioned above, SMACv1 partial observability is very limited. MPE has stochasticity in the initial states, but once those are observed, the rest of the dynamics are deterministic. Furthermore, partial observability and stochasticity cannot only be considered in isolation. For a discussion on the interplay between the two, please refer to section 4 of SMACv2 [1].\n\n**Q10: Multi-agent Mujoco**\n\nI thank the authors for the additional experiment! To correctly interpret the results, however, one needs details on the settings used. I assume 2halfcheetah and 4ant refer to the 2-agent halfcheetah and 4-agent ant default configurations from the repo. Is that correct? Do you provide any \"Other information\" to the diffusion model, and if so, what? \n\nAlso, you should add the results to the revised paper.\n\nGiven my remaining concerns, I will maintain my score for now.\n\n\n[1] https://arxiv.org/pdf/2212.07489.pdf"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660695741,
                "cdate": 1700660695741,
                "tmdate": 1700660695741,
                "mdate": 1700660695741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A3oJL5rrHk",
                "forum": "mHF35XVjmm",
                "replyto": "2VHTIYaXGO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to remaining concerns cont."
                    },
                    "comment": {
                        "value": "**Q10: Multi-agent Mujoco**\n\n**A10:** We are delighted that you acknowledge our added experiments. The naming of different Multi-agent Mujoco tasks follows the off-the-grid datasets (https://github.com/instadeepai/og-marl), where 2halfcheetah and 4ant refer to the 2-agent halfcheetah and 4-agent. The environment setting is the same as the default configurations from the repo of the Multi-agent Mujoco environment (https://github.com/schroederdewitt/multiagent_mujoco). As we mentioned in **A2**, no other information is used in Multi-agent Mujoco experiments. Results on Multi-agent Mujoco are added to the revised paper, accompanying necessary descriptions.\n\n\n**References**\n\n[1] Alcorn, Michael A., and Anh Nguyen. \"baller2vec++: A look-ahead multi-entity transformer for modeling coordinated agents.\" *arXiv preprint arXiv:2104.11980* (2021).\n\n[2] Pan, Ling, et al. \"Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification.\" *International Conference on Machine Learning*. PMLR, 2022."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725541774,
                "cdate": 1700725541774,
                "tmdate": 1700731456840,
                "mdate": 1700731456840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]