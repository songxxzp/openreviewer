[
    {
        "title": "Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method"
    },
    {
        "review": {
            "id": "Wb2SQ8snYu",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_DD4A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_DD4A"
            ],
            "content": {
                "summary": {
                    "value": "The authors first propose a zero-order method with two types of gradient estimators to solve the federated learning problems in the wireless communication environment. The proposed framework ZOFL doesn\u2019t require the knowledge of channel state. Moreover, the authors prove the almost surely convergence and give the convergence rate. Finally, the authors test the performance of ZOFL with experimental results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed algorithm ZOFL is novel and interesting in my point of view. (1) Despite using the zeroth-order method in optimization problems is not a new thing, the proposed algorithm includes the channel state as a part of learning and doesn\u2019t need to analyze the channel, which has not been seen in the literature. (2) Theoretically, the proposed paper proves the almost surely convergence instead of convergence in probability by utilizing Doob\u2019s martingale inequality."
                },
                "weaknesses": {
                    "value": "Despite that the paper proposes an attractive novelty, I suspect some technical proofs have some small problems, which I list in the questions part. It is possible that I\u2019m wrong. Thus, if the authors can explain these questions, I will change my score."
                },
                "questions": {
                    "value": "(1) In the last line of Eq. (15), How E[h_{i,k+1}^2 h_{i,k}^2] be bounded by \\sigma_h^4? Shouldn\u2019t any two of them be related? Similarly, how E[h_{i,k+1}^2 \\sum_{j<l}h_{j,k}h_{l,k}] be bounded by K_{hh}^2 since j and k cannot be equal to i at the same time?\n\n(2) In the appendix C1 page 18, whey g_k and g_k\u2019 are independent if k\\neq k\u2019? since g_k includes h_k and h_{k+1}. If k\u2019=k+1, shouldn\u2019t they be related? This problem also influences the derivation of the following proofs which use Doob\u2019s martingale inequality.\n\n(3) In the Eq (27) on page 20, how can you guarantee that \\rho-\\eps-L\\sqrt{c}\\alpha_{k_l} is positive? If not, the inequality cannot be squared and used in Eq. (28) I think.\n\nBesides the above questions, this paper also has some typos, which I list here.\n\n(1)\tIn equation (11) equality (c), where does the \u20182N\u2019 come from in the second term? \n(2)\tIn equation (12), a vector \\Phi_k is missing before the Hessian matrix.\n(3)\tIn equation (15) inequality (a) where does the \u2018N\u2019 come from? Giving the assumption 2 ||\\Phi_k||^2 should be directly bounded by \\alpha_3^2. I don\u2019t think Cauchy Schwarz works here.\n(4)\t1/\\nu^2 is missing in inequality (a) on page 19 in Appendix C1.\n(5)\tIn Eq. (30) the conditional expectation notation is missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Reviewer_DD4A"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698965275232,
            "cdate": 1698965275232,
            "tmdate": 1699636650956,
            "mdate": 1699636650956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qcJJzEd0c8",
                "forum": "9Gvs64deOj",
                "replyto": "Wb2SQ8snYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive review and we truly appreciate taking the time to look into the details of the Appendices. We also highly appreciate your acknowledgement of our paper\u2019s contribution and we hope that this rebuttal clarifies and answers the raised questions.\n\nTo answer your concerns:\n\n1-\tWe apologize for the confusion in the writing of the proof. In Eq. (15), $\\mathbb{E}[h_{i,k+1}^2 h_{i,k}^2]$ is not bounded by $\\sigma_h^4$, it is equal to both terms $\\sigma_{h}^4+2K_{hh}^2$ (similarly to a multivariate normal distribution) and $\\mathbb{E}[h_{i,k+1}^2 \\sum_{j\\neq i} h_{j,k}^2]$ is equal to $(N-1)\\sigma_{h}^4$ as $ h_{i,k+1}$ and $h_{j,k}$ are independent. The term $\\mathbb{E}[h_{i,k+1}^2 \\sum_{j<l } h_{j,k}h_{l,k}]$ is equal to zero as one of $ h_{j,k}$ and $ h_{l,k}$ will always be independent of the other terms in the product and has a mean equal to zero.\n\n2-\tWe apologize for the misleading presentation of the martingale definition. In fact, $g_k$ and $g_{k\u2019}$ don\u2019t need to be independent for $k \\neq k\u2019$, we only need the stochastic errors at iterations $k$ and $k\u2019$ to be uncorrelated, not independent and this is satisfied as for every $k\u2019<k$, we can always take the law of total expectation: \n $\n\\mathbb{E}\\big[\\mathbb{E}[e_k^T e_{k\u2019}|\\mathcal{H}\\_k]\\big] = \\mathbb{E}\\big[\\mathbb{E}[e_k^T |\\mathcal{H}\\_k] e_{k\u2019} \\big] = 0.\n$\n\nTo clarify, $\\\\{ \\sum_{k=K}^{K'} \\alpha_k e_k \\\\}\\_{K'\\geq K}$    is a martingale as for all    $K\u2019\\geq K$, $X_{K\u2019}=\\sum_{k=K}^{K'} \\alpha_k e_k$ satisfies the following two conditions:\n\n(a)\t$\\mathbb{E}[ X_{K\u2019+1}|X_{K\u2019}]=\\mathbb{E}[\\alpha_{K\u2019+1} e_{K\u2019+1}+ \\sum_{k=K}^{K'} \\alpha_k e_k |\\sum_{k=K}^{K'} \\alpha_k e_k] = 0+ \\sum_{k=K}^{K'} \\alpha_k e_k = X_{K'}$\n\n(b)\t$\\mathbb{E}[\\|\\|X_{K\u2019}\\|\\|^2]<\\infty$\n\nThe proof of (b) follows from the exact same inequalities we use in Appendix C.1 page 19 without the factor $ \\frac{1}{\\nu^2} $ and it\u2019s due to the uncorrelation of the stochastic errors at different iterations and $\\sum_k \\alpha_k^2 < \\infty$.\n\n3-\tWe stated right before eq. (28) \u201cAs $\\alpha\\_{k\\_l}$ is vanishing, \nwe consider  $(k\\_l)\\_{l\\in\\mathbb{N}}$ starting from  $\\alpha\\_{k\\_l} < \\frac{\\rho - \\varepsilon}{L\\sqrt{c}}$.\u201d \n\nSo, we only consider the subsequence $(k\\_l)\\_{l\\in\\mathbb{N}}$ starting from the point \n$\\rho-\\epsilon-L\\sqrt{c}\\alpha_{k_l}>0$.\n\nWe thank you so much for pointing out the typos. Just to clarify, \n\nIn eq. (11), equality (c) should be an inequality with $\\Big(\\sum_{j=1}^{N}\\frac{h_{j,k}}{\\sigma_h^2}+n_{j,k}\\Big)^2 \\leq N \\sum_{j=1}^{N}\\Big(\\frac{h_{j,k}^2}{\\sigma_h^4}+n_{j,k}^2\\Big)$ and we should remove the scalar $2$ from $2N$. This inequality comes from the Cauchy-Schwarz for any real $a_i$:\n\n$\\Big(\\sum\\_{j=1}^{N}1\\cdot a\\_i\\Big)^2 \\leq \\Big( \\sum\\_{j=1}^{N} a\\_i^2 \\Big) \\cdot \\Big( \\sum\\_{j=1}^{N} 1^2 \\Big)  = N \\Big( \\sum\\_{j=1}^{N} a\\_i^2 \\Big) $.\n\nIn eq.(12), you are correct, thank you for pointing it out.\n\nIn eq. (15) inequality (a), N comes from Cauchy-Schwarz similar to what\u2019s explained above.\n\nOn page 19, you are correct $ \\frac{1}{\\nu^2} $ is missing from inequality (a), thank you so much for pointing it out.\n\nIn Eq. (30) the conditional expectation notation is missing, you are correct. Thank you again for pointing it out."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686419589,
                "cdate": 1700686419589,
                "tmdate": 1700686419589,
                "mdate": 1700686419589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "59RybtZoeC",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_b8PD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_b8PD"
            ],
            "content": {
                "summary": {
                    "value": "The authors have considered the federated learning problem over wireless channels. The proposed zero-order method optimizes this process over wireless channels, integrating channel characteristics directly into the algorithm. The authors have provided convergence analysis and experiments for the proposed algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to read. \n\n- The authors have provided theoretical analysis and experiments for the proposed techniques."
                },
                "weaknesses": {
                    "value": "- The motivation of the work is not clear. \n- The authors have mentioned a setting in which server and clients are communicating via wireless challenges, but the unique challenges due to this setting are not clearly mentioned. \n- Why a zeroth order method would address the challenges due to wireless communication setting is not clear. \n- The authors have motivated the use of single point estimates of gradients via mentioning that the ''settings are continuously changing over time\", but it is not cleat what settings are referred to here?\n- Also, in FL setting, the stochastic gradients a usually easily available, so what is the main motivation behind going for zeroth order optimization? \n- It seems like the authors are trying to look at the physical layer aspect of federated learning? But does it require to change the FL algorithm ?\n- The authors have mentioned about sharing the gradients with the server, but in most of the FL techniques, the core idea is to just share the models with the server, what is the motivation to share the gradients? \n- Since the authors are considering to study the impact of wireless impact on FL, it would require to consider the wireless communication settings. In the wireless channel model Hg + n, g is usually the encoded bits which we transmit. But here the authors have used directly g, should it be something like b(g) instead of g directly? \n\n- It is unclear why considering the effect of wireless channels directly in the algorithm updates would have additional benefits. On the other hand, it would add another challenge of getting to know the channel estimating for each device at each instant of transition. \n\n- How are the convergence bounds related to other results in the literature such as with FedAvg, FedProx et? \n- The analysis looks like follows directly from the existing analysis of FL techniques in the literature, what are the additional challenges?"
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Reviewer_b8PD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698989357618,
            "cdate": 1698989357618,
            "tmdate": 1699636650853,
            "mdate": 1699636650853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HgtgNGpV8Y",
                "forum": "9Gvs64deOj",
                "replyto": "59RybtZoeC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read the paper and writing a review. Before replying to the comments and questions raised by the reviewer, we will first clarify the contribution of the paper as we believe it has not been well captured by the reviewer. \n\nIn standard FL over wireless networks, the devices exchange their models or gradients (having usually high dimension) over the wireless channels. For that, the devices encode the long vector representing the model and transmit it to the server. This server has first to decode the information and then does the aggregation of the received models and then exchange the results with all devices. This encoding/decoding implies that the wireless channel coefficients are correctly estimated at each time slot (in practice it is 1ms). This consumes nonnegligible time, computation and power resources. Furthermore, the high dimension of the model requires too much communication overhead over wireless channels. The main motivation of this work is to develop an efficient modified FL method by: i) exchanging scalars instead of high dimension vectors by employing an efficient Zero order method, and ii) including the channel perturbations in the FL learning algorithm (avoiding thus channel estimation).  While there is clear interest of doing that, the difficulty of proving the convergence of the learning algorithm (that incorporates the channel perturbation) is not straightforward at all. We deal with a nonconvex setting, and zero-order method (instead of gradient in standard FL method). By using zero-order method, only a biased estimate of the gradient can be obtained (which complicates the convergence of the FL algorithm that requires exact/unbiased estimate of the gradient). In our case, in addition, the gradient biased estimate has an unbounded variance (while standard Zero order methods assume bounded variance). We proved mathematically that our proposed zero-order method converges almost surely to the correct model. Simulations results show also that the total communication overhead in our case is much less than the standard FL method. This is the main contribution of the paper, and, we invite the reviewer to check this contribution again (proofs and algorithm).  The aforementioned points, which are important to make a fair assessment of the contribution made in this paper, will be made clearer in the final paper. \n\n1-  We proposed a new algorithm for federated learning over wireless networks by i) exchanging scalars (using a zero-order method), and ii)  integrating the channel in the learning itself (allowing hence to avoid estimating the channel at each time which will  save a lot of resources). Please see our explanations above for more details.  \n\n2- The challenges are already mentioned in the \u201ccommunication bottleneck\u201d and \u201cchannel impact\u201d paragraphs. Federated learning over wireless channels constitutes a real issue in terms of uplink congestion. In addition to the uploaded optimization variable/gradient, the piloting (exchanges) needed to know the instantaneous channel state and decode properly, consumes a lot of these uplink resources. This work tries to alleviate these issues by providing an alternative to uploading the full gradient and to trying to guess the channel.\n\n3- Alongside the utility of zero-order methods in general (when explicit gradient computation may be impractical, expensive, or impossible), the main idea of using a zeroth-order method in this paper is that we found a way to construct a gradient estimate by exchanging a scalar only over the wireless medium and to include the channel in the learning itself. We have overcome the need to decode, analyze, and remove the impact of the channel, which saves a lot of communication resources in addition to computation resources. This has never been done before. \n\n4-\tWe meant the stochastic settings changing. As the loss function $ f(\\theta, S)$ is subject to a perturbation $S$, to construct a two-point gradient estimate, the stochastic conditions, i.e., the stochastic variable $S$, have to remain unchanged during both queries of the loss function, i.e., during the calculation of both $ f(\\theta + \\gamma\\Phi, S) $ and $ f(\\theta - \\gamma\\Phi, S)$. This is not always the case in practical scenarios, especially in wireless networks when the stochastic environment continuously changes at a fast rate."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687567631,
                "cdate": 1700687567631,
                "tmdate": 1700687567631,
                "mdate": 1700687567631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ULhdkOzkXG",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_eq2v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_eq2v"
            ],
            "content": {
                "summary": {
                    "value": "This study presents a framework that introduces a zero-order method with one-point and two-point gradient estimators. Unlike previous methods, it directly integrates the wireless channel into the learning algorithm, addressing non-convex FL objectives and channel complexities. The authors theoretically prove the convergence of this zero-order federated learning (ZOFL) framework. Furthermore, the authors demonstrate the convergence behavior for both one-point and two-point estimations compared to FedAvg through experiments with different binary distribution scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors propose two zero-order gradient estimators for FL, which include the noisy wireless channel in the gradient estimation.\n\nThe authors provide a theoretical analysis of their proposed estimators and prove the convergence of their FL algorithms in the non-convex setting.\n\nThe authors conduct a comparison of their proposed zero-order FL algorithm with FedAvg."
                },
                "weaknesses": {
                    "value": "In the abstract, the authors provide theoretical convergence results as a function of K as well as throught the main paper without defining what K represents. \n\nIn the introduction, the authors use \"they,\" but it is unclear to what it refers. Does it refer to all previous works or to the authors in the work they cite?\n\nIn the motivation section, specifically in the \"communication bottleneck\" paragraph, the authors discuss allowing partial participation to reduce the communication bottleneck. The authors could discuss works that optimized the partial participation to reduce the required number of communication rounds, such as power-of-choice, filfl, and divfl. Even in the experiments, they only compare to FedAvg, while several other variants show fewer communication rounds.\n\nWhile the authors focus on the non-convex setting in their theoretical analysis, it would be interesting to see the convergence rate in the (strongly) convex setting as well.\n\nI think section 2.3 should precede section 2.2 for better clarity. One needs to understand the estimators of the gradients before delving into the final algorithm.\n\nIt is very unclear why the authors only consider binary classification tasks, which seem very simple to learn. This makes it difficult to judge the quality of their proposed solution. We need to see if their proposed algorithm works well in settings with multiple classes."
                },
                "questions": {
                    "value": "Can the authors explain why it is particularly interesting to train clients in the wireless setting and include the noisy channel in their estimation rather than using wireless communication protocols to encode and decode messages (if needed) and then conducting FedAvg or variants of FedAvg? By construction of the algorithm, the clients send much less but much more frequently. Why and when is this a more interesting approach?\n\nWhy the algorithm only considers even integers k?\n\nIn section 2.3, Eq. 3 and Eq. 4 do not include the terms \"d/2*gamma\" and \"d/gamma,\" respectively, as defined in the introduction. Can the authors explain the reason for that?\n\nWhile their proposed solution is a zero-order method, why the authors did not compare to previous zero-order methods as well as other variants of FedAvg with optimized client participation. \n\nCan the authors explain why they only consider binary classification tasks in their empirical results? Will their approach perform well in scenarios with multiple classes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6050/Reviewer_eq2v"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699004710393,
            "cdate": 1699004710393,
            "tmdate": 1699636650756,
            "mdate": 1699636650756,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ozzl5kOS3X",
                "forum": "9Gvs64deOj",
                "replyto": "ULhdkOzkXG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read the paper and writing a review.\n\nIn reply to the weaknesses:\n\n1-\tThank you for bringing this to our attention. $K$ is the total number of iterations until convergence.\n\n2-\t\u201cThey\u201d refers to the authors of the previously referenced work.\n\n3-\tWe thought it would be more powerful to assess convergence performance with the ideal case. But we do provide a quantitative comparison in communication efficiency with FedZO in Appendix E.3 whose communication efficiency strategies include partial device participation and many local update steps before uploading.\n\n4-\tWe thank you for this remark. Nonconvex setting is usually more practical (especially that most of loss function in ML are not convex). We therefore concentrated this work on the hardest task, which is the nonconvex setting. The convergence analysis is usually more difficult since one cannot use the strong convex inequality in the proof. Given the limited time and page limit for a conference paper, we prefer to keep the focus on the nonconvex case. Otherwise, the analysis in the paper will be very lengthy, and will not meet the page limit of a conference.\n\n5-\tWe thank you for this advice. We thought it might clarify the steps of how the gradient estimate is constructed using the algorithm before we introduce it. We were worried that its long structure might feel confusing to the reader. Other reviewers preferred to see the algorithm even before section 2.2.\n \n6-\tThe quality of our work is heavily concentrated in its mathematical aspect and not numerical experiments. The simulations served only as a practical example to show the interested reader that the algorithms indeed work. We believe that a mathematical proof of convergence is stronger than simulation, since one can make few tests but cannot simulate all potential settings.   \n\nIn answer to the questions:\n\n1-\tThe communication bottleneck is a real issue of federated learning that can hinder its practical implementation. The delay caused by the queued uploaded information can impede the whole learning process and cause data units to be dropped in addition to causing congestion to other communication applications of the clients using the bandwidth. An important note here too is that decoding information requires estimating the channel scaling at every iteration and that can be time and resource-consuming and add to the already existing congestion. This is the interest of the alternative that we provide as it solves both these issues, scalar values consume nothing in comparison to the needed resources; The exchange is not \u201cmuch\u201d more frequent, and even if it was, the transmitted information size still scales as $O(1)$ and not as a $O(d)$ and this makes all the difference.  \nThis approach is more appealing when the dimensions of the gradient become much greater than the channel capacity. Our proposed algorithm provides a useful and efficient alternative for applications requiring online and seamless implementation. And like any other solution in research, the conditions of the problem at hand dictate the preferred method and the type of compromises to make.  \nAs a side note, coding and decoding messages in fact add noise to the transmitted and received information via quantization and channel estimation errors, so there isn\u2019t really any perfect transmission, and that is rarely considered in the design of standard algorithms.\n\n2-\tThe algorithm runs on even integers k since there are two reception steps for every k, the first reception is when the channel introduces a scaling a $h_{i,k}$ to user $i$\u2019s signal, and the second reception is when the scaling is $h_{i,k+1}$. We may run $k$ on all integers and consider the second scaling as $h_{i,k+\\frac12}$, but we prefer the notation we used in the paper.\n\n3-\tThe standard gradient estimates include the factors $\\frac{d}{2\\gamma}$ and $\\frac{d}{\\gamma}$, respectively, as their expected value is analyzed using Stoke\u2019s theorem to say that the estimator is an unbiased estimate of the gradient of a smoothed version of the objective function and thus a biased estimate of the exact gradient of the objective function. In our analysis, we use Taylor\u2019s theorem to do the analysis and show the bias and so, this is why the form differs a bit. Both forms are acceptable as long as the step sizes are properly chosen."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689919809,
                "cdate": 1700689919809,
                "tmdate": 1700689919809,
                "mdate": 1700689919809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dzNmg6WkwB",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_ufXa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_ufXa"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses zeroth-order optimization in federated learning over wireless channels with unknown channel gains. The main advantage is to reduce the communication overhead, since clients only need to transmit scalar values to the server in the case of zeroth-order optimization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The reduction of client-to-server communication by leveraging zeroth-order optimization is interesting."
                },
                "weaknesses": {
                    "value": "- The main Algorithms 1 and 2 rely on Equations (3) and (4) to provide gradient estimates. However, Equations (3) and (4) do not provide estimated gradients due to the unknown channel gain $h\\_{j,k}$ and noise $n_{j,k}$ terms. The channel gain and noise can drift the parameter update $\\Phi_k$ to arbitrary directions. Since they are unknown, the direction of the gradient remains unknown and the multiplication of $\\Phi_k$ in Equations (3) and (4) may not yield a gradient vector in the correct direction. With possibly incorrect gradient estimates, it is unclear how the algorithms can converge to the correct solution.\n- The system model multiplies the channel gains directly with the values transmitted by clients. It seems some kind of analog transmission without channel coding is considered. However, in practice, all cellular communication nowadays use digital communication with encoding, where the resulting bit error or noise will have very different mathematical expressions. The current system model and result does not seem to extend to such practical systems. It is further unclear why there is no channel considered in server-to-client communication (Line 3 in Algorithms 1 and 2).\n- The experiments are simplified and do not really have a baseline that is compared with, since FedAvg runs in an idealized setting without channel effects. The paper should compare with baselines in the same system setup (i.e., with channel effects of the same statistics). Some well-known FL algorithms with communication efficiency, such as top-k parameter compression with error feedback, should be compared with too.\n- Only very simple binary classification tasks using MNIST and FashionMNIST datasets and simple models have been considered in the experiments. It is not clear how the algorithms perform with more advanced datasets and models. \n- The writing of the paper needs significant improvement. To me, the main contributions (and especially the usefulness of such contributions) remained unclear before page 5, while pages 6-8 include mostly a list of mathematical assumptions and results with only a limited amount of explanation on what it is useful and novel."
                },
                "questions": {
                    "value": "Please try to address the concerns mentioned in Weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244297194,
            "cdate": 1699244297194,
            "tmdate": 1699636650654,
            "mdate": 1699636650654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sWTEovvkDT",
                "forum": "9Gvs64deOj",
                "replyto": "dzNmg6WkwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read the paper and writing a review. To address your concerns:\n\n1- While we appreciate your effort in reviewing our work, we respectfully don\u2019t agree with your claim especially since we provided solid mathematical proof that our estimator is a biased estimator of the gradient and that our algorithm converges. The fact that there are perturbations (channel impact, \u2026) and noises that affect the loss function, does not mean at all that one cannot obtain a biased estimation (biased and not unbiased estimation). It is important to note here that these perturbations affect directly the zero-order information (loss function) and we used the observation/concept that by perturbing the loss function in a smart way, one can obtain a biased estimation of the gradient. This is related to the research field of zero-order optimization and this is the main intuition of why the algorithm works (Flaxman et al., 2004). Of course, the presence of channels and other noises (in addition to the nonconvex setting) makes our analysis challenging. We invite the reviewer to examine the appendix for elaboration, as we prove that equations (3) and (4) are indeed biased gradient estimators using the properties of all stochastic elements involved (and we show that the bias vanishes with time), and we prove mathematically that the algorithms converge. Our paper is not just work based on intuition, it is based on mathematical grounds. We invite the reviewer to check the proofs and we will be happy to answer any comments/questions about them.  \n\n2-\tIn general, channel coding protects the bits (and thus the transmitted information) from stochasticity. Here, our work \u201cembraces\u201d this stochasticity and seamlessly includes it in the learning. This does not mean that what we\u2019re doing is not practical. Our work can be used in current systems. In fact, nowadays, more analog communication is encouraged, especially with mmWave and THz communications, we want to avoid using analog-digital converters as they are very costly. In our work, no such converters are needed, which makes our system cheaper to implement and we don\u2019t need to protect our data. Our estimators are robust to noise and perturbation. This is why what we propose is actually practical. For information, there is an increasing interest nowadays in performing ML over wireless systems by using analog communications, due in part to the reason mentioned above. We invite the reviewer to check the references [a], [b], [c], [d] provided below (even if the setting is different and they use gradient information while we use zero order method, they focus on analog communication).  \n\nIn the description of 1P-ZOFL, we explain that there\u2019s a channel considered in the downlink server-to-client communication, we just represent it within the stochastic variable $S$ for easier comprehension of our algorithm. \n\n3-\tIf we compare against algorithms with the same channel effect or with compression, the performance of the algorithm we compare against will be much worse than the idealized FedAvg. 2P-ZOFL is shown to finally converge to the same result as the idealized FedAvg and 1P-ZOFL shows a highly comparable performance. In other words, we believe the comparison we provided in the paper is stronger than what is suggested by the reviewer. However, we do provide a quantitative comparison in communication efficiency with FedZO in Appendix E.3 whose communication efficiency strategies include partial device participation and many local update steps before uploading.\n\n4-\tThe datasets we used are standard in the research to test new optimization/ learning techniques. The simulations only serve as a numerical example to show that the methods work. We believe that a mathematical proof of convergence is stronger than simulations, since one can make few tests but cannot simulate all potential settings\n\nReferences:\n\n[a] G. Zhu, J. Xu, K. Huang, and S. Cui, \"Over-the-Air Computing for Wireless Data Aggregation in Massive IoT,\" in IEEE Wireless Communications, vol. 28, no. 4, pp. 57-65, August 2021, doi: 10.1109/MWC.011.2000467.\n\n[b] G. Zhu, Y. Wang and K. Huang, \"Broadband Analog Aggregation for Low-Latency Federated Edge Learning,\" in IEEE Transactions on Wireless Communications, vol. 19, no. 1, pp. 491-506, Jan. 2020, doi: 10.1109/TWC.2019.2946245.\n\n[c] W. Fang, Z. Yu, Y. Jiang, Y. Shi, C. N. Jones and Y. Zhou, \"Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning,\" in IEEE Transactions on Signal Processing, vol. 70, pp. 5058-5073, 2022, doi: 10.1109/TSP.2022.3214122.\n\n[d] K. Yang, T. Jiang, Y. Shi and Z. Ding, \"Federated Learning via Over-the-Air Computation,\" in IEEE Transactions on Wireless Communications, vol. 19, no. 3, pp. 2022-2035, March 2020, doi: 10.1109/TWC.2019.2961673."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692426904,
                "cdate": 1700692426904,
                "tmdate": 1700692426904,
                "mdate": 1700692426904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1gndSwY6tN",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_3zxv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_3zxv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes algorithms for one-point and two-point zero-order gradient estimators for federated learning, which relies on querying function values. This is in contrast to first-order and second-order methods for federated learning. The paper claims to be the first method that involves the effects of the wireless channel without explicitly requiring the knowledge of the channel state coefficients. Finally, the paper provides theoretical and experimental evidence for convergence and provides an upper bound on the convergence rate for both their one-point and two-point estimators."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "To my knowledge, the application of zero-order one-point and two-point estimates without explicitly estimating the channel state coefficients is novel in federated learning.\n\nThe algorithm and theoretical contributions are not trivial and warrant more experimentation to determine the relative performance among other competing methods which also claims to save communication resources."
                },
                "weaknesses": {
                    "value": "The experiment set-up is questionable in comparison to existing work, especially when compared to the experiments done in the baseline FedAvg federated learning algorithm. It is not clear why the authors decided to pick only two digits \u201c0\u201d and \u201c1\u201d from the MNIST dataset and only \u201cshirts\u201d and \u201csneakers\u201d from the FashionMNIST dataset. On the other hand, the experiments done in the original FedAvg paper were done on the full multi-class datasets, such as MNIST and CIFAR-10. \n\nThe results were also only compared to FedAvg, with no comparison done against the competing methods (which also use less communication resources) cited in section 1.1.\n\nThe inconsistency in the 2 examples in Section 4 (Experimental Results) makes the discussion unconvincing without a supporting explanation or ablation study. For the first example, a logistic regression model is used, and the images were preprocessed using a lossy autoencoder, and was based on 2 class-labels from MNIST. The second example uses a different model, without compression and on a different dataset. The results would have been more convincing if the set-ups on the 2 dataset were similar. As it stands, I am unsure if the differences between the 2 examples, as illustrated by Figure 2 and 3, are due to the different dataset or the difference in compression or the different type of model used.\n\nThe experiment results and short discussion left much to be desired. It is not clear what the conclusions are from Figure 2 and Figure 3. Are the number of communications rounds the bottleneck or the number of scalar values the bottleneck? It would be better to provide a clearer discussion of the main advantage of the methods proposed in the paper."
                },
                "questions": {
                    "value": "1. Why were the experiment set-ups reduced form (only binary labels) of the MNIST and FashionMNIST? Are the results for ZOFL methods limited to experiments for binary classification tasks?\n\n1. Could the evaluation for the 2 examples (Figure 2 and Figure 3) use a similar model and preprocessing? This would help isolate the reason for the difference in performances. For instance, 2P-ZOFL in Figure 2 shows a much faster rate of convergence initially, when compared to FedAvg, but this is not the case for Figure 3. It is not clear if this means that 2P-ZOFL only converges faster in terms of communication rounds only when logistic regression is used and not when multilayer-perceptron is used.\n\n1. For clarity, can the authors provide the main metric of consideration? Are the number of scalar values more critical than the number of communication rounds in the context of the federated learning example? Perhaps it would be clearer if results on the time taken and capacity of the wireless link is provided.\n\n1. In Section 1.1, under communication bottleneck, several competing methods that save communication resources were cited. How do these methods compare to 1P-ZOFL and 2P-ZOFL?\n\n1. It seems that the proofs hold for a general class of perturbation vectors as stated in Assumption 2, beyond vectors that only consists of 2 unique values for every dimension of the vector (in Appendix E.2), which is interesting. Are there experimental results for these vectors?\n\n1. The results were also only compared to FedAvg, with no comparison done against the competing methods (which also use less communication resources) cited in section 1.1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329311555,
            "cdate": 1699329311555,
            "tmdate": 1699636650562,
            "mdate": 1699636650562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CdR5ODDYEU",
                "forum": "9Gvs64deOj",
                "replyto": "1gndSwY6tN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read the paper and writing a review. We're grateful for the thoughtful insights provided.\nWe must first note that the simulations we provide serve only as numerical examples to show that our method works. They\u2019re not the main contribution. Our main contribution is proposing a new estimator and a new method and providing mathematical grounds for this method. This is the difficult part, and we were able to prove it. We believe that a mathematical proof of convergence is stronger than simulation, since one can make few tests but cannot simulate all potential settings.\n\nTo address your concerns in the weaknesses section:\n\n1-\tThe used classes for the binary classification were chosen at random. The algorithms work equally well for all other choices of classes. We\u2019re aware that FedAvg was originally used to classify multi-class datasets. However, there\u2019s an important difference to understand when comparing zero-order methods to first-order ones and the amount of stochasticity zero-order adds to the problem. While it is possible to classify more than 2 classes and the number of classes can grow up to a certain number, the more classes there are, the more the stochasticity \u201cblurs out\u201d the precise differences in the features that characterize the classes, especially with one-point estimators that have unlimited variance. This is why it\u2019s rare to find papers applying zero-order for multi-class classification. And the fact that we were able to classify images using zero-order is already important and challenging (even with only binary choices), as the features characterizing images are generally a lot more intricate than other types of data and can easily be distorted by the stochasticity. Classifying other types of data is much easier and more fluid. This is a problem with zero-order in general and not specific to our method. We can however provide other examples of classification and non-classification problems if necessary. \n\n2-\tIn the simulations, we only mean to test the convergence performance of our algorithm. This is why we chose to compare with FedAvg, or the \u201cideal\u201d case. While other methods provide other strategies to save communication resources, their convergence speed/performance is generally worse than FedAvg. However, we do provide a quantitative comparison in communication efficiency with FedZO (alongside FedAvg) in Appendix E.3 whose communication efficiency strategies include partial device participation and many local update steps before uploading.\n\n3-\tWe thank you for bringing this point to our attention. We note again that this choice of datasets is random and we only meant to provide different objective function examples. The difference between the two results is indeed due to the type of method used to classify and not the dataset.  In our test runs, classifying images from the MNIST dataset using multilayer perceptron gave the same performance as FashionMNIST. And as can be seen from the figure, FedAvg is also affected by the method used. \n\n4-\tThe bottleneck is due to the number of scalar values (symbol) per communication round and not the number of communication rounds, as those scalar values can accumulate from one round to another, causing a major delay and impediment on the learning process itself and that\u2019s the main issue in federated learning.\n\nTo answer your questions:\n\n1-\tIndeed the results are not limited to experiments of binary classification and numerous other examples can be used. \n\n2-\t Certainly! However, with the limited time provided for the rebuttal, we weren't able to run all simulations and provide the results.\n\n3-\tAs explained above in point (4), the number of scalar values per communication round is what\u2019s critical in the context of federated learning.\n\n4-\tPlease refer to point (2) above.\n\n5-\tThe probability distribution of the direction chosen for the gradient estimate can be any symmetrical distribution. For example, Flaxman et al. (2004) use a uniformly random unit vector and Duchi et al. (2015) use a Gaussian random variable. \n\n6- Please refer to point (2) above."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693237501,
                "cdate": 1700693237501,
                "tmdate": 1700693237501,
                "mdate": 1700693237501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CAOxt31Pdg",
            "forum": "9Gvs64deOj",
            "replyto": "9Gvs64deOj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_hpcJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6050/Reviewer_hpcJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers Federated Learning over wireless channels and proposes a zero-order FL method with one-point and two-point gradient estimators. Only scalar-valued feedback from the devices to the server is considered and the effect of the wireless channel is incorporated in the learning algorithm. Theoretical results in terms of convergence guarantees are provided with some experimental evidence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strengths of the paper are the following:\n\n1) The paper considers some realistic bottlenecks of implementing FL algorithms over wireless channels, e.g., IoT applications and introduces some new ideas in zero order optimization that do not require calculation of gradients at the devices.\n\n2) With the assumptions made in the paper, the convergence analysis for the proposed 1P and 2P ZOFL algorithms seems concrete."
                },
                "weaknesses": {
                    "value": "The paper has the following weaknesses:\n\n1) The wireless channel model assumed in this work is highly simplistic. Eq. (1) refers to a \"flat fading\" channel which completely ignores the effect of multipath and inter-symbol interference (ISI) caused by it, which requires more sophisticated processing at the receiver to mitigate its effect. One cannot simply assume a simple channel model that is unrealistic in order to admit tractable analysis of FL algorithms. Furthermore, in the case of IoT devices for which FL methods are applicable, the channel conditions can vary a lot from more stationary in time to highly time-varying. \n\n2) In the 1P-ZOFL algorithm, how does the device know the value of $\\sigma_h^2$ to be able to send $1/\\sigma_h^2$ to the server? This assumes that the device has knowledge of the wireless channel for the link from itself to the server, which in practical scenarios is only possible if the base station has transmitted pilot/reference signals to the device in prior communication rounds. However, the paper does not specify this at all. Also, the terms $h_{i,k}^{DL}$ are not defined when they first appear.\n\n2) Secondly, most IoT devices (e.g., smartphones, sensors, etc.) as well as the server (which is likely co-located at the cellular base station) have multiple antennas (i.e., MIMO technology) which make estimating the wireless channel not equivalent to estimating a single real-valued scalar value. This may render some or all of the derivations regarding the convergence analysis to be inapplicable.\n\n3) The experimental evaluation is insufficient. It needs evaluations beyond binary image classification. Also, evaluation on more realistic channel models, e.g., those specified by 3GPP such as CDL or TDL models would be helpful to observe the degradation in the proposed algorithms when the assumptions on the channel do not hold."
                },
                "questions": {
                    "value": "Some comments regarding writing:\n\n1) Please refrain from using abbreviations such as \"it's\", \"there's\", etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699578204512,
            "cdate": 1699578204512,
            "tmdate": 1699636650454,
            "mdate": 1699636650454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y9B0KtrIn3",
                "forum": "9Gvs64deOj",
                "replyto": "CAOxt31Pdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6050/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read the paper and writing a review. We also appreciate the acknowledgment and understanding of our contribution. \n\nTo reply to the weaknesses section: \n\n1.\tWe used a standard statistical channel model that is widely used in wireless communications research (textbooks and thousands of IEEE papers). We kindly ask the reviewer to refer to all the references we cited (Yang et al., 2020; Amiri & G\u00fcnd\u00fcz, 2020; Sery & Cohen, 2020; Guo et al., 2021; Sery et al., 2021) and thousand other IEEE papers on wireless and cellular communications that use similar statistical channel models in their research. As an example, we also invite the reviewer to refer to the reference [*] provided below.\n\nIn addition, we do consider that the channel conditions are time-varying, we\u2019re only assuming the channel to be constant during the transmission of one (scalar) symbol. To have a general channel model, we consider that the channel is time correlated (but not constant). We believe this a realistic assumption and widely used in wireless systems. \n\nIn regards to ISI, in our case, there\u2019s no ISI. Every user transmits only one symbol per upload and waits for the server to send the aggregated model, so there\u2019s no \u201cinter-symbol\u201d interference.  After that, all users\u2019 symbols are aggregated by the server, making use of and keeping the stochasticity introduced by their channels. Furthermore, ISI is a widely studied problem in many IEEE papers and several solutions exist (guard interval, \u2026.), and we believe it does not have to be studied in an ML paper.  \n\n2.\t$\\sigma_h^2$ can be estimated beforehand and not every instant. As a side note, we use the variance $\\sigma_h^2$ just to normalize the received signal, but its presence is not necessary and does not affect the convergence of the algorithm nor the convergence rate if we were to remove it. We apologize for the confusion, $h_{i,k}^{DL}$ is the channel perturbation on the downlink server-to-user transmission signal.\n\n3.\tMIMO only changes the dimension of the channel. It\u2019s equivalent to having multiple copies of the signal, so considering MIMO antennas will induce an additional summation over all the antennas. We must also keep in mind that we\u2019re only transmitting one symbol per user. Considering MIMO antennas does not change anything about our work and contribution.\n\n4.\tThe simulations we provide are only numerical examples to show that our method works. They\u2019re not the main contribution. Our main contribution is proposing a new estimator and a new method and providing mathematical grounds for this method. This is the difficult part, and we were able to prove it. We can include other examples if necessary. \n\nFinally, we thank the reviewer for the raised remarks. We would also like to stress that focusing too much on wireless questions/details for an ML paper would somehow lead to an unfair treatment of the paper, as compared to other papers in this area. Thousands of papers don\u2019t use these channel models and our central problem is not a wireless problem. We employed standard models and issues related to the modeling itself are beyond the scope of this paper. These issues have already been studied and resolved. We hope our reply offers clarification.\n\nIn answer to the question, we thank you for pointing this out. We will fix it.\n\nReference: \n\n[*]  Emil Bj\u00f6rnson and Luca Sanguinetti, \u201cMaking Cell-Free Massive MIMO Competitive with MMSE Processing and Centralized Implementation,\u201d IEEE Transactions on Wireless Communications, vol. 19, no. 1, pp.77-90, January 2020.\n\nThis paper has received the prestigious IEEE Marconi Prize Award in Wireless Communications in 2022. We used a similar channel model, and we invite you to check equation (7), where one can see the used channel model."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693772227,
                "cdate": 1700693772227,
                "tmdate": 1700694060887,
                "mdate": 1700694060887,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]