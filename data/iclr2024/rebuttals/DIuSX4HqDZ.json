[
    {
        "title": "Abductive Logical Reasoning on Knowledge Graphs"
    },
    {
        "review": {
            "id": "rJ171TNrRe",
            "forum": "DIuSX4HqDZ",
            "replyto": "DIuSX4HqDZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the task of abductive logical knowledge graph reasoning. It proposes a generation-based method to address knowledge graph incompleteness and reasoning efficiency by generating logical hypotheses. The paper demonstrates the effectiveness of their proposed reinforcement learning from knowledge graphs (RLF-KG) to enhance the hypothesis generation model by leveraging feedback from knowledge graphs. The paper also discusses the tokenization of hypotheses and the use of reinforcement learning to optimize the hypothesis generation model. Overall, the paper aims to utilize information from knowledge graphs to find complex structured hypotheses that explain observations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The research shows that the generation-based method consistently outperforms the search-based method on three different datasets.\n- The method addresses the complexity of logical hypotheses by using a generation-based approach. This allows for the exploration of complex structured hypotheses that can explain the observations, going beyond simple correlations or patterns.\n- The method takes into account the incompleteness of the knowledge graph, which is a common challenge in reasoning tasks. By generating logical hypotheses, the method can fill in the gaps and provide explanations for the given observations."
                },
                "weaknesses": {
                    "value": "- It is inappropriate to call this task as \"abduction\". The task learns a first-order hypothesis that satisfies given groundings, and the generated hypothesis is used for generalising to more unseen groundings. Logical abduction means a grounding-to-grounding hypothesizing, for example, given P(x)->Q(x), observing Q(1), we can abduce P(1). If you want to abduce first-order theories, the you need second-order rules as background knowledge.\n- The presentation of this work can be significantly improved. The description of the proposed method is messy. For example, the caption of Figure 4 is \"step 3\", which is out of blue and makes reader confusing. Furthermore, the illustration of figures and algorithms are also confusing. There's no input and output in algorithms, and what are \"models\" and \"reference models\" in Fig. 4?\n- Fig. 5 proposes 13 types of first-order hypotheses templates, are they complete for the hypothesis space? Is there any proof to the completeness?"
                },
                "questions": {
                    "value": "Please see my above comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587216541,
            "cdate": 1698587216541,
            "tmdate": 1699636246148,
            "mdate": 1699636246148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SzFHprBLVw",
                "forum": "DIuSX4HqDZ",
                "replyto": "rJ171TNrRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We appreciate your feedback and would like to respond to your comments and questions.\n\n## Re W1:\n\nIn our formulation of abductive reasoning on knowledge graphs, given a grounded observation such as Q(1), the goal is the find a hypothesis P(x) such that the set {x | P(x) holds true} closely resembles the set 1. This means that P(1) = True. \n\nTo provide a clearer understanding, we will compare our abductive logical expression generation with inductive rule mining on knowledge graphs:\n\nRule Mining (ILP on KG):\n\nThis task involves a given knowledge graph with some positive examples regarding the relation $speak$. The goal is to find Horn clause rules associated with the $speak$ relation.\n\n$speaks(X, Y) \\\\leftarrow lives(X, A) \\\\land language(A, Y)$\n\nThe variables in the rules are universally quantified, and the quality of the rules is measured by head coverage and precision. \n\nOur task:\n\nWe use a real example from FB15K-237. Our observations include the following entities:\n\n$Grant Heslov, Jason Segel, Robert Towne, Ronald Bass, Rashida Jones$\n\nOur task aims to find a logical expression based on the KG to describe these entities, rather than discovering general rules over the KG. These entities make the following logical expressions true:\n\n$V_?: Occupation(V_?, Actor) \\\\land Occupation(V_?, Screen Writer) \\\\land BornIn(V_?, Los Angeles)$\n\nThis means that Grant Heslov, Jason Segel, Robert Towne, Ronald Bass, and Rashida Jones are actors and screenwriters born in Los Angeles. This logical expression is not a rule, as not all entities from the KG satisfy the logical expression.\n\nUnlike inductive rule mining, which is not grounded, our proposed reasoning task is grounded, focusing on specific entities.\n\n## Re W2: \n\nWe have updated the description of our proposed method, RLF-KG, in Section 3 to improve clarity. The entire process is divided into three steps.\n \nStep 1 involves preparing the training samples and converting the raw data into a format that is suitable for a transformer-based model.\n \nIn Step 2, a generative model is trained in a supervised manner. It is important to note that the resulting model from Step 2 is standalone and can be used to generate hypotheses given observations, although its performance may be relatively low.\n \nThe primary contribution of our method is Step 3, which fine-tunes the trained model via PPO with feedback from the training KG. This step is critical in improving the model's ability to reason over the KG and generate more accurate and complex hypotheses.\nIn regard to the \"model\" and \"reference model,\" both are initialized to be the trained model resulting from Step 2 (which we refer to as the \"original model\"). During Step 3, the \"model\" is optimized, while the \"reference model\" remains fixed. The \"reference model\" is used in the PPO process to prevent excessive divergence between the fine-tuned \"model\" and the \"original model.\"\nThe \"reference model\" is crucial in ensuring that the fine-tuned \"model\" produces syntactically correct hypotheses similar to the \"original model.\" Without limiting divergence, the training process may become difficult to converge.\nOur quantitative experiments (presented in Table 3, Sec. 4.4) demonstrated the effectiveness of Step 3, namely RLF-KG. Figure 6 illustrates the growth in reward during the PPO process, further supporting the effectiveness of our proposed method.\n \nIn addition to the quantitative results, we provide qualitative examples in Appendix G to showcase the improvement in the generated hypotheses for the same observation. These examples illustrate how RLF-KG improves the model's ability to reason over the KG and generate more accurate and complex hypotheses.\n\n## Re W3:\n\nIn line with previous works on KG reasoning [1][2], we utilize similar hypothesis templates as those used in these studies. Due to the exponential increase in the number of logical expression templates with the number of variables, we are only able to evaluate a subset of them in our experiments. However, the selected expression templates are considered empirically significant based on their use in various previous works on complex query answering [1][2].\n \n[1] Query2box: Reasoning over knowledge graphs in vector space using box embeddings.\n\n[2] Beta embeddings for multi-hop logical reasoning in knowledge graphs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033075568,
                "cdate": 1700033075568,
                "tmdate": 1700033209050,
                "mdate": 1700033209050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ny9ZMobP3L",
                "forum": "DIuSX4HqDZ",
                "replyto": "SzFHprBLVw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their clarifications in __Re W2__, however, the example you gave in __Re W1__ is an induction, since it learns a first-order rule that has a set of answers. And the empirical results in __Re W3__ cannot guarantee completeness, which think is fairly easy to prove, for example:\n- A. Cropper and S.H. Muggleton. Logical minimisation of meta-rules within meta-interpretive learning. In Proceedings of the 24th International Conference on Inductive Logic Programming, pages 65-78. Springer-Verlag, 2015. LNAI 9046.\n\nTherefore, I think this work needs significant improvement before published, so I will keep my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327742184,
                "cdate": 1700327742184,
                "tmdate": 1700327742184,
                "mdate": 1700327742184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L8Sx4wE9Zc",
                "forum": "DIuSX4HqDZ",
                "replyto": "MTQibXxpaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_xMZe"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for the reply. The first-order rules induced from a set of groundings can be regarded as abduction, as long as you use second-order rules as background knowledge, like your templates in the paper. It is a bit vague to discriminate induction, \nabduction and even deduction  philosophically, actually all of them can be implemented by deduction (i.e. resolution) \nin higher order logic, therefore it would be better to constrain the discussion within the context of mathematical logic, especially predicate logic."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371536555,
                "cdate": 1700371536555,
                "tmdate": 1700371536555,
                "mdate": 1700371536555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oA6opw1Ag5",
            "forum": "DIuSX4HqDZ",
            "replyto": "DIuSX4HqDZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_jBpD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_jBpD"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores an important problem of abductive logical reasoning  (ALR) over Knowledge Graphs (KG), discusses a practical approach to implementing ALR over KGs, and compares different possible implementations of the proposed approach to each other.  ALR over KG is defined as inferring the most probable logic hypothesis from the KGs to explain an observed entity set, and a generative approach is presented for creating logical expressions based on entity set observations.  Algorithms are presented for (1) sampling hypothesis-observation pairs from a KG under the open-world assumption and (2) implementing reinforcement training from KG feedback.  Additional transformation algorithms (token-to-graph and graph-to-token) that assist in the sampling process are presented.  The authors demonstrate the relative effectiveness of each sub-component of their overall method on three well know datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The authors take on an important problem in knowledge graph reasoning, namely the abduction of plausible rules that could result in an observation set if applied to the KG.  A reasonable baseline algorithm is described and implemented for providing such a capability.  \n\nQuality:  All aspects of the paper appear to be technically correct, and variations of the method are experimentally explored across three KGs.  Authors documentation on all aspects of the techniques are sufficient for reproducibility.  \n\nClarity:  The main effort of the work exceptionally well described, providing good motivation, clear examples of what is being sought, how the algorithms work, and the various experiments performed.\n\nSignificance:  This is exploration is a valuable contribution to the literature."
                },
                "weaknesses": {
                    "value": "Originality: The paper could benefit from a more precise statement of their objective problem, which would then allow for a clearer discussion of how the current effort contrasts with past and future work.\n\nQuality: The objective of the approach is not precisely defined, so it is difficult to discern whether or not any particular intent was achieved.  No future research was discussed.\n\nClarity: covered by the first two items in this section.\n\nSignificance:  I feel that this is valuable, but could be more impactfully significant with a more precise framing of this approach and alternate formulations of abduction."
                },
                "questions": {
                    "value": "1.  The paper criticizes previous approaches without quantitatively comparing them to the current method.  While this may be computationally infeasible on the graphs chosen here, do you have such results you could include -- even if only on very small graphs that highlight and justify your statements?\n\n2. There are many possible formal definitions for abduction, and you choose two slightly different versions in your discussion (one in the abstract, and one in Section 2).  Can you provide a precise statement of what you mean by abduction, rather than proceed by analogy?  For instance, the last paragraph of section 2 mentions \"best explanation\" without providing a quotative definition of best.   Can you connect such precision to your actual implementation?  For instance a precise definition might be made to which your algorithm is an approximation, or a precise definition might be supplied for which your algorithm is an exact solution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Privacy, security and safety"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Reviewer_jBpD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699131339850,
            "cdate": 1699131339850,
            "tmdate": 1699636246061,
            "mdate": 1699636246061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9UZ9RTh4Tu",
                "forum": "DIuSX4HqDZ",
                "replyto": "oA6opw1Ag5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review! We would like to make the following clarifications.\n\n## Re W1: \n\nWe revised and clarified the problem definition in section 2, which involves using abductive reasoning to find a hypothesis (H) that matches an observation set $O = \\\\{v_1, v_2, ..., v_k\\\\}$ as closely as possible. The objective of abductive reasoning is formalized in equation (4), which uses the Jaccard score to measure the similarity between the observations and the corresponding conclusions drawn from the generated conclusions on the hidden graph. \n\n\n## Re W2:\n\nPlease refer to Re:Q1 and Re:Q2 for the objectives of the problem and the approach. In addition, we further discuss the future work on these directions. One possible direction is to explore the use of more advanced reinforcement learning algorithms to improve the performance of the model. Another direction is to investigate the use of larger and more complex knowledge graphs. Our current experiments were conducted on relatively small graphs with 10k up to 100k vertices. It would be interesting to see how the model performs on larger graphs, as well as graphs with more complex structures and relationships.\n\n\n## Re W4: \n\nWe have revised the description of our approach in Section 3.2 to provide further clarity. Please refer to Re: Q2 for details.  \n\n## Re Q1: \n\nOur proposed KG reasoning task is unique and novel, which means there are no previous methods capable of generating complex first-order logical expressions to explain specific observations. In Table 5 and 6, we compare our method with pure symbolic methods, such as brute-force searching the one-hop neighbors of the observations. However, brute-force searching is very slow on graphs with at least 10k vertices, and it struggles to perform well when the hypothesis is complex and structured, involving multiple logical connectives and variables.\n \nIn Appendix G Case Studies, we provide qualitative examples that compare the hypotheses produced by the search-based method, the generation-based method, and the generation-based method after applying RLF-KG.\n\n## Re Q2: \n\nWe want to clarify that the abductive reasoning objectives mentioned in the abstract and at the end of Section 2 are the same. We formally define the \"best\" explanation using formula (4) at the end of Section 2, which is represented by the Jaccard similarity between the conclusion of the generated hypothesis on the testing graph (which was not observed during training) and the observation.\n \nIt is crucial to note that the reward function for the PPO is chosen to approximate the objective by using the Jaccard similarity between the conclusion of the generated hypothesis on the training graph (the observed graph) and the observation. Furthermore, the primary metric used in our experiments is consistent with the task's objective."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033513936,
                "cdate": 1700033513936,
                "tmdate": 1700033513936,
                "mdate": 1700033513936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rejp5tQmwv",
                "forum": "DIuSX4HqDZ",
                "replyto": "9UZ9RTh4Tu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_jBpD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_jBpD"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their clarifications.  I believe this work represents an important step in addressing abductive reasoning, and that that the authors and other research groups can build upon in the future.  I will keep the Rating of 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324345046,
                "cdate": 1700324345046,
                "tmdate": 1700324345046,
                "mdate": 1700324345046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iscIE4G67v",
            "forum": "DIuSX4HqDZ",
            "replyto": "DIuSX4HqDZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_dX8J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_dX8J"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on abductive reasoning with KGs and proposes a generative approach using supervised training to create logical expressions based on observations.   It further improves explanations by minimizing differences between observations and conclusions.      Experimental results show transformer-based models generate robust and efficient logical explanations, achieving state-of-the-art results on abductive reasoning with KGs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This article provides an analysis of the difficulties associated with abductive reasoning in knowledge graphs (KGs) and proposes a generation-based approach to address these challenges. The overall structure of the article is well-organized, and the ideas are presented clearly. Additionally, the experimental findings are objective and cover a wide range of aspects."
                },
                "weaknesses": {
                    "value": "1. This article lacks a clear formalization of the explanation for abductive reasoning. \n2. The motivation for using reinforcement learning algorithms is questionable.\n3. See Questions."
                },
                "questions": {
                    "value": "1. Are the Encoder-Decoder and Decoder-only models used in the experimental section both based on the fundamental transformer architecture?    Does the author consider the substitution of more powerful backbone models?\n2. The effectiveness of the PPO algorithm is not experimentally demonstrated in this paper.     In the context of the \"abductive reasoning for KG\" task, please explain why the PPO algorithm was chosen over other alternatives or how it specifically provides advantages.\n3.  What are the reference models in Fig. 4? Why the method needs it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3014/Reviewer_dX8J"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699416726982,
            "cdate": 1699416726982,
            "tmdate": 1700639837994,
            "mdate": 1700639837994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wqhi0xSP8Z",
                "forum": "DIuSX4HqDZ",
                "replyto": "iscIE4G67v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review, we would like to make the following clarifications on your comments.\n\nRe W1: \n\nWe revised and clarified the problem definition in section 2, which involves using abductive reasoning to find a hypothesis (H) that matches an observation set $O = \\{v_1, v_2, ..., v_k\\}$ as closely as possible. The objective of abductive reasoning is formalized in equation (4), which uses the Jaccard score to measure the similarity between the observations and the corresponding conclusions drawn from the generated conclusions on the hidden graph.\n\nRe W2: \n\nDuring supervised training, the model only learns to minimize the difference between the generated and sample hypothesis sequences. However, this doesn't guarantee that the conclusion of the generated hypothesis on the testing graph will be closer to the observation, which is our ultimate objective. Additionally, this supervision signal doesn't make full use of the knowledge graph's valuable information.\n \nTo address this, we implemented a PPO reward that uses the Jaccard similarity between the conclusion of the generated hypothesis on the training graph and the observation. We believe this reward provides logical information that can improve the quality of generated hypotheses by approximating our task's objective directly.\n \nThe positive results in our experiments in Section 4.4 further support the value of RLF-KG in improving the quality of hypotheses generated through abductive reasoning.\n\n\nRe Q1: \n\nWe confirm that both the encoder-decoder and decoder-only models we used were based on the Transformer architecture. Our main focus in this paper is to introduce the abductive reasoning task and demonstrate how the RLF-KG framework improves performance on top of a supervised trained model. Our intention is to highlight the benefits of incorporating RLF-KG into the abductive reasoning process, rather than simply comparing neural models to search-based models. While more powerful models could be used as baselines, our primary objective remains to emphasize the performance improvements achieved through the application of RLF-KG.\n\nRe Q2: \n\nIn Section 4.4, we present our experimental results, which include a comparison of model performance before and after applying PPO. Table 3 shows that RLF-KG consistently improves hypothesis generation performance across three widely used datasets, using the same metric as the task's objective.\n \nOur goal is to overcome the limitations of supervised training by incorporating information from the knowledge graph. PPO is ideal for this task because it allows us to use the reward from the knowledge graph as feedback, which improves the quality of generated hypotheses by utilizing the available information. As we mentioned in our response to W2, this approach compensates for the limitations of supervised training. \n\nWe chose PPO because it's a proven RL method [1] for transformer-based models.  Although there are newer methods like DPO [2] that use preference information, they're not directly applicable to our knowledge feedback tuning.\n\n\nRe Q3: \n\nThe reference model is a copy of the model obtained from supervised training, and the model being fine-tuned by PPO starts off identical to the reference model. The reference model plays a crucial role in preventing excessive divergence between itself and the fine-tuned model. The KL term in the PPO objective measures this divergence and penalizes it to ensure that the models remain aligned.\n\n[1] Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744.\n\n[2] Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. \"Direct preference optimization: Your language model is secretly a reward model.\" arXiv preprint arXiv:2305.18290 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700031905749,
                "cdate": 1700031905749,
                "tmdate": 1700031905749,
                "mdate": 1700031905749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FMcO1LLlJh",
                "forum": "DIuSX4HqDZ",
                "replyto": "wqhi0xSP8Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_dX8J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_dX8J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the author for clarifying the problems, I think the author solved my confusion very well, so I raise the rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639930246,
                "cdate": 1700639930246,
                "tmdate": 1700639930246,
                "mdate": 1700639930246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JZLTG16kLR",
            "forum": "DIuSX4HqDZ",
            "replyto": "DIuSX4HqDZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_yA9N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_yA9N"
            ],
            "content": {
                "summary": {
                    "value": "This paper approaches the problem of abductive rule generation by proposing a supervised sequence-to-sequence model which is then fine-tuned via RL.\n\nThe goal of the paper is to, given entities, produce logical forms that describe those entities. In particular, the logical hypotheses are executed on a knowledge graph. The resulting entities are compared via Jaccard similarity to the original observations. The hypotheses themselves can be evaluated against ground-truth hypotheses using SMATCH.\n\nThe method is evaluated on 3 existing knowledge graphs. The results show that RLF-KG consistently outperforms the supervised baseline in all settings. Additionally, the proposed generation-based approach is much faster at inference than search-based methods, while having similar entity accuracy but better structural similarity to the ground truth logical forms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach proposed for abductive reasoning is reasonable and was demonstrated experimentally to be accurate and fast.\nThe task and approach seem original, and the writing is relatively clear.\nHowever, I am not certain about its significance."
                },
                "weaknesses": {
                    "value": "The paper proposes the task of abductive logical reasoning and claims it is under-studied, but I am not convinced the task is important.\nThis could be improved by linking to applications, or by showing that abductive logical reasoning is a weakness in popular reasoning methods such as those that use large language models."
                },
                "questions": {
                    "value": "1. I found quite a few typos and grammatical errors, but those did not hamper my understanding of the paper.\n2. Can you move the results of the search baseline to the main table, table 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699421499026,
            "cdate": 1699421499026,
            "tmdate": 1699636245898,
            "mdate": 1699636245898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ugfGNfvPSw",
                "forum": "DIuSX4HqDZ",
                "replyto": "JZLTG16kLR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We would like to respond to your comments and questions.\n\nRe W1:\n\nAbductive logical reasoning on knowledge graphs remains important because current language models, such as LLMs, struggle to reason over factual knowledge. For example, when prompted with the task of generating a logical hypothesis that matches a given observation set, ChatGPT (GPT-3.5) generated a hypothesis that claimed the devices were all first-generation Apple products, despite the observation set including an iPod touch 4th Gen. This demonstrates the model's inability to perform simple abductive reasoning.\n\nHere is the example:  \nInput: \u201cPlease propose a logical hypothesis H(x) such that the set of all entities that make H(x) true should be as close to O as possible: O = {Ipad 1st Gen, Ipod touch 4th Gen, Apple TV 1st Gen}\u201d (the 2nd example in Fig. 1)\n\nOutput: \u201cCertainly! To create a logical hypothesis H(x) that is as close to the given set O = {iPad 1st Gen, iPod touch 4th Gen, Apple TV 1st Gen} as possible, you might consider the following hypothesis: \n\nH(x): The entities in set  x  are first-generation electronic devices produced by Apple.\u201d\n\nHowever, in our framework, the model is trained with feedback from the observed knowledge graph. If the knowledge graph contains basic facts about Apple's products and the model generates a faulty hypothesis, the discrepancy between the observation set and the conclusion of the hypothesis will result in a relatively low reward. This encourages the model to correct the hypothesis and improves its ability to reason over factual knowledge.\n\n\nQ1: We have fixed some typos and improved the presentation.\n\nQ2: We have moved the results of the search baseline to the main table."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032541308,
                "cdate": 1700032541308,
                "tmdate": 1700032541308,
                "mdate": 1700032541308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xdNyhr5ejo",
                "forum": "DIuSX4HqDZ",
                "replyto": "ugfGNfvPSw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_yA9N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Reviewer_yA9N"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. To be clear, I think the methodology and results are solid, but I really want to be sold on the problem in the introduction.\n\nConcretely, this means I would like the following question answered in the introduction: Why is abductive reasoning important? Starting the story from cognitive science, causal inference, or the scientific method could help. One example that's probably too broad: if we want machines that can form generalizable and explainable theories about the world from observations, they need to be able to perform abductive reasoning [1].\n\n[1] Schickore, Jutta, \"Scientific Discovery\", The Stanford Encyclopedia of Philosophy (Winter 2022 Edition), Edward N. Zalta & Uri Nodelman (eds.)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200948911,
                "cdate": 1700200948911,
                "tmdate": 1700200948911,
                "mdate": 1700200948911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hvXDpYpsgI",
            "forum": "DIuSX4HqDZ",
            "replyto": "DIuSX4HqDZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_9m6p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3014/Reviewer_9m6p"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a data-driven approach is proposed for learning logic hypothesis based on observations and background knowledge from knowledge graphs. The key idea lies in tokenizing the hypotheses and learning the generative model from observations to hypotheses. Furthermore, reinforcement learning is utilized to allow training under the reward function about whether the observations can indeed be inferred from the learned hypotheses. Experimental results on benchmark datasets verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies a very meaningful problem: learning logical hypotheses from knowledge graphs.\n\n- The paper proposes some interesting ideas, such as tokenizing of the logical hypotheses, as well as the design of the reward function in reinforcement learning.\n\n- The experimental results show that the proposed method indeed works."
                },
                "weaknesses": {
                    "value": "- In my view, the term of abductive  reasoning is incorrectly used. The process of obtaining hypothesis based on observations and background knowledge is called induction. Abductive reasoning further requires to obtain groundings for variables in the hypothesis. \n\n- The paper misses citation to researches on inductive logic programming (ILP), which is closely related to the problem studied in the paper. An ILP task involves learning logic programs based on logic observations and background knowledge, which is more general than reasoning on knowledge graphs. Furthermore, the term abductive learning has also be proposed before in the ILP area [1]. Related citations should be included and discussed in the paper.\n\n- The experimental results are only quantitative. It would nicer to illustrate some qualitative examples on what kinds of hypotheses can be learned by the proposed method. For example, it would be useful to illustrate whether the proposed approach can learn lengthy hypotheses with significant complexity.\n\n[1] Bridging Machine Learning and Logical Reasoning by Abductive Learning, NeurIPS'19."
                },
                "questions": {
                    "value": "- On the bottom of Page 3, it is said that the logic clauses contains only three operations $\\cup, \\cap, \\neg$. Does this mean that the paper only considers a limited subset of first-order logic? (e.g. propositional logic). Even though this would be enough for knowledge graphs, which can only represent relational information, clarifying the scope of learning is still necessary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699542589042,
            "cdate": 1699542589042,
            "tmdate": 1699636245815,
            "mdate": 1699636245815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CVk9rfGIQ0",
                "forum": "DIuSX4HqDZ",
                "replyto": "hvXDpYpsgI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3014/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We appreciate your feedback and would like to provide some clarifications regarding your comments.\n\n## Re W1:\n\nWe would like to clarify that our task differs from inductive logical programming (ILP). This is because ILP-induced logical hypotheses are general rules, as they are not grounded. In contrast, our task involves grounded logical hypotheses, which specifically describe observations rather than expressing rules. In the knowledge graph domain, inductive reasoning is equivalent to rule mining [1,2,3], discussed in Section 5.\n\nHere are some examples to illustrate the differences:\n\nRule Mining (ILP on KG):\n\nThe task involves a given knowledge graph with some positive examples regarding the relation $speak$. The goal is to find horn clause rules related to the $speak$ relation.\n$speaks(X, Y) \\\\leftarrow lives(X, A) \\\\land language(A, Y)$\nThe variables in the rules are universally quantified, and the quality of the rules is measured by head coverage and precision. However, in our task, we use a real example from FB15K-237. Our observations include the following entities:\n\n$ Grant Heslov, Jason Segel, Robert Towne, Ronald Bass, Rashida Jones $\n\nOur task aims to find a logical expression based on the KG to describe these entities, rather than discovering general rules over the KG. These entities make the following logical expressions true:\n\n$V_?: Occupation(V_?, Actor) \\\\land Occupation(V_?, Screen Writer) \\\\land BornIn(V_?, Los Angeles)$\n\nThis means that Grant Heslov, Jason Segel, Robert Towne, Ronald Bass, and Rashida Jones are actors and screenwriters born in Los Angeles. This logical expression is not a rule, as not all entities from the KG satisfy the logical expression.\n\nTherefore, our problem setting is distinct from inductive logical programming over knowledge graphs, specifically rule-mining.\n\n## Re W2:\n\nWe previously explained the differences between our proposed abductive KG reasoning task and ILP. Additionally, our problem setting significantly differs from the abductive learning suggested in [4]. The study in [4] uses abductive learning (ABL) to form symbolic representations via learning methods and then applies Prolog's abductive logic programming to address hand-written puzzles. These symbolic representations may not be logical expressions, and Prolog's abductive logic programming can only evaluate their truth or falsity, without generating complex first-order structured hypotheses.\n\n## Re W3:\n\nIn the appendix, we have provided several concrete examples in Tables 12, 13, and 14. These tables contain the observations, logical expressions, and their corresponding truth values. As demonstrated in the tables, our proposed method can generate logical expressions that are appropriately long, involve multiple logical connectives, and are precise (as indicated by high Jaccard scores) in describing the observations.\n\n## Re Q1: \n\nAs presented in Equations 1 and 2, we focus on a specific form of first-order logical expressions, which includes existential quantified intermediate variables and logical connectives AND/OR/NOT. We have provided further clarification regarding the scope in Section 2 of the paper.\n\n\n[1] Christian Meilicke, Melisachew Wudage Chekol, Daniel Ruffinelli, and Heiner Stuckenschmidt.\nAnytime bottom-up rule learning for knowledge graph completion. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,\nMacao, China, August 10-16, 2019, pp. 3137\u20133143. ijcai.org, 2019. doi: 10.24963/ijcai.2019/\n435. URL https://doi.org/10.24963/ijcai.2019/435.\n\n[2] Vinh Thinh Ho, Daria Stepanova, Mohamed H. Gad-Elrab, Evgeny Kharlamov, and Gerhard\nWeikum. Rule learning from knowledge graphs guided by embedding models. In Denny\nVrandecic, Kalina Bontcheva, Mari Carmen Suarez-Figueroa, Valentina Presutti, Irene Celino, \u00b4\nMarta Sabou, Lucie-Aimee Kaffee, and Elena Simperl (eds.), \u00b4 The Semantic Web - ISWC\n2018 - 17th International Semantic Web Conference, Monterey, CA, USA, October 8-12,\n2018, Proceedings, Part I, volume 11136 of Lecture Notes in Computer Science, pp. 72\u201390.\nSpringer, 2018. doi: 10.1007/978-3-030-00671-6\\ 5. URL https://doi.org/10.1007/\n978-3-030-00671-6_5.\n\n[3] Luis Galarraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. Fast rule mining in \u00b4\nontological knowledge bases with AMIE+. VLDB J., 24(6):707\u2013730, 2015. doi: 10.1007/\ns00778-015-0394-1. URL https://doi.org/10.1007/s00778-015-0394-1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033378648,
                "cdate": 1700033378648,
                "tmdate": 1700033378648,
                "mdate": 1700033378648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]