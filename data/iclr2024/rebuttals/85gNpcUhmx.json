[
    {
        "title": "Context-Aware Unsupervised Domain Adaptive Lane Detection"
    },
    {
        "review": {
            "id": "rULkKWUfle",
            "forum": "85gNpcUhmx",
            "replyto": "85gNpcUhmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_cVkk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_cVkk"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an unsupervised domain adaptive approach for the lane detection task by utilizing a teacher-student framework and contrastive learning paradigm. Firstly, the method utilizes a positive sample memory module to retain domain-related lane features. Next, unlike existing methods, the authors propose cross-domain contrastive learning to improve feature discrimination. Additionally, a domain-level feature aggregation method is introduced to combine domain-level features with pixel-level features. The proposed unsupervised domain adaptive algorithm achieves the state-of-the-art performance on the TuLane, MuLane, and MoLane datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Idea seems fundamentally sound, paper is well written.\n2. Pixel-level feature aggregation module considers highly uncertain pixel of background class, which makes sense for me.\n3. Experiementation results are provided to support the proposed method."
                },
                "weaknesses": {
                    "value": "1. The experimentation settings in the paper are confusing as it is unclear which dataset is used as the source domain and which one is used as the target domain. This lack of clarity hinders proper understanding and evaluation of the proposed approach.\n2. The paper fails to fully explore publicly available datasets from different domains. It lacks experimentation on datasets such as CuLane and OpenLane, which limits the generalizability and thoroughness of the findings.\n3. The algorithm presented in the paper is only applicable to segmentation-based lane detection methods. This limitation reduces its potential contribution since most of today's algorithms tend to be either transformer-based or keypoint-based, making the proposed approach less relevant to current state-of-the-art techniques."
                },
                "questions": {
                    "value": "While incorporating UBP into the feature aggregation module seems reasonable, the extent to which it contributes to the overall performance remains unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1720/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1720/Reviewer_cVkk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698049321543,
            "cdate": 1698049321543,
            "tmdate": 1699636100900,
            "mdate": 1699636100900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UEtYB7L8Jo",
                "forum": "85gNpcUhmx",
                "replyto": "rULkKWUfle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cVkk"
                    },
                    "comment": {
                        "value": "**Q1: The experimentation settings in the paper are confusing as it is unclear which dataset is used as the source domain and which one is used as the target domain. This lack of clarity hinders proper understanding and evaluation of the proposed approach.**\n\nREPLIES: We fully agree with the reviewers, and in response to this constructive comment, we have added a detailed description of the datasets used for the dataset source and target domains in Appendix A1.\n\n**Q2: The paper fails to fully explore publicly available datasets from different domains. It lacks experimentation on datasets such as CuLane and OpenLane, which limits the generalizability and thoroughness of the findings.**\n\nREPLIES: We totally agree with the reviewer. Following this constructive comment, we have supplemented the crossover experiments using different domain datasets to fully demonstrate the generalizability and performance of our method, including the transfer experiments between CULane and OpenLane. The experimental results are shown in Table 4 of Section 4.3 and Table A7 of Appendix A5. DACCA demonstrates superior performance compared to existing methods across all metrics, particularly when transferring from the CULane dataset to the Openlane dataset. It further confirms the generalization capability of our method.\n\n**Q3: The algorithm presented in the paper is only applicable to segmentation-based lane detection methods. This limitation reduces its potential contribution since most of today's algorithms tend to be either transformer-based or keypoint-based, making the proposed approach less relevant to current state-of-the-art techniques.**\n\nREPLIES: Here we would like to make further clarification. As pointed out in the review, in this work we mainly use a segmentation-based approach to illustrate the whole pipeline of the method, but our method has the potential to be a generalized domain adaptive framework for lane detection. Firstly, the anchor, positive sample, and negative sample in contrastive learning are not limited to pixels, but can be keypoints and queries in a transformer. Secondly, the DFA module is essentially a plug-and-play feature aggregation module. Our future work will validate the effectiveness and advantages of our approach by applying it to other lane line detection methods. We have pointed out it in the conclusion section.\n\n**Q4: While incorporating UBP into the feature aggregation module seems reasonable, the extent to which it contributes to the overall performance remains unclear.**\n\nREPLIES: We fully agree with the reviewer's comments. To highlight the contribution of UBP to the overall performance, we have supplemented the corresponding experiments in Section 4.2. The results given in Table 1 show that the feature aggregation of UBP can effectively improve Accuracy by 1.56%, reduce FP by 2.26% and FN by 2.79%. It also indicates that our method can effectively correct the features of UBP and thus improve the performance of the model."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638113069,
                "cdate": 1700638113069,
                "tmdate": 1700638113069,
                "mdate": 1700638113069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U2gLpxnCdA",
            "forum": "85gNpcUhmx",
            "replyto": "85gNpcUhmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_1nVJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_1nVJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Context-aware Unsupervised Domain-Adaptive Lane Detection (CUDALD) to improve feature discrimination and cross-domain knowledge transferring in the domain-adaptive lane detection field. To this end, the authors introduce two key components: cross-domain contrastive loss and domain-level feature aggregation. The former aims to effectively distinguish feature representations among categories while the latter combines domain-level and pixel-level features to enhance cross-domain context dependency. Extensive experiments on TuLane, MuLane and MoLane datasets verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Aggregating the contextual information from the whole domain for pixel feature enhancement is logically sound and interesting. Despite its simplicity, this concept yields significant performance improvements compared to current state-of-the-art baselines.\n- The authors conducted comprehensive comparison and ablation experiments on a wide range of benchmark datasets, confirming the effectiveness of their proposed approach.\n- This paper is well-structured and well-explained. The method is accompanied by sufficient details and illustrative diagrams, such as Figures 1 and 2. \n- Extensive visualization results and qualitative comparisons, e.g. Figures 4 and 5, further highlight the advantages of the proposed approach."
                },
                "weaknesses": {
                    "value": "- Limited originality in cross-domain contrastive learning. This component heavily relies on the previous work (Wang et al., 2021; 2023) and provides an increment improvement (a positive sample selection). It doesn\u2019t strike me as particularly novel. \n\n- Combinatorial contribution. Each contribution addresses a specific issue in the unsupervised domain-adaptive field. The entire paper lacks content coherence. \n\n- The title fails to effectively represent the contributions of the paper. 'Context' is a broad concept.\nHere, the authors primarily propose aggregating domain-level context for pixel-level feature enhancement. Therefore, it is best to highlight this aspect."
                },
                "questions": {
                    "value": "Please see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762788291,
            "cdate": 1698762788291,
            "tmdate": 1699636100811,
            "mdate": 1699636100811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2H9gApCn4U",
                "forum": "85gNpcUhmx",
                "replyto": "U2gLpxnCdA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1nVJ"
                    },
                    "comment": {
                        "value": "**Q1: Limited originality in cross-domain contrastive learning. This component heavily relies on the previous work (Wang et al., 2021; 2023) and provides an increment improvement (a positive sample selection). It doesn\u2019t strike me as particularly novel.**\n\nREPLIES: We would like to further explain the originality of our work, and expect further kind and careful evaluation from the reviewer. The existing works (Wang et al., 2021; 2023) mainly focus on improving the form of contrastive loss, while the core of our proposed cross-domain contrastive loss is in the selection of samples without any changes to the contrastive loss. That is, we believe that the selection of samples in contrast learning is equally critical, so we re-select the anchor, positive sample, and negative sample involved in contrastive loss, thereby achieving better results.\n\n**Q2: Combinatorial contribution. Each contribution addresses a specific issue in the unsupervised domain-adaptive field. The entire paper lacks content coherence.**\n\nREPLIES: As the reviewer will note, two components, cross-domain contrastive loss and domain-level feature aggregation (DFA), of our approach are complementary and indispensable, not simply combined. The former is responsible for aligning features between different domains, and the latter focuses more on discriminative features inter-categories within a domain. The results of the ablation experiments show that the models are underperforming without either component, as shown in Figures A3 and A4. We sincerely hope the reviewers to give further consideration and re-evaluation to our approach.\n\n**Q3: The title fails to effectively represent the contributions of the paper. 'Context' is a broad concept. Here, the authors primarily propose aggregating domain-level context for pixel-level feature enhancement. Therefore, it is best to highlight this aspect.**\n\nREPLIES: We appreciate the keen insight of the reviewer, and have revised the title of the paper to \u201cUnsupervised Domain Adaptive Lane Detection via Contextual Contrast and Aggregation\u201d, which we believe can highlight the contributions of this work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637952907,
                "cdate": 1700637952907,
                "tmdate": 1700637952907,
                "mdate": 1700637952907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jJX2oVdp4E",
            "forum": "85gNpcUhmx",
            "replyto": "85gNpcUhmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_YuT7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_YuT7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes that ineffectively learning discriminative features and transferring knowledge across domains are two crucial issues in domain-adaptive lane detection task, and introduces a method named context-aware unsupervised domain-adaptive lane detection which is consisted of cross-domain contrastive loss and domain-level feature aggregation to tackle these issues."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe incorporation of contrastive loss and feature aggregation strategies, as evidenced by the experimental results, appears to be a valuable addition to the paper.\n2.\tThe experiment is sufficient."
                },
                "weaknesses": {
                    "value": "1.\tLack of inspiration. While the article focuses on unsupervised domain-adaptive lane detection, it makes me confused whether its proposed issues, i.e., ineffectively learning discriminative features and transferring knowledge across domains could be applied to all unsupervised domain-adaptive segmentation tasks. It prompts consideration of whether the issues discussed are universal or specific to this particular domain. Additionally, the introduction of contrastive loss and feature aggregation, while useful, may not fully justify the article's contribution, as these methods are well-established in domain-adaptation field.\n2.\tThe quality of writing and diagrams in the paper requires improvement. Several definitions provided in the article lack clarity, hindering reader comprehension. For instance, in section 3.2, the term 'anchor' is introduced without a precise definition. Additionally, the role of modules is not adequately explained, such as how the introduction of PSMM ensures the appropriate assignment of positive samples. Furthermore, the figures lack detail and refinement, and the overall layout, as seen in Figure 2, does not appear to be carefully designed.\n3.\tSome conclusions of the article are exaggerated. For example, in section 4.3, the authors claim that their method performs significantly better than other methods when using ERFNet as the detection model, but choosing PyCDA (Accuracy/%: 86.73) and MLDA(Accuracy/%: 88.43) to compare instead of SGPCS with the accuracy of 89.28%."
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699029261339,
            "cdate": 1699029261339,
            "tmdate": 1699636100721,
            "mdate": 1699636100721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PDf8vxVpXn",
                "forum": "85gNpcUhmx",
                "replyto": "jJX2oVdp4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YuT7"
                    },
                    "comment": {
                        "value": "**Q1: Lack of inspiration. While the article focuses on unsupervised domain-adaptive lane detection, it makes me confused whether its proposed issues, i.e., ineffectively learning discriminative features and transferring knowledge across domains could be applied to all unsupervised domain-adaptive segmentation tasks. It prompts consideration of whether the issues discussed are universal or specific to this particular domain. Additionally, the introduction of contrastive loss and feature aggregation, while useful, may not fully justify the article's contribution, as these methods are well-established in the domain-adaptation field.**\n\nREPLIES: We appreciate the reviewer\u2019s expertise and insight. As the reviewer will observe, upon our redistilling, the contribution and inspiration of this paper lies in the selection of samples in contrast learning and the aggregation of effective features. It helps to improve feature discriminative and efficient knowledge transfer across domains in unsupervised semantic segmentation domain adaptive tasks. First, ineffectively learning discriminative features and transferring knowledge across domains are significant challenges encountered in all unsupervised semantic segmentation domain adaptation tasks, including lane detection. Second, although cross-domain contrastive loss has been widely used in unsupervised semantic segmentation domain adaptation tasks, the existing methods focus on the loss function. In contrast, we propose a novel sampling strategy to fully utilize the potential of contrastive loss without modifying the existing contrastive loss. Third, for feature aggregation, to our best knowledge, all existing methods currently perform feature aggregation within a mini-batch. We take a new perspective, i.e., feature fusion using domain-level features, instead of feature aggregation on a mini-batch or an image alone. As Figure A3 in the Appendix demonstrates, domain-level features can better align features between different domains. \n\nThey are the main contributions of our approach, which we believe will give, to some extent, inspiration to the study of sample selection in contrastive learning and how to select effective features for feature aggregation. We sincerely hope the reviewer to re-evaluate it.\n\n**Q2: The quality of writing and diagrams in the paper requires improvement. Several definitions provided in the article lack clarity, hindering reader comprehension. For instance, in section 3.2, the term 'anchor' is introduced without a precise definition. Additionally, the role of modules is not adequately explained, such as how the introduction of PSMM ensures the appropriate assignment of positive samples. Furthermore, the figures lack detail and refinement, and the overall layout, as seen in Figure 2, does not appear to be carefully designed.**\n\nREPLIES: As the reviewer will note, we have done our best to improve the quality of the writing and images in the revision, including pointing out that the anchor is the pixel feature being compared, i.e., the candidate pixel, the feature representation in PSMM is more accurate than pixel-level feature representation, providing more positive signals in the contrastive loss, as well as the improvement of the appearance of each module in the Student-Teacher model structure, as shown in Figure 2.\n\n**Some conclusions of the article are exaggerated. For example, in section 4.3, the authors claim that their method performs significantly better than other methods when using ERFNet as the detection model, but choosing PyCDA (Accuracy/%: 86.73) and MLDA(Accuracy/%: 88.43) to compare instead of SGPCS with the accuracy of 89.28%.**\n\nREPLIES: We totally agree with the reviewer. We have revised and clarified the inaccurate expressions in this manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637617951,
                "cdate": 1700637617951,
                "tmdate": 1700739283950,
                "mdate": 1700739283950,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cGYJN3cTiv",
            "forum": "85gNpcUhmx",
            "replyto": "85gNpcUhmx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_ZmgM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1720/Reviewer_ZmgM"
            ],
            "content": {
                "summary": {
                    "value": "In this work, author target the domain adaption in lane line detection, identifying two specific problems in prior work and proposes corresponding solutions. Firstly, prior work only focus on pixel level feature and ignore inter-lane feature presentation. Secondly, prior work don\u2019t adopt cross domain representation for adoption. Authors propose Context-aware Unsupervised Domain-Adaptive Lane Detection (CUDALD) framework, including a contrastive learning function by redesign the sampling method, and a pixel-domain feature aggregation model to enrich representation. Authors shows that the proposed method achieved SOTA on multiple dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Author have targeted a very specific, often ignored problem, domain adaption for lane line detection, which is a critical problem in modern ADAS. \n2. Author identify two problems in prior work adopting solution from a general domain(eg, domain adaption for segmentation), and propose solution specific designed for laneline domain adaption, showing improvement over prior art."
                },
                "weaknesses": {
                    "value": "1. There\u2019s limited discussion, comparison and analysis over prior work that also target specifically on laneline domain adaption, for instance, MLDA(Li et al., 2022). The statement in introduction that 'Unfortunately, achieving cross-domain context dependency in domain-adaptive lane detection remains unexplored.' is also less accurate as prior works like MLDA already explored this domain.\n2. Author only conduct experiment on TuLane, MuLane, and MoLane, three simulated lane datasets proposed from one single work, the result would be more convincing if authors could conduct domain transfer experiments between Tusimple and Culane, which are more commonly used dataset for the community. \n3. This work is a bit hard to follow, for instance, in 3.3, author use \u2018features from a whole domain is more beneficial\u2019 as a core argument over prior work, without clearly pointing to \u2018what is whole domain and how does the method align with whole domain\u2019 in the following implementation. The implementation is also hard to follow without clearly define \u2018lane feature and domain feature\u2019 in the first place. \n4. The conclusion of \u2018with more advanced lane detection methods, e.g., anchor-based methods\u2019 is also lack of context. Firstly, in the related work, author stated that \u2018In this paper, we consider segmentation-based domain-adaptive lane detection.\u2019, which contradict with the conclusion. Secondly, author hasn\u2019t provide why context why \u2018author based\u2019 method is superiority over other method\u2019. \n5. Author mentioned the term PSMMs for multiple time in the introduction and related work, without explicitly explain its functionality. It would be easier to follow if author could briefly explain its functionality earlier in the paper. \n6. Please also include proper citation of work for table4/5, and the last two page of the paper."
                },
                "questions": {
                    "value": "it's still less clear to me what the motivation and implementation of cross-domain contrastive loss. It seems like the only change author proposed is a novel sampling method, please make this clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1720/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1720/Reviewer_ZmgM",
                        "ICLR.cc/2024/Conference/Submission1720/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1720/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699145484536,
            "cdate": 1699145484536,
            "tmdate": 1700691788964,
            "mdate": 1700691788964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1YeYAq6tQI",
                "forum": "85gNpcUhmx",
                "replyto": "cGYJN3cTiv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1720/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZmgM"
                    },
                    "comment": {
                        "value": "**Q1: There\u2019s limited discussion, comparison and analysis over prior work that also target specifically on laneline domain adaption, for instance, MLDA(Li et al., 2022). The statement in introduction that 'Unfortunately, achieving cross-domain context dependency in domain-adaptive lane detection remains unexplored.' is also less accurate as prior works like MLDA already explored this domain.**\n\nREPLIES: Following the reviewer's comment, we have added a review of methods such as MLDA in the introduction section and supplemented a comparison and discussion with MLDA in the experimental section (Sect. 4.3). At the same time, we have revised and clarified the inaccuracies in Introduction.\n\n**Q2: Author only conduct experiment on TuLane, MuLane, and MoLane, three simulated lane datasets proposed from one single work, the result would be more convincing if authors could conduct domain transfer experiments between Tusimple and Culane, which are more commonly used dataset for the community.**\n\nREPLIES: Following this constructive comment. In this revision, we have supplemented the domain transfer experiments between CULane and Tusimple, detailed in Appendix A.5.\n\n**Q3: This work is a bit hard to follow, for instance, in 3.3, author use \u2018features from a whole domain is more beneficial\u2019 as a core argument over prior work, without clearly pointing to \u2018what is whole domain and how does the method align with whole domain\u2019 in the following implementation. The implementation is also hard to follow without clearly define \u2018lane feature and domain feature\u2019 in the first place.**\n\nREPLIES: As the reviewer will note, we have made our best to clarify and explain some confusing concepts and terms. Specifically, we carefully define lane features and domain features in this revision. The former refers to pixel-level features of a lane, while the latter refers to all lane-line pixel features in the source and target domains. In addition, whole domain features refer to the pixel-level features of lanes in all images in a domain. Since DFA directly fuses domain features into individual lane line pixel features, it enables the model to learn domain-level features better and thus better align with the whole domain.\n\n**Q4: The conclusion of \u2018with more advanced lane detection methods, e.g., anchor-based methods\u2019 is also lack of context. Firstly, in the related work, author stated that \u2018In this paper, we consider segmentation-based domain-adaptive lane detection.\u2019, which contradict with the conclusion. Secondly, author hasn\u2019t provide why context why \u2018author based\u2019 method is superiority over other method\u2019.**\n\nREPLIES: As pointed out in this review comment, the expression in our conclusions was inappropriate and has been revised. We plan to apply our framework to other types of lane detection methods, e.g., anchor-based, and transformer-based approaches, to verify its generality in the future.\n\n**Q5: Author mentioned the term PSMMs for multiple time in the introduction and related work, without explicitly explain its functionality. It would be easier to follow if author could briefly explain its functionality earlier in the paper.**\n\nREPLIES: Following this constructive suggestion, we have added to Introduction a description of the functionality of PSMMs, i.e., saving domain features of each line.\n\n**Q6: Please also include proper citation of work for table4/5, and the last two page of the paper.**\n\nREPLIES: We have cited proper work for Tables 4, A8, and A9 in the revision.\n\n**Q7: it's still less clear to me what the motivation and implementation of cross-domain contrastive loss. It seems like the only change author proposed is a novel sampling method, please make this clear.**\n\nREPLIES: The motivation for proposing cross-domain contrastive loss is to achieve high effectiveness in learning discriminative features. As demonstrated in Figure A4, cross-domain contrastive loss has solved the problem that cross-domain feature dependency only aligns inter-domain features but does not discriminate well inter-category features without modifying the loss function. We have proposed a new sampling policy to implement the cross-domain contrastive loss. In contrastive learning, sample selection is very important, while existing works in (Wang et al., 2021, 2023), only consider positive sample selection but disregard the selection of anchor and negative samples."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637484720,
                "cdate": 1700637484720,
                "tmdate": 1700637484720,
                "mdate": 1700637484720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gPWsR9eUTa",
                "forum": "85gNpcUhmx",
                "replyto": "1YeYAq6tQI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1720/Reviewer_ZmgM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1720/Reviewer_ZmgM"
                ],
                "content": {
                    "title": {
                        "value": "Update Soundness to 3 and my score to 6."
                    },
                    "comment": {
                        "value": "In general I am satisfied author's reply for addressing many concerns, and see how author's revision make the work toward a good direction. \nAs some other reviewer also pointed out, the experiment settings is confusing, especially for people not work in this field. I suggest author to include a section of 'related work' on prior work working on 'domain adaption of lane line'( (Garnett et al., 2020), (Gebele et al., 2022) and MLDA) to provide some background for this domain. The focus of this work should be 'adopting domain adaption to lane detection', rather than 'proposing a novel general domain adaption method'. Thus I feel like author should focus more on explaining the field you're targeting(aka, domain adaption on lane detection, and what is the problem of applying general domain adaption to this field, and what problem does prior work exist(like MLDA only use single frame information as mentioned in draft, etc)). \n\nThis work will be in much a better shape after author reflecting promised changed in point 4, and revised clarity for point 4 and 7. Please also consider moving experiment of Tusimple to main paper as it reflect experiment on one of the most widely used dataset. \n\nI will update Soundness to 3 and my score to 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1720/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691768900,
                "cdate": 1700691768900,
                "tmdate": 1700691768900,
                "mdate": 1700691768900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]