[
    {
        "title": "Tight Rates in Supervised Outlier Transfer Learning"
    },
    {
        "review": {
            "id": "kIf8Z90x15",
            "forum": "nUBLhhVM1l",
            "replyto": "nUBLhhVM1l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_kTQ3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_kTQ3"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the outlier transfer problem, that is, the problem of transfer learning under the setting of outlier detection or rare class classification. The objective of the Neyman-Pearson classification problem, which formalizes the rare class classification problem, is to achieve low classification error on the rare class, while keeping the classification error on the common class under a threshold. However, in practice, we usually have only limited amount of or even none data from the target rare class, but some data from a related source rare class. This is where transfer learning comes into play. The goal of this paper is to theoretically understand when and how the knowledge from a source class can improve the classification performance on a target class under the setting of rare class classification.\n\nThe authors first show that at the population level, under certain assumptions, all the solutions to the source Neyman-Pearson classification problem are also solutions to the target Neyman-Pearson classification problem. Then the authors turn their attention to the finite-sample setting. The authors first define the outlier transfer exponent, which is a notion of discrepancy between source and target under a hypothesis class. With that discrepancy, the authors give a minimax lower bound on the target-excess error, which measures the difference between the expected error of the solution obtained by transfer learning and of the optimal solution. Furthermore, the authors propose an algorithm that does not need any prior knowledge of the discrepancy between the source and target class distributions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studied an important practical problem."
                },
                "weaknesses": {
                    "value": "(1) A lower bound on the target-excess error is not as informative as an upper bound. Is it possible to derive an upper bound on the target-excess error under appropriate conditions?\n\n(2) The algorithm proposed in Section 4.8 requires as input the VC dimension of the hypothesis class. However, in practice, the exact VC dimension may be unknown. Could you please give some practical suggestions on using this algorithm when the exact VC dimension is unknown?\n\n(3) The notation in inequality (4.1) is a little redundant. Since $h_{S, \\alpha}^*$ is a solution to the source problem, the difference between the expected error of any $h$ in the hypothesis class and of $h_{S, \\alpha}^*$ w.r.t. the source distribution must be non-negative. So, there is no need to use the max function.\n\n (4) The theoretical results mainly rely on the previous techniques.\n\n(5) There is no experiment."
                },
                "questions": {
                    "value": "There are several typos, including:\n\n(1) Page 4, in the 5th line in Section 3, the source and target problem are denoted by the same notation.\n\n(2) Page 4, in the last line, the LHS and RHS of the second to last inequality are the same.\n\n(3) Page 6, in the 5th line in Section 4.3, $n_S$ should be $n_T$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Reviewer_kTQ3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735133595,
            "cdate": 1698735133595,
            "tmdate": 1699636699168,
            "mdate": 1699636699168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b6yBXZcmqg",
                "forum": "nUBLhhVM1l",
                "replyto": "kIf8Z90x15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. A lower bound on the target-excess error is not as informative as an upper bound. Is it possible to derive an upper bound on the target-excess error under appropriate conditions?\n\nIt seems that there might have been a misunderstanding on the reviewer's side: we actually provide matching upper-bound (Theorem 4.6 in Section 4.8), and even more so, show that the bound can be achieved adaptively, i.e., with no prior knowledge of the discrepancy between distributions. Together with the lower-bound, our results therefore establish information-theoretic limits for the problem. We emphasize that the lower-bound in fact holds very generally, against any hypothesis class (of VC $\\geq$ 3), and any value of the discrepancy (transfer-exponent) and therefore tightly captures the problem. \n\n2. The algorithm requires knowledge of the VC dimension.\n\nWe first note that the assumption of known VC is common in theoretical works and in fact we know very few results where such an assumption is not made. This is because, the exact VC or an upper-bound thereof is known for many of the common hypothesis classes encountered in the theoretical literature, e.g., linear to polynomial classes, classification trees, fixed architecture Neural networks, etc. \n\nHowever, we agree with the reviewer that in practice, we might not know the tightest upper-bound for a given choice of models, and theory works on uniform concentration then prescribes the use of notions such as Empirical Rademacher Complexity (ERC) which may be estimable in principle (but often not practical). All our upper bounds hold also under ERC (this is evident in our concentration arguments), and we will make sure to emphasize this. \n\n3. The notation in inequality (4.1) is a little redundant. Since $h^*_{S,\\alpha}$ is a solution to the source problem... there is no need to use the max function.\n\nIt appears that the reviewer must have missed a nuance in the definition; the notation is not actually redundant: $h^*_{S,\\alpha}$ is the best classifier among $\\mathcal{H}_{\\alpha} = \\\\{h\\in \\mathcal{H}: R_0(h)\\leq \\alpha\\\\}\\$. \n\nHowever, in the definition, $h$ belongs to a larger class, namely $ \\mathcal{H}_{\\alpha+r} =\\\\{h\\in \\mathcal{H}: R_0(h)\\leq \\alpha+r\\\\}$. Hence, the difference of the risks can be negative.\n\n4. The theoretical results mainly rely on the previous techniques.\n\nNeyman-Pearson classification, as discussed in the first part of the paper, is inherently different from traditional classification. Already, as argued in the first set of results, widely differing distributions may admit the same optimal classifiers (see Section 3). \nMore generally, while some of the tools may appear similar, the analysis requires fundamentally different approaches. For instance a main difficulty arises from the fact that excess risks could be negative as explained above, due to the fact that a learner may operate on a different subset $\\mathcal{H}_{a+r}$ of $\\mathcal{H}$ since it can only estimate $\\mathcal{H}_a$ from data.\n\n5. There is no experiment.\n\nWe refer the reviewer to our answer to a similar question by Reviewer 3fJQ (question 2). Namely, we do not claim to propose a new procedure. Rather the procedure is of a theoretical nature and mainly serves to establish that the lower-bound of Theorem 4.4 is attainable in principle, even without prior knowledge of distributional conditions. We re-emphasize that our main aim in this work is theoretical, and is to shed some needed insights on the limits of performance in outlier transfer. \n\n\nQuestions:\n\nThere are several typos:\n\nThanks for bringing the typos to our attention. We fixed them and uploaded a revised version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105933475,
                "cdate": 1700105933475,
                "tmdate": 1700105933475,
                "mdate": 1700105933475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P6gbbHdH1E",
                "forum": "nUBLhhVM1l",
                "replyto": "b6yBXZcmqg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6346/Reviewer_kTQ3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6346/Reviewer_kTQ3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for your response, and I will consider it in the next discussion stage."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700763522,
                "cdate": 1700700763522,
                "tmdate": 1700700763522,
                "mdate": 1700700763522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bqgWuDXLDr",
            "forum": "nUBLhhVM1l",
            "replyto": "nUBLhhVM1l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_3fJQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_3fJQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a rigorous theoretical analysis of transfer learning in outlier detection. It first considers a simplified setting in which the optimal outlier classifier is the same between source and target distributions to illustrate how outlier detection differs from standard classification. The paper then addresses the much more difficult setting in which the outlier classifiers could differ, proposing an adaptive algorithm with a theoretical guarantee."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is very well-presented.\n- The theory is compelling and elegant.\n- I think that the \"same optimal classifier\" setting between source and target distributions seems unrealistic (e.g., the setting of Figure 1) but I can see why from a theoretical standpoint, analyzing this simpler setting is a good starting point and already there are interesting insights, especially in contrasting this outlier setup to traditional classification.\n- The extension of the transfer exponent to the outlier setting is a valuable contribution."
                },
                "weaknesses": {
                    "value": "- As far as I can tell, this paper does not actually follow the ICLR LaTeX template. For instance, the margins don't appear correct? Please fix this.\n- There are no numerical experiments. I think this paper would improve dramatically with experimental results, especially on real data, and especially on showing how well the adaptive method in Section 4.8 works in practice.\n- Detailed discussion of how applied researchers address this outlier transfer problem in practice would be helpful to provide some point of reference (even if these existing approaches lack guarantees): for instance, even getting a rough understanding of whether there are common conceptual ideas used would be helpful or if actually the methods are just completely different (if so, maybe some discussion of what the key conceptual differences are would be helpful)."
                },
                "questions": {
                    "value": "See \"weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807177606,
            "cdate": 1698807177606,
            "tmdate": 1699636699036,
            "mdate": 1699636699036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KPekuzjzgQ",
                "forum": "nUBLhhVM1l",
                "replyto": "bqgWuDXLDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. This paper does not actually follow the ICLR LaTeX template. For instance, the margins don't appear correct? \n\nThank you for pointing this out; we had not noticed at first that one of the packages we had included unfortunately interacted with the ICLR margin. We have now corrected that and uploaded a revised version. \n\n2. I think this paper would improve dramatically with experimental results ... especially on showing how well the adaptive method in Section 4.8 works in practice.\n\nThe adaptive method is in fact of a theoretical nature and not practical at the moment since it relies on 0-1 risk minimization. It only serves to support our theoretical findings, namely that, even without knowledge of the discrepancy between distributions, it is possible in principle to achieve nearly the same optimal rate as an oracle. As a first theoretical work on the subject of outlier transfer, our aim is to yield such initial insights towards developing practical procedures with similar guarantees, e.g., by relying on properly designed surrogate losses for N-P classification. We do hope however that the reviewer will still appreciate the solid foundation we are laying towards such a practical goal. Our main aims at this point are theoretical---i.e., understanding achievable rates in this under-studied transfer setting---and we therefore never claimed to be proposing an algorithm at this point. We will do our best to clarify this further. \n\n3. Detailed discussion of how applied researchers address this outlier transfer problem in practice would be helpful ...\n\nVarious heuristics have been proposed so far, most of which boiling down to the simplest idea of combining both samples and hoping for the best; as such very little is understood at the moment, which motivates our work. We agree with you that such discussion will really enhance the paper, and we will follow your suggestion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700097171573,
                "cdate": 1700097171573,
                "tmdate": 1700097171573,
                "mdate": 1700097171573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3mz5oDu1Eg",
            "forum": "nUBLhhVM1l",
            "replyto": "nUBLhhVM1l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_Ag52"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6346/Reviewer_Ag52"
            ],
            "content": {
                "summary": {
                    "value": "The paper adopts the traditional framework of Neyman-Pearson classification to formalize supervised outlier detection of transfer learning. The added assumption is that one has access to some related but\nimperfect outlier data. The authors first determine the information-theoretic limits of the problem. Next, they also show that, in principle, these information-theoretic limits are achievable by adaptive procedures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The outlier detection in transfer learning is an interesting and valuable topic in the learning community.\n\n2. The literature part is very clear.\n\n3. The structure of the paper is easy to follow.\n\n4. The setup of the paper is clear\n\n5. The paper provided solid theoretic results on the minimax bounds and rates."
                },
                "weaknesses": {
                    "value": "1. Only finite-sample results are provided. There is no further analysis of asymptotic properties on the large dataset."
                },
                "questions": {
                    "value": "1. If the size is large, will the results have special asymptotic properties?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6346/Reviewer_Ag52"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842491726,
            "cdate": 1698842491726,
            "tmdate": 1700719401702,
            "mdate": 1700719401702,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2PWjlkroxN",
                "forum": "nUBLhhVM1l",
                "replyto": "3mz5oDu1Eg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Only finite-sample results are provided... will the results have special asymptotic properties?\n\n Thank you for the very nuanced question. Unfortunately, besides the fact that our finite-sample results imply consistency, in fact, strong consistency (since success probabilities are exponential), we did not consider other notions of asymptotic convergence, so it is unclear to us at this point whether any special asymptotic phenomena may arise."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700094890973,
                "cdate": 1700094890973,
                "tmdate": 1700095483950,
                "mdate": 1700095483950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QzhgUbHyl4",
                "forum": "nUBLhhVM1l",
                "replyto": "2PWjlkroxN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6346/Reviewer_Ag52"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6346/Reviewer_Ag52"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I'm confused about the claim of \"strong consistency (since success probabilities are exponential)\" in the main results in Theorem 4.4 and 4.6. As no asymptotic property (especially convergence rate) is provided, it would limit the theoretical contribution. \n\nIn addition, I have also read other reviewers' comments and the author's rebuttal. The first comment from the Reviewer kTQ3 and reply from the authors discussed the upper and lower bounds. I found that the upper bound is not tight. There is a large gap between the lower and upper bound. This gap might also limit the theoretical contribution. \n\nIn all, I lowered the confidence score to 4."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719344512,
                "cdate": 1700719344512,
                "tmdate": 1700719344512,
                "mdate": 1700719344512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]