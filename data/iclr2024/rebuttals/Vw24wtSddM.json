[
    {
        "title": "Tree Cross Attention"
    },
    {
        "review": {
            "id": "kYq70q0i78",
            "forum": "Vw24wtSddM",
            "replyto": "Vw24wtSddM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_rpWY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_rpWY"
            ],
            "content": {
                "summary": {
                    "value": "1. The authors introduce a new module called Tree Cross Attention (TCA) that reduces the number of tokens required for efficient inference while maintaining comparable performance to Cross Attention. \n\nTCA organizes data in a tree structure and performs a tree search at inference time to retrieve relevant tokens for prediction. Specifically, TCA only retrieves information from a logarithmic O(log(N)) number of tokens for performing inference, while Cross Attention scans the full set of O(N) tokens. \n\n2. The authors also present ReTreever, a flexible architecture for token-efficient inference that incorporates TCA. \n\n3. Empirically, the paper demonstrates the effectiveness of TCA and ReTreever on various classification and uncertainty regression tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and straightforward to understand.\n\n2. The empirical results that the authors show are impressive. Specifically, the authors show that Perceiver IO's performance drops significantly as the length of the sequence increases, while ReTreever is able to maintain high accuracy across a range of sequence lengths.\n\n3. The proposed method, ReTreever, is able to perform token-efficient inference while achieving better performance than Perceiver IO for the same number of tokens. ReTreever does this by using Tree Cross Attention to retrieve the necessary tokens, only needing a logarithmic number of tokens log(N) << N, making it efficient regardless of the encoder used."
                },
                "weaknesses": {
                    "value": "1. In the evaluation, the authors focus on %tokens metric. How does that translate to wall clock speed up? Or does the tree structure introduce operations that are hard to take advantage of by modern hardware?"
                },
                "questions": {
                    "value": "1. My main question regarding the evaluation is in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698264549162,
            "cdate": 1698264549162,
            "tmdate": 1699636147413,
            "mdate": 1699636147413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u5gfjv8biR",
                "forum": "Vw24wtSddM",
                "replyto": "kYq70q0i78",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rpWY"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the comments and helpful feedback.\n\n> In the evaluation, the authors focus on % tokens metric. How does that translate to wall clock speed up? \n\nThe % tokens do not directly translate to wall clock time. In this work, we focused on % tokens as the metric since it is (1) neither hardware nor implementation specific and (2) % tokens translates towards memory which is a major limiting factor scalability, particularly for (i) attention-based models which are memory intensive and (ii) low-memory devices which are popular (e.g., IoT devices). In contrast, wall clock time (runtime) is highly dependent on the efficiency of the implementation and the hardware. Nonetheless, in the table below, we compare the wall-clock time of the modules used during inference by ReTreever, Transformer+Cross Attention, and Perceiver IO, i.e., Tree Cross Attention, Cross Attention, and Perceiver IO's CA respectively.\n\nWe evaluate the modules on the Copy Task for both a GPU and CPU. Although Tree Cross Attention is slower, all methods only require milliseconds to perform inference. Both Tree Cross Attention and Cross Attention learn to solve the task perfectly. However, Tree Cross Attention uses only $3.5$% of the tokens. As such, Tree Cross Attention trades off between the number of tokens and computational time. Perceiver IO, however, fails to solve the task for the same number of tokens. \n\n|         Model        | % Tokens |     Accuracy    | CPU time (in Milliseconds) | GPU Time (in Milliseconds) |\n|:--------------------:|:--------:|:---------------:|:--------------------------:|:--------------------------:|\n|    Cross Attention   |  100.0%  | **100.0 \u00b1 0.0** |            12.05           |            1.61            |\n| Tree Cross Attention | **3.5%** | **100.0 \u00b1 0.0** |            19.31           |            9.09            |\n|   Perceiver IO's CA  | **3.5%** |    13.4 \u00b1 0.2   |          **10.98**         |          **1.51**          |\n\n\n\n> Or does the tree structure introduce operations that are hard to take advantage of by modern hardware?\n\n\nThe tree approach has a sequential aspect to it related to the height. However, the major aspects of it can still be parallelized on GPUs. For example, during the tree construction, left and right subtrees can be constructed at the same time since they do not share children nodes. Aggregation can also be performed at the same time for nodes at the same depth. Our implementation leverages parallelization in some aspects of the tree approach, but we have not utilized full-parallelization which can lead to further code optimization and runtime efficiency. \n\nWe will be sharing our implementation alongside the camera-ready. We hope it will help improve the efficiency of future implementations of ReTreever and Tree Cross Attention."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372530831,
                "cdate": 1700372530831,
                "tmdate": 1700374762042,
                "mdate": 1700374762042,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WkyYgWXIPD",
            "forum": "Vw24wtSddM",
            "replyto": "Vw24wtSddM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a tree cross attention (TCA) architecture to effectively encode context into a tree structure when performing attention, such that the amount of attended tokens will be less than the actual number of context tokens. The architecture is co-trained with the objective to effectively retrieve context through graph-based searching, and utilize context for performing the task. Experiments on a specific set of tasks (i.e., copy task, gp regression on image completion and time series on human activities) show that TCA is able to achieve a similar performance as full cross attention while attending to much fewer tokens. \n\nThe architecture proposed is generally applicable to a lot of settings broadly, yet the evaluation is too specific and does not adequately proves the generality of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to construct context as a tree is interesting and could have broad implications in constructing context for language models use cases including agent trajectories, in-context learning examples and retrieved documents and more."
                },
                "weaknesses": {
                    "value": "- **More context on the baseline IO Perceiver**: The authors need a background section for IO perceiver so the work is self-contained. With the current version, IO perceiver, though being a famous and well-cited paper, is not clearly stated. \n- **Speedup by attending to fewer context tokens**: One claimed benefit of the method is that it attends to fewer tokens to context when performing the task, which I assume would result in an inference speedup. But the work does not explicitly measure if TCA runs faster than CA. On the other hand, TCA does in most of the cases leads to a slight performance degradation, and it will be important to justify the design with proper inference wall-clock time measurements. \n- **More general evaluation**: The authors claim to have proposed this general architecture, but the evaluating tasks are specific and not as general as expected. The tasks evaluated in the baseline work \u2014 IO perceiver (e.g., pretraining MLM, optical flow and multimodal encoding) seem harder and more general than the ones performed in this paper, (i.e., copy task, gp regression on image completion and time series on human activities). It would be nice to see experimental setups with more significant implications like pre-training, yet there seems to be significant amount of work to be put into actually scale it to these real settings."
                },
                "questions": {
                    "value": "- Does the algorithm run faster than full attention in terms of wall-clock time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698621306733,
            "cdate": 1698621306733,
            "tmdate": 1699636147320,
            "mdate": 1699636147320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PkcaUXWuJ0",
                "forum": "Vw24wtSddM",
                "replyto": "WkyYgWXIPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wmtm (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the comments and helpful feedback.\n\n> More context on the baseline IO Perceiver: The authors need a background section for IO perceiver so the work is self-contained. With the current version, IO perceiver, though being a famous and well-cited paper, is not clearly stated.\n\nThank you for bringing this to our attention. We have added a background section for Perceiver IO.\n\n> More general evaluation: [...] The tasks evaluated in the baseline work \u2014 IO perceiver (e.g., pretraining MLM, optical flow and multimodal encoding) seem harder and more general than the ones performed in this paper, (i.e., copy task, gp regression on image completion and time series on human activities). It would be nice to see experimental setups with more significant implications like pre-training, yet there seems to be a significant amount of work to be put into actually scaling it to these real settings.\n\nWe initially considered evaluating on Perceiver IO's tasks. However, Perceiver IO's experiments used a very large amount of computing resources for their experiments which was significantly outside our capabilities. For example, their experiments used \"a batch size of 1024 and 64 TPUs\"  (Appendix A.1 in Perceiver IO's paper), \"a larger batch size of 8192 and train on 256 TPUs\" (Appendix A.2 in Perceiver IO's paper), and \"1024 batch size on 64 TPUs\"  (Appendix A.2 in Perceiver IO's paper). Specifically for their pre-training experiments in Perceiver IO, the authors used 256 TPUs. In contrast, each of our experiments was run on 1 GPU at a time (Nvidia P100 GPU (16 GB)). \n\nTo ensure our experiments were general and with significant implications, we considered (1) different forms of data such as images and time series and (2) popular and well-benchmarked tasks within their respective literatures:  Neural Processes (GP Regression and Image Completion) and Time Series (Human Activity). Time Series models are popular in various important fields such as healthcare, climate science, ecology, and astronomy. Similarly, Neural Processes have been applied to a wide array of impactful settings (see the recent survey (Jha et al., 2023) for a full list) such as climate modelling and detecting biomarkers in fMRI scans. \n\nThe datasets we considered are the standard benchmark datasets for evaluating the performance of Neural Processes (GP regression and image completion) and Time Series (Human Activity) models. Our results are competitive with that of state-of-the-art while requiring only a logarithmic number of tokens. Due to space limitations, these comprisons were included in the Appendix. Specifically, the tables in the Appendix include results comparing the performance of ReTreever with a total of $18$ baselines: $9$ baselines for Neural Processes and $9$ baselines for Time Series.\n\n\n---\n\nSaurav Jha, Dong Gong, Xuesong Wang, Richard E. Turner, Lina Yao. \"The neural process family: Survey, applications and perspectives.\" arXiv preprint arXiv:2209.00517, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372458015,
                "cdate": 1700372458015,
                "tmdate": 1700374636299,
                "mdate": 1700374636299,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HGmwlUuDsA",
                "forum": "Vw24wtSddM",
                "replyto": "WkyYgWXIPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wmtm (2/2)"
                    },
                    "comment": {
                        "value": "> One claimed benefit of the method is that it attends to fewer tokens to context when performing the task, which I assume would result in an inference speedup.\n> [...] it will be important to justify the design with proper inference wall-clock time measurements. [...]\n> Questions:\n> Does the algorithm run faster than full attention in terms of wall-clock time?\n\n \nIn this work, we focused on % tokens as the metric since it is (1) neither hardware nor implementation specific and (2) % tokens translates towards memory which is a major limiting factor scalability, particularly for (i) attention-based models which are memory intensive and (ii) low-memory devices which are popular (e.g., IoT devices). In contrast, wall clock time (runtime) is highly dependent on the efficiency of the implementation and the hardware. Nonetheless, in the table below, we compare the wall-clock time of the attention mechanisms.\n\n\nWe evaluate the modules on the Copy Task for both a GPU and CPU. Although Tree Cross Attention is slower, all methods only require milliseconds to perform inference. Both Tree Cross Attention and Cross Attention learn to solve the task perfectly. However, Tree Cross Attention uses only $3.5$% of the tokens. As such, Tree Cross Attention trades off between the number of tokens and computational time. Due to space limitations, we have added these results to the appendix. \n\nWe will be sharing our implementation alongside the camera-ready. We hope it will help improve the efficiency of future implementations of ReTreever and Tree Cross Attention.\n\n|         Model        | % Tokens |     Accuracy    | CPU time (in Milliseconds) | GPU Time (in Milliseconds) |\n|:--------------------:|:--------:|:---------------:|:--------------------------:|:--------------------------:|\n|    Cross Attention   |  100.0%  | 100.0 \u00b1 0.0 |            12.05           |            1.61            |\n| Tree Cross Attention | 3.5% | 100.0 \u00b1 0.0 |            19.31           |            9.09            |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372475271,
                "cdate": 1700372475271,
                "tmdate": 1700374672642,
                "mdate": 1700374672642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9PSkAk5yQF",
                "forum": "Vw24wtSddM",
                "replyto": "HGmwlUuDsA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your response! I understand the reasoning behind testing the algorithm in a more small-scale setting, particularly with limited compute resources. However, my concerns about the practical applicability of the method remain given its extreme high cost to possibly to replace cross attention, therefore I will maintain my current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492608823,
                "cdate": 1700492608823,
                "tmdate": 1700492608823,
                "mdate": 1700492608823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7lND4QHahO",
                "forum": "Vw24wtSddM",
                "replyto": "WkyYgWXIPD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wmtm's Response"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response! \n\n> my concerns about the practical applicability of the method remain given its extreme high cost to possibly to replace cross attention, therefore I will maintain my current score.\n\nWe would like to reiterate that Tree Cross Attention (TCA) trades off the memory and runtime. Although the runtime increases with TCA, the memory usage (and % tokens) has decreased significantly as shown in Figure 4 and the table results. \n\nThe trade-off between memory and runtime can be controlled by the design of the tree. Tree Cross Attention sequentially searches down a tree for the relevant tokens. As such, the factor which primarily affects the overall runtime is the height of tree ($H$) which is dependent on the branching factor $b_f \\geq 2$. The relationship between the height of the tree, the branching factor, and the number of tokens is as follows: $H = \\lceil\\log_{b_f}(N)\\rceil$. In our results, we showed the performance with using a binary tree ($b_f=2$). Notably, this corresponds to a tree design with a longer runtime but uses few tokens. By selecting a higher branching factor, the runtime can be decreased at the expense of requiring more tokens as we show in the table below. This feature is not available in standard cross attention. \n\nFrom these results, it is clear that TCA enables more control over the memory and computation time trade-off. Thus making it more tailored to many practical use cases (especially the low memory settings)\n\n| Model                              | Tree Height ($H$) | % Tokens  | CPU Time (in Milliseconds) | GPU Time (in Milliseconds) |\n|------------------------------------|-------------------|-----------|----------------------------|----------------------------|\n| Cross Attention                    | N/A               | $100.0$ % | 12.05                      | 1.61                       |\n| Tree Cross Attention ($b_f = 256$) | 1                 | $100.0$ % | 16.21                      | 2.05                       |\n| Tree Cross Attention ($b_f = 32$)  | 2                 | $15.2$ %  | 14.05                      | 3.99                       |\n| Tree Cross Attention ($b_f = 16$)  | 2                 | $12.1$ %  | 13.25                      | 4.13                       |\n| Tree Cross Attention ($b_f = 8$)   | 3                 | $7.0$ %   | 15.53                      | 5.73                       |\n| Tree Cross Attention ($b_f = 4$)   | 3                 | $3.9$ %   | 16.30                      | 6.45                       |\n| Tree Cross Attention ($b_f = 2$)   | 8                 | $3.5$ %   | 21.92                      | 9.83                       |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499553164,
                "cdate": 1700499553164,
                "tmdate": 1700586734793,
                "mdate": 1700586734793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ne4PbK1WHW",
            "forum": "Vw24wtSddM",
            "replyto": "Vw24wtSddM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_779Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_779Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for reducing the test-time computational cost of attention. Namely, TCA walks down a tree to attend to a single set of sibling leaves, while only attending to a compressed version of the other leaves. This allows the complexity of attention to be logarithmic in the total number of leaves $N$ (the sequence length). This comes at the cost of training a policy to traverse the tree, which must be trained via REINFORCE, an aggregator that compresses and composes leaf representations, as well as defining the tree itself.\n\nExperiments on a copy task, GP regression, image completion, human activity classification show that the method is efficient and performant. Additional analysis highlights the method's memory efficiency compared to full attention."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I enjoyed reading the paper. The paper is well-written and easy to follow. The idea is simple and clever."
                },
                "weaknesses": {
                    "value": "Overall, I believe the paper is pretty complete. I am mostly curious about how to make this method work for self-attention and (masked) autoregressive modeling.\n\nLarger-scale experiments would be appreciated, as the current experiments are quite small-scale. Presumedly the challenges of training the tree expansion policy would increase with harder datasets.\n\nOne suggestion for a larger-scale experiment would be training a translation or summarization model and replacing the encoder attention with tree cross attention."
                },
                "questions": {
                    "value": "## Questions\n1. Would TCA work out of the box for masked language modeling, e.g. BERT?\n2. Did you try using Gumbel-softmax for training the tree expansion policy?\n3. What are the barriers to applying TCA to self-attention? Would aggregation become the most expensive operation?\n\n## Suggestions\n1. In the last paragraph of 3.1, I was a little confused about why k-d trees were needed as I was only thinking about 1D sequences. Having a picture of an image tree and some more prose about different domains would be really nice for motivating and showing the generality of the method.\n2. While including cross attention in the model name pragmatically implies the method is not intended directly for self-attention, it would be nice to add a footnote that the focus is not on autoregressive modeling."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698984543023,
            "cdate": 1698984543023,
            "tmdate": 1699636147260,
            "mdate": 1699636147260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6bqAroOPYP",
                "forum": "Vw24wtSddM",
                "replyto": "Ne4PbK1WHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 779Z"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their very supportive feedback. We are pleased to see your enthusiasm for our work.\n\n\n> Would TCA work out of the box for masked language modeling, e.g. BERT?\n\nTCA supports masked inputs and can be applied similarly to self-attention by setting the query vectors in TCA the same as the context vectors. As such, TCA can be applied to masked language modelling. \n\n> Did you try using Gumbel-softmax for training the tree expansion policy?\n\nWe have not tried using Gumbel-softmax for training the expansion policy but that would be an interesting direction to explore as an alternative to our REINFORCE-based policy training method.\n\n\n> What are the barriers to applying TCA to self-attention? Would aggregation become the most expensive operation?\n\nTCA can be applied to self-attention by setting the query vectors the same as the context vectors. Empirically, we found that aggregation to be a rather efficient operation. For example, on a batch of sequences of length $256$, it took 0.012864 \u00b1 0.002792 seconds. \n\n\n\n> In the last paragraph of 3.1, I was a little confused about why k-d trees were needed as I was only thinking about 1D sequences. Having a picture of an image tree and some more prose about different domains would be really nice for motivating and showing the generality of the method.\n\nThank you for letting us know! We have added in Section 3.1 more details regarding the construction of k-d trees in different domains (sequences and images) to improve the clarity. \n\n> While including cross attention in the model name pragmatically implies the method is not intended directly for self-attention, it would be nice to add a footnote that the focus is not on autoregressive modelling.\n\nThank you for your suggestion. We have updated the paper to include a footnote that clarifies the focus is not on autoregressive modelling."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372382437,
                "cdate": 1700372382437,
                "tmdate": 1700374408173,
                "mdate": 1700374408173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eq3NhPbWih",
            "forum": "Vw24wtSddM",
            "replyto": "Vw24wtSddM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes TCA, a tree-based cross attention module to reduce the complexity of cross attention from $O(N)$ to $O(\\log(N))$, where $N$ is the number of tokens used for cross attention. Given $N$ tokens, TCA first constructs a balanced binary tree representation using standard methods like K-D tree, where the leaf nodes are the token embeddings, and the internal node representations are aggregated using the two children of the internal node. TCA uses reinforcement learning to learn good internal node representations. This construction is only performed once for a set of context tokens. Now, for a given query vector, a tree search is performed to select a subset of nodes ($O(\\log(N))$ size) of the tree for cross attention, resulting in $O(\\log(N))$ complexity for retrieval. Using TCA, the paper further proposes ReTreever, a general-purpose retrieval model that achieves token-efficient inference. The paper compares the ReTreever models with other token-efficient retrieval models like Perceiver IO and show impressive gains over the baseline - little to no drop in performance while leveraging only a small subset of tokens for cross attention."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper is well written and builds the theory coherently.\n- The proposed cross-attention architecture, TCA, along with the general purpose retrieval model, ReTreever is novel.\n- Because ReTreever uses reinforcement learning to learn the internal node representations, the reward used for optimization can be non-differentiable like accuracy, which improves performance over a reward based on cross entropy because the reward model is simpler in case of accuracy.\n- The reasoning behind each of the loss terms in $\\mathcal{L_{ReTreever}}$ is well-explained and it also uses leaf-level cross attention loss to make the training faster.\n- The empirical results on various tasks like copying, uncertainty estimation are impressive using ReTreever, and the paper also has good ablation studies to test the various components of the proposed approach."
                },
                "weaknesses": {
                    "value": "- It would be good if a similar row (as given in Table 2) can be added to Table 1 for Perceiver IO with increased latent tokens that matches the performance of TCA on the copy task.\n- Theoretical complexity is fine, but the paper should also report wall-clock time for ReTreever and compare it with the full Transformer+Cross Attention and Perceiver IO models. I am guessing the tree approach is not parallelizable on accelerated devices like GPUs, but it would be good to see if there's considerable decrease in latency on CPUs.\n- Building on the previous point, wall-clock times for the tree construction and bottom-up aggregation should be reported too.\n- Using ReTreever-full does not make sense and it only confuses the understanding of the reader in my opinion. Either remove it, or add more details like why there is a performance gap between the full cross-attention and ReTreever-full given both are using 100% of the tokens."
                },
                "questions": {
                    "value": "I have asked most of my questions in the weakness section. If the authors can address my questions and add the relevant latency benchmarks too, I am willing to increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2145/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk",
                        "ICLR.cc/2024/Conference/Submission2145/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699281655002,
            "cdate": 1699281655002,
            "tmdate": 1700686496163,
            "mdate": 1700686496163,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u2CFQyc5AM",
                "forum": "Vw24wtSddM",
                "replyto": "eq3NhPbWih",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RnYk (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their very constructive feedback and for their support.\n\n\n> It would be good if a similar row (as given in Table 2) can be added to Table 1 for Perceiver IO with increased latent tokens that matches the performance of TCA on the copy task.\n\nIn the following table, we include results evaluating Perceiver IO with increased number of latent tokens on varying lengths of the copy task ($N \\in \\{128, 256, 512, 1024\\}$). Specifically, we tested using $100\\%$ tokens to evaluate its performance. Although Perceiver IO was able to solve the copy task for $N=128$. Perceiver's performance degraded sharply as the difficulty of the task increased even with $100\\%$ tokens. Furthermore, due to Perceiver IO's encoder which aims to map the context tokens to a set of latent tokens, we ran out of memory (on a Nvidia P100 GPU (16 GB)) trying to train the model on a sequence length of 1024.\n\n|                      | $N=128$  | $N=128$     | $N=256$  | $N=256$     | $N=512$  | $N=512$     | $N=1024$ | $N=1024$   |\n|----------------------|----------|-------------|----------|-------------|----------|-------------|----------|------------|\n| Method               | % Tokens | Accuracy    | % Tokens | Accuracy    | % Tokens | Accuracy    | % Tokens | Accuracy   |\n| Cross Attention      | 100.0%   | 100.0 \u00b1 0.0 | 100.0%   | 100.0 \u00b1 0.0 | 100.0%   | 100.0 \u00b1 0.0 | 100.0%   | 99.9 \u00b1 0.2 |\n| Random               | ---      | 8.3 \u00b1 0.0   | ---      | 8.3 \u00b1 0.0   | ---      | 8.3 \u00b1 0.0   | ---      | 8.3 \u00b1 0.0  |\n| Tree Cross Attention | 10.9%    | 100.0 \u00b1 0.0 | 6.3%     | 100.0 \u00b1 0.0 | 3.5%     | 100.0 \u00b1 0.0 | 2.0%     | 99.6 \u00b1 0.6 |\n| Perceiver IO         | 10.9%    | 17.8 \u00b1 0.0  | 6.3%     | 15.2 \u00b1 0.0  | 3.5%     | 13.4 \u00b1 0.2  | 2.0%     | 11.6 \u00b1 0.4 |\n| Perceiver IO         | 100.0%   | 100.0 \u00b1 0.0 | 100.0%   | 63.6 \u00b1 45.4 | 100.0%   | 14.1 \u00b1 2.3  | 100.0%   | OOM        |\n\n\nTo analyze the reason behind the degradation in performance of Perceiver IO, we evaluated Perceiver IO on the copy task with a varying number of latent tokens (see table below), i.e., $L \\in \\{16,32,64,128\\}$ and varying sequence lengths $N \\in \\{128,256,512\\}$.  Generally, we found that performance improved as the number of latents increased.  However, the results were relatively unstable. For some runs, Perceiver IO was able to solve the task completely. Other times, Perceiver IO got stuck in lower accuracy local optimas. For longer sequences $N=512$, increasing the number of latents did not make a difference as the model simply got stuck in poor local optimas. \n\nWe hypothesize that the poor performance is due to the incompatibility between the nature of the Copy Task and Perceiver IO's model. The copy task (1) has high intrinsic dimensionality that scales with the number of tokens and (2) does not require computing higher-order information between the tokens. In contrast, Perceiver IO (1) is designed for tasks with lower intrinsic dimensionality by compressing information via its latent tokens and iterative attention encoder, and (2) computes higher-order information via a transformer in the latent space. Naturally, these factors make learning the task difficult for Perceiver IO. Due to space limitations, we have added these tables and analyses to the appendix. \n\n| Perceiver IO's Num. Latents ($L$) | Copy Task ($N=128$) Accuracy | Copy Task ($N=256$) Accuracy | Copy Task($N=512$) |\n|:---------------------------:|:-------------------:|:-------------------:|:------------------:|\n|            $L=16$           |    39.83 \u00b1 40.21    |     15.16 \u00b1 0.03    |    13.41 \u00b1 0.15    |\n|            $L=32$           |    58.92 \u00b1 47.44    |    34.75 \u00b1 39.14    |    19.41 \u00b1 12.07   |\n|            $L=64$           |    79.45 \u00b1 41.11    |    27.10 \u00b1 21.23    |    14.00 \u00b1 0.72    |\n|           $L=128$           |    100.00 \u00b1 0.00    |    63.50 \u00b1 39.37    |    13.23 \u00b1 0.42    |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372284482,
                "cdate": 1700372284482,
                "tmdate": 1700372284482,
                "mdate": 1700372284482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E0OthakzE9",
                "forum": "Vw24wtSddM",
                "replyto": "eq3NhPbWih",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RnYk (2/2)"
                    },
                    "comment": {
                        "value": "> Theoretical complexity is fine, but the paper should also report wall-clock time for ReTreever and compare it with the full Transformer+Cross Attention and Perceiver IO models. \n\nIn this work, we focused on % tokens as the metric since it is (1) neither hardware nor implementation specific and (2) % tokens translates towards memory which is a major limiting factor scalability, particularly for (i) attention-based models which are memory intensive and (ii) low-memory devices which are popular (e.g., IoT devices). In contrast, wall clock time (runtime) is highly dependent on the efficiency of the implementation and the hardware. Nonetheless, in the table below, we compare the wall-clock time of the modules used during inference by ReTreever, Transformer+Cross Attention, and Perceiver IO, i.e., Tree Cross Attention, Cross Attention, and Perceiver IO's CA respectively.\n\n\n\nWe evaluate the modules on the Copy Task for both a GPU and CPU. Although Tree Cross Attention is slower, all methods only require milliseconds to perform inference. Both Tree Cross Attention and Cross Attention learn to solve the task perfectly. However, Tree Cross Attention uses only $3.5$% of the tokens.  As such, Tree Cross Attention trades off between the number of tokens and computational time. Perceiver IO, however, fails to solve the task for the same number of tokens.  Due to space limitations, we have added these results to the appendix. \n\n\n|         Model        | % Tokens |     Accuracy    | CPU time (in Milliseconds) | GPU Time (in Milliseconds) |\n|:--------------------:|:--------:|:---------------:|:--------------------------:|:--------------------------:|\n|    Cross Attention   |  100.0%  | **100.0 \u00b1 0.0** |            12.05           |            1.61            |\n| Tree Cross Attention | **3.5%** | **100.0 \u00b1 0.0** |            19.31           |            9.09            |\n|   Perceiver IO's CA  | **3.5%** |    13.4 \u00b1 0.2   |          **10.98**         |          **1.51**          |\n\n\n\n> I am guessing the tree approach is not parallelizable on accelerated devices like GPUs\n\nThe tree approach has a sequential aspect to it related to the height. However, significant aspects of it can still be parallelized on GPUs. For example, during the tree construction, left and right subtrees can be constructed at the same time since they do not share children nodes. Aggregation can also be performed at the same time for nodes at the same depth. Our implementation leverages parallelization in some aspects of the tree approach, but we have not utilized full-parallelization which can lead to further code optimization and runtime efficiency.\n\nWe will be sharing our implementation alongside the camera-ready. We hope it will help improve the efficiency of future implementations of ReTreever and Tree Cross Attention. \n\n\n\n> Building on the previous point, wall-clock times for the tree construction and bottom-up aggregation should be reported too.\n\nFor $256$ tokens, the wall-clock time for constructing a tree is $0.076 \u00b1 0.022$ milliseconds. The wall-clock time for the aggregation is $12.864 \u00b1 2.792$ milliseconds. We have added these numbers to the appendix.\n\n\n\n> Using ReTreever-full does not make sense and it only confuses the understanding of the reader in my opinion. Either remove it, or add more details like why there is a performance gap between the full cross-attention and ReTreever-full given both are using 100% of the tokens.\n\nThank you for your suggestion! We have removed ReTreever-Full from the main paper as it is not the focus."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372302814,
                "cdate": 1700372302814,
                "tmdate": 1700374065729,
                "mdate": 1700374065729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0mSj5yILow",
                "forum": "Vw24wtSddM",
                "replyto": "E0OthakzE9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for additional experimental results and the detailed response. Although I am a bit skeptical about the wall clock time compared to the baseline, I believe the paper proposes a novel tree-based approach for cross attention using a small subset of tokens. Therefore, I am recommending this paper for acceptance and increasing my rating to 8."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2145/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686468329,
                "cdate": 1700686468329,
                "tmdate": 1700686468329,
                "mdate": 1700686468329,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]