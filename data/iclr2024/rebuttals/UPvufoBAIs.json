[
    {
        "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation"
    },
    {
        "review": {
            "id": "XwECcwt6Qa",
            "forum": "UPvufoBAIs",
            "replyto": "UPvufoBAIs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_wu5J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_wu5J"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new domain adaptation method for category-level object pose estimation. The method uses only RGB images, without relying on source domain data or 3D annotations in the target domain. The authors represent an object that belongs to a known category as a cuboid mesh and utilize an existing method to learn the vertex features. The features are iteratively updated in the target domain based on their proximity to corresponding image features. The experiments show much better performance in the target domain compared with the competitors without domain adaptation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe authors handle the problem that neither depth information nor 3D annotations are available in the target domain, which is challenging yet important in real applications.\n\n\u2022\tThe intuition behind the presented domain adaptation method is that the local features, which represent specific parts of the object, are more robust than the global features in the target domain, which makes sense to me.\n\n\u2022\tThe presented method achieves impressive object pose estimation results in multiple datasets with different kinds of nuisance."
                },
                "weaknesses": {
                    "value": "The majority of the figures in this paper such as Fig.2, Fig.2, and Fig.10, exhibit low quality. It would be better if the authors could consider revising them to enhance the clarity and resolution.\n\nI am not familiar with the topic of domain adaptation, so I would not judge the novelty of this paper. Please refer to \u201cQuestions\u201d for my concerns."
                },
                "questions": {
                    "value": "\u2022\tTo my understanding, in the experiments, the baseline models are evaluated in the target domain without domain adaptation. In this context, it is reasonable that those methods cannot generalize well in the target domain. I was wondering if there are some existing domain adaption approaches that can be applied to the baseline models. The evaluation would be more convincing, comparing NeMo + 3DUDA with a competitor such as NeMo + another domain adaption method.\n\n\u2022\tThe pre-render feature maps are generated from the source mesh. As the vertex features are not updated here, how to make sure those feature maps are reliable in the target domain? Are there situations in which the method might be stuck in local optima or even diverge? Conducting an ablation study on the pre-rendered feature maps would be valuable.\n\n\u2022\tIn Sec.3.2.1, the parameter $\\delta$ seems crucial for effectively updating vertex features. The authors mentioned they chose a $\\delta$ such that the majority of source domain features lie within the likelihood score. How to set $\\delta$ in practice? Is it constant? Intuitively, as the vertex features are updated, one would expect the similarity between vertex features and image features to increase. Does it make more sense to update $\\delta$ accordingly?\n\n\u2022\tIt seems the method is time-consuming due to the iterations and pre-rendering. However, the actual time consumption during testing is unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697802056425,
            "cdate": 1697802056425,
            "tmdate": 1699636973753,
            "mdate": 1699636973753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6iKM5jJ0C1",
                "forum": "UPvufoBAIs",
                "replyto": "XwECcwt6Qa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging feedback.\n\n**W1. The majority of the figures in this paper such as Fig.2, Fig.2, and Fig.10, exhibit low quality. It would be better if the authors could consider revising them to enhance the clarity and resolution.**\n\nThis point has also been raised by other reviewers. We have tried our best to address the presentation issues with our initial draft by improving the quality and clarity of the figures. We are happy to take any more suggestions to improve them even further. Please refer to the updated draft for updated and improved figures.\n\n\n**Q1. To my understanding, in the experiments, the baseline models are evaluated in the target domain without domain adaptation. In this context, it is reasonable that those methods cannot generalize well in the target domain. I was wondering if there are some existing domain adaption approaches that can be applied to the baseline models. The evaluation would be more convincing, comparing NeMo + 3DUDA with a competitor such as NeMo + another domain adaption method.**\n\nTo the best of our knowledge, our method appears to be the only current source-free image-only UDA method for object pose estimation. It is unclear what kind of methodology we could combine with NeMo for UDA since that would be nontrivial and could be an interesting new research avenue. We do have baselines (P3D [a] and DMNT [b]), which are improvements over the NeMo model and claim to be robust to OOD as well. P3D uses synthetic and real data combinations and different training methods for robust 3D pose estimation. DMNT uses deformable meshes and neural textures for robust pose estimation. \n\n[a] Yang et. al. - Robust category-level 3d pose estimation from synthetic data\n\n[b] Wang et. al. - Neural textured deformable meshes for robust analysis-by-synthesis \n\n\n**Q2. The pre-render feature maps are generated from the source mesh. As the vertex features are not updated here, how to make sure those feature maps are reliable in the target domain? Are there situations in which the method might be stuck in local optima or even diverge? Conducting an ablation study on the pre-rendered feature maps would be valuable.**\n\nWe discuss this exact problem in Section 3.2 (Multi-Pose Initialization) and provide an ablative example in Figure 7. The source model does sometimes get stuck in local optima when evaluated on the target domain data, and therefore the estimated result may depend on the pose initialization. Therefore, we do multi-pose initializations for our method, and use the final estimates to update only robust, local vertex features irrespective of the global object pose estimated. We have added an ablation for multi-pose initializations in our updated draft (Table 4).\n\n**Q3. In Sec.3.2.1, the $\\delta$ parameter\nseems crucial for effectively updating vertex features. The authors mentioned they chose a $\\delta$ such that the majority of source domain features lie within the likelihood score. How to set in practice? Is it constant? Intuitively, as the vertex features are updated, one would expect the similarity between vertex features and image features to increase. Does it make more sense to update $\\delta$ accordingly?**\n\nFor our experiments, we empirically find that a normalized value between 0.85-0.9 for the parameter works sufficiently well for our model and approximates the coverage of the majority of source domain vertex features. Tightening the parameter during model updates helps the model learn faster. We thank the reviewer for their suggestion! Here is the ablative experiment for bus category on OOD-CV dataset with different threshold values:\n\n| Threshold value  |  Epochs range for convergence |\n|------------------|-------------------------------|\n|        0.8       |            120-150            |\n|       0.85       |             85-100            |\n| Dynamic(.85-.99) |             50-70             |\n\n**Q4. It seems the method is time-consuming due to the iterations and pre-rendering. However, the actual time consumption during testing is unclear.**\n\nThanks for pointing out that this should have been discussed in the paper. Evaluation time for our model (if not doing batched inference) is ~0.33 seconds/sample on a Nvidia RTX 2060 GPU. We have added it in the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335299170,
                "cdate": 1700335299170,
                "tmdate": 1700335299170,
                "mdate": 1700335299170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ao8zYp3x5y",
                "forum": "UPvufoBAIs",
                "replyto": "6iKM5jJ0C1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_wu5J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_wu5J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I have no other questions but it's better to further improve the presentation."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658832955,
                "cdate": 1700658832955,
                "tmdate": 1700658832955,
                "mdate": 1700658832955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FEd8suAgTA",
            "forum": "UPvufoBAIs",
            "replyto": "UPvufoBAIs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_PE77"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_PE77"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the task of category-level 3D pose estimation within the setting of source-free and image-only unsupervised domain adaptation. The authors introduce a novel method called 3DUDA, which is developed based on the observation of the invariance of object local parts and is supported by theoretical insights. 3DUDA utilizes a learnable cuboid feature matrix to represent an object category, and assesses the accuracy of a pose by comparing the feature map of the test image with the one rendered from the categorical cuboid feature matrix using the pose. This render and compare approach enables domain adaptation by iteratively updating the categorical feature matrix and fine-tuning the source model. Experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method, 3DUDA, is developed based on the common observation that certain object parts exhibit invariance across out-of-distribution (OOD) scenarios, and utilizes the categorical learnable cuboid meshes to effectively capture and store the part features at each vertex.\n- To achieve domain adaptation, 3DUDA employs an iterative process that involves updating the features of categorical meshes and fine-tuning the source model through feature-level render and compare optimization.\n- The paper is well-written and presents its ideas in a clear and understandable manner. It is accompanied by comprehensive supplementary materials and theoretical results, which greatly enhance the persuasiveness of the paper."
                },
                "weaknesses": {
                    "value": "- It is recommended to evaluate the proposed method on commonly used datasets for category-level pose estimation, such as REAL275 [1] or Wild6D [2].\n\n- It would be beneficial to include relevant works [3,4,5,6] in the paper to provide a comprehensive overview of the existing literature in the field.\n\n[1] Wang et al., Normalized object coordinate space for category-level 6d object pose and size estimation. CVPR2019.\n\n[2] Fu et al., Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset.\n\n[3] Lin et al., Category-level 6D object pose and size estimation using self-supervised deep prior deformation networks. ECCV2022.\n\n[4] He et al., Towards Self-Supervised Category-Level Object Pose and Size Estimation.\n\n[5] Zhang et al., Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. ICLR2023.\n\n[6] Goodwin et al., Zero-Shot Category-Level Object Pose Estimation. ECCV2022."
                },
                "questions": {
                    "value": "- How does 3DUDA address the issue of minimizing the impact of translation and object size on neural feature rendering?\n- What values are set for the number $R$ of vertices of the mesh and the hyperparameter of $N$ of the clutter model? How do they  impact the performance of the method?\n- The format of citations in Table 1 does not align with the citation format used in the rest of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Reviewer_PE77"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661116174,
            "cdate": 1698661116174,
            "tmdate": 1699636973633,
            "mdate": 1699636973633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zCKzGmHDNf",
                "forum": "UPvufoBAIs",
                "replyto": "FEd8suAgTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Rebuttal (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time. \n\n**W1. It is recommended to evaluate the proposed method on commonly used datasets for category-level pose estimation, such as REAL275 or Wild6D**.\n\n- Our focus in this work is largely on outdoor setups (e.g. OOD-CV) due to relatively more diverse OOD conditions (weather, lighting, context, etc.) available in such datasets. \n\n- Both WILD6D+ and REAL275 datasets are indoor RGBD datasets in NOCS [1] format, which are used primarily for robotic applications and have 5-6 categories. Historically, works have either focused on datasets like Pascal3D+, ObjectNet3D+ or REAL275, WILD6D+ and we follow the same convention. This is likely due to incompatibility between the camera parameters and data formats of the two kinds of datasets. It is not trivial to convert one to another (or convert models of one kind to another) and is beyond the scope of this review period due to technical and time-related limitations. We believe that combining these two baseline branches would strengthen the research field, but it remains a future work.\n\n- Most models [3,4,5] using these datasets rely on the depth channel to run PnP or combine depth with NOCS to solve pose so they cannot be trivially modified as an RGB model (to compare with our method) and require extensive work.\n\n- We therefore follow the convention of datasets used in previous works related to our pose estimation method.\n\nAs an alternative for the Synthetic to Real evaluation done in REAL275 and WILD6D+ benchmarks, here are the results for synthetic to real dataset using the SyntheticP3D source domain dataset[2], which consists of 6 synthetic objects. The real dataset is Pascal3D+. \n\n| Methods           | $\\pi/6$ Accuracy | $\\pi/18$ Accuracy |\n|-------------------|:----------------:|:-----------------:|\n| Resnet-50 General |       53.5       |        13.2       |\n| P3D               |       76.5       |        41.3       |\n| NeMo              |       71.8       |        39.5       |\n| Ours              |       88.9       |        66.7       |\n\n\n[1] Wang et al. - Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation.\n\n[2] Yang et al. - Robust Category-Level 3D Pose Estimation from Synthetic Data.\n\n[3] Lee et al. - TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation.\n\n[4] Zhang et al. - Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild.\n\n[5] Liu et al. - IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation.\n\n**W2. It would be beneficial to include relevant works [3,4,5,6] in the paper to provide a comprehensive overview of the existing literature in the field.**\n\nThanks for pointing out the works. We have added the mentioned references in the updated draft."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334017210,
                "cdate": 1700334017210,
                "tmdate": 1700334017210,
                "mdate": 1700334017210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nBwAjG5qD7",
                "forum": "UPvufoBAIs",
                "replyto": "TiwI9c9l0H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_PE77"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_PE77"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal assessment"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response, which addresses my concerns. Consequently, I will maintain my positive rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661601111,
                "cdate": 1700661601111,
                "tmdate": 1700661601111,
                "mdate": 1700661601111,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pLGn1kDCFh",
            "forum": "UPvufoBAIs",
            "replyto": "UPvufoBAIs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_xesL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_xesL"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of unsupervised category-level pose estimation in a target domain using only RGB images, without access to source domain data or 3D annotations.\nThe authors introduce a method that adapts to a target domain, even when it is complicated by nuisances, without requiring 3D or depth data.\nThey represent object categories with simple cuboid meshes and use a generative model of neural feature activations at each mesh vertex.\nThey focus on updating local mesh vertex features based on their proximity to corresponding features in the target domain, even when the global pose is incorrect.\nThe key insight is the stability of specific object sub-parts across different scenarios, which allows for effective model updates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper suggests effective render-and-compare adaptation pipeline for unsupervised domain adaptation for category-level object pose estimation task.\n\n2. Its proposed method shows the state-of-the-art performance in various corruption scenarios compared to some of the previous methods.\n\n3. The methodology part is intuitive and easy to follow text-wise."
                },
                "weaknesses": {
                    "value": "1. Poor presentation.\nI think this paper holds good insights and corresponding technological contributions.\nYet, the presentation of the whole paper is relatively poor, making it hard to follow the overall message.\nFigure placement is not well-aligned with the text context.\nFigure 3 seems to be a very important description of explaining one of the main contributions of this paper to match sub-vertices, while there is no mention in the main manuscript referring this.\nMoreover, the main manuscript refers to figures in appendix very often, which is very inconvenient to read, while some of these figures seem important enough to be contained in the main paper for effective description. (ex, Figure 5)\nI believe that ablation studies regarding several design choices of the proposed method should be contained in the main manuscript as well, since they can effectively validate the authors claim."
                },
                "questions": {
                    "value": "1. Baseline methods and other datasets\nExcept OOD-CV results, only previous baseline is NeMo. Can the authors explain why there can't be other methods in this comparison? Being better than only one baseline is not enough to claim the superiority of the proposed method.\nAlso, while I acknowledge that the paper mainly focuses on data corruption scenarios, is it possible to compare this type of approach in conventional category-level object pose benchmarks like CAMERA and REAL275 datasets provided by NOCS? I believe it would strengthen the authors' motivation if it can be generally applied to conventional syn-to-real UDA scenarios.\n\n2. GT reliability\nGround truth pose illustrated in Figure 1 and 5 seems to be not perfectly aligned with the image. Can the authors explain how these GTs are obtained, and how they are utilized? Are they only used for evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Reviewer_xesL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823388670,
            "cdate": 1698823388670,
            "tmdate": 1700704907651,
            "mdate": 1700704907651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xXavE7wDOc",
                "forum": "UPvufoBAIs",
                "replyto": "pLGn1kDCFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. In response to their comments:\n\n**W1a. Poor presentation.  I think this paper holds good insights and corresponding technological contributions. Yet, the presentation of the whole paper is relatively poor, making it hard to follow the overall message. Figure placement is not well-aligned with the text context. Figure 3 seems to be a very important description of explaining one of the main contributions of this paper to match sub-vertices, while there is no mention in the main manuscript referring this.**\n\nWe regret that the presentation of our initial draft was deemed poor by the reviewer. According to their suggestions, we have updated our paper. We have reframed the main content to make it easier to follow, improved the quality and placement of figures and made sure that all figures are properly referenced in the text. We request the reviewer to take a look at the newer version to see if the presentation issues are addressed.\n\n**W1b: Moreover, the main manuscript refers to figures in appendix very often, which is very inconvenient to read, while some of these figures seem important enough to be contained in the main paper for effective description. (ex, Figure 5) I believe that ablation studies regarding several design choices of the proposed method should be contained in the main manuscript as well, since they can effectively validate the authors claim.**\n\nWith our restructuring of the paper, we have moved some important details up from the appendix to the main text as we agree with the reviewer that some of them are too important to be deferred to the appendix. We have updated old Figure 3 and moved old Figure 5 to the main draft (refer to Figures 1 and 3 in the updated draft). Unfortunately, numerous experimental analysis performed in this work can not all fit in the space constraints of the main text. We hope that the current restructuring in the new draft bolsters the contributions that we made with this work. We are happy to take more suggestions regarding the same and address any more issues that the reviewer thinks still exist.\n\n**Q1a. Baseline methods and other datasets Except OOD-CV results, only previous baseline is NeMo. Can the authors explain why there can't be other methods in this comparison? Being better than only one baseline is not enough to claim the superiority of the proposed method.** \n\nWe thank the reviewer for drawing attention to this point. We have added results for other baselines' extended results tables in our updated draft (Table 6 and Table 7 (page 22 and 23)) which includes four more comparative methods for every experiment. As can be gleaned from these baselines, like OOD-CV, our method performs exceedingly well even when dealing with extreme UDA setups."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331974105,
                "cdate": 1700331974105,
                "tmdate": 1700331974105,
                "mdate": 1700331974105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0bVBJSWpVu",
                "forum": "UPvufoBAIs",
                "replyto": "pLGn1kDCFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response (Part 2)"
                    },
                    "comment": {
                        "value": "**Q1b. Also, while I acknowledge that the paper mainly focuses on data corruption scenarios, is it possible to compare this type of approach in conventional category-level object pose benchmarks like CAMERA and REAL275 datasets provided by NOCS? I believe it would strengthen the authors' motivation if it can be generally applied to conventional syn-to-real UDA scenarios.**\n\nThank you for drawing our attention to CAMERA and REAL275 datasets. \n\n - Our focus in this work is largely on real, outdoor setups (e.g. OOD-CV) due to relatively more diverse OOD conditions (weather, lighting, context, etc.) available in such datasets. \n\n- Conventionally, 3D pose estimation works have either focused on setups and datasets like PASCAL3D+, ObjectNet3D or on more robotic application based synthetic to real datasets like REAL275, CAMERA and WILD6D+. This is due to some distinct technical and usage difference between these dataset setups. It\u2019s non-trivial to convert the camera calibrations, annotations and dataformats from one to another and would require a significant amount of time and is likely out of scope for the review period. To the best of our knowledge, there is no recent work which has used both of these kinds of datasets together, and we believe this would be a good future work to combine these two types of datasets into one format.\n\n- Both CAMERA and REAL275 datasets are indoor RGBD datasets, which are used primarily for robotic applications and have 5-6 categories. Most models[3,4,5] using these datasets[1] rely on the depth channel to run PnP or combine depth with NOCS[1] to solve pose so they cannot be trivially modified as an RGB model (to be compared with our method) and require extensive work. \n\n- As recommended by the reviewer - for a conventional synthetic to real analysis, here are results for image only unsupervised domain adaptation from SyntheticP3D source domain dataset[2] (which has 6 synthetic object classes) to real Pascal3D+ target domain dataset.\n\n\n| Methods           | $\\pi/6$ Accuracy | $\\pi/18$ Accuracy |\n|-------------------|:----------------:|:-----------------:|\n| Resnet-50 General |       53.5       |        13.2       |\n| P3D               |       76.5       |        41.3       |\n| NeMo              |       71.8       |        39.5       |\n| Ours              |       88.9       |        66.7       |\n\n[1] Wang et al. - Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation.\n\n[2] Yang et al. - Robust Category-Level 3D Pose Estimation from Synthetic Data.\n\n[3] Lee et al. - TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation.\n\n[4] Zhang et al. - Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild.\n\n[5] Liu et al. - IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation.\n\n**Q2. GT reliability Ground truth pose illustrated in Figure 1 and 5 seems to be not perfectly aligned with the image. Can the authors explain how these GTs are obtained, and how they are utilized? Are they only used for evaluation?**\n\nWe want to clarify that the CAD objects in old Figure 1 and 5 (updated Figure 3) are for visualisation only. We use Shapenet3D models and rotate them according to the dataset\u2019s annotations (GT) and model\u2019s (Ours, NeMo) pose estimation results. There might be some noise in GT pose annotation as well the visualization setup which might be the reason for non-perfect alignment. The images as well as their ground truth annotations (GTs) in the figures are obtained from the OOD-CV dataset."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332012187,
                "cdate": 1700332012187,
                "tmdate": 1700427821939,
                "mdate": 1700427821939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fVvwLkeQzU",
                "forum": "UPvufoBAIs",
                "replyto": "pLGn1kDCFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Period End"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe have updated our paper's presentation according to your comments and have covered all the technical questions that you've asked. It'll be of vastly beneficial to us if you can tell us what additional improvements you'd like to see in our paper, so we can make them before the discussion period ends. If you are satisfied with our method and believe we have covered your technical questions, it'll be reassuring if you can kindly update our paper's rating to reflect the same."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661545856,
                "cdate": 1700661545856,
                "tmdate": 1700661545856,
                "mdate": 1700661545856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KvgpDIS8mY",
                "forum": "UPvufoBAIs",
                "replyto": "fVvwLkeQzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_xesL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_xesL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. It resolves my concerns and questions. I change my rating to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704889339,
                "cdate": 1700704889339,
                "tmdate": 1700704889339,
                "mdate": 1700704889339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4Vvnkf9NHC",
            "forum": "UPvufoBAIs",
            "replyto": "UPvufoBAIs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_KkCS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7928/Reviewer_KkCS"
            ],
            "content": {
                "summary": {
                    "value": "This paper study the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. The author propose a new pipline which focus on the  individual local mesh vertex features and utilize their pose ambiguity to iteratively update them based on their prox- imity to corresponding features in the target domain even when the global pose is not correct. The proposed method shows good results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method outperform previous approaches by a large margin\n2. The author evaluate their model on real world nuisances like shape, texture, occlusion, etc. as well as image corruptions and show the robustness of proposed method"
                },
                "weaknesses": {
                    "value": "1. As mentioned in the article, an observation is that global information is noisy, but some local details are robust. I hope there is rigorous explanation and quantitative analysis here to support this hypothesis.\n2. Ablation is not enough, (e.g., Ablation on top-n \n3. I think this method is similar to the iterative optimization often used in instance-level post-processing. It is unfair to compare this method with other single forward methods.\n4. I'm not sure if it's right to say \"normalized real-valued feature activations\" ? (\nThe fourth line from the bottom of the third page)\n5. The figures in this article are very rough and difficult to understand."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7928/Reviewer_KkCS",
                        "ICLR.cc/2024/Conference/Submission7928/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7928/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699470426877,
            "cdate": 1699470426877,
            "tmdate": 1700662931351,
            "mdate": 1700662931351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J62lwNBh9e",
                "forum": "UPvufoBAIs",
                "replyto": "4Vvnkf9NHC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review Response and Clarifications"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. In response to their comments:\n\n**W1. As mentioned in the article, an observation is that global information is noisy, but some local details are robust. I hope there is rigorous explanation and quantitative analysis here to support this hypothesis.**\n \nSection 3.3 provides theoretical justification of how local robustness helps the domain adaptation problem. To see whether this local robustness is observed in real datasets, we calculated the percentage of correctly detected vertices in the target domain. In Figure 1 (in the updated draft), we see that this value has a minimum of 5-20% robust vertex features in the object category \u201cCar\u201d. After adaptation using our method, this percentage increases significantly. This is further illustrated for other object categories in the appendix (Figures 5, 9, 10). If some specific quantitative analysis is missing, we request the reviewer to point it out so that we can update it.\n\n**W2. Ablation is not enough, (e.g., Ablation on top-n**\n\nIt is unclear what the reviewer means by missing ablation studies **(top-n)** in our 3-D pose estimation problem. We believe that we have ablated all major design choices and method motivations in Section A.5. In summary, our ablation (Section A5) includes ablation of\t\n\na. Our motivation and observations regarding local part robustness (Figure 5, 9, 10),\n\nb. The efficacy of the Selective Vertex Feature Update,\n\nc. False Positives in vertex-Feature Similarity,\n\nd. Effect of the learnable concentration parameter,\n\ne. Multi-Pose Initialization,\n\nf. Quality of vertex Features after Selective Adaptation.\n\nIn addition, we have added the following more ablations in the revised version: \n\ng. Effect of number of clutter features hyperparameter,\n\nh. Additional ablation figures to prove the local part robustness observation mentioned in the previous response paragraph. (Figures 9 and 10)\n\nIf the reviewer is referring to the top-n multipose initializations, we do observe faster convergence with increasing number of initial poses. However, during multipose initialization, we only consider estimated poses whose rotational matrix distance (refer to Section 4 (Metrics paragraph)) from one another is at least $\\pi/5$. If the estimated poses are less than the threshold for all, we only consider the pose with the highest similarity to the image feature vector. We did not see any significant improvement in accuracy using more than 5 initial poses for our experiments. Here are ablation results for the car category for different numbers of pose initializations when adapted on OOD-CV (Combined dataset).\n\n| Number of initialized poses | Epochs range for convergence | $\\pi/18$ Accuracy |\n|-----------------------------|------------------------------|-------------------|\n|              1              |            130-170           |        95.3       |\n|              3              |            100-120           |        97.1       |\n|              5              |             50-90            |        97.1       |\n\nIf there are specific ablation studies that the reviewer feels are missing, we would be happy to run them. \n\n**W3. I think this method is similar to the iterative optimization often used in instance-level post-processing. It is unfair to compare this method with other single forward methods.**\n\nIt will be helpful if the reviewer can clarify which methods or provide citations of the works that use iterative optimization and are similar to our UDA method. To the best of our knowledge, our method is the first method to do source-free, image-only unsupervised domain adaptation for 3D object pose estimation. If the reviewer refers to the render-and-compare optimization used in our model, we would like to clarify that this is indeed a common optimization method used for a number of pose estimation methods such as [a,b,c] (this is not our contribution) and it is standard practice to compare such methods with feedforward methods as can be seen in these works. \n\n[a] Li et al. - DeepIM: Deep Iterative Matching for 6D Pose Estimation\n\n[b] Wang et al. - NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation\n\n[c] Chen et al. - Category level object pose estimation via neural analysis-by-synthesis\n\n**W4. I'm not sure if it's right to say \"normalized real-valued feature activations\" ? ( The fourth line from the bottom of the third page)**\n\nThe phrase \"normalized real-valued feature activations\"  refers to feature activations of a neural model which are real numbers and are normalized to have unit norm. We request the reviewer to be more specific as to why they believe the usage is incorrect.\n\n**W5. The figures in this article are very rough and difficult to understand.**\n\nWe have updated the visual clarity of our figures in the main draft (refer to Figures 1, 2 and 3) and modified them to be easier to understand. We hope that the revised figures address this concern of the reviewer."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323387929,
                "cdate": 1700323387929,
                "tmdate": 1700323387929,
                "mdate": 1700323387929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4h8Az9tno",
                "forum": "UPvufoBAIs",
                "replyto": "4Vvnkf9NHC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Deadline"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the reviewer-author period comes to an end, we are yet to receive any response or acknowledgment from you regarding our response to your initial review. We have also asked some clarifications regarding your initial review so we can frame our response better. Please let us know if you have any further questions or if our clarifications and paper updates are satisfactory for an improved rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633244247,
                "cdate": 1700633244247,
                "tmdate": 1700633244247,
                "mdate": 1700633244247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "soArMOeSO4",
                "forum": "UPvufoBAIs",
                "replyto": "J62lwNBh9e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_KkCS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7928/Reviewer_KkCS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for authors detailed response, my concerns are addressed. I will lift my rating to 6"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7928/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662902664,
                "cdate": 1700662902664,
                "tmdate": 1700662902664,
                "mdate": 1700662902664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]