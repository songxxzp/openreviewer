[
    {
        "title": "Travelling Salesman Problem Goes Sparse With Graph Neural Networks"
    },
    {
        "review": {
            "id": "cBmv2LijUn",
            "forum": "mnRLzeNsVN",
            "replyto": "mnRLzeNsVN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_stG3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_stG3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to adopt graph sparsification techniques in the data preprocessing step for the GNN-based TSP solvers. Experiments demonstrate its effectiveness in both aspects of improving the quality of solutions and the running efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The methods are straight-forward and easy to follow."
                },
                "weaknesses": {
                    "value": "- Important baselines are missing. The experiments are more like ablation studies: The authors investigate the performance of GAT/GCN-based solvers with or without the proposed graph sparsification techniques, but does not compare the performance with other solvers mentioned in the related works. To make the contributions strong enough and the results convincing, the paper should compare with the latest methods and outperform them.\n\n- The examples in Figure 1 do not make sense. Message passing in GNN does not propagate the features directly, but with a projection matrix (e.g. GraphSAGE). Furthermore, the problem of over-smoothing of GNN not only exist in complete graphs but also in general graphs [1]. How the proposed graph sparsification technique relieve the problem should be more clearly discussed.\n\n- The covered problems only include the 2D tsp, which limits the the contributions of the proposed techniques.\n\n- It lacks necessary theoretical analysis.\n\n- The proposed 1-tree sparsification method is derivated from LKH which is a very strong TSP solver. Then the use of the technique in data preprocessing indeed brings prior knowledge to the neural solver. It is very hard to say that whether the better performance comes from the graph sparsification, or comes from the prior knowledge for TSP solving. \n\n[1] A SURVEY ON OVERSMOOTHING IN GRAPH NEURAL NETWORKS. https://arxiv.org/pdf/2303.10993.pdf\n\n\u2028Based upon the above points, I believe that the work is still somehow preliminary and the paper does not meet the bar of iclr."
                },
                "questions": {
                    "value": "- The size of instances is not given. \n- The others are in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697952640967,
            "cdate": 1697952640967,
            "tmdate": 1699637024337,
            "mdate": 1699637024337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sVIp8HsbjS",
                "forum": "mnRLzeNsVN",
                "replyto": "cBmv2LijUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time and feedback! We will address the raised questions below.\n\n-) Important baselines are missing. The experiments are more like ablation studies: The authors investigate the performance of GAT/GCN-based solvers with or without the proposed graph sparsification techniques, but does not compare the performance with other solvers mentioned in the related works. To make the contributions strong enough and the results convincing, the paper should compare with the latest methods and outperform them.\n\nThe goal of the paper was to show that GNNs can generally increase their performance when learning TSP by applying sparsification. The aim of the paper was not to create one new framework with improved performance but introduce sparsification as an additional \u201cpreprocessing\u201d step generally improving the performance of learning-based TSP-solvers using TSP \u2013 independent of learning paradigm or the way learning was incorporated in the overall approach (see Encoder-decoder based approaches, search-based approaches and improvement-based approaches in the related work section).\n\n-) The examples in Figure 1 do not make sense. Message passing in GNN does not propagate the features directly, but with a projection matrix (e.g. GraphSAGE). Furthermore, the problem of over-smoothing of GNN not only exist in complete graphs but also in general graphs [1]. How the proposed graph sparsification technique relieve the problem should be more clearly discussed.\n\nWe omitted the projection matrix in the message passing process in the figure to make our point clear \u2013 The feature vectors get flooded with the information of all nodes in the graph.\nThis makes it hard for the decoder architecture to discriminate between the different feature vectors. On the other hand, on the sparsified graph, this problem does not occur.\n\n-) The covered problems only include the 2D tsp, which limits the the contributions of the proposed techniques.\n\nWe note that sparsification could be achieved by other sparsification heuristics on higher dimensional data as well. E.g., k-NN is directly generalizable to higher-dimensional data.\n\n-) It lacks necessary theoretical analysis.\n\nOur paper shows the importance of sparsification based on empirical results.\n\n-) The proposed 1-tree sparsification method is derivated from LKH which is a very strong TSP solver. Then the use of the technique in data preprocessing indeed brings prior knowledge to the neural solver. It is very hard to say that whether the better performance comes from the graph sparsification, or comes from the prior knowledge for TSP solving.\n\nWe emphasize that we do not run the entire LKH algorithm prior to applying the GNN. Instead, we only use the candidate set generation of LKH based on 1-Trees. With this candidate set, the LKH algorithm would still perform an extensive and time-consuming k-opt heuristic/search which we do not do here for computing our sparse graphs.\n\n-) The size of instances is not given.\n\nWe use TSP instances of size n=100 which is also stated in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491710851,
                "cdate": 1700491710851,
                "tmdate": 1700491710851,
                "mdate": 1700491710851,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DZriqUIYEP",
            "forum": "mnRLzeNsVN",
            "replyto": "mnRLzeNsVN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_CDdM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_CDdM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two data preprocessing methods for solving the TSP with GNNs, i.e., k-nearest neighbors heuristic and 1-Trees, which make the corresponding TSP instances sparse by deleting unpromising edges. Experiments are carried out to determine the better sparsification method and the relationships between different data distributions/training dataset sizes and sparsification parameter k."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tSparsification (or pruning, candidate selection, etc.) methods are important for solving the TSP as they can substantially reduce the computational complexity, and is commonly used in learning-based algorithms and heuristic algorithms for the TSP.\n2.\tThe paper is overall well written."
                },
                "weaknesses": {
                    "value": "1.\tK-nearest neighbors heuristic is already used for sparsification in the input layer (k=20 for TSP100) of GCN by Joshi et al. (2019), Fu et al. (2021) and Xin et al. (2021b) followed this setting. And the proposed \u201c1-Trees\u201d method is similar to the edge candidate set construction process of the LKH algorithm using the 1-tree structure. Thus, the main contribution of this paper seems to be selecting the proper k of k-nn when the problem size is fixed at 100, and transplanting the 1-tree method of LKH as a data preprocessing procedure for learning-based methods. Therefore, the novelty of this paper is not significant enough.\n\n2.\tThe problem size is fixed at 100 in the experiments so that the generalization ability of the proposed method over different problem sizes is unclear. I recommend the authors add the following question in section 4: how does the problem size n (amount of cities in one TSP instance) relate to the sparsification parameter k?\n\n3.\tComparative experiments with state-of-the-art TSP algorithms is not provided. It is uncertain whether the \u201c1-Trees\u201d method or changing the hyperparameter k of k-nn in existing methods like Joshi et al. (2019); Fu et al. (2021); Xin et al. (2021b) can enhance the performance of state-of-the-art learning-based TSP algorithms."
                },
                "questions": {
                    "value": "1. Please clarify the novelty of this paper in comparisons with the literature papers.\n\n2. how does the problem size n (amount of cities in one TSP instance) relate to the sparsification parameter k?\n\n3. Comparative experiments with state-of-the-art TSP algorithms is not provided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698311731026,
            "cdate": 1698311731026,
            "tmdate": 1699637024206,
            "mdate": 1699637024206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ERLLiNOtNA",
                "forum": "mnRLzeNsVN",
                "replyto": "DZriqUIYEP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time and feedback! We will address the raised questions below.\n\n-) K-nearest neighbors heuristic is already used for sparsification in the input layer (k=20 for TSP100) of GCN by Joshi et al. (2019), Fu et al. (2021) and Xin et al. (2021b) followed this setting. And the proposed \u201c1-Trees\u201d method is similar to the edge candidate set construction process of the LKH algorithm using the 1-tree structure. Thus, the main contribution of this paper seems to be selecting the proper k of k-nn when the problem size is fixed at 100, and transplanting the 1-tree method of LKH as a data preprocessing procedure for learning-based methods. Therefore, the novelty of this paper is not significant enough.\n\nOther papers have only considered k-NN so far for a fixed k. Many papers did, however, not perform any sparsification at all. The purpose of the paper is to indicate the importance of sparsification, allowing GNN encoders to increase their performance. We note that the previously considered k-NN approach is severely limited to the uniform data distribution and fails on non-uniform data (consider a graph with two k+1 big coordinate clusters). \nThe 1-Tree method has been successfully used within the candidate set generation of LKH, however, to the best of our knowledge we are the first ones to apply it in this setting where we aim to generate promising sparse TSP graphs for GNNs.\n\n-) The problem size is fixed at 100 in the experiments so that the generalization ability of the proposed method over different problem sizes is unclear. I recommend the authors add the following question in section 4: how does the problem size n (amount of cities in one TSP instance) relate to the sparsification parameter k?\n\nWe will consider this question in an updated version of the paper.\n\n-) Comparative experiments with state-of-the-art TSP algorithms is not provided. It is uncertain whether the \u201c1-Trees\u201d method or changing the hyperparameter k of k-nn in existing methods like Joshi et al. (2019); Fu et al. (2021); Xin et al. (2021b) can enhance the performance of state-of-the-art learning-based TSP algorithms.\n\nThe goal of the paper was to show that GNNs can generally increase their performance when learning TSP by applying sparsification. The aim of the paper was not to create one new framework with improved performance but introduce sparsification as an additional \u201cpreprocessing\u201d step generally improving the performance of learning-based TSP-solvers using TSP \u2013 independent of learning paradigm or the way learning was incorporated in the overall approach (see Encoder-decoder based approaches, search-based approaches and improvement-based approaches in the related work section)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491612097,
                "cdate": 1700491612097,
                "tmdate": 1700491612097,
                "mdate": 1700491612097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NWoX36aOaC",
            "forum": "mnRLzeNsVN",
            "replyto": "mnRLzeNsVN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_oBRi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_oBRi"
            ],
            "content": {
                "summary": {
                    "value": "The authors observe that the sparsed TSP graph with KNN and 1-tree could improve the performance of the GNN-based method and reduce training time. The topic of studying the sparsity of TSP graph is interesting. The observation also seems reasonable. However, the sparsity like KNN has been used by previous work on TSP and VRP. The used 1-tree method was borrowed from LKH. Therefore, I think the contributions are quite marginal."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The observations are interesting.\n\nThe experiment design is mostly reasonable."
                },
                "weaknesses": {
                    "value": "Quite some related works about neural-based methods for TSP and VRP are missing, especially from TOP AI conferences.\n\nThe sparsity like KNN has been used by previous work on TSP and VRP. The used 1-tree method was borrowed from LKH. Therefore, I think the contribution are quite marginal."
                },
                "questions": {
                    "value": "The results of GAT with dense graphs are quite bad, which makes the results less convincing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552639358,
            "cdate": 1698552639358,
            "tmdate": 1699637024091,
            "mdate": 1699637024091,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IjNh2aUNpf",
                "forum": "mnRLzeNsVN",
                "replyto": "NWoX36aOaC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time and feedback! We will address the raised questions below.\u201d\n\n-) Quite some related works about neural-based methods for TSP and VRP are missing, especially from TOP AI conferences.\n\nTo the best of our knowledge, we included all relevant related work, but we are happy to extend the related work section with additional papers if the reviewer is open to share which papers in particular are missing.\n\n-) The sparsity like KNN has been used by previous work on TSP and VRP. The used 1-tree method was borrowed from LKH.\n\nTo the best of our knowledge, no other papers have extensively studied sparsification so far. If papers applied sparsification, it was typically k-NN with a fixed k and only on the uniform data distribution. We point out that k-NN is not promising on non-uniform data (consider a graph with two k+1 big coordinate clusters). To the best of our knowledge, 1-Tree has not been used for sparsification of graphs afterwards passed to GNNs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491529792,
                "cdate": 1700491529792,
                "tmdate": 1700491529792,
                "mdate": 1700491529792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "omaEy6mRnW",
            "forum": "mnRLzeNsVN",
            "replyto": "mnRLzeNsVN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_gtua"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8240/Reviewer_gtua"
            ],
            "content": {
                "summary": {
                    "value": "This paper use one-tree for edge elimination for GNN. \nThe proposed method achieves an up to \u00d72 performance improvement w.r.t. the optimality gap and a decrease in runtime by 10% during training and validation, when applied to GCNs. For GATs, the improvements in regards of runtime and optimality gap are even bigger when sparsifying the data first."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. one-tree based sparsity saves time.\n2. The introduction and related work sections are well-written."
                },
                "weaknesses": {
                    "value": "This paper is very hard to follow.\n\n1. One-tree has been proposed and existed for many year, introducing sparsity to GNN is not a new idea,\nsee https://arxiv.org/abs/2006.07054\n\n2. weak evaluation, on TSP 100 only.\n\n\n\nWe employ GNN for TSP with the aspiration of learning promising edges without the need for human-designed heuristics. However, the use of one-tree heuristics already narrows down the edge set. This means the sparsity is largely dependent on human-designed heuristics rather than data-driven ones.\n\nAlso, this sparsity is only limited to TSP and is not able to generalize any other problem."
                },
                "questions": {
                    "value": "1. The table is very confusing, why select different training size? The goal is to investigate how sparsity affect GNN, not training size.\n\n2. How to train your GNN, supervised or reinforcement or even unsupervised? How to get the TSP length? \nMy understanding is that the code is using reinforcement learning framework based on Jin et al.\nBut in Jin et al. The authors report a 0.16\\% on TSP-100. They further study TSP random200, TSP random500 and TSPLIB from 1~1002. \nIf the paper use the same model, they should evaluate on the same dataset with Jin et al.\n\n3. In the paper ```We summarize that for the GCN, smaller k led to the overall best results, whereas for the GAT there is a tendency for bigger k (but not dense graphs!) to lead to the best results.```, this is more confusing, that means graph sparsifying can be different for different GNN models, then how we decide $k$ when we use a different GNN model? \n\n4. We report up to \u00d722 improvements for the optimality gap while reducing the runtime by 50\\%.  Can you reveal more details about the training and evaluation, how to get these results?\n\n\n\nJin et al. Deep reinforced multi-pointer transformer for the traveling salesman problem"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698505235,
            "cdate": 1698698505235,
            "tmdate": 1699637023972,
            "mdate": 1699637023972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O2Nof7dwLH",
                "forum": "mnRLzeNsVN",
                "replyto": "omaEy6mRnW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8240/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time and feedback! We will address the raised questions below.\n\n-) One-tree has been proposed and existed for many year, introducing sparsity to GNN is not a new idea\n\n1-Tree is indeed not a new concept, however, it has not been used in the way we propose: to sparsify TSP graphs for GNN application. The linked paper does not extensively investigate sparsification for GNNs when solving the TSP. They focus on the generalization performance of neural architectures when solving TSP. They only use k-NN which is not promising on non-uniform data (consider a graph with two k+1 big coordinate clusters).\n\n-) Weak evaluation of TSP 100 only.\n\nSmaller instances can often be solved easily with state-of-the-art neural solvers which is why disregarded them. We will try to consider bigger instances in an updated version of the paper.\n\n-) We employ GNN for TSP with the aspiration of learning promising edges without the need for human-designed heuristics. However, the use of one-tree heuristics already narrows down the edge set. This means the sparsity is largely dependent on human-designed heuristics rather than data-driven ones.\n\nWe show that sparsification leads to better results compared to dense graphs, nevertheless. We believe that using heuristics in the sparsification process is not necessarily bad.\n\n-) Also, this sparsity is only limited to TSP and is not able to generalize any other problem.\n\nIt can be adapted to other problems by using other heuristics for the sparsification process.\n\n-) The table is very confusing, why select different training size? The goal is to investigate how sparsity affect GNN, not training size.\n\nWe want to investigate the influence of sparsification on different data set sizes. On bigger datasets, the GNN is (possibly) able to learn the importance of the edge by itself by having access to more data to learn from. On smaller datasets, sparsification is could be more relevant as there is not enough data to learn the importance of all the individual edges.\n\n-) How to train your GNN, supervised or reinforcement or even unsupervised? How to get the TSP length? My understanding is that the code is using reinforcement learning framework based on Jin et al. But in Jin et al. The authors report a 0.16% on TSP-100. They further study TSP random200, TSP random500 and TSPLIB from 1~1002. If the paper use the same model, they should evaluate on the same dataset with Jin et al.\n\nWe use a RL framework but point out that our sparsification can be used independently of the used learning paradigm. There are other works that explain how GNNs can be incorporated to capture the different TSP lengths. We did not use the model of Jin et al. directly but used the code as a basis to incorporate our GNN operating on sparse TSP graphs. \n\n-) In the paper \u201cWe summarize that for the GCN, smaller k led to the overall best results, whereas for the GAT there is a tendency for bigger k (but not dense graphs!) to lead to the best results.\u201d, this is more confusing, that means graph sparsifying can be different for different GNN models, then how we decide  when we use a different GNN model?\n\nWe are indeed unable to derive general rules on how to generally choose the level of sparsification dependent on the chosen GNN. We note, however, that sparsification improves performance and consider the best sparsification level as an additional hyperparameter."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491383629,
                "cdate": 1700491383629,
                "tmdate": 1700491474687,
                "mdate": 1700491474687,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]