[
    {
        "title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing"
    },
    {
        "review": {
            "id": "J56AxRM2o5",
            "forum": "wKNKnXjCfT",
            "replyto": "wKNKnXjCfT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on identifying direct causes of a target variable under possible intervention environments by leveraging the invariant causal prediction (ICP) mechanism. By the basic observations that the prediction error for Y using its causal parents is more minimal than other predictors that contain non-parent variables, the authors present two approaches that learn the invariant set of variables with the smallest error for predicting Y, i.e., MMSE-ICP and fastICP, where the latter method has lower complexity. The experimental results, both on the simulated data and gene expression data, show the proposed methods are able to learn the correct ICP set with increasing samples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors gave a good overview on the related literature.\n\n2. This paper is well-written and well-organization.\n\n3. The experimental results verify the efficientness of the proposed method of the proposed methods."
                },
                "weaknesses": {
                    "value": "1. My main concern is how to obtain the minimized MSE in practical application, which is a key issue to the proposed theoretical results and it should be properly proofed if possible.\n\n2. I note that the simulation data is only generated by a linear model. It should be clearly defined if the proposed theoretical results only focus on the linear model. \n\n3. If the non-parents node has no noise term, e.g., X_2=2E and X_1=X_2, it seems that the MMSE(X_1 X_2) is equal to (X_1), which may not ensure the correct ICP set output by the proposed method. In other words, the author should claim the causal model used in this paper, such as SCM or ANM. If my understanding is not correct, can you clarify what generation process assumptions you are making?"
                },
                "questions": {
                    "value": "1. My question is how to test the constraint \"isInvariant(Y,S)\" (refer to the algorithm). Can you give some illustrations to its implementations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4290/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ",
                        "ICLR.cc/2024/Conference/Submission4290/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546824092,
            "cdate": 1698546824092,
            "tmdate": 1700555693676,
            "mdate": 1700555693676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bNXED39pya",
                "forum": "wKNKnXjCfT",
                "replyto": "J56AxRM2o5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> My main concern is how to obtain the minimized MSE in practical application, which is a key issue to the proposed theoretical results and it should be properly proofed if possible.\n\nWe acknowledge that defining an effective strategy to obtain the MSE is important to ensure the proposed methods return valid answers. One could employ more sophisticated methods such as cross-validation or doubly-robust machine learning to obtain more accurate MSE estimates. For example, CORTH (Soleymani et al. 2022) leverages doubly-robust machine learning to devise a more accurate test for causal parents. However, CORTH is also a more computationally costly procedure which makes applying ICP_CORTH to the gene expression problem not tractable. The MSE estimation scheme that we adopted seems to give useful results in practice and further exploration of the MSE estimate is left for future work.\n\n- Soleymani et al. \"Causal Feature Selection via Orthogonal Search.\" 2022\n\n> I note that the simulation data is only generated by a linear model. It should be clearly defined if the proposed theoretical results only focus on the linear model.\n\nWe are sorry that this is not made clear in the paper. The proposed theoretical results are not dependent on linear data generation. For problems where data generation is non-linear, a non-linear invariance test can be swapped out for the linear invariance test used in the experiment. For example, see Heinze-Demel et al. 2018.\n\n- Heinze-Deml et al. \"Invariant causal prediction for nonlinear models.\" 2018\n\n> If the non-parents node has no noise term, e.g., X_2=2E and X_1=X_2, it seems that the MMSE(X_1 X_2) is equal to (X_1), which may not ensure the correct ICP set output by the proposed method. In other words, the author should claim the causal model used in this paper, such as SCM or ANM. If my understanding is not correct, can you clarify what generation process assumptions you are making?\n\nWe thank the reviewer for pointing out this case.\nIn the noiseless case, it is true that MMSE(X_1, X_2) == MMSE (X_1). MMSE-ICP will not pick {X_1, X_2} over {X_1}. This is because MMSE-ICP will first test for {X_1} then test for {X_1, X_2} (line 3, Algorithm 1). When MMSE-ICP finds that {X_1} is invariant, it will skip over {X_1, X_2} (line 4, Algorithm 1).\n\nHowever, it may be possible that MMSE(X_1) == MMSE(X_2) if X_1 is a deterministic function of X_2. Thus, our approach may output {X_2} instead of {X_1}. Thus, it is true that our approach depends on the assumption that there is noise in the system. However, it does not assume that the noise is additive (ANM) or the generation follows some specific structural causal model (SCM).\n\n> My question is how to test the constraint \"isInvariant(Y,S)\" (refer to the algorithm). Can you give some illustrations to its implementations?\n\nThe 'IsInvariance' operation is the same as used in prior work (Peters et al., 2016) and (Mogensen et al., 2022). Specifically, it tests whether the means and variances of the average prediction errors (residuals) across all environments are the same. The pseudo code is given below.\n\n```\np_vals = []\nfor E_label in groups:\n\t\tin_group, out_group = residuals[E == E_label], residuals[E != E_label]\n\t\tp_val1 = t_test(in_group, out_group) // returns p-value \n\t\tp_val2 = levene_test(in_group, out_group) // returns p-value based on (Levene 1960)\n\t\tp_vals.append(2*min(p_val1, p_val2))\nreturn min(p_vals) < threshold\n```\n\n- Levene. \"Robust tests for equality of variances\". 1960"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700001329748,
                "cdate": 1700001329748,
                "tmdate": 1700001329748,
                "mdate": 1700001329748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X4mseV7CfO",
                "forum": "wKNKnXjCfT",
                "replyto": "bNXED39pya",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. I keep my score due to (i). the estimation of MSE may rely on some trick, and It is difficult to guarantee its convergence under the non-linear function model theoretically; (ii) the problem statement requires further clarity, such as the applicable model and its discussion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290396601,
                "cdate": 1700290396601,
                "tmdate": 1700290396601,
                "mdate": 1700290396601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wWhQw9CLTg",
                "forum": "wKNKnXjCfT",
                "replyto": "J56AxRM2o5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the prompt response.\nWe would like to clarify the two points raised.\n\n(i) As explained in Section 4.2, our estimate of the MSE is a straightforward average of the predicted residuals across environments.\nIt is obtained by (1) fitting a linear model to predict Y using the data from all environments, (2) calculating the prediction error for each sample, (3) average the prediction errors.\n\nWe are unsure what \"convergence\" means in the context of causal discovery.\n\nAs for whether the MSE estimation is good enough for problems with non-linear data-generation, our SOTA result on the real data benchmark of Kemmeren et al. (2014) demonstrates, our proposed approach can be useful for practical applications. In addition, we want to emphasize that improving the MSE estimation is orthogonal to the main contributions of this paper. Our proposed approaches will only get better with more accurate MSE estimation. Finally, we would like to underscore that under the infinite (large-enough) data regime, an adoption of an infinite (large-enough) capacity (non-linear) model, instead of a linear model, will theoretically guarantee that the proposed approach can achieve an unbiased (arbitrarily good) estimate of minimum MSE. \n\n\n(ii) We are not sure what is meant by \"applicable model\" and would be grateful for further explanation.\nWe believe our statement of the problem we aim to tackle has been sufficiently clear.\nAs we describe in the introduction, our main goal is to identify the causal parents of a target variable. The causal parents can be used to build robust ML models or to design interventions to change the target variable value.\nAs we describe in Section 3, we rely on the error inequality to implement an efficient version of ICP."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494476566,
                "cdate": 1700494476566,
                "tmdate": 1700494476566,
                "mdate": 1700494476566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hY7lCws2Un",
                "forum": "wKNKnXjCfT",
                "replyto": "J56AxRM2o5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ZvvZ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. For the first question, I mean that whether a non-linear invariance test is fragile if data generation is non-linear, eg., fitting a non-linear model.\n\nSecondly, \"applicable model\"  means \"assumptions for the data generation\".\n\n\nThere are some suggestions:\n\n(1). provide a more detailed definition to \"environments\", and illustrate \"how to fit a linear model to predict Y using the data from all environments\".\n(2). formalize the data generation process, which may be an important \"problem definition\" in causal discovery.\n(3). distinguish/explain the \"intervention\" and \"environment\" if possible.\n\n\nOverall, based on the author's response, most of my issues are addressed. I made some slight adjustments to the scores, and I suggest that the feedback should be incorporated into the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555661436,
                "cdate": 1700555661436,
                "tmdate": 1700555817229,
                "mdate": 1700555817229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HSbfUkbgS",
                "forum": "wKNKnXjCfT",
                "replyto": "J56AxRM2o5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the clarification and constructive feedback.\n\nWe assume that the data are generated following an underlying causal system that can be represented with a DAG, in which the value of each variable/node is generated via a causal **mechanism** based on the values of its parents.\nIn the DAG, there is a target variable Y and all other variables are considered as Xs.\nSome Xs may be ancestors of Y, while some others may be descendants.\n\nFollowing common terminology, we use **intervention** to refer to a change of **mechanism** at a variable.\nThe data from an **environment** are generated from the same set of **mechanisms**.\nTwo different **environments** can have some differing **mechanisms**.\nHowever, the **mechanism** at Y, i.e., the way Y is generated from its parents, is assumed to be unchanged (or invariant) across all **environments**.\nWe assume that the knowledge of where and how the remaining **mechanisms** differ between two **environments** are unknown.\nSince the location of the **mechanism** changes are not known, these are often referred to as **unknown-target interventions**.\nAll of these assumptions are identical to the setup in Peters et al. and Mogensen et al.\n\n- Peters et al. \"Causal inference by using invariant prediction: identification and confidence intervals.\" 2016\n- Mogensen et al. \"Invariant Ancestry Search.\" 2022\n\nAlthough these assumptions are standard in the invariant causal prediction literature, we realized that the wider causal inference community may be unfamiliar with this setup, especially the concept of **unknown-target interventions**.\n\nWe will update the draft to integrate all the comments above to make the problem setup clearer."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582701090,
                "cdate": 1700582701090,
                "tmdate": 1700582701090,
                "mdate": 1700582701090,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1GuC3H7lJQ",
            "forum": "wKNKnXjCfT",
            "replyto": "wKNKnXjCfT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_DSFM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_DSFM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to identify the invariance subset of covariates across different domains, by exploiting the property of mean squared error (MSE). Compared to the original ICP, the proposed method required fewer interventions in identification. Further, a more scalable version called FastICP was proposed. The complexity analysis was provided. Experiments on conducted on synthetic datasets and real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Theoretical analysis regarding the identification of the invariance set and the complexity analysis are provided. \n2. Extensive experiments are conducted. Particularly, the application on a large graph demonstrates the effectiveness of the proposed methods. \n3. Generally, this paper is well-organized."
                },
                "weaknesses": {
                    "value": "1. It would be practically important to identify the set $PA(Y)$ compared to $DE(E) \\cap PA(Y)$, which may be done by exploiting MSE since the former has smaller MSE if $PA(Y) \\cap (DE(E))^c \\neq \\emptyset$. \n2. The specific procedures for the 'IsInvariance' operation (located at line 1 in Algorithm 1) are not provided, although it appears to involve a test for conditional independence. \n3. The running time should be provided on synthetic dataset."
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760271742,
            "cdate": 1698760271742,
            "tmdate": 1699636396912,
            "mdate": 1699636396912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OR93xgSsUm",
                "forum": "wKNKnXjCfT",
                "replyto": "1GuC3H7lJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It would be practically important to identify the set $PA(Y)$ compared to $DE(E)\\cap PA(Y)$, which may be done by exploiting MSE since the former has smaller MSE if $PA(Y)\\cap(DE(E))^c\u2260\\emptyset$.\n\nIt is true that $PA(Y)$ has smaller MSE than $DE(E)\\cap PA(Y)$, when the latter is a strict subset of the former. However, it may not be possible to identify $PA(Y)$ since we do not assume that the data contain interventions at every node $X$.\n\nWhen the data contain interventions at every node $X$, $PA(Y)$ is the only set that achieves \u201cinvariant\u201d prediction error across environments/domains/sites. Unfortunately, due to logistical constraints, the data gathered may not contain interventions at every node. For example, when a node $X$ denotes weather and it was always sunny during the data gathering period, then no intervention on $X$ has been observed in the data.\n\nWhen the number of observed interventions in the data is fewer than the number of $X$s, there are many sets that can achieve \u201cinvariant\u201d prediction error. These sets are deemed \u201cinvariant\u201d only because of the limitation of the data gathering. If we have seen more interventions, we could have rejected all the other sets that are not $PA(Y)$.\n\nConsider the following 3 \u201cinvariant\u201d sets:\n\n- $DE(E) \\cap PA(Y)$\n- $PA(Y)$\n- $PA(Y) \\cup (CH(Y) \\cap (DE(E))^c)$\n\nThey are in increasing order of cardinality and in decreasing order of MSE. It is not possible to pick out $PA(Y)$ because $PA(Y)$ has neither the smallest cardinality nor the smallest MSE among all the \u201cinvariant\u201d sets. If we chose the \u201cinvariant\u201d set with minimum MSE, then we would have chosen a set that includes not only parents of $Y$ but also children of $Y$.\n\nInstead of considering all \u201cinvariant\u201d sets, MMSE-ICP and fastICP only consider \u201cinvariant\u201d sets that are subsets of $DE(E) \\cap ND(Y)$. Among the subsets of $DE(E) \\cap ND(Y)$, it is possible to identify $DE(E) \\cap PA(Y)$ because it has the minimum MSE.\n\n> The specific procedures for the 'IsInvariance' operation (located at line 1 in Algorithm 1) are not provided, although it appears to involve a test for conditional independence.\n\nThe 'IsInvariance' operation is the same as used in prior work (Peters et al., 2016) and (Mogensen et al., 2022). Specifically, it tests whether the means and variances of the average prediction errors (residuals) across all environments are the same.\n\n- Peters et al. \"Causal inference by using invariant prediction: identification and confidence intervals.\" 2016\n- Mogensen et al. \"Invariant Ancestry Search.\" 2022\n\n> The running time should be provided on synthetic dataset.\n\nWe recorded the time each method took when the number of nodes and the number of interventions are equal to 6 (this corresponds to the scenario in Fig 2). This scenario is chosen because all the methods can complete exhaustive search for a problem of this size, thus allowing us to conduct a numerical comparison between all methods. For problems with 100 variables, ICP and IAS will not be able to search through all subsets.\n\nThe table below reports the numbers of seconds elapsed on average when executing on an Intel Xeon 6126 CPU core (@ 2.60GHz).\nN denotes the sample size.\nThe numbers in brackets are the standard deviation across different simulations.\n\nSince the official implementation of FGES_MB, ICP, and ICP_CORTH are not written in Python so direct comparison with fastICP should be viewed with caution.\nHowever, we can see that:\n\n1. Among ICP-like methods with exponential worst-case complexity, fastICP is faster than MMSE-ICP, ICP, ICP_CORTH, and IAS.\n2. fastICP is also slightly faster than UT-IGSP for large sample size (1E5 samples).\n\n| Method      | Language  |     N=1E2     |     N=1E3    |     N=1E4   |      N=1E5  |\n|-------------|-----------|---------------|---------------|---------------|----------------|\n| FGES_MB     | Java      | 2.342 (0.028) | 0.246 (0.004) | 0.364 (0.006) |  1.340 (0.019) |\n| UT-IGSP     | Python    | 0.026 (0.002) | 0.040 (0.008) | 0.059 (0.017) |  0.607 (0.223) |\n| ICP         | R         | 0.190 (0.006) | 0.325 (0.004) | 1.436 (0.026) | 20.296 (0.421) |\n| ICP_CORTH   | R         | 1.662 (0.104) | 1.857 (0.122) | 3.759 (0.543) | 28.312 (7.061) |\n| IAS         | Python    | 0.016 (0.007) | 0.056 (0.010) | 0.112 (0.022) |  0.659 (0.132) |\n| MMSE-ICP    | Python    | 0.035 (0.010) | 0.067 (0.015) | 0.138 (0.027) |  0.932 (0.187) |\n| fastICP     | Python    | 0.028 (0.002) | 0.036 (0.002) | 0.070 (0.004) |  0.516 (0.030) |\n\nThis table has been added to Appendix C."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700001157957,
                "cdate": 1700001157957,
                "tmdate": 1700001157957,
                "mdate": 1700001157957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gARUhom3qR",
            "forum": "wKNKnXjCfT",
            "replyto": "wKNKnXjCfT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_ru9u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4290/Reviewer_ru9u"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the limitations of existing methods (e.g., ICP) that identify causal parents of a target via exploiting distribution shifts. Two approaches are proposed based on an error inequality and have good theoretic guarantees of identifiability of invariant variables. Experiments also show imporved results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Two novel algorithms for finding causal parents of a targets, with both theoretic and empirical validations.\n\n- Presentation is really good."
                },
                "weaknesses": {
                    "value": "Testing invariance may not be easy for general problems."
                },
                "questions": {
                    "value": "The paper writing is clear and I have a few questions/suggestions:\n\n- It is claimed that the number of interventions required can be much less than the number of variables in the graph, which improves ICP. In my understanding, this is because both observed information (utilized by traditional causal discovery method like GES and PC) and interventional information are used. Is this correct? Please highlight and discuss this point more in the paper.\n\n- in the experiment, fig. 5, ICP and the proposed methods have close F1, but ICP's recall is much lower. Does ICP have a higher precision in this case?\n- please use \\citet and \\citep separately. This really affects reading flow.\n\nOverall, I think this paper improves an existing method with both theoretic and empirical validations. The current version is in a good shape and only needs minor edits. I vote for an acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4290/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070887289,
            "cdate": 1699070887289,
            "tmdate": 1699636396819,
            "mdate": 1699636396819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1Q7Da2CZsN",
                "forum": "wKNKnXjCfT",
                "replyto": "gARUhom3qR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Testing invariance may not be easy for general problems.\n\nIt is true that leveraging invariance testing to build generalizable ML models in general problems has not been widely explored. However, this is partly due to the limitations of ICP since ICP (1) cannot tackle problems with many variables and (2) does not give informative outputs when there are insufficient interventions. These limitations have been explored in (Arjovsky et al. 2019); (Rosenfeld et al. 2021); and (Mogensen et al., 2022). The proposed fastICP addresses both of these limitations and therefore promises to expand the applicability of invariance testing. \n\nIn addition, our proposed approach does not require knowing which variables are intervened upon (unknown intervention targets) so it can be easily applied to problems where the data come from multiple domains/sites.\n\n- Arjovsky et al. \"Invariant risk minimization.\" 2019\n- Rosenfeld et al. \"The Risks of Invariant Risk Minimization.\" 2021\n- Mogensen et al. \"Invariant Ancestry Search.\" 2022\n\n> It is claimed that the number of interventions required can be much less than the number of variables in the graph, which improves ICP. In my understanding, this is because both observed information (utilized by traditional causal discovery methods like GES and PC) and interventional information are used. Is this correct? Please highlight and discuss this point more in the paper.\n\nICP, MMSE-ICP, and fastICP all use observational and interventional data.\nHowever, our methods differ from ICP in the way they pick out the set of causal parents from multiple detected invariant sets.\n\n- ICP takes the intersection of the invariant sets. When there are insufficient interventions, there can be multiple non-overlapping invariant sets so taking the intersection results in a non-informative output.\n- Our methods select the best set from the given invariant sets by leveraging the error inequality (Theorem 1). As our experiments demonstrate, this usually yields more informative outputs than ICP.\n\n> In the experiment, fig. 5, ICP and the proposed methods have close F1, but ICP's recall is much lower. Does ICP have a higher precision in this case?\n\nFor the result in Fig. 5, ICP only has higher precision when the number of samples is 1E5.\n\n> Please use \\citet and \\citep separately. This really affects reading flow.\n\nWe thank the reviewer for the suggestion. We have updated the paper accordingly.\n\n> Overall, I think this paper improves an existing method with both theoretical and empirical validations.\n\nWe thank the reviewer for the positive feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700000201330,
                "cdate": 1700000201330,
                "tmdate": 1700000201330,
                "mdate": 1700000201330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2SbI6zJaw0",
                "forum": "wKNKnXjCfT",
                "replyto": "1Q7Da2CZsN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ru9u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Reviewer_ru9u"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifications and I'm happy to see this paper be accepted."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311511814,
                "cdate": 1700311511814,
                "tmdate": 1700311511814,
                "mdate": 1700311511814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xFhO6PKydI",
                "forum": "wKNKnXjCfT",
                "replyto": "gARUhom3qR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4290/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the prompt response."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4290/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494594966,
                "cdate": 1700494594966,
                "tmdate": 1700494594966,
                "mdate": 1700494594966,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]