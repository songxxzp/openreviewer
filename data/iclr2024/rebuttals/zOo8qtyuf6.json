[
    {
        "title": "Towards Robust 3D Pose Transfer with Adversarial Learning"
    },
    {
        "review": {
            "id": "MSq6spNydF",
            "forum": "zOo8qtyuf6",
            "replyto": "zOo8qtyuf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_MJQ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_MJQ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to improve the robustness of the 3D pose transfer task, by modifying architecture designs and introducing adversarial samples during training. For architecture designs, their 3D-PoseMAE consists of multi-scale masking in the encoder and channel attention in the decoder to reduce the reliance on local geometry. For the exploration of adversarial samples, they propose an intuitive way to generate adversarial samples with existing PGD-based methods. Experiments on SMPL-NPT, FAUST and DFAUST show that the proposed method outperforms existing SOTA on noisy raw scans and unseen domains."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- They consider a more realistic scenario where the inputs are noisy, which proves to be challenging for previous methods.\n\n- They propose a complete pipeline to improve model robustness by generating adversarial samples and building a more practical model that can deal with them."
                },
                "weaknesses": {
                    "value": "- [Masking] Sec.2.1 claims that existing 3D MAE-based works use multiple subsets to conduct the masking and are not applicable to large-size point clouds. Instead, the proposed multi-scale masking strategy overcomes this challenge. Based on Sec.2.1, the paper highlights their multi-scale masking. However, (Zhang et al., 2022)(add: Yang etal 2023) also use the multi-scale encoder. Therefore, I am not sure why the proposed masking strategy is good for large-size point clouds. For the proposed ratio masking, the description is vague. What are the details of ratio masking? Especially, (add: Yang etal 2023) introduces block/patch/point-wise masking. In this case, it would be better to show the nature of the design to handle large-size point clouds, compared to multi-scale masking methods.\n\n\n- [Channel-wise Attention] For the design of the decoder, this paper highlights that there are redundant local geometric representations like wrinkles and small tissues. Therefore, the decoder prefers to use channel-wise attention. It is true that there are redundant local geometric representations. I am not sure if totally disregarding local geometric representations with channel-wise attention is reasonable. Based on (add: Hermosilla et al 2019), it seems a multi-level receptive field is a good choice. The channel-wise only attention can be treated as an extreme case. Moreover, after Eq.2, the paper claims *the fine-grained spatial geometric information of the target meshes is preserved by the gradual integration*. Does this mean the decoder tries to learn spatial and geometric information from multi-scale information? This part needs to be introduced in detail.\n\n\n- [Denoising] Multi-scaling and channel attention are both performing denoising. It would be better to discuss the proposed method in terms of denoising.\n\n- [Adversarial Training and Evaluation] For all the methods w/wo adversarial training, Tab.2 should report the evaluation results on clean data and adversarial samples.\n\n- [Eq.5] Adversarial samples of regression are unlike those of classification. The boundaries among different categories are not applicable for regression tasks. Therefore, Eq.4 does not hold because there is rarely equality in regression tasks. Similarly, Eq.5 is hard to achieve. Because **the direction** and **the magnitude** of adversarial samples are arbitrary, This part should be carefully formulated and discussed. It would be better to discuss adversarial learning (add: Wang et al 2021) or adversarial samples (add: Jain et al 2019) of regression tasks.\n\n- [Motivation] This paper introduces adversarial learning, multi-scaling, and channel attention, which improve the robustness of the model. Those techniques are commonly used for point cloud denoising but seem not to be specially designed for pose transfer. It would be better to highlight the connection between those techniques and pose transfer.\n\n- [Generalization] It would be better to evaluate on unseen datasets to validate the robustness and the generalization, such as SMPL-NPT -> FAUST.\n\n\n\nMinor\n\n- Sec 2.2 refers to Eq.7 and Eq.8, which are in the appendix. It would be better to show the important formulas in the main paper.\n\nReference\n\n- (add: Yang et al 2023) GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds. CVPR2023.\n- (add: Hermosilla et al 2019) Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning. ICCV2019.\n- (add: Wang et al 2021) When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks. CVPR2021.\n- (add: Jain et al 2019) On the Robustness of Human Pose Estimation. CVPRW2019."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677469199,
            "cdate": 1698677469199,
            "tmdate": 1699636877539,
            "mdate": 1699636877539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "TWn5C23i56",
            "forum": "zOo8qtyuf6",
            "replyto": "zOo8qtyuf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_bbpQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_bbpQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces adversarial learning to the task of 3D pose transfer to improve the generalization capacity. The adversarial samples are generated by minimizing a reversed form of a reconstruction loss between the prediction and the ground truth poses. The authors further propose a channel-wise attention operation based on existing masked autoencoding architecture to learn a more compact and efficient 3D pose representation. Experimental results on both clean and noisy inputs show the better performance of the proposed method compared with existing 3D pose transfer techniques."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper tackles a very practical problem in 3D pose transfer, which is to improve the robustness of 3D pose transfer models such that it can be applied to noisy inputs or raw data directly without tedious preprocessing. \n2. The proposed method achieves significant improvements over existing 3D pose transfer methods for both clean and noisy inputs."
                },
                "weaknesses": {
                    "value": "1. The paper is not well structured. Related works are not discussed separately in the paper.  The method part is a bit redundant, especially the section on adversarial training. The motivation, problem definition and also the magnitude of adversarial samples can actually be written in a more precise way. \n2. The technical contribution of the proposed method is a bit incremental. The main contributions are the introduction of adversarial training and channel-wise attention, which are adopted from existing works with some modifications. \n3. Some related works on 3D pose transfer are lacking. \n[a] Keyang Zhou, Bharat Lal Bhatnagar, and Gerard Pons- Moll. Unsupervised shape and pose disentanglement for 3d meshes. In European Conference on Computer Vision (ECCV), 2020.\n[b] Jinnan Chen Chen Li Gim Hee Lee. Weakly-supervised 3D Pose Transfer with Keypoints. ICCV 2023\n4. Some equations and figures are referred to wrongly. For example, Eq.7 and Eq.8 on the 6th page, Fig.8 on the 9th page."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7334/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7334/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7334/Reviewer_bbpQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741579743,
            "cdate": 1698741579743,
            "tmdate": 1699636877372,
            "mdate": 1699636877372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "STYVVcPKtD",
            "forum": "zOo8qtyuf6",
            "replyto": "zOo8qtyuf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_Sf1p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_Sf1p"
            ],
            "content": {
                "summary": {
                    "value": "A method for 3D pose transfer is proposed in this manuscript. There are three main contibutions. The multi-scale masked encoder for 3D pose reconstruction learning, the 3D-PoseMAE decoder with Channel-wise attention which is designed for efficient pose transfer learning without focusing on redundant local geometric representations (such as wrinkles, small tissues), and a adversarial samples generation method. The proposed method is evaluated on three benchmark datasets, and extensive experiments are conducted for evaluated the performance of it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method achieves state-of-the-art performance.\n2. Extensive experiments are conducted to show different aspects of the proposed method.\n3. Some pratical attempts, such as the muti-scale masking strategy, are introduced in this paper."
                },
                "weaknesses": {
                    "value": "1. The novelty of the proposed method is limitted. Similar ideas, such as multi-scale learning, channel-wise attention, and the objectives for adversarial sample generation in Eq.5 can also be seen in other papers of similar tasks [1,2]. The proposed ideas are good attempts for application, but are not novel enough for academic publication.\n2. For the 3D pose transfer task, to generate 3D pose that is similar to the ground truth is one of the goal. Anohter goal should be generating a physically convincing 3D poses. For example, interpenetration usually happens when the input 3D pose shape is skinny and the shape of the output pose is fat. The transferred pose should keep the motion semantics of the input pose and also avoids introducing flaws such as interpenetration [3]. The flaw avoidance part is missed by this manuscript.\n3. Some minors. For example in page 6, Eq.4 and Eq.5 are wrongly marked as Eq. 7 and Eq. 8.\n\n[1] Villegas, R., Yang, J., Ceylan, D., & Lee, H. (2018). Neural kinematic networks for unsupervised motion retargetting. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8639-8648)\n[2] Rempe, D., Birdal, T., Hertzmann, A., Yang, J., Sridhar, S., & Guibas, L. J. (2021). Humor: 3d human motion model for robust pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 11488-11499).\n[3] Zhang, J., Weng, J., Kang, D., Zhao, F., Huang, S., Zhe, X., ... & Tu, Z. (2023). Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13864-13872)."
                },
                "questions": {
                    "value": "1. As the proposed method is evalauted on the digital human body dataset, the intrinsic structure of human body is well-learned. I am wondering whether the proposed method could also achieve good performace on cartoon mesh, for example the mixamo dataset. As for the cartoon character, the transfer of mesh details such hair, cloth are also important.\n2. What about trained on human body and test on cartoon data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769265957,
            "cdate": 1698769265957,
            "tmdate": 1699636877237,
            "mdate": 1699636877237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "dxVISbp0Bo",
            "forum": "zOo8qtyuf6",
            "replyto": "zOo8qtyuf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_qTTF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_qTTF"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to improve the model's robustness to the input in 3D pose transfer task. An adversarial sample based training pipeline is introduced, together with a novel 3D pose masked autoencoder. The generating of adversarial examples and multi-scale masking are introduced to make the 3D pose transfer robust to the noise in input poses and even further able to be extended to real-world data like raw point clouds/scans. Experimental results demonstrate better quality of the transferred meshes and model generalizability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The multi-scale masked 3D encoder could save computation as the aggregated feature is calculated.\n\nThe 3D poseMAE decoder with channel-wise attention is different from the spatial attention in the GC-Transformer. But the contribution is minor.\n\nPromising experimental results are demonstrated."
                },
                "weaknesses": {
                    "value": "The masking in multi-scale masked 3D encoder is similar to random sampling the point cloud for encoding, but in the concept of mask auto-encoder. The idea of masking does not share much with the idea of He et al, 2022, which applies heavy masking in the input and predicts the masked content. \n\nThe idea of adversarial training for 3D pose transfer does not make sense. The adversarial training in computer vision tasks mostly has different input and output spaces, e.g. image space and semantic space for classification task. The adversarial training then aims to make the output space robust to small perturbation in the input space. However, in the 3D pose transfer task, the input and output spaces are the same. Any perturbation on the input mesh should be reflected in the output mesh. \n\nThe overall process of the adversarial training is not clear. As I understand, there should be a function for the generation of adversarial samples, which I suppose to be equation 5. There should be another loss function to train the model with both the original input samples and the adversarial samples. However, the design of this loss function is missing. The mentioned equation is not the function that will be used. \n\nThe generation of adversarial samples seems to be very inefficient.\n\nThe main content is expected to be self-contained. However, Eq. 8 is mentioned on page 6, but described in the appendix.\nThe texts in Figure 3 are too small to recognize, especially when printed on paper."
                },
                "questions": {
                    "value": "Is there any difference between the masking in multiscale masked 3D encoder with point cloud random sampling? Are other sampling methods, like FPS based sampling considered in applying the masking?\n\nWhat is the ratio of easy samples that are filtered out by SOR?\n\nCan you provide some examples of the adversarial samples? How to set the magnitude of the perturbation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698974523307,
            "cdate": 1698974523307,
            "tmdate": 1699636877119,
            "mdate": 1699636877119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "GHANrsXnpj",
            "forum": "zOo8qtyuf6",
            "replyto": "zOo8qtyuf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_yQax"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7334/Reviewer_yQax"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose adversarial training for the task of 3D pose transfer using a novel architecture based on multi-scale MAE and channel-wise attention in the decoder. In particular, they rely on dynamic adversarial samples-based data augmentation (computed on the fly for every batch) to conduct adversarial training. The authors empirically validate the benefit of their proposed approach on three datasets and show that their method outperforms the considered baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The presentation of the paper is good.\n- Individual components in the proposed architecture are detailed sufficiently."
                },
                "weaknesses": {
                    "value": "- Limited novelty, as both components, MAE and adversarial training, are well-known in the literature. In particular, the adversarial training performed in the current setting is well-known in image classification literature where the adversarial samples are generated for every batch.\n\n- The core novel additions in the current paper are multi-scale MAE and channel-wise attention in the decoding block.  Although, these provide benefit in performance, frameworks such as Multi-scale MAE has already been proposed in literature [a]. \n\n- Besides, evaluation against adversarial attacks is incomplete (only considering white-box attacks). \n\n- There is no proper related work section in the paper. For instance, it is difficult for the reader to understand the considered baselines such as NPT, GC-transformer, and the critical differences. Table 4 is helpful, but the authors should consider adding a section after the introduction. I suggest adding a related work section on 3D pose transfer methods, 3D pose data augmentation methods, and 3D adversarial training and defenses. In the current version, the related work is quite sparse.\n\n\nReferences:\n\na. Zhang, R., Guo, Z., Gao, P., Fang, R., Zhao, B., Wang, D., ... & Li, H. (2022). Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training. Advances in neural information processing systems, 35, 27061-27074."
                },
                "questions": {
                    "value": "- Please clarify the attack performed in Table 2 for evaluation against adversarial samples. Is this a FGM or PGD attack? \n\n- In Table 2, what is the performance of adversarial training on the clean samples? Do you observe a standard accuracy-robustness tradeoff in this task? \n\n- My other concern is that FGSM+AT is shown to be vulnerable to PGD attacks and also leads to catastrophic overfitting (CO) during training [a]. As such, several improvements [b,c,d, e] to FGSM+AT have been studied to enhance performance against adversarial attacks. In my view, the authors should carefully assess the robustness of FGSM+AT against strong attacks and also clarify categorically if they observed CO during training\n\n- What is the performance on clean and adversarial samples with different attacking budgets in FGM training? How to set this attack budget to achieve the proper balance between performance on clean and adversarial samples?\n\n\n- Authors should conduct an ablation with more advanced data-augmentation strategies to understand the true benefit from adversarial training (AT). Maybe adding noise to the input during training can also be considered as a way to decrease overfitting. There are also advanced pose augmentation methods such as [f] and maybe even more advanced ones. The authors should benchmark against these augmentation strategies and show the benefits of AT compared to them.\n\nOverall, the paper lacks thorough evaluation with strong data augmentation schemes with training on the clean samples, nor does it show that the proposed adversarial training (AT) is robust to the strongest of the attacks.\n\na. Rice, L., Wong, E., & Kolter, Z. (2020, November). Overfitting in adversarially robust deep learning. In International Conference on Machine Learning (pp. 8093-8104). PMLR.\n\nb. Andriushchenko, M., & Flammarion, N. (2020). Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems, 33, 16048-16059.\n\nc. Vivek, B. S., & Babu, R. V. (2020, June). Single-step adversarial training with dropout scheduling. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 947-956). IEEE.\n\nd. Li, B., Wang, S., Jana, S., & Carin, L. (2020). Towards understanding fast adversarial training. arXiv preprint arXiv:2006.03089.\n\ne. Kim, H., Lee, W., & Lee, J. (2021, May). Understanding catastrophic overfitting in single-step adversarial training. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 9, pp. 8119-8127).\n\nf. Gong, K., Zhang, J., & Feng, J. (2021). Poseaug: A differentiable pose augmentation framework for 3d human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8575-8584)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7334/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699162694164,
            "cdate": 1699162694164,
            "tmdate": 1699636876966,
            "mdate": 1699636876966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]