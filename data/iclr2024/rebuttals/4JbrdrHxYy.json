[
    {
        "title": "The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models"
    },
    {
        "review": {
            "id": "gEgOqNhbtA",
            "forum": "4JbrdrHxYy",
            "replyto": "4JbrdrHxYy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed the annotation-free instance segmentation method using vision foundation models (i.e., SAM and CLIP). They found that SAM shows high recall rates but relatively low precision. Also, DINO is suitable to obtain salient regions but is not suitable to obtain instance-level masks. To address the challenges, they discovered that clustering the features of CLIP's specific middle layer can be effectively used for adequate prompts for SAM. The class of each instance mask is classified using the CLIP. Consequently, the proposed method outperforms the previous state-of-the-art annotation-free instance segmentation methods. In addition, it shows a competitive performance to existing open vocabulary object detection methods without any annotations."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "## 1. Great motivation and findings.\n\nI strongly agree with the motivation of this work.\nNamely, SAM itself may not be suitable for instance segmentation on COCO due to the high recall and low precision rates. \nAlso, DINO may be suitable for salient object detection, but not for instance segmentation.\nMotivated by the limitations of vision foundation models, this paper designed a new pipeline for annotation-free instance segmentation using fine-grained features from CLIP and SAM.\n\nIn addition, the proposed classification-first-then-discovery pipeline is convincing in resolving CLIP's misclassifying issue.\n\n## 2. Well-structured and well-written paper\n\nI enjoyed reading this paper because the motivation is clear and understandable, the proposed pipeline is well-explained with proper illustrations.\n\n## 3. Outstanding performance\n\nCombined with CLIP and SAM, the proposed method outperforms the existing methods by a large margin on COCO dataset.\nEach proposed component is well-ablated."
                },
                "weaknesses": {
                    "value": "## 1. Limited technical novelty\n\nI feel that leveraging the features on particular middle layers of CLIP and applying K-Means clustering on CLIP's features are not technically novel but well-engineered.\n\nOf course, the proposed annotation-free instance segmentation pipeline is interesting.\nThis paper seems to be an engineering paper that addresses how to effectively use CLIP and SAM and is not far from an academic paper.\n\n## 2. Behind the outstanding performance.\n\nI think the key feature of the proposed method is how to properly cluster objects in an image (because we regard that SAM can segment anything when bounding box prompts are properly introduced).\nAt first, I was surprised by the outstanding performance of the proposed method on COCO 2017 dataset.\nHowever, there is a question that the outstanding performance was due to the well-aligned semantic domain between the features of CLIP and the COCO dataset; the clustered object boundary from CLIP may be well-aligned to the ground-truth object region in COCO dataset.\nI wonder if the target dataset was human part segmentation or medical segmentation, the CLIP features are still valid for annotation-free instance segmentation.\nIt would be great if this paper discuss it."
                },
                "questions": {
                    "value": "Due to some concerns in the Weakness part, my initial rating is borderline.\nAfter a discussion with the authors and other reviewers, I will finalize the rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper includes ethical considerations and potential negative social impacts, and there don't seem to be any issues to worry about."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649323450,
            "cdate": 1698649323450,
            "tmdate": 1699636123036,
            "mdate": 1699636123036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EdeWCZVVbt",
                "forum": "4JbrdrHxYy",
                "replyto": "gEgOqNhbtA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CLaV"
                    },
                    "comment": {
                        "value": "> W1: Limited technical novelty. I feel that leveraging the features on particular middle layers of CLIP and applying K-Means clustering on CLIP's features are not technically novel but well-engineered. Of course, the proposed annotation-free instance segmentation pipeline is interesting. This paper seems to be an engineering paper that addresses how to effectively use CLIP and SAM and is not far from an academic paper.\n> \n\nThank you for your thoughtful review and your comments regarding our technical novelty.  We understand your perspective and acknowledge that the application of K-Means clustering to CLIP\u2019s features may seem like an engineered technique when viewed solely from the algorithm execution process. We would like to clarify the technical novelty and contribution that underpin our work as follows:\n\n- **Semantic-Aware Initialization:** we propose a novel semantic-aware initialization technique to leverage semantic activation of CLIP to automatically initialize clustering centers and determine the number of clusters. Without our semantic-aware initialization, K-Means fails to produce general and reliable clustering results for delineating object boundaries. As shown in Figure 5, despite clustering the same intermediate features in CLIP, it is only when utilizing the proposed initialization technique that stable clustering and boundary discovery are achieved. The core motivation behind semantic-aware initialization is that by **initializing certain clustering centers near the outer edges** of the semantic activation map, **boundaries between objects within the activation map** can be clustered and outlined (Please refer to **Appendix L** for a comprehensive explanation). Semantic-aware initialization is a unique technical contribution that sets our work apart from conventional clustering approaches.\n- **Classification-First-Then-Discovery Pipeline:** we propose a classification-first-then-discovery pipeline, which represents a novel paradigm in the field of annotation-free instance segmentation. In the pipeline, we introduce a boundary metric to identify boundaries and transform the instance segmentation challenge into a fragment selection task. This pipeline has not been explored in prior work and offers a fresh perspective on solving the instance segmentation problem.\n- **The Boundary Discovery Property of CLIP**: we are the first to discover that CLIP's specific intermediate layers can delineate object edges, marking an interesting academic finding rather than just an engineering achievement. As discussed in Appendix C.1, we believe that the hierarchical representation structure, distinctive pretraining approach, and training data may be the reasons why CLIP's intermediate features have the potential for instance-level localization. Similar to the salient object discovery capability of the DINO model and the presence of in-context learning and prompting learning in LLM, we hope that this discovery can assist the academic community in better understanding CLIP and exploring its various features and underlying principles of contrastive language-image pre-training.\n- **Clear Motivation and In-depth Analysis with Academic Significance:** Instead of simply combining SAM and CLIP, we conduct a comprehensive analysis of DINO, SAM, and CLIP. This allows us to clearly illustrate our motivation for effectively leveraging foundation models, rather than superficially combining them without thorough consideration. We believe that the effective utilization of CLIP and SAM for annotation-free instance segmentation, as demonstrated in our paper, is a valuable contribution to both the engineering and academic communities.\n- **Simple yet Super Effective and Efficient:** Through our in-depth analysis and well-explored motivation, we were able to propose a method that is both super simple but also super effective. Furthermore, thanks to its simplicity, it is also highly efficient, achieving a 10-fold increase in processing speed compared to previous state-of-the-art unsupervised object discovery methods like CutLER.\n\n> W2: I wonder if the target dataset was human part segmentation or medical segmentation, the CLIP features are still valid for annotation-free instance segmentation.\n> \n\nIn addition to the COCO dataset, we apply the same clustering algorithm and exactly the same hyperparameters on the Pascal VOC dataset (as shown in Table 3), and achieve consistently positive results. \n\nFollowing your suggestion, we also conducted an evaluation of our method on the human part segmentation task, as depicted in the newly added **Figure 12**. We employ the same clustering method and hyperparameters for this evaluation, and the results further demonstrate the generality of our discovery and the robustness of our approach."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147413085,
                "cdate": 1700147413085,
                "tmdate": 1700147413085,
                "mdate": 1700147413085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6mZaPecx5I",
            "forum": "4JbrdrHxYy",
            "replyto": "4JbrdrHxYy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_5mwu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_5mwu"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method called ZIP, which\ncombines CLip and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free,\n complex-scene-capable, open-vocabulary object detection and instance segmentation.\n\nZip establishes state-of-the-art performance in various settings, including training-free, self-training, \nand label-efficient finetuning.\n\nThe overall idea is based on a discovery that CLIP can provide a highly\nbeneficial and strong instance-level boundary prior in the clustering results of its\nparticular intermediate layer.\n\nThe proposed method combines the CLIP classification score with the SAM localization score using a geometric mean. \nThis refinement boosts the confidence scores of masks, achieving a slightly higher precision."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good ZSL model proposed- combining CLIP and SAM, and giving semantics meaning to edges.\n\nLarge amount of experimental results, exhibiting superiority with SoA, ablation studies etc\nprovided for benchmark datasets.\nIllustrations of results are vivid and eye-catching -= say in Fig. 10."
                },
                "weaknesses": {
                    "value": "The statement used - The devil is in the object boundaries;\nis it not in line with a part work, which hypothesized - \"all you need are priors\"?\n\nAlthough the  analytics presented in Eqns. (1) - (7) are clear, unable to find much of novelty in them.\neg Cosine similarity, boundary score etc.\n\nWhat about inference ? How many GPUs are required, and what is the timing then ?\nI could not locate that information in document.\n\nA generic opinion:\nThe recent trend of using a large GPU cluster to implement detectors/segmentors is something of a concern, I believe.\nIts like exploiting massive computing power to solve a problem. Either use of a large set of activation/attention heads, \\\nor for pre-training tasks in self-supervision, meta-learning are the main reasons of using so. What is the limit now ? \nThe focus is shifting from devising novel learning algorithms to use of larger to mega-clusters."
                },
                "questions": {
                    "value": "Page 2 - the line:\n....higher precision yet not satisfaction enough.\nmay be corrected (English).\n\nYou state - \"classification-first-then-discovery pipeline\"; in page 3:\nWhat if the reverse is done ? Edges in images, as boundaries of objects, may hypothetically lie\non Decision boundaries between binary classes. So discover the edges first may be a better approach, rather than\nrelying on classification to provide those edges ?\n\nDino (also Clip?) uses transformers (ViT) , right ?\n- is that the reason for the need for large computational resource to solve your problem,\nwhen you combine the two?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678387869,
            "cdate": 1698678387869,
            "tmdate": 1699636122963,
            "mdate": 1699636122963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GdhqeXpTB7",
                "forum": "4JbrdrHxYy",
                "replyto": "6mZaPecx5I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5mwu"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. In order to facilitate a prompt discussion and exchange of ideas, we now address your most critical concerns first. We will respond to all questions and revise the paper as soon as possible. Thanks for your understanding!\n\n> W1: The statement used - The devil is in the object boundaries; is it not in line with a part work, which hypothesized - \"all you need are priors\"?\n> \n\nThank you for your valuable suggestions! In fact, \"all you need are priors\" is among the title candidates we consider. However, we ultimately choose \"the devil is in the object boundaries\" because we want to highlight that discerning object boundaries is the crucial aspect of how to empower foundation models for annotation-free instance segmentation. \n\nWhile we recognize that \"the devil is in the object boundaries\" may not be the optimal choice, we will continue to explore a more suitable title (such as \"Unveiling The Object Boundary: Empower Foundation Models Towards Annotation-free Instance Segmentation\"). If you have any suggestions or recommendations, please feel free to share them with us. Thank you!\n\n> W2: Although the analytics presented in Eqns. (1) - (7) are clear, unable to find much of novelty in them. eg Cosine similarity, boundary score etc.\n> \n\nAlthough the implementation of our Zip may seem straightforward, involving simple calculations in Equations (1)-(7), the observations, motivations, technical contributions, and innovations behind Zip are far from trivial. \n\nMoreover, it is precisely due to its simplicity that our method not only demonstrates effectiveness in terms of performance but also remarkable efficiency. When compared to the previous best-performing method, CutLER, under default settings, our algorithm typically delivers a 10 times increase in processing speed.\n\nIn addition, our novelies are summary as follows:\n\n- We are the **FIRST to effectively utilize foundation models (CLIP and SAM) for annotation-free instance segmentation.** Such effective utilization is not trivial and cannot be achieved by a naive combination. Simply combining SAM and CLIP results in unsatisfactory performance due to SAM's semantic-unaware issue of generating masks with varying degrees of granularity yet lacking instance-aware discernment (see Appendix B) and CLIP's misclassification issue (see Appendix F). Based on these observations, we propose that the **key to effectively utilizing foundation models for the instance segmentation task is to probe foundation models to delineate the boundaries between individual objects!**\n- We are the **FIRST to discover CLIP's special intermediate layers can outline object edges** through the proposed **novel semantic-aware feature clustering**. By incorporating this discovery, we address challenges in complex scenarios with multiple instances, under annotation-free instance segmentation. To ensure stable and general clustering of object edges, we devise a semantic-aware clustering initialization technique, which is crucial.\n- We are the **FIRST to propose a novel classification-first-then-discovery pipeline** for annotation-free instance segmentation task. Mere identification of boundary information does not inherently and straightforwardly aid instance segmentation without our classification-first-then-discovery pipeline. In the pipeline, we introduce a boundary metric to identify boundaries and transform the instance segmentation challenge into a fragment selection task, a novel approach not explored by previous work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147173535,
                "cdate": 1700147173535,
                "tmdate": 1700147473170,
                "mdate": 1700147473170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cMaye1qZlF",
            "forum": "4JbrdrHxYy",
            "replyto": "4JbrdrHxYy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze the weakness of the previous computer vision foundation models. By discovering the instance-level understanding of CLIP, the authors then utlize it to propose a novel training-free method to address the issues by clustering the activation maps from CLIP. Zip demonstrates significant performance improvements compared to the previous state-of-the-art on the COCO dataset across various settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The idea is novel, inspiring and useful.\n2. A detailed analysis of the issues in previous works is provided.\n3. Significant improvements over previous state-of-the-art on various COCO settings.\n4. The authors are aware and thoroughly investigate the weaknesses and limitations of the work."
                },
                "weaknesses": {
                    "value": "1. It is challenging to follow the explanation of how the clustering works. It would be helpful if the authors could include some figures to illustrate this process more clearly.\n2. The robustness of the clustering algorithm is concerning when applying to other datasets or settings."
                },
                "questions": {
                    "value": "- Q1. The work is super interesting, but it takes me a long time to understand how the clustering works.\n   - What do the colors represented in \"Clustering\" in Fig.3?\n   - How does the clustering algorithm draw the boundaries in Fig.2 and Fig.3?\n   - Taking \"Semantic Clues\" in Fig. 3 as example, what will it look like after applying Eq. 5?  I assume that there will be a single box surrounded all the green parts? How can instances be seperated after that?\n   - The authors stated that after obtaining the feature maps for each category, it will then run the clustering algorithm. Please clarify how to run the algorithm and why it will work.\n- Q2. How stable is the clustering algorithm? Will the results change drastically if optimal hyperparameters are not found? For example, which intermediate layer should be used? Is there any experiment on how the choice of $K$ will affect the results? Are there any guidelines? Please also provide some insights for Eq.7.\n- Q3. Is it appropriate to fully adopt the setting from CutLER? To my understanding, CutLER does not involve any annotation during training. While Zip also does not use annotation from COCO, but it did exploit the language information from CLIP. \n- Q4. Is it possible to compare Zip to other open-vocabulary works, such as Openseed[1]? I am aware that Zip is annotation-free, and the authors have stated that it will be a future work to utilize annotations at Section J. However, the work would be more convincing and useful if it is possible to somehow utilize annotations from COCO since the raw performance of Zip is still significantly behind Mask-RCNN.\n\nI belive it is an interesting and promising work. However, the explanation of clustering is not clear enough. I am willing to raise my score if the authors can reasonably address my concerns and questions.\n\n[1] Zhang, Hao, et al. \"A simple framework for open-vocabulary segmentation and detection.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD",
                        "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695403932,
            "cdate": 1698695403932,
            "tmdate": 1700623194875,
            "mdate": 1700623194875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K7SJkfSh6f",
                "forum": "4JbrdrHxYy",
                "replyto": "cMaye1qZlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WVZD"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. In order to facilitate a prompt discussion and exchange of ideas, we now address your most critical concerns first. We will respond to all questions and revise the paper as soon as possible. Thanks for your understanding!\n\n> W1+Q1: It is challenging to follow the explanation of how the clustering works. It would be helpful if the authors could include some figures to illustrate this process more clearly.\n> \n\nThank you for your valuable suggestions! We have added an in-depth explanation of the clustering process in **Appendix L**. Please refer to it for a detailed understanding. The core of the clustering process is our semantic-aware initialization, which leverages the semantic activation map to automatically initialize clustering centers and determine the number of clusters for K-Means. \n\nThe core motivation behind semantic-aware initialization is that **by initializing some clustering centers near the outer edges of the semantic activation map, boundaries between objects within the activation map can even be clustered and outlined.** Despite the semantic activation map's inability to differentiate instances with the same classes, its edges continue to signify the boundaries between objects of distinct classes. By initializing some clustering centers near these edges (specifically, the outer edges of the semantic activation map), we can effectively group together the boundaries between objects from **BOTH** the same and different classes into one cluster (i.e., the \"boundary\" cluster) due to their high feature similarity on the specific intermediate layer of CLIP. This results in the boundaries of objects from the same class also being identified by the \"boundary\" cluster, thereby enable the distinction of these objects through the \"boundary\" cluster. As shown in the Figure 5, even though the middle three figures cluster the same intermediate features in CLIP, only clustering with our semantic-aware initialization can deliver stable clustering and boundary discovery. \n\n> W2: The robustness of the clustering algorithm is concerning when applying to other datasets or settings.\n> \n\nIn addition to the COCO dataset, we apply the same clustering algorithm and exactly the same hyperparameters on the Pascal VOC dataset (as shown in Table 3), and achieve consistently positive results. \n\nWe also conduct an evaluation of our method on the human part segmentation task, as depicted in the newly added **Figure 12**. We employ the same clustering method and hyperparameters for this evaluation, and the results further demonstrate the robustness of our approach. \n\n> Q2: How stable is the clustering algorithm? Will the results change drastically if optimal hyperparameters are not found? For example, which intermediate layer should be used? Is there any experiment on how the choice of\u00a0\u00a0K will affect the results? Are there any guidelines? Please also provide some insights for Eq.7.\n> \n\nThanks for the valuable feedback. We agree that hyperparameter tuning and large search space are challenging in most unsupervised methods. As a reference, the previous SOTA CutLER has nine hyperparameters. \n\nEssentially, our clustering algorithm has two hyperparameters. Most importantly, these hyperparameters are relatively independent, easy to determine, and consistent across different datasets and experimental settings. \n\n- The setting of threshold $\\tau$=0.7 in Equ (5) is consistent across different datasets. The $\\tau$ determines whether the pixel blocks are activated given a category, i.e., the threshold to judge whether pixel blocks are the category's foreground or background. It is an unavoidable hyper-parameter to binarize a similar/confidence map to a binary mask in the previous unsupervised object detection methods. Fortunately, our approach only needs a coarse binary mask to help initialize cluster centers instead of a precise one for final prediction, increasing the robustness to $\\tau$ and setting $\\tau$ consistently over different tasks and datasets.\n- The threshold in Equ (7) is set to 0.15 in all the experiments. Equ (7) roughly estimates the number of clusters based on estimating the number of object categories in single images. In reality, our method only needs to set the number of clusters within a certain range (15~30) to work effectively, thanks to our reformulation instance segmentation by fragment selection.\nOur method is even robust to over-segmentation caused by a slightly larger number of clusters (as long as it is not too small) because our subsequent fragment selection (Sec 3.4) can regroup over-segmented fragments into an instance.\n| Pascal VOC | AP50 | AR100 |\n| --- | --- | --- |\n| K=15 | 21.9 | 25.5 |\n| Ours | 22.4 | 28.4 |\n| K=30 | 21.1 | 26.6 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147043290,
                "cdate": 1700147043290,
                "tmdate": 1700147043290,
                "mdate": 1700147043290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MHNV1g4zG5",
                "forum": "4JbrdrHxYy",
                "replyto": "cMaye1qZlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed replies. I have no more questions and willing to raise my score.\nHere are some minor issues.\n1. In Appendix L, should Figure 13 be Figure 11 instead?\n2. I assume the outer edge at Figure 11(d) should be the contour of the oranges not the brown box of the images? It is somewhat confusing.\n3. The discussion of which layer to use in different scenarios is still lacking.\n4. The explanations of semantic aware initialization should be intergrated for the final version. I can't understand how it work without looking at the appendix."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581027859,
                "cdate": 1700581027859,
                "tmdate": 1700582259872,
                "mdate": 1700582259872,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cxAeQOrRHl",
            "forum": "4JbrdrHxYy",
            "replyto": "4JbrdrHxYy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to annotation-free instance segmentation. The authors identify that a specific middle-level feature in CLIP encapsulates instance-level boundary information. Leveraging this insight, they develop a pipeline named \"Zip\" for annotation-free instance segmentation. The paper also pioneers a \"classification-first-then-discovery\" paradigm. Through extensive experiments, the proposed method significantly outperforms its baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The utilization of CLIP's middle-level feature to generate instance proposals presents a compelling and novel approach. This stands in stark contrast to traditional RPN-based or embedding-based proposal generation methods, offering a fresh perspective in the realm of annotation-free instance segmentation. It also provide comprehensive experiment results to prove its leading performance."
                },
                "weaknesses": {
                    "value": "1. **Figure 1's Layout**: The sequencing in Figure 1 is somewhat perplexing. A more intuitive order, such as DINO, Prev. SOTA, SAM, Clustering, and then Ours, might enhance clarity. Additionally, specifying the exact method when referencing \"Prev. SOTA\" would provide better context to the readers.\n\n2. **Figure 2's Clarity**: The visualization in Figure 2 appears cluttered, making it challenging to discern the boundaries in images labeled A through D. Simplifying or enhancing the contrast could make the distinctions more evident.\n\n3. **Understanding Figure 3**: Figure 3 is somewhat intricate, especially when trying to comprehend the roles of \"activation\" and \"boundary.\" A more detailed caption or a supplementary explanation might aid in understanding.\n\n4. **Discussion on DINO and SAM**: While it's essential to provide context, the extensive space dedicated to discussing the limitations of DINO and SAM might be excessive. Streamlining this section could make the paper more concise and focused on the primary contributions."
                },
                "questions": {
                    "value": "1. **Semantic-Aware Initialization**: Could you provide a more in-depth explanation of the Semantic-Aware Initialization mentioned on page 17? Specifically, I'm unclear about how the parameters for K-means clustering are determined.\n\n2. **Visualization in Figure 2**: The differentiation between gray and orange in Figure 2 could be clearer. Enhancing the contrast by making the background color more transparent might help in better distinguishing the two.\n\n3. **Ambiguous Object Representation**: How does the middle-level clustering handle images where the concept of an object is ambiguous? For instance, in scenarios like a box with a person's image on one side or stacked blocks. In such cases, it's debatable whether the person's image should be considered a separate instance or if each block in the stack should be individually segmented.\n\n4. **Handling Occlusions**: How does your method address situations where an object is partially occluded and appears as two separate parts? For example, if a dog is behind a tree, would your system recognize both parts as belonging to the same object?\n\n5. **Locating Middle-Level Features**: I'm intrigued about the process of pinpointing the middle-level feature. Is the method you've employed reproducible across different training weights? For instance, would the results be consistent if you were to use the openCLIP model (or the official CLIP model, if you've already utilized openCLIP)?\n\n6. **Exploring DINO/DINOv2 Features**: Have you considered using the middle-level features from DINO or DINOv2? Given that MaskCut can discern between closely packed objects using DINO features, I wonder if DINO/DINOv2 might also possess a middle-level feature adept at boundary extraction.\n\n7. **Resolution of Middle-Level Feature**: The resolution of your middle-level feature appears to be quite high. Could you specify the image input size and the dimensions of your middle-level feature?\n\n8. **Self-Training Visualization**: It might be beneficial to include a comprehensive visual representation of the self-training process. Relying solely on textual differences compared to CutLER and directing readers to refer to CutLER might not be the most user-friendly approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730923281,
            "cdate": 1698730923281,
            "tmdate": 1699636122738,
            "mdate": 1699636122738,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "egmGRLpXaI",
                "forum": "4JbrdrHxYy",
                "replyto": "cxAeQOrRHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g6s6"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. In order to facilitate a prompt discussion and exchange of ideas, we now address your most critical concerns first. We will respond to all questions and revise the paper as soon as possible. Thanks for your understanding!\n\n> W1+W2+Q2:  Figure 1's Layout, Figure 2's Clarity, and Visualization in Figure 2\n> \n\nThanks for your suggestion. We will improve the alignment in Figure 1 and enhance the clarity of boundaries in Figure 2 to enhance visualization.\n\n> W3: Understanding Figure 3.\n> \n\nThank you for pointing this out. We will revise the caption as follows: ZIP follows a classification-first-then-discovery approach, consisting of four steps: 1) Classification first to obtain semantic clues provided by CLIP, where the semantic clues indicate the approximate activation regions of potential objects. 2) Clustering on CLIP's features at a specific intermediate layer to discover object boundaries with the aid of our semantic-aware initialization. The semantic-aware initialization leverages semantic activation to automatically initialize clustering centers and determine the number of clusters. 3) Localization of individual objects by regrouping dispersed clustered fragments that have the same semantics, all while adhering to the detected boundaries. 4) Prompting SAM for precise masks for each individual object. \n\n> W4: Discussion on DINO and SAM.\n> \n\nThank you for your suggestion. We will relocate some of the discussions about DINO and SAM to the supplementary materials and place a stronger emphasis on our primary contributions in the introduction. \n\n> Q1: **Semantic-Aware Initialization**: Could you provide a more in-depth explanation of the Semantic-Aware Initialization mentioned on page 17?\n> \n\nThanks for pointing this out. We have added an in-depth explanation of the semantic-aware initialization in **Appendix L**. Please refer to it for a detailed understanding. \n\nThe core motivation behind semantic-aware initialization is that **by initializing some clustering centers near the outer edges of the semantic activation map, boundaries between objects within the activation map can even be clustered and outlined.** Despite the semantic activation map's inability to differentiate instances with the same classes, its edges continue to signify the boundaries between objects of distinct classes. By initializing some clustering centers near these edges (specifically, the outer edges of the semantic activation map), we can effectively group the boundaries between objects from **BOTH** the same and different classes into one cluster (i.e., the \"boundary\" cluster) due to their high feature similarity on the specific intermediate layer of CLIP. This results in the boundaries of objects from the same class also being identified by the \"boundary\" cluster, thereby enabling the distinction of these objects through the \"boundary\" cluster. As shown in Figure 5, even though the middle three figures cluster the same intermediate features in CLIP, only clustering with our semantic-aware initialization can deliver stable clustering and boundary discovery."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146577316,
                "cdate": 1700146577316,
                "tmdate": 1700146821575,
                "mdate": 1700146821575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tedRCzs0r9",
                "forum": "4JbrdrHxYy",
                "replyto": "XB1OBDYST8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
                ],
                "content": {
                    "comment": {
                        "value": "Overall I think this observation proposed in this paper is interesting, but its significance is undermined by two major issues: Firstly, the figures are not clear enough to effectively demonstrate the detailed pipeline. Secondly, the observation's exclusivity to official CLIP and its non-replicability in openCLIP limit its broader impact, as it leaves unanswered questions about the underlying properties causing these results"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706053708,
                "cdate": 1700706053708,
                "tmdate": 1700706053708,
                "mdate": 1700706053708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5zgN5bvK2B",
            "forum": "4JbrdrHxYy",
            "replyto": "4JbrdrHxYy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose an annotation free instance segmentation through combining CLIP and SAM, where the authors claim that CLIP has a better capability of obtaining the boundaries. They evaluate their method on COCO and PASCAL VOC datasets. Their framework named Zip outperforms some of the unsupervised annotation free methods from SOA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- interesting results in Table 1 and Table 2 improving over the compared methods\n- Interesting direction to work on establishing methods for annotation free instance segmentation"
                },
                "weaknesses": {
                    "value": "- Weak novelty as the method is simply utilizing other foundation models without proposing anything specific to the instance segmentation task. The clustering techniques are not really showing anything novel in its formulation. Novelty is more in the pipeline.\n\n- There are no quantitative results to support their claim that DINO is worse than CLIP in identifying the boundaries as far as I have seen but if there are please clarify in the response. Also is CLIP better than Up-DetR or better than DINO, DINOv2 in identifying boundaries. Meaning if they apply same clustering and everything on these models' features how would it fair against CLIP.\n\n- Up-DetR is not clearly stated in the related work and I am wondering if the authors have inspected its use\nDai, Zhigang, et al. \"Up-detr: Unsupervised pre-training for object detection with transformers.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n- Equation 3 and the method is not quite clear for example when referring to performing inner product of matrices. But there is no mention of the matrices shapes which can help clarify ambiguities in the mathematical formulation.\n\n- The method is quite heuristic depending on hyper parameters used in Eq. 4 theta_1, theta_2.\n\n- Their results are quite low in Table 1 compared to for example SAM when coupled with ViTDet as reported in original SAM paper. It has to be highlighted on why the previous results weren\u2019t compared to. For example if it was not fully annotation free in SAM paper, please detail that in the paper then.\n\n- Typos needs to be fixed e.g. \u201cclassification-frist-then-discovery \u201c"
                },
                "questions": {
                    "value": "- Table 3 analysis on the architecture its not clear how the AP50 climbed to 44.9%, I am not sure how did this happen and is this still annotation free? Why it is not the final results compared in Table 1.\n\n- F.g 6 C.1 its not clear what\u2019s the x-axis?\n\n- How did you retrieve CutLER results in Table 1? Why is there one class aware and class agnostic?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766936503,
            "cdate": 1698766936503,
            "tmdate": 1699636122670,
            "mdate": 1699636122670,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d8JcCO6upx",
                "forum": "4JbrdrHxYy",
                "replyto": "5zgN5bvK2B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 11pM"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. In order to facilitate a prompt discussion and exchange of ideas, we now address your most critical concerns first. We will respond to all concerns and revise the paper as soon as possible. Thanks for your understanding!\n\n> W1: Weak novelty as the method is simply utilizing other foundation models without proposing anything specific to the instance segmentation task. The clustering techniques are not really showing anything novel in its formulation. Novelty is more in the pipeline.\n> \n\nThank you for your rigorous consideration. \n\nFirst, we attempt to directly address the reviewer's question regarding the novelty of our utilization of the foundation model and clustering technology. \n\n- **Our method goes far beyond the simple utilization of foundation models without proposing anything specific to the instance segmentation task.** On the contrary, one of our key contributions and novelties lies in how effectively we probe foundation models for annotation-free instance segmentation.\n    - SAM itself is not suitable for instance segmentation on COCO, exhibiting high recall but low precision rates primarily because of its semantic-unaware and mostly edge-oriented approach. Also, DINO may be suitable for salient object detection, but not for instance segmentation. Please refer to the second paragraph of the Introduction and Appendix B.\n    - The simple utilization of CLIP+SAM results in a less-than-ideal performance (see Table 1), highlighting the non-trivial nature of utilizing CLIP and SAM for annotation-free instance segmentation.\n        - CLIP+SAM: CLIP first generates a semantic activation map, and then SAM directly selects points from the activation map to prompt SAM in instance segmentation. This simple utilization results in clustered objects of the same class being unable to be distinguished as individual instances, as the semantic activation map only identifies these objects as a whole.\n        - SAM+CLIP: SAM first produces numerous valid segmentation masks, and CLIP then recognizes these proposals. However, this simple combination yields poor performance due to the SAM's semantic-unaware issue of generating masks with varying degrees of granularity yet lacking instance-aware discernment (see Appendix B) and CLIP\u2019s misclassification issue (see Appendix F).\n    \n    Based on these observations and motivations, we propose that the **key to effectively utilizing foundation models for the instance segmentation task is to probe foundation models to delineate the boundaries between individual objects!** \n    \n- **For clustering, we propose a novel semantic-aware initialization** technique to automatically initialize clustering centers and determine the number of clusters. **Without** our novel semantic-aware initialization, clustering techniques (such as the K-means we used in our clustering) **CANNOT** generate general and reliable clustering results for outlining object boundaries. As shown in Figure 5, even though the middle three figures cluster the same intermediate features in CLIP, only clustering with the proposed initialization technique can deliver stable clustering and boundary discovery.\n    - The core motivation behind semantic-aware initialization is that **by initializing some clustering centers near the outer edges of the semantic activation map, boundaries between objects within the activation map can even be clustered and outlined.** Despite the semantic activation map's inability to differentiate instances with the same classes, its edges continue to signify the boundaries between objects of distinct classes. By initializing some clustering centers near these edges (specifically, the outer edges of the semantic activation map), we can effectively group together the boundaries between objects from **BOTH** the same and different classes into one cluster (i.e., the \"boundary\" cluster) due to their high feature similarity on the specific intermediate layer of CLIP. This results in the boundaries of objects from the same class also being identified by the \"boundary\" cluster, thereby enabling the distinction of these objects through the \"boundary\" cluster.\n    - We have added a detailed explanation of the clustering process and motivation in **Appendix L**. Please refer to it for a comprehensive understanding."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146345680,
                "cdate": 1700146345680,
                "tmdate": 1700147554821,
                "mdate": 1700147554821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1SkfsvAlM0",
                "forum": "4JbrdrHxYy",
                "replyto": "5zgN5bvK2B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1922/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "---\n\nSecond, we emphasize our novelty from a comprehensive perspective. \n\n- We are the **FIRST to effectively utilize foundation models (CLIP and SAM) for annotation-free instance segmentation**. Such effective utilization is not trivial and cannot be achieved by simply combining foundation models.\n- We are the **FIRST to discover CLIP's special intermediate layers can outline object edges through the proposed semantic-aware feature clustering**. By incorporating this discovery, we address challenges in complex scenarios with multiple instances, under annotation-free instance segmentation. To ensure stable and general clustering of object edges, we devise a semantic-aware clustering initialization technique, which is crucial.\n- We are the **FIRST to propose the classification-first-then-discovery pipeline** for annotation-free instance segmentation task. Mere identification of boundary information does not inherently and straightforwardly aid instance segmentation without our classification-first-then-discovery pipeline. In the pipeline, we introduce a boundary metric to identify boundaries and transform the instance segmentation challenge into a fragment selection task, a novel approach not explored by previous work.\n\n> W2: there are no quantitative results to support their claim that DINO is worse than CLIP in identifying the boundaries as far as I have seen but if there are please clarify in the response. Also is CLIP better than Up-DetR or better than DINO, DINOv2 in identifying boundaries. Meaning if they apply same clustering and everything on these models' features how would it fair against CLIP.\n> \n\nThank you for your suggestions. However, we cannot **apply the same clustering and everything on features** from DINO, DINOv2, or the Up-DetR models due to the following reasons: \n\n- Our clustering relies on the proposed semantic-aware initialization (see **Appendix L**), which uses CLIP's semantic activation map to effectively initialize clustering centers and determine the number of clusters. However, DINO, DINOv2, or the Up-DetR models cannot produce such a semantic activation map for a class. As a solution, we utilize CLIP to acquire the semantic activation map and subsequently apply the same clustering method on DINOv2. The results are depicted in Figure 13, clearly illustrating that CLIP's clustering results can identify edges, such as the object boundaries of the \"person\", a task that DINOv2 does not work well.\n- We reformulate the instance segmentation task as a fragment selection task, and the progress of fragment selection also depends on the semantic activation map (see Equ.3). Therefore, we cannot leverage DINO, DINOv2, or the Up-DetR models to accomplish fragment selection.\n\nDINO and DINOv2 can detect salient objects in object-centric images from ImageNet. However, they are not suitable for localizing multiple objects in more complex scenes from COCO. In object detection scenes, DINO and DINOv2 struggle to distinguish between instance-level objects and often miss potential objects within the background.\n\n> W3: Up-DetR is not clearly stated in the related work\n> \n\nUp-DetR is an unsupervised **pre-training** method for object detection. Without annotated data for object detection, Up-DetR cannot solely perform annotation-free object detection or instance segmentation. Up-DetR requires fine-tuning on an object detection dataset after unsupervised pre-training to enable effective object detection. In contrast, our ZIP is entirely annotation-free and does not rely on object detection or instance segmentation annotations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1922/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146465465,
                "cdate": 1700146465465,
                "tmdate": 1700146778708,
                "mdate": 1700146778708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]