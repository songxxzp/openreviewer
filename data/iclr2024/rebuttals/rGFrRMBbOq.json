[
    {
        "title": "Progressive Fourier Neural Representation for Sequential Video Compilation"
    },
    {
        "review": {
            "id": "nPmum0tBx5",
            "forum": "rGFrRMBbOq",
            "replyto": "rGFrRMBbOq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission566/Reviewer_H5Ez"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission566/Reviewer_H5Ez"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a novel practical task scenario where a pretrained video neural implicit representation needs to continually learn new data while keeping the learned information. It proposes a novel Progressive Fourier Neural Representation method to tackle this, which continuously learns a compact subnetwork for each video in Fourier space. The proposed method is tested on several datasets with multiple metrics including PSNR and SSIM, and proves to outperform the baseline and other competiters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of encoding new videos into existing video INRs is important in practice, and the proposed method addresses it well according to the experiment results with higher PSNR and SSIM compared to other competitiers.\n\n- Besides the detailedly listed final metric results, it also shows abundant ablations on several hyperparameters and such as which layer to choose for the proposed model to learn etc., in both the main paper and the supplementary.\n\n- Many baselines are discussed in the related work section as well as compared technically in the experiments."
                },
                "weaknesses": {
                    "value": "- Although the quantitative results are listed in details for every video in two settings, overall the proposed method is only tested on two datasets, and both UVG series. More diverse choices would make the results more solid, such as the DAVIS dataset series etc.\n\n- The compression performance (model size) is not well tested and displayed, especially in the tables. Figure 2 and its paragraph discussed some but is still not clear to link with the values in other tables.\n\n- The illustration of the proposed method is relatively limited, e.g. about the details in Fourier space, which might hide the novelty and complexity of the proposed method.\n\n- Not many visual comparisons are provided especially in the main paper, and Figure 7 is not specifically explained on their differences and advantages etc.\n\n- [Minor] Section 3.1 said that Figure 1 is one \"possible\" structure, while there isn't any other design mentioned in this paper. Maybe there can exist more variants but if not mentioned then Figure 1 is just exactly \"our proposed structure\" to be clearer.\n\n- [Minor] Figure 1 is overall good but the font and diagram size is a bit small compared to frame images.\n\n- [Minor] Sometimes the HNerv paper is noted as Nerv (but with the correct 2023 reference)."
                },
                "questions": {
                    "value": "- In the abstract and introduction, it is illustrated that since the INRs learn to memorize videos \"regardless of data relevancy or similarity\", it is hard for them to be generalized to multiple complex data and thus continual learning is important for the models to learn new videos without forgetting previously learned videos. I agree that memorizing videos regardless of data relevancy or similarity limits INRs' efficiency and scalability, but shouldn't this characteristic help an INR to learn multiple unrelated videos compared to those that learn with data relevancy or similarity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Reviewer_H5Ez",
                        "ICLR.cc/2024/Conference/Submission566/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825568160,
            "cdate": 1698825568160,
            "tmdate": 1700522303406,
            "mdate": 1700522303406,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ug92OeNGP6",
                "forum": "rGFrRMBbOq",
                "replyto": "nPmum0tBx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Large-scale dataset (DAVIS50)."
                    },
                    "comment": {
                        "value": "- **Large-scale dataset (DAVIS50)**\n\n    As reviewer MyBt and H5Ez suggested, we prepare a large-scale sequential video dataset, the Densely Annotation Video Segmentation dataset (DAVIS50). To validate our algorithm and investigate the limitations, we conducted the experiments on 50 video sequences with 3455 frames with a high-quality, high-resolution (1080p). We have included the primary results to show the effectiveness of our PFNR. For simplicity, the table includes the PSNR performances of 10, 20, 30, 40, and 50 video sessions.\n\n\n    | Method                     | 10    | 20    | 30    | 40    | 50    | Avg.PSNR/BWT |\n    |----------------------------|-------|-------|-------|-------|-------|--------------|\n    | STL,NeRV                   | 28.92 | 31.10 | 34.96 | 29.35 | 28.87 |  31.41 / -   |\n    | WSN, c=30.0 \\%             | 19.20 | 20.80 | 23.39 | 21.56 | 21.45 |\t21.56 / 0.0 |\n    | PFNR, c=30.0 \\%, $f$-NeRV2 | 23.14 | 23.14 | 23.14 | 23.14 | 23.08 |\t24.22 / 0.0 |\n    | PFNR, c=30.0 \\%, $f$-NeRV3 | **25.58** | **27.79** | **31.42** | **27.22** | **24.88** |  **27.57** / **0.0** |\n    | MTL                        | 23.10 | 23.19 | 24.63 | 22.84 | 23.45 |  24.57 / -   |\n\n    The overall PFNR performances (STL, WSN, PFNR, and MTL) on the DAVIS50 dataset on the UVG8/17 dataset are lower than those on the UVG17 dataset. However, the performance trends of PFNR observed on the DAVIS50 dataset are consistent with the experimental results obtained from the UVG8/17 dataset. Regarding sequential and multi-task learning, the current performance falls short of achieving the target of 30 PSNR. This indicates a need for future work focused on designing model structures capable of sequence learning on large-scale datasets."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127800752,
                "cdate": 1700127800752,
                "tmdate": 1700127800752,
                "mdate": 1700127800752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZsC0yKEUsk",
                "forum": "rGFrRMBbOq",
                "replyto": "nPmum0tBx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Compression Performances"
                    },
                    "comment": {
                        "value": "- **Compression Performances**\n\n    To support the compression performances (Figure 2), we have prepared the sparsity-wise (c=10%, 30%, 50%, 70%) PSNR performances as shown in supplemental Table 17. Moreover, we have showed the progressive accumulated model capacity of PFNR which is less than 100\\% (avg. 95\\%), as shown in Fig. 3(b). Thus, the sparse and 8bit-quantized PFNR lead to parameter efficient PSNR performances. The core parts of the bit-wise quantization results are follows as.\n\n    | **PFNR, $f$-NeRV2** | **Sparsity** | **Avg. PSNR** | **Sparsity** | **Avg. PSNR** |\n    |---------------------|--------------|---------------|--------------|---------------|\n    | **bit=4**           | c=10.0 \\%    | 24.79         | c=30.0 \\%    | 24.19         |\n    | **bit=8**           | c=10.0 \\%    | 28.09         | c=30.0 \\%    | 29.33         |\n    | **bit=16**          | c=10.0 \\%    | 28.10         | c=30.0 \\%    | 29.36         |\n    | **bit=32**          | c=10.0 \\%    | 28.10         | c=30.0 \\%    | 29.36         |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127848561,
                "cdate": 1700127848561,
                "tmdate": 1700127848561,
                "mdate": 1700127848561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UjWgKLmlaE",
                "forum": "rGFrRMBbOq",
                "replyto": "nPmum0tBx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Illustration of the proposed PFNR."
                    },
                    "comment": {
                        "value": "- **Illustration of the proposed methods**\n    As reviewer MyBt suggested, we have prepared the ablation studies on the proposed FSO to show its effectiveness for sequential neural implicit representations (please see our responses to MyBt's ablation requests).  \n\n    - First, we show the performances of only real part (ignore an imaginary part) in f-NeRV2/3. The PSNR performances of only real part were lower than those of both real and imaginary parts in f-NeRV2/3. We infer that the imaginary part of the winning ticket improves the implicit neural representations.\n\n\n    - Second, we also investigate the effectiveness of only FSO without Conv.  in f-NeRV2/3. The PSNR performances were lower than FSO with Conv block. Therefore, the ensemble of FSO and Conv improves the implicit representations.\n\n\n    - Lastly, we investigate the effectiveness of sparse FSO in STL. The sparse FSO boots the PSNR performances in STL. These ablation studies further strengthen the effectiveness of FSO for sequential neural implicit representations.\n\n- **Visual Comparisions**\n\n    With limited space and capacity, we provided only important figures and attached more figures with public WSN code in the Appendix at submission. We will upload our revised script soon.\n    \n    - Supplementary Figure 7 shows the video generation (from t=0 to t=2) with c = 30.0%, showing the PFNR\u2019s output quality and PSNR scores on the UVG17 dataset. At the human recognition level, approximately 30 PSNR provides a little bit of blurred generated images as shown in WSN\u2019s PSNR(29.26); if the PSNR score is greater than 30, we hardly distinguish the quality of sequential neural implicit representations (city session). Thus, our objective is to maintain the sequential neural implicit representation of the approximately 30 PSNR scores. We have achieved this target PSNR score on the UVG17 dataset. \n\n    - We also show the quantized and encoded PFNR's video generation results in Supplementary Figure 8. We demonstrate that a compressed sparse solution in FP8 (PFNR with $c=30.0 \\%$, $f$-NeRV2) generates video representations sequentially without a significant quality drop. Additionally, the results of the FP4 showed that the compressed model can not create image pixels in detail."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128125155,
                "cdate": 1700128125155,
                "tmdate": 1700128125155,
                "mdate": 1700128125155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vyOjAymSnW",
                "forum": "rGFrRMBbOq",
                "replyto": "nPmum0tBx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Motivations and Improving scripts."
                    },
                    "comment": {
                        "value": "- **Question on Motivations**\n\n    - We agree with the reviewer that learning multiple unrelated videos at human and machine-level recognition is essential regardless of data relevancy or similarity.\n\n    - However, our algorithm assumed that even though the low similarity among unrelated videos, they share some representations in common at the neural implicit representation level as shown in Supplementary Figure 6 (Transfer Matrices); the upper triangular looks meaningless Forward Transfer (FWT), however, there might be some shared representations among uncorrelated videos and Figure 3(b) of reused weight (green, at least 10.0 \\% for all video session) also support our claims (the remaining weights (90 \\%) could be assigned to represent current session video).\n\n    - Therefore, we could conclude that PFNR leverages both aspects of relevancy and irrelevancy through adaptively reused weight selections for sequential video training.\n\n\n- **Minor Comments**\n    - Variant of PFNR\n\n        A straightforward variant of PFNR could depend on the layer position of FSO. In our experiments, the NeRV block layer of FSO determines its variants of PFNR, i.e., f-NeRV2 or f-NeRV3. Another variant of PFNR ignores imaginary parts, as shown in ablation studies. We have stated the importance of imaginary parts in FSO through the ablation studies despite its advantages of reducing parameters for the imaginary parts.\n\n\n    - Figure 1 size and typos\n\n        - As reviewers suggested, we have increased the font and diagram sizes in Figure 1.\n\n        - We have corrected some of the references. We would inspect any typos and references again for the final scripts."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128244278,
                "cdate": 1700128244278,
                "tmdate": 1700128244278,
                "mdate": 1700128244278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zlAQmMqy46",
                "forum": "rGFrRMBbOq",
                "replyto": "nPmum0tBx5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_H5Ez"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_H5Ez"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses. I think they address most of my concerns. I'd like to update my overall ratings from 5 to 6.\n\n~~Besides, I mentioned about more visual comparison in the reviews and if they can be further provided I think it would be help a lot to improve the quality of this work.~~I've seen some new figures in the revised pdf. Thank you for the update."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522406098,
                "cdate": 1700522406098,
                "tmdate": 1700522531882,
                "mdate": 1700522531882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G5WwUR57Dx",
            "forum": "rGFrRMBbOq",
            "replyto": "rGFrRMBbOq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission566/Reviewer_MyBt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission566/Reviewer_MyBt"
            ],
            "content": {
                "summary": {
                    "value": "The response provides a comprehensive analysis of a work focused on Neural Implicit Representation (NIR) for video data encoding. The work introduces a novel method, Progressive Fourier Neural Representation (PFNR), to improve the accumulation and transfer of neural implicit representations for complex video data across sequential encoding sessions. PFNR leverages a sparsified neural encoding in Fourier space, enabling better adaptation for future videos and lossless decoding for previous representations. The method shows impressive performance gains over continual learning baselines on UVG8/17 video sequence benchmarks.\n\nStrengths of the work include the novel and straightforward concept of combining Fourier representation with sparsification and the method's superior performance over baselines with the same capacity. However, weaknesses are noted in the manuscript\u2019s clarity, particularly in interpreting tables and comparing performance to baselines, as well as in the diverse configuration for Fourier transform.\n\nSeveral questions are raised, seeking clarification on the commonality of NIR usage, details on Fourier transform configurations, baseline performances in Single Task Learning, comparisons to WSN, the meaning of outperforming an upper bound, and a typo in Table 3. These questions aim to probe deeper into the work\u2019s methodology, performance, and presentation for a clearer understanding and evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed idea, combining Fourier representation and its sparsification is a straightforward and novel concept.\n2. With the same capacity, the proposed method provides better performance over baselines."
                },
                "weaknesses": {
                    "value": "1. The target task seems to be small compared to the generality of the method.\n2. The tables are a little hard to interpret.\n3. The current version of the manuscript is missing comparable performance to its baselines.\n4. The diverse configuration for Fourier transform."
                },
                "questions": {
                    "value": "1. Is it common to use neural implicit representation (NIR)? As I know, many works adopt the term implicit neural representation (INR) rather than NIR.\n2. The configuration for Fourier transform?\n    - What is the temporal length of frames ($d_\\nu$ in the paper right?) Does the $d_\\nu$ affect the final performance?\n    - Some sparsification protocols for Fourier transform ignore an imaginary part in both Fourier and inverse Fourier transform. \n3. The performances of baselines in Single Task Learning (STL).\n    - What is the performance of the proposed method with Single Task Learning (STL)? Does the proposed method impact negatively due to the proposed components for continual learning?\n    - How about the performance only FSO without Conv block in NeRV block?\n4. Comparison to WSN\n    - What is the averaged PSNR and MS-SSIM performance of the proposed method and WSN on UVG8/17 with varying capacity? It is better to visualize in plot of the performance and capacity rather than table because the capacity of the model seems to be important for this setting. I conjecture that table 8 is the good starting point.\n5. Meaning of upper bound \n    - The proposed method, PFNR, outperforms the upper bound, MLT. What is the reason behind?\n6. Typo\n    - Table 3 shows the result of MS-SSIM while the caption has PSNR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Reviewer_MyBt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855234932,
            "cdate": 1698855234932,
            "tmdate": 1700500185504,
            "mdate": 1700500185504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xmaFpZK4g6",
                "forum": "rGFrRMBbOq",
                "replyto": "G5WwUR57Dx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation Studies on PFNR."
                    },
                    "comment": {
                        "value": "Thank you sincerely for your constructive feedback. We have summarized the reviewer's comments on the following main points: large-scale dataset, the performance of STL, ablation study on $f$-NeRV, the definition $d_v$ and its performances, and minor correction. Regarding these points, we provide detailed responses as follows. We have included all these discussions in our scripts to strengthen our novelty further and to be used broadly for various image enhancement and generation fields.\n\n- **Large-scale dataset (DAVIS50)**\n\n    Please refer to our responses to Reviewer H5Ez's request for large-scale experimental results. \n\n- **Performances of baselines in STL**\n    \n    Our f-NeRV blocks positively impact single-task learning (STL) as follows. Mainly, f-NeRV3 boosts the PSNR performances of NeRV superiorly. This result could impact high-quality image-generation tasks.\n\n    | **UVG8**             | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **avg. PSNR** |\n    |----------------------|-------|-------|-------|-------|-------|-------|-------|-------|---------------|\n    | STL, NeRV            | 39.66 | 36.28 | 38.14 | 42.03 | 36.58 | 29.22 | 37.27 | 31.45 | 36.33         |\n    | STL, NeRV, $f$-NeRV2 | 39.73 | 36.30 | 38.29 | 42.03 | 36.64 | 29.25 | 37.35 | 31.65 | 36.40         |\n    | STL, NeRV, $f$-NeRV3 | **42.75** | **37.65**  | **42.05** | **42.36**  | **40.01**  | **34.21**  | **40.15** |   **36.15**    |  **39.41**              |\n\n\n- **PFNR performance without Conv. in $f$-NeRV Blocks**\n\n    We show the performances of only FSO without Conv. block in NeRV block. The PSNR performances were lower than FSO with Conv block. Therefore, the ensemble of FSO and Conv improves the implicit representations. We have included this result in our final supplementary script.\n\n    | **UVG8**                               | **1**     | **2**     | **3**     | **4**     | **5**     | **6**     | **7**     | **8**     | **avg. PSNR** |\n    |-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|---------------|\n    | **PFNR, c=50 \\%, $f$-NeRV2** | **34.46** | **33.91** | **32.17** | **36.43** | **25.26** | **20.74** | **30.18** | **25.45** | **29.82**   |\n    | **PFNR, c=50 \\%, $f$-NeRV2 w/o Conv.**   |   30.05   |  32.10    | 30.12     |\t31.82  |\t24.00  |\t19.60  |\t28.21  | \t24.47    |\t27.54    |\n    | **PFNR, c=50 \\%, $f$-NeRV2** | **36.45** | **35.15** | **35.10** | **38.57** | **28.07** | **23.06** | **32.83** | **27.70** | **32.12**   |\n    | **PFNR, c=50 \\%, $f$-NeRV2 w/o Conv.**   |   35.46   | 35.06     |  34.98    |  38.23    | 28.00     | 22.98     | 32.57     | 27.45       | 31.84     |\n\n\n- **$f$-NeRV's performances without imaginary parts**\n\n    We show the performances of only real part (ignore an imaginary part) in f-NeRV2. The PSNR performances of only real part were lower than those of both real and imaginary parts, as shown in above. We infer that the imaginary part of the winning ticket improves the implicit neural representations. We have included this result in our final supplementary script.\n\n    | **UVG8**                                    | **1**     | **2**     | **3**     | **4**     | **5**     | **6**     | **7**     | **8**     | **Avg. PSNR** |\n    |---------------------------------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|---------------|\n    | **PFNR, c=50 \\%, $f$-NeRV2**                | **34.46** | **33.91** | **32.17** | **36.43** | **25.26** | **20.74** | **30.18** | **25.45** | **29.82**     |\n    | **PFNR, c=50 \\%, $f$-NeRV2 w/o imag.** | 34.34     | 33.79     | 32.04     | 36.4      | 25.11     | 20.59     | 30.17     | 25.27     | 29.71         |\n    | **PFNR, c=50 \\%, $f$-NeRV2**                | **36.45** | **35.15** | **35.10** | **38.57** | **28.07** | **23.06** | **32.83** | **27.70** | **32.12**     |\n    | **PFNR, c=50 \\%, $f$-NeRV2 w/o imag.** | 35.66     | 34.65     | 34.09     | 37.95     | 25.80     | 21.94     | 32.17     | 26.91     | 31.15         |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125820494,
                "cdate": 1700125820494,
                "tmdate": 1700125820494,
                "mdate": 1700125820494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eMoK6r9k6n",
                "forum": "rGFrRMBbOq",
                "replyto": "G5WwUR57Dx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Upper-bound (MTL) and Minor correction."
                    },
                    "comment": {
                        "value": "- **Meaning of upper-bound (MTL)**\n\n    We observed the two points regarding the results on our PFNR's comparable performances of MTL.\n    \n    First, as the number of tasks increases, the performances of MTL tend to decrease (see, UVG8, UVG17, and DAVIS50). This is because uncorrelated individual samples could hinder the update gradients of MTL task models' parameters. From this, we speculate that the video sample's statistics (correlations, mean, and variance) should be considered to acquire the best performances of MTL in training.\n\n    Second, we have discussed the main representational difference between WSN and PFNR in Figure 4. The PFNR tends to capture local textures broadly (representations with high frequency and low variances) at lower layer (NeRV3) while while WSN focuses on local objects. This behavior of PFNR could leads to higher PSNR performances. To support our observations, we would conduct an additional representational analysis in terms of the frequency and variance in our future works.\n\n- **Minor points**\n\n    Thank you for your concerns about writing configuration, terminology, and typos. We would revise all configuration, terminology, and typos if possible.\n\n    - Writing configuration\n\n        Thank you for your constructive suggestions. Based on your feedback, we would move plots and images to supplementary and increase the portion of interpretable tables in the main script. \n\n    - Terminology (NIR or INR)\n\n        As the reviewer stated, some previous works follow the term, implicit neural representation (INR), but currently, a similar term, neural implicit representation (NIR) is also observed in various tutorials and works.\n\n    - Typos\n\n        We have revised the typo: Avg. MS-SSIM \u2192 Avg. PSNR in Table 3."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126006995,
                "cdate": 1700126006995,
                "tmdate": 1700126006995,
                "mdate": 1700126006995,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Leph1UccpX",
                "forum": "rGFrRMBbOq",
                "replyto": "G5WwUR57Dx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Temporal length and Averaged performances"
                    },
                    "comment": {
                        "value": "- **Temporal length and Performances**\n\n    - the temporal length of frames ($d_v$\u00a0in Li et al., 2020a)\n\n        There is one difference between physical modeling and neural implicit representation settings. Following the previous definition (Li et al., 2020a), $d_v$ is the temporal length. In the NIR setting, the function $f$ learns a time-specific continuous output (an image) given a discrete-time index (a time). \n    \n    - The final performance of $d_v$\n\n        If we would investigate the performances of $d_v$, we should consider the FPS of the video. As the reviewer (inAP) suggested, we investigated the performances of video samples according to their FPS. From our observations, the higher the FPS is, the better the PSNR is. Thus, our PFNR is a more time-specific continuous method than the others in physical modeling.\n\n\n- **Averaged PSNR Performances and Visualization**\n\n    The following tables show the averaged PSNR and MS-SSIM performances of WSN on UVG8/17 with varying capacity (c=10%, 30%, 50%, 70%). We have included more generated images in the Supplementary.\n\n    | UVG8            | Avg. PSNR | Avg. MS-SSIM |\n    |-----------------|-----------|--------------|\n    | STL, NeRV       | 36.33     | 0.97         |\n    | EWC             | 12.58     | 0.30         |\n    | iCaRL           | 26.51     | 0.80         |\n    | ESMER           | 21.92     | 0.61         |\n    | WSN             | 27.26     | 0.84         |\n    | PFNR, $f$-NeRV2 | 29.82     | 0.90         |\n    | PFNR, $f$-NeRV3 | 32.12     | 0.93         |\n    | MTL             | 30.78     | 0.91         |\n\n\n    | UVG17           | Avg. PSNR | Avg. MS-SSIM |\n    |-----------------|-----------|--------------|\n    | STL, NeRV       | 36.94     | 0.97         |\n    | EWC             | 11.38     | 0.30         |\n    | iCaRL           | 26.67     | 0.65         |\n    | ESMER           | 20.95     | 0.62         |\n    | WSN             | 26.42     | 0.82         |\n    | PFNR, $f$-NeRV2 | 29.36     | 0.90         |\n    | PFNR, $f$-NeRV3 | 31.72     | 0.94         |\n    | MTL             | 29.42     | 0.90         |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126114105,
                "cdate": 1700126114105,
                "tmdate": 1700126114105,
                "mdate": 1700126114105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Run3TxDe7D",
                "forum": "rGFrRMBbOq",
                "replyto": "Leph1UccpX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_MyBt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_MyBt"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed explanations and extensive experiments.\nI increased my score from 6 to 8 because the rebuttal solved most of my concerns.\nI hope the comments from all reviewers will be included in the future version of the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500317487,
                "cdate": 1700500317487,
                "tmdate": 1700500317487,
                "mdate": 1700500317487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o7O6BVPsaY",
            "forum": "rGFrRMBbOq",
            "replyto": "rGFrRMBbOq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission566/Reviewer_uKbd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission566/Reviewer_uKbd"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the neural implicit representation in continual learning settings and proposes a novel method to encode the learned knowledge compactly. With this design, the model becomes able to accumulate neural representations for multiple videos with few decoding losses for previous videos. Experiments on UVG8/17 verifies its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed learning scenario is very practical in real applications, that is, encoding multiple videos continually via a neural implicit representation. With this setting, the generalization ability of NIR can be evaluated.\n\n2. I like the idea that incorporates the Lottery Ticket Hypothesis to find subnetworks for each video session. Frozing these subnetworks proves to be an effective way to encode existing knowledge about previous videos.\n\n3. The designed experiments verify its ability to adapt to new training sessions. With the proposed methods, the performance achieved significant improvements."
                },
                "weaknesses": {
                    "value": "1. The writing is not so clear. It's hard to learn the connection of Sec 3.1 with other sections.\n\n2. The experiments miss one important point. All tables show the results of the newly added sessions. However, to verify the \"lossless decoding\" stated in the abstract, previous videos need also to be evaluated. Otherwise, the reported metrics only verify the quick adaptation ability of the proposed method.\n\n3. There are some neglected details.\n- At the 2nd line of Sec 3, the cited paper is E-NeRV instead of NeRV.\n- In the caption of Fig 1, the symbol $H_N$ is used without definition and no further usage.\n- At the line before Eq 3, should it be \"session s\" instead of \"session t\"?\n- I suggest moving Algorithm 1 to the same page as Sec 3.2, where it is referred."
                },
                "questions": {
                    "value": "In Algorithm 1, the operation at line 7 is not differentiable, so how to calculate $\\partial L / \\partial \\rho$?\nI'm willing to change my score if all my issues can be solved"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Reviewer_uKbd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698916809324,
            "cdate": 1698916809324,
            "tmdate": 1700722976883,
            "mdate": 1700722976883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JldCTAUBAF",
                "forum": "rGFrRMBbOq",
                "replyto": "o7O6BVPsaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Lossless decoding (forget-free)"
                    },
                    "comment": {
                        "value": "Thank you sincerely for your constructive feedback. We have summarized the reviewer's comments on three main points: lossless decoding (forget-free), Straight-through estimator, and improving writing. Regarding the three points, we provide detailed responses as follows. \n\n- **Lossless decoding (forget-free)**\n\n    We have prepared the PFNR transfer matrices on the UVG17 dataset to show the lossless decoding, as shown in Supplementary Figure 6. The lower triangular estimated by each session subnetwork denotes that our PFNR is a forget-free method, and the upper triangular calculated by the current session subnetwork denotes the video similarity between the source and the target videos. The following PFNR transfer matrice results from PFNR, c=30.0\\%, $f$-NeRV3.\n\n    |     | S1    |       |       |       | S5    |       |       |       |       | S10   |       |       |       |       | S15   |       |       |\n    |-----|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n    | S1  | **33.63** | 7.57  | 8.75  | 9.97  | 8.70  | 7.28  | 9.77  | 9.49  | 9.72  | 8.88  | 8.48  | 10.03 | 9.98  | 9.29  | 7.36  | 7.50  | 7.83  |\n    |     | **33.63** | **39.24** | 12.44 | 9.45  | 5.90  | 7.02  | 9.60  | 9.63  | 8.91  | 11.42 | 7.29  | 9.59  | 9.84  | 7.30  | 5.27  | 4.48  | 10.41 |\n    |     | **33.63** | **39.24** | **34.21** | 11.08 | 5.92  | 6.91  | 10.49 | 11.13 | 9.91  | 14.91 | 7.36  | 11.29 | 11.54 | 7.48  | 5.28  | 4.73  | 12.21 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | 7.96  | 8.59  | 10.53 | 11.55 | 10.69 | 11.19 | 9.23  | 10.49 | 11.58 | 8.78  | 7.29  | 6.60  | 9.48  |\n    | S5  | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | 8.41  | 8.33  | 6.70  | 8.56  | 5.58  | 9.30  | 7.08  | 7.02  | 10.75 | 12.31 | 10.85 | 5.30  |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | 8.08  | 7.69  | 9.18  | 6.90  | 9.56  | 6.97  | 7.35  | 8.83  | 7.53  | 6.07  | 6.99  |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | 10.02 | 11.21 | 11.00 | 8.74  | 11.52 | 11.55 | 8.86  | 7.36  | 5.93  | 10.54 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | 10.43 | 11.30 | 8.10  | 10.95 | 11.73 | 7.78  | 6.05  | 5.37  | 10.19 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | 10.55 | 9.56  | 11.18 | 11.58 | 9.48  | 7.61  | 6.16  | 10.24 |\n    | S10 | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18** | 7.44  | 11.61 | 12.07 | 7.37  | 4.97  | 4.27  | 14.71 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18** | **22.97** | 7.50  | 7.63  | 9.08  | 8.74  | 7.21  | 7.10  |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18** | **22.97** | **24.36** | 14.14 | 7.83  | 6.31  | 5.30  | 11.44 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18**| **22.97** | **24.36** | **32.50** | 8.09  | 6.26  | 5.33  | 11.42 |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18**| **22.97** | **24.36** | **32.50** | **30.22** |  8.39  | 8.36  | 6.93  |\n    | S15 | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18**| **22.97** | **24.36** | **32.50** | **30.22** | **27.62** | 9.32  | 4.75  |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.1** | **38.17** | **29.79** | **26.56** | **36.18** | **22.97** | **24.36** | **32.50** | **30.22** | **27.62** | **29.15** | 3.64  |\n    |     | **33.63** | **39.24** | **34.21** | **37.79** | **34.05** | **27.17** | **38.17** | **29.79** | **26.56** | **36.18** | **22.97** | **24.36** | **32.50** | **30.22** | **27.62** | **29.15** | **35.68** |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126556833,
                "cdate": 1700126556833,
                "tmdate": 1700127484030,
                "mdate": 1700127484030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nGao0S8dwv",
                "forum": "rGFrRMBbOq",
                "replyto": "o7O6BVPsaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Straight-through estimator to update weight schore $\\rho^{\\ast}$ and Improving scripts."
                    },
                    "comment": {
                        "value": "- **Straight-through estimator for $\\frac{\\partial L}{\\partial \\rho^{\\ast}}$**\n\n    To update the weight score $\\rho^{\\ast}$, we have used the Straight-through estimator [1,2,3], as stated in Sec. 3.2. The Straight-through estimator estimates the gradients of a function. Specifically, it ignores the derivative of the threshold function (binary mask) and passes on the incoming gradient as if the function were an identity function. So, the threshold function (binary mask) is bypassed in the backward pass.\n\n    [1] Neural networks for machine learning, Hinton, 2012\n\n    [2] Estimating or propagating gradients through stochastic neurons for conditional computation- CoRR2013\n\n    [3] What\u2019s hidden in a randomly weighted neural network?- CVPR2020\n\n- **Improving writing**\n\n    Based on the reviewer's constructive feedback, we have revised the descriptions (particularly, Sec. 3.1.). The minor revised points are as follows:\n\n    - We have revised the typos: the cited paper (NeRV), \"session t\".\n    - We have redefined the $H_N$ and used to explain the trainable function $f$. For simplicity equation, we ignore the $H_N$.\n    - We have moved Algorithm 1 to the same page as Sec. 3.2.\n\n    We would improve the entire script to be more transparent and strengthen our novel points in the final scripts. We will upload our revision soon."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126781350,
                "cdate": 1700126781350,
                "tmdate": 1700126781350,
                "mdate": 1700126781350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0JmU9dSl0w",
                "forum": "rGFrRMBbOq",
                "replyto": "nGao0S8dwv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_uKbd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_uKbd"
                ],
                "content": {
                    "title": {
                        "value": "Issues well solved"
                    },
                    "comment": {
                        "value": "All my issues have been solved, so I'm willing to raise my score"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722952962,
                "cdate": 1700722952962,
                "tmdate": 1700722952962,
                "mdate": 1700722952962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LidXaaBhGG",
            "forum": "rGFrRMBbOq",
            "replyto": "rGFrRMBbOq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission566/Reviewer_inAP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission566/Reviewer_inAP"
            ],
            "content": {
                "summary": {
                    "value": "This article addresses the existing problem in Neural Implicit Representation (NIR) that these models specialize in learning only one mapping between target data and fail to generalize when implemented over more data, thus limiting scalability. The authors propose a modification on top of the NeRV pipeline [Chen et al, 2021a]. In order to capture information while doing a fine discretization of model parameters, the authors transform the weights to the Fourier space, on the lines of the work done by [Li et al., 2020a]. Experimentation has been done on Video Task-incremental Learning and comparisons have been made against relevant baselines. The experiments closely follow NeRV [Chen et al, 2021a] and HNeRV [Chen et al., 2023] to make fair comparisons. Results are satisfactory and visualizations look good.\n\n\nHao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim, and Abhinav Shrivastava. Nerv: Neural representations for videos. Advances in Neural Information Processing Systems, 34:21557\u201321568, 2021a.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020a\n\nHao Chen, Matt Gwilliam, Ser-Nam Lim, and Abhinav Shrivastava. Hnerv: A hybrid neural representation for videos. arXiv preprint arXiv:2304.02633, 2023."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The article benefits from generally good writing, appropriate use of mathematical language. \n\nBackground literature survey is highly appropriate and extensive.\n\nThe article builds on top of a NeRV pipeline, addressing relevant and interesting shortcomings. The rationale behind the use of Fourier Subneural Operators has been developed clearly.\n\nExtensive experimentation and provision of visual results in the paper and the supplementary file also adds to the strengths of the paper. Results are satisfactory.\n\nThe ability of the model to generate videos has also been shown in the supplementary document which only adds to the broader impacts of the research done by the authors."
                },
                "weaknesses": {
                    "value": "The methodology section may appear somewhat difficult to some readers without the necessary background in the domain. This however is only a minor weakness that does not affect the final rating.\n\nIt is not very clear if the authors used the exact frame sizes of the videos as available, or if the authors did any spatial or temporal downsampling on the videos to fit the model training pipeline, as is usual with most papers pertaining to video data.\n\nIt appears that the model may be limited by computational expenses as the datasets used are that of short video clips."
                },
                "questions": {
                    "value": "How does the proposed architecture compare with existing baselines in terms of computational requirement? Does the FSO module increase the computational requirements significantly from the baseline NeRV?\n\nHow many frames for each video was considered during training? Can the authors elaborate the video frame resolution vs frame rate trade-off i.e. what\u2019s the maximum number of frames that can be considered at a time at a certain resolution or what\u2019s the maximum resolution that can be processed at a certain number of frames?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission566/Reviewer_inAP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission566/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936526306,
            "cdate": 1698936526306,
            "tmdate": 1699635983841,
            "mdate": 1699635983841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wmjuh4yVWp",
                "forum": "rGFrRMBbOq",
                "replyto": "LidXaaBhGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dataset statistics and Performances."
                    },
                    "comment": {
                        "value": "Thank you sincerely for your constructive feedback. We have summarized the reviewer's comments on two main points: data statistics of FPS and PSNR and computational expenses. Regarding the two points, we provide detailed responses as follows. \n\n- Dataset statistics\n\n    We have prepared the dataset statistics of the UVG Videos as shown in supplementary tables 5 and 6. For example, the UVG17 Video Sessions are below. We provide the frame size and resolutions for training and inferences.\n\n- FPS and PSNR performances.\n\n    As shown in supplementary tables 5 and 6, we have extracted sample frames according to each video\u2019s FPS and Length (sec) during training. There are two kinds of videos: 300 and 600 frames. We train each video for 150 epochs. Statistically, the average PSNR of 120 FPS was better than those of 50 FPS, as shown below. When considering 30 PSNR is known as a sufficient resolution, 50 FPS-based sequence training could be adequate using the proposed PFNR. Moreover, the number of frames does not seem to be a critical factor to PSNR when considering that the PSNR of video session 1 (bunny), with 132 (720 x 1080) frames, is greater than the 30 PSNR score.  We would include this results to our scripts.\n\n    | **UVG17**  | **FPS / Resolution** | **Avg. PSNR** | **FPS / Resolution** | **Avg. PSNR** |\n    |------------|----------------------|---------------|----------------------|---------------|\n    | **STL**    | 50 / 1920 x 1080     | **35.97**     | 50 / 1920 x 1080     | 37.56         |\n    | **iCaRL**  | 50 / 1920 x 1080     | **21.94**     | 50 / 1920 x 1080     | 21.06         |\n    | **ESMER**  | 120 / 1920 x 1080    | **21.43**     | 50 / 1920 x 1080     | 19.25         |\n    | **WSN**    | 120 / 1920 x 1080    | **27.01**      | 50 / 1920 x 1080     | 25.95         |\n    | **PFNR, $f$-NeRV2** | 120 / 1920 x 1080 | **29.65**  | 50 / 1920 x 1080     | 28.36         |\n    | **PFNR, $f$-NeRV3** | 120 / 1920 x 1080 | **31.53**  | 50 / 1920 x 1080     | 31.47         |\n    | **MTL**           | 120 / 1920 x 1080 | **29.64**  | 50 / 1920 x 1080     | 28.83         |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126292966,
                "cdate": 1700126292966,
                "tmdate": 1700126292966,
                "mdate": 1700126292966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KiT73Lcjcb",
                "forum": "rGFrRMBbOq",
                "replyto": "LidXaaBhGG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Computational expanses and improvement of scripts."
                    },
                    "comment": {
                        "value": "- Computational expanses\n\n    We train and test two baselines (NeRV, ESMER) with $f$-NeRV2 using one GPU (TITAN V, 12G) with a single batch size to investigate the computational expenses and decoding FPS on the UVG8 dataset, as shown below. In STL, NeRV with $f$-NeRV2 costs more computational times than NeRV. In VCL, memory buffer-based methods, i.e., ESMER, cost more training time since they replay the memory buffer in sequential training. On the other hand, architecture-based methods, i.e., PFNR, provide parameter-efficient, faster, forget-free solutions in training while cost computation expenses in the decoding process. Considering these limitations and advantages, we would find a more parameter-efficient FSO algorithm in future work.\n\n    | UVG8                 | Training time [hours] | Decoding FPS |\n    |----------------------|-----------------------|--------------|\n    | STL, NeRV            | 13.10 [h]             | 56.88        |\n    | STL, NeRV, $f$-NeRV2 | 25.66 [h]             | 31.72        |\n    | ESMER                | 101.71 [h]            | 56.93        |\n    | PFNR, $f$-NeRV2      | 34.52 [h]             | 31.55        |\n\n\n- Improvmentment of domain backgrounds.\n    \n    We would improve domain backgrounds (particularly in Sec. 3.1. FSO) for readers in various research fields. The reviewer's constructive feedback and comments have strengthened our novelty and guided future work direction. We would include all these discussions in our scripts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126423375,
                "cdate": 1700126423375,
                "tmdate": 1700126423375,
                "mdate": 1700126423375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uEtVznUYzF",
                "forum": "rGFrRMBbOq",
                "replyto": "KiT73Lcjcb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_inAP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission566/Reviewer_inAP"
                ],
                "content": {
                    "comment": {
                        "value": "I am comfortable to keep my current score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission566/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682742505,
                "cdate": 1700682742505,
                "tmdate": 1700682742505,
                "mdate": 1700682742505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]