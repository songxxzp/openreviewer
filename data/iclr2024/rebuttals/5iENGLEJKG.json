[
    {
        "title": "Interpreting and Controlling Vision Foundation Models via Text Explanations"
    },
    {
        "review": {
            "id": "klZe7hVqZ9",
            "forum": "5iENGLEJKG",
            "replyto": "5iENGLEJKG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_ZFTq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_ZFTq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method to interpret the latent tokens in pretrained vision-language models like CLIP using natural language descriptions. The key idea is to map the latent token embeddings to the final output space by disabling self-attention and propagating through the feedforward layers. This allows retrieving the closest text description for each token from the model's vocabulary. The authors demonstrate how these interpretations can provide insights into the model's reasoning and enable controlling model behaviors like fixing adversarial attacks, reducing biases, and replacing entities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Novel method to interpret transformer tokens with language; doesn't require retraining or new data.\n\n2. Solid motivation,clear methodology, extensive experiments across multiple datasets.\n\n3. Well-written paper with clear explanation of the approach and results.\n\n4. Interpretability is valuable for ML model transparency and trust. Controlling models via token editing has useful applications."
                },
                "weaknesses": {
                    "value": "1. More analysis could be provided on how distribution shifts from removing attention affect interpretation quality.\n\n2. More comparisons to related interpretation methods would further validate advantages.\n\n3. Significance could be boosted by showing applications beyond the demonstrated tasks."
                },
                "questions": {
                    "value": "Can you provide more details on how the distribution shift introduced by disabling attention affects interpretation quality? Is performance very sensitive to this?\n\nWhat processes did you use to create the vocabularies for retrieving token interpretations? How important is vocabulary size and coverage?\n\nHow do your interpretation results compare qualitatively to other methods like saliency maps or concept activation vectors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376485704,
            "cdate": 1698376485704,
            "tmdate": 1699636077083,
            "mdate": 1699636077083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dAUBf4fbSX",
                "forum": "5iENGLEJKG",
                "replyto": "klZe7hVqZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable reviews. We have addressed the concerns below."
                    },
                    "comment": {
                        "value": "**Analysis of distribution shift**\n\nThank you for asking for clarification on the distribution shift. We provide numerical and visualization analysis of distribution shift in appendix B. We applied random smoothing to address the distribution shift (Levine et al., 2019; Salman et al., 2019; Cohen et al., 2019) and found random smoothing improves experimental results. The experiment performance with and without random smoothing therefore provides a proxy evaluation of distribution shift\u2019s effects on interpretation quality. Most of our experiments (causal intervention in Fig. 4a; fixing typographical attack in table 1,2) show that distribution shift\u2019s effects on interpretation are small. In a few other cases (entity editing in table 3; debias CelebA in table 4), addressing distribution shift with random smoothing improved the result more significantly.\n\n**Comparison to other interpretation methods**\n\nThank you for asking for more comparison with other interpretation methods. To our knowledge, our method is among the first to provide token level interpretations of concepts. Previously methods mostly either interpret model weights or attribute prediction to image areas. Token level interpretation provides unique advantages of understanding better model reasoning processes and more flexible model editing. Therefore, it is difficult to directly compare our method with previous interpretation methods.\n\nIn our paper, we use rollout attention saliency map to find the image region that is most influential on the token being interpreted. This provides a qualitative visual verification that our interpretation is correct, and allows us to quantitatively verify correctness through saliency overlap experiment. \n\nHowever, saliency map only provides correspondence between image area and token of interest. It cannot provide a conceptual explanation of a token. Although one could mask out images to keep only saliency map regions and find a text description of that region, this method would provide a same explanation for tokens that have similar saliency maps. However, our method is able to interpret tokens corresponding to the same image area differently. For example, in the fifth row of Figure 3, L12T50, L12T42, L13T1 all correspond to similar image areas, but interpretation is different. This allows for interpretations of different levels of concepts (e.g. \u201cL12T35: green and blue coloured spots\u201d contains low level features; \u201cL13T1: a person walking down the street\u201d contains high level concepts).\n\nWe also conducted additional experiments that show our interpretation enabling better model editing in addressing typographical attacks and spurious correlations than other interpretation methods (see next response)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691533430,
                "cdate": 1700691533430,
                "tmdate": 1700693590549,
                "mdate": 1700693590549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sH4NvUywxP",
                "forum": "5iENGLEJKG",
                "replyto": "klZe7hVqZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Quantitative experiment: Comparison with other interpretation methods**\n\n_Fixing Typographical Attacks_\n\nWe consider the following methods of producing saliency maps. \n\n- Raw attention values from penultimate layer to final layer CLS token\n- GradCAM [1] with the same implementation for ViT in [3]: raw attention values from penultimate layer to final layer CLS tokens * their gradient respect to similarity with prompt \u201cthis can be described as a text\u201d \n- Rollout attention from CLS token to input layer [2]\n- Rollout attention from CLS token to input layer where each attention matrix is multiplied by its gradient respect to similarity with prompt \u201cthis can be described as a text\u201d [3]\n\nWith all these saliency maps, we mask the parts of the image with map > threshold with 0. We test different thresholds and report best performance with each map.\n\nWe perform the editing on the ImageNet typographical attack experiment (Table 2). Here\u2019s the result:\n\n| Method                                  | Accuracy (original image) \u2191 | Accuracy (attack image) \u2191 |\n| --------------------------------------- | --------------------------- | ------------------------- |\n| Raw Attention                          | 98.20%                      | 81.40%                    |\n| Raw Attention * Grad [1] | 95.40%                  | 74.60%                    |\n| Rollout Attention [2] | 96.60%                     | 52.20%                    |\n| Rollout Attention * Grad [3] | 98.20%                 | 55.60%                    |\n| Ours | 99.20%                      | 88.80%                    |\n| Ours (w/ RS) | 99.20%                | **89.20%**                    |\n\nOur method outperforms the best saliency map based method (raw attention) by 7.8% on attack image accuracy.\n\n_Reducing spurious correlations_\n\nWe conducted the removing spurious correlation experiment (Table 4) too with saliency maps. For raw attention and rollout attention, we mask out parts of the image with map > threshold with 0. For gradient based map, we take gradient respect to similarity to \u201cthis can be described as hair\u201d and mask out parts of the image with map < threshold with 0. We report performance under the best threshold for each saliency map.\n\n| Method | Weighted Average \u2191 | Male Gray Hair\u2191 | Male Non-Gray Hair\u2191 | Female Gray Hair\u2191 | Female Non-Gray Hair\u2191 |\n| --- | --- | --- | --- | --- | --- |\n| Baseline | 58.22% | 99.67% | 15.85% | 19.68% | 99.67% |\n| Raw Attention | 70.00% | 90.71% | 42.00% | 59.50% | 88.92% |\n| Raw Attention * Grad [1] | 66.17% | 99.10% | 19.01% | 49.52% | 98.86% |\n| Rollout Attention [2] | 69.95% | 91.61% | 38.27% | 59.98% | 91.12% |\n| Rollout Attention * Grad [3]| 62.48% | 99.43% | 25.20% | 28.53% | 98.78% |\n| Our Intervention | 81.66% | 97.23% | 74.01% | 58.00% | 98.29% |\n| Our Intervention (RS) | 83.91% | 97.23% | **74.80%** | **66.56%** | 97.80% |\n\nOur method outperforms best saliency map based method (raw attention) by 24.56% on worst class accuracy.\n\n[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2016). Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. arXiv preprint arXiv:1610.02391.\n\n[2] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4190\u20134197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385.\n\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782\u2013791, June 2021."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691597642,
                "cdate": 1700691597642,
                "tmdate": 1700691597642,
                "mdate": 1700691597642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x1tOOcGRDk",
                "forum": "5iENGLEJKG",
                "replyto": "klZe7hVqZ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Effects of vocabulary**\n\nOne advantage of our framework is that it could produce interpretation with any vocabulary of any type and size. In general, a larger and diverse vocabulary set provides more granular and comprehensive interpretations. We conducted an ablation study on how vocabulary size influences model editing results based on interpretations by measuring performance on reducing CelebA spurious correlation (Appendix F, Fig. 12). In general, a larger vocabulary (we tested up to 5000) performs better, but a balance between vocabulary and object word list size is important.\n\nChoosing specialized vocabulary might help improve interpretations on domain-specific images. For example, we used 2088 unique nouns and adjectives extracted from remote sensing image captions of RSICD dataset (Lu et al.) to interpret satellite images.\n\n\n**Process of vocabulary creation**\n\nTo interpret VAW, CelebA, ImageNet images, we used vocabulary extracted from MILAN annotations (Hernandez et al., 2022) and GPT description of ImageNet (Mao et al., 2022b) which contain descriptions of most daily life object attributes. To interpret satellite images, we used 2088 unique nouns and adjectives extracted from remote sensing image captions of RSICD dataset (Lu et al.).\n\n\n**Applications beyond demonstrated tasks**\n\nBeyond the demonstrated tasks, our method could be generalized to pinpoint the internal mechanism for any kind of model errors. One can then build models that emphasize self-consistency of internal concepts and avoid overemphasis on certain biased features. \n\nWe also conducted additional demonstrations to show that our interpretation allows for understanding how adversarial attacks impact the model in Appendix A. We show that adversarial attack impacts model the most starting from layer 10 and provide some example latent token interpretations before and after attack."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691640429,
                "cdate": 1700691640429,
                "tmdate": 1700691640429,
                "mdate": 1700691640429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7HoTl2441e",
                "forum": "5iENGLEJKG",
                "replyto": "x1tOOcGRDk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Reviewer_ZFTq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Reviewer_ZFTq"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer ZFTq"
                    },
                    "comment": {
                        "value": "Thank you for the responses.  They provide clarification on the questions raised.  After discussing with other reviewers, I will reassess my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706389920,
                "cdate": 1700706389920,
                "tmdate": 1700706389920,
                "mdate": 1700706389920,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2j4rFXBGIK",
            "forum": "5iENGLEJKG",
            "replyto": "5iENGLEJKG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_aYwS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_aYwS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to interpret the semantic meaning of vision foundation models by retrieving the closest text description. Specifically, the method turns off the inter-token cross attentions in the latter layers and only keeps the self-attention, in order to get the final CLS representation that corresponds to only the target token. Saliency attention visualizations and quantitative results are provided to show the correctness of the interpretations. To demonstrate the application of the proposed method in terms of controlling and intervening the model behaviors, the paper runs experiments on three tasks: fixing topographical attacks, intervening object entities, and removing gender bias."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper runs experiments on three practical tasks to showcase the applications of the derived interpretations. It is exciting to see that the interpretations can improve the model\u2019s robustness towards text/topographical attacks and gender bias, as well as intervening the model\u2019s decision in object classification in satellite images.\n\n2. Abundant visualizations and numbers are reported to show the advantage of the proposed method over random intervention."
                },
                "weaknesses": {
                    "value": "1. The major limitation is that the method can only be applied on the CLIP model (at least only CLIP is shown in the paper). Since CLIP aligns the visual embeddings (CLS) with the texts, the method can retrieve texts based on the CLS embedding after turning off the cross-attentions. Otherwise, if the model\u2019s token embeddings are not trained to be aligned with the texts, it is doubtful whether the method can still work or not. It would be best if the authors can show that the method can workin on other non-text trained models like MAE, DINOv2, etc.\n\n2. Most of the comparisons are against the \u201crandom baseline\u201d, which is not a very strong baseline. For example, additional stronger baselines should be compared with, like the ones listed in the \u201cmodel interpretation\u201d in the related works sections. Moreover, it\u2019s better to include related work [1].\n\n3. The method is based on text-retrieval, which assumes a finite set of candidate texts (closed world). In related work, the authors criticize that Koh et al (2020) (concept bottleneck network) is limited to a closed vocabulary, but isn\u2019t this work also closed vocabulary? \n\n4. The method is training free. As an extension, can the method be extended for improving model training? \n\n[1] (arxiv 2301.13081 STAIR: Learning Sparse Text and Image Representation in Grounded Tokens)"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698432483664,
            "cdate": 1698432483664,
            "tmdate": 1699636076994,
            "mdate": 1699636076994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mSqgqJ6nrG",
                "forum": "5iENGLEJKG",
                "replyto": "2j4rFXBGIK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable reviews. We have addressed the concerns below."
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable comments. We are glad that the reviewer found our work to be exciting. We addressed the questions below:\n\n**Interpreting Non-Text Trained Models**\n\nThank you for pointing this out. Our framework is general to all transformer architecture. For non-text trained models like DINOv2 and MAE, one can train a linear mapping from final CLS embedding (if available) or average pooling of all final layer embeddings to CLIP image-text embedding space. Then, one can interpret latent tokens in these models with our method. \n\n**Comparison to other interpretation methods**\n\nThank you for pointing us to STAIR, which is an interesting extension of CLIP that provides better concept representation through sparse token space. We have revised our paper and included it in related work.\n\nThank you for asking for more baselines. To our knowledge, our method is among the first to provide token level interpretations of concepts. Previously methods mostly either interpret model weights or attribute prediction to image areas. Token level interpretation provides unique advantages of understanding better model reasoning processes and more flexible model editing. Therefore, it is difficult to directly compare our method with previous interpretation methods. \n\nHowever, we conducted additional experiments to compare the effectiveness of addressing typographical attacks and spurious correlation between our interpretation and saliency mask based interpretation methods (see next response)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691332265,
                "cdate": 1700691332265,
                "tmdate": 1700693241533,
                "mdate": 1700693241533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ctOy4tGG40",
                "forum": "5iENGLEJKG",
                "replyto": "2j4rFXBGIK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Quantitative experiment: Comparison with previous interpretation method**\n\n_Fixing Typographical Attacks_\n\nWe consider the following methods of producing saliency maps. \n\n- Raw attention values from penultimate layer to final layer CLS token\n- GradCAM [1] with the same implementation for ViT in [3]: raw attention values from penultimate layer to final layer CLS tokens * their gradient respect to similarity with prompt \u201cthis can be described as a text\u201d \n- Rollout attention from CLS token to input layer [2]\n- Rollout attention from CLS token to input layer where each attention matrix is multiplied by its gradient respect to similarity with prompt \u201cthis can be described as a text\u201d [3]\n\nWith all these saliency maps, we mask the parts of the image with map > threshold with 0. We test different thresholds and report best performance with each map.\n\nWe perform the editing on the ImageNet typographical attack experiment (Table 2). Here\u2019s the result:\n\n| Method                                  | Accuracy (original image) \u2191 | Accuracy (attack image) \u2191 |\n| --------------------------------------- | --------------------------- | ------------------------- |\n| Raw Attention                          | 98.20%                      | 81.40%                    |\n| Raw Attention * Grad [1] | 95.40%                  | 74.60%                    |\n| Rollout Attention [2] | 96.60%                     | 52.20%                    |\n| Rollout Attention * Grad [3] | 98.20%                 | 55.60%                    |\n| Ours | 99.20%                      | 88.80%                    |\n| Ours (w/ RS) | 99.20%                | **89.20%**                    |\n\nOur method outperforms the best saliency map based method (raw attention) by 7.8% on attack image accuracy.\n\n_Reducing spurious correlations_\n\nWe conducted the removing spurious correlation experiment (Table 4) too with saliency maps. For raw attention and rollout attention, we mask out parts of the image with map > threshold with 0. For gradient based map, we take gradient respect to similarity to \u201cthis can be described as hair\u201d and mask out parts of the image with map < threshold with 0. We report performance under the best threshold for each saliency map.\n\n| Method | Weighted Average \u2191 | Male Gray Hair\u2191 | Male Non-Gray Hair\u2191 | Female Gray Hair\u2191 | Female Non-Gray Hair\u2191 |\n| --- | --- | --- | --- | --- | --- |\n| Baseline | 58.22% | 99.67% | 15.85% | 19.68% | 99.67% |\n| Raw Attention | 70.00% | 90.71% | 42.00% | 59.50% | 88.92% |\n| Raw Attention * Grad [1] | 66.17% | 99.10% | 19.01% | 49.52% | 98.86% |\n| Rollout Attention [2] | 69.95% | 91.61% | 38.27% | 59.98% | 91.12% |\n| Rollout Attention * Grad [3]| 62.48% | 99.43% | 25.20% | 28.53% | 98.78% |\n| Our Intervention | 81.66% | 97.23% | 74.01% | 58.00% | 98.29% |\n| Our Intervention (RS) | 83.91% | 97.23% | **74.80%** | **66.56%** | 97.80% |\n\nOur method outperforms best saliency map based method (raw attention) by 24.56% on worst class accuracy.\n\n[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2016). Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. arXiv preprint arXiv:1610.02391.\n\n[2] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4190\u20134197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385.\n\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782\u2013791, June 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691358113,
                "cdate": 1700691358113,
                "tmdate": 1700691358113,
                "mdate": 1700691358113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1lC6zgyXtz",
                "forum": "5iENGLEJKG",
                "replyto": "2j4rFXBGIK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Openness of vocabulary**\n\nOur approach works with vocabulary of any type and size, which makes our approach work on open vocabulary. In contrast, Koh et al (2020)\u2019s method interprets a small set of predefined concepts, where the concept set is built into model architecture that is task-specific.\n\nFor our method, we showed that large vocabulary (5000 descriptions) is beneficial for producing better descriptions in our framework in section 4.3. Our method also works on any type of vocabulary, which makes it easily adapt to different tasks. To enable better interpretations in domain specific tasks, one can simply choose a vocabulary set that contains domain-specific languages (for example, we used RSICD captions to interpret satellite images in UC Merced Land Use Dataset).\n\n**Improving model training with our interpretations**\n\nThank you for your suggestion on extending our method to improve model training. Our approach can indeed be generalized to enhance model training. A few possible ways include: 1) regularize the model to emphasize self-consistency of internal concept representations for more concrete visual reasoning and enhanced robustness against adversarial attacks (we demonstrate how our method help understand adversarial attacks in Appendix A); 2) increase model robustness by steering the model to avoid overemphasis on certain classes of features."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691427686,
                "cdate": 1700691427686,
                "tmdate": 1700691427686,
                "mdate": 1700691427686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XCPGnS2ugY",
            "forum": "5iENGLEJKG",
            "replyto": "5iENGLEJKG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_jZ36"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_jZ36"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to interpret intermediate representations (referred to as *latent tokens*) in vision-language Transformer models (e.g. CLIP) by retrieving relevant natural language descriptions for individual latent tokens. The authors do so by removing the self-attention operations after a latent token, and using the last layer token representation to retrieve a text description. The authors posit that the retrieved text for a latent token provides an interpretation for it. They further posit that manipulating these latent tokens can be used to edit and control model behavior."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments are very convincing. The author use their methodology to not only do model interpretation (which would have been sufficient imo), but also for model controllability. The results for interpretability (causality and saliency map overlap) are very convincing. \n\n- The controllability experiments are also very well designed, and demonstrate consistent results across three different kinds of model editing -- typography attacks, entity editing and gender debiasing.\n\n- The paper is well-written overall. Some of the experiment setups are a little hard to understand, because the setting is slightly artificial,"
                },
                "weaknesses": {
                    "value": "- I am skeptical of the motivation behind the methodology. Specifically, typically the CLS representation from the vision encoder is used as the query to do text retrieval in CLIP, but do we know that latent tokens corresponding to other image patches from the final layer retrieve meaningful text concepts? There is no theoretical motivation -- which isn't strictly needed, but I would be much likelier to trust the method beyond just the empirical results.\n\n- It's unclear what the benefit of these natural language descriptions for latent tokens is, or how these descriptions should be leveraged. The examples in Section 4.3 (\"Our Interpretations Reveal Visual Reasoning\") do not seem very convincing, and are more about how one chooses to interpret the tokens' NL descriptions (e.g. I would not think \"motor vehicle\" + \"valley\" = \"overpass\" is necessarily a correct reasoning, let alone whether that's actually how the model went about the reasoning process)."
                },
                "questions": {
                    "value": "- What is the benefit of the proposed method over the saliency maps that are presented in Figure 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798268635,
            "cdate": 1698798268635,
            "tmdate": 1699636076929,
            "mdate": 1699636076929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lmxxnbTdMc",
                "forum": "5iENGLEJKG",
                "replyto": "XCPGnS2ugY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable reviews. We have addressed the concerns below."
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. We are glad that the reviewer found our experiments well-designed and convincing. We address the reviewer\u2019s concerns below:\n\n**Motivation behind our methodology**\n\nThank you for your suggestion on theoretical motivation. \n\nLet $x_{cls}$ be the CLS embedding after final projection layer. Let $x_i$ be the $i$-th non-CLS embedding after the final projection layer. Let $v_j$ be the embedding of $j$-th text description.\n\nWe know that we can retrieve $\\hat{j}$-th text as a meaningful text description for $x_{cls}$ by taking $\\hat{j}= \\arg\\max_{j} x_{cls}^T v_j$. \n\nNow consider the average of all non-CLS embedding: $x_{agg} = \\frac{1}{N}\\sum_{i=1}^N x_i$. This average aggregates information from all image patches. We will empirically show that we could retrieve accurate text descriptions from averaged non-CLS tokens by showing $\\arg\\max_{j} x_{cls}^T v_j = \\arg\\max_{j} x_{agg}^T v_j$.\n\nSince the averaged token is a linear combination of each non-CLS token, $x_{agg}^T v_j =  \\frac{1}{N}\\sum_{i=1}^N x_i^T v_j$, being able to retrieve correct text descriptions for $x_{agg}$ implies that we could retrieve correct text descriptions for each individual non-CLS token $x_i$.\n\nWe conducted a two-class zero-shot classification using CLS token and average of non-CLS tokens. CLS token embedding attains 99.5% accuracy; averaging non-CLS token embeddings on the final layer attains 97.5% accuracy. This experiment shows that indeed $\\arg\\max_{j} x_{cls}^T v_j = \\arg\\max_{j} x_{agg}^T v_j$, and non-CLS tokens also encode correct information for text retrieval. (The experiment is conducted on 100 images from each class from UC Merced Land Use Dataset with beach class and forest class).\n\n**Benefits of natural language descriptions**\n\nThe text explanations produced from our method reveal information encoded in each latent token. This could help users easily pinpoint where do models make errors or produce bias, and users could address the issues with model editing guided by the interpretations (as we showed in fixing typographical attack and reducing spurious correlation experiments.)\n\n**Using our interpretations to understand visual reasoning**\n\nThank you for mentioning our usage of text descriptions to understand visual reasoning. Using text descriptions of latent tokens to understand the model reasoning process in for example \u201cmotor vehicle + valley = overpass\u201d is not a discretionary interpretation but grounded in both text descriptions and model mechanics. In figure 3, referenced by section 4.3, we show interpretation of tokens on layer K+1 and tokens on layer K that the K+1 layer tokens pays most attention to (based on attention weights). For example, \u201coverpass\u201d pays most attention to \u201cmotor vehicle\u201d and \u201cvalley,\u201d so we reason that this combination of local features into global features allows the model to understand the entire image as \u201coverpass.\u201d \n\nWe agree that attention value alone doesn\u2019t fully demonstrate causal relationship in reasoning. Therefore, we also conduct the Object Entity Intervention experiment in section 4.2. By showing that replacing car tokens with airplane tokens changes a model's understanding of the image from highway to airport, we demonstrate that our interpretation of local concepts are indeed causally related to how model reasons about global concepts."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690675216,
                "cdate": 1700690675216,
                "tmdate": 1700690675216,
                "mdate": 1700690675216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nS11Yfq2lD",
                "forum": "5iENGLEJKG",
                "replyto": "XCPGnS2ugY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Benefits over saliency map**\n\nIn our paper, we use rollout attention saliency map to find the image region that is most influential on the token being interpreted. This provides a qualitative visual verification that our interpretation is correct, and allows us to quantitatively verify correctness through saliency overlap experiment. \n\nHowever, saliency map only provides correspondence between image area and token of interest. It cannot provide a conceptual explanation of a token. Although one could mask out images to keep only saliency map regions and find a text description of that region, this method would provide a same explanation for tokens that have similar saliency maps. However, our method is able to interpret tokens corresponding to the same image area differently. For example, in the fifth row of Figure 3, L12T50, L12T42, L13T1 all correspond to similar image areas, but interpretation is different. This allows for interpretations of different levels of concepts (e.g. \u201cL12T35: green and blue coloured spots\u201d contains low level features; \u201cL13T1: a person walking down the street\u201d contains high level concepts).\n\nWe also conducted additional demonstrations to show that our interpretation allows for understanding how adversarial attacks impact the model in Appendix A. We show that adversarial attack impacts model the most starting from layer 10 and provide some example latent token interpretations before and after attack. Salient regions are unable to provide such understanding because there's no visible difference of image before and after adding adversarial attack."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690864628,
                "cdate": 1700690864628,
                "tmdate": 1700690864628,
                "mdate": 1700690864628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0FExIjyAmS",
                "forum": "5iENGLEJKG",
                "replyto": "XCPGnS2ugY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response continued"
                    },
                    "comment": {
                        "value": "**Quantitative experiments comparing to saliency map based editing**\n\nWe also conducted additional experiments that show our interpretation enabling better model editing in addressing typographical attacks and spurious correlations than saliency-map based methods:\n\n_Fixing Typographical Attacks_\n\nWe consider the following methods of producing saliency maps. \n\n- Raw attention values from penultimate layer to final layer CLS token\n- GradCAM [1] with the same implementation for ViT in [3]: raw attention values from penultimate layer to final layer CLS tokens * their gradient respect to similarity with prompt \u201cthis can be described as a text\u201d \n- Rollout attention from CLS token to input layer [2]\n- Rollout attention from CLS token to input layer where each attention matrix is multiplied by its gradient respect to similarity with prompt \u201cthis can be described as a text\u201d [3]\n\nWith all these saliency maps, we mask the parts of the image with map > threshold with 0. We test different thresholds and report best performance with each map.\n\nWe perform the editing on the ImageNet typographical attack experiment (Table 2). Here\u2019s the result:\n\n| Method                                  | Accuracy (original image) \u2191 | Accuracy (attack image) \u2191 |\n| --------------------------------------- | --------------------------- | ------------------------- |\n| Raw Attention                          | 98.20%                      | 81.40%                    |\n| Raw Attention * Grad [1] | 95.40%                  | 74.60%                    |\n| Rollout Attention [2] | 96.60%                     | 52.20%                    |\n| Rollout Attention * Grad [3] | 98.20%                 | 55.60%                    |\n| Ours | 99.20%                      | 88.80%                    |\n| Ours (w/ RS) | 99.20%                | **89.20%**                    |\n\nOur method outperforms the best saliency map based method (raw attention) by 7.8% on attack image accuracy.\n\n_Reducing spurious correlations_\n\nWe conducted the removing spurious correlation experiment (Table 4) too with saliency maps. For raw attention and rollout attention, we mask out parts of the image with map > threshold with 0. For gradient based map, we take gradient respect to similarity to \u201cthis can be described as hair\u201d and mask out parts of the image with map < threshold with 0. We report performance under the best threshold for each saliency map.\n\n| Method | Weighted Average \u2191 | Male Gray Hair\u2191 | Male Non-Gray Hair\u2191 | Female Gray Hair\u2191 | Female Non-Gray Hair\u2191 |\n| --- | --- | --- | --- | --- | --- |\n| Baseline | 58.22% | 99.67% | 15.85% | 19.68% | 99.67% |\n| Raw Attention | 70.00% | 90.71% | 42.00% | 59.50% | 88.92% |\n| Raw Attention * Grad [1] | 66.17% | 99.10% | 19.01% | 49.52% | 98.86% |\n| Rollout Attention [2] | 69.95% | 91.61% | 38.27% | 59.98% | 91.12% |\n| Rollout Attention * Grad [3]| 62.48% | 99.43% | 25.20% | 28.53% | 98.78% |\n| Our Intervention | 81.66% | 97.23% | 74.01% | 58.00% | 98.29% |\n| Our Intervention (RS) | 83.91% | 97.23% | **74.80%** | **66.56%** | 97.80% |\n\nOur method outperforms best saliency map based method (raw attention) by 24.56% on worst class accuracy.\n\n[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2016). Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. arXiv preprint arXiv:1610.02391.\n\n[2] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4190\u20134197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385.\n\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782\u2013791, June 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690911146,
                "cdate": 1700690911146,
                "tmdate": 1700693012772,
                "mdate": 1700693012772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6k4SpB8VE5",
            "forum": "5iENGLEJKG",
            "replyto": "5iENGLEJKG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_jxBW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1479/Reviewer_jxBW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to interpret the \u201clatent\u201d representations of vision transformers. To interpret a visual token representation of layer K, the paper proposes to disable the self-attention for layers higher than layer K, and then use the final layer representation to calculate text-token similarity. The ability to find such latent tokens enables several applications: fixing typographical attacks, intervening in the reasoning procedure, and reducing spurious correlations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Such an interpretation approach is definitely new; it is a novel observation that one could simply disable the self-attention from above a certain layer (K) and then use the representation as a representation for latent token at layer K. The paper uses causal intervention and saliency map overlap to verify the effectiveness of the approach."
                },
                "weaknesses": {
                    "value": "My main concern is that I do not quite see the method\u2019s advantage compared to gradient-based methods that find important input regions (e.g., Grad-CAM) given a text description.\n\n(1). The first question is why we want to find latent tokens but not salient regions?\n\nConceptually, the biggest advantage is that there might exist \u201chigh-level\u201d and \u201cabstract\u201d latent tokens. For example, in Figure 3, using Grad-CAM to find regions corresponding to \u201coverpass\u201d might result in a lot of matched image patches while using the proposed method can find one single latent token corresponding to the concept.\n\nHowever, this is not reflected in the quantitative experiments. For the fixing typographical attacks and reducing spurious correlations experiments, ideally a gradient-based baseline could be included, where we seek to use grad-cam to find and zero-out a few important image patches. Then one could compare whether the proposed method can achieve the same performance but zeroing out less tokens. Otherwise, it is hard to claim that the method can find \u201chigh-level\u201d latent tokens.\n\n\n(2). Suppose we wish to find latent tokens corresponding to a text description, why should we resort to the proposed method but not a gradient-based method where we calculate the gradient with respect to each latent tokens of every layer? This seems like a more principled way to obtain important latent tokens."
                },
                "questions": {
                    "value": "I am not sure I get the motivation behind the Intervening in the Reasoning Procedure experiment. Under what settings would this be useful?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1479/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833030259,
            "cdate": 1698833030259,
            "tmdate": 1699636076854,
            "mdate": 1699636076854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cp9vRFWBVz",
                "forum": "5iENGLEJKG",
                "replyto": "6k4SpB8VE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your valuable reviews. We have addressed the concerns below."
                    },
                    "comment": {
                        "value": "We are glad that the reviewer found our method to be novel. We ran the suggested experiments and addressed the concerns below.\n\n**Advantage of text descriptions of latent tokens over explaining with salient regions**\n\nWhen interpreting latent tokens with salient regions, if different latent tokens correspond to the same salient region, the resulting explanation would be the same. However, our method is able to interpret tokens corresponding to the same image area differently. For example, in the fifth row of Figure 3, L12T50, L12T42, L13T1 all correspond to similar image areas, but interpretation is different. This allows for interpretations of different levels of concepts (e.g. \u201cL12T35: green and blue coloured spots\u201d contains low level features; \u201cL13T1: a person walking down the street\u201d contains high level concepts).\n\nWe also conducted additional demonstrations to show that our interpretation allows for understanding how adversarial attacks impact the model in Appendix A. We show that adversarial attack impacts model the most starting from layer 10 and provide some example latent token interpretations before and after attack. Salient regions are unable to provide such understanding because there's no visible difference of image before and after adding adversarial attack.\n\nWe also conducted additional quantitative experiments to show that text descriptions of latent tokens with our method produce better model editing than saliency map based editing (see next response)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690352190,
                "cdate": 1700690352190,
                "tmdate": 1700693187828,
                "mdate": 1700693187828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fu0eZoSprT",
                "forum": "5iENGLEJKG",
                "replyto": "6k4SpB8VE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Quantitative experiment: Comparison with gradient-based method**\n\nThank you for asking for more baseline comparison. We conducted the following additional experiments to compare our methods against saliency based methods.\n\n_Fixing Typographical Attacks_\n\nWe consider the following methods of producing saliency maps. \n\n- Raw attention values from penultimate layer to final layer CLS token\n- GradCAM [1] with the same implementation for ViT in [3]: raw attention values from penultimate layer to final layer CLS tokens * their gradient respect to similarity with prompt \u201cthis can be described as a text\u201d \n- Rollout attention from CLS token to input layer [2]\n- Rollout attention from CLS token to input layer where each attention matrix is multiplied by its gradient respect to similarity with prompt \u201cthis can be described as a text\u201d [3]\n\nWith all these saliency maps, we mask the parts of the image with map > threshold with 0. We test different thresholds and report best performance with each map.\n\nWe perform the editing on the ImageNet typographical attack experiment (Table 2). Here\u2019s the result:\n\n| Method                                  | Accuracy (original image) \u2191 | Accuracy (attack image) \u2191 |\n| --------------------------------------- | --------------------------- | ------------------------- |\n| Raw Attention                          | 98.20%                      | 81.40%                    |\n| Raw Attention * Grad [1] | 95.40%                  | 74.60%                    |\n| Rollout Attention [2] | 96.60%                     | 52.20%                    |\n| Rollout Attention * Grad [3] | 98.20%                 | 55.60%                    |\n| Ours | 99.20%                      | 88.80%                    |\n| Ours (w/ RS) | 99.20%                | **89.20%**                    |\n\nOur method outperforms the best saliency map based method (raw attention) by 7.8% on attack image accuracy.\n\n_Reducing spurious correlations_\n\nWe conducted the removing spurious correlation experiment (Table 4) too with saliency maps. For raw attention and rollout attention, we mask out parts of the image with map > threshold with 0. For gradient based map, we take gradient respect to similarity to \u201cthis can be described as hair\u201d and mask out parts of the image with map < threshold with 0. We report performance under the best threshold for each saliency map.\n\n| Method | Weighted Average \u2191 | Male Gray Hair\u2191 | Male Non-Gray Hair\u2191 | Female Gray Hair\u2191 | Female Non-Gray Hair\u2191 |\n| --- | --- | --- | --- | --- | --- |\n| Baseline | 58.22% | 99.67% | 15.85% | 19.68% | 99.67% |\n| Raw Attention | 70.00% | 90.71% | 42.00% | 59.50% | 88.92% |\n| Raw Attention * Grad [1] | 66.17% | 99.10% | 19.01% | 49.52% | 98.86% |\n| Rollout Attention [2] | 69.95% | 91.61% | 38.27% | 59.98% | 91.12% |\n| Rollout Attention * Grad [3]| 62.48% | 99.43% | 25.20% | 28.53% | 98.78% |\n| Our Intervention | 81.66% | 97.23% | 74.01% | 58.00% | 98.29% |\n| Our Intervention (RS) | 83.91% | 97.23% | **74.80%** | **66.56%** | 97.80% |\n\nOur method outperforms best saliency map based method (raw attention) by 24.56% on worst class accuracy.\n\n[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2016). Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. arXiv preprint arXiv:1610.02391.\n\n[2] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4190\u20134197, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.385. URL https://aclanthology.org/2020.acl-main.385.\n\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 782\u2013791, June 2021.\n\n**Advantage over interpreting token via gradient**\n\nThank you for suggesting a comparison between our method and gradient-based method. If we understand the method you proposed correctly, given a token and a final layer CLS embedding, your proposed method calculates the gradient between the token and the cosine similarity between final layer CLS embedding and each text description. Then the text description for the token would be the description with the largest gradient. Please correct us if our understanding is inaccurate. \n\nIntuitively, your proposed method assigns X as token description if with a small change in token, the final layer CLS embedding \u2019s similarity to X will change a lot. One advantage of our method over this method is that our method finds a direct description for the latent token. In contrast, your proposed method might fail to find a direct description of the token. For example, a token that encodes \u201ctire\u201d might influences CLS\u2019s similarity to \u201ccar\u201d a lot, but your proposed method can only find \u201ccar\u201d but not a direct description \u201ctire\u201d for the token."
                    },
                    "title": {
                        "value": "Response continued"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690472964,
                "cdate": 1700690472964,
                "tmdate": 1700690930888,
                "mdate": 1700690930888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SwfOjEIsS9",
                "forum": "5iENGLEJKG",
                "replyto": "6k4SpB8VE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1479/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Motivation for ''Intervening in Reasoning Procedure'' Experiment**\n\nWhile the saliency map overlap and causal intervention experiments in section 4.1 demonstrates that our interpretation is faithful, we hope to show that our interpretation also explains the causal influence between a token and final image representation. By replacing car tokens with airplane tokens and successfully changing final image representation from encoding highway to airport, we show that our interpretation not only explains what each token represents but also the causal relationship between a token\u2019s and final image\u2019s representation in ViT\u2019s reasoning process."
                    },
                    "title": {
                        "value": "Response continued"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1479/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690510262,
                "cdate": 1700690510262,
                "tmdate": 1700691152053,
                "mdate": 1700691152053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]