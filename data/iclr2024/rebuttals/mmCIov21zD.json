[
    {
        "title": "RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields"
    },
    {
        "review": {
            "id": "5FZqAM4nE8",
            "forum": "mmCIov21zD",
            "replyto": "mmCIov21zD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission607/Reviewer_x3wF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission607/Reviewer_x3wF"
            ],
            "content": {
                "summary": {
                    "value": "Proposed a NeRF-based SLAM system (built upon Co-SLAM) to reconstruct the static 3D scene map in dynamic environments. To handle invalid sampling rays within dynamic objects, we filtered them out using a motion mask generation approach based on checking inliers and outliers using the fundamental matrix. Additionally, we introduced an Edge reprojection loss to remove the velocity constant assumption."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-motivated and clearly written. It proposes techniques to address invalid sampling rays within dynamic objects, improving pose accuracy and robustness. Extensive evaluations support these claims, and the paper has a sufficient number of references."
                },
                "weaknesses": {
                    "value": "Although I believe the paper is well-motivated and presents promising results in terms of both pose estimation and reconstruction, I have several concerns.\n1. Doesn't introduce substantial architectural changes or novelty for NeRF-based SLAM. Such as multi-resolution hash encoding from instant ngp, joint optimization and store a subset of pixels to represent each keyframe from Co-SLAM, etc.\n2. Enhancing additional losses (e.g., Edge reprojection loss) and masks to improve accuracy in dynamic object scenarios. How does the system's performance compare in terms of pose estimation and reconstruction in static scenes? Is it competitive with other SLAM systems?\n3. I assume the GBA here is not an actual GBA like deployed in loop closure."
                },
                "questions": {
                    "value": "The questions are listed in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Reviewer_x3wF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698171924544,
            "cdate": 1698171924544,
            "tmdate": 1699635988256,
            "mdate": 1699635988256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3eYKk1jKCI",
                "forum": "mmCIov21zD",
                "replyto": "5FZqAM4nE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responce to reviewer x3wF"
                    },
                    "comment": {
                        "value": "Thanks for the positive and detailed review as well as the valuable suggestions for improvement. We would like to address the reviewer's concerns as follows: \n\n**W1: More substantial architectural changes details**.   \nThanks for the suggestion. We have added the more substantial architectural details for our SLAM system in Appendix A.1 in the revised paper. \n\n**W2: More experiments in static scenes**.   \nThanks for the suggestion. To better demonstrate the system performance of pose estimation and reconstruction in a static scene, we choose the real-world RGB-D dataset [1] rather than the synthetic datasets [2,3]. Compared with the current the state of the art neural rgb-d SLAM, we evaluate the pose estimation and reconstruction results on three represented sequences. Additional detailed comparison results are available in Table 9 in Appendix A.5. \n\nCompared with our baseline methods Co-SLAM [4], `our method does not compromise the performance of the original SLAM methods in terms of tracking and mapping in static scenes`. In fact, it achieves competitive results.Notably, our proposed optimization algorithm is not restricted to a specific slam system. Thus, it can also be applied to other neural rgb-d slam methods to improve the data association between the inter-frame. \n\nCompared with the E-SLAM [5], our SLAM system requires less storage space and achieves real-time running at 10fps, despite several hours of training time needed for E-SLAM. Consequently, our method strikes a well-balanced compromise between accuracy and speed, demonstrating competitive performance of pose estimation when compared to state-of-the-art methods.\n\n\n> [1] J \u0308urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the evaluation of rgb-d slam systems. In IROS, 2012. \n\n> [2] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint, 2019. \n\n> [3] Dejan Azinovi \u0301c, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural rgb-d surface reconstruction. In CVPR, 2022. \n\n> [4] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam. In CVPR, 2023.\n\n> [5] JMohammad Mahdi Johari, Camilla Carta, and Franc \u0327ois Fleuret. Eslam: Efficient dense slam system based on hybrid representation of signed distance fields. In CVPR, 2023.   \n\n\n**W3: GBA defination**.   \nYes, your assumption is correct. We do not contribute to the loop closure methods with neural radiance fields. In this paper, GBA represents an operation that samples rays from entire keyframes management set and performs a global bundle adjustment to reduce the pose estimation errors and obtain a more consistent implicit map representation. We are also considering incorporating a loop closure module in future work to implement GBA in loop closure and reduce accumulated errors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651512063,
                "cdate": 1700651512063,
                "tmdate": 1700651512063,
                "mdate": 1700651512063,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b6kAKNRUCt",
            "forum": "mmCIov21zD",
            "replyto": "mmCIov21zD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission607/Reviewer_az7R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission607/Reviewer_az7R"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel RGB-D dynamic SLAM system featuring a neural radiance representation. The system builds upon the open-sourced RGB-D neural SLAM Co-SLAM, introducing a motion mask (comprising optical flow and semantic information) to filter out invalid dynamic rays during training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The exploration of dynamic SLAM with neural radiance field representation is a relatively new and promising avenue.\n2. The paper is well-written and easy to follow.\n3. The evaluation results are visually compelling and present a convincing case for the proposed method."
                },
                "weaknesses": {
                    "value": "A key concern in this paper is the insufficiently explained rationale for incorporating neural radiance field representation in dynamic SLAM, along with a noticeable absence of robust baseline comparisons during the evaluation.\n\nPlease see questions for details."
                },
                "questions": {
                    "value": "1. While the adoption of neural radiance fields in dynamic SLAM is a novel exploration, the motivation behind this choice is not entirely clear. A more detailed explanation of the potential benefits compared to existing dense dynamic SLAM methods would enhance the paper's clarity.\n2. The evaluation primarily compares the proposed method against static neural SLAM, which may not be entirely fair. It would be beneficial to include related works as stronger baselines, including traditional SLAM methods like MID-Fusion[1] and Droid-SLAM[2], as well as NeRF SLAM methods like vMAP[3] and BundleSDF[4].\n3. Although Fig. 3 displays TSDF fusion results, these results are not reported in tables. Comparing the results with and without the motion mask, as proposed in this paper, using TSDF fusion would be valuable and should be considered a baseline method.\n4. The paper's visualization results are impressive; however, for a SLAM system, it would be preferable to include videos or real-world demonstrations as additional supplementary material.\n\n[1]Mid-fusion: Octree-based object-level multi-instance dynamic slam. ICRA 2019.\n[2]Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS 2021.\n[3]vMAP: Vectorised object mapping for neural field slam. CVPR 2023\n[4]BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects. CVPR 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Reviewer_az7R"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698540188745,
            "cdate": 1698540188745,
            "tmdate": 1699635988177,
            "mdate": 1699635988177,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "09csAw9GcS",
                "forum": "mmCIov21zD",
                "replyto": "b6kAKNRUCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responce to reviewer az7R (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the insightful and detailed review as well as the valuable suggestions for improvement. We would like to address the reviewer's concerns as follows: \n\n**W1: Motivation of neural RGB-D SLAM over dense RGB-D SLAM in dynamic scene**.   \nThanks for the suggestions. We have revised the introduction to motivate the benefits compared to existing dynamic dense RGB-D SLAM methods. All the modified text is highlighted in blue in the revised version.\n\nCompared with the existing dense RGB-D dynamic SLAM methods, the key advantage of neural RGB-D SLAM lies in its neural implicit representation method. The neural scene representations have attractive properties for mapping in dynamic scenes, including `improving noise and outlier handling, geometry estimation capabilities for unobserved scene parts, high-fidelity reconstructions with reduced memory usage, and the ability to generate high-quality static background images from novel views`. Moreover, neural radiance fields also open up possibilities for the subsequent development of dynamic SLAM, such as distilling perceptive information and accomplishing object-based tracking or reconstruction.\n\nFinally, please allow us to briefly introduce our motivation and contribution to building Rodyn-SLAM system. Existing neural RGB-D SLAM methods rely on a static environment assumption and do not work robustly within a dynamic environment due to the inconsistent observation of geometry and photometry. To tackle this problem, we propose a dynamic object identification method and filter the invalid sample ray to recover the static scene map. To further improve the accuracy of pose estimation in dynamic scenes, we propose novel and robust pose optimization methods utilizing edge projection loss to enhance data association between convisible frames.\n\n\n**W2: More experiment results with stronger baselines**.    \nThanks for the suggestion. Due to the current limitation of NeRF-based SLAM methods in handling dynamic scenes, we propose a general robust dynamic Nerf-based SLAM method RoDyn-SLAM, which constitutes the innovation of this paper. We have selected state-of-the-art NeRF-based SLAM methods as our benchmark for comparison, serving as robust baselines. Following your suggestions, we have added the results of MID-Fusion [1] and Droid-SLAM [2] in Table 2 and Table 3 in the revised paper, respectively. Due to the advanced factor graph optimization algorithm and extensive training data, Droid-SLAM has achieved superior pose estimation results in dynamic scenes compared to Nerf-based SLAM methods.\n\n`Bundle-SDF [3] and vMAP [4]` are object-based neural SLAM methods rather than a general nerf-based SLAM method. It relies on external conditions, such as the `availability of pose prior` or the requirement for the `camera to remain stationary`. Thus, we do not add the comparison results of these methods in the revised paper. The more detailed analysis is summarized as:\n\n* Bundle-SDF is designed for estimating the object pose and reconstructing the object mesh. Our approach aims to estimate the camera pose in dynamic scenes by leveraging a static background. The objectives of these two methods are distinct. Additionally, the validation datasets used for Bundle-SDF require the camera to remain stationary, allowing for the pose estimation of moving objects. The datasets we employed for evaluation do not meet these conditions. \n\n* vMAP primarily emphasizes joint reconstruction of objects, relying on estimated poses from other methods i.e. ORB-SLAM3 to provide the initial pose. Therefore, we believe that the direct comparison of pose optimazation is unfair. We attempted to make a comparison of reconstruction performance with vMAP given GT pose on TUM RGB-D dynamic sequence. However, the final reconstruction results for dynamic scenes were notably blurred. Actually, due to the absence of joint optimization between the camera pose and implicit map, the dynamic objects can not be filtered, resulting in significant blurriness and floaters in the reconstructed scene.\n\n> [1] Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Mid-fusion: Octree-based object-level multi-instance dynamic slam. In ICRA, 2019.\n \n> [2] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. In NeurIPS, 2021.\n\n> [3] DBowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas M \u0308uller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. In CVPR, 2023. \n\n> [4] Xin Kong, Shikun Liu, Marwan Taher, and Andrew J Davison. vmap: Vectorised object mapping for neural field slam. In CVPR, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651380955,
                "cdate": 1700651380955,
                "tmdate": 1700738491637,
                "mdate": 1700738491637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4N8qU2VZ6L",
                "forum": "mmCIov21zD",
                "replyto": "09csAw9GcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Reviewer_az7R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Reviewer_az7R"
                ],
                "content": {
                    "title": {
                        "value": "Revisions"
                    },
                    "comment": {
                        "value": "Thanks a lot for the author's response.\nI didn't see the revision. Could you please check if it has been uploaded?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655038868,
                "cdate": 1700655038868,
                "tmdate": 1700655038868,
                "mdate": 1700655038868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bc9NahuYgl",
            "forum": "mmCIov21zD",
            "replyto": "mmCIov21zD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission607/Reviewer_H57b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission607/Reviewer_H57b"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a implicit representation based RGB-D SLAM system, that is able to handle dynamic in the scene well. Two major features, namely 1) motion masking from both semantic and motion segmentation, and 2) per-frame tracking with edge consistency loss, are described in detail. Evaluation and ablation are sufficient, and the numbers seem pretty strong."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper revisits an old topic in dynamic SLAM, i.e., getting rid of dynamic pixels before explicit / implicit optimization process. The idea of fusing motion segmentation mask over multiple keyframes is sound, and the evaluation reported are comprehensive and solid.\n- As a system paper, the author did a great job covering both the overall system design, and the key components (masking and tracking) that contributes to the better performance of overall system. Writing and visualization are very clear to follow."
                },
                "weaknesses": {
                    "value": "- While the overall writing is good, there are a few places that worthy of fix: e.g., in section 3.2 eq (4) there is no introduction on $j$ and $k$; also typos such as Camear in Fig 1. A thorough proofread is recommended."
                },
                "questions": {
                    "value": "It's more of a general question on the subject of RGB-D only SLAM: modern visual-inertial solution has been proven to be very accurate and robust in providing highly accuracy 6DoF camera pose at a local scene. Therefore it sounds reasonable to formulate (implicit) mapping as a separate problem from pose tracking. How do you see this project going next on the mapping side?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810874985,
            "cdate": 1698810874985,
            "tmdate": 1699635988108,
            "mdate": 1699635988108,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5eeMnWUIul",
                "forum": "mmCIov21zD",
                "replyto": "Bc9NahuYgl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responce to reviewer H57b"
                    },
                    "comment": {
                        "value": "Thanks for the positive and detailed review as well as the valuable suggestions for improvement. We would like to address the reviewer's concerns as follows: \n\n**W1: Lack introduction in Section 3.2 eq (4)**.   \nThank you for pointing it out. We have added the introduction of the meaning of variables $j$ and $k$ in Equation 4 to address confusion about the expression\u2018s significance. $j$ and $k$ stand for the keyframe ID, illustrating the optical flow mask warping process from the k-th to the j-th keyframe. All the modified text is highlighted in blue in the revised version.\n\n**W2: Typos in Fig 1**.   \nThank you for pointing out the spelling mistake in Fig 1. It is a typo and has been corrected. We have reviewed this paper to reduce occurrences of spelling issues in the revised paper.\n\n**Q1: The project going next on the (implicit) mapping**.   \nIt's my honor to discuss this question with you. We appreciate your opinion, and this is a question we have been contemplating recently as well. Actually, for the dynamic SLAM, it is crucial to `identify the dynamic objects` and `enhance the data association` in the tracking or mapping process. We follow this principle in designing the Rodyn-SLAM system, providing a specific technical solution based on neural RGB-D SLAM. While the method is relatively universal, minor adjustments (not based on depth information) may be necessary for different sensor inputs. \n\nIn a sense, the coupled inertial measurement is also an observation to enhance the data association utilizing IMU preintegration theory in optimization process. However, it is insufficient to rely only on inter-frame constraints from IMU if the visual tracking part is not adjusted. Although the modern visual-inertial solution can improve the robustness and accuracy compared with the pure visual solution in dynamic environments, the precision may not satisfy the purpose of separate mapping. \n\nFormulate (implicit) mapping as a separate problem from pose tracking has a latent problem in which the tracking pose may not be consistent with the current mapping results. Certainly, without considering the above practical problems, we can also conduct research on the separate mapping in dynamic scenes. Since the background motion has been provided, independently estimating the pose and reconstructing the geometry structure of dynamic objects becomes a crucial and outstanding problem. It seems that employing `object-based neural implicit mapping and object-level tracking` could offer improved solutions to the aforementioned issues."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651219067,
                "cdate": 1700651219067,
                "tmdate": 1700651219067,
                "mdate": 1700651219067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6gh2E9u7kM",
            "forum": "mmCIov21zD",
            "replyto": "mmCIov21zD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission607/Reviewer_HXpV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission607/Reviewer_HXpV"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for dealing with dynamic environments for the purpose of rib d slam.  A motion mask appears to be created from the optical flow field and a semantic mask.  A neural radiance field is computed and sampled rays that are invalid are used to help generate the motion mask."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents the algorithm used with a clear description.  Results are presented comparing to other techniques in the field."
                },
                "weaknesses": {
                    "value": "Only 2 datasets are used for testing.\nSome acronyms are not provided what is ATE? in the results section.\nIt does not appear that the authors address the degenerate cases for computing the fundamental matrix, how does their method handle this?\nWith the comparisons, you should have compared to orb slam 3 and possibly DVO slam, an indirect and direct traditional method.\nThere is no mention of computation times, can this run in real time and what type of computing resources are required for such.  With the computation of a radiance field and the extra processes, it is hard to see this functioning in real time"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission607/Reviewer_HXpV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission607/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700670678991,
            "cdate": 1700670678991,
            "tmdate": 1700670678991,
            "mdate": 1700670678991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5X9W4Yttfe",
                "forum": "mmCIov21zD",
                "replyto": "6gh2E9u7kM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "initial response to Reviewer HXpV"
                    },
                    "comment": {
                        "value": "Thanks. As the Reviewer HXpV submitted the review comments very late (on the last day of author-reviewer discussion period), please allow some time for a more comprehensive reply."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705404202,
                "cdate": 1700705404202,
                "tmdate": 1700727608518,
                "mdate": 1700727608518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aM9HFSY6gy",
                "forum": "mmCIov21zD",
                "replyto": "6gh2E9u7kM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responce to reviewer HXpV (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestions and address the reviewer\u2019s concerns as follows:\n\n**Q1: Testing dataset**.   \nFollowing traditional dynamic RGB-D SLAM methods [1,2,3], we assess the pose estimation results and reconstruction quality on the TUM RGB-D and BONN RGB-D datasets. As indicated in these papers, these datasets are sufficient to evaluate the performance of dynamic SLAM system. Additional evaluation results are presented in Appendix A.2, specifically in Tables 7 and 9 of the revised paper.\n\n**Q2: ATE meaning**.   \nWe have illustrated the meaning of ATE in the original paper. Please refer to the \"Metric\" part in the Experiment section for more details.\n\n**Q3: Comparison with ORB-SLAM3 and DVO SLAM**.   \nAs our method is a neural-based dynamic SLAM method, we selected the traditional dynamic SLAM methods, such as ReFusion [1], DynaSLAM [2], and MID-Fusion [3], as the baseline methods instead of the general static SLAM methods. ORB-SLAM3 [4] and DVO SLAM [5] rely on a static environment assumption and do not work robustly within a dynamic environment due to the inconsistent observation of geometry. \n\nDynaSLAM has showcased competitive pose estimation results in comparison to DVO-SLAM. We add the comparison results of DVO-SLAM in Table 2 in the revised paper, cited from DynaSLAM. Please refer to the original DynaSLAM paper for more details.\n\nORB-SLAM3 primarily introduces multiple map fusion and tightly integrated visual-inertial fusion techniques via MAP estimation. However, it still depends on the assumption of a static environment, making it similar to ORB-SLAM2 [6] in this regard. We have included the ORB-SLAM2 pose estimation results in Table 2, cited from ReFusion. To address your concerns, we also evaluated the results of ORB-SLAM3 and included them in Table 10 in Appendix A.6.\n\nOur comparison results with ORB-SLAM3 and DVO SLAM are as follows:\n\n| ATE[cm]      | f3/wk_xyz | f3/wk_half|  f3/wk_static |  f3/st_half| Avg.\n| :---        |    :----:   |   :----:   |     :----:   | :----:   |  :----:   | \n|  DVO SLAM      |  59.7   |     52.9   |   21.2  | 6.2 |  35.0 |\n|  ORB-SLAM3    |   28.1  | 30.5   | 2.1  | 2.6 | 15.9 |\n|  Ours | 8.3  | 5.6 | 1.7  | 4.4 | 5.0 |\n\n**Q4: Degenerate case for computing the fundamental matrix**.   \nActually, the issue of degeneracy in fundamental matrix computation is a fundamental problem in multi-view geometry and should be investigated as a standalone problem. This is not the contribution of our paper. We employ an engineering trick to address this problem, similar to approaches used in ORB-SLAM2 and ORB-SLAM3. We compute the fundamental matrix $\\mathbf{F}$ and homography matrix $\\mathbf{H}$, utilizing the RANSAC method and reprojection error to score the computed matrices $SF$ and $SH$. Then, we determine the ratio of scores with the equation $rh = SH/(SH+SF)$. If the value of $rh$ is bigger than 0.5, we choose the homography matrix for the pose estimation; otherwise, we choose the fundamental matrix.\n\n> [1] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019.\n\n> [2] Berta Bescos, Jose M Facil, Javier Civera, and Jose Neira. Dynaslam: Tracking, mapping, and inpainting in dynamic scenes. RAL, 2018.\n\n> [3] Binbin Xu, Wenbin Li, Dimos Tzoumanikas, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Mid-fusion: Octree-based object-level multi-instance dynamic slam. In ICRA, 2019.\n\n> [4] Carlos Campos, Richard Elvira, Juan J Gomez Rodr\u0131guez, Jose MM Montiel, and Juan D Tardos. Orb-slam3: An accurate open-source library for visual, visual\u2013inertial, and multimap slam. IEEE TRO, 2021.\n\n> [5] Christian Kerl, J \u0308urgen Sturm, and Daniel Cremers. Dense visual slam for rgb-d cameras. In IROS, 2013.\n\n> [6] Raul Mur-Artal and Juan D Tardos. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE TRO, 2017."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729808131,
                "cdate": 1700729808131,
                "tmdate": 1700740459592,
                "mdate": 1700740459592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KfbEiAjdpk",
                "forum": "mmCIov21zD",
                "replyto": "6gh2E9u7kM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responce to reviewer HXpV (2/2)"
                    },
                    "comment": {
                        "value": "**Q5: Computing resources and Computation times analysis**.   \nWe have reported the time consumption of the tracking and mapping process in Section 4.4. Please refer to Table 5 in the original paper. Our system outperforms E-SLAM [7] in terms of time consumption and maintains a comparable level with Co-SLAM [8]. Therefore, our system can run in real-time like Co-SLAM, which is reported as a real-time SLAM method in paper [8]. In terms of computing resources, we run our RoDyn-SLAM on an RTX 3090Ti GPU, taking roughly 4GB of memory in total (please see \"Implementation detail\" part in the Experiment section).\n\n\n**Contribution**.\nPlease allow us to briefly review the contribution of our Rodyn-SLAM. We propose a robust dynamic RGB-D SLAM with neural implicit representation. Existing neural RGB-D SLAM methods rely on a static environment assumption and do not work robustly within a dynamic environment due to the inconsistent observation of geometry and photometry. To tackle this problem, we propose a dynamic object identification method that combines the optical flow and semantics prior to generating the motion mask and filters the invalid sample ray to recover the static scene map. To further improve the accuracy of pose estimation in dynamic scenes, we propose a divide-and-conquer pose optimization algorithm utilizing edge projection loss to enhance data association between convisible frames. Extensive experiments are conducted on the two challenging datasets, and the results show that RoDyn-SLAM achieves state-of-the-art performance among recent neural RGB-D methods in both accuracy and robustness.\n\n> [7] JMohammad Mahdi Johari, Camilla Carta, and Franc \u0327ois Fleuret. Eslam: Efficient dense slam system based on hybrid representation of signed distance fields. In CVPR, 2023. \n\n> [8] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam. In CVPR, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729902690,
                "cdate": 1700729902690,
                "tmdate": 1700731227083,
                "mdate": 1700731227083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]