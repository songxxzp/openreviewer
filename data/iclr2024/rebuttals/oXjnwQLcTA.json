[
    {
        "title": "Score Models for Offline Goal-Conditioned Reinforcement Learning"
    },
    {
        "review": {
            "id": "kiMTqmIRtD",
            "forum": "oXjnwQLcTA",
            "replyto": "oXjnwQLcTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_9ANq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_9ANq"
            ],
            "content": {
                "summary": {
                    "value": "This paper formulates the offline goal conditioned reinforcement leanring (GCRL) problem as an occupancy matching problem and leverages the popular DICE methods to transform this problem into an objective that can be exclusively trained on offline dataset. Specifically, they adopts ValueDICE[1]-like method for this reformulation. The authors argue that by doing so, the offline GCRL problem can be solved without necessitating a well-trained discriminator. Instead, it learns unnormalized densities or scores that allow it to produce optimal goal-reaching policies, greatly enhancing the training results. In addition, this paper does not directly optimize the objectives derived from the DICE reformulation (Eq. 9), but adopts IQL[2] to optimize some surrogate objectives (Eq. 10-12). From my perspective, this approach mitigates the side-effect of residual learning in Eq. 9 since the surrogate objective in Eq. 11 is optimized via semi-gradient, which is believed to enjoy better performances than residual learning. This paper conducts some experiments on low-dimensional and image-based tasks to support the effectiveness of the proposed method, and also carries out some ablation stuidies on stochastic environments with varied noise levels.\n\n[1] Imitation Learning via Off-Policy Distribution Matching, ICLR 2020\n\n[2] Offline Reinforcement Learning with Implicit Q-Learning, ICLR 2022"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Despite GoFAR[3] already employing DICE methods to address offline GCRL problem, this paper is the first to eliminate the need for an additional discriminator.\n2. The experimental results are promising, outperforming baselines (especially the most related GoFAR[3]) across a wide range of evaluation settings.\n\n[3] How far i'll go: offline goal-conditioned reinforcement learning via f-advantage regression, NeurIPS 2022."
                },
                "weaknesses": {
                    "value": "## Problem formulation\n1. The target goal-transition distribution $q(s,a,g)$ defined in Section 3.1 might not be a valid discounted visitation distribution that fulfills the Bellman-flow constraint. Therefore, this suggests that the  occupancy matching problem in Eq. 4 may not be appropriately formulated.\n\n## Experiment\n2. This paper optimizes surrogate objectives using IQL[2] rather than directly solving the objective derived from DICE reformulation, but does not provide explainations for this choice. Moreover, this paper does not conduct enough ablation studies on this aspect. It raises a question about the performance of GoFAR using this same technique. In my view, optimizing the surrogate objectives in Eq. 10-12 can mitigate the side-effect of residual learning in Eq. 9. Therefore, GoFAR may also obtain large performance gains using the IQL tricks.\n3. Given that GoFAR does not employ IQL surrogate objectives, I cannot ensure the comparison in Figure 2 is fair. It would be more fair if the authors could also apply this technique to GoFAR and compare it against GoFAR using this approach.\n\n## Others\n4. Some potential overclaims. Section 3.1 is essentially an extension of the conclusion of GoFAR[3] from state-occupancy matching to state-action-occupancy matching. The authors should provide more discussions on the relationships with GoFAR. Although they have made statements like \"Proposition 1 extends the insights of formulating GCRL as an imitation learning problem from Ma et al. for goal-transition distributions when matching state-action-goal visitations\", this similarity should be made clearer.\n5. The Problem Formulation section (Section 2) is largely similar to the one in GoFAR[3] paper, with only minor rewording. In my view, this section should be reorganized or rewording a lot to avoid potential plagiarism."
                },
                "questions": {
                    "value": "Please refer to weakness for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7619/Reviewer_9ANq",
                        "ICLR.cc/2024/Conference/Submission7619/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697983380851,
            "cdate": 1697983380851,
            "tmdate": 1700203124367,
            "mdate": 1700203124367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7PA1CpCnRN",
                "forum": "oXjnwQLcTA",
                "replyto": "kiMTqmIRtD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9ANq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We provide clarifications below and would be happy to discuss further.\n\n1. **The target goal-transition distribution defined in Section 3.1 might not be a valid discounted visitation distribution that fulfills the Bellman-flow constraint. Therefore, this suggests that the occupancy matching problem in Eq. 4 may not be appropriately formulated.**\n\nWe would like to clarify that irrespective of the goal-transitions distribution being an infeasible visitation distribution (which we discuss in the paper as well), the distribution matching objective in Eq.4 still maximizes a lower bound to the goal-conditioned RL objective. This is proved in Propositions 1 and 2 in the paper.\n\n\n\n2. **This paper optimizes surrogate objectives using IQL. It raises a question about the performance of GoFAR using this same technique. Therefore, GoFAR may also obtain large performance gains using the IQL tricks.**\n\nWe would first like to point out a common misconception: GoFAR does not learn via residual learning even though it seems like it from Algorithm 2 in their paper. Their official code [linked here](https://github.com/JasonMa2016/GoFAR/blob/cfb2d4e36d69c5ec16edf6d127464eb263925e70/rl_modules/gofar_agent.py#L171) actually shows that semi-gradient learning is being used in GoFAR.\n\nOur practical algorithm adapts the derived GCRL objective for the offline setting by using in-sample maximization to avoid overestimation. We appreciate the suggestion and have added this ablation in the Appendix [Table 13]. GoFAR with IQL is seen to have nearly the same performance as vanilla GoFAR.\n\nWe note that the in-sample maximization and the constrained policy optimization used in the practical algorithm is not a contribution of our work and is simply taken from [1,2,3] and it has been shown previously unconstrained optimization is detrimental in offline setting [4].  \n\n\n4. **The authors should provide more discussions on the relationships with GoFAR. Proposition 1 similarity should be made clearer.**\n\n\nWe appreciate the suggestion and have clarified this in the updated paper. Our contribution lies mainly in proposing a mixture-distribution matching objective for GCRL as well as providing a connection to the original GCRL objective in Proposition 2.\n\n\n We do want to point out that while Proposition 1 bears similarity in the high-level intuition of distribution matching but differs in the fact that GoFAR matches state-goal transitions, which requires them to construct another lower bound (see Proposition 4.2 and Lemma B.2 in GoFAR[5]) in order to obtain a tractable optimization objective. In contrast, goal-transition distributions allow us to craft an objective that we can directly optimize. We believe that this is an important observation in itself. \n\nPlease find our expanded discussion on the theoretical and algorithmic differences of our method compared to GoFAR in the general discussion.\n\n5. **The Problem Formulation section (Section 2) is largely similar to the one in GoFAR paper, with only minor rewording. In my view, this section should be reorganized**\n\nWe appreciate the reviewer for pointing this out. Although our problem setting is exactly the same we recognize the importance of setting up our own context for the work. We have updated the Problem Formulation to address reviewers' concerns. The suggestions have been incorporated into the revision.\n\n----------\n\nPlease let us know if there are any remaining questions or concerns. We hope the reviewer can reassess our work in light of these clarifications and additional empirical results.\n\n----------\n\nReferences:\n\n[1]:Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline reinforcement learning with implicit q-learning.\" arXiv preprint arXiv:2110.06169 (2021).\n\n[2]: Peng, Xue Bin, et al. \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\" arXiv preprint arXiv:1910.00177 (2019).\n\n[3]: Sikchi, Harshit, et al. \"Dual rl: Unification and new methods for reinforcement and imitation learning.\" Sixteenth European Workshop on Reinforcement Learning. 2023.\n\n[4]: Fujimoto, Scott, David Meger, and Doina Precup. \"Off-Policy Deep Reinforcement Learning without Exploration. CoRR abs/1812.02900 (2018).\" arXiv preprint arXiv:1812.02900 (2018).\n\n[5]: Ma, Yecheng Jason, et al. \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via $ f $-Advantage Regression.\" arXiv preprint arXiv:2206.03023 (2022)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979111961,
                "cdate": 1699979111961,
                "tmdate": 1699979174924,
                "mdate": 1699979174924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z46C5E5UJK",
                "forum": "oXjnwQLcTA",
                "replyto": "7PA1CpCnRN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_9ANq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_9ANq"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!!"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the thorough responses and my concerns are well-resolved. Thus, I'm happy to increase my score to 6. Thank you!!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203101978,
                "cdate": 1700203101978,
                "tmdate": 1700203101978,
                "mdate": 1700203101978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PtAfWzue1X",
            "forum": "oXjnwQLcTA",
            "replyto": "oXjnwQLcTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_ojhF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_ojhF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an offline GCRL algorithm from the occupancy matching perspective. The problem setting basically follows GoFAR [1], using a similar DICE-based construction. The difference is that GoFAR is formulated as a V-DICE (learn V and perform goal-conditioned state occupancy matching) and does not use HER; while this paper adopts Q-DICE (learn both Q and $\\pi$ by solving a max-min problem, and performs goal-conditioned state-action occupancy matching) and uses HER to generate desirable goal-reaching data ($q(s,a,g)$). The tricky part is that the final practical algorithm is essentially a goal-conditioned version of in-sample learning algorithm which bears lots of similarities with methods like IQL[2], SQL/EQL[3], and XQL[4]. Although such in-sample learning algorithms like SQL/EQL and XQL have been show to have some connection with DICE-based methods, there are some important distinctions. There are some noticeable theoretical gaps here. The proposed method has a number of strange design choices and the algorithm development is not very principled in several places. See the following strengths and weaknesses for detailed comments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed practical algorithm provides a reasonable approach to combine goal-conditioning and in-sample learning offline RL.\n- The performance is good in a number of GCRL tasks. The experiments are comprehensive.\n- Provide experiments on vision-based tasks."
                },
                "weaknesses": {
                    "value": "There are several key weaknesses in the paper.\n- The problem setting and Section 3.1 largely follow GoFAR[1] with minor changes. A key motivation from the authors is that methods like GOFAR need an unstable discriminator-based construction. However, this does not really hold. As in the GoFAR paper[1], their authors clearly mentioned in the paper that the discriminator can be bypassed by using the reward in the dataset.\n- Although using HER to generate augmented goal-transition samples could potentially improve distribution coverage and may contribute to certain level performance improvement. However, as discussed in GoFAR, using HER could also lead to sensitive hyperparameter tuning and suffer from hindsight bias.\n- The biggest problem of this paper is the gap between theoretical derivation and the practical algorithm. Based on the augmented samples from HER, the proposed method constructs a goal-conditioned Q-DICE objective which needs to solve a max-$\\pi$ and min-$S$ (analogous to Q function in typical Q-DICE algorithm like AlgaeDICE[5]). This actually caused some stability issues due to extracting $\\pi$ through the max-min optimization problem. Hence the practical algorithm directly jumps to an in-sample learning framework which is similar to IQL[2], SQL/EQL[3], and XQL[4]. Although there are some connections between the DICE-based method and previous in-sample learning methods, there are also some distinctions. An apparent difference is that the DICE-based method requires minimizing $S$ (analogous to Q in other Q-DICE methods) in the first term of Eq.(9) using samples from initial states $d_0$, while in in-sample learning algorithms, this can be sampled from the whole dataset $\\mathcal{D}$ (Eq.(10)). Second, DICE-based method only learn Q (similar to S in this paper) or V, while the previous in-sample learning algorithms learn both Q and V. In my opinion, the paper starts with a DICE formulation and goes a long way to turn it into a non-DICE algorithm. If the authors check the SQL/EQL[3] paper, a more straightforward approach will be starting with its implicit value regularization framework and designing a proper, goal-conditioned regularization function $f$, which will provide a neat and more principled algorithm.\n- The proposed algorithm has many hyperparameters, e.g. $\\tau$ in Eq.(10), $\\beta$ in Eq.(11), and $\\alpha$ in Eq.(12). The paper conducts heavy tuning to obtain the best performance. First of all, for an offline RL algorithm, introducing too many hyperparameters and requiring heavy parameter tuning is an extremely bad practice. In practical offline RL applications, it is almost impossible to evaluate or tune model parameters given restricted access to the real environment. In practice, no one will use an offline RL algorithm if it needs careful hyperparameter tuning to achieve good performance.\n\n\n**References:**\n\n[1] Ma, J. Y., et al. Offline goal-conditioned reinforcement learning via $ f $-advantage regression. NeurIPS 2022.\n\n[2] Kostrikov I, Nair A, Levine S. Offline Reinforcement Learning with Implicit Q-Learning ICLR 2022.\n\n[3] Xu, H., et al. Offline RL with no OOD actions: In-sample learning via implicit value regularization. ICLR 2023.\n\n[4] Garg, D., Hejna, J., Geist, M., & Ermon, S. Extreme Q-Learning: MaxEnt RL without Entropy. ICLR 2023.\n\n[5] Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., & Schuurmans, D. Algaedice: Policy gradient from arbitrary experience."
                },
                "questions": {
                    "value": "- Please report the hyperparameter $\\alpha$ values in your experiments.\n- How will the proposed method perform if not tuned individually for each task? Such as using 1~3 sets of hyperparameters. \n- All datasets in the experiments are mixed with some expert data. How will the proposed method perform if only uses sub-optimal data samples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7619/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7619/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7619/Reviewer_ojhF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135863047,
            "cdate": 1698135863047,
            "tmdate": 1700040421309,
            "mdate": 1700040421309,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2dQIzetlg2",
                "forum": "oXjnwQLcTA",
                "replyto": "PtAfWzue1X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ojhF (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We provide clarifications below and would be happy to discuss further.\n\n1. **A key motivation from the authors is that methods like GOFAR need an unstable discriminator-based construction. However, this does not really hold. As in the GoFAR paper[1], their authors clearly mentioned in the paper that the discriminator can be bypassed by using the reward in the dataset.**\n\nWhile GoFAR author suggests that discriminator can be bypassed by using rewards, their experiments are hardcoded to use discriminator (official code line [link](https://github.com/JasonMa2016/GoFAR/blob/cfb2d4e36d69c5ec16edf6d127464eb263925e70/train.py#L130). Please find our results using reward function instead of a discriminator with GoFAR in Table 13 of the revised paper and an extended discussion of differences with GoFAR in the general response comment. GoFAR without discriminator reduces to an RL with a sparse rewards algorithm and has poor performance.\n\n\n2. **Although using HER to generate augmented goal-transition samples could potentially improve distribution coverage and may contribute to certain level performance improvement. However, as discussed in GoFAR, using HER could also lead to sensitive hyperparameter tuning and suffer from hindsight bias.**\n\nFigure 4 in GoFAR demonstrates that HER does not improve performance for GoFAR. Our work, as well as GoFAR, compares to baselines with access to HER data augmentation. A fair comparison directs that we hold our method to the same standards. Removing hindsight bias from HER is an orthogonal exploration (ex. [1]) and our method presents a goal-conditioned policy learning approach applicable for any offline data distribution. We have added a comparison of our method without HER in Table 12 where it can be seen that HER contributes to a small performance gain, even without which we outperform GoFAR. \n\n3. **The final practical algorithm is essentially a goal-conditioned version of in-sample learning algorithm which bears lots of similarities with methods like IQL, SQL/EQL, and XQL.**\n\n\nThe work diverges from prior works like  IQL, SQL/EQL, and XQL in the fundamental sense that we optimize for \\textit{distribution matching as opposed to reward maximization}. A goal-conditioned version of an in-sample learning algorithm would consider minimizing the bellman optimality error with sparse reward functions, whereas our loss function presents a reward-free contrastive objective with smoothness regularization.\n\nOur practical objective uses in-sample maximization as a tool rather than the algorithm. In-sample maximization is a general technique to avoid overestimation of the maximum by avoiding OOD data points and is not specific to offline RL methods [2]. We also note that a connection to in-sample learning methods should not be surprising as prior work [3] shows in-sample learning can be derived from DICE."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978570471,
                "cdate": 1699978570471,
                "tmdate": 1699978570471,
                "mdate": 1699978570471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uls6c1RXbA",
                "forum": "oXjnwQLcTA",
                "replyto": "Ql2CMgN69W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_ojhF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_ojhF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for the detailed responses and clarifications. While I still have some reservations regarding the gaps between the theoretical derivation and the final practical algorithm, I think the authors have done a reasonable job in their rebuttal discussions. Therefore, I have increased my evaluation score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040403469,
                "cdate": 1700040403469,
                "tmdate": 1700040403469,
                "mdate": 1700040403469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aMuQJ57V33",
            "forum": "oXjnwQLcTA",
            "replyto": "oXjnwQLcTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_V1Bw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_V1Bw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for offline goal-conditioned RL, integrating occupancy matching with a convex dual formulation so that the learning objective is converted for better leveraging on suboptimal offline data. Instead of behavior cloning, contrastive RL, or RL with sparse reward, the proposed method is built upon the direction of occupancy matching but without learning an additional discriminator. The proposed method is supported by theorems and evaluated with comprehensive simulation experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well-written, and the concepts are clearly and concisely explained\n* The authors provide theoretical contributions.\n* The paper's contributions are supported by empirical analyses on a range of benchmarks, demonstrating the advantage of using SMORe for suboptimal offline data, especially the evaluation of robustness and high-dimensional observation space."
                },
                "weaknesses": {
                    "value": "* Could you please elaborate more about the technical differences between SMORe and GoFAR? My understanding is that the main difference is whether the training involved a discriminator. Does any other difference in details improve the novelty of SMORe?"
                },
                "questions": {
                    "value": "* Is the \"0.25\" in equation 9 a fixed number or a kind of coefficient that can be tuned? \n\n* Any reason why you make the mixture of random/medium and expert data following 4.1 EXPERIMENTAL SETUP?  I thought that there is already a pipeline for collecting random, medium, and expert sub-dataset?\n\n* Can you explain why SMORe has an even higher discounted return in Figure 2 under 0.5 noise level compared with 0 noise? In addition, I am interested in the variance of Figure 2.\n\n* Is there any insight or analysis into why pick and place tasks in Figure 3 are relatively difficult for other baselines compared with the remaining tasks while the proposed method can have a significant improvement in pick and place tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698463742701,
            "cdate": 1698463742701,
            "tmdate": 1699636924669,
            "mdate": 1699636924669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iEdEuik4Yb",
                "forum": "oXjnwQLcTA",
                "replyto": "aMuQJ57V33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V1Bw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We provide clarifications below and would be happy to discuss further.\n\n1. **Could you please elaborate more about the technical differences between SMORe and GoFAR? My understanding is that the main difference is whether the training involved a discriminator. Does any other difference in details improve the novelty of SMORe?**\n\nOur novelty compared to GoFAR can be summarized in two parts: a) Theoretical: 1. We present a novel objective of mixture distribution matching for GCRL that provably optimizes the GCRL objective without requiring a discriminator. 2.  \nGoFAR considers matching state-goal distributions which requires constructing a loose lower bound (Proposition B.1 and Lemma B.1 in [1]) that is not tight for any $f$-divergence. Using goal-transition distribution (state-action-goal) allows us to create a lower bound to the original GCRL objective that is tight for KL divergence. b) Algorithmic: 1. GoFAR requires a three-step learning process of first learning discriminator, then value, and finally extracting a policy. Our method presents a simpler two-step approach by directly learning the optimal score function and corresponding policy. Empirically our method achieves significant gains across the board compared to GoFAR. \n\nWe refer the reviewer to the general discussion comment where we present a more thorough comparison with GoFAR.\n\n\n2. **Is the \"0.25\" in equation 9 a fixed number or a kind of coefficient that can be tuned?**\n\n0.25 is a fixed number dictated by the use of $\\chi^2$ divergence. It weighs the smoothness regularization vs the contrastive-objective ensuring the resulting objective we optimize has principled connections to the original GCRL objective we are interested in.\n\n3. **Any reason why you make the mixture of random/medium and expert data following 4.1 EXPERIMENTAL SETUP? I thought that there is already a pipeline for collecting random, medium, and expert sub-dataset?**\n\nWe do not collect our own dataset for Fetch or Sawyer environments. These datasets are part of the benchmark established by previous work [1,2]. We explain the dataset collection procedure used the by previous work in the appendix only for completeness.\n\nNote that the locomotion tasks for GCRL as well their datasets are introduced by us for the first time.\n\n3.**Can you explain why SMORe has an even higher discounted return in Figure 2 under 0.5 noise level compared with 0 noise? In addition, I am interested in the variance of Figure 2.**\n\nWe suspect that is a consequence of different quality of transitions in dataset collected with different noise levels. It can also be a result of noise with finite seeds. We appreciate the suggestion of showing variance and have included the standard deviation now in the updated paper. \n\n4. **Is there any insight or analysis into why pick and place tasks in Figure 3 are relatively difficult for other baselines compared with the remaining tasks while the proposed method can have a significant improvement in pick and place tasks?**\n\nThe final pick and place task is more complicated in the sense that it requires precise manipulation. The gripper can collide with the drawer resulting in failure to complete the tasks. Our hypothesis is that, in other tasks, mistakes are more forgivable and allow other baselines to perform reasonably as well. \n\n--------------------\n\nPlease let us know if there are any remaining questions or concerns. We hope the reviewer can reassess our work in light of these clarifications and additional empirical results.\n\n-----------------\n\nReferences:\n\n[1]  Ma, Yecheng Jason, et al. \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via $ f $-Advantage Regression.\" arXiv preprint arXiv:2206.03023 (2022).\n\n[2] Yang, Rui, et al. \"Rethinking goal-conditioned supervised learning and its connection to offline rl.\" arXiv preprint arXiv:2202.04478 (2022)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979376008,
                "cdate": 1699979376008,
                "tmdate": 1699979577642,
                "mdate": 1699979577642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3yTOsS03Fu",
                "forum": "oXjnwQLcTA",
                "replyto": "aMuQJ57V33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up on rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide a thoughtful review and are motivated that they found the work well-written and acknowledged our theoretical contributions and empirical experiments.\n\nWe believe we have responded to the questions and concerns in full, but if something is missing please let us know and we would be happy to add it. If we have addressed your concerns we would appreciate it if the reviewer could reassess our work in light of these clarifications and additional empirical results."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324483830,
                "cdate": 1700324483830,
                "tmdate": 1700324913742,
                "mdate": 1700324913742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ClbPJtNjo",
                "forum": "oXjnwQLcTA",
                "replyto": "3yTOsS03Fu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_V1Bw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Reviewer_V1Bw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all your detailed clarifications, which make this manuscript much clearer. I do not have any further questions, and I will keep my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625137198,
                "cdate": 1700625137198,
                "tmdate": 1700625137198,
                "mdate": 1700625137198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nC71faUsvU",
            "forum": "oXjnwQLcTA",
            "replyto": "oXjnwQLcTA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_Wf6E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7619/Reviewer_Wf6E"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach for offline goal-conditioned reinforcement learning, known as SMORe, which is derived from a mixture-distribution matching perspective and eliminates the need for learning a discriminator. The paper demonstrates that SMORe outperforms state-of-the-art baselines in various robot manipulation and locomotion tasks, including high-dimensional observations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and clearly motivates the problem of offline GCRL. It uses convex duality theory to derive a dual optimization problem that can use offline data to learn score functions and policies, which provides a rigorous theoretical analysis of the proposed method. It also presents extensive empirical results that demonstrate the effectiveness and robustness of SMORe on challenging benchmarks."
                },
                "weaknesses": {
                    "value": "One weakness of the paper is that the claim of being discriminator-free is somewhat overclaiming. From Eq. 12, the S-function can be seen as a Q-function and the M-function can be seen as a V-function. In this case, although the framework does not have an explicit discriminator, the S-function actually plays the role of a discriminator. What's more, the proposed method requires two networks, S and M, to learn the optimal policy, while many works only require a network (such as contrastive RL). This implies that the proposed method has more parameters and computational complexity.\nAnother weakness is that it is unclear how much of the performance gain comes from each component. A possible suggestion is to add an experiment without using expectile regression and AWR, as well as experiments that show how they work separately. This would clarify the role of each part in the proposed method."
                },
                "questions": {
                    "value": "Why do the WGCSL and GCSL methods have similar performance in Table 7-10, while show a large difference in Table 1 for the following four environments: CheetahTgtVel-m-e, CheetahTgtVel-r-e, AntTgtVel-m-e and AntTgtVel-r-e? What factors could explain this discrepancy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7619/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744152048,
            "cdate": 1698744152048,
            "tmdate": 1699636924508,
            "mdate": 1699636924508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FgXgEBt9Vp",
                "forum": "oXjnwQLcTA",
                "replyto": "nC71faUsvU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7619/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Wf6E"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We provide clarifications below and would be happy to discuss further.\n\n1. **One weakness of the paper is that the claim of being discriminator-free is somewhat overclaiming. In this case, although the framework does not have an explicit discriminator, the S-function actually plays the role of a discriminator.**\n\n\nWe clarify that S-function is not a discriminator, as it reasons about the unnormalized long-term value of an action in reaching a goal. Contrary to GoFAR which requires a three-step process - a. Learn reward as discriminator b. Train optimal value function using this reward c. Extract Policy using the value function, our method requires two steps -- Learn the optimal score/value function and extract policy. Our score function is not learned via a classification loss and does not resemble a discriminator.\n\n\n\n2. **What's more, the proposed method requires two networks, S and M, to learn the optimal policy, while many works only require a network (such as contrastive RL). This implies that the proposed method has more parameters and computational complexity.**\n\nWhile works like contrastive RL only require a single network, they are provable suboptimal as we discuss in the paper. While our method is indeed more computationally complex than contrastive RL, it bears the same computational complexity as GoFar[1] and IQL[2]. Additionally, it is stable to train with little hyperparameter tuning ($\\tau$) in all environments and also robust to few-expert coverage in the offline dataset. In real-world problems, often data not the compute is often the bottleneck. \n\n\n\n3.  **Another weakness is that it is unclear how much of the performance gain comes from each component. A possible suggestion is to add an experiment without using expectile regression and AWR, as well as experiments that show how they work separately. This would clarify the role of each part in the proposed method.**\n\n\n\nOur practical algorithm adapts the derived GCRL objective for the offline setting by using in-sample maximization to avoid overestimation. We appreciate the suggestion and have added this ablation in the Appendix Table 16.  We observed a value function blowup leading to poor policy performance without using techniques to constrain learned policy to the offline data distribution. An identical approach was taken by [6, NeurIPS 2023] when adapting their preference learning method to an offline setting.  We have also added another ablation of using expectile loss and AWR in GoFAR in Table 13.\n\n\nWe note that the in-sample maximization and the policy optimization used in the practical algorithm is not a contribution of our work and is taken from [2,3,4] and it has been shown previously unconstrained optimization is detrimental in offline setting [5]. \n\n\n4. **Why do the WGCSL and GCSL methods have similar performance in Table 7-10, while show a large difference in Table 1 for the following four environments: CheetahTgtVel-m-e, CheetahTgtVel-r-e, AntTgtVel-m-e and AntTgtVel-r-e? What factors could explain this discrepancy?**\n\nWe appreciate the reviewer pointing out this interesting observation for a comparison of baselines between WGCSL and GCSL. Our hypothesis was the following: The datasets we consider are bi-modal consisting of transitions coming from suboptimal trajectories and expert trajectories. GCSL given a goal, is able to easily distinguish goals to select a particular mode and get good performance. WGCSL relies on learning a critic that additionally influences the policy. Any error in the critic, can translate to policy error which compounds over time.\n\n-------------------\n\nPlease let us know if there are any remaining questions or concerns. We hope the reviewer can reassess our work in light of these clarifications and additional empirical results.\n\n----------------\n\nReferences:\n\n\n[1] Ma, Yecheng Jason, et al. \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via $ f $-Advantage Regression.\" arXiv preprint arXiv:2206.03023 (2022).\n\n[2] Kostrikov, Ilya, Ashvin Nair, and Sergey Levine. \"Offline reinforcement learning with implicit q-learning.\" arXiv preprint arXiv:2110.06169 (2021).\n\n[3] Peng, Xue Bin, et al. \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\" arXiv preprint arXiv:1910.00177 (2019).\n\n[4] Sikchi, Harshit, et al. \"Dual rl: Unification and new methods for reinforcement and imitation learning.\" Sixteenth European Workshop on Reinforcement Learning. 2023.\n\n[5] Fujimoto, Scott, David Meger, and Doina Precup. \"Off-Policy Deep Reinforcement Learning without Exploration. CoRR abs/1812.02900 (2018).\" arXiv preprint arXiv:1812.02900 (2018).\n\n[6]: Hejna, Joey, and Dorsa Sadigh. \"Inverse Preference Learning: Preference-based RL without a Reward Function.\" arXiv preprint arXiv:2305.15363 (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7619/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979558063,
                "cdate": 1699979558063,
                "tmdate": 1699979558063,
                "mdate": 1699979558063,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]