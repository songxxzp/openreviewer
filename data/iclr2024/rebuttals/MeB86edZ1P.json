[
    {
        "title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks"
    },
    {
        "review": {
            "id": "ONAde7cRvk",
            "forum": "MeB86edZ1P",
            "replyto": "MeB86edZ1P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission828/Reviewer_Me3Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission828/Reviewer_Me3Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper applies Oja's rule to implement gradient projection for continual learning. The method uses lateral circuits to avoid storing gradient explicitly, and works with spiking networks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To the best of my knowledge, the method is novel. Orthogonalized gradients are known to perform well for continual learning, and Oja's rule is known to do PCA, but (as far as I know) they've never been combined like in this paper. The resulting method doesn't need to explicitly store gradients, and computes the projection matrix on the go with bio plausible operations (and not recursive least squares). This is an advantage over other projection-based methods. \n\nThe method only uses Hebbian and anti-Hebbian plasticity, which makes it suitable for neuromorphic hardware and also biologically plausible. There's a caveat for biological networks though: the lateral circuits are only active during the backward pass, and don't interfere with the forward one. However, an exact biological implementation seems out of scope for this work.\n\nPerformance: on all (standard) benchmarks the method performs very well and usually outperforms other algorithms."
                },
                "weaknesses": {
                    "value": "Some (not very critical) weaknesses:\n\n1. The method needs to create new subspaces for each task, and then coordinate activity in a new subspace with the old ones. It's not clear if that can scale to many tasks (e.g. due to noise in PCA through Oja's rule) \n2. The lateral connectivity doesn't influence forward propagation (and it shouldn't due to projections), which might make it hard to implement with real neurons (not so sure about neuromorphic chips).\n3. There are no evaluations on non-spiking networks, so it's not clear if the performance improvement over other methods is due to those being poorly suited for spikes or this method being better at continual learning."
                },
                "questions": {
                    "value": "The proposed architecture looks like a model of memory, since neurons only update their weights if they haven't seen a specific input before. Can all task-specific lateral circuits be combined into a single associative memory module, like a Hopfield net, with each new memory being a new $y$ for the task?\n\nBefore Sec. 2:\n>  Our results indicate that lateral circuits, which are long ignored by popular feedforward neural network\n\nI\u2019d say that lateral circuits are often present implicitly through normalization layers.\n\nTab. 2: boldface should be used for the best performing method in a column, not the author's method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698422477606,
            "cdate": 1698422477606,
            "tmdate": 1699636010336,
            "mdate": 1699636010336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eUZSgRPHBT",
                "forum": "MeB86edZ1P",
                "replyto": "ONAde7cRvk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Me3Q (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our work and providing valuable comments. We respond to your comments and questions as follows.\n\n1. About scalability to many tasks.\n\nOur experiments verified effectiveness on common datasets with many tasks: PMNIST (10 tasks), CIFAR-100 (10 tasks), miniImageNet (20 tasks), 5-Datasets (5 tasks), also with similar or distinct input domains and online or multi-epoch training settings. So it can scale to many tasks. \n\nActually, although there can be slight noise for PCA, our method enables unbiased streaming PCA that leverages all data, while previous gradient projection methods either only use a small batch of data to perform SVD and PCA [1,2] which can have larger noise and bias, or use recursive least square algorithm [3] that introduces a parameter $\\alpha$ for calculation of inverses which will have bias too. Our method has improvements over them.\n\n2. The lateral connectivity does not influence forward propagation.\n\nYes, our lateral connections with subspace neurons mainly modify the activity traces (i.e., eligibility traces for SNNs under some training algorithms) of feedforward neurons for weight update, and do not influence the model\u2019s forward propagation. While it mainly influences the calculation of gradients, this does not mean that it is only active during the backward pass \u2013 as discussed in Section 4.1, since HLOP only modifies presynaptic activity traces, this lateral projection can be parallel with the forward and backward propagation of the main network. That means, during the forward propagation of the main network, lateral connections can be simultaneously activated to modify traces of feedforward neurons, and once global error signals reach, weight updates are calculated based on the error signal and modified eligibility traces. We may make some conjectures that with some mechanisms such as the refractory period or adaptive threshold, the (fast) lateral connection may not induce spike signals for forward propagation but influence eligibility traces. It can be interesting future work to further consider if there can be biological correspondence and it is currently out of scope for this work.\n\n3. About evaluations on non-spiking networks.\n\nThank you for your valuable suggestion. Here we supplement all the corresponding results of ANNs that replace the spiking neuron by ReLU activation.\n\n| Neural Network | Method | PMNIST (ACC, BWT) | CIFAR-100 (ACC, BWT) | miniImageNet (ACC, BWT) | 5-Datasets (ACC, BWT) |\n| :----: | :----: | :----: | :----: | :----: | :----: |\n| SNN | *Multitask* | *96.15*, / | *79.70*, / | *79.05*, / | *89.67*, / |\n| SNN | Baseline | 70.61, -28.88 | 75.13, -4.33 | 40.56, -32.42 | 45.12, -60.12 |\n| SNN | MR | 92.53, -4.73 | 77.67, -1.01 | 58.15, -8.71 | 79.66, -14.61 |\n| SNN | EWC | 91.45, -3.20 | 73.75, -4.89 | 47.29, -26.77 | 57.06, -44.55 |\n| SNN | HAT | N.A., N.A. | 73.67, -0.13 | 50.11, -7.63 | 72.72, -22.90 |\n| SNN | GPM | 94.80, -1.62 | 77.48, -1.37 | 63.07, -2.57 | 79.70, -15.52 |\n| SNN | **HLOP (ours)** | 95.15, -1.30 | 78.58, -0.26 | 63.40, -0.48 | 88.65, -3.71 |\n| ANN | *Multitask* | *96.81*, / | *80.33*, / | *82.05*, / | *88.65*, / |\n| ANN | Baseline | 73.55, -25.67 | 71.47, -7.82 | 38.87, -31.58 | 68.54, -30.32 |\n| ANN | MR | 92.44, -5.00 | 75.59, -2.68 | 55.15, -6.79 | 83.98, -9.29 |\n| ANN | EWC | 90.16, -3.46 | 72.49, -6.01 | 47.41, -21.46 | 68.60, -29.90 |\n| ANN | HAT | N.A., N.A. | 71.31, 0.00 | 56.98, -1.64 | 88.40, -3.63 |\n| ANN | GPM | 94.92, -1.56 | 77.56, -1.42 | 65.13, -0.96 | 84.88, -9.17 |\n| ANN | **HLOP (ours)** | 95.25, -1.28 | 78.98, -0.34 | 66.80, 1.93 | 88.68, -2.74 |\n\nAs shown in the results, the performance of some methods can differ considering ANNs and SNNs. Particularly, on 5-Datasets, HLOP has more improvements for SNNs. It may imply that our method can be better combined with spikes, for example probably because subspaces expanded by spike signals are harder to learn and therefore our Hebbian learning performing streaming PCA has more advantages over GPM that only performs SVD/PCA on a small batch of data which can be biased. Also, our method is promising for ANNs as well. As there is no systematic study on the difference between the continual learning performance of ANNs and SNNs, it can be interesting for future work to study it.\n\n4. About associative memory module.\n\nAs discussed in Appendix F, our Hebbian learning to extract principal subspaces of streaming data is to some extent similar to memory but is not in an associative approach, and can well fit into our projection formulation. Currently, we have no idea if an associative memory module can fit into the projection formulation, because in the task we mainly require some information about the general subspace for restriction rather than specific associative memory for a given input."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362123296,
                "cdate": 1700362123296,
                "tmdate": 1700362123296,
                "mdate": 1700362123296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bxjoQ7NcPT",
                "forum": "MeB86edZ1P",
                "replyto": "QBq1BYCMLe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_Me3Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_Me3Q"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response! I think it addressed all of my concerns, so I'm happy with the current score of 8.\n\nI also read the concerns raised by other reviewers, but I believe they're mostly addressed by the paper/responses. I should also note that the new ANN experiments significantly improve the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594026211,
                "cdate": 1700594026211,
                "tmdate": 1700594026211,
                "mdate": 1700594026211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NJ7EslKRUJ",
            "forum": "MeB86edZ1P",
            "replyto": "MeB86edZ1P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission828/Reviewer_Txkh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission828/Reviewer_Txkh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Hebbian learning based orthogonal projection (HLOP) as a novel method for implementing orthogonal projection using neuronal operations. Building upon the method of calculating projection matrices through SVD proposed by Saha et al. (2021), HLOP combines the properties of Hebbian learning to approximate orthogonal projection using learned weight matrices. This is the first approach that fully utilizes neuronal operations for implementing orthogonal projection. Furthermore, HLOP outperforms several baseline methods on multiple continual learning datasets, demonstrating the reliability of the proposed method through experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper introduces for the first time a method that fully utilizes neuronal operations to approximate orthogonal projection matrices, and achieves the best performance surpassing multiple baseline methods on various datasets in continual learning task.\n2.The implementation based on Hebbian learning can seamlessly integrate with different training methods and aligns well with the parallel local learning approach of neuromorphic chips.\n3.HLOP can be directly applied to SNNs, providing a new approach for continual learning in SNNs."
                },
                "weaknesses": {
                    "value": "1.The method lacks sufficient detail in the methodology section. Providing complete formulas or a specific illustrative example would enhance understanding. \n2.The baselines compared in the study include EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), and GPM (Saha et al., 2021). It would be valuable to investigate if there are recent works that achieve better results than these methods.\n3.Although the continual learning approach in this study relies solely on neuronal operations without directly utilizing past data, the increasing number of subspace neurons with each new task learned implies a form of data compression and storage to some extent.\n4. Although the paper uses Hebbian learning to achieve a pure neuronal operation method, it seems that the newly added subspace neurons do not participate in the model's forward process. If this is the case, then despite being a pure neuronal operation approach, it is essentially just an estimation method for orthogonal projection matrices.\n5. While the paper demonstrates the effectiveness of HLOP, it does not provide an explanation or analysis of why the weight matrices obtained through Hebbian learning can serve as a substitute for the orthogonal projection matrix."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Reviewer_Txkh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698576247642,
            "cdate": 1698576247642,
            "tmdate": 1700990119472,
            "mdate": 1700990119472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DxBZgM8Pdd",
                "forum": "MeB86edZ1P",
                "replyto": "NJ7EslKRUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Txkh"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We respond to your comments and questions as follows.\n\n1. Method details.\n\nIn Section 4, we mainly illustrate the procedure of our method in Figures 1 and 2, and formulas are in the texts due to the space limit. More detailed descriptions of HLOP can be found in Appendix C, and details of the combination with different SNN training methods are presented in Appendix A.\n\n2. Baselines.\n\nThank you for the suggestion. Since not many continual learning methods consider SNNs and we do not find SNN baselines on these common datasets, we mainly implement and compare representative methods of different kinds of methods (i.e., memory replay, regularization, and gradient projection) based on their released codes under our SNN settings, as we cannot exhaustingly reimplement all ANN continual learning methods. Note that the performance of some methods can differ considering ANNs and SNNs (please see our response to Reviewer Me3Q).\n\nThere are some recent works [1] improving the gradient projection method GPM [2], which is our sota baseline. We try to test TRGP [1] based on their released code under our setting. On PMNIST and CIFAR-100, it does not outperform GPM as shown below, while on miniImageNet and 5-Datasets, we encounter failures that training fails after one or two tasks (i.e., the performance for new tasks is only random guess, but old tasks are maintained). It may probably be the difference between ANN and SNN, or hyperparameters and training settings, or some bugs in the code. Given the limited time, we cannot thoroughly analyze their method and codes. We think that current comparison is enough to show our superiority.\n\n| Method | PMNIST (ACC, BWT) | CIFAR-100 (ACC, BWT) |\n| :----: | :----: | :----: |\n| GPM | 94.80, -1.62 | 77.48, -1.37 |\n| TRGP | 94.56, -1.08 | 76.77, -2.61 |\n| **HLOP (ours)** | 95.15, -1.30 | 78.58, -0.26 |\n\n3. Subspace neurons and data storage.\n\nYes, our subspace neurons to some extent represent memory of some information of past tasks, and previous gradient projection method is also called Gradient Projection Memory [2], because we need information (of past tasks) to guide the restriction on weight update to solve forgetting. However, it is not explicit memory and thus does not require storing raw data which may bring concerns about data privacy [2].\n\n4. Subspace neurons do not participate in the model\u2019s forward process.\n\nOur lateral connections with subspace neurons mainly modify the activity traces (i.e., eligibility traces for SNNs under some training algorithms) of feedforward neurons for weight update, and do not influence the model\u2019s forward propagation. While it mainly influences the calculation of gradients, this does not mean that it is only active during the backward pass \u2013 as discussed in Section 4.1, since HLOP only modifies presynaptic activity traces, this lateral projection can be parallel with the forward and backward propagation of the main network. That means, during the forward propagation of the main network, lateral connections can be simultaneously activated to modify traces of feedforward neurons, and once global error signals reach, weight updates are calculated based on the error signal and modified eligibility traces. We may make some conjectures that with some mechanisms such as the refractory period or adaptive threshold, the (fast) lateral connection may not induce spike signals for forward propagation but influence eligibility traces. It is interesting future work to further consider if there is biological correspondence and it is currently out of scope for this work.\n\n5. Why weight matrices obtained through Hebbian learning can serve as a substitute for orthogonal projection matrix.\n\nAs mentioned in Section 4.1, this is the known result that Hebbian learning can extract principal components of streaming inputs [3,4]. It has been shown that our considered formulation enables weights to converge to a dominant principal subspace [4]. In Appendix C, we also mentioned an intuitive explanation: Hebbian learning can be viewed as stochastic gradient descent for minimizing the objective function \n\n$ \\min_{\\mathbf{H}_l'} \\mathbb{E} \\left\\lVert \\mathbf{x}_l - \\mathbf{H}_l^{\\prime\\top}\\mathbf{H}_l'\\mathbf{x}_l \\right\\rVert^2 $, \n\nand in our setting that updates new lateral weights with Hebbian learning, it can be viewed as minimizing \n$ \\min_{\\mathbf{H}_l'} \\mathbb{E} \\left\\lVert \\mathbf{x}_l - \\mathbf{H}_l^\\top\\mathbf{H}_l\\mathbf{x}_l - \\mathbf{H}_l^{\\prime \\top}\\mathbf{H}_l'\\mathbf{x}_l \\right\\rVert^2 $ \nto extract the new principal subspace not included in the existing subspace neurons.\n\n[1] TRGP: Trust region gradient projection for continual learning. ICLR, 2022.\n\n[2] Gradient projection memory for continual learning. ICLR, 2021.\n\n[3] Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 1982.\n\n[4] Global analysis of oja\u2019s flow for neural networks. IEEE Transactions on Neural Networks, 1994."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362025805,
                "cdate": 1700362025805,
                "tmdate": 1700362025805,
                "mdate": 1700362025805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eQaHaUMLGM",
            "forum": "MeB86edZ1P",
            "replyto": "MeB86edZ1P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a task-incremental continual learning (CL) method for spiking neural networks.\nIt can be categorized as a CL approach based on orthogonal gradient projection.\nIn orthogonal gradient projection approaches, the update $\\Delta \\mathbf W^P$ of each layer is projected to a subspace to minimize changes to the outputs for previous tasks.\nThis is achieved by performing PCA on the input data for each layer and projecting the gradient to the subspace orthogonal to the principal subspace of the input data.\n\nThe main technical novelty of this work is the application of a Hebbian learning rule to perform PCA.\nThis rule is claimed to be more suitable for neuromorphic hardware."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n- The combination of spiking neural networks and continual learning seems interesting, although this is not the first work to address such problems.\n- Code is provided in the supplementary material."
                },
                "weaknesses": {
                    "value": "### Not Comparing Other Approximations of PCA\n\nThe essence of the proposed Hebbian approach is to perform PCA.\nWhile there is a huge literature on more efficient approximate PCA with various forms of tradeoffs, the Hebbian approach is just one of such variants of PCA.\n\nI think the authors need to justify why their Hebbian approach is particularly suitable for spiking neural networks.\nThey vaguely argue that the Hebbian rule only requires \"neuronal operations,\" but the neuroscience-backed algorithm eventually boils down to some matrix-vector arithmetic, just like many other approximate PCA algorithms.\nCurrently, I do not see any reason that the Hebbian rule should be more suitable for spiking neural networks while others are not.\n\n### Task-Incremental Settings\n\nThis paper exclusively focuses on the task-incremental settings.\nTask-incremental CL is often considered the most naive and easiest form of CL.\nEspecially, providing task IDs even at test time is far from realistic and significantly reduces its practical utility.\nI believe that relying solely on task-incremental experiments is insufficient to establish meaningful results."
                },
                "questions": {
                    "value": "- Is the Hebbian rule a better fit for spiking neural networks compared to other approximate PCA approaches?\n- If it is, what makes the Hebbian rule more suitable, and why aren't the other approaches as effective?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646662886,
            "cdate": 1698646662886,
            "tmdate": 1700611792061,
            "mdate": 1700611792061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3Xxv4jc4uD",
                "forum": "MeB86edZ1P",
                "replyto": "eQaHaUMLGM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer braR"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We respond to your comments and questions as follows.\n\n1. About Hebbian learning and other approximations of PCA. \n\nIt is true that there are many PCA approximation algorithms. However, a basic focus of our work is neuromorphic computing, and the biologically inspired Hebbian learning well fits neuronal computation.\n\nWe first introduce more background on neuromorphic computing. Neuromorphic computing [1] aims at the computation inspired by the structure and function of human brains. At the hardware level, neuromorphic chips are designed to imitate biological neurons, such as their spiking property and synaptic connections with local storage of weights for in-memory computation, for highly energy-efficient and parallel event-driven computation with avoidance of frequent memory transpose (the computation architecture is different from the commonly used hardware with von Neumann architecture such as CPU or GPU). At the algorithm level, we are interested in developing methods compatible with properties and operations of neurons so that they are possible for deployment on hardware. As neuromorphic hardware is under development considering software-hardware co-design, most algorithms are simulated on common computational devices while considering neuromorphic properties.\n\nFor PCA algorithms, most of them require a lot of operations not corresponding to neuronal activities and connections, and the learning/update of the matrix does not follow the (biological) learning rule that may be designed for neuromorphic computing considering the locality property. \n\nOn the other hand, biologically inspired Hebbian learning well fits neural computation, and can also achieve sota convergence rate in streaming PCA [2]. From the perspective of general computation, neuroscience-backed algorithms will indeed boil down to matrix-vector arithmetic as other methods. But considering neuromorphic computing, Hebbian learning can be mapped to the computation of neurons and synapses and implemented on neuromorphic hardware, while other methods do not. So Hebbian learning is more suitable for SNNs.\n\nAdditionally, Hebbian learning enables unbiased streaming PCA from large data for better subspace construction than PCA methods adopted by previous works. This also has improvement and leads to better performance.\n\n2. About the task-incremental setting. \n\nWe would like to clarify that we consider both task-incremental and domain-incremental settings, as what previous gradient projection methods do. Continual learning is classified into three fundamental types: task-incremental, domain-incremental, and class-incremental [3]. Task-CL requires task id at training and testing, domain-CL shares the whole network and does not require task id, while class-CL requires comparing new tasks and old tasks and inferring context without task id. Many previous works call both task- and domain-incremental as task-incremental, and here we distinguish them as in [3] to emphasize that our method does not necessarily need task id. Following experimental setups of previous gradient projection methods [4,5], we consider both task- (CIFAR-100, miniImageNet, 5-Dataets) and domain-CL (PMNIST) settings. As for class-CL, as discussed in Related Work, it inevitably requires some kind of replay of old experiences for good performance since it expects explicit comparison between new classes and old ones [6,4]. So we also do not focus on it and it can be future work to study if HLOP can be combined with some replay methods or context-dependent processing modules similar to biological systems [7] (e.g., with a task classifier) for better class-incremental tasks.\n\nOur PMNIST experiment in Table 2 is domain-CL that does not require task id, since the classifier is shared. Here we further supplement the domain-incremental setting results on the larger 5-Datasets with a larger network (reduced ResNet-18) to show the scalability. The comparison results are below (where HAT requires task id and is not feasible) and the results also show the superior performance of HLOP.\n\n| Method | Domain-CL 5-Datasets (ACC, BWT) |\n| :----: | :----: |\n| *Multitask* | *82.81*, / |\n| Baseline | 32.46, -74.86 |\n| MR | 70.26, -24.12 |\n| EWC | 35.80, -65.18 |\n| HAT | N.A., N.A. |\n| GPM | 45.62, -56.28 |\n| **HLOP (ours)** | **79.59**, **-9.94** |\n\n[1] Towards spike-based machine intelligence with neuromorphic computing. Nature, 2019.\n\n[2] ODE-inspired analysis for the biological version of Oja\u2019s rule in solving streaming PCA. COLT, 2020.\n\n[3] Three types of incremental learning. Nature Machine Intelligence, 2022.\n\n[4] Gradient projection memory for continual learning. ICLR, 2021.\n\n[5] TRGP: Trust region gradient projection for continual learning. ICLR, 2022.\n\n[6] Brain-inspired replay for continual learning with artificial neural networks. Nature Communications, 2020.\n\n[7] Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361271668,
                "cdate": 1700361271668,
                "tmdate": 1700361271668,
                "mdate": 1700361271668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tva5GMHYLT",
                "forum": "MeB86edZ1P",
                "replyto": "3Xxv4jc4uD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. Unfortunately, however, my primary concern about justifying Hebbian learning is unresolved.\nIn the review, I asked the authors to justify the claim that \"the Hebbian approach is particularly suitable for spiking neural networks.\"\nIn the authors' response, the only relevant parts are:\n> biologically inspired Hebbian learning well fits neural computation\n\n> Hebbian learning can be mapped to the computation of neurons and synapses and implemented on neuromorphic hardware, while other methods do not\n\nThese are just repetitions of the claim, while I requested evidence for it.\nI'm lowering the soundness and contribution scores accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700438599979,
                "cdate": 1700438599979,
                "tmdate": 1700438599979,
                "mdate": 1700438599979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fvXxJVWLhr",
                "forum": "MeB86edZ1P",
                "replyto": "8ajUbMSc30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for more detailed comments.\nI wanted a more rigorous and scientific argument than \"both Hebbian learning and SNNs are inspired by biological neurons,\" which is just an analogy.\n\nThe last response provides an argument that is much closer to what I anticipated:\n> Intel\u2019s Loihi [1] is one of the most famous neuromorphic chips for SNNs with on-chip learning abilities, and it only supports Hebbian-type learning rules or with a third reward modulator.\n\nAnd there is a more precise description I found in [1]:\n>SNN synaptic weight adaptation rules must satisfy a **locality constraint**: each weight can only be accessed and modified by the destination neuron, and the rule can only use locally available information, such as the spike trains from the presynaptic (source) and postsynaptic (destination) neurons.\n\n\nAfter reading this, however, I am more puzzled.\nIf neuromorphic chips require a learning algorithm that fulfills the locality constraint, it cannot handle regular back-propagation, which is part of the proposed method.\nI looked into the PyTorch code in the supplementary material, and there surely is the standard backpropagation code, which would not be runnable on neuromorphic chips.\n```\nloss.backward()\noptimizer.step()\n```\n\nGiven that the proposed method has not been tested on actual neuromorphic hardware, I am skeptical about its compatibility with neuromorphic chips.\nTo counter my argument, demonstrating the proposed approach on real neuromorphic hardware would be the most effective.\nI'm open to correction if there's any misunderstanding on my part."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562705550,
                "cdate": 1700562705550,
                "tmdate": 1700562705550,
                "mdate": 1700562705550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ePsH6l7CWK",
                "forum": "MeB86edZ1P",
                "replyto": "49eMzNjIZH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Reviewer_braR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\nIt addressed most of my concerns, and as a result, I'm adjusting my score accordingly and recommending acceptance.\nPlease understand that I misunderstood several key concepts due to my limited expertise in SNNs and neuromorphic hardware.\nIt was a pleasure to engage in an insightful discussion."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611760527,
                "cdate": 1700611760527,
                "tmdate": 1700611760527,
                "mdate": 1700611760527,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T5H5HwOROl",
            "forum": "MeB86edZ1P",
            "replyto": "MeB86edZ1P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission828/Reviewer_oRoY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission828/Reviewer_oRoY"
            ],
            "content": {
                "summary": {
                    "value": "Unlike biological intelligence, current deep learning suffers from catastrophic forgetting\u2014 upon learning new tasks, networks often lose the ability to solve previously learned tasks. \nOne set of methods to solve catastrophic forgetting is orthogonal gradient projection, in which the learning gradient for new tasks are projected to a subspace that is approximately orthogonal to the subspace of the gradient of the network w.r.t. the weights for old tasks.\nHowever, they are not applicable to Spiking Neural Networks (SNNs), which is the predominant architecture for neuromorphic learning.\nThis paper proposes Hebbian Learning based Orthogonal Projection, or HLOP, an orthogonal gradient projection method that extracts principal subspaces of neuronal activities using Hebbian learning.\nHLOP is compatible with SNNs, and outperforms other continuous learning methods on several computer vision datasets.\nThus, it may be a promising new direction for continual neuromorphic learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The authors introduce a novel combination of Hebbian learning with orthogonal projection to solve catastrophic forgetting for SNNs. While using Hebbian learning to find principal subspaces is not a new technique, its application to this problem is both novel and elegant.\n- HLOP achieves strong empirical performance, surpassing all other continual learning methods that the authors benchmarked against."
                },
                "weaknesses": {
                    "value": "- The final paragraph of the intro that discusses your contributions is a bit dense. I suggest breaking it up, and allocating more intro space to discuss your contributions, as it\u2019s the most important part of your intro. In particular, I think you should have a separate paragraph to discuss the experimental setup and results, and include some performance numbers to quantify the strength of your method; currently its strength is hard to judge from the intro.\n\n- The background on SNNs provided in Section 3.1 is a word-for-word replica of Section 3.1 in Xiao et al. (2022).\nIt\u2019s fine to reuse definitions from previous work, but a direct replica like this should be explicitly attributed to avoid plagiarism concerns (even if it\u2019s your own work).\n\n- There is almost no discussion on the current limitations/challenges of HLOP and the experimental design, making it hard to judge the tradeoffs between using HLOP versus other approaches.\nI\u2019d like to see the authors also include a discussion on the potential downsides of HLOP and limitations of the evaluation process.\n\n- Minor issue, but the sentence in Section 5.2 explaining the weight transport problem of backprop is very long and hard to parse.\nI suggest either rewriting it and breaking it down to smaller chunks, or removing it entirely as it\u2019s not central to your work; just stating that FA and SS are more biologically-plausible and more amenable to neuromorphic hardware is enough."
                },
                "questions": {
                    "value": "- At the end of Section 3.2, you mention that previous methods \u201ccannot be implemented by neuronal operations for neuromorphic computing,\u201d but do not provide further explanation. \nTo me, this is important because it provides critical motivation for your method; it highlights why your novel approach using Hebbian learning is required.\nI understand that these methods cannot be directly implemented on SNNs out-of-the-box, but can you elaborate on why it is difficult or infeasible to adapt them for SNNs?\n\n- The performance of HLOP is close to the upper bound (specified by Multitask performance) for each dataset except miniImageNet, where there is a large gap. Can you provide reasoning or intuition for why this is the case?\n\n- Your results show that HLOP outperforms several other continual learning methods. Can I assume that the results achieved with HLOP are state-of-the-art on these datasets? Or are there other methods that you did not compare against?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission828/Reviewer_oRoY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780388005,
            "cdate": 1698780388005,
            "tmdate": 1699636010065,
            "mdate": 1699636010065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NSeMCnObSV",
                "forum": "MeB86edZ1P",
                "replyto": "T5H5HwOROl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oRoY (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our work and providing valuable comments. We respond to your comments and questions as follows.\n\n1. Writing issues. \n\nThank you for your kind advice. Following your suggestions, we reorganize/modify descriptions of the final paragraph of the intro as well as Section 3.1, and move the sentence in Section 5.2 to the footnote. Please check the updated version.\n\n2. Limitations/challenges of HLOP and evaluation. \n\nAs for our method, HLOP introduces additional computational costs for learning the lateral weights of each layer during training, which will increase training costs in our current implementation codes. While this process can theoretically be parallel to the normal forward-backward propagation of network training as discussed in the paper, such parallelization may not be easily realized in established deep learning libraries like PyTorch. So for our implementation of GPU training, the training time would be slightly longer. It can be future work on low-level code optimization or consideration of parallel neuromorphic computing hardware. Additionally, HLOP currently requires a manual specification for the number of subspace neurons, which may be improved for automatic and adaptive allocation. For example, the Generalized Hebbian Algorithm can perform Gram-Schmidt orthonormalization on the rows of weight matrices, which may help to sweep out unnecessary neurons and connections, but it requires some non-local information and may not be directly suitable to our method. It can be interesting for future work to study improvements. \n\nAs for the evaluation, similar to other gradient projection methods, this work mainly focuses on task-incremental and domain-incremental continual learning settings. There is another class-incremental setting which, as discussed in Related Work, inevitably requires some kind of replay of old experiences for good performance since it expects explicit comparison between new classes and old ones [1,2]. So following previous works, our evaluation mainly focuses on task- and domain-incremental settings. It can be future work to study if HLOP can be combined with some replay methods or context-dependent processing modules similar to biological systems [3] (e.g., with a task classifier) for better class-incremental tasks.\n\nWe have added the above discussions in Appendix F in the revised version.\n\n3. Why previous methods cannot be implemented by neuronal operations for neuromorphic computing. \n\nWe would like to first introduce more background on neuromorphic computing. Neuromorphic computing aims at the computation inspired by the structure and function of the human brain. At the hardware level, neuromorphic chips are designed to imitate biological neurons, such as their spiking property and synaptic connections with local storage of weights for in-memory computation, for highly energy-efficient and parallel event-driven computation with avoidance of frequent memory transpose (the computation architecture is different from the commonly used hardware with von Neumann architecture such as CPU or GPU). At the algorithm level, we are interested in developing methods compatible with some properties and operations of neurons so that they are possible for deployment on the hardware. Also, note that neuromorphic hardware is under development considering software-hardware co-design, so most algorithms are simulated on common computational devices while considering neuromorphic properties.\n\nWhen it comes to previous methods mentioned in Section 3.2, they do not consider neuromorphic properties and require complex computation beyond neuronal operations. First, for calculation of the projection matrix, they require operations such as SVD on a batch of data or calculating inverses of matrices, which needs a lot of (general) operations not corresponding to neuronal activities and connections, and the learning/update of the projection matrix does not follow the (biological) learning rule that may be designed for neuromorphic computing considering the locality property. Instead, we propose to leverage biologically inspired Hebbian learning which can also be directly integrated into the lateral circuit. Second, they do not consider how projection can be implemented. They require matrix multiplication of a projection matrix with weight gradients, which cannot correspond to operations in neurons and synaptic connections. While neurons can perform matrix multiplication considering neuronal activations and local connection weights, the above calculation cannot be directly mapped to such computation and it is unclear how the projection matrix can be locally stored. Instead, we propose a lateral neural circuit to modify the presynaptic activity traces for projection. The previous methods can be implemented on common devices as in our experiments, but considering neuromorphic computing, they are difficult to realize and we seek methods that have more similarity to biological neurons."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360344995,
                "cdate": 1700360344995,
                "tmdate": 1700360344995,
                "mdate": 1700360344995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]