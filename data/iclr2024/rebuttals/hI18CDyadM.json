[
    {
        "title": "Adaptive Window Pruning for Efficient Local Motion Deblurring"
    },
    {
        "review": {
            "id": "53RSL4PPu1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_S2zK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_S2zK"
            ],
            "forum": "hI18CDyadM",
            "replyto": "hI18CDyadM",
            "content": {
                "summary": {
                    "value": "This paper proposes a local motion deblurring Transformer with adaptive window pruning, which only deblur the active (blurry) windows. The windows are classified as active/inactive according to the predicted bluriness confidence score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I think this paper has a novel idea and achieves good results in terms of performance and efficiency. I tend to accept this paper due to following reasons.\n\n1. adaptive window pruning that saves computation on unnecessary attention windows.\n\n2. bluriness confidence prediction that works well for both local motion prediction and global motion prediction\n\n3. annotated local blur masks on ReLoBlur\n\n4. well-designed experiments, well-presented figures and well-written paper."
                },
                "weaknesses": {
                    "value": "Below are some concerns and suggestions.\n\n1. Since the confidence predictor only uses MLP layers. How many pixels did you shift? Is the feature shift necessary to enlarge the receptive field of the neighbourhood? \n\n2. What is the mask prediction accuracy on validation set?\n\n3. How did you decide the border when annotating the masks for blurry moving objects?\n\n4. If a patch is always abandoned, how is it processed? What layers it will be passed into during inference?\n\n5. It could be better to provide the results of two special cases (masks are all-ones/all-zeros) in the tables as a reference.\n\n6. Why did you only report real-world results on large images? Do you have a chart for comparison on PSNR/FLOPs/runtime under different image resolutions.\n\n7. The key of this method is the adaptive window pruning. It is better to provide an ablation study with the rest tricks as a baseline (i.e., no window pruning in training and testing).\n\n\nMinor:\n\n1. Unfinished paragraph in page 2.\n\n2. For figure 1, I think it might be better to visualise the attention window borders, for example, by adding solid lines to show the windows.\n\n3. The summarised contributions are a bit overlapped (point 1 and point 2). I think it's better to claim adaptive window pruning and bluriness confidence prediction as two contributions.\n\n4. I think there is no need to distinguish between AdaWPT-F and AdaWPT-P. Just be simple and united. The name of AdaWPT is enough.\n\n5. Comparison in Figure 6 is not visible.\n\n6. Is there an example on a globally clear image (e.g., a still scene). Contrary to Figure 7, will the decision map be all zeros?\n\n7. Are there other similar works in image restoration/deblurring? Are there some connections between this method and some \"blur kernel prediction + restoration\" methods (e.g. Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution)?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697371750705,
            "cdate": 1697371750705,
            "tmdate": 1699636469643,
            "mdate": 1699636469643,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qpghAF2fi0",
                "forum": "hI18CDyadM",
                "replyto": "53RSL4PPu1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to questions 1-6"
                    },
                    "comment": {
                        "value": "**The number of shifting pixels and the necessity.** In each AdaWPT block, we shift 4 tokens, which is half of the window size. The number of shifting pixels is proportional to the token size of each block. For example, for AdaWPT 1 and 9, whose resolution is $512\\times512$, the number of shifting pixels is $4\\times4$. Similarly, for AdaWPT 2 and 8, the number of shifting pixels is $8\\times8$... For AdaWPT 5, the number of shifting pixels is $64\\times64$. As the block's resolution decreases, the number of shifting pixels increases, effectively expanding the receptive field. This shifting operation aids in enlarging the receptive field of the local neighborhood, thereby enhancing the deblurring performance. Noticeably, we have trained our proposed network without the Feature Shift/Reverse layer, and the absence of these layers results in a PSNR drop of approximately 0.1dB.\n\n**The prediction accuracy on the validation set.** We calculate the prediction accuracy by $(TP + TN) / N$, where $TP$ refers to the number of pixels that our model correctly predicts to be blurry, $TN$ refers to the number of pixels our model correctly predicts to be sharp, and $N$ is the total number of pixels. The mask prediction accuracies vary in different AdaWPT blocks. The highest is 94.51\\%. Figure 12 in the new version of our Appendix reports the accuracy of each AdaWPT block. Most AdaWPT blocks exhibit high prediction accuracies, indicating the effective recognition of local blurs by our proposed model. We have incorporated this discussion into the new version of Appendix E.6.\n\n**About the blur mask borders.** We subtract the sharp and blurred images to observe the differences in local regions. This difference map helps us to determine the approximate position of the blurriness and the border. To prevent leakage, we extend the blurry region by 5 pixels, ensuring that all pixels within the annotated mask are considered blurry. Regarding pixels between blurry and sharp areas, we adhere to a principle: a diffuse spot encompassing more than 5 pixels is categorized as a blurry region, while conversely, it is classified as a sharp region. We add this discussion in the new version of Appendix C.\n\n**How abandoned patches are processed.** Because the abandoned patches are sharp, they do not go through Transformer layers. They directly go to the Window Compound layer during inference. The Window Compound layer is incorporated \\textcolor{red}{to integrate the abandoned windows and selected windows into a unified feature map in inference. This is explained in the new version of Section 2.2, highlighted in red.\n\n**Results of special cases (masks are all-ones/all-zeros).** I guess you refer to the results of inputting globally blurred images and sharp images. When inputting globally blurred images, the masks are all one, and the decision maps are nearly in all-white color, as is shown in Appendix D and Figure 7. When inputting globally blurred images, the masks are all zero, and the decision maps are nearly in all-black color. The results of these extreme experiments demonstrate the model\u2019s ability to effectively distinguish between blurred and sharp\nimages, showcasing its robustness. We add the results and discussion in the new version of Appendix E.7, highlighted in red.\n\n**Results under different image resolutions.** In the initial version of our paper, we presented results based on large images due to the ReLoBlur testing data exclusively comprising images with substantial resolutions. In this revision, we augment our findings by including results for various resolutions in the subsequent chart. Employing the nearest interpolation, we down-sample the ReLoBlur testing data and conduct comparisons between our proposed model and baselines using both middle-resolution and small-resolution images. The results depicted in the following chart demonstrate that our proposed LMD-ViT outperforms other methods across all resolutions, delivering rapid and efficient inference. This discussion has been incorporated into the updated version of Appendix E.4.\n\n| Image Resolution | Methods  | $\\uparrow$PSNR | $\\uparrow$SSIM  | $\\uparrow$PSNR$_w$ | $\\uparrow$SSIM$_w$ | Inference time | FLOPs |\n|----------|----------|----------|----------|----------|----------|----------|----------|\n| ReLoBlur* ($1076\\times718$) | Restormer [1] | 34.28 | 0.9130  | 29.01 | 0.8510 | 0.99s | 1.723T |\n| ReLoBlur* ($1076\\times718$)   | Uformer [2] | 32.88 | 0.8747  | 28.38 | 0.8293 | 0.25s | 1.158T|\n| ReLoBlur* ($1076\\times718$)   | LMD-ViT (ours) | 34.59 |  0.9176 | 29.39 | 0.8586 | 0.23s | 266.074G|\n| ReLoBlur* ($538\\times359$)     | Restormer [1] | 33.10 | 0.8903  | 28.53 | 0.8376 | 0.26s | 450.210G |\n| ReLoBlur* ($538\\times359$)   | Uformer [2] | 32.88 | 0.8747  | 28.38 | 0.8293 | 0.25s | 321.657G|\n| ReLoBlur* ($538\\times359$)   | LMD-ViT (ours) | 33.52  |  0.8978 | 28.85  | 0.8451  | 0.10s |162.627G|\n\n[1] Zamir et al, Restormer, CVPR 2022\n\n[2] Wang et al, Uformer, CVPR 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411874104,
                "cdate": 1700411874104,
                "tmdate": 1700411874104,
                "mdate": 1700411874104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r3rtBP02tp",
            "forum": "hI18CDyadM",
            "replyto": "hI18CDyadM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
            ],
            "content": {
                "summary": {
                    "value": "The presented work delves into an interesting research problem: local motion deblurring. It introduces a novel approach known as LMD-ViT, constructed using \"Adaptive Window Pruning Transformer blocks (AdaWPT).\" AdaWPT selectively prunes unnecessary windows, focusing powerful yet computationally intensive Transformer operations solely on essential windows. This strategy not only achieves effective deblurring but also preserves the sharp regions, preventing distortion. Moreover, their method significantly accelerates inference speed. Additionally, they have provided annotated blur masks for the ReLoBlur dataset. The utility of this approach is showcased on both local and global datasets, where it demonstrates substantial performance improvements on local motion deblurring (the ReLoBlur dataset) and competes favorably with baseline methods in the realm of global deblurring."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis paper addressed problems of single image local motion deblurring, which is very essential in today\u2019s photography industry. The presented method is the first to apply sparse ViT in single image deblurring and may inspire the community to enhance image quality locally.\n2.\tThe proposed pruning strategy including the supervised confidence predictor, the differential decision layer and pruning losses are reasonable and practical. It combines window pruning strategy with Transformer layers, only allowing blurred regions to go through deblurring operations,  resulting in not only proficient deblurring but also the preservation of sharp image regions.\n3.\tThe quantitative and perceptual deblurring performances are obvious compared to baseline methods.\n4.\tThe presented method derives a balance between local motion deblurring performance and inference speed, as shown in the ablation study and experiments. The proposed method reduced FLOPs and the inference time largely without deblurring performances dropping on local deblurring data.\n5.\tThe authors provided annotated blur masks for the ReLoBlur dataset, enhancing the resources available to the research community."
                },
                "weaknesses": {
                    "value": "1.\tThe authors did not mention whether the presented method LMD-ViT requires blur mask annotation during inference. This is crucial because if the method does require blur masks before inference, it would be helpful to provide instructions on how to generate them beforehand and assess their practicality.\n2.\tThe proposed method uses Gumble-Softmax as the decision layer in training and Softmax in inference. The equivalence of the two techniques in training and inference is not discussed.\n3.\tIn the user study experiment, the absence of an explanation regarding the camera equipment used is notable. This is important because when images from the same camera share a common source, the blurriness often exhibits a consistent pattern. Therefore, including images from the same camera would allow us to assess the proposed method's robustness.\n4.\tSome references are missing, like \u201cWindow-based multi-head self-attention\u201d in page 2, and \u201cLeFF\u201d in Section 2.4.1."
                },
                "questions": {
                    "value": "please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726082242,
            "cdate": 1698726082242,
            "tmdate": 1699636469560,
            "mdate": 1699636469560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0u6TIcAXa2",
                "forum": "hI18CDyadM",
                "replyto": "r3rtBP02tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Whether LMD-ViT requires blur masks in inference.** In the inference phase, the input consists solely of locally blurred images without accompanying blur masks. During training, blur mask annotations are utilized to guide the prediction of blurriness confidence. However, in the inference phase, our network is capable of autonomously predicting the locations of blurred regions without requiring additional blur mask annotations. We add this explanation in the second paragraph of Section 3.1, highlighted in red.\n\n**The equivalence of training and testing.** The difference between the training and inferring process lies in the decision layer. In inference, we use Softmax with a threshold to sparse the tokens. However, this technique is not suitable for training because setting a threshold to all images during training increases the instability of parallel training. Therefore, we use Gumbel-Softmax in training to overcome the non-differentiable problem of sampling from a distribution. These two methods have the same effect. Softmax produces a probability distribution over classes, while Gumbel-Softmax introduces stochasticity into the process of selecting discrete values in a differentiable way. Both of them sparse the tokens and distinguish between sharp and blurry tokens.\n\n**The details of acquiring real-world locally blurred images in the user study.** In the user study experiment, we acquired locally blurred images using a static Sony industrial camera and a static Fuji XT20 SLR camera. The camera sensors and shooting environments in this experiment differ from those in the ReLoBlur dataset and the GoPro dataset. Consequently, the blur patterns observed in the user study may exhibit variations compared to the training data. The visual results presented in Figure 4 demonstrate the robustness of our proposed model in effectively addressing real-world instances of local motion blur. We add the details in the third paragraph of Section 3.2, highlighted in red.\n\n**About the references.** Thank you for your advice. We have added the references of `\"Window-based multi-head self-attention\" on page 2, and \"LeFF\u201d in Section 2.4.1. The modifications are highlighted in red in the new version of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324794305,
                "cdate": 1700324794305,
                "tmdate": 1700324794305,
                "mdate": 1700324794305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K53cromjSt",
                "forum": "hI18CDyadM",
                "replyto": "r3rtBP02tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer SKAv:\n\nThank you for taking the time to provide valuable feedback and constructive comments. Your advice has greatly helped us enhance the quality of our paper and proposed method. As we are nearing the end of the discussion period, we would like to inquire if our rebuttal has successfully addressed your concerns. If you have any additional questions, please feel free to respond to our responses. We are willing to address any remaining concerns you may have. \n\nBest Wishes,\n\nAuthors of paper \"Adaptive Window Pruning for Efficient Local Motion Deblurring\""
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587267697,
                "cdate": 1700587267697,
                "tmdate": 1700587267697,
                "mdate": 1700587267697,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aAoyznWwwz",
            "forum": "hI18CDyadM",
            "replyto": "hI18CDyadM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to tackle local motion deblurring. Existing deblurring literature mostly focuses on general deblurring (without specifying global or local regions). This could be a disadvantage in computation and generalization in scenarios where only a small part of a high-resolution image is blurred. Therefore, this work proposes a transformer-based network called LMD-ViT. The authors make use of adaptive window pruning and blurriness confidence predictor in the transformer blocks to ensure the network focuses on local regions with blurs. Quantitative and qualitative results are presented on the ReLoBlur and GoPro datasets. The effectiveness of different design choices is analyzed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "An adaptive window pruning strategy is adopted to focus the network computation on localized regions affected by blur and speed up the Transformer layers.\n\nA carefully annotated local blur mask is proposed for the ReLoBlur dataset to improve the performance of local deblurring methods."
                },
                "weaknesses": {
                    "value": "The organization of the paper can be improved.\n\n1) The methodology (Sec. 2) consists of too many (unnecessary) acronyms. Moreover, there are some inconsistencies when citing previous works (for example, LBAG (Li et al., 2023), LBFMG (Li et al., 2023), etc.). It would be better for the submission would strongly benefit from polishing the writing.\n\nThe settings of the experiments need more explanation.\n\n2) It is not clear why the GoPro dataset is used for training along with the ReLoBlur training set. In previous works, such as LBAG, only the ReLoBlur dataset is used (see Table 4).\n\nThe novelty of the submission needs to be clarified.\n\n3) It would be better to discuss the differences between LBAG and Uformer. Compared to LBAG, it simply substitutes the CNN architecture with a Transformer. All the other modules, including sparse ViT, W-MSA (Window-based multi-head self-attention), and LeFF (locally- enhanced feed-forward layer), have been introduced in previous deblurring works.\n\nThe fairness of the experiments.\n\n4) The transformer baselines, such as Restormer and Uformer-B, are not trained with the local blur masks, which are deployed during their training by the proposed methods. This makes the comparison in Table 1 unfair.\n\nUnclear parts.\n\n5) Please explain in detail how the authors manually annotate the blur masks for the ReLoBlur dataset.\n\n6) The baseline results reported in Table 1 are higher than those in their original papers, e.g., LBAG. It would be better to give the reasons and more details when introducing Table 1. The baseline results reported in Table 1 are higher than those in their original papers, e.g., LBAG for 34.85 dB.\n\nTypo: \nTable table 4 in Appendix C.\nDecision map in Figure 7."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760530276,
            "cdate": 1698760530276,
            "tmdate": 1699636469403,
            "mdate": 1699636469403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nFFmaGVA4t",
                "forum": "hI18CDyadM",
                "replyto": "aAoyznWwwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**About the organization and writing.** Thank you for your advice. To improve the writing of our paper, we have made several modifications: \n\n1) We have removed the second paragraph from Section 1 and incorporated discussions about our novelty in the same section.\n\n2) We have consolidated the explanations for AdaWPT-F and AdaWPT-P under a unified term, AdaWPT, in Section 2. Accordingly, we've updated Figure 2 to reflect this change without differentiating between AdaWPT-F and AdaWPT-P.\n\n3) We add more discussions in the new version of our Appendix. Specifically, the blur mask annotation details in Appendix C, the effectiveness of the joint training technique in Appendix E.3, deblurring results under different resolutions in Appendix E.4, the effectiveness of our backbone in Appendix E.5, mask prediction accuracy in Appendix E.6, results of special cases in Appendix E.7, as suggested by the reviewers. \n\n4) To maintain the consistency of citing LBAG [1], we replace \"LBFMG\" [1] with \"LBAG\" [1] in the new version of our paper. (Notably, LBFMG [1] is a blur mask generation method in paper LBAG. In the original version of our paper, we wrote LBFMG [1] when discussing blur mask annotations.) \n\nAll the modifications are highlighted in red in the new version of our paper. \n\n**Why we use the GoPro [2] and ReLoBlur dataset [1] for joint training.**\nWe train with the GoPro dataset [2] and ReLoBlur dataset [1] together mainly because of two reasons. Firstly, joint training could prevent our model from over-fitting and improve the model's robustness. Secondly, we expect our proposed LMD-ViT to deblur both globally and locally. Training with the GoPro [2] and ReLoBlur [1] datasets together improves both the local and global deblurring performances. We compare models trained solely on the ReLoBlur dataset [1] and jointly trained with the two datasets in the Table below. The results show improvements (+0.13dB in local deblurring and +0.42dB in global deblurring) when we add the GoPro dataset [2] to train. We've added this discussion to the new version of Appendix E.3.\n\n| Training data | Training data          | Testing data | Testing data |\n| ------- | ------- |------- | ------- |\n| ReLoBlur      | ReLoBlur & GoPro | ReLoBlur      | GoPro |\n|          \u2713         |                              |   35.29 dB / 0.9280 |   31.74 dB / 0.9271  |\n|                      |              \u2713             | 35.42 dB / 0.9285   |   32.16 dB / 0.9318  |\n\n[1] Li et al, Real-World Deep Local Motion Deblurring, AAAI 2023\n\n[2] Nah et al, Deep multi-scale convolutional neural network for dynamic scene deblurring, CVPR 2017"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323824395,
                "cdate": 1700323824395,
                "tmdate": 1700323824395,
                "mdate": 1700323824395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5It7hvnRCI",
                "forum": "hI18CDyadM",
                "replyto": "aAoyznWwwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ePoq:\n\nThank you for taking the time to provide valuable feedback and constructive comments. As we are nearing the end of the discussion period, we would like to inquire if our rebuttal has successfully addressed your concerns.\n\nIf you have any additional questions, please feel free to respond to our responses. We are more than willing to address any remaining concerns you may have. Once again, thank you for all the helpful comments provided.\n\nBest Wishes,\n\nAuthors of paper \"Adaptive Window Pruning for Efficient Local Motion Deblurring\""
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587174492,
                "cdate": 1700587174492,
                "tmdate": 1700587174492,
                "mdate": 1700587174492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3dDGrlSHzA",
                "forum": "hI18CDyadM",
                "replyto": "5It7hvnRCI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. \n\nI would like to clarify whether Restormer (Zamir et al., 2022) and Uformer-B (Wang et al., 2022) in Table 1 and Table 9 were trained on the same dataset as the proposed method.\n\nIt is difficult to determine whether the performance improvement of the proposed method is due to the extra training data (ReLoBlur & GoPro) or the extra label (mask) based on the current version.\n\nTable 4 is a relatively fair comparison. All methods were trained with the same training set and tested on the GoPro dataset, where the proposed method achieved the second-best performance. This also raises questions about the benefits of the proposed framework that using masks. In other words, while the proposed framework can benefit local blur, it restricts the performance on global blur.\n\nIn summary, I still have concerns about the fairness of the experiment and the performance of the proposed method.\n\nBest regards"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726262243,
                "cdate": 1700726262243,
                "tmdate": 1700726262243,
                "mdate": 1700726262243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cXof5Oloar",
                "forum": "hI18CDyadM",
                "replyto": "aAoyznWwwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4860/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ePoq,\n\nThank you for your inquiry. \n\nFirstly, all the baselines, including Restormer [3] and Uformer-B [4] in Table 1 and Table 9 were trained on the same dataset as our proposed method. \n\nFor your second concern **whether the performance improvement of the proposed method is due to the extra training data or the extra label**, we think the performances are improved for both of the two reasons. We've done experiments before with controlled variables. Firstly, we train our method with the GoPro dataset [2] and the ReLoBlur dataset [1] together with our newly annotated masks, like the results presented in response to your second question. In our previous response regarding \"Why we use the GoPro [2] and ReLoBlur dataset [1] for joint training\", experiments No.1 and No.2 share the same training settings, differing only in the inclusion of the GoPro dataset [2]. Therefore, the results in our previous response substantiate that joint training can enhance deblurring performance. Secondly, the verify the influences of blur masks, we have included the results of training with masks annotated by the LBFMG [1] method in the updated table below. The results show that our newly annotated blur masks also contribute to the enhancement. Moreover, the updated table could further strengthen the assertion that training with both the ReLoBlur dataset [1] and the GoPro dataset [2] is a superior choice compared to training solely with the ReLoBlur dataset [1]. We have incorporated these updated findings in Table 7 of the new version. \n\n| Training data | Training data| Blur mask | Blur mask | Testing data | Testing data |\n| ------- | ------- | ------- | ------- | ------- | ------- |\n| ReLoBlur    | ReLoBlur +GoPro   | LBFMG's   | Ours  | ReLoBlur              | GoPro|\n|\u2713\t\t     |                                |\u2713                 |           |35.24 dB / 0.9283 |31.60 dB / 0.9252 |\n|                    |\u2713\t                      |\u2713                 |           |35.29 dB / 0.9280 |31.74 dB / 0.9271\n| \u2713                |                                 |                   | \u2713        |35.31 dB / 0.9270 |31.14 dB / 0.9217 |\n|                    |   \u2713                           |                   | \u2713        |35.42 dB / 0.9285 |32.16 dB / 0.9318 |\n\nFor your third concern of **fairness** and **global deblurring performances**, we ensure that all experiments are conducted fairly and traceably. We provided the results of training with baselines and extra data (blur mask) in the previous response and illustrated that with the same information, our model achieves the best local deblurring performance. \n\nOur module design is focused on local deblurring, that is, how to adaptively remove some areas that do not participate in the calculation while ensuring that there is a local performance improvement. From Table 1 in our main paper, we can see that our proposed method LMD-ViT has achieved great advantages while reducing a lot of computation. Table 4 shows the comparable performance of global deblurring and makes perfect sense, but global deblurring is not the focus of our design module concerns.\n\nIf our response addresses your concerns adequately, could you please update the rankings of our paper? Thank you for your time and interest in our work!\n\nBest wishes,\n\nAuthors of paper \"Adaptive Window Pruning for Efficient Local Motion Deblurring\"\n\n[1] Li et al, Real-World Deep Local Motion Deblurring, AAAI 2023\n\n[2] Nah et al, Deep multi-scale convolutional neural network for dynamic scene deblurring, CVPR 2017\n\n[3] Wang et al, Uformer: A general u-shaped transformer for image restoration, CVPR 2022\n\n[4] Zamir, Restormer: Efficient transformer for high-resolution image restoration, CVPR 2022"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4860/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738297518,
                "cdate": 1700738297518,
                "tmdate": 1700738339999,
                "mdate": 1700738339999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]