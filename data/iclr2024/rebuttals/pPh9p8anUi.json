[
    {
        "title": "PBADet: A One-Stage Anchor-Free Approach for Part-Body Association"
    },
    {
        "review": {
            "id": "9AZpbVRwzW",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_iAuQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_iAuQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose an association method for part-body detection. Their framework is based on YOLO, which is anchor-free.  In experiments, they show PBADet's good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is easy to read.  \n2. Association for body and parts is  worth studying.  \n3. The method is evaluated in several datasets."
                },
                "weaknesses": {
                    "value": "1.The main problem of this paper is that part-to-body offsets have been proposed in previous works [1]. Furthermore\uff0cin [1], they also try local center offsets to improve the performance. Hence, considering the not new idea and no other designs, the technical learning of this paper is limitted.  \n[1] Jin L, Wang XJ. Grouping by Center: Predicting Centripetal Offsets for the Bottom-up Human Pose Estimation. TMM, 2022.  \n2. \"Single-stage\" is controverial. This method is bottom-up but still need two-stage processing, i.e., detection and grouping. And single-stage method has been studied in several reaseraches [2]. There are  obvious differences between single-stage methods and this paper.  \n[2] Single-Stage Multi-Person Pose Machines, ICCV, 2019.  \n3. To verify the association efficiency, author should compare your association method with other grouping methods, such as associative embeddings and methods in [1], under the same part detection accuracy.    \n[3] Associative Embedding: End-to-End Learning for Joint Detection and Grouping.  \n4. The focus of this paper is association, but the evaluation metric is AP. More metrics on association quality should be discussed."
                },
                "questions": {
                    "value": "Refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Reviewer_iAuQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697520216876,
            "cdate": 1697520216876,
            "tmdate": 1700529232037,
            "mdate": 1700529232037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i0edHekjzF",
                "forum": "pPh9p8anUi",
                "replyto": "9AZpbVRwzW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer iAuQ,\n\nThank you for your detailed review and for raising critical points about our paper. Your feedback is essential for refining our work, and we have addressed each of your concerns below.\n\n1. Originality of Part-to-Body Offsets:\n\nA:  Thank you for highlighting the paper [1] that discusses a concept similar to our part-to-body center offset. However, it is crucial to note the distinct focus of our work on the part-body association task, as opposed to the multi-person pose estimation task emphasized in [1]. This distinction is elaborated in Section 2 \"Related Work\" of our paper.\n\nThe part-body association task, exemplified by models like BodyHands, Hier R-CNN, and BPJDet, involves detecting the bounding boxes of both parts and bodies and establishing their association relationships. In contrast, multi-person pose estimation primarily focuses on detecting individual keypoints of each person.\n\nTo illustrate the practical application of our approach, consider hand gesture recognition in a scan room setting. Here, it is essential for the system to respond exclusively to the technician's gestures while ignoring the patient\u2019s hands to prevent unintended interactions. This scenario requires not just the detection of hands (parts) but also the identification of their associated body, necessitating bounding boxes for both hand and body for effective gesture recognition and person classification.\n\nAs discussed in [4], while multi-person pose estimation can be adapted for the part-body association task, it requires additional models for detecting part and body bounding boxes. Using pose estimation for association, i.e., determining whether a keypoint lies within a part's bounding box, introduces a two-stage heuristic process that can add complexity and potentially reduce efficiency. Our method addresses this gap by providing a more streamlined, single-stage solution for the part-body association, enhancing both the efficiency and applicability of the approach in practical scenarios.\n\n[4] Who are raising their hands? Hand-raiser seeking based on object detection and pose estimation\n\n2. Definition of 'Single-Stage' Method:\n\nA: In addressing your point about the classification of our method as 'single-stage', particularly in the context of multi-person pose estimation, we acknowledge that there could be a perspective from which it might be seen as a two-stage method. However, our definition of 'stages' is centered around the process of association prediction. We follow the definition of stages in other hand-body association works such as BPJDet, which is considered to be single-stage.\n\nIn earlier works like [4] and BodyHands [5], the task of association prediction typically involves two distinct stages. The first stage is the detection of part and body bounding boxes, and the second stage encompasses either employing pose estimation with a heuristic strategy or using a learned association network to link the part and body. This two-stage process, though effective, introduces additional complexity and potential inefficiencies.\n\nContrasting this, both our method and BPJDet streamline the association task. These models are designed to simultaneously output the part and body bounding boxes along with their association vectors. This integration significantly simplifies the process, effectively making it a single-stage approach in terms of association prediction. By directly outputting both detection and association information, our model avoids the complexities inherent in traditional two-stage methods, thereby enhancing efficiency and reducing computational overhead. \n\n[5] Whose hands are these? Hand detection and hand-body association in the wild\n\n**Continue in the next comment**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939521824,
                "cdate": 1699939521824,
                "tmdate": 1699939953470,
                "mdate": 1699939953470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rSem0vqkj7",
                "forum": "pPh9p8anUi",
                "replyto": "i0edHekjzF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_iAuQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_iAuQ"
                ],
                "content": {
                    "title": {
                        "value": "About the originality"
                    },
                    "comment": {
                        "value": "In your answer, you only state the difference between the two tasks, i.e., pose estimation and part-body association. I do not deny the two tasks have differences and also share some commons. However, the main problem is the same idea of your work and the reference [1], which is using centripetal offset as a grouping clues. Furthermore, with the same idea, you have not proposed other modules or designs to give more lights for this problem.\n\nEven if your paper is accepted, I hope you can discuss [1] in related work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190875112,
                "cdate": 1700190875112,
                "tmdate": 1700190875112,
                "mdate": 1700190875112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uBHBaocaSG",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_vvPE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_vvPE"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a method for correlating part to body relationships in a single multi-person RGB image. The method is a single-stage approach that adopts the popular center-based detection architecture. To correlate a subject and multiple detected body parts, a common approach is to estimate center offsets from each body center to the underlying body part. But this may be ineffective since in case of occlusion/close interaction multiple body centers may point to the same body part. Therefore, this paper proposes to simultaneously estimate the offset of the body part position from the detected body part toward the center of the subject. This means that instead of estimating center-to-part offsets, estimating part-to-center offsets for each detected body part will avoid ambiguities in close interaction situations. The proposed method greatly improves the performance of hand association in BodyHands."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. BodyHands performance improvements are impressive with simple associative design changes. Changing from center-to-part to part-to-center offset greatly alleviates the ambiguity of hand associations in training.\n\n2. Interesting insights. The proposed method can achieve significant performance improvements in hand association on BodyHands. This demonstrates the interesting insight that the position of the hand is more ambiguous than the position of the center of the body. Because more occlusion/interaction occurs in the hand area."
                },
                "weaknesses": {
                    "value": "1. Writings. It is important to clearly present ideas and implementation. The current presentation is very vague and difficult to understand. It is beneficial to emphasize conceptual-level differences in the introduction. But before that, the paper should at least introduce the method/idea clearly. Additionally, in the Methods and related work section, consider clearly presenting differences compared to previous methods.\n\n2. In Figure 2, please highlight the parts where the proposed approach makes design changes. In Fig. 1, the direction of arrows are not very obvious. Besides, differences in qualitative results are difficult to distinguish. Please consider emphasizing it further."
                },
                "questions": {
                    "value": "1. Will the proposed method be open-sourced? It seems hard to re-implement the results with limited details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588825388,
            "cdate": 1698588825388,
            "tmdate": 1699636061924,
            "mdate": 1699636061924,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JEgQVYAHaH",
                "forum": "pPh9p8anUi",
                "replyto": "uBHBaocaSG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer vvPE:\n\nThank you for your thorough review and constructive feedback. We appreciate your emphasis on clarity in presentation and the importance of detailed documentation for reproducibility. Please find below our responses to your comments and queries.\n\n1. Improving Clarity in Writing:\n\nA: We acknowledge your concerns regarding the clarity of presentation in our paper, particularly in the Methods and Related Work sections. We understand the importance of clearly articulating the conceptual differences between our method and previous methods. We will undertake a thorough revision of these sections to enhance clarity and ensure that our method and its unique aspects are presented in a more comprehensible and detailed manner.\n\n2. Revising Figures for Enhanced Clarity:\n\nA: We appreciate your suggestions regarding Figure 2 and Figure 1. In the revised version of the paper, we will highlight the parts in Figure 2 where our approach introduces design changes. We will also address the clarity of the arrows in Figure 1 and enhance the differentiation in qualitative results to ensure that the unique aspects of our approach are more evident.\n\n3. Availability of Code and Re-Implementation Details:\n\nA: We have outlined the primary details under Section 4.1 \"EVALUATION PROTOCOLS\" in the main paper. Regarding the availability of our code, we are in the process of finalizing its release, pending approval. We are actively working towards this goal and plan to update the revised paper accordingly. Furthermore, we are committed to providing even more detailed implementation to ensure that other researchers and practitioners can reliably reproduce our results.\n\nWe are grateful for your feedback, as it plays a crucial role in refining our work. We are dedicated to addressing these points comprehensively in the revised version of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938401209,
                "cdate": 1699938401209,
                "tmdate": 1699938401209,
                "mdate": 1699938401209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RClQJgedvC",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_TS1Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_TS1Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents PBADet, a one-stage, anchor-free object detection model that efficiently identifies and associates object parts, such as human body parts, to their main body. Utilizing multi-scale feature maps and a unique part-to-body center offset, PBADet offers improved accuracy and robustness over existing models. It addresses inefficiencies through a task-aligned learning strategy and a simplified decoding process for part-to-body associations, making it a streamlined and effective solution in object detection."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The framework is adaptable for various part-to-body association challenges beyond human body parts.\n2. The model adopts an anchor-free paradigm, potentially improving detection performance for objects of varying sizes.\n3. Incorporation of a task-alignment learning strategy for bounding box, class predictions, and part-body association."
                },
                "weaknesses": {
                    "value": "1. The paper doesn't explicitly address how the model deals with occluded parts.\n2. The dense, per-pixel prediction approach might lead to increased computational demands.\n3. The performance might be highly sensitive to the tuning of loss function hyper-parameters.\n4. There's a lack of in-depth comparative analysis with traditional anchor-based models.\n5. The model's efficiency in handling images with multiple overlapping objects is unclear."
                },
                "questions": {
                    "value": "1. How does PBADet address the challenge of occluded parts in the detection process?\n2. How does the model perform in scenarios with multiple overlapping or closely situated objects?\n3. Have you tested the model\u2019s scalability and performance across various domains other than human part-to-body associations?\n4. Could you provide more comparative performance of PBADet against traditional anchor-based detection models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Reviewer_TS1Z"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624460509,
            "cdate": 1698624460509,
            "tmdate": 1700458431278,
            "mdate": 1700458431278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W0N8rlcgsT",
                "forum": "pPh9p8anUi",
                "replyto": "RClQJgedvC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer TS1Z:\n\nThank you for your detailed review and insightful queries regarding our paper. Your feedback is invaluable in improving our work. Below, we address each of your points to clarify our methodology and findings.\n\n1. Handling of Occluded Parts:\n\nA: Our method does not explicitly address occluded parts. For instance, in hand-body associations, if a hand is not visible (occluded), it is not associated with a body. This approach demonstrates an advantage over methods like BPJDet, which outputs a constant number of part center offsets regardless of visibility, making the association decoding more complex. Our part-to-body center offset definition ensures that no offset is attached to undetected (invisible) parts, enhancing the method's relevance and efficiency.\n\n2. Dense Per-Pixel Prediction and Computational Efficiency:\n\nA: The one-stage anchor-free object detection approach that we employ, with its dense per-pixel prediction, is established for real-time performance on many platforms. It is more efficient than many two-stage proposal-based object detection methods and traditional anchor-based models, which require multiple predefined anchor boxes for each prediction. Please refer to our response to Reviewer AkLB: \"3. Model Capacity and Computational Complexity\".\n\n3. Sensitivity to Loss Function Hyper-Parameters:\n\nA: We have calibrated the association loss weight (\u03bb_assoc=0.2) to ensure that all losses are of the same magnitude. Other hyper-parameters are set as defaults in established object detection models. Note that, we maintain these parameters consistently across different datasets to demonstrate the generalizability of our method. For NMS thresholds, we select optimal values for each dataset considering their specific levels of crowdedness, which is in line with protocols followed in previous methods, such as BPJDet, ensuring a fair and effective comparison.\n\n4. Comparative Analysis with Anchor-Based Models:\n\nA: We acknowledge the importance of an in-depth comparative analysis with traditional anchor-based models. BPJDet, a state-of-the-art anchor-based method, has already been compared in our paper. Additionally, we have introduced a new ablation study in our revised paper to compare our model with the traditional anchor-based YOLOv7 model, using the same association definition of part-to-body center offset as in our approach. This comparison has been instrumental in highlighting the advantages of the anchor-free design, especially in the context of part-body association tasks. Our findings show that the anchor-free version of YOLOv7 outperforms its anchor-based counterpart, validating the effectiveness of our method. Furthermore, we compared the traditional anchor-based YOLOv7 model with BPJDet, which also adopts an anchor-based approach but utilizes a body-to-part association definition. This comparison demonstrates that our part-to-body association definition significantly enhances performance, offering clear evidence of the superiority of our approach over previous methods.\n\n| Methods                  | Param (M) | Size | Hand AP\u2191 | Cond. Accuracy\u2191 | Joint AP\u2191 |\n|--------------------------|-----------|------|----------|-----------------|-----------|\n| BPJDet (anchor-based body-to-part)    | 41.2      | 1536 | 85.3     | 86.80           | 78.13     |\n| Ours (anchor-based part-to-body)     | 36.9      | 1024 | 88.4     | 92.31           | 85.28     |\n| Ours (anchor-free part-to-body)      | 36.9      | 1024 | 89.1     | 92.62           | 85.98     |\n\n\n5. Applicability Across Various Domains:\n\nA: We have expanded our research to include experiments on the AnimalPosedataset [1] [2], demonstrating our model's scalability and performance in domains beyond human part-to-body associations. The qualitative results of these experiments are included in the revised paper.\n\n[1] Cross-Domain Adaptation for Animal Pose Estimation\n\n[2] AP-10K: A Benchmark for Animal Pose Estimation in the Wild\n\n6. Performance in Scenarios with Overlapping or Closely Situated Objects:\n\nA: We have conducted experiments on the COCOHumanParts dataset, which includes scenarios where people overlap and are closely situated, as illustrated in Figure 4. Additionally, our Appendix includes experiments on the CrowdHuman dataset, featuring more densely overlapping situations. \n\nWe hope these responses address your concerns and provide a clearer understanding of our approach and its capabilities. We are committed to making the necessary revisions to reflect these clarifications and enhance the overall quality of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938257554,
                "cdate": 1699938257554,
                "tmdate": 1699938257554,
                "mdate": 1699938257554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0pxfV7iCee",
                "forum": "pPh9p8anUi",
                "replyto": "W0N8rlcgsT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_TS1Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_TS1Z"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author's rebuttal feedback as well as other reviews. The rebuttal has answered many of my questions, and I have increased my original rating score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458517569,
                "cdate": 1700458517569,
                "tmdate": 1700458517569,
                "mdate": 1700458517569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WvwmNwK0hd",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_NDUe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_NDUe"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of human body part detection and association. The authors present a one-stage, anchor-free method to tackle this problem. Specifically, they introduce a part-to-body center offset to capture the relationship between parts and their corresponding bodies. The authors conduct experiments on BodyHands, COCOHumanParts, and CrowdHuman datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- The proposed single-part-body center offset is efficient and can accommodate a large number of body parts without increasing the number of offsets as the number of parts grows. This makes the proposed method more efficient compared to previous methods.\n- The paper presents good experimental evaluations and comparisons with previous methods.\n- The paper also presents experimental evidence to study the benefits of each proposed component in ablation studies."
                },
                "weaknesses": {
                    "value": "-Table 2 doesn't mention AP for small objects. The proposed method performs better than previous methods on medium and large objects. How does this compare to small objects?\n\n-How do the ablation studies change on the COCOHumanParts dataset? I am curious about the performance of different components of the proposed method on different object sizes: small, medium, and large."
                },
                "questions": {
                    "value": "Please see the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1346/Reviewer_NDUe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764533205,
            "cdate": 1698764533205,
            "tmdate": 1699636061767,
            "mdate": 1699636061767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IgYXOWmjK4",
                "forum": "pPh9p8anUi",
                "replyto": "WvwmNwK0hd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer NDUe,\n\nThank you for your constructive feedback and insightful questions regarding our paper. We appreciate the opportunity to clarify the aspects you have highlighted. Please find our responses to your queries below.\n\n1. AP for Small Objects:\n\nA: We acknowledge your observation about the absence of AP for small objects in Table 2. It is important to note that in the original Hier R-CNN paper, small objects are further categorized into small and tiny objects, and AP for small objects is not explicitly provided. Similarly, another comparison method, BPJDet, also does not offer AP details for small objects. We will include the APs for small objects in our revised paper. We emphasize that Table 2 demonstrates our method's competitive object detection accuracy relative to state-of-the-art approaches. However, the critical metric for our task is association accuracy, where, as shown in Table 3, our method achieves superior performance. This distinction underscores our method's effectiveness in part-body association, which is central to our research objective.\n\n2. Ablation Study on COCOHumanParts Dataset:\n\nA: We have not conducted ablation studies on the COCOHumanParts dataset in our original submission. We acknowledge the importance of such analysis for a comprehensive understanding of our method's performance across different object sizes (small, medium, and large). We would like to highlight that our ablation studies were conducted on the BodyHands dataset, a large-scale dataset with unconstrained images and annotations for hand and body locations and correspondences. This dataset comprises 18,861 training images and 1,629 test images. The results from the BodyHands dataset should provide convincing evidence of our method's efficacy. Nevertheless, we understand the value of additional ablation studies on the COCOHumanParts dataset and, time permitting, we aim to conduct this analysis and share the findings.\n\nYour feedback has been invaluable in helping us improve the clarity and comprehensiveness of our research. We are committed to addressing these points and enhancing the overall quality of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937463746,
                "cdate": 1699937463746,
                "tmdate": 1699937463746,
                "mdate": 1699937463746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kS1xELWs4x",
                "forum": "pPh9p8anUi",
                "replyto": "IgYXOWmjK4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_NDUe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_NDUe"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for addressing my concerns. I have read the other reviews, and I would like to keep my original score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618450205,
                "cdate": 1700618450205,
                "tmdate": 1700618450205,
                "mdate": 1700618450205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DGJm1OoDH8",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_AkLB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_AkLB"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a one-stage anchor-free approach (PBADet) for part-body association detection via singular part-to-body center offset that captures the relationship between parts and their parent bodies. It is an extension to one-stage anchor-free object detection methods for part-body association to identify part-body relationships. The approach supports multiple parts-to-body associations without compromising the accuracy and robustness of the detection process. The approach is evaluated on BodyHands, COCOHumanParts, and CrowdHuman datasets and the performance is comparable to the state-of-the-art (SOTA)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is written well and easy to follow. The idea is very good and is inspired by the recent task alignment learning to enhance interaction between the two tasks for high-quality predictions.  \n\nThe rationale behind the anchor-free prediction, and part-to-body associations using models like YOLOv5, YOLOv7, and YOLOV8 is justified. \n\nA thorough experimental evaluation using well-known benchmarked datasets. On each dataset, the performance of the proposed approach is compared to the state-of-the-art and explains the performance gain and its impact on the overall accuracy.\n\nThe importance of the individual module model is experimentally evaluated. \n\nInteresting visualization to show qualitative comparison and highlight the erroneous predictions."
                },
                "weaknesses": {
                    "value": "The anchor-free object representation uses multi-scale feature maps. How many scales have been used?  Is this the same as the backbone (e.g., YOLOv7) feature representation?\n\nThe approach uses a singular part-to-body center offset. The body center is the bounding box center or something else. Also, how important is accuracy in detecting the body center influencing the overall performance?\n\nIt would be nice to have a section on model capacity and computational complexity (e.g. Params, GFLOPS, per-image inference time, etc) to further improve the article. This should be compared to the other SOTA models.\n\nThe multi-scale module has a significant impact on the performance improvement w.r.t. the baseline. What could be the reason?\n\nThe loss weights ($\\lambda$ and NMS thresholds $\\tau$ values) are optimal for a given dataset?"
                },
                "questions": {
                    "value": "Please refer to the \"Weakness\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784907376,
            "cdate": 1698784907376,
            "tmdate": 1699636061682,
            "mdate": 1699636061682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MWnExbZeZN",
                "forum": "pPh9p8anUi",
                "replyto": "DGJm1OoDH8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer AkLB,\n\nWe greatly appreciate your thorough review and constructive feedback on our paper. Your points have helped us identify areas for clarification and improvement. Please find our responses to your queries below.\n1. Number of Scales Used:\n\nA: In our experiments, the multi-scale feature maps align with the backbone feature representations. Specifically, the YOLOv7 model generates P3-P5 outputs (i.e., three scales), and the YOLOv5l6 model provides P3-P6 outputs (i.e., four scales). This consistency with the backbone ensures optimal utilization of the multi-scale feature capabilities.\n\n2.  Clarification on Body Center / Part-to-Body Center Offset:\n\nA: In our model, the body center is determined as the center of the bounding box, following standard practices in bounding box detection. Specifically, the center is calculated using the formula ((r+l)/2, (b+t)/2) for a bounding box defined by coordinates (r, l, t, b). Our ablation study (Table 4) demonstrates that incorporating association loss (for detecting part-body center offset) does not adversely affect hand detection accuracy, showcasing the robustness of our approach.\n\n3. Model Capacity and Computational Complexity:\n\nA: We thank you for highlighting the importance of discussing model capacity and computational complexity. Our paper provides detailed information on model parameter sizes and input image sizes in Tables 1 and 3. To further emphasize the efficiency of our model, we have included a comprehensive analysis of inference speed.\n\nSpecifically, we have assessed the frames per second (FPS) of PBADet using YOLOv7 and YOLOv5l6 detectors on a GPU V100. With an input size of 1280x1280, PBADet achieves approximately 84 FPS with YOLOv7 and 63 FPS with YOLOv5l6. These numbers indicate that PBADet is well-suited for real-time applications, demonstrating significant efficiency in processing.\n\nFor context, we compared this with the ResNet-101-FPN-based detector Mask R-CNN, which is utilized in methods such as BodyHands [1] and Hier R-CNN [2]. The official FPS for Mask R-CNN is about 10 to 20 per image on an NVIDIA V100 GPU & NVLink [3] [4]. When considering the additional computational load due to the association network in BodyHands [1] and Hier R-CNN [2], it is evident that these methods lag significantly behind PBADet in terms of speed. Note that our method has an input size of 1024x1024 while BodyHands [1] and Hier R-CNN [2] use an input size of 1536x1536 (note that the original Mask-CNN uses 800x800), which further enlarges the FPS gap. This disparity in processing efficiency highlights the advanced capability of PBADet, particularly when it comes to real-time processing demands.\n\n[1] Whose Hands Are These? Hand Detection and Hand-Body Association in the Wild, CVPR 2022\n\n[2] Hier R-CNN: Instance-Level Human Parts Detection and A New Benchmark, TIP 2020\n\n[3] Mask R-CNN\n\n[4] https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n\n4. Impact of Multi-Scale Module:\n\nA: Our ablation study in Table 4 indicates that interpreting the center offset as a normalized absolute distance without multi-scale considerations leads to a reduction in hand prediction accuracy and joint AP. This is because each point in the multi-scale dense feature predicts a center offset, and it is challenging to achieve convergence for different scales to output the same absolute distance. The multi-scale module thus plays a critical role in enhancing performance.\n\n5. Optimization of Loss Weights and NMS Thresholds:\n\nA: The loss weights during training remain the same across different datasets. However, the NMS thresholds are not loss weights and are post-processing parameters in object detection. NMS thresholds are optimally selected for each dataset since different datasets present varying levels of crowdedness. This approach is in line with protocols followed in previous methods, such as BPJDet, ensuring a fair and effective comparison.\n\nWe hope these clarifications address your concerns and contribute to a better understanding of our work. We are committed to making the necessary revisions in our paper to reflect these clarifications and enhance its overall quality."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937327062,
                "cdate": 1699937327062,
                "tmdate": 1699937357663,
                "mdate": 1699937357663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cc123mnqC7",
                "forum": "pPh9p8anUi",
                "replyto": "MWnExbZeZN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_AkLB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Reviewer_AkLB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my concerns.\n\nI have gone through the reviews of other reviewers comments/questions and authors' reply to those. Most of raised questions are addressed convincingly. Thus, I am inclined in accepting. Thank you."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480580009,
                "cdate": 1700480580009,
                "tmdate": 1700480580009,
                "mdate": 1700480580009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ksN27VPJP7",
            "forum": "pPh9p8anUi",
            "replyto": "pPh9p8anUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_BLsk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1346/Reviewer_BLsk"
            ],
            "content": {
                "summary": {
                    "value": "-\tIt would be better if more details about the association process could be given.\n-\tHow to choose the value of the hyper-parameter K used in L_{assoc}? The experiments would be more comprehensive if the value of K could be analyzed in the ablation study.\n-\tThe reviewer wonders whether the models are trained from scratch or the pretrained YOLO weights are used. It is not mentioned in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tCompared to the body-to-part center offset used in the previous one-stage method BPJDet, the part-to-body center offset proposed in this paper has better scalability in terms of the number of parts and avoids degrading the overall object detection performance.\n-\tThe PBADet method proposed in the paper achieves state-of-the-art performance on BodyHands, COCOHumanParts and CrowdHuman datasets.\n-\tThis paper is well organized, clearly presented, and effectively clarifies the differences from previous methods."
                },
                "weaknesses": {
                    "value": "-\tThe author states that the proposed part-to-body center offset guarantees a one-to-one correspondence between parts and bodies. However, the paper does not elaborate on how the situation is handled when the predicted center offsets of multiple parts with the same category point to the same body. How to define the order for associating multiple parts with the same category? These details are not described in Section 3.4. \n-\tIn Figure 2, it seems that 'P3' and 'P5' in the 'Multi-scale features' are labeled incorrectly."
                },
                "questions": {
                    "value": "-\tIt would be better if more details about the association process could be given.\n-\tHow to choose the value of the hyper-parameter K used in L_{assoc}? The experiments would be more comprehensive if the value of K could be analyzed in the ablation study.\n-\tThe reviewer wonders whether the models are trained from scratch or the pretrained YOLO weights are used. It is not mentioned in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1346/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792838385,
            "cdate": 1698792838385,
            "tmdate": 1699636061594,
            "mdate": 1699636061594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dYU9jk4SVb",
                "forum": "pPh9p8anUi",
                "replyto": "ksN27VPJP7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1346/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer BLsk,\n\nThank you for your valuable feedback and insightful comments on our paper. We appreciate the opportunity to clarify and address your concerns.\n\n1.  Handling Multiple Parts Pointing to the Same Body:\n\nA: We acknowledge your concern regarding how our method handles situations where multiple parts of the same category point to the same body. As detailed in Section 3.4 \"DECODING PART-TO-BODY ASSOCIATIONS\", our network outputs the center offset of a part, pointing to the body to which the part belongs. We use Euclidean distance between the estimated body center (part center plus the center offset) and the centers of unassigned bodies that enclose the part. The body with the smallest distance (i.e., the distance of the estimated body center and body center) is then assigned as the corresponding body for the given part. This approach ensures accurate and efficient part-to-body association.\n\n2. Typographical Error in Figure 2:\n\nA: We are grateful for your observation regarding the labeling error in Figure 2, where 'P3' and 'P5' in the 'Multi-scale features' were incorrectly labeled. We have corrected this typo in the revised version of the paper to ensure clarity and accuracy in our visual representations.\n\n3. Clarification on the Hyper-parameter K in L_{assoc}:\n\nA: Regarding the hyper-parameter K used in L_{assoc}, we have chosen K=13, aligning with the default value used in established anchor-free object detection methods such as YOLOv7 and YOLOv8. This choice is substantiated in our ablation study (Table 4), where it is shown that performance degrades when K equals the number of anchor points (i.e., w/o Task-Align). \n\n4. Model Training from Scratch or Pretrained YOLO weights:\n\nA: We appreciate your inquiry about whether our model was trained from scratch or using pretrained YOLO weights. To clarify, we followed the same protocol established in the BPJDet paper [1] that uses pretrained YOLO weights. This implementation detail will be explicitly stated in the revised manuscript to avoid any ambiguity.\n\n[1] Body-Part Joint Detection and Association via Extended Object Representation\n\nYour feedback has been instrumental in enhancing the clarity and accuracy of our work. We are committed to making the necessary revisions to address these points thoroughly."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1346/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937183970,
                "cdate": 1699937183970,
                "tmdate": 1699939641068,
                "mdate": 1699939641068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]