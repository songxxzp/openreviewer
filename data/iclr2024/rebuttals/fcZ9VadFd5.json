[
    {
        "title": "Emergence of Equivariance in Deep Ensembles"
    },
    {
        "review": {
            "id": "9Lbdqu5eEV",
            "forum": "fcZ9VadFd5",
            "replyto": "fcZ9VadFd5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_waBb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_waBb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proves that an infinite ensemble of neural networks becomes equivariant with data augmentation under mild assumptions. They use neural tangent kernels to show the equivariance. This property is also empirically evaluated with three tasks such as rotated image classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper theoretically shows that the equivariance emerges by model ensemble without hand-crafted architecture design. This direction, obtaining equivariance while we can freely choose networks, is important in practical usage. \n\nAlthough the paper is theory-flavored, it is easy to read and follow."
                },
                "weaknesses": {
                    "value": "The main finding (emergence of equivariance in deep ensembles) is not very surprising. Data augmentation imposes a bias on a model toward invariance/equivariance, and for me, it's natural to see the averaged model archives that property. I mean, if we have an infinite number of data instances and the model capacity is large enough, the model trained for many steps would be equivariant. The neural tangent approach is of course different from this asymptotic approach, but the main idea should be the same. \n\nThe experiments have room for improvement.\n1. Instead of equivariance, invariance is evaluated.\n2. Only a cyclic group is considered so it is not clear what kind of consequence can we get for more complex groups such as SO(2), SO(3), or SE(3). \n3. No comparison with equivariant networks such as steerable CNNs."
                },
                "questions": {
                    "value": "In Equation (13) you assume that we can get the index permutation. However, for continuous groups such as SO(2) we cannot do this. Can you generalize the entire theory to avoid this issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720586780,
            "cdate": 1698720586780,
            "tmdate": 1699636864620,
            "mdate": 1699636864620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aqRC2Alk0B",
                "forum": "fcZ9VadFd5",
                "replyto": "9Lbdqu5eEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> **Strengths:**\nThe paper theoretically shows that the equivariance emerges by model ensemble without hand-crafted architecture design. This direction, obtaining equivariance while we can freely choose networks, is important in practical usage.\nAlthough the paper is theory-flavored, it is easy to read and follow.\n> \n\nWe thank the reviewer for the positive feedback. \n\n> **Weaknesses:**\nThe main finding (emergence of equivariance in deep ensembles) is not very surprising. Data augmentation imposes a bias on a model toward invariance/equivariance, and for me, it's natural to see the averaged model archives that property. I mean, if we have an infinite number of data instances and the model capacity is large enough, the model trained for many steps would be equivariant. The neural tangent approach is of course different from this asymptotic approach, but the main idea should be the same.\n> \n\nIt is indeed intuitive that the ensemble becomes equivariant on the training- and test data when trained with data augmentation. However, our results go significantly beyond this intuition in several important directions:\n\n1. Our theoretical analysis shows that for large-width ensembles, the equivariance is not only approximate, as expected from data augmentation, but indeed exact.\n2. For training with data augmentation, one would expect (approximate) equivariance to hold only on the data manifold, but not away from it since the ensemble is only trained on the data manifold. This would be true even in the asymptotic case of infinite training data and -time. However, both our neural tangent kernel based arguments and our experiments show that the ensemble becomes a truly equivariant function *which is also equivariant away from the data manifold*.\n3. *We show that the ensemble is equivariant for all training times.* This is in contrast to the above intuition which would suggest that the ensemble becomes more and more equivariant throughout training as it learns to fit the augmented training data.\n\n> The experiments have room for improvement.\n>1. Instead of equivariance, invariance is evaluated.\n> \n\nIn the new version of the manuscript, we have extended our experiments to also include an equivariant task, see Appendix C and in particular Figure 10. We have trained ensembles of fully-connected networks to predict the cross product in $\\mathbb{R}^3$, a task which is equivariant with respect to rotations in SO(3). Our experiments show that the predictions of even small ensembles are significantly more equivariant than the predictions of their ensemble members. This is the case even though we only augment with a small finite subset of the continuous symmetry group and holds on- as well as off manifold.\n\n> 2. Only a cyclic group is considered so it is not clear what kind of consequence can we get for more complex groups such as SO(2), SO(3), or SE(3).\n> \n\nIn order to address this point, we have extended our experiments on FashionMNIST. As detailed in Section B.2 in the appendix of the revised manuscript, we have measured invariance with respect to the full SO(2) group of ensembles trained on finite subgroups $C_4$, $C_8$ and $C_6$. Our experiments summarized in Figure 5 show that even for moderately-sized subgroups, the invariance of the ensemble is very high, in accordance to Lemma 6. Furthermore, the ensemble output is much more invariant than individual ensemble members, as is the case for invariance with respect to finite subgroups.\n\nNote that our new experiments on the cross product concern the continuous symmetry group SO(3)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683674295,
                "cdate": 1700683674295,
                "tmdate": 1700683674295,
                "mdate": 1700683674295,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qXsA3wmNXj",
            "forum": "fcZ9VadFd5",
            "replyto": "fcZ9VadFd5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_8JsL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_8JsL"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors prove that when trained with data augmentation, deep ensembles are equivariant during training. They use the theory of NTKs and explicitly prove that the deep ensembles are equivariant regardless of the training step or data. However, this is limited by the fact that ensembles are finite, networks are not infinitely wide, and there is a limit to data augmentation for continuous groups. The authors further provide error bounds considering these limitations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Although I am not very familiar with neural tangent kernels, the authors presented the work in a way that was easy to follow. Theorem 4 in particular seems like a very strong result. The authors further consider practical and very relevant limitations such as the finite ensemble, continuous groups, and finite width and prove error bounds. The experiments support the theory."
                },
                "weaknesses": {
                    "value": "The type of data augmentation considered seems perhaps a little strong. By using all elements of the group orbit, it naturally lends itself to rewriting the group action as permutations, which seems to be critical in the proof. However, many common data augmentation strategies involve loss of information (e.g. random crops, random non-circular shifts, etc.). If the authors could provide any insights or foreseeable limitations of this work for other data augmentation types, that would be very helpful."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Reviewer_8JsL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806959906,
            "cdate": 1698806959906,
            "tmdate": 1699636864495,
            "mdate": 1699636864495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H9iyZTkYBb",
                "forum": "fcZ9VadFd5",
                "replyto": "qXsA3wmNXj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> **Strengths:**\nAlthough I am not very familiar with neural tangent kernels, the authors presented the work in a way that was easy to follow. Theorem 4 in particular seems like a very strong result. The authors further consider practical and very relevant limitations such as the finite ensemble, continuous groups, and finite width and prove error bounds. The experiments support the theory.\n> \n\nWe thank the reviewer for the positive feedback. \n\n> **Weaknesses:**\nThe type of data augmentation considered seems perhaps a little strong. By using all elements of the group orbit, it naturally lends itself to rewriting the group action as permutations, which seems to be critical in the proof. However, many common data augmentation strategies involve loss of information (e.g. random crops, random non-circular shifts, etc.). If the authors could provide any insights or foreseeable limitations of this work for other data augmentation types, that would be very helpful.\n> \n\nAlthough many commonly used data augmentation strategies involve a loss of information and are therefore not strictly speaking symmetry transformations, they can usually be interpreted as approximating some group transformation. E.g. random crops approximate scalings and non-circular shifts approximate translations. In particular for network architectures which use local transformations of the input features like convolutions or windowed self-attention, the deviations from a strict symmetry transformation are often restricted to some regions of the input, e.g. the edges. On these grounds, we expect the observed effects of emerging equivariance for ensembles trained with data augmentation to hold approximately in the case of lossy augmentation strategies.\n\nThe approximation in going from an augmentation involving information loss to an augmentation which is an exact symmetry transformation is comparable to augmenting with a finite subgroup of a continuous symmetry group and evaluating equivariance for the full symmetry group. For this case, we provide a theoretical bound on the equivariance error in Lemma 6. As our new experiments (see Appendix B.2, and in particular Figure 5, of the revised manuscript) on the FashionMNIST dataset in this setting demonstrate, shows the ensemble small equivariance errors on the full symmetry group, even when trained on relatively small finite subgroups. Therefore, we expect a similar emergence of true equivariance when going from a lossy data augmentation scheme to one which involves exact symmetry transformations."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683563485,
                "cdate": 1700683563485,
                "tmdate": 1700683563485,
                "mdate": 1700683563485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BUtL1FEcFG",
                "forum": "fcZ9VadFd5",
                "replyto": "H9iyZTkYBb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7253/Reviewer_8JsL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7253/Reviewer_8JsL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors and am satisfied with the response. I maintain my original score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711784839,
                "cdate": 1700711784839,
                "tmdate": 1700711784839,
                "mdate": 1700711784839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NagVHUETTb",
            "forum": "fcZ9VadFd5",
            "replyto": "fcZ9VadFd5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_FAEm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_FAEm"
            ],
            "content": {
                "summary": {
                    "value": "This work considers deep ensembles in the infinite width limit / NTK regime. For a deep ensemble on a dataset with equivariant data augmentation to a symmetry group, the work shows that the deep ensemble is equivariant at all points in its training evolution. Bounds are given on the behavior of different approximations to this, in the cases of: finite ensembles and finite subgroups for data augmentation. Empirical results show that numerically trained ensembles approach equivariance as width or number of models in the ensemble increase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Well-written and well-organized Section 5: the proof sketch is nice.\n2. Interesting empirical results that support the theory. The ensembles become more equivariant as width or number of models increases."
                },
                "weaknesses": {
                    "value": "1. Do these results prescribe any particular practical methods, or does it give particular insights on models trained in practice? There does not seem to be much discussion on this. For instance, do people often train ensembles on equivariant data, and how does this compare to single models?\n2. Could use more details on the critical assumption on the input layer, see question 1 below."
                },
                "questions": {
                    "value": "1. Does your assumption on the networks depending on input through $w^{(k)}x$ on Page 5 really hold for CNNs? CNNs have their filter coefficients initialized via centered Gaussians, but the underlying matrix is not (because of weight sharing). Thus, there are orthogonal transformations on the input that may change the output (e.g. permute top left with top right pixel).\n2. Intuitively, what does the deep ensemble output look like at initialization? I am trying to intuit why it is equivariant then.\n3. Could you give more explanation or intuition about $C(x)$ in Lemma 6 in the main text?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7253/Reviewer_FAEm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698940673278,
            "cdate": 1698940673278,
            "tmdate": 1699636864398,
            "mdate": 1699636864398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZMQ0cpEtZM",
                "forum": "fcZ9VadFd5",
                "replyto": "NagVHUETTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> **Strengths:**\n>1. Well-written and well-organized Section 5: the proof sketch is nice.\n>2. Interesting empirical results that support the theory. The ensembles become more equivariant as width or number of models increases.\n> \n\nWe thank the reviewer for the positive feedback.\n\n> **Weaknesses:**\n>1. Do these results prescribe any particular practical methods, or does it give particular insights on models trained in practice? There does not seem to be much discussion on this. For instance, do people often train ensembles on equivariant data, and how does this compare to single models?\n> \n\nIn our view, emergent equivariance of deep ensembles is mainly interesting in scenarios for which ensembles are also used for different reasons such as uncertainty prediction and robustness. It is an important contribution of our manuscript to prove that equivariance comes for free in these scenarios. We stress that deep ensembles are widely used and thus this is a finding of immediate practical value. For example, in protein structure prediction, many models use an ensemble to estimate the uncertainty of the prediction, see e.g. [1, 2]. There are even some works in this context that rely on non-equivariant architectures and ensure equivariance by averaging over an appropriate subset of the group orbit [3]. It is an exciting direction of future research to harness our insights to impose equivariance by ensemble. Note that we demonstrate in the revised Appendices B.2 and C that deep ensembles can also lead to emergent SO(2) and SO(3) equivariance, respectively.\n\n[1] Ruffolo, Jeffrey A., et al. \"Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies.\"\u00a0*Nature communications*\u00a014.1 (2023): 2389. [https://www.nature.com/articles/s41467-023-38063-x](https://www.nature.com/articles/s41467-023-38063-x)\n\n[2] Abanades, Brennan, et al. \"ImmuneBuilder: Deep-Learning models for predicting the structures of immune proteins.\"\u00a0*Communications Biology*\u00a06.1 (2023): 575. [https://www.nature.com/articles/s42003-023-04927-7](https://www.nature.com/articles/s42003-023-04927-7)\n\n[3] Martinkus, Karolis, et al. \"AbDiffuser: full-atom generation of in-vitro functioning antibodies.\"\u00a0NeurIPS 2023, *arXiv preprint arXiv:2308.05027*\u00a0(2023).\n\n> 2. Could use more details on the critical assumption on the input layer, see question 1 below.\n> \n\nSee below\n\n> **Questions:**\n>1. Does your assumption on the networks depending on input through\u00a0$w^k x$\u00a0on Page 5 really hold for CNNs? CNNs have their filter coefficients initialized via centered Gaussians, but the underlying matrix is not (because of weight sharing). Thus, there are orthogonal transformations on the input that may change the output (e.g. permute top left with top right pixel).\n> \n\nIt is true that there are orthogonal transformations to a CNN which change its output. However, in the NTK, we are concerned with the expectation value over the (inner product of) derivatives of the output with respect to the trainable parameters. If we perform an orthogonal transformation of the input domain, the derivatives change, but their expectation value over initializations remains the same since the initialization distribution is the same for all filter components.\n\n> 2. Intuitively, what does the deep ensemble output look like at initialization? I am trying to intuit why it is equivariant then.\n> \n\nThe ensembles output at initialization is constant, see Eq. 6. As a result, the network is trivially equivariant for all symmetry groups. Data augmentation ensures that this equivariance is not broken by training although the output is no longer constant.\n\n> 3. Could you give more explanation or intuition about\u00a0C(x)\u00a0in Lemma 6 in the main text?\n> \n\nThere are some aspects of C that can indeed be intuited:\n- The constant C vanishes at the beginning of training as the ensemble is trivially equivariant due to its constant output.\n- The constant depends on an expectation over initializations involving the Libschitz constant as well as the expected gradient of the network over initializations. This is to be expected since violations of equivariance with respect to the continuous symmetry will strongly depend on how drastically the ensemble members can change between the points on the group orbit covered by the discretization of the continuous symmetry."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683475433,
                "cdate": 1700683475433,
                "tmdate": 1700683475433,
                "mdate": 1700683475433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HhD1CTuiEe",
            "forum": "fcZ9VadFd5",
            "replyto": "fcZ9VadFd5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_wXhZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7253/Reviewer_wXhZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows how, in a large width limit and with the inclusion of data augmentation, a generic deep ensemble becomes inherently equivariant. This equivariance is observed at each training step, irrespective of the chosen architecture, contingent upon the utilization of data augmentation. Notably, this equivariance extends beyond the observed data manifold and emerges through the collective predictions of the ensemble, even though individual ensemble member is not equivariant. It provides both theoretical proof, utilizing neural tangent kernel theory, and experiments to support and validate these observations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper presents a very interesting idea of the emergence of equivariance with data augmentation and model ensembles.\n\n2) This paper is generally well-written.\n\n3) The theoretical claims in the paper are sound."
                },
                "weaknesses": {
                    "value": "1) This paper lacks a proper comparison with other methods that can bring equivariance without any constraint on the architecture like [1, 2, 3, 4, 5]. When showing the out-of-distribution transformation results it'll be great to compare with those methods. The current results in the paper are more like ablations of the proposed augmentation and ensembling technique. It is not clear where it stands with other architecture-agnostic equivariance methods. (even if the proposed method does poorly compared to those it'll be good to have those results)\n\n2) The author claims data augmentation is the only alternate method to bring equivariance in a non-equivariant model. I'll refer these papers [1,5] to the authors where they show that equivariance can be achieved using symmetrization and canonicalization. It'll be nice to include those as well in the paper. Especially symmetrization is closely related to the idea of ensembling because you pass different transformations of the same image throughout the same network before you average. My intuition is that symmetrization keeps the architecture the same and transforms the input, whereas the current work keeps the input the same and learns a transformer version of weights or each of the networks learning to process different transformations of the input. It'll be great if the authors can shed some light on the connection and discuss architecture agnostic body of work.\n\n\n[1] Puny, O., Atzmon, M., Ben-Hamu, H., Misra, I., Grover, A., Smith, E. J., & Lipman, Y. (2021). Frame averaging for invariant and equivariant network design. arXiv preprint arXiv:2110.03336.\n\n[2] Mondal, A. K., Panigrahi, S. S., Kaba, S. O., Rajeswar, S., & Ravanbakhsh, S. (2023). Equivariant Adaptation of Large Pre-Trained Models. arXiv preprint arXiv:2310.01647.\n\n[3] Basu, S., Sattigeri, P., Ramamurthy, K. N., Chenthamarakshan, V., Varshney, K. R., Varshney, L. R., & Das, P. (2023, June). Equi-tuning: Group equivariant fine-tuning of pretrained models. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 6788-6796).\n\n[4] Basu, Sourya, et al. \"Equivariant Few-Shot Learning from Pretrained Models.\" arXiv preprint arXiv:2305.09900 (2023).\n\n[5] Kaba, S. O., Mondal, A. K., Zhang, Y., Bengio, Y., & Ravanbakhsh, S. (2023, July). Equivariance with learned canonicalization functions. In International Conference on Machine Learning (pp. 15546-15566). PMLR."
                },
                "questions": {
                    "value": "See the weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698977986788,
            "cdate": 1698977986788,
            "tmdate": 1699636864305,
            "mdate": 1699636864305,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6d6MIzwkfQ",
                "forum": "fcZ9VadFd5",
                "replyto": "HhD1CTuiEe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "### **Strengths:**\n\n**1. This paper presents a very interesting idea of the emergence of equivariance with data augmentation and model ensembles.\n2. This paper is generally well-written.\n3. The theoretical claims in the paper are sound.**\n\nWe thank the reviewer for the positive feedback. \n\n### **Weaknesses:**\n\n1. **This paper lacks a proper comparison with other methods that can bring equivariance without any constraint on the architecture like [1, 2, 3, 4, 5]. When showing the out-of-distribution transformation results it'll be great to compare with those methods. The current results in the paper are more like ablations of the proposed augmentation and ensembling technique. It is not clear where it stands with other architecture-agnostic equivariance methods. (even if the proposed method does poorly compared to those it'll be good to have those results)**\n\nIn light of your comments, we have substantially expanded the related works section discussing similarities and differences to the suggested references (see point immediately below). \n\nFurthermore, we have added additional numerical experiments in Appendix B.2 comparing deep ensembles to the canoncialization method of [5]. Briefly summarized, we find the following: We use the exact same model architecture for both the predictor of the canonicalization and the members of the deep ensemble and train on FashionMNIST. Canonicalization leads to an exactly equivariant model whereas our method yields approximate equivariance due to finite number of ensemble members and finite width. As a result, a comparison in the orbit same prediction metric is trivial and clearly shows the benefits of the canonicalization approach. On the other hand, deep ensembles have the advantage that they naturally benefit from well-known advantages of deep ensembles, such as improved accuracy and robustness as well as natural uncertainty estimation. Interestingly, we find a comparable scaling of equivariance to the full SO(2) group when we compare deep ensembles trained with $C_k$ data augmentation with the corresponding canonicalized $C_k$ models, see Figure 5 and 6. Furthermore, we find that deep ensembles lead to a validation accuracy of 91% while the canonicalized model reaches 87%. Both comparisons however come with important caveats: canonicalization allows for full SO(2) equivariance by choosing an appropriate canonicalizer. Futhermore, we are comparing an ensemble of models with a single canonalized model (with the same architecture). An ensembling of canonalized models most likely closes the performance gap. \n\nIn our view, emergent equivariance of deep ensembles is particularly interesting in scenarios for which ensembles are also used for different reasons such as uncertainty prediction and robustness. It is an important contribution of our manuscript to prove that an approximate form of equivariance comes for free in these scenarios. We stress that deep ensembles are widely used and thus this is a finding of immediate practical value.\n\nFinally, we want to highlight that our manuscript is a theory paper. Its main objective is to theoretically elucidate surprising emergent capabilities of deep ensembles by using neural tangent kernel theory. We did not aim to propose a highly competitive method which would likely involve applying several tricks of the trade which are harder to model theoretically, such as sampled data augmentation as well as non-iid ensemble member selection."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683276712,
                "cdate": 1700683276712,
                "tmdate": 1700683276712,
                "mdate": 1700683276712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]