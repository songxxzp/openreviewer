[
    {
        "title": "Forward Gradient Training of Spiking Neural Networks"
    },
    {
        "review": {
            "id": "XEaDcNyTdr",
            "forum": "yBP36xQhZl",
            "replyto": "yBP36xQhZl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_D141"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_D141"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a novel approach for training Spiking Neural Networks (SNN) known as Forward Gradient Training (FGT). FGT introduces a spatial error signal allocation scheme, providing a solution to distribute errors across the network. By solely relying on forward propagation, FGT enables direct supervision signal transmission across layers, thus avoiding the layer-by-layer error propagation inherent in backpropagation. This methodology proves to be more conducive to online learning on chip architectures compared to traditional methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The writing in this article is commendable, exhibiting clear and fluid expression. The derivation of mathematical formulas is coherent, enhancing its readability significantly.\n2. This article breaks free from the conventional framework of Backpropagation Through Time (BPTT) and boldly explores a new training approach. It introduces a more biologically plausible spatial error propagation method called Forward Gradient with Momentum Feedback Connections."
                },
                "weaknesses": {
                    "value": "1. Compared to BPTT, FGT incurs larger training costs and is relatively slower on GPUs.\n2. The article claims that this method takes a step forward towards online learning on chips, but it does not delve into the implementation of FGT on a chip or the associated storage and computational costs."
                },
                "questions": {
                    "value": "1. The current article only validates the effectiveness of FGT on shallow and simpler network structures, showing comparable results to BPTT. However, its performance on deeper and more complex networks remains unexplored. The limited performance on smaller networks suggests that if this method fails to scale well to larger networks, the significance of this work may be limited. For example, the authors could discuss why the proposed framework provides a more feasible implementation of synchronous circuits. Also, there are existing methods that estimate the FLOPS and energy cost of a given NN based on its memory and operation. The authors could provide such comparisons as well. In addition, it is also necessary to provide the comparison with other  training methods on CIFAR-100 as well as ImageNet."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726703218,
            "cdate": 1698726703218,
            "tmdate": 1699636408421,
            "mdate": 1699636408421,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cxh35SS0Qc",
                "forum": "yBP36xQhZl",
                "replyto": "XEaDcNyTdr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D141 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We respond to your comments and questions as follows. The citations are the same as the paper unless specified.\n\n1. About training costs on GPU. \n\nFirst, our training cost comparison is with BP based on online training methods (Xiao et al., 2022) rather than BPTT. As described in the paper, our work is built upon online methods and the comparisons mainly focus on different spatial credit assignment methods given the online methods. For BPTT, the training memory cost is proportional to time steps, while online training methods have constant memory costs and is smaller than BPTT (e.g., for the considered 6 time steps, there can be $2-3\\times$ reduction). Our method also has smaller memory costs than BPTT.\n\nSecond, we would like to emphasize that our method mainly focuses on developing neuromorphic-friendly algorithms rather than only considering efficiency on GPU. Our ultimate goal is to consider efficient neuromorphic hardware, and as it is currently immature, we develop methods with simulation on GPU. Our method has more neuromorphic properties and is more plausible than BP. And compared with random-feedback-based methods (DFA), our method significantly improves performance to a similar level as BP. We can realize these targets with comparable costs, which could be acceptable.\n\nIn Appendix E.1, we also discussed training costs conditions on potential neuromorphic computing hardware in the text, and our FGT (S) is better than BP (if possible) considering neuromorphic computing. On GPU, the training costs of FGT (or FGT(S)) is (slightly) larger than BP because GPU does not follow neuromorphic architectures and we do not optimize low-level codes of our method as BP in the established PyTorch libraries (the results are only brief comparisons). It can also be further improved, e.g., with parallelization of computation for forward gradients.\n\nMeanwhile, our developed variant FGT (S) is more practical in real applications and also has about the same costs and speed as BP on GPUs, as shown in Table 6 in Appendix E, which can also scale to large-scale networks as shown in the following response to the third point.\n\n2. About discussion of implementation on a chip or the associated storage and computational costs.\n\nCurrently, the development of neuromorphic hardware is immature and is also developing considering software-hardware co-design [1], so most algorithms are simulated on common computational devices while considering properties of neuromorphic computing, without experiments on real neuromorphic chips. The compatibility for implementation is measured by fitness to neuromorphic properties (or biological plausibility) because neuromorphic hardware will be designed following them for efficient event-driven and in-memory computation. FGT tackles BP\u2019s problems of weight transport and separate phases for error signal propagation as it only requires unidirectional forward propagation across layers, conforming to properties of neuromorphic computing. We do not restrict the method to specific hardware, but follow the basic properties that will guide the development of hardware and algorithms. In Section 4.4, we also discussed some details on how the signal propagation of forward gradient can be made biologically plausible, proposing the variants of FGT (e.g., FGT (S), FGT (Q1, S)) which are also verified in experiments.\n\nWe have discussed storage and computational costs on potential neuromorphic computing hardware in Appendix E.1, and we further supplement a comparison table for clearer presentation in this revision. We quote the discussions and the table here: \u201c*for memory costs, \u2026 On the other hand, if we consider neuromorphic computing with unidirectional synapses, BP (if possible) should maintain all backward synapses between successive layers, while FGT, FGT (S), and DFA keep direct feedback connections from the top layer, which can be much smaller than BP since middle layers usually have much more neurons. As shown in Table 5, the memory costs of DFA and FGT (S) can be much smaller than BP*\u201d, \u201c*for operation numbers, BP requires a backward propagation across $N$ layers, DFA requires direct feedback to $N$ layers, FGT requires $N$ forward propagation of forward gradients for different layers (across $N, N\u22121, ..., 1$ layers) and direct feedback to $N$ layers, and FGT (S) requires a forward propagation across average $N/2$ layers and direct feedback to $N$ layers which is more practical than FGT. Direct feedback to $N$ layers may need fewer operations than BP across $N$ layers as middle layers usually have more neurons (for example, propagation from 400 to 400 neurons requires much more operations than from 10 to 400 neurons). So with a rough estimation, DFA needs much fewer operations than BP, and FGT (S) needs fewer operations than BP, as shown in Table 5*\u201d.\n\n(continued below)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365310071,
                "cdate": 1700365310071,
                "tmdate": 1700365310071,
                "mdate": 1700365310071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RFGFmSKqsk",
                "forum": "yBP36xQhZl",
                "replyto": "2aiqHek7zb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4363/Reviewer_D141"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4363/Reviewer_D141"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the explanation and for providing further results.  It seems that the efficacy of the method for large networks remains a concern. Regarding the feasibility of neuromorphic chips, bio-plausibility is not a main concern. The key issue is whether the algorithm is robust to the deviation in arrival time, which is typically not ensured in asynchronous circuits. It would be helpful if the authors could provide a discussion from this perspective with a detailed investigation of the executing steps of the proposed method."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450661543,
                "cdate": 1700450661543,
                "tmdate": 1700450661543,
                "mdate": 1700450661543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PhaDDAlXUZ",
            "forum": "yBP36xQhZl",
            "replyto": "yBP36xQhZl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_NYct"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_NYct"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a FGT method for training SNNs that requires unidirectional FW prop and direct feedback from top layer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper talks about bio-plausibility and hardware compatibility.\n\nThere are some theoretical guarantees that the paper talks about with performance better than random feedback.\n\nThe authors have shown their method works on simple datasets."
                },
                "weaknesses": {
                    "value": "While the paper presents promising results, there may be an overstatement of the method's superiority without thorough comparison to a wide range of existing methods. There are substantial works [1-2] and many more that target hardware efficiency that have shown SOTA performance.\n[1] Li, Yuhang, et al. \"SEENN: Towards Temporal Spiking Early-Exit Neural Networks.\" arXiv preprint arXiv:2304.01230 (2023).\n[2] Kim, Youngeun, et al. \"Exploring lottery ticket hypothesis in spiking neural networks.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\nI am not sure if the authors have a quantitative way of suggesting their method's superiority to others in terms of accuracy or efficiency.\n\nFinally, can the authors comment if their work has any limitations in terms of scalability or applicability to different data?\n\nI am willing to change my rating post-rebuttal if the reviewers can give me a better justification about their results as compared to SOTA."
                },
                "questions": {
                    "value": "See weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777512552,
            "cdate": 1698777512552,
            "tmdate": 1699636408358,
            "mdate": 1699636408358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uOL1Hy1RCO",
                "forum": "yBP36xQhZl",
                "replyto": "PhaDDAlXUZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NYct (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We respond to your comments and questions as follows. The citations are the same as the paper unless specified.\n\n1. About discussion and comparison to works with SOTA performance. \n\nOur work is a different line from these works, and they can be orthogonal and combined with each other. Specifically, recent works with SOTA performance focus on training high-performance SNNs on common hardware (e.g., GPU) with training methods such as BPTT and other improvements of models, and the trained models are expected to be directly deployed on neuromorphic hardware. Their hardware efficiency is about trained models. Our work, on the other hand, aims at developing algorithms that adhere to neuromorphic properties and are expected to pave paths for on-chip training of SNNs. Our hardware efficiency is about the training algorithm.\n\nThese two lines of work are compatible with each other. First, most recent SOTA works focus on the improvement of models and are still based on BP for training. Our work considers a different error propagation method and is orthogonal to them, which can be possibly combined in the future. With biologically plausible training methods that pave paths for potential on-chip training, we may also expect energy-efficient training directly on hardware, because GPUs are not designed for event-driven and in-memory computation, and training SNNs on GPUs is costing. \n\nSecond, considering real applications, we can also first pretrain a high-performance SNN on GPU as these works, and after deploying it on hardware, we may expect on-chip training of the models to deal with problems such as hardware mismatch or tuning the model for new data or tasks so that the model can adaptively evolve for real applications. \n\nTo demonstrate the effectiveness for such scenario, we supplement an experiment on finetuning a pretrained ResNet-34 on ImageNet under potential hardware noise. This task is on the ground that there can be hardware mismatch for deploying trained SNN models (Yang et al., 2022; [1]), and we may expect finetuning models directly on hardware to better deal with the problem. We simulate a kind of potential hardware noise by considering the stochastic neuron setting as described in Section 3.1, which may correspond to some hardware noise such as thermal noise. We consider finetuning a pretrained ResNet-34 model released by Xiao et al. (2022), which is trained under the deterministic setting (i.e., no noise) with the original test accuracy as 65.15%, under the noise setting with different levels of noises. All methods are trained for only 1 epoch and the results are below (where the noise parameter corresponds to the hyperparameter for the sigmoid function of the logistic noise, the larger the parameter the larger the noise scale).\n\n| Noise parameter | Direct test | DFA 1 epoch | FGT (S) 1 epoch | *BP 1 epoch* |\n| :----: | :----: | :----: | :----: | :----: |\n| 1/6 | 49.17 | 44.42 | **58.56** | *61.40* |\n| 1/8 | 58.51 | 58.34 | **62.14** | *63.05* |\n\nAs shown in the results, our proposed FGT (S) can effectively finetune such large-scale model, while previous biologically plausible methods with random feedback (DFA) totally fail. BP is not biologically plausible and its results are only for reference. FGT (S) has lower performance than BP under the condition of 1 epoch because it may have a slower convergence rate than BP (as studied in Section 5). Our proof-of-concept results mainly verify the effectiveness of our method for such scenarios and large-scale models. Future work can consider more SOTA models.\n\nWe do not overstate our contribution as we do not aim at outperforming models with SOTA performance, but propose a new biologically more plausible training algorithm and show that it can achieve similarly promising results as BP which is adopted by training those models. Our experiments also mainly make fair comparisons with BP under the same settings and show the effectiveness of our method with better neuromorphic properties, without overstating superiority over SOTA models. \n\nWe have added the above discussion and experiments, and cited your provided valuable references in the revised paper.\n\n2. About the method\u2019s superiority. \n\nOur method is superior to BP mainly because it tackles BP\u2019s problems of weight transport and separate phases for error signal propagation as it only requires unidirectional forward propagation across layers, conforming to properties of neuromorphic computing. At the same time, our method can achieve comparable performance as BP, largely outperforming previous biologically plausible methods with random feedback. In short, the advantage lies in promising performance with better properties, paving paths to biologically plausible on-chip training of SNNs."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365129205,
                "cdate": 1700365129205,
                "tmdate": 1700365129205,
                "mdate": 1700365129205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "01MB9VZpP0",
            "forum": "yBP36xQhZl",
            "replyto": "yBP36xQhZl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_Azqo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4363/Reviewer_Azqo"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed forward gradient training (FGT) for spiking neural networks to perform biologically plausible and hardware friendly supervised learning. The key objective is how to deliver error signals at the last layer without backpropagating through previous layers. The authors developed forward surrogate gradient to address the non-differentiable functions of SNN and empirically demonstrated promising accuracy compared to BP and DFA. Also, FGT combined with local learning (LL), which optimizes the local loss estimated from local readout layers, shows more improved accuracy. Experiments are performed in various benchmark datasets, such as N-MNIST, DVS-Gesture, CIFAR-10, and CIFAR-100."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation of the authors is reasonable given many prior works on supervised SNN are still relying on backpropagation. Also, the biological plausibility and hardware compatibility are reasonable motivation to some extent to explore other supervised learning methods. \n\n- The differences between FGT, backpropagation (BP), and direct feedback alignment (DFA) are well described in the figure and mathematically formulated. The paper is generally well written and provides enough information to understand the proposed approach with detailed appendix. \n\n- FGT and LL couples the global and local supervised learning in SNN empirically showing good results. Table 3 and Figure 3 comprehensively summarize the performance of FGT and LL in the various datasets and comparison methods."
                },
                "weaknesses": {
                    "value": "- The main concern is the novelty of the proposed method. The key contribution will be the section 4.1, where the authors present forward surrogate gradient to make forward gradient applicable to SNN. However, forward gradient, auto differentiation, and local learning are still existing concepts in the prior works.\n\n- Despite the repeated emphasis on hardware compatibility and importance of on-chip training, the relevant experimental results are not presented. Appendix E discusses the training costs, but FGT is still worse than BP.\n\n- The discussion on why BP is biologically implausible is enough, but why FGT and LL are biologically plausible and what are not plausible are not discussed clearly."
                },
                "questions": {
                    "value": "- The higher training cost of FGT and LL makes sense to some extent as CPU/GPU are not SNN-friendly architectures. However, SNN accelerators are still being developed, and it is difficult say there is a standard architecture for neuromorphic computing. For example, if there are dedicated hardware accelerators for BP-based supervised SNN, BP will be more hardware compatible than FGT.\n\n- It is not easy to make connections to well-known biological or biologically-inspired learning rules such as Hebbian learning or spike-time dependent plasticity (STDP). If the layer-wise error propagation is the main difference with BP, the proposed method is loosely related to biological learning rules."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779818341,
            "cdate": 1698779818341,
            "tmdate": 1699636408264,
            "mdate": 1699636408264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZV9fFmiRSI",
                "forum": "yBP36xQhZl",
                "replyto": "01MB9VZpP0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Azqo (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We respond to your comments and questions as follows. The citations are the same as the paper.\n\n1. About novelty. \n\nActually, our key contribution is **Section 4.2** where we propose a novel method for forward gradients, i.e., momentum feedback connections, while Section 4.1 is only our minor contribution that generalize the concept of forward gradient to the surrogate gradient condition of SNNs. The important problem of forward gradients in previous works (Silver et al., 2022; Baydin et al., 2022; Ren et al., 2023) is that the vanilla forward gradient is a projection to a random direction, so it suffers from large variance and performs poorly, which is also verified in our experiments. To tackle the problem, in Section 4.2, we propose the novel forward gradient with momentum feedback connections, which can largely reduce the variance and significantly improve the performance, also with theoretical analysis. With the proposed method, our FGT can achieve a similar performance as BP, which is **not** achieved by all previous works on forward gradients. This is one of our key contributions. Additionally, another novel contribution is presented in **Section 4.4**, where we also propose variants of our method with more biologically plausible considerations (e.g., FGT (S), FGT (Q1, S)) and we conduct experiments to verify their effectiveness, which has not been considered by previous works. Our method is novel and different from existing forward gradient methods.\n\n2. About results considering hardware compatibility and training costs. \n\nCurrently, the development of neuromorphic hardware is immature, so most algorithms are simulated on common computational devices while considering properties of neuromorphic computing, without experiments on real neuromorphic chips. The compatibility is measured by fitness to neuromorphic properties (or biological plausibility) because neuromorphic hardware is designed following these properties for efficient event-driven and in-memory computation. Note that an important feature of neuromorphic computing is *unidirectional* synaptic connections for event-driven and *in-memory* computation to avoid frequent memory transport which accounts for the majority of energy costs in real hardware (the computation architecture is expected to be different from the commonly used hardware with von Neumann architecture such as CPU or GPU). This means that each synapse between two neurons only allows unidirectional communications, and its weight value is stored locally. This is the reason why BP can hardly be suitable for (future) neuromorphic hardware, because BP will require two different forward and backward connections between successive layers with exactly the same weight, which will need frequent memory exchange (and also two stages, etc.) that is not compatible to the basic thought of neuromorphic computing. Our proposed FGT (and variants), on the other hand, avoids BP\u2019s problems and is thus more biologically plausible and compatible with neuromorphic hardware.\n\nAs for the discussion on training costs in Appendix E, Table 6 is only a rough comparison on GPU and we have also discussed conditions on potential neuromorphic computing hardware in the text, where our FGT (S) is better than BP (if possible) considering neuromorphic computing. We also supplement a comparison table (Table 5 in the revised version, also shown below) for clearer presentation of this. On GPU, the training costs of FGT (or FGT(S)) is (slightly) larger than BP because GPU does not follow neuromorphic architectures and we do not optimize low-level codes of our method as BP in the established PyTorch libraries (the results are only brief comparisons). It can also be further improved, e.g., with parallelization of computation for forward gradients. \n\nMeanwhile, our developed variant FGT (S) is more practical in real applications and also has about the same training costs and time as BP on GPUs, as shown in Table 6 in Appendix E.1.\n\n(continued below)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364952982,
                "cdate": 1700364952982,
                "tmdate": 1700364952982,
                "mdate": 1700364952982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l786ZJUF4w",
                "forum": "yBP36xQhZl",
                "replyto": "01MB9VZpP0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Azqo (Part 2/2)"
                    },
                    "comment": {
                        "value": "(continued from the preceding paragraph)\n\nConsidering neuromorphic computing, FGT (S) can be better and we quote our discussion and comparison table in Appendix E.1 here: \n\u201c*for memory costs, \u2026 On the other hand, if we consider neuromorphic computing with unidirectional synapses, BP (if possible) should maintain all backward synapses between successive layers, while FGT, FGT (S), and DFA keep direct feedback connections from the top layer, which can be much smaller than BP since middle layers usually have much more neurons. As shown in Table 5, the memory costs of DFA and FGT (S) can be much smaller than BP*\u201d, \u201c*for operation numbers, BP requires a backward propagation across $N$ layers, DFA requires direct feedback to $N$ layers, FGT requires $N$ forward propagation of forward gradients for different layers (across $N, N\u22121, ..., 1$ layers) and direct feedback to $N$ layers, and FGT (S) requires a forward propagation across average $N/2$ layers and direct feedback to $N$ layers which can be more practical than FGT. Direct feedback to $N$ layers may need fewer operations than BP across $N$ layers as middle layers usually have more neurons (for example, propagation from 400 to 400 neurons requires much more operations than from 10 to 400 neurons). So with a rough estimation, DFA needs much fewer operations than BP, and FGT (S) needs fewer operations than BP, as shown in Table 5*\u201d.\n\n| Method | Memory | Operations |\n| :----: | :----: | :----: |\n| BP (if possible) | $O((N-1)n^2+mn)$ | $O((N-1)n^2+mn)$ |\n| DFA | $O(Nmn)$ | $O(Nmn)$ |\n| FGT (S) | $O(Nmn)$ | $O(\\frac{1}{2}(N-1)n^2+mn+Nmn)$ |\n\nThe above table (Table 5 in Appendix E.1) is an estimation of training costs on potential neuromorphic hardware. For illustration, we consider neural networks with $N$ hidden layers with $n$ neurons for each layer and $m$ neurons for the output layer, where $m \\ll n$. The costs mainly focus on additional synaptic costs besides the normal forward procedure. Our method FGT (S) is expected to be better than BP considering costs, and achieving much better performance than DFA.\n\nAdditionally, we also would like to emphasize that our method mainly focuses on developing neuromorphic-friendly algorithms rather than only considering efficiency on GPU. We realize our targets, i.e., more neuromorphic properties and a similar level of performance as BP, with comparable costs, which could be acceptable.\n\n3. About why FGT and LL are plausible. \n\nFGT tackles BP\u2019s problems of weight transport and separate phases for error signal propagation as FGT only requires unidirectional forward propagation across layers, conforming to properties of neuromorphic computing. This makes FGT more biologically plausible. LL is also more biologically plausible than BP because it is local learning without signal backpropagation across layers, avoiding the above problems of BP. \n\nConsidering what is not plausible, the original FGT requires propagating real-valued forward gradients for all layers, which may be hard to realize. So in Section 4.4, we have further discussed how the signal propagation of forward gradient can be made plausible with bursts and random sampling of layers, proposing the variants of FGT (e.g., FGT (S), FGT (Q1, S)) which are also verified in experiments.\n\n4. About whether BP can be more hardware compatible than FGT. \n\nWhile it is true that neuromorphic hardware is still being developed, BP is not a good choice because it is not compatible with the basic thought of neuromorphic computing and can have larger costs even if it is supported (e.g., frequent memory exchange, more synapses considering forward and backward between all successive layers, etc.), as discussed in the above response to the second point. Our FGT (and variants) is not specific to certain hardware, but follows the basic properties of neuromorphic computing that will guide the development of hardware, which may be more suitable. We believe that developing biologically plausible training methods is important for SNNs.\n\n5. About connection to biologically-inspired learning rules. \n\nIn Section 4.3, we have discussed the connection to three-factor Hebbian learning. Our method is based on previous online training methods of SNNs, which is connected to the three-factor rule, while our method further answers how the global modulator can be determined \u2013 in our method, it is directly propagated from the top layer, corresponding to a more plausible neuro-modulation than previous methods that should be backpropagated across layers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365020930,
                "cdate": 1700365020930,
                "tmdate": 1700365434447,
                "mdate": 1700365434447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]