[
    {
        "title": "Tool-Augmented Reward Modeling"
    },
    {
        "review": {
            "id": "Rr8S0Vp25s",
            "forum": "d94x0gWTUX",
            "replyto": "d94x0gWTUX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_zDVk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_zDVk"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how to augment reward models with tools and proposes a new training framework, THEMIS, to do so. The Themis approach trains the model to select the correct tool for a given prompt and completion and reason about how the tools used should impact the assigned preference. In addition, the authors provide a tool-related RM dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "**Strength 1**: The idea of augmenting reward models with tools is very interesting, novel, and timely.\n\n**Strength 2**: The proposed method provides a nice and logical way for tools to be included in the reward design process.\n\n**Strength 3**: This paper provides some interesting experiments such as application to RLHF and scaling experiments."
                },
                "weaknesses": {
                    "value": "**Weakness 1**: One of my main concerns is lack of experiments on standard reward modeling datasets. There are many datasets not included in the paper such as the Anthropic HH dataset, Stack Overflow, OpenAI WebGPT, and ChatGPT comparisons datasets. They do conduct analysis on a small portion of the HH dataset, but not on the provided testing set. In addition, they show worst test accuracy than is reported in some other papers that only use conventional reward modeling [1]. Since the main claim of the paper is that by using tools they can improve the accuracy of reward models, I think their method should be validated on these popular datasets.\n\n[1] Dong, Hanze, et al. \"Raft: Reward ranked finetuning for generative foundation model alignment.\" arXiv preprint arXiv:2304.06767 (2023).\n\n**Weakness 2**: Little Hyperparameter study. THEMIS introduces various hyperparameters, but the sensitivity of model performance to these hyperparameters is not discussed.\n\n**Weakness 3**: This paper does not discuss a significant limitation of this method: the difficulty of creating the dataset. The dataset creation process consists of various complex steps, involves tool selection and design of heuristics. This seems to be difficult to scale to large scale preference datasets.\n\n\n**Weakness 4**: I think that this paper could use a more in depth discussion of related works. In particular, various works have attempted to use similar tools such as a compiler in the reward design process [2,3,4,5] and [4] use it to guide the reward model training. Discussing these works could help better frame the contribution of this work.\n\n[2] Le, Hung, et al. \"Coderl: Mastering code generation through pretrained models and deep reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 21314-21328.\n\n[3] Shen, Bo, et al. \"Pangu-coder2: Boosting large language models for code with ranking feedback.\" arXiv preprint arXiv:2307.14936 (2023).\n\n[4] Bukharin, Alexander, et al. \"Deep Reinforcement Learning from Hierarchical Weak Preference Feedback.\" arXiv preprint arXiv:2309.02632 (2023).\n\n[5] Shojaee, Parshin, et al. \"Execution-based code generation using deep reinforcement learning.\" arXiv preprint arXiv:2301.13816 (2023)."
                },
                "questions": {
                    "value": "How exactly do you get the scalar reward from the reward model? Is this done with a separate output layer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7704/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7704/Reviewer_zDVk",
                        "ICLR.cc/2024/Conference/Submission7704/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698553357103,
            "cdate": 1698553357103,
            "tmdate": 1700583145554,
            "mdate": 1700583145554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BaRo9XmQgX",
                "forum": "d94x0gWTUX",
                "replyto": "Rr8S0Vp25s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zDVk"
                    },
                    "comment": {
                        "value": "Dear Reviewer zDVk,\n\nMany thanks for your detailed and constructive comments.\n\n**Response to weaknesses**:\n\n**For W1**: Thank you for your valuable feedback. In response to your concern, we have conducted experiments on two standard reward datasets: WebGPT Comparison and HH-RLHF. The results are as follows: We partitioned the WebGPT Comparison dataset into 13.6K training samples and 2.4K test samples. Additionally, we randomly extracted 50K samples from HH-RLHF as training samples, combining them with our TARA dataset. The results reveal that our Themis outperforms other vanilla reward models, with the exception of RAFT (LLaMA-7B). It's noteworthy that RAFT (LLaMA-7B) performs SFT on the 112K positive training samples of the HH-RLHF dataset and then executes reward modeling training on 100K pairwise samples.\nIn contrast, Themis does not perform SFT and is exclusively trained on 50K samples from HH-RLHF (vs. RAFT trained on 100k samples on RMs as well as SFT on 112K samples) in a multi-task learning setting (WebGPT Comparison + HH-RLHF + TARA), achieving comparable performance with RAFT (LLaMA-7B). We believe these additional experiments address your concerns and provide valuable insights into the effectiveness of our approach on standard reward modeling datasets. Thank you for bringing this to our attention, and we appreciate your thoughtful review.\n\n| Model | WebGPT Comparisons | HH-RLHF |\n| --- | --- | --- |\n| Deberta [1] | - | 69.25 |\n| GPT-Neo-2.7B [2] | - | 68.27 |\n| RAFT (LLaMA-7B) [2] | - | **79.52** |\n| RM (Bert-Large) | 66.57 | 64.20 |\n| RM (Vicuna-7B) | 66.68 | 72.34 |\n| Themis (Vicuna-7B) | **69.53** | 77.59 |\n\n**For W2**: There are two types of hyper-parameters in our work: training configurations ($\\alpha, \\beta, \\omega \\in \\{0, 1\\}$ in Sec 2.2) and experiment hyper-parameters (such as batch size and learning rate). The hyper-parameters $\\alpha, \\beta, \\omega$ correspond to different training settings as shown in the following:\nWe have provided a comprehensive list of experiment hyperparameters and training details in Appendix C.1, specifically in Table 8 and Table 9 of the submitted version. However, it's crucial to acknowledge that we did not conduct an extensive hyperparameter tuning (especially for batch sizes) in this study due to limitations in our training resources. To maximize resource utilization, we opted for the largest feasible batch size and set the learning rate to 1e-5. We appreciate your observation and acknowledge the potential value of a more extensive hyperparameter study, which could be explored in future work with additional resources.\n\n| Model | $\\alpha$ | $\\beta$ | $\\omega$ |\n| --- | --- | --- | --- |\n| RM (Bert-Large) | 0 | 0 | 0 |\n| RM (Vicuna-7B) | 0 | 0 | 0 |\n| Themis | 1 | 1 | 1 |\n| &nbsp;&nbsp;&nbsp; w/o $L_\\text{Observation}$ | 1 | 0 | 1 |\n| &nbsp;&nbsp;&nbsp; w/o $L_\\text{Rationale}$ | 1 | 1 | 0 |\n\n**For W3**: Thank you for your insightful comment. We appreciate your attention to the dataset creation challenges, and we have incorporated a discussion of these limitations in the Limitation section of our paper. Despite the inherent complexities, we have implemented measures to streamline the process and facilitate scaling to larger preference datasets.\nTo address scalability concerns, we have established an automatic pipeline for dataset construction involving multi-agent interactions using LLMs. Furthermore, we have integrated standard preference data (hh-rlhf) with our tool-augmented data during reward model training. The resulting model exhibits superior generalization performance compared to the vanilla reward model, as detailed in Section 4.4.\nWhile tool selection and heuristic design are integral to tool-augmented datasets, we have automated and accelerated these processes for scalability. By defining the tool invoke format, we can generate numerous instances through our automatic pipeline, contributing to efficient scaling. We hope this can answer your question.\nFurthermore, we will open-source our tool-augmented dataset and code to the research community. We believe this transparency will foster collaboration and further development in this area. Thank you for your valuable feedback, and we look forward to contributing to the broader research community.\n\n**For W4**: \nWe sincerely appreciate your valuable feedback. In response to your suggestion, we have enhanced the discussion on related works in Section 5 of the paper. The updated section now delves into the works you referenced, providing a more comprehensive context that helps better frame the contribution of our work in relation to existing literature. We hope this revision meets your expectations, and we thank you for bringing this to our attention.\n\nWe trust that the aforementioned clarification could address your concerns. We genuinely look forward to engaging in further discussion and appreciate your time and consideration. Thank you for valuable feedback!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409896203,
                "cdate": 1700409896203,
                "tmdate": 1700445989214,
                "mdate": 1700445989214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0dRNmwEjSC",
                "forum": "d94x0gWTUX",
                "replyto": "Rr8S0Vp25s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[1] https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2\n\n[2] Dong, Hanze, et al. \"Raft: Reward ranked finetuning for generative foundation model alignment.\" arXiv preprint arXiv:2304.06767 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409922284,
                "cdate": 1700409922284,
                "tmdate": 1700409922284,
                "mdate": 1700409922284,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BHVKYmmKlO",
                "forum": "d94x0gWTUX",
                "replyto": "0dRNmwEjSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_zDVk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_zDVk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed response."
                    },
                    "comment": {
                        "value": "Thank you for taking the time to run additional experiments and answering my questions. My concerns have largely been addressed. \n\nI just have one follow up question. For the WebGPT and HH datasets, is the reason for Themis's superior performance due to the use of the search tool?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583119294,
                "cdate": 1700583119294,
                "tmdate": 1700583119294,
                "mdate": 1700583119294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EGMjwPQq2p",
                "forum": "d94x0gWTUX",
                "replyto": "Rr8S0Vp25s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zDVk Follow-Up"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and for revisiting our experiments! The superior performance of Themis on the WebGPT and HH datasets can be attributed to two key factors:\n1. **Utilization of Search Tools**:\n    * Themis leverages the Wiki Search and Google Search tools, which prove highly beneficial for tasks associated with the WebGPT and HH-RLHF datasets. By tapping into these search tools, Themis gains access to additional pertinent information, enhancing its ability to comprehend and address the intricacies of the tasks.\n2. **Benefit of the Rationale Stage**:\n    * The incorporation of the Rationale stage in Themis prompts a Chain-of-Thought (CoT) approach to reward generation. During this stage, the tool-augmented reward model engages in information aggregation, synthesis, and self-reflection. This process empowers Themis to navigate through complex scenarios, providing a nuanced and contextually relevant basis for generating rewards. The Rationale stage plays a pivotal role in Themis's capacity to reason effectively and contribute to its superior performance on these datasets.\n\nWe value your continued engagement and hope that this response could adequately addresses your concerns. If you have any further questions or require additional clarification, please feel free to reach out."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643520094,
                "cdate": 1700643520094,
                "tmdate": 1700644675580,
                "mdate": 1700644675580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Drpkvb5ubb",
            "forum": "d94x0gWTUX",
            "replyto": "d94x0gWTUX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_wTHw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_wTHw"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests leveraging primitive tools in the reward model to enhance reward estimation for RLHF settings. The authors conducted extensive experiments to show that by having tool-grounded reward models, it is possible to boost the reward model accuracy and translate them to RLHF gains.\n\nUse of tools such as web search or calculator to improve the performance of an LLM is not particularly new. I understand that the current literature has mostly focused on inherent knowledge in the models at the alignment training.\n\n** updated score after reading the response and revisions"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Having a level of reasoning and interpretability is great feature to have for reward models\n- The experiments and provided implementation details look comprehensive"
                },
                "weaknesses": {
                    "value": "1- How to trust tools is an important aspect to consider here. At least in the examples, it looks like there is a risk of biasing the reward model and generative model to outputs of specific tools being used. This could be concerning as tools are not necessarily unbiased.\n\n2- It is not entirely clear how GPT-4 is used to generate RM training data. Note that GPT-4 itself is a system if the proposal is to use GPT-4 to train RM, one can argue why not directly train RM on GPT-4 data or use GPT-4 directly as reward model.\n\n3- When alpha in eq. 2 is set to zero, we converge to typical tool use via simple prompting right? the RM is still a pretrained model and can be prompted to use tools even without explicit loss terms on tool use. Is this understanding correct?\n\n4- Model size of 7B is quite small to capture knowledge to compete with the tools used in this paper. I think a more realistic setup would be to take a larger model and the gap between say Wiki tool and that result could look very different.\n\n5- I think the write-up could improve, especially for Sec 3.1, I had difficulty understanding exactly how GPT-4 was used and how training data was prepared"
                },
                "questions": {
                    "value": "(see above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No particular concern. Added one related point in my review, looking forward to hearing from authors."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7704/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7704/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7704/Reviewer_wTHw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688195340,
            "cdate": 1698688195340,
            "tmdate": 1700584057961,
            "mdate": 1700584057961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sL9fRE6B5x",
                "forum": "d94x0gWTUX",
                "replyto": "Drpkvb5ubb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wTHw (Part 1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer wTHw,\n\nMany thanks for your detailed and constructive comments.\n\n**Response to weaknesses**:\n\n**For W1**. Your concern about the potential bias introduced by the tools is indeed crucial.  As you said, the tools may produce biased aspects or wrong aspects here, such as invalid tool invocations, incorrect tool-invoked results, biased content, and unexpected errors such as network failure. To avoid the reward model interference by these biases, we have implemented several measures:\n1. **Strict Quality Control**: We rigorously control the quality of the training set, filtering out invalid tool invocations generated by GPT-4 to ensure the reliability of the training data.\n2. **Noise Injection**: Recognizing the possibility of tools not always providing accurate outputs, we intentionally introduce \"noise\" instances in the training data. This strategy helps train the reward model to produce correct rewards even in scenarios where the tools may not work optimally. An example is shown in the following. Our Themis model encounters an error during the invocation of the *Wiki Search tool* and no results are returned. Importantly, our model demonstrates a capacity for reasoning based on its internal understanding, unaffected by the absence of tool output, which highlights the ability of our model to engage in self-reflection and form independent judgments when faced with unexpected tool behavior. \n\n    > Question: What do you call the chinese writing system?\n    >\n    > Answer: Standard Mandarin\n    >\n    > **Thought**: I need to invoke the Wiki Search tool to search Chinese writing system\n    >\n    > **Action**: Wiki Search\n    >\n    > **Action Input**: Chinese writing system\n    >\n    > **Observation**: An error occurred during the tool invoke, so no result was returned.\n    >\n    > **Rationale**: The answer provided is \"Standard Mandarin,\" which is incorrect in the context of the question. The question is asking about the Chinese writing system, and \"Standard Mandarin\" refers to a spoken language, not a writing system. The tool's execution, however, resulted in an error, and no relevant information was retrieved. Based on this, the answer is already incorrect, and the tool's failure to provide information doesn't impact the correctness of the answer.\n\n3. **Rationale Generation**: A key aspect of our Themis approach is the Rationale generation stage. Here, the tool-augmented reward model aggregates and synthesizes previously acquired information and undergoes a reasonable reasoning process, which involves RM's self-reflection instead of listening to the results of the tool.  The emphasis on rationale generation encourages Themis to form reasoned decisions, relying on a process of synthesis and self-reflection rather than solely relying on the output of the tools. This approach enhances the model's ability to reason independently and avoid undue influence from tool biases. This is an example in which the model generates incorrect tool invocations, leading to biased observations. However, it is crucial to note that our model identifies the errors in tool results through the Rationale module. Importantly, even in the presence of biased information, our model maintains its capacity to provide reasonable answers, showcasing its robustness and ability to navigate through biases introduced by tool invocations.\n\n    > Question: Why is the flight/fight our defence mechanism when remaining calm and composed may serve better outcomes?\n    >\n    > Answer: The fight or flight response is an evolutionary response that happens in stressful situations. It can be helpful in situations where there is a perceived threat to survival. The fight or flight response is characterized by feeling bodily sensations of stress, such as an increased heart rate and faster breathing. This readies the body to either fight the danger or flee from it. The body\u2019s sympathetic nervous system is responsible for this response, which triggers the release of adrenaline and noradrenaline...\n    >\n    > **Thought**: The information given in the answer is well-established knowledge within the field of psychology and does not pertain to a current event. Therefore, I don't think I need to use a tool to verify this.\n    > \n    > **Action**: Google Search\n    >\n    > **Action Input**: None\n    >\n    > **Observation**: 1 of 4. pronoun \u00b7 \u02c8n\u0259n. singular or plural in construction. Synonyms of none. 1. : not any. 2. : not one : nobody. 3...\n    > \n    > **Rationale**: Although the executor's tool results did not provide any specific results or contradictions, the question asked why remaining calm and composed may serve better outcomes as a defense mechanism compared to the flight/fight response. However, the answer only explained what the fight or flight response is and how it works. The question about the potential advantage of remaining calm and composed in stressful situations wasn't addressed. Thus, it's reasonable to reason that the answer is incorrect."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408965148,
                "cdate": 1700408965148,
                "tmdate": 1700409888289,
                "mdate": 1700409888289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0jWbgdNPiW",
                "forum": "d94x0gWTUX",
                "replyto": "Drpkvb5ubb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wTHw (Part 2/3)"
                    },
                    "comment": {
                        "value": "**For W2**. GPT-4 is a closed-source model that poses significant challenges in terms of cost and accessibility. Additionally, the practicality of conducting reward model training on GPT-4 is hindered by the necessity to add extra fully connected layers for reward prediction since OpenAI API did not support this feature. Given its high cost and limited availability for public use, employing GPT-4 directly as a reward model becomes impractical for wider applications. Therefore, we just utilize the strong in-context learning ability of GPT-4 and prompt GPT-4 to generate partial data, including tool-invoking processes (*Thought* and *Action* in Sec 2.2) and rationales (*Rationale*). Subsequently, we meticulously filter out invalid tool invocations and incorrect rationales to ensure the quality and reliability of the generated training data. \n\n**For W3**.  Your question is insightful, and we appreciate the opportunity to clarify. When alpha is set to zero, the model transforms into a vanilla reward model, specifically corresponding to RM (Vicuna-7B/Bert-Large) as indicated in Table 1 and Equation 1. In this configuration, the model generates a scalar reward solely through a fully connected layer, devoid of any tool-invoking or explanation-generating processes.\nWith alpha set to zero, it's crucial to note that the reward model cannot spontaneously produce a tool use process without explicit loss terms on tool use. While we can resort to in-context learning (ICL) by prompting the model with a few tool-use examples, it is important to highlight that this approach may still lag behind in finetuning on different tools (with $\\alpha=1$). Additionally, it is constrained by the maximum context length, as the contextual examples of ICL can consume significant length. \n\n**For W4**. In Sec 4.2 (Scaling Trends in Themis), we explored the correlation between the performance and the scale of models. To enhance the training efficiency and reduce the resource cost, we utilized the parameter-efficient fine-tuning method LoRA which only trains specific modules, as shown in Table 1. The results elucidated a positive correlation between the scale of the model and its overall performance, and highlighted a substantial gap in the *WikiSearch* tool, as you rightly pointed out. \nTaking your valuable advice into consideration, we have conducted full-parameter fine-tuning with Themis (Vicuna-33B), and reported the results as follows.  As illustrated in the table below, Themis with 33B demonstrates superior performance compared to its 7B counterparts, exhibiting an approximately 2% absolute improvement in preference prediction. Notably, the larger models showcase optimal capabilities in harnessing *Wiki Search*, *Google Search*, and *Translator* tools. This trend aligns with the findings from our LoRA experiments, which we have incorporated into Table 1. We value your thoughtful review and trust that these additional experimental results adequately address your concerns.\n| Model | Calendar | Calculator | Weather | Code | Translator | WikiSearch | Google Search | Multi | Avg. |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Themis (Vicuna-7B) | 100.00 | 98.05 | 100.00 | 99.47 | 90.91 | 93.31 | 64.92 | 99.31 | 93.31 |\n| Themis (Vicuna-33B) | 100.00 | 97.40 | 100.00 | 99.47 | 93.54 | 96.55 | 73.72 | 99.31 | 95.21 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409006522,
                "cdate": 1700409006522,
                "tmdate": 1700409528807,
                "mdate": 1700409528807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3kR6Lf7qXS",
                "forum": "d94x0gWTUX",
                "replyto": "Drpkvb5ubb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wTHw (Part 3/3)"
                    },
                    "comment": {
                        "value": "**For W5**. Sorry for the confusion about the data collection steps. Due to the page limits, we have revised Section 3.1 and added detailed prompts for each agent in Appendix B in the submitted version for a more comprehensive understanding. Furthermore, we have also revised Section 3.1 in the draft version, hope this can address some of your concerns. We simplify the dataset construction steps as follows:\n* Step 1: Initially, we collect a reward dataset featuring each instance comprising a question, a positive answer, and a negative answer. To construct this dataset, we employ two distinct approaches: resume open-source, high-quality datasets, and generation from scratch with some heuristic methods. However, the above methods usually only produce positive answers. To address this concern, we leverage a GPT-4 as a negative generation agent to generate antagonistic negative answers, which will be described in Step 3.\n* Step 2: Subsequently, we define the name, and API calls of some tools including *Calculator, Code Interpreter, Translator, Google, Weather, Calendar Search*, and *Wiki Search*.\n* Step 3: In this step, we leverage the capabilities of  GPT-4 to generate the tool invocation processes and negative samples. To achieve this, we design a simulated environment where GPT-4 plays three different agents: *the negative generation agent*, *the tool agent*, and *the rationale agent*. Taking the Calculator tool as an example, we extract some instances from GSM-8K. However, the data only contain questions and positive answers. To address this limitation, we prompt the negative generation agent to generate the negative answer, constructing the pairwise preference samples. Subsequently, we query the tool agent to generate the tool invocation process, encompassing the *Thought*, *Action*, and *Action Input* stages. Then humans execute the tools based on the generated tool information, yielding the *Observation*. Finally, we amalgamate the above information (inputted question-answer, tool invocation process, observations) and prompt the rationale agent to produce the *Rationale* stage. For a more comprehensive understanding of this process, additional details are available in Appendix B.2.\n* Step 4: We exclude tool-invoked processes acquired in Step 3 that exhibit invalid formats. Subsequently, we discard processes exceeding three interaction steps, lacking relevant function calls, or manifesting parsing errors in their execution results.\n\nWe hope the above can address some of your concerns.  And we sincerely look forward to further discussion. Thank you for your time and consideration!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409069115,
                "cdate": 1700409069115,
                "tmdate": 1700409543987,
                "mdate": 1700409543987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nKWSeewn4U",
                "forum": "d94x0gWTUX",
                "replyto": "3kR6Lf7qXS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_wTHw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_wTHw"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to Reviewer wTHw"
                    },
                    "comment": {
                        "value": "Thank you for the detailed responses and revisions. Based on the responses and new information presented in the paper, I'm updating the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583996068,
                "cdate": 1700583996068,
                "tmdate": 1700583996068,
                "mdate": 1700583996068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MrriWd9a1U",
                "forum": "d94x0gWTUX",
                "replyto": "Drpkvb5ubb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wTHw Follow-Up"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and the time you took to consider our responses and revisions! We appreciate the reviewer's insightful feedback, which contributes to the robustness and clarity of our research."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643586997,
                "cdate": 1700643586997,
                "tmdate": 1700645188618,
                "mdate": 1700645188618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NpKwNl7kbO",
            "forum": "d94x0gWTUX",
            "replyto": "d94x0gWTUX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_bKyt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7704/Reviewer_bKyt"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a tool-augmented preference modeling approach, named Themis, to address the limitations of conventional reward models (RMs) in aligning language models with human preferences. The approach integrates external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces. The paper presents experimental results demonstrating the effectiveness of Themis in enhancing the performance of RMs across various tasks. The authors also provide a comprehensive tool-augmented reward dataset (TARA) for further research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper addresses an important issue in reward modeling by introducing a tool-augmented approach to enhance the effectiveness of RMs.\n\n(2) The proposed methodology of integrating external tools into RMs is innovative and practical, allowing for dynamic decision-making and reasoning processes.\n\n(3) The experimental results demonstrate significant improvements in preference ranking and outperformance of Themis compared to baseline RMs, validating the effectiveness of the approach."
                },
                "weaknesses": {
                    "value": "The description of the method is not very clear. My understanding is that the reward model first generates some explanations based on the inputted question and answer, and then connects a fully connected layer to the final hidden state to produce a scalar reward."
                },
                "questions": {
                    "value": "1. Why is there a significant difference between the results of Themis (Vicuna-7B + LoRA) and Themis in table1?\n2. Does beta=0, w=0 in table1 mean RM (Vicuna-7B)?\n3. I understand that all the values in table2 are accuracies, so why is the binary classification accuracy lower than 50 for RM (Vicuna-7B, Zero-shot)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766303293,
            "cdate": 1698766303293,
            "tmdate": 1699636938510,
            "mdate": 1699636938510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h2V6gUp1E2",
                "forum": "d94x0gWTUX",
                "replyto": "NpKwNl7kbO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bKyt"
                    },
                    "comment": {
                        "value": "Dear Reviewer bKyt,\n\nMany thanks for your detailed and constructive comments. \n\n**Response to Weaknesses**:\n\nYou accurately captured the essence of our approach. Our reward model initiates tool-invoking processes and generates explanations based on the inputted question and answer. Subsequently, we append an ``<eos>`` token, and the scalar reward is produced by a fully connected layer using the last hidden states corresponding to the ``<eos>`` token. To enhance clarity, we have revised the method description in Section 3, providing more details. We trust that these modifications improve the overall clarity of our method description.\n\n**Response to Questions**:\n\n**For Q1**: In Themis (Vicuna-7B + LoRA), we adopt a strategy where specific parameter-efficient modules (using **LoRA**) are trained, while the **pretrained weights of Vicuna are frozen**. This approach significantly reduces training resource costs but may lead to a performance decrease compared to the full-parameter fine-tuning setting.\n\n**For Q2**: RM (Vicuna-7B) represents a **vanilla reward model**. In this configuration, the model generates a **scalar reward solely through a fully connected layer**, without engaging in any tool-invoking or explanation-generating processes. Therefore, in the context of our methodology, RM (Vicuna-7B) aligns with the setting where $\u03b1=0$. \n\n**For Q3**: The observed lower performance in the **zero-shot setting** is primarily attributed to the fact that it was evaluated **without training on HH-RLHF datasets**. In contrast, the **fine-tuning** results demonstrate performance **exceeding 50%**. More specifically, the lower accuracy for RM (Vicuna-7B, Zero-shot) is a result of the specific design choices made in the RM model, particularly in the zero-shot setting. The evaluation on the HH-RLHF* dataset in the zero-shot scenario revealed unsatisfying performance, as indicated by an accuracy below 50%) As outlined in response to a previous question (Q2), RM (Vicuna-7B) employs a scalar reward generated solely through a fully connected layer. This approach, while chosen for its simplicity and efficiency, has inherent limitations, especially in the context of a shallow and weak signal. The model relies on the input question and answer, which may result in overfitting to training samples, particularly when evaluating with the complexities of the hh-rlhf* dataset in the zero-shot setting. In contrast, the proposed method outperforms vanilla RMs in both zero-shot and fine-tuning settings (as shown in Table 2). Moreover, it's essential to note that the task is not a traditional binary classification problem. RM (Vicuna-7B, Zero-shot) is designed to predict the preference score for both negative and positive pairs, then compute the correctness (acuracy) of the ranking order. We hope our response can address your questions.\n\nWe hope the above can address your concerns.  We sincerely appreciate your time and thoughtful consideration. We look forward to any further discussion and appreciate your ongoing engagement with our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407645305,
                "cdate": 1700407645305,
                "tmdate": 1700408663345,
                "mdate": 1700408663345,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TPOMS7uFMD",
                "forum": "d94x0gWTUX",
                "replyto": "h2V6gUp1E2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_bKyt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Reviewer_bKyt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I sincerely appreciate your time in improving the overall clarity of your method.\n\nFor Q3, If you output a random score for both negative and positive pairs, the probability that the score of a positive pair is higher than that of a negative pair is 50%. I'm still struggling to understand why the accuracy for RM using Vicuna-7B in a zero-shot setting is lower than 50%.\n\nI think another baseline could be considered. When using GPT-4 for labeling, we could also allow GPT to use the tool to provide intermediate results, engage in a chain of thought (COT), do the reasoning, and then give the final score. What advantages and disadvantages do you think your method has compared to GPT-4?\n\nDespite the questions raised by other reviewers, I believe the motivation of this work is commendable."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578004075,
                "cdate": 1700578004075,
                "tmdate": 1700578004075,
                "mdate": 1700578004075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "whcNTbzZEU",
                "forum": "d94x0gWTUX",
                "replyto": "NpKwNl7kbO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bKyt Follow-Up (Part 1/2)"
                    },
                    "comment": {
                        "value": "**Re: Q3 Follow-Up**. We appreciate the reviewer's continued engagement and would like to address the additional concern regarding the accuracy of RM using Vicuna-7B in a zero-shot setting.\n\nThe observed lower performance is **primarily due to the challenges associated with domain generalization in vanilla RM**. Specifically, the **RM (Vicuna-7B) trained on tool-related data** using the vanilla RM objective (without the ability of tool use) may encounter **overfitting to the domain-specific data** (including *Calendar*, *Calculator*, *Weather*, *Code*, *Translator*, *Wikipedia*, and *Google Search*). Consequently, it achieves less than 50% accuracy (worse than random) on the HH dataset concerning human alignment, highlighting the vulnerability of vanilla RM to overfitting within the in-domain data. On the other hand, **Themis, which is also trained on tool-related data, attains a 55% accuracy** on the HH dataset (human alignment domain) in the zero-shot setting. This **underscores the exceptional out-of-domain generalization** in preference prediction, credited to the effective integration of tool use.\n\nAdditionally, we would like to emphasize two key factors influencing zero-shot performance:\n1. **Dataset Distribution and Domain Differences**: It is crucial to emphasize the substantial disparity in data distribution between our dataset, TARA, which is tool-related, and the HH-RLHF dataset, which focuses on human alignment with helpful and harmless content. The distinct nature of these datasets introduces challenges in achieving optimal zero-shot performance. The inherent differences in domain and data characteristics contribute to the observed lower accuracy in the zero-shot setting.\n2. **Impact of Model Parameter Size**: The size of the model plays a significant role in zero-shot performance. Larger models, such as RM(Vicuna-7B), may exhibit a susceptibility to overfitting (easy to converge), as evidenced by the rapid convergence in just one training epoch (See Figure 3 Left). In contrast, a smaller model like RM (Bert-Large) requires more epochs to fit the reward data. The phenomenon of overfitting in a large model could explain the lower accuracy in the zero-shot setting, where the model may struggle to generalize effectively to unseen datasets. It's noteworthy that RM (Vicuna-7B) shows improved performance by +30% when finetuned on a smaller set of HH-RLHF instances (500), demonstrating the adaptability of the large model to relevant (in-domain) training data.\n\nTo sum up, the challenges observed in zero-shot accuracy can be attributed to the significant differences in dataset distribution and the impact of model parameter size. We are committed to providing a more comprehensive understanding of these challenges and implementing necessary refinements to address them in the final version of our work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643323045,
                "cdate": 1700643323045,
                "tmdate": 1700658545671,
                "mdate": 1700658545671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JahOrqCEJ1",
                "forum": "d94x0gWTUX",
                "replyto": "NpKwNl7kbO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bKyt Follow-Up (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Re: GPT-4 as the RM Baseline**. Thank you for your valuable suggestion. We appreciate the insightful idea of incorporating a ChatGPT or GPT-4 baseline in future experiments. Below, we outline the advantages and disadvantages of our proposed Themis model in contrast to GPT-4:\n\n|  | **RM (Themis)** | **RM (GPT-4)** |\n|---|---|---|\n| **Advantage** | (1) Ease of Application and Fine-tuning;  (2) Domain Specialization and Prompt-free Usage;  (3) Better Training/Inference Stability;  (4) Consistent Reward Format;  (5) Cost-effectiveness.  | (1) Larger model and strong capability;  (2) Strong generalization ability with prompt engineering.  |\n| **Disadvantage** | (1) Need data to fine-tune. | (1) High cost in time and money;  (2) Need prompt engineering;  (3) Unsupported RM fine-tuning;  (4) Closed-source. |\n\n\n\n**Advantages:**\n1. **Ease of Application and Fine-tuning**: \n      * **Themis**: Can be easily applied and fine-tuned, making it compatible with traditional reward models.\n      * **GPT-4**: A closed-source model currently not supported by fine-tuning in the reward modeling context. It lacks the capability to add an additional fully-connected layer and conduct contrastive reward modeling training.\n2. **Domain Specialization and Prompt-free Usage**: \n      * **Themis**: Facilitates easy domain specialization through fine-tuning, allowing adaptation to specific contexts. It also excels in prompt-free usage, automatically determining when and how to call pre-defined tools during inference.\n      * **GPT-4**: Requires complex prompt design and execution in each stage for domain-specific tasks. In particular, it relies on in-context learning ability and intricate prompt design for generating various stages, such as tool invocation and rationale stages. Importantly, prompt designs may need adjustment with GPT-4 updates.\n3. **Better Training/Inference Stability**: \n      * **Themis**: Offers stability, avoiding recent downtimes associated with GPT-4/ChatGPT services due to increased calling burden. Themis is designed to be a reliable choice for model training.\n      * **GPT-4**: Suffers from various factors during training, such as OpenAI API services, network failures, and other potential disruptions.\n4. **Consistent Reward Format**: \n      * **Themis**: Produces continuous scalar rewards through a fully connected layer, ensuring a consistent format.\n      * **GPT-4**: Generates rewards in natural language format, necessitating the formulation of rules to extract reward values from prompted results.\n5. **Cost-effectiveness:** \n      * **Themis**: Serves as a reusable model that is significantly more cost-effective than GPT-4, providing an economical alternative for reward modeling tasks. \n      * **GPT-4**: Requires extra cost for API calling, adding to the overall expenses.\n\n**Disadvantages:**\nOne notable disadvantage of Themis is the requirement for some data for fine-tuning. This contrasts with GPT-4, which can potentially leverage pre-existing knowledge without fine-tuning.\n\n\n\nWe highly appreciate your valuable input, and we are committed to integrating these insights into our upcoming experiments and the refined version of our work. Thank you for acknowledging the motivation behind our tool-enhanced reward models.\n\nWe hope these updates adequately address your concerns and kindly encourage you to reconsider our review score in light of these clarifications. Your contribution significantly enhances the quality of our research, and we thank you for your thoughtful feedback."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644302222,
                "cdate": 1700644302222,
                "tmdate": 1700658789119,
                "mdate": 1700658789119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]