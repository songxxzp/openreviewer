[
    {
        "title": "Multisensory Geospatial Models via Cross-Sensor Pretraining"
    },
    {
        "review": {
            "id": "7ojIL2vyun",
            "forum": "qWSk62REeK",
            "replyto": "qWSk62REeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_V45f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_V45f"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a pretraining methodology based on MAE for multi-sensor\ndata, specifically with RGB data, 14-channel Sentinel 2 data, 2-channel\nsynthetic aperture radar (SAR), and digital surface models (DSM).\n\nDifferent sensors are given different tokenizing encoder layers, and different\ndecoder layers, but all sensors share the same Swin backbone. \n\nMultiple downstream tasks (land-use classification, segmentation, cloud\nsegmentation, pan-sharpening) are evaluated.\n\nHeterogeneity of the data is mitigated with heterogeneous batches and\nper-sensor encoders.\n\nXGeoSet contains 2 million images and is a union of 4 other datasets.\n\nExperiments demonstrate performance increases on a range of downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The research questions are clearly laid out.\n\n* Evaluation is well thought out and comprehensive."
                },
                "weaknesses": {
                    "value": "* Evaluation results do not contain error bars, which makes it difficult to\n  compare pointwise results.\n\n* A large part of the paper is in the Appendix. This is a strength and a\n  weakness, the studies are comprehensive, but it makes scoping difficult in\n  the context of a conference."
                },
                "questions": {
                    "value": "> XGeoSet, encompassing over 2 million images.\n\nYou should state the size of the images (on average if they are heterogeneous).\n2 million 1024x1024 images is a lot different than 2 million 128x128 images.\n\n\n> Paired data in the abstract. \n\nThis could use clarification. I think this means you could have 2 or more\nsensor readings for the same spacetime location? \n\n> In Section 4: When specified, 1% BEN and 1% SEN12MS-CR are also employed for ablation studies. \n\nIt is unclear what 1% means in this context. Also BEN is undefined at this\npoint (it is defined later in section 4.1) but it should be defined on first\nusage.\n\n\n> The models are pre-trained for either 100 epochs for ablation studies or 800 epochs \n\nCan you be confident that trends wont change down the line?\n\n\n> Table 5: Distillation from other pretraining model vs pretraining from scatch\n\nCould the result that from-scratch works better be due to not spending as much time tuning the other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8376/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549514973,
            "cdate": 1698549514973,
            "tmdate": 1699637042209,
            "mdate": 1699637042209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qruuAS5A0q",
            "forum": "qWSk62REeK",
            "replyto": "qWSk62REeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_KKqz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_KKqz"
            ],
            "content": {
                "summary": {
                    "value": "The submitted manuscript introduced a method for \"cross-sensor\" pretraining of geospatial data. The underlying transformer architecture uses a shared encoder with a mixture-of-experts (MOE) setup to capture dependencies between different input sensors. Pretrianing is done as masked image modelling on the following modalities: RGB, Sentinel-2, Sentinel-1, and a Digital Surface model (DSM). Results demonstrate a minor performance gain for some downstream tasks and larger outperformance for some other downstream tasks as compared to baselines and SOTA.\n\nThis submission introduces indeed an interesting approach for the remote sensing community. I am not so sure about the ICLR community and its focus on methodological novelty. The method proposed uses known ML or computer vision methods applied on remote sensing data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The submission has the following strength:\n\n**#1.** easy to read and follow\n\n**#2.** is well-structured\n\n**#3.** present an novel combination of known approaches to the interesting domain of remote sensing data"
                },
                "weaknesses": {
                    "value": "The submission has the following weaknesses:\n\n**#1.** As already mentioned in the summary, this work presents little methodological novelty and rather applies known methods from ML/CV to the remote sensing domain. There is nothing wrong with this, but I think that this work would be a much better match for a remote sensing journal than ICLR.\n\n**#2.** This work claims \"to our best knowledge, it is the first multi-sensor geospatial pretraining of such kind\" i.e., able to deal with paired and unpaired data. Then it reports related work such as Liu et al. (2022a), Chen & Bruzzone (2022) and Scheibenreif et al. (2022) doing pretrained with paired data from Sentinel-2 and Sentinel-1. Why is there no comparison with this related work presented (pretrained models of these works are available)? As far as it goes for \"unpaired\" data, I can not see any ablation study showing the method's capability to outperform on \"unpaired\" data. I am asking because the overall motivation of this work is given by Fig. 1. which is showing \"paired\" image modalities. Maybe I missed it and as far as I understand \"Section 4.2 COMPARISON WITH SINGLE SENSOR PRETRAINING\", the authors compare results on paired pretraining, not explicitly pretraining on \"unpaired\" data. Again, in Section 4.2, I would love to see comparisons against the related work mentioned above.\n\n**#3.** I have difficulties understanding the expression: \"...we have made discoveries, yet to be reported...initiating pretraining from scratch has been observed to yield superior results compared to leveraging existing foundational models\". I am not sure what the authors mean by this? Do they compare to pretrianing from other geospatial models, pretraining from ImageNet or just training from scratch? \n\n**#4.** It would be great to see how this method is able to perform beyond remote sensing data. The approach might be applicable on other domains where multiple modalities are present such as e.g., medical images or other sensor fusion setups.\n\n**#5.** Unfortunately, the reported performance of the presented method only marginally outperforms GFM (Mendieta et al., 2023). Looking at Table 3, it would be great to see GFM performance at the \"SEN12MS-CR\" downstream to better compare both models."
                },
                "questions": {
                    "value": "I have the following question:\n\n**#1.** Is there any ablation study showing the performance of the proposed model for \"unpaired\" data sources? Maybe the authors know a setup, which could demonstrate the proposed methods capability to work on \"unpaired\" data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8376/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8376/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8376/Reviewer_KKqz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8376/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689003903,
            "cdate": 1698689003903,
            "tmdate": 1699637042086,
            "mdate": 1699637042086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "sddYAS3qzN",
            "forum": "qWSk62REeK",
            "replyto": "qWSk62REeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_Tpx5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_Tpx5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a self-supervised model for multi-modal remote sensing images. Based on masking strategy and cross-modality reconstructions, this foundation model is then used for different benchmark tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written, the results seems good on the various downstream tasks, and the model architecture and training is relevant. This foundation model could be useful for the community."
                },
                "weaknesses": {
                    "value": "It is not clearly indicated whether the codes and the pre-trained models will be accessible after publication or not.\n\nI would need a clarification. While the model seems to be able to process up to 4 modalities, it looks as if only 2 are simultaneously used, either during pre-training or during downstream tasks. Would it be significantly different to just have 2 different models with each 2 modalities? Was it not possible to find a dataset or task using 3 modalities?"
                },
                "questions": {
                    "value": "All images seems to be aligned (co-registered), yet for example SAR images are initially in radar geometry and the projection leads to data loss and artifacts.  Do you think it would be possible to use (I mean re-train) your model even if the images are in different geometries?\n\nWhy 8 experts? \n\nPlease explain better how to go from pre-training to downstream tasks: what are the pieces that are used, what is added, what is learned, etc\n\nSection 3.2: How does the 'either' (reconstructing itself or its paired sensor) is implemented in practice? is it random? or always both are performed?\n\nTypos/smaller remarks:\n'geospaital'\ndefine MoE in part 1\n'methodology. .'\n'or' --> random? or both?\n'Section ??'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8376/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795098560,
            "cdate": 1698795098560,
            "tmdate": 1699637041970,
            "mdate": 1699637041970,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "rb2L3KLMGU",
            "forum": "qWSk62REeK",
            "replyto": "qWSk62REeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_2AtA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8376/Reviewer_2AtA"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the problem of pretraining computer vision models for multimodal remote sensing data. The proposed method is based on masked autoencoders (MAEs). The basic idea is to perform cross-modal reconstruction (with some probability, when paired modalities are available) and traditional input reconstruction otherwise. This allows for pretraining on heterogeneous datasets which may include some locations that are imaged with multiple modalities and other locations which are only imaged with a single modality. All modalities have separate patch embedding layers and decoders, while the encoder is shared. The pretrained representations are evaluated on a number of downstream geospatial tasks. \n\n# References\n\n@inproceedings{jean2019tile2vec,\n  title={Tile2vec: Unsupervised representation learning for spatially distributed data},\n  author={Jean, Neal and Wang, Sherrie and Samar, Anshul and Azzari, George and Lobell, David and Ermon, Stefano},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={33},\n  number={01},\n  pages={3967--3974},\n  year={2019}\n}"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper considers an important question.\n* Integrating information from multiple aligned modalities is a reasonable thing to do, and the proposed approach is intuitive and sensible. Broadly, I like the proposed approach. \n* The proposed method is evaluated on a number of reasonable remote sensing domain tasks. \n* The paper includes sensible ablation studies. \n* The paper claims to be the first self-supervised method in the geospatial domain that can handle paired and unpaired multimodal data. Based on a quick search, I didn't find obvious counterexamples. Perhaps someone with more domain experience can weigh in. \n* Full hyperparameters are specified in the appendix. \n* Qualitative results included in appendix."
                },
                "weaknesses": {
                    "value": "# Experiment issues\n* As the paper points out, the results in Table 3 are \"replicated from previous reports\" and they are based on a variety of backbone architectures. This makes it very difficult to draw any conclusions about the proposed method from Table 3. \n* Based on section 3.3, it sounds like pretraining datasets were chosen to maximize the performance of the proposed method. This does not seem to be the makings of a fair comparison...\n* The stability of the method/results under retraining is not clear. \n* The results in Figure 3 seem like they would be confounded by dataset size. The advantage of SAR seems slight - couldn't it just be a regularization effect? \n* How were hyperparameters tuned? \n* All of the datasets in this paper vary in resolution, and this doesn't seem to be controlled for anywhere - isn't this a confounder for some of the experiments in the paper? Couldn't some of the performance differences be due to similarities between the resolution of pretraining and downstream tasks, as opposed to other factors like the modality? This may be relevant for e.g. Table 8, 9, and 10. \n\n# Other issues\n* Some important related work on pretraining in the geospatial domain is missing, e.g. [jean2019tile2vec]. Please double check to make sure other important works were not omitted."
                },
                "questions": {
                    "value": "* See \"Experiment issues\". \n* Given the information under \"Experiment issues\", does the paper make a clear case that the proposed method provides a stable and substantial advantage over existing alternatives?\n\n# Misc. Comments\n* There are two periods at the end of the first paragraph in section 3. \n* Why does it make sense to have per-sensor embeddings instead of per-channel embeddings? Different bands can differ substantially, and can be considered different modalities. \n* I would like to see a discussion of scalability as the number of sensors increases. \n* Shouldn't $C_m$ be $C_e$ in section 3.2? \n* Is it appropriate to give a new name (\"XGeoSet\") to the collection of 4 (pre-existing) datasets that are used for pretraining? It doesn't seem necessary to me. \n* Have there been any attempts to balance pretraining across datasets or resolutions? Might that help? \n* Are there some results about which the cross-modal losses? Which modality pairs are possible to cross-predict, and which are not? For instance, DSM->RGB seems quite difficult. \n* Broken reference in section 3.4 (\"??\")\n* The discussion of batch size is confusing - section 3.4 says that batch can be depicted as a set indexed by $i \\in \\{1, \\ldots, N\\}$ where $N$ is the number of sensors. This implies that the batch size is $N$. But later the batch size is stated as 2048. \n* Please include a citation for GeoLifeCLEF - which year's version was used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8376/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699321276526,
            "cdate": 1699321276526,
            "tmdate": 1699637041860,
            "mdate": 1699637041860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]