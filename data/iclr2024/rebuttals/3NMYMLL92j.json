[
    {
        "title": "Brain encoding models based on binding multiple modalities across audio, language, and vision"
    },
    {
        "review": {
            "id": "ihlewsU2NA",
            "forum": "3NMYMLL92j",
            "replyto": "3NMYMLL92j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_77Kr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_77Kr"
            ],
            "content": {
                "summary": {
                    "value": "The authors examine whether activations extracted from multimodal neural networks can fit signals derived from functional magnetic resonance imaging (fMRI) measurements obtained while participants watch videos. The paper also explores algorithms that generate captions, convert captions to speech and then use text, audio and video information to try to fit fMRI responses. Mysteriously, the authors conclude that the role of the visual system is to extract semantic informaiton."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The question of studying the similarities and differences between different modalities is important and worth studying. \n\nIt would be very useful for the field to understand how the brain represents visual, auditory, and text information (but this is not studied in the current paper)."
                },
                "weaknesses": {
                    "value": "Before jumping onto showing correlations (Fig. 2), it would be useful to show the actual activations both in the networks and in the fMRI signals to better understand how the correlations are computed. \n\nThe basic norms of scientific reporting are not followed here. Axis should be labeled, error bars should be defined. \n\nIt would also be useful to spell out the number of features and training used in each case. Does the order in Fig. 2 reflect the number of features or the amount of training in each modality or the successes of the neural network models in each modality? \n\nThe features are correlated and therefore it is hard to deduce anything from the fitting analyses. For example, if there is a ball in the image, and the text says ball and the audio says ball, then one can find that language areas can be fit by \"visual\" features but this does not mean that the language areas represent visual features. Conversely, visual areas can be fit by text, not because visual areas represent text. To understand the relationship between different modalities, we need rigorous controlled experiments that can prove uncorrelated feature dimensions. Unfortunately, this problem is ubiquitous throughout the paper. \n\nThere are no comparisons with different baselines, different neural network models, ablation studies."
                },
                "questions": {
                    "value": "Are the videos shown with sound? If so, why not use the actual sound and caption from the video? \n\nWhat is the point of converting caption to speech? In the best case scenario, the caption to speech is perfect and the information is redundant. In the worst case scenario, the speech is a bad rendering of the caption and merely adds noise. \n\nIt would be useful to conduct experiments where there is only visual information that is dissociated from audio information and from language information, experiments with only language information, etc. Even better, one could run experiments where different modalities are orthogonalized (e.g. show a ball and present the word chair). Once the modalities are rigorously decorrelated, it may be possible to begin to disentangle the contribution of different modalities to brain signals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605531164,
            "cdate": 1698605531164,
            "tmdate": 1699636134080,
            "mdate": 1699636134080,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FWliXJo6eB",
                "forum": "3NMYMLL92j",
                "replyto": "ihlewsU2NA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2026/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Overall contributions of our paper**\n\nBefore we respond to individual queries, we thought of summarizing the contributions and novelty of the paper to give a broader context. Understanding how neural network representations are related to human brain representations has been a central question in deep learning and our paper makes an important addition to the literature in this aspect. It is true that prior works already used similar frameworks (using representations from transformers to build encoders that predict fMRI data/images). However, our work makes significant novel contributions in demonstrating how to leverage joint information across modalities for better brain alignment as mentioned in brief in the following:\n\n* While earlier works investigated alignment with image+captions and image+language features from language models (LM), for the first time, we investigated how the joint representations learned from video+captions+audio align with the brain responses. \n* We use the recent popular ImageBind model to learn a unified representation space that binds semantics across video clips, text captions, and synthesized audio. \n* Our results verify our hypothesis that the joint embedding space leads to better alignment as compared to alignment with unimodal representations. This is possible due to leveraging cross-modality complementary information effectively, reducing modality-specific noise, and facilitating knowledge transfer between modalities. \n* It is to be noted that the movie clips were presented silently to the subjects and there is no audio or subtitles (captions) available as the movies were constructed by concatenating sequences of 10-20s video clips. \n* There are unfortunately no high-quality captions readily available for the short-clips dataset [1]. Thus, the captions and the synthesized audio for the captions form an additional data contribution to the video clips dataset.\n\n[1] Tang, J., Du, Meng., & Huth, A. G. Brain encoding models based on multimodal transformers can transfer across language and vision, NeurIPS (2023).\n\n**Before jumping onto showing correlations (Fig. 2), it would be useful to show the actual activations both in the networks and in the fMRI signals to better understand how the correlations are computed.**\n\nWe understand the reviewer's standpoint about investigating how the brain responds to video clips and how the deep networks process video information before looking at the performance with joint representations. Although these basic questions are very important, they have already been addressed in previous works [1], [2], [3]. They have become important standard results in computational cognitive neuroscience. Consequently, we focused on presenting the correlation results indicating the accuracy of joint representations, as shown in Fig 2, along with brain surface visualizations in Fig 3. Further, we elaborate on the associated methods relevant to our computations that differ from previous studies in the methods section.\n\n[1] Nishimoto, S. et al. Reconstructing visual experiences from brain activity evoked by natural movies. Curr. Biol. 21, 1641\u20131646 (2011).\n\n[2] Huth, A. G. et al. Decoding the Semantic Content of Natural Movies from Human Brain Activity. Front. Syst. Neurosci. 10, 81 (2016).\n\n[3] Popham, S. F. et al. Visual and linguistic semantic representations are aligned at the border of the human visual cortex. Nat. Neurosci. 24, 1628\u20131636 (2021).\n\n**The basic norms of scientific reporting are not followed here. Axis should be labeled, error bars should be defined.**\n\nThanks for your concern. We believe that we have followed the scientific norms for all the figures in our paper. We have clearly defined the plot labels and the axes. Also, we have mentioned the implications of the error bars wherever necessary. \n* For instance, in Figure 2 main paper, the x-axis represents the whole brain and the y-axis represents the Average Pearson Correlation values. The different colors (legend)  correspond to the ImageBind Video (blue), ImageBind Text (orange), and ImageBind Audio (green). \n* Similarly, for Figure 2 (right), the x-axis represents the ROIs (Primary Visual, Early Visual, Dorsal Visual, Ventral Visual, VWFA, MT and Late Language), and the y-axis represents the Average Pearson Correlation values. The different colors (legend)  correspond to the ImageBind Video (blue), ImageBind Text (orange), and ImageBind Audio (green). All of these are already defined in the paper. The same holds for other figures. It would be great if you could clearly identify which axis label is missing and in which figure. We would be happy to clarify further."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237911681,
                "cdate": 1700237911681,
                "tmdate": 1700237911681,
                "mdate": 1700237911681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FTOPHi1GUZ",
            "forum": "3NMYMLL92j",
            "replyto": "3NMYMLL92j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_bZKi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_bZKi"
            ],
            "content": {
                "summary": {
                    "value": "The paper evaluates the effectiveness of multimodally aligned features in understanding fMRI brain responses to videos. Using a dataset of fMRI scans from subjects watching videos, the study generates multi-event captions and synthesized audio to create a joint embedding across audio, text, and video. This joint embedding trains models to predict fMRI responses. Key findings indicate that the visual system primarily focuses on converting visual input into semantic descriptions, and multimodal alignment enhances the prediction of brain activity compared to unimodal approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors provides concise interpretability of their model's performance. They relate the outputted embeddings to specific regions of the brain, giving good intuition of how the human responds cognitively to external stimuli. \n- The authors provide sufficient ablation study to identify each modalities affect on performance."
                },
                "weaknesses": {
                    "value": "Maybe the authors can use another metric (MSE) to quantify the error in the fMRI activity prediction."
                },
                "questions": {
                    "value": "For future work, have the authors considered using different models for the text, video, and audio encoder to validate whether these findings generalize across different models as well? \nIt would also be interesting work to do a canonical correlation analysis to measure the relationship between the generated joint embeddings and the fMRI signals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2026/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2026/Reviewer_bZKi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724070191,
            "cdate": 1698724070191,
            "tmdate": 1699636134002,
            "mdate": 1699636134002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qQSuD7yHLn",
            "forum": "3NMYMLL92j",
            "replyto": "3NMYMLL92j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_TnaU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2026/Reviewer_TnaU"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses multimodally aligned features from visual, auditory and semantic domain to build encoding models to predict fMRI response to silent videos. The paper compares encoding model performances across different single modality models and multimodal models and showed that alignment help with brain prediction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is presents an interesting idea of leveraging multimodal alignment to probe the multimodal representation in the human brain. The idea itself is relatively novel.\n\nThe paper is written clearly."
                },
                "weaknesses": {
                    "value": "The findings of the paper lack novelty. The main message from the results, namely, visual perception carries semantic information that can be predicted with semantic-based encoding model, is well known and demonstrated across modalities and with different models. \n\nGiven the above, it is not surprising to me that other modality that are not present in the stimuli could individually predict brain responses to some extend (and that they each perform worse than the visual model), since the extracted feature from these modalities all share the semantic information contained in the visual feature and in the brain. It is also not surprising that, with the ImageBind model each modality gains a small boost in performance because of more semantic information they gain from the visual modality.\n\nSince the authors didn\u2019t make comparison between imageBind models to other single or multi- modal models, it is unclear whether ImageBind help at all in terms of overall brain prediction. \n\nThroughout the paper, comparison of models are largely limited to univariate bar plots or contrasts plots in brain maps. Without analysis like variance partitioning or feature regression between model space it is unclear whether these models from different modalities indeed predicts any regions uniquely.\n\nImageBind as a useful tool to bridge across modalities, perhaps has more potentials in aiding our understanding of the brain with stimuli that actually consists of multiple modalities (movies with actual audios). With those one could study, for example, difference of subtitles vs video transcripts encoding in the brain with presence of visual input."
                },
                "questions": {
                    "value": "In Figure 1, I think the colors of the arrow or the order of the modality are flipped."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801767079,
            "cdate": 1698801767079,
            "tmdate": 1699636133929,
            "mdate": 1699636133929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]