[
    {
        "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching"
    },
    {
        "review": {
            "id": "IROuaXFdrF",
            "forum": "Ebt7JgMHv1",
            "replyto": "Ebt7JgMHv1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_ERkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_ERkw"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies a challenge for the approach of looking for subspaces corresponding to causal factors of deep neural network output.  It claims that activation patching and subspace patching approaches are at risk of an interpretability illusion where the patching fails to change the causal pathways actually leading to undesired input.  It develops the concepts of dormant pathways and causally disconnected pathways to explain why the interpretability illusion is possible.  The paper illustrates its model of the interpretability illusion using a toy example.  Furthermore, it presents real-life case studies.  One set of case studies focuses on language models, particularly the indirect object identification task.  For the IOI task, previous works have used activation patching to correct a network to output the indirect object of a sentence rather than the direct object.  The paper identifies an approach to the task, using distributed alignment search, that is subject to the interpretability illusion.  It also identifies an approach, using prior knowledge about the network to identify the nodes responsible for the error, that correctly patches the network.  The paper explains the difference between these approaches using the concepts of dormant and causally disconnected pathways.  Furthermore, the paper also studies factual recall in language models as an additional case study.  It presents a set of experiments where it shows that the vulnerability of activation patching to the interpretability illusion depends on the layer which is targeted.  The paper also includes some additional discussion on connections between activation patching and rank-one model edits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a highly relevant work for the field of deep neural network interpretability, as it identifies a serious obstacle to the activation and subspace patching approaches.  The concepts of dormant and causally disconnected pathways are intuitive, and they are developed with the aid of theory and experiment.  The paper is clearly written, and figures help illustrate the concepts of the interpretability illusion, and the mechanisms involved in the IOI task.  This paper is significant for both identifying a practical problem and for developing intuitions that deepen our understanding of interpretability methods for deep neural networks."
                },
                "weaknesses": {
                    "value": "The definitions seem rather brittle.  Requiring strict equality in the definitions of causally disconnected and dormant seems to be very limiting for practice."
                },
                "questions": {
                    "value": "1. How do we know that the theoretical account for the interpretability illusion is actually the explanation for the failure of activation patching in the case studies presented?\n2. Are there a set of empirical predictions (e.g. about the directions of activations or gradients) that could either falsify or support the theoretical model?\n3. Why not relax the definitions of \"causally disconnected\" or \"dormant\" to not require strict equality, but rather to specify that the effect of the the patching be small within some threshold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697829496074,
            "cdate": 1697829496074,
            "tmdate": 1699637168130,
            "mdate": 1699637168130,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3aC5IxMcer",
                "forum": "Ebt7JgMHv1",
                "replyto": "IROuaXFdrF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review, and for the encouraging\nfeedback on the relevance and clarity of our work. Below, we address the\nreviewer\u2019s questions and other concerns:\n\n> The definitions seem rather brittle. Requiring strict equality in the definitions of causally disconnected and dormant seems to be very limiting for practice. [...] Why not relax the definitions of \"causally disconnected\" or \"dormant\" to not require strict equality, but rather to specify that the effect of the the patching be small within some threshold?\n\n**We acknowledge that requiring strict equality in the definition of a dormant\nsubspace is limiting. In our revision of the paper, we have changed the\ndefinition to allow for approximate equality instead**, which reflects the reality\nbetter. In our experiments (Figure 20 and Figure 12) we find that the orthogonal\ncomplements of the causally disconnected components of the subspaces we find are\nrelatively dormant compared to the causally disconnected components.\n\nAs for causally disconnected subspaces, **it is not too limiting to require strict\nequality, and all our empirical examples of causally disconnected subspaces obey\nstrict equality**. Recall that a subspace of the activations of some model\ncomponent is causally disconnected if changing the activation only along this\nsubspace (leaving the component orthogonal to the subspace unchanged) does not\nchange the model\u2019s output. This assumption is provably satisfied when, for\nexample, we consider the post-GELU activations of an MLP layer, and\n$v_{disconnected}$ is in the kernel of the layer\u2019s down-projection. This is\nbecause the only causal pathway by which the MLP activations affect the rest of\nthe model is via the layer\u2019s down-projection. A down-projection is a linear\noperation, and adding a vector in its kernel does not change its output.\n\n> How do we know that the theoretical account for the interpretability illusion is actually the explanation for the failure of activation patching in the case studies presented? [...] Are there a set of empirical predictions (e.g. about the directions of activations or gradients) that could either falsify or support the theoretical model?\n\nWe connect our theoretical model of the illusion to our experiments using the following approach:\n- Suppose we have a 1-dimensional subspace $v$ of the post-GELU activations of an\nMLP layer that we want to argue exhibits the illusion\n- Decompose $v = v_{nullspace} + v_{rowspace}$, where $v_{nullspace}$ is the orthogonal projection of $v$ on the kernel of $W_out$, and $v_{rowspace}$ is the orthogonal projection of $v$ on the orthogonal complement of the kernel of $W_out$.\n- By definition, $v_{nullspace}$ is a causally disconnected direction (as\nexplained in our response to the reviewer\u2019s concern about the definitions of\ncausally disconnected and dormant subspaces). This means that the only\ndownstream effect of patching along $v$ is the change of the activation along\n$v_{rowspace}$. Thus, the only consequential difference between patching along\n$v$ and patching along $v_{rowspace}$ is the magnitude of the change of the\nactivation along $v_{rowspace}$.\n- Therefore, a **key experiment to check for the illusion** is to patch\nonly long $v_{rowspace}$ and compare the effect to patching along $v$. If the effect\nfrom patching along $v$ is much stronger, this suggests the presence of the\nillusion, because the disconnected component must vary substantially in order to\nchange the coefficient along $v_{rowspace}$ \n- To further argue that v_rowspace is approximately dormant, we use different\napproaches for the two tasks:\n  - For the IOI task, we plot the distribution of activations, colored by the\n  feature of interest (position of the IO name in the sentence), and projected\n  on the $v_{rowspace}$ or $v_{nullspace}$ directions (Figure 20). This shows\n  that $v_{rowspace}$ is significantly more dormant than $v_{nullspace}$;\n  - For factual recall, we again decompose the directions found by DAS (which\n  are in the post-gelu activations of MLP layers) in the same way.\n    - Again by definition, the nullspace components are causally disconnected.\n    - To check how dormant the rowspace components are, we ran the experiment\n    shown in Figure 12 in the paper, where we project the difference of\n    activations of the two examples we are patching between on the causally\n    disconnected component (the orthogonal projection on $\\ker W_{out}$), and\n    observe cosine similarity between this projection and the original direction\n    of ~0.9; this implies that the cosine similarity with the rowspace component\n    is ~0.4; thus, the rowspace component is relatively dormant compared to the\n    nullspace one.  \n    - In our revision of the paper, we have included a note on this in the main body of the text."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178403604,
                "cdate": 1700178403604,
                "tmdate": 1700178403604,
                "mdate": 1700178403604,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QyYMV6Whly",
                "forum": "Ebt7JgMHv1",
                "replyto": "3aC5IxMcer",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_ERkw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_ERkw"
                ],
                "content": {
                    "title": {
                        "value": "Good idea for validating theory"
                    },
                    "comment": {
                        "value": "Your response is helpful for thinking about experimentally testing for the validity of your theory.  You describe a test\n\n> A key experiment to check for the illusion is to patch only [a]long $v_{rowspace}$ and compare the effect to patching along $v$.\n\nWhich I agree would be practical, and could be used to quantify the prevalence of the effect in various networks.  Do you think this experiment could be operationalized into a method for detecting interpretability illusions?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582840295,
                "cdate": 1700582840295,
                "tmdate": 1700582840295,
                "mdate": 1700582840295,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xFtOgcaFYy",
            "forum": "Ebt7JgMHv1",
            "replyto": "Ebt7JgMHv1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_8jAi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_8jAi"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that subspace activation patching, a technique in mechanistic interpretability used to find subspaces that are \u201ccausally\u201d responsible for models to produce certain outputs, may not be reliable. In particular, the paper explains why these may be misleading, involving the activation of dormant pathways. Real-world examples involving indirect object identification (IOI) and fact editing are shown where such techniques are misleading."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ The paper presents an interesting hypothesis that subspace activation patching methods can be misleading because of dormant pathways. On an intuitive level, this is an interesting hypothesis and needs to be considered for future applications of activation patching."
                },
                "weaknesses": {
                    "value": "The paper does not pose an explicit hypothesis that can be falsified, limiting its scientific validity. Further, the \"illusion\" seems to make **problematic assumptions** and has unclear implications. \n\n- The hypothesis seems to be that every patched subspace \u201cv\u201d discovered by DAS can be decomposed into two directions \u201cv_disconnected\u201d and \u201cv_dormant\u201d (with specific properties), and that \"patching along the sum of these directions, the variation in the disconnected part activates the dormant part, which then achieves the causal effect\". The latter statement assumes that the model behaves somewhat linearly along these subspaces (i.e., the output of the model along direction \"v\" is given by a sum of its outputs along \"v_disconnected\" and \"v_dormant\"), which is a strong hypothesis given that these models are fundamentally non-linear. While the paper provides some evidence for the existence of some \"disconnected\" and \"dormant\" directions, unfortunately, I do not see evidence justifying the apparent linear behaviour of the underlying model.\n\n- The definition of the \"dormant\" subspaces is confusing in the context of this work. If the model output remains unchanged for in-distribution patching and changes only for out-of-distribution samples $x,y \\sim \\mathcal{D}$, what procedures presented in this work result in out-of-distribution samples/activations? As far as I can tell, all procedures described involve patching with in-distribution data.\n\n- Is the hypothesis that such a decomposition does not exist for \u201ctrue\u201d subspace directions?\n\n---\n\nThe paper (especially sections 4-7) is difficult to read, is very dense, and has **several omitted details**. \n\n- The main paper on its own does not seem to be self-contained and seems to contain a significant number of references to the appendix. \n\n- The main hypothesis involves the claims that (1) subspace directions can be decomposed into two directions (disconnected, dormant) with specific properties, and (2) the effect of such directions adds somewhat linearly to the model outputs. However, the paper fails to connect this terminology (\"disconnected, dormant directions\") in sections 4-7, making it difficult to verify whether the experiments confirm or deny the hypothesis. \n\n- The writing and presentation are sloppy. For example, in Section 4, \u201cker W_out\u201d is never defined, and it is unclear how the results in Table 1 and Figure 3 relate to the description in section 4.3. What is ABB / BAB in Figure 3? What does \u201cconnected\u201d in Table 1 refer to? Overall, how do Table 1, Figure 3, and Figure 4 illustrate support for the presented hypothesis?\n\n---\n\nOverall, while this work may present a useful point, its writing (especially sections 4-7) makes it impossible to verify this. I suspect that a thorough rewrite is necessary to clarify its central point."
                },
                "questions": {
                    "value": "- For a future draft, I encourage the authors to present an explicit falsifiable hypothesis that facilitates both experimentation and analysis. \n\n- It might be helpful to comment on the downstream applications and practical utility of such activation patching techniques and mechanistic interpretability, particularly to better understand the implications of the \"illusion\". What are the use cases for identifying model components (neurons/subspaces) responsible for model behavior? What evaluation metrics are available to test whether the components have been correctly identified?\n\n- Do these results for subspace activation patching also hold for usual activation patching? It seems like subspace patching is a generalization, and thus it must, and also I see that the toy example is given for the case of usual activation patching, but the experimental results and the paper's messaging are specific to subspace patching."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698099848311,
            "cdate": 1698099848311,
            "tmdate": 1699637168008,
            "mdate": 1699637168008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xkqddfGePu",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying fundamental aspects of our work that were miscommunicated"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful and thorough review. We are sorry\nthat reading the paper was difficult, and we thank the reviewer for pointing out\nseveral confusing parts of the paper. **However, there are some fundamental\naspects of our work that were miscommunicated, and we would like to clarify them\nbelow**. We have uploaded a revision that we hope will alleviate these concerns,\nwith changes marked in red.\n\n> The paper does not pose an explicit hypothesis that can be falsified, limiting\nits scientific validity. [...] The hypothesis seems to be that every patched\nsubspace \u201cv\u201d discovered by DAS can be decomposed into two directions\n\u201cv_disconnected\u201d and \u201cv_dormant\u201d (with specific properties)\n\nThe key message of our work is not that the illusion described will always\nhappen given some set of assumptions. Instead, **our central focus is showing\nthe *existence* of the illusion in practical cases of interest** for ML\ninterpretability. In the cases that we empirically discovered, the subspaces\nfound indeed decompose as a sum of two orthogonal vectors with the properties\ngiven in Section 3 of the paper, and we found this decomposition a helpful way\nto think about the mechanistic underpinnings of the illusion. **We do not claim\nto provide specific conditions that will *guarantee* that the illusion will\noccur**, or that such a decomposition will exist. This means that our work takes\nthe form of demonstrating the existence of the illusion, rather than proving a\nfalsifiable hypothesis that it should always happen, which we expect is too\nstrong a claim.\n\nAs other reviewers have noted, this is a valuable contribution to the\ninterpretability literature, as **the existence of such cases demonstrates that\none should not apply subspace patching techniques blindly**. In particular, a\nkey takeaway is that one should not rely solely on end-to-end metrics when\nevaluating optimization-based subspace patching methods for the purpose of\nlocalizing features in language models. \n\nWe remark that we also provide several theoretical and heuristic reasons to\nexpect the illusion to be prevalent in practice in Section 7, but they are not\nload-bearing to our main message.\n\n> ...\"patching along the sum of these directions, the variation in the\ndisconnected part activates the dormant part, which then achieves the causal\neffect\". The latter statement assumes that the model behaves somewhat linearly\nalong these subspaces (i.e., the output of the model along direction \"v\" is\ngiven by a sum of its outputs along \"v_disconnected\" and \"v_dormant\"), which is\na strong hypothesis given that these models are fundamentally non-linear.\n\n**We make no assumptions of linearity, only about the behavior of the model when\nchanging the activation only along v_disconnected, or only along v_dormant**. In\nparticular, to address the reviewer\u2019s objection, we only need a weaker set of\nassumptions: that **changing the activation only along v_disconnected does not\nchange model outputs**. This assumption is provably satisfied when, for example,\nwe consider the post-GELU activations of an MLP layer, and v_disconnected is in\nthe kernel of the layer\u2019s down-projection. This is because the only causal\npathway by which the MLP activations affect the rest of the model is via the\nlayer\u2019s down-projection. A down-projection is a linear operation (though\nsubsequent model layers are not linear), and adding a vector in its kernel does\nnot change its output, thus adding v_dormant, or adding v_dormant and\nv_disconnected must have exactly the same downstream effect.\n\nTo unpack this in more detail, we believe that in the review, the words \u201c...the\noutput of the model along direction \"v\" is given by a sum of its outputs along\n\"v_disconnected\" and \"v_dormant\"...\u201d are intended to mean \u201cwhen changing the\nactivation along v, this results in the same change in model output as the sum\nof the changes when changing only along v_disconnected and when changing only\nalong v_dormant, adjusting the magnitudes of the changes by the projection of v\non the two vectors\u201d. \n\n**This is a true mathematical fact, but it is independent of any linearity\nassumptions on the neural network**. Recall that by construction v =\n(v_disconnected + v_dormant) / sqrt(2). Changing some activation A along v alone\nis the same as changing it along v_disconnected and v_dormant simultaneously\n(with coefficient 1/sqrt(2)), and leaving it unchanged along all directions\northogonal to the plane spanned by v_disconnected and v_dormant. Let\u2019s call this\nnew activation A\u2019. By assumption, changing the activation A\u2019 along\nv_disconnected alone results in no change in model output; so this activation A\u2019\nwill produce the same model output as the activation A\u2019\u2019 where only v_dormant\nwas changed. This shows that the change in outputs introduced by changing along\nv is the same as the change in outputs introduced by changing along v_dormant\nalone, plus the change in outputs after changing along v_disconnected alone\n(which is zero)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175814331,
                "cdate": 1700175814331,
                "tmdate": 1700175814331,
                "mdate": 1700175814331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zw0CfQEisl",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response on dormant/disconnected directions, and connecting theory to experiments"
                    },
                    "comment": {
                        "value": "Continuing from our previous response, we would like to clarify some other aspects of our work that were miscommunicated.\n\n> The definition of the \"dormant\" subspaces is confusing in the context of this work. If the model output remains unchanged for in-distribution patching and changes only for out-of-distribution samples, what procedures presented in this work result in out-of-distribution samples/activations? As far as I can tell, all procedures described involve patching with in-distribution data.\n\nWhile the reviewer\u2019s concern would be true in the case of full component activation patching, **it is possible for subspace activation patching between in-distribution data to result in out-of-distribution activations** when the subspace is a proper subspace of activation space.  \n\nTo explain in more detail, imagine a 2-dimensional activation space, where\nin-distribution activations are always on the x-axis, ie the y-coordinate is\nalways zero. Consider subspace activation patching along the vector\n$v=(1/\\sqrt(2), 1/\\sqrt(2))$ which makes equal angles with the x and y axes. If\nwe patch along $v$ from an example with activation $(x_1, 0)$ into an example\nwith activation $(x_2, 0)$ with $x_1 \\neq x_2$, the result is the activation\n\n$$(x_2, 0) + 1/\\sqrt(2) * (x_1-x_2) * v =  ((x_1+x_2)/2, (x_1-x_2)/2)$$\n\nIn particular, this activation is now out-of-distribution, since it has a\nnonzero component along the y axis.\n\n> Is the hypothesis that such a decomposition does not exist for \u201ctrue\u201d subspace directions?\n\n**A definition of \u201ctrue\u201d subspace directions is outside the scope of our work**.\nRather, our central message is that the existence of such a decomposition, and\nan empirical observation showing that removing the causally disconnected\ncomponent of a subspace greatly reduces the effect of the subspace activation\npatch, should be taken as cause for concern when attributing features to\nparticular subspaces.\n\n> [...] However, the paper fails to connect this terminology (\"disconnected, dormant directions\") in sections 4-7, making it difficult to verify whether the experiments confirm or deny the hypothesis.\n\nWe do connect the terminology of disconnected and dormant directions to our experiments in several ways:\n- For the IOI task, we orthogonally decompose the $v_{MLP}$ subspace as a sum $v_{MLP} = v_{MLP}^{nullspace} + v_{MLP}^{rowspace}$, where $v_{MLP}^{nullspace}$ is the orthogonal projection of $v_{MLP}$ on the kernel of $W_{out}$, and $v_{MLP}^{rowspace}$ is the orthogonal projection on the rowspace of $W_{out}$. \n  - By definition, $v_{MLP}^{nullspace}$ is a causally disconnected direction. \n  - We show that the direction $v_{MLP}^{rowspace}$ is significantly less activated by the feature we are patching than the direction $v_{MLP}^{nullspace}$ in Figure 20 in the Appendix. This shows that $v_{MLP}^{rowspace}$ is relatively dormant, compared to $v_{MLP}^{nullspace}$. \n  - In our revision of the paper, we have rewritten large parts of Subsection 4.3. to clarify the relationship of the experiments to our model of the illusion.\n- For factual recall, we again decompose the directions found by DAS (which are in the post-gelu activations of MLP layers) in the same way. \n  - Again by definition, the nullspace components are causally disconnected.\n  - To check how dormant the rowspace components are, we ran the experiment\n  shown in Figure 12 in the paper, where we project the difference of\n  activations of the two examples we are patching between on the causally\n  disconnected component (the orthogonal projection on $\\ker W_{out}$), and\n  observe cosine similarity between this projection and the original direction\n  of ~0.9; this implies that the cosine similarity with the rowspace component\n  is ~0.4; thus, the rowspace component is relatively dormant compared to the\n  nullspace one.\n  - In our revision of the paper, we have included a note on this in the main body of the text."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176579342,
                "cdate": 1700176579342,
                "tmdate": 1700176579342,
                "mdate": 1700176579342,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PmjEKwoyve",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response on clarity of writing and self-containedness"
                    },
                    "comment": {
                        "value": "> The main paper on its own does not seem to be self-contained and seems to contain a significant number of references to the appendix.\n\nWe acknowledge that there is quite a bit of material in the appendix, and are\nsorry that this made our work harder to follow. We have tried to condense the\ncentral points of our work in the main body of the paper by making the following\nchanges, which have significantly reduced the dependence of the main body in\nsections 4 and 6 on the supplementary material by including the following\ndetails in the main body:\n- IOI dataset we used (Section 4.1)\n- Computing the direction $v_{grad}$ (Section 4.2)\n- Computing directions using DAS (Section 4.2)\n- Details for fact patching experiments (Section 6.1)\n- We moved what was previously Figure 2 to the supplementary material, as we felt that it makes an insufficient contribution relative to the space it occupies.\n- Similarly, we moved the statement of what was previously Lemma 6.1. To the\n  appendix, and explained its result informally\n\nWe hope this alleviates your concern. If there are any other parts of the\nappendices which felt important to understanding the main text, we would\nappreciate you bringing them to our attention.\n\n> The writing and presentation are sloppy. For example, in Section 4, \u201cker W_out\u201d is never defined, and it is unclear how the results in Table 1 and Figure 3 relate to the description in section 4.3. What is ABB / BAB in Figure 3? What does \u201cconnected\u201d in Table 1 refer to? Overall, how do Table 1, Figure 3, and Figure 4 illustrate support for the presented hypothesis?\n\nWe have addressed these issues as well as others pointed out by the other reviewers in our updated draft. In particular, we rewrote Subsection 4.3 to fix various problems and better connect the writing to the table and figure. To answer the reviewer\u2019s questions specifically:\n- $\\ker W_{out}$ is the kernel of the down-projection of the MLP layer where the\n  illusory direction v_MLP is contained.\n- The main text now contains detailed descriptions of all interventions involved, with short names matching those appearing in Table 1. \n- Figure 3 relates to the text in Section 4.3 through the paragraph now titled\n\u201cPatching $v_{\\text{MLP}}$ activates a dormant pathway through the\nMLP.\u201d Specifically, Figure 3 illustrates how the subspace activation patch along\n$v_{MLP}$ takes the output of the MLP layer along the gradient direction of the\nname mover query matrices off-distribution.\n- Here and elsewhere in the paper, \u201cABB\u201d means prompts where the IO name comes\nfirst, and \u201cBAB\u201d means prompts where the S name comes first. We thank the\nreviewer for pointing out this omission. The notation is now explained in the\ncaption of Figure 3.\n\nOverall, Table 1 supports our claim for the existence illusion by showing that\nthe subspace patch along $v_{MLP}$ has a significant effect on model outputs\n(much stronger than the effect of patching the full MLP layer\u2019s activations),\nbut this effect disappears when we restrict to the causally relevant component\nof $v_{MLP}$, denoted $v_{MLP}^{rowspace}$ in our revision and introduced in the\n\u201cMethodology\u201d part of Subsection 4.3.  Figure 3 further supports our model of\nthe illusion by showing how the MLP layer\u2019s output on the causally relevant\ndirection $v_{grad}$ (given by the gradient of name-mover attention scores) is\ntaken significantly out of distribution by the patch along $v_{MLP}$. Finally,\nFigure 4 (which is about factual recall) has similar message as Table 1: it\nshows that subspace patching along the directions found by DAS is much more\neffective at changing a model\u2019s completion of a fact than patching the entire\nMLP layer, or patching along only the causally relevant component."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177355093,
                "cdate": 1700177355093,
                "tmdate": 1700177355093,
                "mdate": 1700177355093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OeT3R1Riik",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response on motivation for mechanistic interpretability and other related work"
                    },
                    "comment": {
                        "value": "> It might be helpful to comment on the downstream applications and practical utility of such activation patching techniques and mechanistic interpretability, particularly to better understand the implications of the \"illusion\". What are the use cases for identifying model components (neurons/subspaces) responsible for model behavior? \n\nMechanistic interpretability has been used in several downstream applications, some of which are: removing toxic behaviors from a model while otherwise preserving performance by minimally editing model weights (Li et al [1]), changing factual knowledge encoded by models in specific components to e.g. enable more efficient fine-tuning in a changing world (Meng et al [2]), improving the truthfulness of LLMs at inference time via efficient, localized inference-time interventions in specific subspaces (Li et al [3]), and studying the mechanics of gender bias in language models (Vig et al [4]).  **We have added these references to the introduction of the paper in our revision.**\n\n[1] Maximilian Li, Xander Davies, and Max Nadeau. Circuit breaking: Removing model behaviors with targeted ablation. In DeployableGenerativeAI, 2023\n\n[2] Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. \u201cLocating and\nediting factual associations in GPT\u201d. In: Advances in Neural Information Processing\nSystems. 2022.\n\n[3] Kenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inference-time intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341.\n\n[4] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas\nSakenis, Jason Huang, Yaron Singer, and Stuart Shieber. \u201cCausal mediation analysis\nfor interpreting neural nlp: The case of gender bias\u201d. In: arXiv preprint arXiv:2004.12265\n(2020).\n\n> What evaluation metrics are available to test whether the components have been correctly identified?\n\n**This is an area of active research to which our work contributes, though our work is far from the final word on the matter**. Some concrete and general techniques to evaluate LLM interpretations have been proposed, such as causal scrubbing (Chan et al. [5], mentioned in the related work of our paper). Other common techniques are based on ablations, such as (full component) activation patching, as discussed in our work. More recently, techniques like DAS have been proposed that use subspace activation patching to evaluate mechanistic interpretations of LLMs. Part of our contribution is exhibiting a situation where full-component and subspace activation patching reach different conclusions about the mechanics of the model\u2019s behavior, and arguing why the subspace-level explanation is misleading.\n\n[5] Lawrence Chan, Adri\u00e0 Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. \u201cCausal Scrubbing: a method for rigorously testing interpretability hypotheses\u201d. https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177644724,
                "cdate": 1700177644724,
                "tmdate": 1700177644724,
                "mdate": 1700177644724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EqYfXdkhql",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response on full-component vs subspace activation patching"
                    },
                    "comment": {
                        "value": "> Do these results for subspace activation patching also hold for usual activation patching? It seems like subspace patching is a generalization, and thus it must, and also I see that the toy example is given for the case of usual activation patching, but the experimental results and the paper's messaging are specific to subspace patching.\n\nThe toy example is of a linear neural network with one hidden layer, where we intend the layer to be a single \u201ccomponent\u201d (the same way we consider an MLP\u2019s post-GELU activations at a given token and layer in an LLM to be a single \u201ccomponent\u201d). This is admittedly a matter of semantics in this simple example. We have added a clarification about this in our revision.\n\n(The following two paragraphs also appear in the response to reviewer **YsUK**) This is a great question. **A key property of subspace activation patching not shared with full-component activation patching is that it may create out-of-distribution activations for the given component, even when patching only between in-distribution examples**. This makes it particularly vulnerable to an MLP-in-the-middle type of illusion, where we take the output of the MLP layer off-distribution in order to write some causally important information in the residual stream. Intuitively, this would not be possible to such an extent when patching the full hidden activation of the MLP layer, because its activations for in-distribution examples do not generally write important information to the residual stream (as can be confirmed by doing the full patch). \n\nHowever, full-component activation patching can still take the activations of the model as a whole off-distribution. It is an interesting question for future work whether a variant of the illusion can be exhibited for full-component patching. For example, if two heads always cancel each other out (one outputs +v and the other -v), we could patch just one of them, such that they no longer cancel out. Thus, it may be possible to patch only one of these components and throw the model off-distribution. However, we expect such scenarios of \u201cperfect cancellation\u201d to be rarer in practice."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177817167,
                "cdate": 1700177817167,
                "tmdate": 1700177817167,
                "mdate": 1700177817167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0CYDuJOWS1",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concluding response"
                    },
                    "comment": {
                        "value": "We apologize again for the miscommunication and lack of clarity in our writing. We have tried to clarify the parts of the paper flagged by the reviewer in our updated revision. We hope that these changes and our detailed responses alleviate the reviewer\u2019s concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177855782,
                "cdate": 1700177855782,
                "tmdate": 1700177855782,
                "mdate": 1700177855782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "brbuErVCcZ",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder about end of discussion period"
                    },
                    "comment": {
                        "value": "Dear reviewer **8jAi**, this is a gentle reminder that the end of the discussion\nperiod draws near (in approximately 48 hours). **We have responded with a\nrebuttal to your comments**, and we hope you will respond back, letting us know\nif your concerns have been alleviated. If you have any remaining concerns, we\nwould be happy to continue the discussion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569385587,
                "cdate": 1700569385587,
                "tmdate": 1700569385587,
                "mdate": 1700569385587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tVtpRIHHY7",
                "forum": "Ebt7JgMHv1",
                "replyto": "xFtOgcaFYy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_8jAi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_8jAi"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. While I carefully read the updated paper and rebuttal, I want to clarify the following aspects:\n\n**Re: falsifiable hypothesis**: Note that a falsifiable hypothesis does not require specifying the precise conditions under which the illusion occurs. In this case, a hypothesis can also be stated as \"there exist pre-trained ML models such that the following property holds with their activations: ...\". I believe that presenting a falsifiable hypothesis falls well within the scope of this work and, in general, any scientific work. The key idea is to present a concrete test to validate the hypothesis (i.e., the provided explanation) BEFORE looking at the experimental evidence. \n\n**Re: definition of dormant directions**: The paper defines a dormant direction as \"We say U is dormant if $M_{U_c \u2190u_y} (x) \u2248 M(x)$ with high probability over x, y \u223c D, but not over any x, y\". This definition critically relies upon having access to in-distribution and out-of-distribution inputs \"x\". Yet the rebuttal states that \"While the reviewer\u2019s concern would be true in the case of full component activation patching, it is possible for subspace activation patching between in-distribution data to result in out-of-distribution activations when the subspace is a proper subspace of activation space\". \n\n(1) This is a contradiction: does the definition apply to inputs or patched activations, or likely, both? (If so, please clarify in the main text)\n\n(2) The presented arguments are circular. The authors state that patching creates OOD activations, yet the definition is about model output invariance in the presence of patching. Can you please clarify? \n\n(3) Why doesn't full activation patching, similar to subspace patching, also result in OOD activations? Both these techniques create \"synthetic\" activations, and thus, there is a high chance of both being OOD."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597967866,
                "cdate": 1700597967866,
                "tmdate": 1700598349891,
                "mdate": 1700598349891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cu7rJXUWse",
            "forum": "Ebt7JgMHv1",
            "replyto": "Ebt7JgMHv1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_YsUK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9274/Reviewer_YsUK"
            ],
            "content": {
                "summary": {
                    "value": "This work finds and shows that subspace activation patching may be subject to interpretability illusions caused by activating a dormant pathway (i.e., does not respond to input changes) but is activated by a causally disconnected feature (i.e., is activated by the input change but is not connected to the model\u2019s output). They show that this illusion also relates to rank-one fact editing (e.g., Meng et al. [1]) and explains its recently found inconsistencies [2]. Finally, they demonstrate that further analysis, e.g., manual circuit analysis, can mitigate the interpretability illusion.\n\n[1] Meng, Kevin, et al. \"Locating and editing factual associations in GPT.\" NeurIPS 2022.\n\n[2] Hase, Peter, et al. \"Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models.\" arXiv 2023."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Given the rising popularity of mechanistic interpretability and (subspace) activation patching, this paper addresses a very important and significant issue in a timely manner.\n\n* The interpretability illusion is well-motivated and clearly introduced. The formal definition (p. 5) is sound. It is also clear that the illusion is not a mere artifact of the chosen experimental settings but is present in most cases with high probability.\n\n* The experimental design to showcase the interpretability illusion is well-designed.\n\n* The discussion on how one can prevent the interpretability fallacy is important and sound. It paves a way for future work that relies on automatic activation patching methods, to avoid false interpretations of model behavior.\n\n* The discussion on the presence of the interpretability illusion from a mechanistic viewpoint is sound.\n\n* The paper is clearly written, (mostly) easy to follow, and self-containing."
                },
                "weaknesses": {
                    "value": "This paper is a very strong submission. It is well-motivated, sound, and clear. The only \u201cmajor\u201d weakness is that code is not provided and minor comments (see below)."
                },
                "questions": {
                    "value": "* Why do the authors solely focus on subspace activation patching? The identified interpretability illusion should also hold for (automatic) component activation patching (i.e., consider the entire activation space as the (sub)space).\n\n* There seems to be an error in the indices in the toy example in Appendix A.3.\n\n## Suggestions\n\n* While Fig. 1 clearly demonstrates the interpretability illusion, it is hard to parse. It may be good to make it more accessible/easier to parse, as it demonstrates the main insight of the paper.\n\n* It\u2019d be good to add the relation of the vector $v$ to the subspace $U$ in Sec. 3.\n\n* It\u2019d be good to match the notation of Tab. 1 and the respective text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9274/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9274/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9274/Reviewer_YsUK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9274/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698421107963,
            "cdate": 1698421107963,
            "tmdate": 1699637167888,
            "mdate": 1699637167888,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yIRPjeLZPy",
                "forum": "Ebt7JgMHv1",
                "replyto": "cu7rJXUWse",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review, and for the encouraging\nfeedback on the timeliness and soundness of our work. Below, we address the\nreviewer\u2019s questions and other concerns:\n\n> code is not provided\n\nWe are working on releasing an anonymized version of our code in the next few\ndays. We will provide a link to it in a subsequent response once it is\navailable.\n\n> Why do the authors solely focus on subspace activation patching? The\nidentified interpretability illusion should also hold for (automatic) component\nactivation patching (i.e., consider the entire activation space as the\n(sub)space).\n\nThis is a great question. A key property of subspace activation patching not\nshared with full-component activation patching is that it **may create\nout-of-distribution activations for the given component, even when patching only\nbetween in-distribution examples**. This makes it particularly vulnerable to an\nMLP-in-the-middle type of illusion, where we take the output of the MLP layer\noff-distribution in order to write some causally important information in the\nresidual stream. Intuitively, this would not be possible to such an extent when\npatching the full hidden activation of the MLP layer, because its activations\nfor in-distribution examples do not generally write important information to the\nresidual stream (as can be confirmed by doing the full patch). \n\nHowever, full-component activation patching can still take the activations of\nthe model as a whole off-distribution. It is an interesting question for future\nwork whether a variant of the illusion can be exhibited for full-component\npatching. For example, if two heads always cancel each other out (one outputs +v\nand the other -v), we could patch just one of them such that they no longer\ncancel out. Thus, it may be possible to patch only one of these components and\nthrow the model off-distribution. However, we expect such scenarios of \u201cperfect\ncancellation\u201d to be rarer in practice. \n\n> There seems to be an error in the indices in the toy example in Appendix A.3.\n\nThis is correct; the sentence \u201c... the linear subspace of H_2 and H_3 defined by\nthe unit vector\u2026\u201d should be \u201c...H_1 and H_2\u2026\u201d instead. We thank the reviewer for\npointing this out, and we have corrected it in our revision of the paper. \n\n> While Fig. 1 clearly demonstrates the interpretability illusion, it is hard to\nparse. It may be good to make it more accessible/easier to parse, as it\ndemonstrates the main insight of the paper.\n\nWe acknowledge that Figure 1 is somewhat involved and thank the reviewer for\npointing this out. To make it more readable, we have included a \u201cdecomposition\u201d\nof this figure into four figures to be read sequentially. Due to space\nlimitations, we include this new figure as Figure 26 in the Appendix, and we\npoint to it from the main text. We also re-did the original figure in TikZ to\nimprove readability. We hope that this makes the phenomenon clearer.\n\n> It\u2019d be good to add the relation of the vector v to the subspace U in Sec. 3.\n\nWe thank the reviewer for pointing this out; the relationship is that, in the\nspecial case of 1-dimensional subspace patching we consider, $U$ is the subspace\nspanned by the (unit) vector $v$. We hope that clarifies this part of the paper;\nwe have added it to our revision.\n\n> It\u2019d be good to match the notation of Tab. 1 and the respective text.\n\nWe thank the reviewer for pointing this out. This is a concern shared by another\nreviewer as well, and we acknowledge that Table 1 can be improved in many ways.\nIn the revision we have uploaded, we have made the notation more coherent. We\nhave also re-parametrized the fractional logit diff metric to make it more\nintuitive and readable."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178727014,
                "cdate": 1700178727014,
                "tmdate": 1700178727014,
                "mdate": 1700178727014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UZuqcdsaoQ",
                "forum": "Ebt7JgMHv1",
                "replyto": "yIRPjeLZPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_YsUK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9274/Reviewer_YsUK"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the thorough reply. I appreciate the effort of the authors to provide code and strongly encourage them to do so. I would also encourage the authors to include Fig. 26 instead of Fig. 1 in the main text but understand that page limit may be hindering in doing so. Lastly, I appreciate the discussion on subspace vs. entire activation space patching that could be also featured in the paper.\n\nOverall, I believe that this paper constitutes a very important contribution to the interpretability community and consequently should be highlighted as such at the conference."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9274/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559739234,
                "cdate": 1700559739234,
                "tmdate": 1700559739234,
                "mdate": 1700559739234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]