[
    {
        "title": "Trust Regions for Explanations via Black-Box Probabilistic Certification"
    },
    {
        "review": {
            "id": "9hg6AggS9L",
            "forum": "Of2RhzJ8UJ",
            "replyto": "Of2RhzJ8UJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a formal approach to \"explanation certification\" in machine learning. Given a model with query access, a sample point, and a set quality metric, they aim to identify the largest hypercube centered at the sample point. Within this hypercube, any applied explanation should meet the set quality criterion. To achieve this, they introduce \"Ecertify,\" a method that samples different points around the sample point within a hypercube's width and evaluates them based on the quality metric. The size of the hypercube are then iteratively adjusted until the maximum size is identified. The authors introduce several sampling techniques - 'unif', 'unifI', and 'adaptI', and provide probabilistic guarantees on their effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel problem of explanation certification and highlight the benefits, i.e., insight into model behavior in a specific region, stability of explanation, reusing explanations, etc.\n\nThey provide rigorous experimental  analysis to support their findings."
                },
                "weaknesses": {
                    "value": "While the problem statement is compelling, the methodology employed seems somewhat heuristic. Given a metric \\(f(\\cdot)\\), the goal is to identify a hypercube wherein all points satisfy the requirement \\(f(\\cdot) > \\theta\\). The approach involves sampling points within the hypercube, querying the function, and subsequently adjusting the hypercube's width size. The probabilistic guarantees suggest that as the number of queries approaches infinity, the error diminishes with high probability. This is reminiscent of the Maximum Likelihood Estimation (MLE) of a uniform distribution \\([0,b]\\) using samples \\( \\max_i x_i \\). Here, \\(\\hat{b} < b\\), and as \\(i\\) tends towards infinity, one approaches \\(b\\) at an exponential rate. What differentiates one sampling method from another if all have similar convergence rate?\n\nThe paper seems to overlook the role of the quality function \\(f\\) in its analysis. The provided probabilistic guarantees necessitate knowledge of \\(f's\\) cumulative distribution function (CDF), which is unknown. Nonetheless, the paper's discussion of special cases, such as characterizing CDFs for linear models and enhancing certification for Lipschitz models, is intriguing. However, relegating this information to the appendix is a missed opportunity. I recommend that the authors emphasize these aspects more prominently in the main body of the paper.\n\nIdentifying a hypercube in a d-dimensional space may lack practical relevance, especially since features in real-world datasets aren't always uniformly scaled.\n\nAdditionally, there's existing literature on robust counterfactual explanations amidst noisy implementations [1] and other related works that focus on identifying a region where a model's prediction remains consistent. These studies also employ a similar strategy of sampling around a point and querying the model. While their formulations might differ from yours, it's essential to acknowledge and cite these works\n\nThe presentation of the methodology in the paper, especially regarding 'unifI' and 'adaptI', could benefit from further clarity. Incorporating a visual representation or figure summarizing the methods might be beneficial. The inclusion of the video in the supplementary material is commendable.\n\n[1] https://arxiv.org/pdf/2203.06768.pdf"
                },
                "questions": {
                    "value": "Address weaknesses section.\nGenerally, I find your problem interesting, however I find your methods to be heuristic (sample and query; probabilistic guarantees are directly from a law of large numbers style argument). I would expect for a method a leverage the quality function $f$ for a better solution. I might be missing something and would like the authors to clarify the non-triviality of their method. I am open to revising my score ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1448/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551830541,
            "cdate": 1698551830541,
            "tmdate": 1699636073614,
            "mdate": 1699636073614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1cQ1CM4eJn",
                "forum": "Of2RhzJ8UJ",
                "replyto": "9hg6AggS9L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 3Uus"
                    },
                    "comment": {
                        "value": "> What differentiates one sampling method from another if all have similar convergence rate? I would expect for a method to leverage the quality function $f$ for a better solution.\n\nAs mentioned in Section 5, although the rate of convergence is exponential in $Q$ for all three strategies, we see that for unifI (eq. 4) and adaptI (eq. 5) the exponent heavily depends on the cdfs of the quality function $f$ around the prototypes that are sampled. Hence, if we have even one good prototype that gets sampled, both unifI and adaptI will leverage this and perform better than unif. adaptI goes a step further and samples (exponentially) more around the promising prototypes. Hence, our methods do leverage $f$ and the benefit of this is seen in the experiments (see Figure 2), where adaptI is faster than all the other methods since it is able to hone in on the low fidelity examples. Moreover, looking at Tables 5 and 7 in the appendix where we report the computed bounds based on Theorem 1, we observe that the bounds for adaptI converge faster than for unifI, which in turn converge faster than for unif (see especially $Q=100$). These observations are also mentioned in the experimental section.\n\n> The paper seems to overlook the role of the quality function (f) in its analysis... I recommend that the authors emphasize these aspects (characterizing CDFs for linear models and enhancing certification for Lipschitz models) more prominently in the main body of the paper.\n\nAs mentioned in our previous response, our bounds do not overlook the quality function $f$, and in fact heavily depend on the specific sampling strategy paying close attention to where it samples. We are glad that you liked our analysis of the special cases. We have pointed to this section in the introduction and related work. In terms of adding more material from the appendix to the main body, we would be grateful if you could suggest what could be removed as given the tight page limit of 9 pages and having now added also a figure in the introduction (see our last response below), we are finding it challenging to make space for it. In the main body as it stands, we chose to focus more on cdf estimation and cdf-free bounds from extreme value theory in Section 6.1, as these approaches apply more widely than the special cases.\n\n> Identifying a hypercube in a d-dimensional space may lack practical relevance, especially since features in real-world datasets aren't always uniformly scaled.\n\nAs mentioned in the discussion, our approach could also be used to find hyper-rectangles with some additional book-keeping. The relative lengths could correspond to the non-uniform scalings. Moreover, given the scalings one could rescale when applying our algorithm so that spreads are uniform and scale back when outputting the regions. Additionally, if the data lies on a lower dimensional manifold, one could apply our methods in the latent space (e.g. latent space of an autoencoder) and then decode to obtain free-form regions in the input space. This is also mentioned in Appendix G.\n\n> there's existing literature on robust counterfactual explanations amidst noisy implementations [1] and other related works... While their formulations might differ from yours, it's essential to acknowledge and cite these works\n\nWe have now cited such works in the related works section. Thank you for pointing us to them.\n\n> 'unifI' and 'adaptI', could benefit from further clarity. Incorporating a visual representation or figure summarizing the methods might be beneficial.\n\nWe have now added Figure 1 to the paper which tries to convey the main ideas behind the strategies. More detailed description is provided in Section 4. We did this to bring the benefit of the supplementary videos that you appreciated into the main paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006919010,
                "cdate": 1700006919010,
                "tmdate": 1700006919010,
                "mdate": 1700006919010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A43PzzqN0M",
                "forum": "Of2RhzJ8UJ",
                "replyto": "1cQ1CM4eJn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your rebuttal. I have reviewed your response along with other reviews and rebuttals. I will keep my current score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717042294,
                "cdate": 1700717042294,
                "tmdate": 1700717042294,
                "mdate": 1700717042294,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o2R1MgXjmd",
            "forum": "Of2RhzJ8UJ",
            "replyto": "Of2RhzJ8UJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a black box explanation certification method to provide insight into model behavior and ensure explanation stability. Empirical evaluations on synthetic and real data demonstrate the effectiveness of the proposed work. This work seems to have solid theoretical analyses and deals with an interesting topic. However, the work is not well-written. In particular, some intuition and concepts are not clearly provided, making the work difficult to follow."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work deals with an interesting topic, i.e., finding a hypercube of samples to make the explanations stable for the samples within the hypercube.\n2. The authors provide rich theoretical analyses to support their claim."
                },
                "weaknesses": {
                    "value": "1. The design of the methodology is quite trivial. The authors only provide the pseudo-code of the method without clear explanations and descriptions at a high level, which makes the reviewer unable to get the intuition behind the idea.\n2. Although the research topic is interesting, the motivation is not strong and it is unclear how important stability is for the XAI. Is there a practical scenario to illustrate the significance of the research topic?\n3. It is unclear the pros and cons of the three sampling strategies. The authors did not provide a deeper discussion of the three sampling strategies.\n4. The experiments are confusing. What is the purpose of ZO and why is it used as a baseline? Why does the convergence of w to a similar value indicate the accuracy of the explanation?"
                },
                "questions": {
                    "value": "1. What is the intuition of method design? At the least, the author should provide one figure to illustrate why the method is designed in this way and how the method works.\n2. Could the authors provide a real-world application to demonstrate the significance of the research?\n3. What are the pros and cons of the three sampling strategies? What kind of scenario are they suitable for and what insight do they give practitioners?\n4. The authors claimed the method is model agnostic and quite general. So can the method generalize to graph data?\n5. To what extent is the found w stable? i.e., the bound of P (f (x);w) >=theta."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754700026,
            "cdate": 1698754700026,
            "tmdate": 1699636073540,
            "mdate": 1699636073540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rnvH1v9oRy",
                "forum": "Of2RhzJ8UJ",
                "replyto": "o2R1MgXjmd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer pTb7"
                    },
                    "comment": {
                        "value": "> What is the intuition of method design? At the least, the author should provide one figure to illustrate why the method is designed in this way and how the method works.\n\nWe have now added Figure 1 to the paper which tries to convey the main ideas behind the strategies. More detailed description is provided in Section 4. Also note that we had uploaded videos showing the working of the three strategies in our original submission.\n\n> Could the authors provide a real-world application to demonstrate the significance of the research?\n\nOne real-world application is *explanation reuse*. As mentioned in (Dhurandhar et al., 2019) (and also in our experience), given today's cloud-driven world where a model could exist on one cloud platform and explainability is offered as a service by a different corporation, each query to the black box model has an associated cost and hence finding explanations for many examples in a dataset can be prohibitive, not just in terms of time and network traffic but also monetarily. Finding regions where an existing explanation is valid can produce significant savings in all these three aspects, since one now does not have to find explanations for every example which typically takes $1000$s of queries (e.g. $5000$ is the default for LIME).\n\nAnother example is in finance where banks want to find cohorts where a certain action (related to credit cards, loans, etc.) might have similar effect on all members of the cohort. Our approach will provide such cohorts where a certain explanation is valid for the cohort and hence recourse may be possible. More applications can be found in  (Liao et al., 2022), where they surmise that stability of explanations, which will lead to such trust regions, is important for stakeholders performing tasks such as model improvement, domain learning, adapting control and capability assessment.\n\n> What are the pros and cons of the three sampling strategies? What kind of scenario are they suitable for and what insight do they give practitioners?\n\nAs mentioned in the Experiments section, the pro of unif is that it is the simplest and also sometimes the fastest strategy. unifI and adaptI are more complicated but are preferable when you have $1000$s or $10000$s of features respectively. Our advice would be to use unif for up to $100$s of features, unifI for $1000$s of features, and adaptI for $10000$s of features or greater.\n\n> The authors claimed the method is model agnostic and quite general. So can the method generalize to graph data?\n\nYes, the method can work for graph data if the graphs are embedded in some compact space. For example by vectorizing adjacency matrices or using embeddings from Graph Neural Networks (GNNs).\n\n> To what extent is the found $w$ stable? i.e., the bound of $P (f (x);w) >=\\theta$.\n\nAs seen in the experiments (see Tables 2-4 in the appendix), the found $w$ is quite stable from $Q=1000$. Even $Q=100$ is sufficient for low dimensions. The bounds are also quite tight from $Q=1000$ (see Tables 5, 7 in the appendix).\n\n> What is the purpose of ZO and why is it used as a baseline?\n\nAs mentioned in the related work, ZO is a standard approach used for black-box adversarial attacks. We adapted this method to our setup since ours is a novel problem in XAI with no prevalent baselines, where we thought an adapted version of ZO would be the closest and fairest baseline.\n\n> Why does the convergence of $w$ to a similar value indicate the accuracy of the explanation?\n\nFirst note that the convergence of $w$ relates to the accuracy of the estimated *trust region width* for a given threshold $\\theta$ and has nothing to do with the quality or accuracy of an *explanation*. For the synthetic experiments, we know the true $w$ and hence we can validate the different methods with certainty. For the real datasets, the true $w$ are unknown. Thus the different methods converging to similar values gives some confidence that the methods are returning an accurate enough $w$, as the mechanics behind the methods are quite different (especially between ZO and the others) and hence their converging to a similar value just by chance is unlikely."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006810647,
                "cdate": 1700006810647,
                "tmdate": 1700011700862,
                "mdate": 1700011700862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CXXo6EVaOB",
                "forum": "Of2RhzJ8UJ",
                "replyto": "rnvH1v9oRy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. I have reviewed your response along with other reviews and rebuttals. I will keep my current score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722330126,
                "cdate": 1700722330126,
                "tmdate": 1700722330126,
                "mdate": 1700722330126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ItMb4N6wVd",
            "forum": "Of2RhzJ8UJ",
            "replyto": "Of2RhzJ8UJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel certificate of explanability in a hyper-ball centered at a given sample that is model-agnostic. The authors provide rigorous formulation, solutions with theoretical guarantees and analysis, and experiments on many datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe novel formulation, adopting from certificate for adversarial robustness, is very interesting and could potentially be applied in practice\n2.\tThe analysis and explanation of the proposed methodologies, described in Algorithm 1 is very clear and comprehensive. \n3.\tThe theoretical analysis of the performance guarantees of Algorithm 1 and 2 is novel and useful for different strategies to explore the regions\n4.\tNumerical results are abundant, including large-scale images datasets and synthetic datasets."
                },
                "weaknesses": {
                    "value": "1.\tDespite that the authors mentioned in Section 8 Discussion that the l_\\infinty ball can potentially be generalized to hyper-rectangles or l_p balls, all these hyper-balls may not carry semantic meanings of the samples. Therefore even if we can certificate that the pseudo-samples within the hyper-balls have high fidelity, those pseudo-samples may not carry meaningful information, since human interpretation on images is very different from Euclidean distances. It is encouraged that the authors address this concern with a discussion. \n2.\tThe connection between the Algorithm 1 and Algorithm 2 proposed in Section 4 seems to be detached from the original optimization formulated in equation 1. It is encouraged that the authors strengthen the connection.\n3.\tThe authors mention several advantages of the novel explainability certificate, including stability of the explanations and explanation reuse, but did not provide numerical results or analysis on these benefits over existing explainable AI tools. It is encouraged that the authors \n4.\tThis certificate is closely related to adversarial examples, and in related literature there are also studies on the maximal hyper-balls that will not suffer from a decision change. Their algorithm is also similar to the searching algorithm proposed in Algorithm 1. The novelty here is the quality metric and the explanation function. It is encouraged that the authors include more discussion on different explanation functions, or give concrete examples in Section 2. \n5.\tIn XAI, the explainability of features is sometimes more important than individual samples. Is it possible to extend the techniques to features?\n6.\tOne of the most important aspects in XAI tools is visualization. It is encouraged that the authors add visualization of the regions, provide some intuitive explanations, with simple (semi-)synthetic data."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses. I will consider raising the scores if the authors could adequately address my questions in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802939701,
            "cdate": 1698802939701,
            "tmdate": 1699636073427,
            "mdate": 1699636073427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jAMyJUNQF3",
                "forum": "Of2RhzJ8UJ",
                "replyto": "ItMb4N6wVd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer KmSN"
                    },
                    "comment": {
                        "value": "> Despite that the authors mentioned in Section 8 Discussion that the $l_\\infty$ ball can potentially be generalized to hyper-rectangles or $l_p$ balls, all these hyper-balls may not carry semantic meanings of the samples. Therefore even if we can certificate that the pseudo-samples within the hyper-balls have high fidelity, those pseudo-samples may not carry meaningful information...\n\nYes you are right that all examples in a region may not be realistic input examples in the domain lying on the data manifold. In such cases one could after the region is found consider only realistic examples in the region for their downstream task. In a sense we would provide a superset of examples for which the explanation is valid. \n\nAnother option as discussed in Appendix G is one could simply apply our methods to the latent space\n(e.g. as learned by an auto-encoder) rather than the input space. Thus, although the regions will\nbe hypercubes in the latent space they will be more free-form in the input space which might be interesting. As such, our approaches should still apply.\n\n> The connection between the Algorithm 1 and Algorithm 2 proposed in Section 4 seems to be detached from the original optimization formulated in equation 1.\n\nAlgorithm 1 decides on a width $w$ and Algorithm 2 tries to certify that width (i.e. check if $f_{x_0}(x) \\ge \\theta \\quad \\forall x \\in B_\\infty(x_0, w)$). If certified, Algorithm 1 chooses a bigger width which again Algorithm 2 tries to certify. Thus, the two algorithms are tightly linked to the optimization formulated in eq. 1, where we are trying to find the maximum possible (certified) width.\n\n> It is encouraged that the authors include more discussion on different explanation functions, or give concrete examples in Section 2.\n\nExample explanation functions would be linear like in LIME, logistic regression, small decision trees, rule lists, interpretable neural architectures such as CoFrNet. We have now mentioned some of these in Section 2.\n\n> Is it possible to extend the techniques to features?\n\nA thing to note here is that when we find trust regions we are finding instances in the input space where an explanation, which are typically feature importances, is valid\nfor all of them. In other words, the feature importances hold for all examples in that region. Hence, we are highlighting important features in that region.\n\nHowever, if your goal is to find features that are similar one could potentially transpose the data matrix and find regions of similar features, where the explanations would now point to important instances.\n\n> It is encouraged that the authors add visualization of the regions, provide some intuitive explanations, with simple (semi-)synthetic data.\n\nWe have now added Figure 1 to the paper which tries to convey the main ideas behind the strategies. More detailed description is provided in Section 4. Also note that we had uploaded videos showing the working of the three strategies in our original submission.\n\n> The authors mention several advantages of the novel explainability certificate, including stability of the explanations and explanation reuse, but did not provide numerical results or analysis on these benefits over existing explainable AI tools.\n\nOur approach compliments the current work in XAI where XAI toolkits (viz. AIX360, Captum, InterpretML, etc.) contain predominantly explanation methods and/or (quality) metrics. One can take these methods and metrics and apply our approach on top to obtain trust regions, which could be used for any of the applications outlined in the introduction. As such in the experiments we do provide a comparison between LIME and SHAP using our methodology that is of a different flavor than previous works. This type of analysis can be used to compare and contrast XAI methods\non individual examples, on regions, as well as on entire datasets, and across different models. Also note that our methods perform quite well around $Q=1000$ where we obtain a region (i.e. a set of examples) in which an explanation is valid. This is (potentially) much fewer queries than say LIME-like methods which by default query a model $5000$ times per example it wants to find an explanation for."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006448861,
                "cdate": 1700006448861,
                "tmdate": 1700006549509,
                "mdate": 1700006549509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ms0j62jVdm",
                "forum": "Of2RhzJ8UJ",
                "replyto": "jAMyJUNQF3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their response. I will keep my current score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667642164,
                "cdate": 1700667642164,
                "tmdate": 1700667642164,
                "mdate": 1700667642164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CzCxSj1TDr",
            "forum": "Of2RhzJ8UJ",
            "replyto": "Of2RhzJ8UJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_46DG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_46DG"
            ],
            "content": {
                "summary": {
                    "value": "In the explainability problem, we are given a classifier model and an input, and we seek to find an explanation for the classification. This paper considers the problem of finding a certificate for explanations with respect to some input quality metric. The certificate(also called a trust region) is the largest hypercube centered at the example such that at least 1-\\delta of the points in the hypercube meet the quality criteria. The trust region has multiple applications, including explanation reuse and analysis of model behavior. \n\nThe paper first introduces the problem formally and then presents three techniques for certification, the simplest of which involves uniform sampling to determine the density of points that violate the quality metric. The other techniques refine the random sampling strategy for faster convergence.  The algorithms are implemented, and the experiments highlight that the certification technique is faster than comparable methods while providing roughly similar trust regions. An interesting insight from the experiments is that LIME trust regions can be larger than SHAP if the desired fidelity is on the lower side."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality and Significance: The idea of certification of explanations and its potential application to explanation reuse seems to be novel and quite interesting. The experimental evaluation is relevant, and the insights are valuable."
                },
                "weaknesses": {
                    "value": "I felt only the first of the three techniques is well motivated, while the other two don't really contribute to the main idea of the paper. Since the main contribution is a formalization of the novel problem, it would be nice to have a simple example to illustrate what a certification looks like.\n\nThere are also a few issues with the readability:\n1) What is fidelity? Perhaps the definition is not strictly necessary for understanding the paper, but since it is used so many times, I think it is critical to define it.\n2) the pseudo-code and the explanation in section 4 are not well written. I may be wrong, but the pseudo-code seems to be executing a binary search, in which case it is needlessly obscured."
                },
                "questions": {
                    "value": "For w1>w2>w3, it seems to be possible that hypercubes of halfwidth w1 and w3 could be valid trust regions while w2 isn't. Since the problem statement involves finding the largest certificate, would the certification really be meaningful around the example? For instance, if the explanation passes the quality threshold over most of the domain but is bad around important points, it seems that the certificate could just be the entire domain, revealing little about the point of interest. Is my understanding correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810441924,
            "cdate": 1698810441924,
            "tmdate": 1699636073341,
            "mdate": 1699636073341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QwAKzQeCb1",
                "forum": "Of2RhzJ8UJ",
                "replyto": "CzCxSj1TDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 46DG"
                    },
                    "comment": {
                        "value": "> I felt only the first of the three techniques is well motivated\n\nIn Section 4 last paragraph, we motivate the three strategies. As mentioned there and further referenced in Appendix D, we previously shared videos to provide more intuition about the methods. Now in the introduction we have added a figure to further clarify the working of these methods.\n\nAs such, unifI ($2^{\\text{nd}}$ technique) resembles a dynamic grid search where we randomly pick different locations in the current region and search around them. The $3^{\\text{rd}}$ technique adaptI does a dynamic adaptive search where we again randomly pick different locations in the current region and search around them, but additionally focus our search more and more around the locations that we find more promising points (a.k.a. low fidelity examples).\n\n> it would be nice to have a simple example to illustrate what a certification looks like.\n\nAs mentioned above we have now added Figure 1 to the paper which tries to convey the main ideas behind the strategies. More detailed description is provided in Section 4. Also note that we had uploaded videos showing the working of the three strategies in our original submission.\n\nCertification in Figure 1 would be our algorithm outputting $w=0.5$ with the (certification) probability based on Theorem 1 being $1$ for all three strategies.\n\n> What is fidelity?\n\nAs you rightly mentioned, the exact definition of fidelity is not critical to the exposition but, to improve clarity, we now in Section 2 point to eq. 14 in the appendix, where we have defined fidelity consistent with previous works (Dhurandhar et al., 2022; 2023; Ramamurthy et al., 2020).\n\n> ... but the pseudo-code seems to be executing a binary search, in which case it is needlessly obscured.\n\nAlgorithm 1 doubles or halves the width depending on if the current region was certified or not. It is similar to binary search but with subtle differences such as the doubling and also what upper bound width to set when a region is found to be invalid (i.e. the \"else\" statement). Algorithm 2 embodies the three strategies for certifying a fixed region and is very different than binary search.\n\n> For w1>w2>w3, it seems to be possible that hypercubes of halfwidth w1 and w3 could be valid trust regions while w2 isn't.\n\nThis is not possible. We are trying to find a connected (indeed, convex) region in the input space around a point where its explanation is valid for all examples within this region (with high probability). If w2 is found to be invalid our approach will return only w3 in this case.\n\n**A respectful request to reconsider:** We see that you expressed relatively low confidence (2) in your initial assessment. We hope that we have addressed what seems to be your main comment about having an illustrative example to clarify and contrast the three algorithms. We have also answered your other questions (fidelity, binary search, w1>w2>w3). In light of our response and the nature of your comments, we hope that you will reconsider your rating of 3 (reject). Many thanks in advance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700006167635,
                "cdate": 1700006167635,
                "tmdate": 1700009033938,
                "mdate": 1700009033938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FQ49yNvOtK",
            "forum": "Of2RhzJ8UJ",
            "replyto": "Of2RhzJ8UJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses an interesting question in the area of robust explanations: How do we find a region around a point such that an explanation remains more or less unchanged? Additionally, they enforce a constraint that one only has black-box access to the function f(x) which computes the fidelity of the local explanation of x0 at x and the evaluation of the model g(x). \n\nThe paper formalizes the problem statement. Then, they provide a strategy for addressing this problem that adaptively expands a bounding box around the point using samples in that region. They provide theoretical guarantees on the performance of their proposed strategy using probabilistic methods. They also provide experimental results on several datasets to demonstrate the efficacy of their approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is an interesting one from a mathematical standpoint. \nAn algorithm is proposed to find robust regions along with theoretical guarantees. \nIntroducing an adaptive strategy is a good idea.\nExperiments have been performed on a variety of datasets which include image and tabular datasets, as well as two popular explanation techniques namely LIME and SHAP.\n\nIt is interesting to connect it back to Lipschitz functions and also piecewise linear functions in the Appendix."
                },
                "weaknesses": {
                    "value": "There is insufficient motivation for this problem. For instance, why would one care for the explanations to remain the same in a region? For instance, if the model itself is undulating in a region, why should local explanations remain stable? The width of the robust region found would accordingly be large or small. How would this be informative? The motivation could be strengthened. Perhaps, a toy example could be useful.\n\nThe term black-box access is unclear since here they are not only accessing the model output but also its local explanation function. I would think black-box access typically means just computing the model output g(x), but additional functions are being computed here. So, calling it black-box access is a bit misleading.\n\nTo certify a single point itself, one requires many queries. Then, to certify multiple points, the query would significantly increase. Could the authors comment further on this computational complexity?\n\nAlso, in the experiments, it seems that this certification is being done for just one point (?) How do the results vary when trying to certify different points? Could you discuss more on the average over the entire dataset?\n\nThe experiments also do not show a uniform trend. What does that mean in this context? The experimental section needs some clarity on what should one expect to see.\n\nThe presentation of the algorithm can be significantly improved with much more clarity. The paper is quite difficult to read. Not much intuition is provided.\n\nAlso, closely related works in the area of counterfactual explanations (and some of the references therein) also talk about robust regions for counterfactual explanations.\n[1] Finding Regions of Counterfactual Explanations via Robust Optimization: https://arxiv.org/pdf/2301.11113.pdf\n[2] Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees: https://arxiv.org/pdf/2305.11997.pdf\n[3] Consistent counterfactuals for deep models: https://arxiv.org/abs/2110.03109"
                },
                "questions": {
                    "value": "Questions are provided along with the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1448/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88",
                        "ICLR.cc/2024/Conference/Submission1448/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811744321,
            "cdate": 1698811744321,
            "tmdate": 1700690339904,
            "mdate": 1700690339904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bKrKB6UkhF",
                "forum": "Of2RhzJ8UJ",
                "replyto": "FQ49yNvOtK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ot88 1/2"
                    },
                    "comment": {
                        "value": "> There is insufficient motivation for this problem. For instance, why would one care for the explanations to remain the same in a region?\n\nAs mentioned in the third paragraph of the introduction, there are multiple motivations for this work. We elaborate on some of them here.\n\n**Explanation reuse:** As mentioned in (Dhurandhar et al., 2019), given today's cloud-driven world where a model could exist on one cloud platform and explainability is offered as a service by a different corporation, each query to the black box model has an associated cost and hence finding explanations for many examples in a dataset can be prohibitive, not just in terms of time and network traffic but also monetarily. Finding regions where an explanation is valid can produce significant savings.\n\n**Insights for different stakeholders:** In (Liao et al., 2022)'s award winning work it was surmised through (AI and domain expert) user studies that stability of explanations is particularly important in recourse for stakeholders performing tasks such as model improvement, domain learning, adapting control and capability assessment. This is because without stability actions may produce unintended outcomes. Of course, as you mention, depending on the degree of non-linearity of the black box model, the sizes of the regions in which these explanations are valid will change, but this is precisely where approaches like ours would be useful in informing the user of how broadly the provided explanation would be applicable.\n\n**Meta-metric:** Given a quality metric (viz. fidelity, stability, etc.) that the user cares about, our approach could be used to ascertain which explanation method obeys the quality metric better, which would correspond to larger trust regions (on average) returned by our method.\n\n> The term black-box access is unclear since here they are not only accessing the model output but also its local explanation function.\n\nAccess to the model is only through querying, a.k.a. black-box access as it is called in the literature. The local explanation function is typically separate from the model (e.g. LIME, SHAP, etc.). In fact, it could even be just feature attributions provided through some unknown process (viz. domain knowledge from a subject matter expert). Our approach works irrespective of how the explanation is obtained, and thus accesses not only the (black-box) model but also the explanation function only through querying. This justifies in our opinion the use of the term black-box. \n\n> ... to certify multiple points, the query would significantly increase. Could the authors comment further on this computational complexity?\n\nWe see from the experiments that $Q=1000$ typically gives accurate enough regions. Also note that once we obtain a region, we would not certify other points within that region since we already know that the computed explanation is valid for them. This would reduce the number of queries. More importantly, LIME-like methods (by default) query around $5000$ times to obtain an explanation for a single point and are heavily used despite this cost. In contrast, our approach could potentially find an explanation for a bunch of points (those lying in the region) with say $Q=1000$.\n\n> How do the results vary when trying to certify different points? Could you discuss more on the average over the entire dataset?\n\nIn Appendix F we report results with more points. The results will of course vary depending on the point and the behavior of the black-box model around it. *The key point here is not that the region sizes will vary but that we have provided a mechanism to find these regions for a wide range of explanation methods, quality metrics (with their thresholds) and black-box models.* The averages over a dataset will again depend on the non-linearity of the black-box model, the stability of the explanation method and the threshold set for the quality metric. For instance, for FICO using Boosted Trees, we find that for $\\theta=0.75$ LIME produces larger regions than SHAP (on average) and hence is possibly more favorable at this threshold, but for a larger threshold such as $\\theta=0.9$ SHAP is more favorable.\n\n> The experimental section needs some clarity on what should one expect to see.\n\nAs mentioned in the first line of Section 7, we are trying to show through the experiments that our methods recover accurate regions and do so more efficiently than say adapted ZO approaches. We are also trying to show that our bounds derived in Section 5 are computable (see Tables 5,6,7 in the appendix). As discussed in the last paragraph of the \"Observations\" section, we also show how our approach can be used to compare explanations methods."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005811426,
                "cdate": 1700005811426,
                "tmdate": 1700005811426,
                "mdate": 1700005811426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChMEmbaUl6",
                "forum": "Of2RhzJ8UJ",
                "replyto": "5vWAemW0uj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
                ],
                "content": {
                    "title": {
                        "value": "Increased Score"
                    },
                    "comment": {
                        "value": "Based on the responses, I have increased my score to Accept."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1448/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690391040,
                "cdate": 1700690391040,
                "tmdate": 1700690391040,
                "mdate": 1700690391040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]