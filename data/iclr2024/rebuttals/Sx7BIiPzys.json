[
    {
        "title": "Variational Bayesian Last Layers"
    },
    {
        "review": {
            "id": "2zHIwsqRN5",
            "forum": "Sx7BIiPzys",
            "replyto": "Sx7BIiPzys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_b26R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_b26R"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Variational Bayesian Last layers, a technique to perform uncertainty estimation in standard neural network architectures. \nThe method performs bayesian learning only for the last layer in the neural networks using Variational Inference.\nThis results in a scalable and simple technique that shows strong performances in standard benchmarks for regression and classification."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. I found the paper very interesting and easy to read. Uncertainty estimation is an important and active research area in deep learning.\n\n1. To the best of my knowledge, the idea is novel (although not groundbreaking)\n\n1. The method is scalable, simple to implement in standard architectures, and achieves very competitive performances (especially considering its simplicity)\n\n1. While being Bayesian only on the last neural network layer the method is in principle not as powerful as other techniques, as the authors rightly claim simpler methods that are easy to implement are what is being more commonly used in practice (e.g. Bayesian dropout, stochastic weight averaging)\n\n1. The appendix is extensive and addresses all the details I felt were missing in the main paper"
                },
                "weaknesses": {
                    "value": "I did not identify any major weaknesses, only some points for improvement\n\n1. To increase the impact of the paper you need to make sure that people that are not too familiar with VI are able to easily implement the paper. This means:\n    1. Make the code publicly available, especially to show how to best implement the \"mixed parameterization\" discussed in appendix D \n    1. set good default hyperparameters\n\n1. It would be useful to draw the graphical models of the models presented in Section 2, to help the reader visualize the random variables in play and their (hyper)priors/parameters\n\n1. In case you need more space in the paper, I would move to the appendix some of the details on the generative classification model, especially considering the poorer performances."
                },
                "questions": {
                    "value": "None, aside from the minor points presented in the weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698087696436,
            "cdate": 1698087696436,
            "tmdate": 1699636626735,
            "mdate": 1699636626735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9wBsLNWh0E",
                "forum": "Sx7BIiPzys",
                "replyto": "2zHIwsqRN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very positive feedback. \n\n> To increase the impact of the paper you need to make sure that people that are not too familiar with VI are able to easily implement the paper. This means:\n> Make the code publicly available, especially to show how to best implement the \"mixed parameterization\" discussed in appendix D\n> set good default hyperparameters\n\nWe are currently finalizing public releases of our code in both pytorch and jax, which will be released in time for the camera-ready version of the paper. \n\n> It would be useful to draw the graphical models of the models presented in Section 2, to help the reader visualize the random variables in play and their (hyper)priors/parameters\n> In case you need more space in the paper, I would move to the appendix some of the details on the generative classification model, especially considering the poorer performances.\n\nWe thank the reviewer for these points. We have moved some details to the appendix, as noted."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635010102,
                "cdate": 1700635010102,
                "tmdate": 1700635010102,
                "mdate": 1700635010102,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mLGkQTisNv",
            "forum": "Sx7BIiPzys",
            "replyto": "Sx7BIiPzys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_tT7z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_tT7z"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives efficient optimization objectives of the posterior distribution over the parameters of the last layer of neural networks for common machine learning applications such as classification, regression, and generative classification. Theoretically, their variational inference objective functions are derived in closed form and therefore enjoy the property of not requiring sampling - meaning the cost of enabling uncertainty quantification for a broad class of deep learning architectures is marginal (parameters other than the last layer are learned by maximum a posteriori). Experimentally, they validate these novel variational inference algorithms using standard benchmarks from UCI, a large language model used for sentiment analysis, and an image classification problem."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is an excellent paper and a significant contribution - well done! The authors make clear how they build on the existing literature in Bayesian deep learning to create a novel advance that is practical and easy to implement. This is significant and should enable more work to push the frontier of the \"best of both worlds\", with neural networks serving as function approximators and Bayesian methods enabling sample-efficiency and quantification of uncertainty that is required for practical deployment of deep learning."
                },
                "weaknesses": {
                    "value": "Visualizations of how tight or loose the bounds in the main text could help build more intuition; comparisons in terms of speed or efficiency to variational inference algorithms that do require sampling (such as Monte-Carlo objectives like VIMCO) could also help guide practitioners in making the correct trade-off depending on FLOPs of compute available versus the required accuracy of posterior approximation/uncertainty quantification."
                },
                "questions": {
                    "value": "For the Resnet image recognition and sentiment analysis experiments, what was the additional compute required (or time taken per iteration, if available)? The sample-efficiency is great, and understanding the practical overhead rather than theoretical complexity would be great for larger models that are in broad use."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698335782709,
            "cdate": 1698335782709,
            "tmdate": 1699636626603,
            "mdate": 1699636626603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hTEnYj57sA",
                "forum": "Sx7BIiPzys",
                "replyto": "mLGkQTisNv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very positive comments. \n\n> Visualizations of how tight or loose the bounds in the main text could help build more intuition; comparisons in terms of speed or efficiency to variational inference algorithms that do require sampling (such as Monte-Carlo objectives like VIMCO) could also help guide practitioners in making the correct trade-off depending on FLOPs of compute available versus the required accuracy of posterior approximation/uncertainty quantification.\n\nWe conducted experiments to investigate tighter bounds on the softmax (based on Knowles and Minka, 2011), which did not meaningfully change performance. Thus, we believe that (surprisingly) the tightness of the bound does not meaningfully impact performance, and do not emphasize it. We believe complexity/run time versus performance is a more meaningful metric which we emphasize. \n\n> For the Resnet image recognition and sentiment analysis experiments, what was the additional compute required (or time taken per iteration, if available)? The sample-efficiency is great, and understanding the practical overhead rather than theoretical complexity would be great for larger models that are in broad use.\n\nWe have included the runtime for our model compared to a standard model in for the ResNet."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634995987,
                "cdate": 1700634995987,
                "tmdate": 1700634995987,
                "mdate": 1700634995987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QLTuoubpKN",
            "forum": "Sx7BIiPzys",
            "replyto": "Sx7BIiPzys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_vugz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_vugz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel way to learn Bayesian last layer (BLL) networks by deriving a sampling-free approach for optimizing normal, categorical, and generative classification likelihoods,\nby relying on variational inference techniques. \nThe approach is then evaluated on a series of regression and classification tasks against a range of baselines.\n\n\n_____\n_Edit: Given the improved presentation and evaluation, I increased my score._"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "BLL networks are an interesting approach to solve the scalability problem Bayesian neural networks tend to suffer from. \nThe paper introduces another variation to this family of approaches that is relatively straightforward, easy to understand, and implement.\nThe method is properly evaluated as the number of experimental setups is reasonably extensive both with respect to architectures and experimental tasks."
                },
                "weaknesses": {
                    "value": "Straight-forward contributions can be seen both as a strength and as a weakness depending on the situation. \nThey are a strength if they are an easy solution to a complex problem that might not improve upon current approaches in all situations, but most. \nThey are a weakness if they do not provide a clear theoretical benefit above current approaches and also come without clear performance improvements.\nFor me, the results point to the latter case as they are rather mixed despite some strong wordings of the authors in their claims. \n\nThe abstract promises \"improve[d] predictive accuracy, calibration, and out of distribution detection over baselines.\", similarly in the contributions, and conclusion \nparts of the paper. Even more, the method not only improves, but it also performs \"extremely strong\" and \"exceptionally strong\".  \nThese are some exceptionally strong statements given the actual performance.  \n\nFocusing on each of the experiments in turn. The first problem is the presentation, e.g., what does a bold number mean (see question below)?\n\n_Regression Experiments._ Of \nthe six data sets (see question below on this number) the proposal improves on two, slightly on one, equally on two (although better than neural net-based baselines), and worse than most of its baselines in the final one. Calling this \"strong performance\" is rather misleading. Two additional, though potentially minor, problems are that all of the baselines are simply cited from prior work (Watson et al., 2021). Given the wide performance variations between different train/test splits that can be observed for various UCI data sets the results are not entirely trustworthy. (Note that the reported error intervals are most likely standard errors, as is common on UCI, instead of standard deviations. But which they are is never specified.)\nSecondly, the authors acknowledge in the appendix that there might be differences in the way the training data is normalized compared to the cited results. \n(Whether these problems strongly influence the results, or bias them in favor or against VBLL is unclear.)   \n\n_Classification._\nWhile \"Extremely strong accuracy results\" are mentioned, it just performs as well or worse than competitive baselines like Laplace or Ensembles. The same for ECE, NLL, OOD detection.  where \"exceptionally strong performance\" is claimed.\n\nThe method is somewhat simpler than baselines, but it lacks a convincing argument for why this should matter. As the authors advertise this simplicity, there should be additional results on practical runtime improvements compared to the baselines to provide some evidence for the claim that a reader should use this approach. \n\n### Minor\n- Dropout is first claimed to have a \"substantially higher computational cost\" (Sec 1) and appears later as a \"comparatively inexpensive\" method (Sec 4).  Additional forward passes at test time are indeed rather inexpensive instead of a high computational cost.\n- When submitting to ICLR please make sure that you follow the ICLR style guide. E.g., Table captions belong above tables, not below.\n\n### Typos\n- Sec 5.2 first par: \"We refer to this model **as** G-VBLL-MAP...\"\n- (6): $y|\\rho$"
                },
                "questions": {
                    "value": "- Why was this specific subset of six UCI data sets chosen? The original work by Hern\u00e1ndez-Lobato and Adams (2015), who introduced this set of experiments had ten, and even Watson et al. (2021) who the authors cite as relying on for their setup used ~~seven~~ different sets. _(PostRebuttal Edit: I misread the reference, Watson et al. use the full set of experiments.)_\n- Can the authors provide further results on the empirical runtime of the proposed approach, not just a theoretical one?\n- What was the principle according to which average numbers are bolded? E.g., in Energy RMSE a huge range of means is bold (from 0.39 to 0.47), but 0.43 is missing;  CIFAR-100 AUC has the same pattern, huge range, some missing, etc. \n- (very minor) What is the irony in BLL methods being popular (Sec 2.4)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Reviewer_vugz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420209714,
            "cdate": 1698420209714,
            "tmdate": 1700753979545,
            "mdate": 1700753979545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dNZFkiQjQh",
                "forum": "Sx7BIiPzys",
                "replyto": "QLTuoubpKN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. \n\n> The abstract promises \"improve[d] predictive accuracy, calibration, and out of distribution detection over baselines.\", similarly in the contributions, and conclusion parts of the paper. Even more, the method not only improves, but it also performs \"extremely strong\" and \"exceptionally strong\". These are some exceptionally strong statements given the actual performance.\nFocusing on each of the experiments in turn. The first problem is the presentation, e.g., what does a bold number mean (see question below)?\n\nWe understand that you feel the claimed contributions of the method are stated too strongly. We have revised our stated contributions to be more moderate. \n\n> Regression Experiments. Of the six data sets (see question below on this number) the proposal improves on two, slightly on one, equally on two (although better than neural net-based baselines), and worse than most of its baselines in the final one. Calling this \"strong performance\" is rather misleading. Two additional, though potentially minor, problems are that all of the baselines are simply cited from prior work (Watson et al., 2021). Given the wide performance variations between different train/test splits that can be observed for various UCI data sets the results are not entirely trustworthy. (Note that the reported error intervals are most likely standard errors, as is common on UCI, instead of standard deviations. But which they are is never specified.) Secondly, the authors acknowledge in the appendix that there might be differences in the way the training data is normalized compared to the cited results. (Whether these problems strongly influence the results, or bias them in favor or against VBLL is unclear.)\n\nWe partially disagree with the reviewer about the strength of the method on regression experiments. The cases where our approach outperforms baselines, the outperformance is substantial. Otherwise, the method typically performs on par with the previous state of the art. Indeed, we believe our method is one of the largest jumps in performance on this task reported. Moreover, we note competitive baselines such as RBF GP and Ensembles are dramatically more expensive than our method. \n\nWhile we report baselines from (Watson et al. 2021), we note several things. First, part of the reason we built upon their results is the robustness of their experimental procedure. They perform 20 trials in which the train/val/test sets are chosen randomly, and the val set is used to automatically select the termination point of training. We follow this procedure in our experiments. On the topic of normalization, we believe our normalization matches that used in (Watson et al. 2021). In particular, they say that they use a zero-mean, unit-covariance prior \u201cin whitened data space\u201d. It is unclear if this denotes a direct transformation of their data (which would potentially skew reported NLLs) or indicates that prior terms are transformed to match the data. We center the data to be zero-mean, and normalize features to unit covariance in the input to the network, but do not re-scale the labels. We combine our re-scaling of the data with a zero-mean prior. Thus, comparing these strategies, we believe the practical effects of our data processing are nearly identical, although the effect of normalizing feature inputs to the network may vary. \n\n> Classification. While \"Extremely strong accuracy results\" are mentioned, it just performs as well or worse than competitive baselines like Laplace or Ensembles. The same for ECE, NLL, OOD detection. where \"exceptionally strong performance\" is claimed.\nThe method is somewhat simpler than baselines, but it lacks a convincing argument for why this should matter. As the authors advertise this simplicity, there should be additional results on practical runtime improvements compared to the baselines to provide some evidence for the claim that a reader should use this approach.\n\nWe have weakened our claims. While we do not strictly dominate in all metrics, the performance of our approach is highly competitive, which is especially notable given the simplicity of the approach. \n\nDirect comparison of runtimes for each model is difficult, because some methods are post-hoc only, some require a per-epoch step, and some require a change to each training step. Thus, timing depends on other training details such as batch size. We have included runtimes for our method in comparison to standard (non-Bayesian) network evaluation, and expanded our discussion of complexity for baseline methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634957863,
                "cdate": 1700634957863,
                "tmdate": 1700634957863,
                "mdate": 1700634957863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5MLJrut1uF",
            "forum": "Sx7BIiPzys",
            "replyto": "Sx7BIiPzys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_tM8T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5903/Reviewer_tM8T"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce variational Bayesian last layers as a novel approach for approximate inference in Bayesian deep learning models. The main contribution is three-fold: (i) following the current trend in Bayesian deep learning the authors propose to use a variational approximation to the last-layer posterior, (ii) the authors introduce closed-form bounds on the ELBO for different likelihood functions, (iii) the authors show that the simple approach can result in improved performance for regression and some classification tasks.\n\n--\n\nI have adjusted my score based on the author's response."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow in most parts. Moreover, the work is well-motivated and I enjoyed that the authors brought back old ideas to the BDL community, e.g., using the discriminant analysis as a likelihood model.\n2. I believe the exposition of the method is well done in most places, though slightly dense here and there, and helped in understanding the general idea of the proposed method. Moreover, I believe that the method is correct and an interesting contribution to the field. I think it is important to see more work on deterministic approaches to uncertainty quantification in deep learning. \n3. The experimental section shows promising results, especially in the case of regression."
                },
                "weaknesses": {
                    "value": "Overall: My main concern with the paper is the weak empirical evaluation and limited novelty of the work, that is, it seems it is essentially an application of known techniques to the special case of last-layer posteriors.\n\nComments:\n1. Section 2.4 lists various related works, which I believe the author claims to optimize the log marginal via gradient descent. I have not checked every citation, but it appears to me that this statement is false for at least a subset of the cited papers. It might be good to revise the exposition.\n2. Eq 12 is some weighted ELBO, weighted with T for the purpose of generality, according to the authors. However, T never seems to be used later and makes the connection to the common ELBO less transparent. I believe the paper would improve in clarity if T is dropped.\n3. Section 3.4 is very dense and it could help the reader if this section is improved in its presentation. \n4. For the experiments, I would have expected assessments under distributional shift, a comparison to recent deterministic approaches (e.g., Zeng et al 2023 or Dhawan et al 2023), and a large-scale application of the approach as it acts on the last-layer only and should be applicable in more realistic scenarios (e.g., ImageNet).\n\n\nMinor:\n- Page 3, Eq 11 cites \"Harrison et al 2018\", which I looked up but didn't find any relevant content that would discuss the use of the marginal in Bayesian deep learning as a standard objective. What is the reason for the citation?\n\nCitations:\n- [Zeng et al 2023] Zeng et al, \"Collapsed Inference for Bayesian Deep Learning\", NeurIPS 2023.\n- [Dhawan et al 2023] Dhawan et al, \"Efficient Parametric Approximations of Neural Network Function Space Distance\", ICML 2023."
                },
                "questions": {
                    "value": "1. Eq. 14 uses a rather loose bound, is it possible that this is the reason the approach underperforms in the classification settings compared to the regression setting? If so, is there any way to obtain a tighter bound?\n2. How does the method perform if it is used only as a post-hoc approach, meaning, without adaptation of the feature map? In large-scale applications, this is a particularly relevant setting and the proposed method could be a promising plug-in replacement.\n3. From what I understand T is never actually used. Is this correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5903/Reviewer_tM8T"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5903/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709043168,
            "cdate": 1698709043168,
            "tmdate": 1700704211627,
            "mdate": 1700704211627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4JIqb0Zo4g",
                "forum": "Sx7BIiPzys",
                "replyto": "5MLJrut1uF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5903/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, which we address point-by-point. \n\n> Overall: My main concern with the paper is the weak empirical evaluation and limited novelty of the work, that is, it seems it is essentially an application of known techniques to the special case of last-layer posteriors.\n\nWe agree with the review that the methodological novelty may appear limited compared to other papers on Bayesian deep learning. However, our fundamental motivation is that the recent history of machine learning has made clear the importance of building inexpensive but effective methods, and the importance of nailing the details in methods. We believe that the depth of our work\u2013derivation of novel deterministic training objectives, experimental analysis, treatment of complexity and parameterization, and analysis of hyperparameters\u2013is critical to building useful systems that may be deployed at scale. \n\nWe have added several elements to the experimental evaluation. In particular, we have added an evaluation of post-training methods (in which the VBLL is added to a model with frozen features) and added a bandit problem, in which VBLLs show extremely strong performance.\n\n> Section 2.4 lists various related works, which I believe the author claims to optimize the log marginal via gradient descent. I have not checked every citation, but it appears to me that this statement is false for at least a subset of the cited papers. It might be good to revise the exposition.\n\nWe emphasize that the list of papers in section 2.4 are papers in which Bayesian last layer models have been used. Not all of these papers used the marginal likelihood objective for training these models; we have clarified this in the text. \n\n> Eq 12 is some weighted ELBO, weighted with T for the purpose of generality, according to the authors. However, T never seems to be used later and makes the connection to the common ELBO less transparent. I believe the paper would improve in clarity if T is dropped.\n\nThe goal of including the factor of 1/T is to yield an objective that is more amenable to standard minibatch optimization, as is used in standard supervised learning. This is especially important to highlight when aggregation across minibatches should be done by averaging (as opposed to summation) and to weight the regularization terms. We have highlighted the importance of this. \n\n> Section 3.4 is very dense and it could help the reader if this section is improved in its presentation.\n\nWe have made a collection of changes to the writing in this section to improve presentation, including splitting this section into two sections: one on training objectives and one on prediction and OOD.\n\n> For the experiments, I would have expected assessments under distributional shift, a comparison to recent deterministic approaches (e.g., Zeng et al 2023 or Dhawan et al 2023), and a large-scale application of the approach as it acts on the last-layer only and should be applicable in more realistic scenarios (e.g., ImageNet).\n\nWe emphasize that, in terms of model parameter count, the wide ResNets are relatively large (~40M) parameters. Indeed, they are much larger compared to non-wide ResNets. We prioritized these experiments to enable comparison to established baselines such as SNGP. To include experiments on scalability, we also included the experiments in which we trained models on LLM features. \n\nTo include experiments that are more similar to distribution shift (and also include elements of active learning to investigate the usefulness of the uncertainty quantification) we included bandit experiments, as described in the response to all reviewers. \n\n> Page 3, Eq 11 cites \"Harrison et al 2018\", which I looked up but didn't find any relevant content that would discuss the use of the marginal in Bayesian deep learning as a standard objective. What is the reason for the citation?\n\n[Harrison ea 2018] trains on the marginal likelihood, computed analytically in the regression case by iterating over the data, in the multi-task/few-shot learning setting. It also uses a Bayesian last layer approach that is structurally the same as our regression model."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634906285,
                "cdate": 1700634906285,
                "tmdate": 1700634906285,
                "mdate": 1700634906285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PlDMeHepzo",
                "forum": "Sx7BIiPzys",
                "replyto": "SxDjhQuwI0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5903/Reviewer_tM8T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5903/Reviewer_tM8T"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed response and for answering my questions/concerns. Consequently, I have changed my score to reflect the rebuttal and advocate for accepting the work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5903/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704280414,
                "cdate": 1700704280414,
                "tmdate": 1700704280414,
                "mdate": 1700704280414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]