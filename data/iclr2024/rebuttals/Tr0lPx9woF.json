[
    {
        "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models"
    },
    {
        "review": {
            "id": "WscZXqGGIZ",
            "forum": "Tr0lPx9woF",
            "replyto": "Tr0lPx9woF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a post-training N:M pruning solution for LLMs. The method is built upon two components: (1) relative importance and activation, which considers both the weights and activations within LLMs for a better weight importance estimation; and (2) channel permutation, which can better preserve important weights under n:m sparsity through rearranging channel orders. Experiment results demonstrate the effectiveness of the proposed method and its superiority over existing baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The channel permutation and the Hungarian algorithm are novel techniques, and the experiments demonstrate the effectiveness under n:m sparsity\n2. The paper is easy to follow, and the authors conduct extensive experiments."
                },
                "weaknesses": {
                    "value": "1. The first technique, relative importance, and activation, seems to be an incremental improvement. \n2. In Section 5.3, which discusses N:M sparsity, I was anticipating experimental results on smaller LLMs such as Llama2-7b, and a higher sparsity ratio than 50%. This would potentially highlight the advantages of the proposed method over SparseGPT and Wanda more effectively. Could the authors provide their insights on this?\n3. Regarding the inference latency under n:m sparsity, I would like to suggest that instead of providing layer-wise speedup, the authors could consider providing end-to-end latency for the pruned LLMs. My reason for this suggestion is that I am curious about whether n:m sparsity is indeed an effective structured pruning pattern within LLMs, especially when compared to pure structured pruning methods such as LLM-Pruner. Could the authors provide their perspective on this?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717386534,
            "cdate": 1698717386534,
            "tmdate": 1700640732856,
            "mdate": 1700640732856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jhOP5InYkb",
                "forum": "Tr0lPx9woF",
                "replyto": "WscZXqGGIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hn9p,\n\nThank you for your insightful feedback and positive evaluation of our manuscript. We appreciate your recognition of the strengths of our work, particularly the novelty of our channel permutation technique and the comprehensiveness of our experimental evaluation. We address your concerns and questions below:\n\n**Weakness 1: The first technique, relative importance, and activation, seems to be an incremental improvement.**\n\n**Reply:** Thank you for your comments. We acknowledge that the improvement of RIA over prior work is modest, around 0.1 - 0.2 points. However, it is important to establish the baseline for this evaluation. If the baseline is zero, an improvement of 0.1-0.2 points may seem negligible. Yet, our objective is to recover the performance of the dense model after pruning. Therefore, the relevant improvement should be measured in relation to the performance drop from the dense model. As reported in our article: \"Notably, our method achieves a 50% improvement in mitigating the performance drop of the Dense model compared to SparseGPT (16% in LLaMA and LLaMA2 model family), and a 17% improvement compared to Wanda (13% in LLaMA and LLaMA2 model family).\" We believe that any improvement over 10% should not be considered incremental, and this improvement is consistent across all models in our study.\n\nWe apologize for not providing a detailed formula to compute this percentage, which may have led to confusion. To address the reviewer's concern, we have added an explanation of this formula in Appendix J of the newly uploaded manuscript. Here is a simplified version for clarity:\n\nGiven two post-training pruning methods, A and B, with perplexities P(A) and P(B), and the baseline perplexity P(D) of the dense network, the percentage by which method B prevents the performance drop compared to method A can be computed as $\\frac{P(A) - P(B)}{P(A) - P(D)}$. This formula provides a fair evaluation of the newly proposed method compared to previous methods, considering the dense model's performance in the post-training pruning field. Using this calculation, the improvement of RIA in preventing performance drops compared to Wanda is 17% across all models, and 50% compared to SparseGPT across all models.\n\n**Weakness 2:** In Section 5.3, which discusses N:M sparsity, I was anticipating experimental results on smaller LLMs such as Llama2-7b, and a higher sparsity ratio than 50%. This would potentially highlight the advantages of the proposed method over SparseGPT and Wanda more effectively. Could the authors provide their insights on this?\n\n**Reply:**  Thank you for your suggestion. In response, we have conducted additional experiments with varying sparsity levels of N:M sparsity \u2014 25% (1:4 sparsity), 50% (2:4 sparsity), and 75% (3:4 sparsity) \u2014 on the Llama2-7b model with the default setting in our article. \n\nTo explore this further, we compared two pruning strategies: pruning from layer 4 to layer 32 which is the solution proposed by LLM-pruner to prevent the performance drop and pruning all layers. Our findings confirm that preserving the first and last layers indeed mitigates performance degradation.\n\nOur results indicate that at a 75% sparsity level, there is a significant decline in performance. Although in comparison to Wanda, there is a significant improvement in RIA, the perplexity goes too high to be implemented in real cases. \n\nHere is a summary of our results:\n\n**Pruning from layer 4 to layer 30:**\n|                               | 1:4 (25% sparsity) | 2:4 (50% sparsity) | 3:4 (75% sparsity) |\n| ----------------------------- | ------------------ | ------------------ | ------------------ |\n| LLM-Pruner (structured)       | 26.04              | 77.90              | 464.34             |\n| Activation-based (structured) | 30.23              | 525.5              | 8692.2             |\n| Wanda (semi-structured)       | 5.84               | 9.21               | 432.6              |\n| RIA (semi-structured)         | 5.82               | 8.91               | 442.6              |\n| RIA + CP (semi-structured)    | 5.74               | 8.03               | 436.1              |\n\n**Pruning all the layers:**\n\n|                               | 1:4 (25% sparsity) | 2:4 (50% sparsity) | 3:4 (75% sparsity) |\n| ----------------------------- | ------------------ | ------------------ | ------------------ |\n| LLM-Pruner (structured)       | 28.88              | nan                | 13570              |\n| Activation-based (structured) | 39.54              | 10467              | nan                |\n| Wanda (semi-structured)       | 5.94               | 12.15              | 2863.3             |\n| RIA (semi-structured)         | 5.90               | 11.27              | 1891.13            |\n| RIA + CP (semi-structured)    | 5.81               | 10.12              | 1532.72            |\n\n[1] LLM-Pruner: On the Structural Pruning of Large Language Models, NeurIPS 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572125183,
                "cdate": 1700572125183,
                "tmdate": 1700572125183,
                "mdate": 1700572125183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i7iTsw2okY",
                "forum": "Tr0lPx9woF",
                "replyto": "WscZXqGGIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4 (2/2)"
                    },
                    "comment": {
                        "value": "**Weakness 3:** Regarding the inference latency under n:m sparsity, I would like to suggest that instead of providing layer-wise speedup, the authors could consider providing end-to-end latency for the pruned LLMs. My reason for this suggestion is that I am curious about whether n:m sparsity is indeed an effective structured pruning pattern within LLMs, especially when compared to pure structured pruning methods such as LLM-Pruner. Could the authors provide their perspective on this?\n\n**Reply:** Thank you for your valuable suggestions and questions. We separated your question into two parts:\n\n1) The performance of N:M sparsity and structured sparsity. We adopted LLM-pruner as the representative structured pruning method to compare with our methods. The results are shown in the above reply. We use the detailed hyperparameter setting: C4 with 128 samples as calibration data, and Wikitext2 as the evaluation data. The PPL of structured pruning (LLM-pruner) increases significantly with just 25% sparsity. This suggests that structured pruning is not effective in post-training pruning scenarios, as the pruned information from the entire channels cannot be recovered without retraining or fine-tuning. Also, To further address the reviewer's concern, we add **Appendix E** to explain this phenomenon in detail.\n\n\n2. We here offer the inference latency of the structured pruned model with 50% sparsity and 2:4 sparsity model. Our experiment is on LLaMA2-7b, the sequence length of the input is 12. The deployment is on 2 Nvidia A100 80GB in parallel on Ubuntu 22.04.1 system. Here, we offer the inference latency with varying batch sizes.\n\n|                         | 1         | 8         | 16        | 64                 |\n| ----------------------- | --------- | --------- | --------- | ------------------ |\n| dense                   | 281.13 ms | 547.82 ms | 1011.81ms | 3742.76 ms |\n| structured 50% sparsity | 238.92 ms | 326.14 ms | 616.13 ms | 2181.69 ms         |\n| 2:4 sparsity            | 225.60 ms | 357.48 ms | 731.81 ms | 2495.41 ms         |\n\nThe table shows that structural 50% sparsity marginally outperforms the 2:4 sparsity model as batch sizes increase. It's important to note that when dealing with smaller input batches, both structured 50% sparsity and 2:4 sparsity models do not significantly boost inference speed. However, as the batch size grows, the acceleration for 2:4 sparsity approximates 1.5x, whereas the structured 50% sparsity tends to reach about 1.7x acceleration.\n\nIn conclusion, given the performance and the inference latency analysis, although structured pruning could provide a faster acceleration than N:M sparsity with the same sparsity level, the performance drop is unignorable even when the sparsity is very low. Therefore, in the current situation, the N:M constraint sparsity is the only sparsity pattern that strikes the balance between performance and acceleration speed. \n\nWe thank the suggestion the Reviewer gave and we added this part in Appendix E to offer a comprehensive comparison of N:M sparsity and structured sparsity."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573342798,
                "cdate": 1700573342798,
                "tmdate": 1700573651801,
                "mdate": 1700573651801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KJS1x3KykN",
                "forum": "Tr0lPx9woF",
                "replyto": "i7iTsw2okY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. It addresses most of my concerns. I will improve the rating score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640717542,
                "cdate": 1700640717542,
                "tmdate": 1700640717542,
                "mdate": 1700640717542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ee64xYOgiK",
                "forum": "Tr0lPx9woF",
                "replyto": "WscZXqGGIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for increasing your score"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe are happy to hear that your concerns have been addressed. Thanks for your support!\n\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688005490,
                "cdate": 1700688005490,
                "tmdate": 1700688030141,
                "mdate": 1700688030141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gm3A8gSp0e",
            "forum": "Tr0lPx9woF",
            "replyto": "Tr0lPx9woF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
            ],
            "content": {
                "summary": {
                    "value": "* The paper introduces a relative importance pruning metric which leads to more uniform pruning patterns.\n* The paper proposes to apply channel permutations found by a scalable heuristic in order to relax the pattern restrictions imposed by n:m pruning.\n* Both techniques are evaluated on Llama models for perplexity and zero-shot tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed techniques are relatively simple to implement in practice and in particular the channel reordering seems to be quite effective.\n* The paper is easy to understand and provides clear visualizations of the key algorithms.\n* Evaluation is carried out on strong Llama models and not just on older OPT ones.\n* The Appendix contains interesting additional studies like combining RIA with SparseGPT reconstruction.\n* The paper also considers practical acceleration of sparse models in Table 5."
                },
                "weaknesses": {
                    "value": "* The observation that encouraging a more uniform sparsity pattern is beneficial was also made by Wanda, RIA seems to be an extension of that (also across columns). Similarly, that permutation reordering can be helpful for n:m sparsity was found by [Pool & Yu, 2021], this paper only introduces a simpler but more scalable heuristic for finding such a permutation, based on average activation values. While there is some novelty, it is not particularly high.\n* For unstructured sparsity, the improvements of RIA over prior work are relatively small at around 0.1-0.2 points in perplexity. The impact of the more advanced linear sum assignment permutation method also seems rather minor. At the same time, perplexity increases from the dense baseline are still quite large, especially for 2:4. Hence, it is not clear how useful the corresponding sparse models would be in practice.\n* There does not appear to be any code in the Supplementary. I hope the authors plan to open-source their work to aid reproducability.\n\nWhile I do not think that the paper brings particularly strong new ideas or practical results, I find it interesting that encouraging even more uniformity than Wanda is beneficial, and that permutation reordering is quite effective for n:m pruning even for massive LLMs. Hence, I am currently leaning towards acceptance."
                },
                "questions": {
                    "value": "* How does 4:8 perform with channel permutations in the setup of Table 3?\n* Did you carry out any additional ablation studies around parameter a other than Table 2? I am curious if a = 0.5 is generally the sweet spot or if it was just picked for simplicity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769804682,
            "cdate": 1698769804682,
            "tmdate": 1699636046121,
            "mdate": 1699636046121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6J3CT8kvxR",
                "forum": "Tr0lPx9woF",
                "replyto": "gm3A8gSp0e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3 (part 1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hn9p,\n\n\n\nThank you for your thoughtful evaluation and constructive feedback on our manuscript. We appreciate your recognition of the strengths of our work, including its simplicity, clarity, and practicality. Your insights have been instrumental in guiding our revisions and enhancements. We address your concerns and questions as follows:\n\n\n\n**Weakness 1:** The observation that encouraging a more uniform sparsity pattern is beneficial was also made by Wanda, RIA seems to be an extension of that (also across columns). Similarly, that permutation reordering can be helpful for n:m sparsity was found by [1], this paper only introduces a simpler but more scalable heuristic for finding such a permutation, based on average activation values. While there is some novelty, it is not particularly high.\n\n\n\n**Reply:** We are grateful for your insightful feedback and the opportunity to clarify the unique contributions of our work. Our approach distinguishes itself in two critical aspects.\n\nFirstly, RIA incorporates a consideration of the relative importance of weights. As written in the article, \"In practice, we find similar issues also exist in other prevalent pruning metrics, e.g., Wanda prunes around 600 channels out of 5120 channels, with more than 10% channels corrupted. Given that well-trained LLMs contain unique information in the input and output channels, it is critical to avoid channel corruption in post-training pruning.\" The proposal of relative importance adopts a more uniformly pruning structure that prevents this issue. To address the reviewer's concern, we created Appendix E in the newly uploaded manuscript that explains well the motivation to introduce the relative importance. The performance across all the tests involved in this article, for instance, PPL across all the models on both unstructured and semi-structured sparsity, the sensitivity test of sparsity and number of samples, zero-shot performance, suggests that incorporating relative importance into the pruning metrics offers a consistent advantage against Wanda.\n\nSecondly, the proposal of the channel permutation in this article is to offer a solution to introduce N:M sparsity in large language models. As we introduced in Appendix G: \"The greedy method [1] is challenging to implement in LLMs due to its prohibitive running time, as shown in Table 1. We conducted only a single experiment for performance comparison on LLaMA2-13b, which took us 3 days. We tested the greedy method with 100 escape iterations to handle the permutation, the PPL on Wikitext-2 with a permuted 2:4 constraint is 8.01, which is comparable to our CP method (7.99). However, the CP's execution time was just 30 minutes, making it possible to be applied in LLMs.\" This is a big improvement since no previous works of post-training pruning really stepped into the N:M sparsity field because of the big drop in performance. However, in this article, with an acceptable channel permutation time, the case of RIA (2:4+CP) performs and surpasses the dense model with a large amplitude on BoolQ, MNLI, and RTE datasets of zero-shot performance. We want to draw more attention to the application of how to utilize the N:M constraint since it is currently the best way to retain performance while also offering inference speedup in real cases because it is hardware-friendly. Our method offers a solution to make N:M sparsity possible to be adopted in deploying LLMs.\n\nIn addition, as commented in the Reviewer's content, it is true that separating the channels into groups is heuristic, but it has its nature. To better explain our motivation for introducing this and to address the reviewer's concern, we added context in Appendix A that shows the reason why our method can have better results. In short, the post-channel-permutation distribution (2:4 + CP) of input channels\u2019 retained degrees can be seen in Figure 5, closely mirroring the results of unstructured pruning which is the global optimal of the specific pruning metrics (for instance RIA, Sparsegpt, and RIA).\n\nIn summary, while our work builds on existing concepts in the field, the refinements and innovations we introduce, particularly in terms of scalability and efficiency, contribute significantly to the practical application of N:M sparsity in large model training.\n\n[1] Channel permutations for N: M sparsity, NeurIPS 2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569224332,
                "cdate": 1700569224332,
                "tmdate": 1700577727483,
                "mdate": 1700577727483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LavS2J2uTd",
                "forum": "Tr0lPx9woF",
                "replyto": "fWYB02C0Aa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for the clarification regarding the 4:8 + CP typo. Unfortunately, this makes the CP results much less impressive. Initially, I thought that 2:4 + CP significantly outperforms 4:8, as indicated by this (incorrectly labeled) column, which I was quite positively surprised by. This now puts the CP improvements in the same 10-20% relative ballpark as the ones of RIA. As acknowledged in my initial review, these are indeed small steps forward, but are unlikely to have a significant impact on the current practicality of LLM pruning by themselves.\n\nIn terms of novelty, I still maintain that RIA and CP are extensions of insights from prior work. This is again reasonable but does not lead to particulary strong conceptual or methodological novelty of the submission.\n\nHence, I believe that my initial positive, but not overly so, assessment remains justified."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652142608,
                "cdate": 1700652142608,
                "tmdate": 1700652142608,
                "mdate": 1700652142608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "godfHfldna",
            "forum": "Tr0lPx9woF",
            "replyto": "Tr0lPx9woF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_ee73"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_ee73"
            ],
            "content": {
                "summary": {
                    "value": "Main Contribution:\n- The paper proposes two new methods for efficient post-training pruning of large language models (LLMs):\n    1) Relative Importance and Activation (RIA), a new pruning metric that considers both weight and activation information to determine weight importance. \n    2) Channel Permutation, a method to maximize retention of important weights when converting a model to N:M sparsity for hardware acceleration.\n\nNovelty:\n- RIA provides better pruning performance than prior state-of-the-art methods by avoiding pruning entire channels and using activations to assess weight importance.  \n- Channel Permutation reformulates the input channel permutation problem as a linear sum assignment problem, allowing efficient optimization using the Hungarian algorithm.\n\nExperiments:\n- Experiments conducted on LLMs including LLaMA, LLaMA-2, and OPT ranging from 7B to 70B parameters.\n- Tasks: Language modeling (Wikitext-2 perplexity) and zero-shot classification (5 commonsense datasets).\n- Compared RIA to magnitude pruning, SparseGPT, and Wanda for unstructured pruning.  \n- Evaluated Channel Permutation combined with RIA and other methods under N:M sparsity.\n\nResults:\n- RIA outperforms prior state-of-the-art post-training pruning methods in both unstructured and N:M sparsity settings.\n- Channel Permutation further improves performance under N:M sparsity by efficiently finding better channel arrangements.\n- Together, RIA and Channel Permutation provide an effective pipeline for LLM pruning and acceleration with negligible performance loss.\n\nConclusion:\n- RIA and Channel Permutation establish new state-of-the-art results for efficient one-shot post-training pruning of LLMs.\n- The proposed methods enable practical acceleration and size reduction of large models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Proposes two novel methods (RIA and Channel Permutation) that provide state-of-the-art performance for post-training pruning of large language models.\n\n2. Comprehensive experiments conducted on multiple popular LLMs across a range of model sizes from 7B to 70B parameters.\n\n3. Evaluated on diverse tasks including language modeling and zero-shot classification to demonstrate generalization. \n\n4. Provides both theoretical analysis and empirical results to demonstrate the efficiency and efficacy of the proposed techniques.\n\n5. RIA and Channel Permutation can be readily combined into an effective pipeline for practical LLM pruning and acceleration, with negligible performance loss."
                },
                "weaknesses": {
                    "value": "Overall the manuscript has solid contributions, but expanding the variety of models, tasks, and languages could strengthen the demonstrated effectiveness. Testing scalability and comparing to other recent techniques would also help round out the evaluation. But within the chosen scope, the paper delivers valuable advancements for efficient LLM pruning.\n\n* For \"We employ 128 samples from the C4 dataset\":\nusing only 128 samples from C4 as the calibration data is quite limited. With so few samples, the activation statistics may not sufficiently capture the full distribution."
                },
                "questions": {
                    "value": "* what is the channel here in the Transformer models? Transformer models does not have channel or column.\n\n* for \"\"We employ 128 samples from the C4 dataset\", is it possible/worth to do the experiments on a larger and more diverse calibration set (128 might be limited)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814196205,
            "cdate": 1698814196205,
            "tmdate": 1699636046039,
            "mdate": 1699636046039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "05e7bl9lUH",
                "forum": "Tr0lPx9woF",
                "replyto": "godfHfldna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2 (part 1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer ee73,\n\nThank you for acknowledging the contributions of our manuscript and for providing valuable feedback to further enhance its impact. We are pleased to address the highlighted concerns and questions.\n\n**Weakness 1:** Overall the manuscript has solid contributions, but expanding the variety of models, tasks, and languages could strengthen the demonstrated effectiveness. Testing scalability and comparing to other recent techniques would also help round out the evaluation. But within the chosen scope, the paper delivers valuable advancements for efficient LLM pruning.\n\n**Reply:** We thank the suggestion of the reviewer and we agree that we need to test the scalability and comparing to other recent techniques. To address the reviewer's concern, we added experiments on LLM-pruner and pruning only based on the activation which can be seen as a comparison between structured pruning, N:M pruning and unstructured pruning. In addition, we also included the experiments of combining post-training pruning and post-training quantization into consideration. Please refer to the general comments. \n\n**Weakness 2:** For \"We employ 128 samples from the C4 dataset\": using only 128 samples from C4 as the calibration data is quite limited. With so few samples, the activation statistics may not sufficiently capture the full distribution.\n\n**Reply:** Thank you for your insightful suggestion. In response, we have conducted a comprehensive comparison using varying numbers of samples from 2 to 512. Please refer to Figure 9 in the new uploaded manuscript. The experiments are conducted on LLaMA2-7b. The calibration dataset used in this experiment is C4 and evaluation is assessed on Wikitext2. The sparsity is always fixed at 50%. \n\nPPL changes regarding the increase of number of samples.\n\n|           | 2        | 8        | 32       | 128      | 256      | 512      |\n| --------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| SparseGPT | 7.46     | 6.59     | 6.23     | 6.03     | 5.99     | 5.99     |\n| Wanda     | 6.12     | 6.04     | 6.03     | 5.97     | 5.97     | 5.97     |\n| RIA       | **5.86** | **5.85** | **5.85** | **5.83** | **5.83** | **5.83** |\n\nPruning time (s) changes regarding the increase of number of samples.\n\n|           | 2    | 8    | 32   | 128  | 256  | 512  |\n| --------- | ---- | ---- | ---- | ---- | ---- | ---- |\n| SparseGPT | 462  | 508  | 790  | 1040 | 2245 | 7842 |\n| Wanda     | 54   | 147  | 156  | 158  | 417  | 1426 |\n| RIA       | 54   | 62   | 138  | 158  | 384  | 1565 |\n\nOur findings indicate that increasing the number of calibration samples does not significantly enhance performance. However, it does lead to a substantial increase in the pruning time of all the methods. \n\nWe believe this outcome is due to the distinct nature of calibration samples as compared to training samples. Calibration data is primarily utilized for calculating the activation distribution, rather than for shaping the loss curve. This distinction suggests that a large dataset is not as critical for calibration as it is for training.\n\nFurthermore, this approach aligns with methodologies used in other notable works, such as SparseGPT and Wanda, which also employ 128 calibration data samples.\n\nIt's noteworthy that the performance of RIA remains stable as the number of samples increases, achieving a notably low Perplexity (PPL) with as few as two calibration samples."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559284491,
                "cdate": 1700559284491,
                "tmdate": 1700559284491,
                "mdate": 1700559284491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h0QIWj5rVP",
                "forum": "Tr0lPx9woF",
                "replyto": "godfHfldna",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2 (part 2/2)"
                    },
                    "comment": {
                        "value": "**Questions 1:** what is the channel here in the Transformer models? Transformer models does not have channel or column.\n\n**Reply:** Thank you for your question. In our study, when we refer to 'channels' in the context of Transformer models, we are specifically referring to the dimensions of the input or output weights in the model's layers, which aligns with a concept frequently used in the context of MLP.\n\nIn more detail, the term 'channel' in our usage can be understood as analogous to rows or columns of the weight matrices in Transformer layers. For example, in a fully connected layer of an MLP or a linear layer of a Transformer, each row (or column, depending on the implementation) of the weight matrix can be thought of as a 'channel'. These 'channels' represent different learned features or filters that the model applies to the input data.\n\nThis terminology, while more commonly associated with CNNs, is also applicable to Transformer models, especially when discussing the dimensions of weights and their transformations. The use of the term 'channel' in this context allows us to discuss the architecture and optimization of these models more precisely, particularly in relation to weight sparsity and efficiency improvements, which are central themes in our research.\n\nOur adoption of this terminology is consistent with established literature, including the NVIDIA article [1], which discusses channel permutations as a method for implementing sparsity in neural networks. It's also important to note that similar terminologies are used in other works in the field [2].\n\n[1] Channel permutations for N: M sparsity, NeurIPS 2021\n\n[2] A simple and effective pruning approach for large language models\n\n\n\n**Question 2:**  for \"\"We employ 128 samples from the C4 dataset\", is it possible/worth to do the experiments on a larger and more diverse calibration set (128 might be limited)?\n\n**Reply:** Thanks for the question. We have done the experiments and included them in the second response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559381280,
                "cdate": 1700559381280,
                "tmdate": 1700577636419,
                "mdate": 1700577636419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gGvsqofxWr",
            "forum": "Tr0lPx9woF",
            "replyto": "Tr0lPx9woF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the growing demand for efficient memory and computation in large language models (LLMs). Existing post-training pruning methods have attempted to reduce model size and computation but have not achieved optimal performance. The paper introduces a plug-and-play solution for post-training pruning of LLMs, featuring two innovative components: 1) Relative Importance and Activations (RIA), a novel metric that efficiently considers weight and activations in LLMs, and 2) Channel Permutation, a new approach to maximize the preservation of important weights with N:M sparsity. These components can be combined to enhance N:M structured pruning of LLMs. Empirical experiments demonstrate that RIA alone surpasses existing pruning methods on various LLMs. Moreover, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, while providing practical speed-up on specific hardware."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Consider both weights and activations for unstructured pruning in LLMs is novel.\n- Particularly, the consideration of relative weight importance is a novel approach.\n- Channel permutation is a simple yet effective method for achieving N:M sparsity."
                },
                "weaknesses": {
                    "value": "The reviewer recognizes the novelty and simplicity of the overall approach but has raised substantial concerns. The main issues pertain to the weaknesses in the baselines, which make it challenging for me to be convinced of the effectiveness of the proposed method. Moreover, the motivation and analysis provided appear to be inadequate. For example, concerning the former issue, the following questions come to mind: even if the proposed method can enhance the performance of N:M sparsity-based approaches, are N:M sparsity-based methods genuinely effective? Are they superior to contextual sparsity-based methods? \n\nI describe specific questions and suggestions regarding concerns as follows: \n\n- Insufficient experimental support for motivation: This paper argues the existence of 'channel corruption' asserting that removing input/output channels results in decreased performance as observed in prior works. However, the paper lacks empirical evidence to substantiate this claim. It would be valuable if the authors could include preliminary experiments to provide a basis for their motivation.\n- According to AWQ [1], activation-aware weight quantization, which selects important weights based on activation distribution rather than weight distribution, outperforms traditional weight-based quantization. Inspired it, the reviewer suggests that it would be meaningful to consider baseline methods based on activation-based weight pruning for comparison. Therefore, the authors might incorporate and compare unstructured pruning based solely on activations in Table 2.\n- In addition to the comparisons with N:M sparsity methods in Table 4 and Table 5, it is advisable to include a comparison with other structured pruning techniques in terms of performance and inference speed improvement. For instance, including a method like Dejavu [2] in the comparison would enhance the comprehensiveness of the evaluation.\n- What is the relevance of the experiments in Figure 2 to the claim that activation outliers exist independently of the dataset and model's parts? The reviewer thinks that even if activation values exhibit a high correlation between two datasets, it is possible that activation outliers can be eliminated. Therefore, it would be helpful to clarify the connection between Figure 2 and the claim about activation outliers.\n- Can we expect additional performance improvements when combined with post-training quantization methods such as Smooth Quant [3] or AWQ [1]?\n\n[Minor]\n- Why is the title \"plug-and-play\"?\n- The hyperlink in the 6-page appendix seems to be incorrect.\n\n[1] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n\n[2] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time, ICML 2023\n\n[3] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, ICML 2023"
                },
                "questions": {
                    "value": "Please address the concerns in Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699212339001,
            "cdate": 1699212339001,
            "tmdate": 1699636045980,
            "mdate": 1699636045980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hwRBdEf9lm",
                "forum": "Tr0lPx9woF",
                "replyto": "gGvsqofxWr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1 (part 1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer NPvK,\n\nThank you for your review of our paper. We appreciate your positive evaluation and also the questions to our submission which aims at improving our presentation and encouraging us to process more empirical evidence to support our methods.\n\nWe will reply to each of the weakness you mentioned and the questions you raised.\n\n**Weakness 1:** Insufficient experimental support for motivation: This paper argues the existence of 'channel corruption' asserting that removing input/output channels results in decreased performance as observed in prior works. However, the paper lacks empirical evidence to substantiate this claim. It would be valuable if the authors could include preliminary experiments to provide a basis for their motivation.\n\n**Reply:** Thank you for your feedback. We understand the need for empirical evidence to support our assertion of 'channel corruption' and have taken steps to address this in our revised manuscript Appendix E (Evidence of Channel Corruption).\n\n**1. Evidence of Channel Corruption:** We have updated our manuscript to include new empirical data demonstrating channel corruption. As illustrated in Figure 6, our analysis of the Wanda [1] model shows that approximately **10% of channels exhibit corruption**. This evidence directly supports our claim and provides a clearer understanding of the phenomenon.\n\n**2. Detrimental Impact of channel corruption on Model Performance:**\n\n- We have conducted experiments using LLM-Pruner [2] which is a SOTA structured pruning method used in LLMs. We also compare our method with the baseline structured pruning method of pruning merely based on the activation aligning with the experiment you proposed in **Weakness 2**. We utilize this comparison to further illustrate the effects of channel corruption.\n\n- We separated the experiments to two different Tables. One utilized the strategy in LLM-Pruner that only pruned the layers from 4-30 layers and the other one pruned all the layers.\n\n- Our results indicate: 1) The performance of LLM-Pruner, perplexity, notably increases at already a low sparsity level of 25%. 2) Using LLM-Pruner's approach, which involves skipping 6 out of 32 layers, led to lower perplexity scores (PPLs). However, the balance between performance and the increased inference speed due to sparsity requires further exploration. We aim to continue investigating this issue promptly for this study.\n\n**3. Additional Section in Appendix E:** To further address your concerns, we have added a dedicated section in Appendix E of our manuscript. This section explicitly discusses the influence of channel corruption on post-training pruning methods. Due to time constraints, our current experiments are limited to the LLaMA2-7b model. We plan to extend our investigation to more models to provide comprehensive evidence of channel corruption and its implications.\n\n**Prune from layer 4 to layer 32**\n\n|                               | 1:4 (25% sparsity) | 2:4 (50% sparsity) | 3:4 (75% sparsity) |\n| ----------------------------- | ------------------ | ------------------ | ------------------ |\n| LLM-Pruner (structured)       | 26.04              | 77.90              | 464.34             |\n| Activation-based (structured) | 30.23              | 525.5              | 8692.2             |\n| Wanda (semi-structured)       | 5.84               | 9.21               | **432.6**          |\n| RIA (semi-structured)         | 5.82               | 8.91               | 442.6              |\n| RIA + CP (semi-structured)    | **5.74**           | **8.03**           | 436.1              |\n\n**Prune all the layers**\n\n|                               | 1:4 (25% sparsity) | 2:4 (50% sparsity) | 3:4 (75% sparsity) |\n| ----------------------------- | ------------------ | ------------------ | ------------------ |\n| LLM-Pruner (structured)       | 28.88              | nan                | 13570              |\n| Activation-based (structured) | 39.54              | 10467              | nan                |\n| Wanda (semi-structured)       | 5.94               | 12.15              | 2863.3             |\n| RIA (semi-structured)         | 5.90               | 11.27              | 1891.13            |\n| RIA + CP (semi-structured)    | **5.81**           | **10.12**          | **1532.72**        |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558855210,
                "cdate": 1700558855210,
                "tmdate": 1700558855210,
                "mdate": 1700558855210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zuQ4KG5NWq",
                "forum": "Tr0lPx9woF",
                "replyto": "gGvsqofxWr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1 (part 2/3)"
                    },
                    "comment": {
                        "value": "**Weakness 2:** According to AWQ [1], activation-aware weight quantization, which selects important weights based on activation distribution rather than weight distribution, outperforms traditional weight-based quantization. Inspired it, the reviewer suggests that it would be meaningful to consider baseline methods based on activation-based weight pruning for comparison. Therefore, the authors might incorporate and compare unstructured pruning based solely on activations in Table 2.\n\n\n\n**Reply:** \n- Thanks for the question. Pruning solely on activation means the whole channel will be removed together, thus pruning on activation actually belongs to structured pruning. We have applied this method on **LLaMA2-7b, LLaMA1-13b, and LLaMA1-30b**. The results are included in the Table on the last response and also in updated **Table 2** as an ablation test in the newly uploaded version of the manuscript.\n\n- The results indicate that relying solely on activation, the PPL **increases significantly** with just 25% sparsity, reaching **over 5000 at 50% sparsity** on each model. This suggests that **structured pruning is not effective** in post-training pruning scenarios, as the pruned information from the entire channels cannot be recovered without retraining or fine-tuning.\n\n\n\n**Weakness 3:** In addition to the comparisons with N:M sparsity methods in Table 4 and Table 5, it is advisable to include a comparison with other structured pruning techniques in terms of performance and inference speed improvement. For instance, including a method like Dejavu [2] in the comparison would enhance the comprehensiveness of the evaluation.\n\n\n\n\n**Reply:** \n\n- To enhance the comprehensiveness of our evaluation, we have included additional results from the **LLM-pruner [2]**, a **state-of-the-art structured pruning** method in our article, as detailed in our first response. These results demonstrate the superiority of our N:M semi-structured pruning.\n\n\n- As for **Dejavu [3]**, we would like to clarify that **Dejavu** cannot be fairly considered for comparison in this study because it differs fundamentally from our method in terms of its **operational framework**. \n\n  - **Dejavu** is not a post-training pruning method as it introduces **additional parameters** and **requires the predictor to be trained for several epochs** (50, as per the provided code). \n  - Consequently, **Dejavu** requires computational time resources, while our method is designed to be applied **without retraining, offering a 'plug-and-play' solution** for practical scenarios.\n\n- To address this distinction and your concern, we have added the following statement in the related work section of our manuscript.\n\n  \u201cNote that the method proposed in this article is designed for application without retraining and finetuning, which differentiates it from techniques like Dejavu [3] that require additional training steps. Consequently, our comparisons are focused on methods that do not involve retraining, such as Magnitude, Wanda, and SparseGPT.\u201d\n\n- Regarding inference speed improvement, we offered the **acceleration rate** of the semi-structured pruning in Section 5.4. For the end-to-end inference acceleration of structured sparsity and semi-structured sparsity, we here offer their inference latency. Our experiment is on LLaMA2-7b, the sequence length of the input is 12. The deployment is on 2 Nvidia A100 80GB in parallel on Ubuntu 22.04.1 system. Here, we offer the inference latency with varying batch sizes.\n\n  |                         | 1         | 8         | 16        | 64         |\n  | ----------------------- | --------- | --------- | --------- | ---------- |\n  | dense                   | 281.13 ms | 547.82 ms | 1011.81ms | 3742.76 ms |\n  | structured 50% sparsity | 238.92 ms | 326.14 ms | 616.13 ms | 2181.69 ms |\n  | 2:4 sparsity            | 225.60 ms | 357.48 ms | 731.81 ms | 2495.41 ms |\n\n  The table shows that structural 50% sparsity marginally outperforms the 2:4 sparsity model as batch sizes increase. It's important to note that when dealing with smaller input batches, both structured 50% sparsity and 2:4 sparsity models do not significantly boost inference speed. However, as the batch size grows, the acceleration for 2:4 sparsity approximates 1.5x, whereas the structured 50% sparsity tends to reach about 1.7x acceleration."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558918229,
                "cdate": 1700558918229,
                "tmdate": 1700580018686,
                "mdate": 1700580018686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QESyyfuf9s",
                "forum": "Tr0lPx9woF",
                "replyto": "gGvsqofxWr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1 (part 3/3)"
                    },
                    "comment": {
                        "value": "**Weakness 4:** What is the relevance of the experiments in Figure 2 to the claim that activation outliers exist independently of the dataset and model's parts? The reviewer thinks that even if activation values exhibit a high correlation between two datasets, it is possible that activation outliers can be eliminated. Therefore, it would be helpful to clarify the connection between Figure 2 and the claim about activation outliers.\n\n**Reply:** \n- Thanks for the reviewer raising this question. To address the reviewer\u2019s concern, we added the below content in Section 3.3 in the updated file. \n\n  \u201cWe offer evidence that the Spearman Rank correlation between pairs of the activations of different datasets is positive. This positivity is a necessary condition to incorporate the activation into our RIA formula. And indeed it is always satisfying.\u201d\n\n**Weakness 5:** Can we expect additional performance improvements when combined with post-training quantization methods such as Smooth Quant or AWQ?\n\n**Reply:** \n- Thank you for your insightful question. Because there is not a remarkable decrease in performance using quantization, it indicates that our method could also be applied in combination with quantization. This will be a new and valuable point to comment on in the article.\n\n\n- We tested on two quantization methods: **GPTQ [4]** and **AWQ [5]**. Here are the results for pruning + GPTQ and pruning + AWQ on LLaMA2-7b, weight bit (wbit) = 4, using C4 for calibration and Wikitext2 for evaluation. We still get the **best performance** on two quantizations among all pruning methods:\n\n  |                                       | Magnitude | SparseGPT | Wanda | RIA      |\n  | ------------------------------------- | --------- | --------- | ----- | -------- |\n  | Unstructured (50% sparsity)           | 16.02     | 6.99      | 6.92  | **6.81** |\n  | Unstructured (50% sparsity) With GPTQ | 15.21     | 7.59      | 7.41  | **7.28** |\n  | Unstructured (50% sparsity) With AWQ  | 17.12     | 7.15      | 7.10  | **6.97** |\n\n\n\n- We have identified two primary strategies for merging post-training pruning with quantization: first pruning then quantizing (a), and first quantizing then pruning (b.) \n  - Our findings indicate **a preference for (a) pruning before quantizing.** \n  - This is potentially because for (b), conventional block-wise quantization relies merely on the min and max weights within a block, and pruning can lead to a reduction in these extremes, thereby potentially aiding quantization.\n  -  In contrast, (a) quantizing prior to pruning adversely affects the computation of weight importance for pruning, resulting in diminished performance.\n\n- The discussion is added as Appendix I in our manuscript.\n\n\n\n\n**Minor:** **Why is the title \"plug-and-play\"?**\n\n**Reply:** \n\n- The reason we call it plug-and-play is because of 3 reasons. \n\n  - Ignorable Pruning time.\n  - No need for any additional finetuning or retraining.\n  - With channel permutation, our method can perform well in the zero-shot experiments when adopting N:M sparsity pattern while currently other post-training pruning methods cannot.\n\n- To further address the reviewer\u2019s concern, we introduce in the discussion: \u201cWe decide to call our method \u201cplug-and-play\u201d because, in the zero-shot experiment, it demonstrated performance similar to the original dense model with no need for any additional finetuning or retraining whereas, the other post-training pruning method such as Sparsegpt and Wanda are not \u201cPlug-and-play\u201d because they don\u2019t have channel permutation.\u201d\n\n**Minor:** **The hyperlink in the 6-page appendix seems to be incorrect.**\n\n**Reply:** Thank you. We have fixed that in our updated version of the paper.\n\n\n\nReference:\n\n[1] Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning, NeurIPS 2022\n\n[2] SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot, ICML 2023\n\n[3] LLM-Pruner: On the Structural Pruning of Large Language Models, NeurIPS 2023\n\n[4] GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS, ICLR 2023\n\n[5] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558968423,
                "cdate": 1700558968423,
                "tmdate": 1700580519166,
                "mdate": 1700580519166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LU9SbEBWB6",
                "forum": "Tr0lPx9woF",
                "replyto": "QESyyfuf9s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
                ],
                "content": {
                    "title": {
                        "value": "Response by reviewer"
                    },
                    "comment": {
                        "value": "Thank you for the response and extensive experiments. Most critical concerns are addressed and I would like to keep my rating as weak accept. \n\nI have additional minir questions regarding W3 and W4. \n\nRegarding W3, while Dejavu trains the predictor, it do not need to retrain LLMs directly. Also, the predictor is just MLP layers, so 50 epochs predictor training time might be just a few GPU minutes and required computing resource is negligible. Thus, the reviewer still think that we can regard Dejavu as the post-training pruning since we do not retrain LLMs. \nCan the authors conduct experiments to compare Dejavu and the proposed method and discuss the strength of N:M sparsity-based methods at the inference stage in terms of inference time and performance compared with context-based pruning like Dejavu?\nConsidering Dejavu use additional components and resources, the experimental results will not affect to rate the proposed method. \n\nRegarding W4, It is still difficult for me to understand. Why a positive correlation between two datasets is the necessary condition to include activation values to RIA formula? Could the authors explain more about it? Also, then does outliers no more related with RIA formula?\n\nThanks, reviewer."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620118675,
                "cdate": 1700620118675,
                "tmdate": 1700620118675,
                "mdate": 1700620118675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]