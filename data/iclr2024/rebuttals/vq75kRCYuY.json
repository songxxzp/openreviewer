[
    {
        "title": "SOLO: Surrogate Online Learning at Once for Spiking Neural Networks"
    },
    {
        "review": {
            "id": "YUQKmOEQtI",
            "forum": "vq75kRCYuY",
            "replyto": "vq75kRCYuY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_Ev3G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_Ev3G"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel training method called SOLO, which uses surrogate strategies to perform end-to-end learning with low computational complexity. It is easy to implement on neuromorphic hardware and is evaluated on various static and neuromorphic datasets. The method is compared with existing methods like BPTT, STBP, E-prop, and DECOLLE. The paper also demonstrates SOLO's robustness to hardware-related noises and reliability issues, making it suitable for deployment on neuromorphic computing substrates."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "On-chip single-time backpropagation: SOLO is a surrogate online learning method that trains deep SNNs end-to-end using spatial gradients and surrogate strategies to reduce computational complexity and memory requirements. It also introduces a simple three-factor learning rule for online on-chip learning on neuromorphic hardware.\n\n\n\nHardware awareness: This algorithm considers too many compatibility issues between neuromorphic computing and SNN. It introduces neuron models like pPLIF and pPLI and uses hardware-friendly surrogate strategies like boxcar function and always-on pooling. The evaluation is done given the hardware-related noise."
                },
                "weaknesses": {
                    "value": "The paper lacks clear theoretical justification for the proposed SOLO method, relying on empirical results and biological plausibility without mathematical analysis or proof of convergence.\n\n\n\nUnfair comparison: The paper compares SOLO with offline methods like BPTT and STBP but does not compare it with the newest online methods like OTTT (Xiao et al., NeurIPS 2022), SpikeRepresentation (Meng et al., CVPR 2022), and so on.\n\n\n\nLack of clarity: Some of the mathematical expressions lack proper definition and notation. I am confused by some details.\n\n\n\nMinor: the citation is not proper in the content. I think the author should use \u2018\\citep{}\u2019 instead of \u2018\\cite{}\u2019 most of the time."
                },
                "questions": {
                    "value": "For equation 6, why is there item $\\theta(U^\\sim[t]-\\theta^\\sim_{th})$ rather than $\\theta(abs(U^\\sim[t]-\\theta^\\sim_{th})<p)$.\n\n\n\nWhy does pTRACE need a clamp function $k$? I think equation (5) really ensembles the proposed pPLI (equation (2)). Why don\u2019t you simply use pPLI as a surrogate?\n\n\n\nPlease point out the difference between the current proposed SOTA online training methods and propagate-only-once training methods. Examples are OTTT (Xiao et al., NeurIPS 2022) and spike representation (Meng et al., CVPR 2022).\n\n\n\nHow do we implement SOLO on a neuromorphic platform when it has a float-point derivation?\n\n[1] Xiao et al., Online Training Through Time for Spiking Neural Networks, NeurIPS 2022\n[2] Meng et al., Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation, CVPR 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698263205252,
            "cdate": 1698263205252,
            "tmdate": 1699636544219,
            "mdate": 1699636544219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5B4JY2NA54",
                "forum": "vq75kRCYuY",
                "replyto": "YUQKmOEQtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer Ev3G"
                    },
                    "comment": {
                        "value": "First and foremost, I would like to express my gratitude for your insightful comments and suggestions.\n\nPlease check above 'Initial Response to Reviewer Comments'.\n\n1. definition and notation\n\n We have endeavored to express the 'lack of proper definition and notation' issue within the equations as much as possible. Should there be any elements that were not adequately addressed, we have explained the relevant notations in Appendix A.1.~B.2..\n\n2. citation method\n\nWe implement the citation method you suggested. We are grateful for your guidance on this matter.\n\n3. Equation 6 and the clamp function of pPTRACE\n\nWe have explored both scenarios and found that $\\Theta(\\tilde{U[t]}-\\tilde{\\theta_{th}})$ is more suitable. pPTRACE is ultimately proposed to output binary values named eligible spikes. As seen in Appendix A.4, pPTRACE is explained along with the clamp function, k, and the relevant information is present. The rationale behind using the clamp function is as follows:\n\n 3-1. As forward propagation occurs, the generated spikes are added to the eligible potential. Unlike neurons, pPTRACE does not reset after a certain event (in the case of neurons, a spike), allowing it to accumulate increasing values. We need a limit range for binarization, hence the necessity for a clamp function (see Appendix Figure 1).\n\n 3-2. While torch.clamp() could have been used to set a limit range, it is unable to create a gradient, thus making it ineffective in generating a gradient that flows with the time constant of pPTRACE.\n\nBecause pPTRACE doesn\u2019t have any threshold and just accumulates potential, the Heaviside function is more suitable than a box range. Moreover, the Heaviside function is also suitable for hardware implementation as it can be implemented with just one comparator.\n\nThe pPLI neuron acts as a spike counter in SOLO, counting the network's output spikes, which is a completely different role from pPTRACE. Depending on the mode of leaky integration or integration, the pPLI neuron counts (or accumulates) spikes and is utilized during loss calculation.\n\n4. floating-point neuromorphic platform\n\nThe experimental data presented in the paper is implemented in a GPU-based floating-point computing environment. We have implemented the surrogate gradient using PyTorch's autograd and custom gradient functions.\n\nAdditionally, SOLO attempts to maintain the conventional SNN layout, which includes spike generators (or spike routers), weights, and a neuron core. Additional computational components such as pPLIF neurons, pPLI neurons (equivalent to spike counters), and surrogate strategies are designed for ease of hardware implementation. Therefore, if your platform has a conventional SNN layout and you have access to computing components for weight update, SOLO can function well on a floating-point-based neuromorphic platform, both automatically and manually. We are also very interested in digital neuromorphic platforms and are actively working to deploy SOLO on a board system. If you need any further information, we would be happy to assist."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232194703,
                "cdate": 1700232194703,
                "tmdate": 1700234115404,
                "mdate": 1700234115404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PrzTcZNppH",
            "forum": "vq75kRCYuY",
            "replyto": "vq75kRCYuY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an online learning method for SNNs. A spiking neuron layer without firing is used to accumulate outputs. Then four surrogate strategies are proposed:\n1. Using a boxcar surrogate function with only a 0/1 gradient.\n2. Using an always-on gradient in loss.\n3. Redefining the gradient of max pooling to propagate gradients to those elements that are not the maximum values in the pooling windows.\n4. Using eligible traces to calculate gradient online.\n\nThe proposed methods are validated on some popular datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "As an online learning method, this paper achieves O(1) memory complexity, which is meaningful for the SNN community.\n\nThe proposed method is hardware-friendly and has the potential to be applied to neuromorphic chips."
                },
                "weaknesses": {
                    "value": "The accuracy drops sharply in all datasets except for the toy MNIST dataset, which can not show the effectiveness of the proposed methods. I am afraid that the plain SNN with a simple Real Time Recurrent Learning method will get close performance to the proposed methods.\n\nAs a comparison, OTTT [1] is also an online training method and achieves much higher accuracy even on the challenging ImageNet dataset.\n\n[1] Xiao, Mingqing, et al. \"Online training through time for spiking neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 20717-20730."
                },
                "questions": {
                    "value": "I do not understand the necessity of \"surro2: Always-On beyond Spike Accumulation\". The authors claim that they \"ensuring error propagation across all classes\". But the gradient to each class is not zero in most cases unless the neuron that represents a class outputs 0 at all time-steps when it is not the target class (or outputs 1 at all time-steps when it is the target class)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698322396852,
            "cdate": 1698322396852,
            "tmdate": 1700725182362,
            "mdate": 1700725182362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fzLOaKC8MP",
                "forum": "vq75kRCYuY",
                "replyto": "PrzTcZNppH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer TEM3"
                    },
                    "comment": {
                        "value": "First and foremost, I would like to express my gratitude for your insightful comments and suggestions.\n\nPlease check above 'Initial Response to Reviewer Comments'.\n\n1. Accuracy\n\nWe aim to compare the performance of learning algorithms using the same training configuration. Therefore, we use the results shown in AutoSNN as a baseline. Additionally, we compare BPTT, STBP, and SOLO under the same training configuration, which includes the utilization of the pPLIF neuron model, the accumulative neuron model, regularization, etc.\n\nHowever, unlike the regularization technique used in AutoSNN, we do not employ spike regularization [1]. This approach involves counting all spikes and incorporating them as an additional term in the weight update equation, which requires extra hardware operations and implementation. Instead, SOLO relies on the regularization effect of the accumulative neuron model, which operates similarly to a neuron. This method does not require any additional regularization term in the weight update equation, making it more advantageous for hardware implementation.\n\nFurthermore, AutoSNN is originally composed of 13 convolutional layers and 1 fc layer, and through light-weighting, it is configured with 11 convolutional layers. (See Appendix Figure 4) However, recent state-of-the-art (SOTA) models employ bigger models such as ResNet 18, 34, 50, and others; hence, they are expected to exhibit higher accuracy on large datasets like CIFAR-100 and ImageNet.\n\n[1] Pellegrini, et al. Low-activity supervised convolutional spiking neural networks applied to speech commands recognition. 2021 IEEE Spoken Language Technology Workshop.\n\n2. surro2: Always-On beyond Spike Accumulation\n\nFirst, we calculate the loss based on the spike counting method by counting the output spikes of the network at each time step. In conventional SNNs, like STBP, a'spike counter\u2019 performs this role, and in SOLO, a component named 'accumulative neuron' serves this purpose.\n\n In STBP, where backpropagation is conducted for information on all time steps, every output spike uniformly propagates the error across classes (see Appendix Figure 3). However, in SOLO, backpropagation is only conducted for information about the final time step, meaning that classes without output spikes in the final time step do not receive error propagation. To address this, we employ 'Always-On beyond Spike Accumulation'.\n\nAs you mentioned, at the final time step, the gradient for each class is not zero, even if it is not the target class. It is true. However, the values for the non-target class are involved in the computation of the mean squared error (MSE) loss with the target vectors, leading to negative error, which is appropriately integrated into the network. As a result, this process prevents the non-target class from being output. Additionally, constantly sending the potential of all time steps aids in achieving a learning curve for SOLO that is similar in speed to STBP (see Figure 2). Furthermore, this approach is also beneficial for hardware operations and implementation.\nAdditionally, the results of the ablation study for surrogate strategies, conducted as described in Appendix B.5, are documented. In the case of AW (Always-On beyond Spike Accumulation), the best accuracy is observed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231938784,
                "cdate": 1700231938784,
                "tmdate": 1700234093513,
                "mdate": 1700234093513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KdmspgE0Ap",
                "forum": "vq75kRCYuY",
                "replyto": "fzLOaKC8MP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, the low performance of your SNNs may be caused by the using of a small network structure. Can you provide the performance of using SOLO on the same structure from previous SOTA research?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536035940,
                "cdate": 1700536035940,
                "tmdate": 1700536035940,
                "mdate": 1700536035940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JJex5oFcDV",
                "forum": "vq75kRCYuY",
                "replyto": "PrzTcZNppH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer TEM3"
                    },
                    "comment": {
                        "value": "Hi,\n\nThe results of SOLO utilizing various architectures are detailed in Appendix C.6. Utilizing the Resnet19-Zheng network [1], SOLO achieved a 65.72% accuracy on the CIFAR-100 dataset, compared to 57.86% accuracy when employing AutoSNN. This demonstrates that as the network grows, the accuracy sufficiently increases. However, our paper primarily aims to compare the performance of learning algorithms under the same training configuration and specifically using the AutoSNN network.\n\nAdditionally, as you noted, we are currently experimenting with the Resnet18 architecture, which incorporates pre-activation residual blocks and has already been implemented in the SLTT model. [2] (It is expected that the ResNet18 model will yield higher results by using pre-activation residual blocks.) The coding for this model is complete, and it is now being tested on various datasets without some details (such as regularization, etc.). This process may take some time, but we will share the results as soon as they become available.\n\n[1] Hanle Zheng, et al. Going deeper with directly-trained larger spiking neural networks. Proceedings of the AAAI Conference on Artificial Intelligence, 10, 2021.\n\n[2] Qingyan Meng, et al. Towards memory- and time-efficient backpropagation for training spiking neural networks, 2023a."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571224143,
                "cdate": 1700571224143,
                "tmdate": 1700571278677,
                "mdate": 1700571278677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cUdB8B5RAi",
                "forum": "vq75kRCYuY",
                "replyto": "PrzTcZNppH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer TEM3"
                    },
                    "comment": {
                        "value": "We conduct experiments with SOLO utilizing ResNet18 without any regularization, such as dropout.  We achieve accuracy \n of 92.21% on CIFAR-10 (AutoSNN - 91.33% / resnet19 - 91.10%) and 70.25% on CIFAR-100. (AutoSNN - 57.86% / resnet19 - 65.72%). \n\nAlthough not yet optimized, the training configuration remains the same as the original one. (See details in Appendix C.4)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721415762,
                "cdate": 1700721415762,
                "tmdate": 1700721497977,
                "mdate": 1700721497977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AW6Nq9xRcr",
                "forum": "vq75kRCYuY",
                "replyto": "cUdB8B5RAi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Reviewer_TEM3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the latest result. I am willing to raise my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725165348,
                "cdate": 1700725165348,
                "tmdate": 1700725165348,
                "mdate": 1700725165348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UxrWjn3ZEv",
            "forum": "vq75kRCYuY",
            "replyto": "vq75kRCYuY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_gzEk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_gzEk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Surrogate Online Learning at Once (SOLO) for training SNNs in a hardware-friendly manner. It only leverages spatial gradient at the final time step for low computational complexity. Experiments are conducted on static and neuromorphic datasets to verify the effectiveness."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper considers online SNN training methods to promote online on-chip learning, which is an important topic."
                },
                "weaknesses": {
                    "value": "1. The motivation to only consider the gradient at the last time step is not convincing enough, and the experimental results are quite poor. There is no formal/theoretical justification for the claim \u201cwe believe that the information of the accumulative neurons in the final time step could yield the most distinct and clear error value among all given time steps\u201d. It is obvious that only considering spatial gradient for the last time step will lose a lot of information on previous time steps, and the experimental results indeed show a significant drop in accuracy, especially for neuromorphic datasets with temporal sequences. For static datasets, there is no temporal information and binary neural networks (or taking T=1 for SNNs) can easily work well, so experiments are not surprising or appealing. It is unclear what\u2019s the advantage of the proposed method over existing online training methods [1,2].\n\n2. The idea of the proposed method may, to some extent, be viewed as a special case of a recent work [3]. It proposes a method SLTT, which drops the temporal dependency of BPTT and only uses spatial gradients at each time step, and it further proposes a variant SLTT-k, which randomly samples k time steps for the spatial gradient. The method in this paper may be viewed as taking k=1 and fixing the considered time step as the last time step. However, this paper ignores gradients for previous time steps, leading to much poorer performance.\n\n[1] A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 2020.\n\n[2] Online Training Through Time for Spiking Neural Networks. NeurIPS, 2022.\n\n[3] Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks. ICCV, 2023."
                },
                "questions": {
                    "value": "1. It is not clear enough why pPLIF is more straightforward for hardware implementation than PLIF. If consider deploying trained models, $\\beta$ for the current in PLIF can be absorbed into the weight. If consider training models, it is also unclear for pPLIF how the gradient for the learnable membrane time constant can be calculated on hardware."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648658104,
            "cdate": 1698648658104,
            "tmdate": 1699636544026,
            "mdate": 1699636544026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dgdXq6zbKy",
                "forum": "vq75kRCYuY",
                "replyto": "UxrWjn3ZEv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer gzEk"
                    },
                    "comment": {
                        "value": "First and foremost, I would like to express my gratitude for your insightful comments and suggestions.\n\nPlease check above  'Initial Response to Reviewer Comments'.\n\n1. Similar to SLTT\n\nWe have reviewed the SLTT paper, and as you mentioned, its intent is somewhat similar to SOLO, and SOLO can be considered a special case of SLTT-K. However, SLTT-K performs backpropagation for randomly sampled K time steps at each training point, and there's no results of the case where K = 1 (minimum K = 2). Furthermore, SLTT-K ultimately requires memory to hold spatial states for specific K-time steps, making O(1) memory complexity impossible on neuromorphic platforms. In contrast, SOLO has the advantage of being able to directly access all components for weight updates at the final time step, transitioning to the weight update phase without the need for memory on neuromorphic platforms. In summary, in our work, SOLO demonstrates performance in harsher training configurations and hardware environments compared to SLTT.\n\n2. pPLIF and gradient toward the time constant\n\nYou can find information on the PLIF neuron model in Appendix A.1, 'Neuron Model.' In the PLIF neuron model, the parameter \\beta is shared for use both in the computation of membrane potential and in the modulation of current. \\beta for the current couldn\u2019t be ignored (or absorbed to current) when calculating the gradient with respect to \\beta. In Appendix A.5, 'Related Gradient Chain,' you can find the process used to calculate the gradient with respect to the membrane constant in PLIF neurons, comparing it with pPLIF neurons and SOLO. On pPLIF neurons and SOLO. In the case of pPLIF neurons and SOLO, the gradient flowing through the time constant is simplified, which is expected to facilitate easier hardware implementation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231829618,
                "cdate": 1700231829618,
                "tmdate": 1700234074898,
                "mdate": 1700234074898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qw23zSBdnK",
            "forum": "vq75kRCYuY",
            "replyto": "vq75kRCYuY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_wijC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5382/Reviewer_wijC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a surrogate online learning method-SOLO, to efficiently train spiking neural networks. The main idea is to consider a backward path only at the final step, which disentangles the temporal dependencies of the conventional BPTT-type training method. The authors show that by doing so, the performance on several benchmark tasks does not decrease significantly, while largely reducing the required memory and training time. This shows the potential to be implemented in the neuromorphic hardware in the future."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written, with very clear illustration on the motivations, methods and implementations. \n- The paper proposes a new way to efficiently train spiking neural networks, and this method shows the potential to solve the on-chip learning challenge of neuromorphic chips."
                },
                "weaknesses": {
                    "value": "- The paper, however, lacks of a enough investigation and comparison with the existing methods. Aiming to cut the temporal dependencies to optimize the SNN training is not a new idea [ref. 1-3], what are the main differences (except for the \u201clast time step\u201d part) compared with them? For instance, the intrinsic idea, the approximation way, even the three-factor-rules part are quite similar as in [1]. \n- The resulting performance decreases, if not significantly, still quite a lot, on many datasets. One might suspect the availability of this method in real use cases.\n\nRef:\n1. Online Training Through Time for Spiking Neural Networks, NeurIPS22\n2. Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks, ICCV23\n3. Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time, Nature MI"
                },
                "questions": {
                    "value": "See the above weakness part.\nIn addition, in table 1 and table 2, the baseline performance looks not very high, e.g., for CIFAR 10, SNN SOTA is already close to 95% with 4-6 time steps, and CIFAR100 around 73%, but in these tables, these number are relatively low, could you explain the reasons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5382/Reviewer_wijC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5382/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699245415260,
            "cdate": 1699245415260,
            "tmdate": 1699636543923,
            "mdate": 1699636543923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pNnpfILMHJ",
                "forum": "vq75kRCYuY",
                "replyto": "Qw23zSBdnK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5382/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer wijC"
                    },
                    "comment": {
                        "value": "First and foremost, I would like to express my gratitude for your insightful comments and suggestions.\n\nPlease check above 'Initial Response to Reviewer Comments'.\n\n1. Three-factor-rule and OTTT\n\nSince OTTT is also based on backpropagation, its three-factor rules may be somewhat similar to those of SOLO.\n\n2. accuracy\n\n We aim to compare the performance of learning algorithms using the same training configuration. Therefore, we use the results shown in AutoSNN as a baseline. Additionally, we compare BPTT, STBP, and SOLO under the same training configuration, which includes the utilization of the pPLIF neuron model, the accumulative neuron model, regularization, and etc.\n\n However, unlike the regularization technique used in AutoSNN, we do not employ spike regularization [1]. This approach involves counting all spikes and incorporating them as an additional term in the weight update equation, which requires extra hardware operations and implementation. Instead, SOLO relies on the regularization effect of the accumulative neuron model, which operates similarly to a neuron. This method does not require any additional regularization term in the weight update equation, making it more advantageous for hardware implementation.\n\nFurthermore, AutoSNN is originally composed of 13 convolutional layers and 1 fc layer, and through light-weighting, it is configured with 11 convolutional layers. (See Appendix Figure 4) However, recent state-of-the-art (SOTA) models employ bigger models such as ResNet 18, 34, 50, and others; hence, they are expected to exhibit higher accuracy on large datasets like CIFAR-100 and ImageNet. \n\n\n[1] Pellegrini, et al. Low-activity supervised convolutional spiking neural networks applied to speech commands recognition. 2021 IEEE Spoken Language Technology Workshop."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5382/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231669185,
                "cdate": 1700231669185,
                "tmdate": 1700234057353,
                "mdate": 1700234057353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]