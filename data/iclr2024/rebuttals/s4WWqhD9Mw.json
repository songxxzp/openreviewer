[
    {
        "title": "Holmex: Human-Guided Spurious Correlation Detection and Black-box Model Fixing"
    },
    {
        "review": {
            "id": "tLxi9lbDmM",
            "forum": "s4WWqhD9Mw",
            "replyto": "s4WWqhD9Mw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_cHZ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_cHZ2"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach for detected spurious correlations in datasets and transfer editing to transfer the interpretable knowledge to a black box model and improve model accuracy on various tasks. The paper discussed how to detect spurious correlations between concepts in the images and class labels using vision language models such as CLIP. It proposes to train 2 white box models based on frozen CLIP model\u2019s autoencoder backbone and trainable MLP layer the weights of which represent the interpretable importance scores of the concepts (similar to TCAV). One of the white-box models contains the spurious concepts and the other one doesn\u2019t. The differences between 2 white box models are then transferred to the black-box model.\nThe authors conduct multiple experiments to show the effectiveness of their approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper addresses an important problem of identifying and fixing spurious correlations in vision models.\n2) It discusses the challenge of entangled concepts and proposes a technique to improve disentanglement using a baseline/neutral concept such as the concept of others.\n3) The paper performs through experimentation for different types of spurious correlations (Co-occurrence, Picture style and class attribute)."
                },
                "weaknesses": {
                    "value": "1) The paper has multiple important contributions but they are a bit intertwined. In some cases it sounds that the authors use the term bias when they refer to spurious correlations. It would be good to make the terminology consistent and clear.\n2) Overall I think that it is a bit hard to follow the paper in terms of understanding the full picture. There are multiple models involved and figure 1 attempts to explain it but it is unclear what bias is and what `compare weights with concept vectors` really means.\n3) It is unclear how the human is involved in the guiding of spurious correlation detection and model fixing. It seems that according to the algorithm listing 1, the output of the algorithm is presented to humans but it is unclear how humans guide the process as the title of the paper suggests. \n4) In Figure 4 it is unclear how we decide to incorporate the spurious example C_cat into the white-box. How is human involved in that process ?\n\nMinor\n\neep learning models -> deep learning models"
                },
                "questions": {
                    "value": "1) How scalable is the proposed method  ?\n2) Is accuracy the main metric used in evaluation experiments ?\n3) Why are the experimental results mainly focused on showing the advantage for ensemble models ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821364418,
            "cdate": 1698821364418,
            "tmdate": 1699636436444,
            "mdate": 1699636436444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HoFboEXPCN",
                "forum": "s4WWqhD9Mw",
                "replyto": "tLxi9lbDmM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Questions in weakness**\n\nThe definition of bias in our experiments can be found in the section of experiments, and the `compare weights with concept vectors' part is described in detail in section 4. Algorithm 1 presents a list like Figure 10 in the Appendix to humans, then humans will check if there are spurious correlations. After humans decide which concept is a spurious correlation, the fixing method in Figure 4 can be applied.\n\n**Q1: How scalable is the proposed method**\n\nWe applied our method on a large dataset of 40k images (the Celeb-A task), and the running time of the whole method is within 10 mins.\n\n**Q2: Is accuracy the main metric used in evaluation experiments**\n\nYes, it is.\n\n**Q3: Why are the experimental results mainly focused on showing the advantage of ensemble models?**\n\nPeople may think that Ensembling is very similar to Transfer Editing as they both do arithmetic among different models and can be helpful for OOD settings. Therefore, we conduct experiments to show the difference between the two methods as well as how we can combine the two methods together to further improve the performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663352822,
                "cdate": 1700663352822,
                "tmdate": 1700663352822,
                "mdate": 1700663352822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rzdtIFjy1F",
            "forum": "s4WWqhD9Mw",
            "replyto": "s4WWqhD9Mw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Holmex for detecting and mitigating spurious correlations based on concept vectors. In spurious correlation detection, the method is based on CLIP and makes two contributions: (1) subtracting a background concept vector (Section 4.1.2) and (2) proposing a new algorithm for stable detection of spurious correlation. In spurious correlation mitigation, the paper proposes transfer editing to mitigate spurious correlations in a black box model. The experiments are conducted on multiple datasets and tasks to show Holmex\u2019s performance on spurious correlation detection and mitigation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper studies an important problem.\n* The code is released for better reproducibility."
                },
                "weaknesses": {
                    "value": "## Concerns about the method\n\n### Subtracting background concept vector (Section 4.1.2) \nFirst, I am confused by the motivation of this part of the method. I understand the argument that irrelevant concepts (e.g., cat and airplane) have high cosine similarities. However, I am completely lost for the \u201cmodel editing experiment\u201d where \u201ca linear layer after the concept activation layer\u201d was trained. Such as the model editing experiment was not introduced before and the details are completely left to Appendix A.1, which was not clear to me either. Second, I wonder why not a simple alternative solution would not suffice. Following the equation on the bottom of Page 4, we can compute \n$\nP(y = c \\mid z) = \\frac{\\exp(t_c^\\top z / T) }{\\sum_{c' \\in C} \\exp(t^\\top_{c'} z / T )}\n$\n, where $T$ is temperature in softmax, $c$ is one concept, and $\\mathcal{C}$ is the set of all concepts. You can choose a low temperature to reduce the similarity among different concepts.\n\n### Transfer difference of logits (step 2 in Section 5.1, page 7)\nDifferent models (i.e., white-box and black-box) can have different scales in logits. Although the paper has a discussion of \u201cThe scale of logits\u201d on page 7, my question is still not answered. \n\n## Concerns about the experiments\n\n### Datasets and Metrics\nI appreciate the authors' efforts in doing experiments for three types of biases. However, I don\u2019t think the paper explains the motivation for creating new evaluation settings and metrics. There are many previous benchmarks and evaluation settings for both (1) spurious correlation detection ([1,2] and (Wu et al., 2023)) and (2) bias mitigation benchmarks ([3-6]).\n\n### Comparison Methods\nThe proposed method is only compared with a limited number of methods. For spurious correlation detection, the paper is only compared with the PCBM method and its variants. Why not compare with DISC (Wu et al., 2023), which is also a concept-based method? For spurious correlation mitigation, many methods, especially methods that do not rely on concept vectors [1,2,7-9], are not compared.\n\n\n## References\n\n[1] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re, \u201cDomino: Discovering Systematic Errors with Cross-Modal Embeddings,\u201d in ICLR, 2022.\n\n[2] Gregory Plumb, Nari Johnson, Angel Cabrera, and Ameet Talwalkar, \u201cTowards a More Rigorous Science of Blindspot Discovery in Image Classification Models,\u201d TMLR, 2023.\n\n[3] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu, \u201cOoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,\u201d in CVPR, 2022.\n\n[4] Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang, \u201cDistributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization,\u201d in ICLR, 2020.\n\n[5] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim, \u201cA Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others,\u201d in CVPR, 2023.\n\n[6] Robik Shrestha, Kushal Kafle, and Christopher Kanan, \u201cAn Investigation of Critical Issues in Bias Mitigation Techniques,\u201d in WACV, 2022.\n\n[7] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin, \u201cLearning from Failure: Training Debiased Classi\ufb01er from Biased Classi\ufb01er,\u201d in NeurIPS, 2020.\n\n[8] Evan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn, \u201cJust Train Twice: Improving Group Robustness without Training Group Information,\u201d in ICML, 2021.\n\n[9] Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel, \u201cEnvironment Inference for Invariant Learning,\u201d in ICML, 2021."
                },
                "questions": {
                    "value": "I expect the authors to address my concerns in the response:\n\n1. Why not use software with temperature to address the problem of irrelevant concepts with high similarity?\n2. Do you assume that white-box and black-box models share a similar logit scale? If so, this approach is not generalizable enough to claim the \u201cblack-box model fixing.\u201d\n3. Why create new evaluation settings with new metrics?\n4. Add experiments to compare with a broader range of methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822384768,
            "cdate": 1698822384768,
            "tmdate": 1699636436338,
            "mdate": 1699636436338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rsnZU1mQeD",
                "forum": "s4WWqhD9Mw",
                "replyto": "rzdtIFjy1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: Why not use softmax with temperature to address the problem of irrelevant concepts with high similarity?**\n\nWe didn't understand what you meant by using temperature to address the issue. The irrelevant concepts with high cosine similarity will be an issue when we represent the concept with this raw text embedding and capture the concept strength by inner-product with raw text embedding with image embeddings. It is not an issue in the original CLIP classifier. We calculate the similarity by cosine-similarity, its range is from -1 to 1. If using cosine-similarity divided by $T$, then its range will change from $-1/T$ to $1/T$. The similarity does not depend on the absolute magnitude. It depends on the relative magnitude.\n\n**Q2: Do you assume that white-box and black-box models share a similar logit scale?**\n\nThere is no free lunch for the black-box model fixing. \nConsider a black-box model that just randomly outputs results. No method could cure this black-box model, and the only thing we can do is ignoring its outputs. One important intuition is that a normal model should output a proper probability distribution over class labels. For example, if there are 80\\% rainy samples and 20\\% sunny samples for the same given input $x$ in the training set, then a normal model will output a probability distribution similar to $(0.8, 0.2)$. Since the mapping between probabilities and logits is bijective (if we ignore the constant shift), we can derive that the logits are roughly $(1.375+constant, 0+constant)$ based on the probabilities $(0.8, 0.2)$. Thus, a normal model's logits can be roughly decided by the proper probabilities. As a result, if the model does not output probabilities too different from the proper ones, applying $y_b + \\Delta$ should keep logits on the right scale.\n\n**Questions about Comparison Methods**\n\nWe discuss the difference between DISC and our method, why we cannot compare it under our setting, and why we raise our setting. Those discussions are in the comment reply of comparing to DISC. We add a new section in Appendix H to illustrate the difference between our method and the others. We highly recommend reading those tables in Appendix H for a better understanding. As for those non-concept-based methods, we cannot make a fair and reasonable setting, neither. We use the example of salience-map-based methods to illustrate the difficulty and please refer to our reply to review NL8r for this part."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663245166,
                "cdate": 1700663245166,
                "tmdate": 1700663245166,
                "mdate": 1700663245166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JMbv9Fs2Gu",
                "forum": "s4WWqhD9Mw",
                "replyto": "rsnZU1mQeD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
                ],
                "content": {
                    "comment": {
                        "value": "### Subtracting background concept vector\nNote that temperature is used in CLIP pertaining (see \"scaled pairwise cosine similarities\" in Figure 3 \"Numpy-like pseudocode for the core of an implementation of CLIP.\" in the original CLIp paper [10]). Thus, it is reasonable to adjust the scale of the cosine similarity so that the similarity of unpaired concepts can be reduced. Although the authors argue that the scale of the cosine similarity will change after applying temperature, it is noteworthy that the ultimate goal is to compute the score between the concept of an image embedding, where the scores can still be more discriminative among different concepts after applying the temperature.\n\n### Scale of logits\nI appreciate the authors' explanation of the scale of logits, which addressed my concerns.\n\n### Benchmark and Comparison Methods\nI partially agree with the authors on the difficulties in comparing against previous concept-based bias detection methods on bias detection benchmarks. However, I still believe that it is necessary to compare with previous bias mitigation methods, including ones without relying on concepts [1,2,7-9] on standard bias mitigation benchmarks [3-6].\n\nSince I still have unaddressed concerns, I keep my rating as \"3: reject.\"\n\n#### References\n\n[10] Learning Transferable Visual Models From Natural Language Supervision. https://arxiv.org/abs/2103.00020"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718962559,
                "cdate": 1700718962559,
                "tmdate": 1700718962559,
                "mdate": 1700718962559,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OGKayoAPyR",
            "forum": "s4WWqhD9Mw",
            "replyto": "s4WWqhD9Mw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_NL8r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_NL8r"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Holmex, a method designed for human-guided spurious correlation detection and black-box model fixing. It enables humans in the deep model debugging process by addressing two main tasks:\n\nDetecting Spurious Correlations: Holmex uses pre-trained vision-language models to create separable vectors representing high-level and meaningful concepts. It proposes a novel algorithm based on these concept vectors to detect conceptual spurious correlations in training data, and this algorithm is more stable than previous methods.\n\nFixing Biased Black-Box Models: Unlike prior approaches that focus on making biased models interpretable and editable, Holmex is compatible with arbitrary black-box models. It introduces a novel technique called \"transfer editing\" to transfer revisions made in interpretable models to correct spurious correlations in black-box models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The strengths of the paper are as follows:\n\n1. Improved Concept Embeddings: The paper enhances the quality of concept embeddings by reducing the entanglement of raw text embeddings. It achieves this by subtracting a vector of the background word, which is a useful contribution. This improvement is crucial for accurate detection of spurious correlations.\n\n2. Novel Detecting Algorithm: The paper introduces a novel detecting algorithm that is specifically designed to reveal correlations between concepts and labels in a stable manner. This algorithm enhances the reliability and stability of the spurious correlation detection process.\n\n3. Transfer Editing Technique: The paper proposes a transfer editing technique, which is a novel method for transferring revisions made by humans in white-box models to black-box models. This approach enables the fixing of spurious correlations in black-box models, making it a versatile and impactful contribution.\n\nThe paper conducts extensive experiments on multiple datasets with different types of biases, including co-occurrence bias, picture style bias, and class attribute bias. This demonstrates the effectiveness and applicability of the Holmex method across a range of real-world scenarios, which is a significant strength in showcasing its practical utility."
                },
                "weaknesses": {
                    "value": "The paper does not cite several works in this domain. Some of the missing citations are:\n\n1. Salient ImageNet: How to detect spurious correlations in deep learning? ICLR 2022.\n2. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. ICLR 2023.\n3. Wilds: A benchmark of in-the-wild distribution shifts. PMLR, 2021.\n\nSalient ImageNet provides a scalable methodology for identifying spurious correlations at scale. The paper does not include any comparison with that method, rather no citation is provided. Similarly, the latter paper provides a method for robustifying against spurious correlations. Again, no citation provided. \n\nThis provides a strong evidence that the paper is written without a thorough research of the prior work."
                },
                "questions": {
                    "value": "There is no comparison against several of the group robustness methods presented in the prior works. Given the extent of the literature on robustness against spurious correlations, results comparing the accuracy of the proposed method against the baseline are not acceptable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698871516793,
            "cdate": 1698871516793,
            "tmdate": 1699636436258,
            "mdate": 1699636436258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UMDnkvh2zc",
                "forum": "s4WWqhD9Mw",
                "replyto": "OGKayoAPyR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We have cited all the mentioned papers in the revision paper, and we added a new section in Appendix H to illustrate the difference between our method and the others. We believe reading those tables in Appendix H can help better understand the difference. For the task setting in Wilds, we discuss the difference in the common reply about DISC. Below, we use the salience-map-based method as an example to show its limitations and why we cannot conduct the comparison. \nWe have done thorough research on the prior work and those discussions about the salience map can be also found in related work.\n\n**Limitation of salience map and why we cannot compare with it**\n\n\\noindent\nThe first limitation of the salience map is that it cannot bring high-level interpretations. Take the fire truck in Canberra as an example. Since the fire trucks in Canberra are in yellow, `yellow' becomes a spurious correlation when we apply the model to other cities. If we use a salience map for interpretation, it might focus on the fire truck's body. However, we cannot tell the detailed reason why the model pays attention to it. The reason might be the color, texture, or other things that make the model see the fire truck's body. Thus, it is hard to evaluate if it is correct when a salience map shows the truck body. Another limitation is that the salience map only interprets a single image instead of a class. In contrast, our method can provide class-level interpretations. Those differences make the comparison between our method and the salience map not appropriate in a fair and reasonable setting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663104136,
                "cdate": 1700663104136,
                "tmdate": 1700663104136,
                "mdate": 1700663104136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dOfCvMDfUs",
            "forum": "s4WWqhD9Mw",
            "replyto": "s4WWqhD9Mw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents HOLMEX, a method to identify a model's reliance on spurious correlations, and fix it. The general idea is to construct concept vectors using a large pre-trained model like CLIP, and then surface the correlation between a concept vector and a label to a human. The human can then use their inductive bias/domain knowledge to trim correlations that happen to be spurious. Once the spurious concept and label tuple is detected, the authors then propose a transfer technique to edit the model. In the transfer editing technique, you train two 'white-box' models: 1) where the spurious concept has been removed, and 2) where the spurious concept is present. These whitebox concepts are basically softmax linear layers on top on the clip representations for the input samples. The hope here is that the weights of these two whitebox models capture the spurious direction. To perform transfer editing, you take a difference between the logits of the two whitebox models, and add that to the logits of the blackbox model. They couple this approach with ensembling and show that such an approach leads to improved model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this papers sets out an important problem and presents a scheme for addressing that problem. I list below some nice aspects of this work: \n\n- **Modular Approach**: The paper separates detecting spurious correlation from fixing it.\n- **Clear scheme**: The paper describes its scheme very clearly, and tries to justify each step of the scheme.\n- **Demonstrates performance improvement**: The paper also shows that the transfer editing schemes and ensembling leads to improved performance across the board across all the tasks tested.\n- **Control experiment**: I liked that the authors included a control experiment for a model with no spurious signals. The approach shows the kind of null behavior you would want in that setting."
                },
                "weaknesses": {
                    "value": "Below I discuss some of the weaknesses of the scheme presented here. \n\n- **Too many moving parts**: While the scheme presented is modular. As it stands, there are several decisions that need to be gotten right for the overall scheme to work. Here is what I mean: 1) it looks like the traditional concept vectors (derived from model embeddings) are ineffective, so we need a modified version, 2) One needs a background word, 3) One needs to train a linear classifier to estimate correlations, 4) One needs to train two separate linear classifiers again to do editing for each spurious concept that you want to remove. This means that if you have 20 concepts to remove, then you would be training 40 linear classifiers to remove the effect of these 20 spurious concepts for that label alone. If any of the steps that I have listed does not work, then the entire scheme does not work. \n\n- **Over reliance on CLIP**: I think the dependence on CLIP in this work is quite worrisome. I think the CLIP embeddings are effective probably because the CLIP dataset is quite large, so those embeddings don't suffer from the issues the authors noticed. For example, imagine that you wanted to now fix a model that solely relies on CLIP embeddings as its classifier, then I assume the approach here would be ineffective? \n\n- **Logit Correction in Transfer Editing**: I am surprised that the editing scheme here works since we can simply think of this as shifting the distribution of the logits. However, it requires that the output space for the black-box models be the same size as that of the model you want to edit."
                },
                "questions": {
                    "value": "Here are some questions for the authors:\n- How do you think the transfer editing approach here relates to the task vectors approach? See: Editing models with Task Arithmetic, and Task Arithmetic in the Tangent Space. It seems like you could avoid training two white-box models by adopting the task arithmetic editing approaches in the above papers.\n\n- What is the justification for why the concept vectors from raw embeddings does not work? What if I have a model that just uses the clip embedding itself for classification, but the clip embedding has spurious correlations too? Is this approach just inheriting the limitations of clip representations? What about if I have satellite images or a setting where CLIP is not useful?\n\n- Ensembling: Did you test ensembling alone in Table 3? It would be interesting to see whether simply ensembling recovers the performance gains that you see in that table. I ask this because ensembling has been shown to give OOD benefits."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699294427129,
            "cdate": 1699294427129,
            "tmdate": 1699636436183,
            "mdate": 1699636436183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yFf5C5M3Ir",
                "forum": "s4WWqhD9Mw",
                "replyto": "dOfCvMDfUs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Question in weakness: If we have 20 concepts to remove**\n\nFor this case, you can still train 2 white-box models, one contains those 20 concepts and one does not. Thus, you don't need to train 40 models.\n\n**Q1: Transfer editing vs Task Arithmetic**\n\nOne similarity between transfer editing and task arithmetic is that they both do some meaningful arithmetic combination. One major difference between transfer editing and task arithmetic is that transfer editing does arithmetic among different models while task arithmetic does arithmetic among different weights of the same model. We think task arithmetic also has a potential in model fixing, but it may have two drawbacks. First, we need extra datasets with annotated concepts. For example, to cure the model 'cat with keyboard', we need two datasets 'dog with keyboard' and 'dog without keyboard'. Second, we need access to the weights of black-box models. Those two requirements are drawbacks of task arithmetic since sometimes fine-tuning or re-training a black-box model can be costly, and accessing the model weights is prohibited for some API-based black-box models. However, transfer editing does not need any extra datasets, and as long as we can access the final output probabilities of black-box models, we can recover their logits without knowing their weights. Thus, from this point of view, transfer editing is very different from task arithmetic.\n\n**Q2: Reliance on CLIP**\n\nComparing the equation at the end of page 4, the raw embedding replaces $t_{other}$ by a zero vector. We believe the zero vector is not as appropriate as the background vector since CLIP is not trained in that manner. In the case of satellite images where CLIP is not useful, we suggest using some related datasets or vision-language contrastive models to obtain meaningful concept vectors. So when CLIP does not work well, we just resort to other methods to obtain an off-the-shelf model. We want to emphasize again that in our paper we focus on how to extract concepts from off-the-shelf models.\n\n**Q3: More ensembling results**\n\nYes, you can find those results in Table 22 and Table 23 in Appendix."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662728878,
                "cdate": 1700662728878,
                "tmdate": 1700662728878,
                "mdate": 1700662728878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R0Cv5hbu2R",
                "forum": "s4WWqhD9Mw",
                "replyto": "yFf5C5M3Ir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their response. I am inclined to keep my current rating. Overall, my chief concern about the complexity of the proposal remains. I'll take some time to digest the rest of the feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4585/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673114428,
                "cdate": 1700673114428,
                "tmdate": 1700673114428,
                "mdate": 1700673114428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]