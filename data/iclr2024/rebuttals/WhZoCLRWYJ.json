[
    {
        "title": "Light Schr\u00f6dinger Bridge"
    },
    {
        "review": {
            "id": "Jp8ToWoM0p",
            "forum": "WhZoCLRWYJ",
            "replyto": "WhZoCLRWYJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_o21Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_o21Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a fast, light-weighted, solver for Schr\u00f6dinger bridge by using a Gaussian mixture parametrization on the energy potentials. This simplifies computation of the static EOT map that would otherwise be intractable, resulting in efficient learning process. Extensive experiments are conducted on low/mid dimensional benchmark, single-cell dataset, and unpaired image translation in latent space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is notably simple and elegant. It effectively combines key insights from previous works and elevates them to address a significant problem within the SB community.\n\n- The paper is well-written, and the thorough comparison to related works, especially in Table 1, is particularly valuable. Additionally, the comprehensive discussions in the appendix about limitations and broader impacts are appreciated."
                },
                "weaknesses": {
                    "value": "- The proposed method on image dataset requires pretrained latent space (512 dimension) that is already structurally informative.\n\n- On the discussion of tractable / real-world SB given pairing, a few important references such as \"Aligned SB\" and \"image-to-image SB\" are missing. \n\n- Given that the proposed method is computationally light weighted, it'll be beneficial to have some quantitative comparison (actual runtime, memory etc) to prior works.\n\n- All \"Schrodinger\" should be changed to Schr\u00f6dinger."
                },
                "questions": {
                    "value": "- Can the author provides image experiments without the latent space? I suggest smaller dataset such as AFHQ 32 or 64 for faster evaluation given the limited rebuttal period. Otherwise, can the author provide and include discussions on the scalability of the proposed method? Given Thm 3.4, it seems like the method could be applied to these scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698511247167,
            "cdate": 1698511247167,
            "tmdate": 1699636528732,
            "mdate": 1699636528732,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OE1bmM3gpb",
                "forum": "WhZoCLRWYJ",
                "replyto": "Jp8ToWoM0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer o21Y, Thank you for your comments. Here are the answers to your questions.\n\n**(1) The proposed method on image dataset requires pretrained latent space (512 dimension) that is already structurally informative.**\n\nWe agree with the reviewer.\n\n**(2) Given that the proposed method is computationally light weighted, it'll be beneficial to have some quantitative comparison (actual runtime, memory etc) to prior works.**\n\nFollowing your question, we considered several new high-dimensional setups, provided the qualitative comparisons with several EOT/SB solvers and compared their training time. Please see **Q1 in the general answer** to all the reviewers.\n\n**(3) Can the author provides image experiments without the latent space? (...). Otherwise, can the author provide and include discussions on the scalability of the proposed method? Given Thm 3.4, it seems like the method could be applied to these scenarios.**\n\nAlthough theoretically our parameterization could approximate any SB given sufficiently many components in the mixture, practically the required number of such components may bee too large.\n\nPlease note that in **Appendix F** (limitations, point 3), we already emphasized that we develop a solver for SB applications in moderate-dimensional spaces, not for large-scale generative modeling. Just like the basic Gaussian mixture model is **not used** for generative modeling, our solver is **not aimed** for learning large-scale generative models. Still the basic Gaussian mixture is a very easy-to-use baseline for many data analysis tasks, so we do believe that our solver will play an analogous role in the field of SB. Specifically, we already see that despite its simplicity it outperforms many methods.\n\n\n**(4) All \"Schrodinger\" should be changed to Schr\u00f6dinger.**\n\nThanks for noting. Done.\n\n**(5) SB given pairing (...) references such as \"Aligned SB\" and \"image-to-image SB\" are missing**\n\nFollowing your suggestion, we added the references to **Section 4** (related works) of the revised paper.\n\n**Concluding remarks**.\nWe would be grateful if you could let us know if the explanations we gave have been satisfactory in addressing your concerns about our work. We are also open to discussing any other questions you may have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693089700,
                "cdate": 1700693089700,
                "tmdate": 1700693089700,
                "mdate": 1700693089700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RiqLh9kuL3",
                "forum": "WhZoCLRWYJ",
                "replyto": "OE1bmM3gpb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_o21Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_o21Y"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I thank the author for the response. I keep my current score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697910486,
                "cdate": 1700697910486,
                "tmdate": 1700697910486,
                "mdate": 1700697910486,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5jdbuMGSff",
            "forum": "WhZoCLRWYJ",
            "replyto": "WhZoCLRWYJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_KWE5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_KWE5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method to approximate the Schr\u00f6dinger Bridge problem. Following the connections of the Schr\u00f6dinger Bridge problem to entropic optimal transport, the authors want to learn the true entropic OT plan. They parametrize the optimal entropic OT plan as the product of two probabilities, the source measure $p_0$ and a conditional plan $\\pi(x_1|x_0)$ (up to a normalization constant). Following this novel parametrization, they show that they can optimize the parametrized plan without the knowledge of the true entropic OT plan. To deal with the normalization constant, they use a mixture of Gaussian representations to get a tractable form of each of the components. They then explain the inference and training procedure. Finally, they perform several experiments to show the practicability of the proposed method (2D synthetic data, an EOT benchmark, single-cell dynamics and unpaired image-to-image experiment)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "i) The parametrization of the entropic OT plan is novel (to the best of my knowledge) and interesting.\n\nii) The derived loss is very interesting as learning the parametrized model does not require knowing the true entropic OT map.\n\niii) The discussion on the normalization constant and the proposed parametrization (with a mixture of Gaussians) to overcome this issue is appealing (even if the computation of the normalization constant is based on existing works).\n\niv) The method has been tested in different experiments.\n\nv) The paper is clear and easy to read."
                },
                "weaknesses": {
                    "value": "1. In the single-cell experiments, the competitor results are taken from another paper [Tong et al., 2023]. As the evaluation is performed on a leave-one-domain out and training on the others, it is questionable that the training procedure was the same. I believe that some of these competitors should be reproduced by the authors to ensure that the training setting is similar, especially as the unpaired image-to-image experiment is a quantitative experiment without competitors.\n\n2. The experiments were performed in a relatively small dimensional setting: the unpaired image-to-image experiments used a pre-trained feature extractor of dimension 512, the single-cell data experiment used the representation of the 5 whitened principal components (ie dimension of 5), and the EOT benchmark data were (at most) of dimension 128. Therefore, the question of the performance of the proposed method in high-dimensional data is legitimate, especially as the authors use a mixture of Gaussian representation.\n\n3. Little is said about the statistical estimation (with respect to the data dimension and sample size) of the proposed parametrization to the true entropic OT plan $\\pi^\\star$. It is known that it also suffers from the curse of dimension [2,3].\n\n4. Some related work is missing and should be discussed and mentioned [1,2,3,4]\n\n[1] Stochastic optimization for large-scale optimal transport, Genevay et al, Neurips 2016\n[2] On the sample complexity of entropic optimal transport, Rigollet et al.\n[3] Minimax estimation of smooth optimal transport maps, Hutter et al.\n[4] Learning with minibatch Wasserstein, Fatras et al., Aistats 2020"
                },
                "questions": {
                    "value": "1. How does your method depend on the dimensionality of the data? I think that this is a legitimate question, especially with the normalization constant being approximated by a mixture of Gaussians. I recommend adding more dimensionality in some experiments (like single-cell by considering a larger number of principal components. Maybe 5, 50, 200, 500, 1000?) to study how the proposed method performs as the dimension grows. I also recommend adding this discussion to the limitation paragraph in Appendix E. \n\n2. On the single-cell experiment of the unpaired image-to-image experiment, could you show the training speed to reach convergence and the number of iterations it took? I checked the appendix and I did not find such plots. Maybe it would be interesting to compare to other OT solvers (sinkhorn or stochastic variants) on a simple problem to see the different behaviour. (I acknowledge that the Sinkhorn solvers are only usable in a discrete setting.) \n\n3. I found the limitation discussion in Appendix E interesting and it could have been in the main paper. I recommend moving it to the main paper. Maybe some discussions about related work could go in the appendix instead. \n \nI am ready to reconsider my score if the authors can reproduce some of the competitor results and consider the highest dimensional setting of single-cell data experiments (dimension of +1000). It will depend on how well the proposed algorithm will behave on medium dimensionality.\n\n\n----- EDIT POST REBUTTAL -----\n\nThank you for your answer. I have read the rebuttal.\n\n[Single-cell experiments] Thank you for the novel experiments on the single-cell trajectory problem. I find them interesting and encouraging, especially with the different dimensions. \n\nI understand the motivation for using a different metric than W_1 to compare the different approaches. However as it is not the standard metric, it is hard for me to compare your method with other standard single-cell trajectory methods. As the authors-reviewer discussion has ended (due to a late rebuttal submission), I would have appreciated seeing the Wasserstein 1 metric for the different approaches. Indeed, the novel dataset was first considered by [Tong et al., 2023] where they used the W_1 distance. As the authors did not reproduce their method with the considered metric, it is hard to compare with more standard approaches on single-cell datasets like ODE/SDE-based approaches [Tong et al., 2023]. Unfortunately, two of the considered competitive methods are not standard in the single-cell trajectory literature. Therefore, I still think that the experimental section lacks reproduced competitors especially as the metrics are different.\n\n[Novel theoretical section] Thank you for the novel appendix H. It brings some light to the proposed methodology. I suggest the authors include Theorem H.1 in the main paper.\n\nIn my opinion, this is a borderline paper. I acknowledge that the proposed approach is new and interesting but I still think that it lacks a more rigorous experimental section to understand its practical performance with the current literature. Therefore, I will keep my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Reviewer_KWE5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678941223,
            "cdate": 1698678941223,
            "tmdate": 1700752923092,
            "mdate": 1700752923092,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fypbTUzfAM",
                "forum": "WhZoCLRWYJ",
                "replyto": "5jdbuMGSff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer KWE5, Thank you for your comments. Here are the answers to your questions.\n\n**(1) the single-cell experiments (...) relatively small dimensional setting (...) I am ready to reconsider my score if the authors can reproduce some of the competitor results and consider the highest dimensional setting of single-cell data experiments**\n\nFollowing your question, we considered several new high-dimensional setups, provided the qualitative comparisons with several EOT/SB solvers and compared their training time. Please see **Q1 in the general answer** to all the reviewers.\n\n**(2) Little is said about the statistical estimation (with respect to the data dimension and sample size) of the proposed parametrization to the true entropic OT plan (...)**\n\nFollowing your question, we added a new **Appendix H** where we demonstrated that our solver experiences a usual **parametric** convergence rate in the statistical error  ($\\sim \\frac{1}{\\sqrt{N}}+\\frac{1}{\\sqrt{M}}$, where $N,M$ are sample sizes), please see **Q2 in the general response** to all the reviewers. In the future, it would be interesting to further study the trade-off between the statistical and approximation errors, their dependence on $K$ (number of components) and $D$ (dimension).\n\n**(3) Some related work is missing and should be discussed and mentioned [1,2,3,4]**\n\nPlease note that work [3] has already been mentioned in **Appendix E** (related work: discrete OT). Following your suggestion, we additionally mentioned [2,4] in **Appendix E** and [1] in **Section 4** (related work: continuous EOT).\n\n**(4) Comparison with Sinkhorn**\n\nWe have done comparison with the (discrete) Sinkhorn algorithm as a part of our additional comparison, please see again **Q2 in the general answer** to all the reviewers and Appendix C.4. We see that for small $\\epsilon$ our solver yield even faster convergence than the Sinkhorn algorithm.\n\n**Concluding remarks**.\nWe would be grateful if you could let us know if the explanations we gave have been satisfactory in addressing your concerns about our work. If so, we kindly ask that you consider increasing your rating. We are also open to discussing any other questions you may have.\n\n**References**\n\n[1] Stochastic optimization for large-scale optimal transport, Genevay et al, Neurips 2016\n\n[2] On the sample complexity of entropic optimal transport, Rigollet et al.\n\n[3] Minimax estimation of smooth optimal transport maps, Hutter et al.\n\n[4] Learning with minibatch Wasserstein, Fatras et al., Aistats 2020"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692995202,
                "cdate": 1700692995202,
                "tmdate": 1700692995202,
                "mdate": 1700692995202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PFm3cOi7FH",
            "forum": "WhZoCLRWYJ",
            "replyto": "WhZoCLRWYJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_cSx4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_cSx4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a fast Schrodinger Bridge (SB) solver based on the parameterization of Schrodinger potentials. SB is a dynamic version of the Entropic Optimal Transport (EOT), and there exists plenty of solvers for these problems, they requires complex parameterization by the form of neural networks / or is sensitive with the entropy regularization parameters, and thus are costly in terms of evaluation on larger scale datasets. The authors of this work propose a solution to this problem by consider a settings where the continuous SB is associated with a Wiener prior (as a reference measure). The key idea is to consider the parameterization of the Schrodinger potential as a mixture of Gaussian, and rewrite the SB optimization objective following this, with easy to compute mean and (scalar diagonal) covariance matrix. The authors provide some theoretical analysis of their algorithm, along with empirical demonstrations on synthetic dataset and realistic dataset (single cell data population dynamic and image-to-image translation)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Algorithms based on well-studied theory of SB and EOT.\n- The paper is well-written with clear structure.\n- Well-motivated problem: lightweight solver is much needed for problem that usually requires heavy computational power."
                },
                "weaknesses": {
                    "value": "* **Novelty of the paper:** it seems like the paper is just a combination of the two previous works [1, 2]. Admittedly, the authors have clearly elaborated in their paper of such cases, but I do not see the clear novelty in the methodology part. Even on the proof of the universal theorem of the Gaussian mixture model, one can see that half of it is straightforward calculation from the two aforementioned paper.  \n* **Unclear benefit the of LightSB solver in realistic setting:** for the single cell dataset using W1 distance as a metric, LightSB's performance leaves some gaps with the two best methods, however I do not see author's comment about this. The other benchmark on unpaired image-to-image translation is very hard to judge, as for this task there is no quantitative metric to compare with other solvers. I do not know why the authors omit unconditional image generation tasks, as this is an important and popular benchmark that has FID as a standard metric. Morevoer, there have already exist results on some of the neural EOT solvers or the diffusion SB solver (using iterative proportional fitting)/diffusion SB matching (using iterative Markovian fitting), or flow matching/rectified flow in this task, so it is easy to compared with the baseline. \n*  **Questionable theoretical result:** in the proof the universality of the Gaussian mixture parameterization for SB (theorem 3.4), in the paragraph below equation (29), I fail to understand why the authors wrote \n\n> \"Besides, it also has scalar covariances of its components because multiplier $exp(\u2212 |x_1|^2 /2\\epsilon )$\u2019s covariance is scalar itself\" \n\n, but what I understood in the paper's settings is that $x_1 \\sim \\pi_1$ is an unknown distribution, without assumptions on its parametric form. This is a key argument, as without it the factorization failed to be what the authors claimed in the statement of the theorem, I hope the authors could clarify this to me, otherwise it would be a hole in the proof of an important analysis of the paper.    \n\n[1] Petr Mokrov, Alexander Korotin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. arXiv preprint arXiv:2304.06094, 2023.  \n[2] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems, 2023a."
                },
                "questions": {
                    "value": "See weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Reviewer_cSx4"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751434201,
            "cdate": 1698751434201,
            "tmdate": 1700707733481,
            "mdate": 1700707733481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qCAkqYOTB0",
                "forum": "WhZoCLRWYJ",
                "replyto": "PFm3cOi7FH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "Dear Reviewer cSx4, Thank you for your comments. Here are the answers to your questions.\n\n**(1) Novelty [...]: [...] the paper is just a combination of the two previous works [1, 2]. Admittedly, the authors have clearly elaborated in their paper of such cases, but I do not see the clear novelty in the  methodology part.**\n\nIn the mentioned paper [2], the authors propose a scheme that allows to construct, for a given distribution $p_0$, a new distribution $p_1$ with a known conditional entropy regularized optimal transport (entropic OT) plan $\\pi^*(y|x)$. However, the authors **did not propose any algorithm** to solve entropic OT between two given distributions $p_0$ and $p_1$.\n\nIn the other paper [1], the authors propose a new **algorithm** based on the connection between entropic OT in dual form and energy based models. Due to the use of energy-based like objective, they need to use Langevin sampling at both training and inference steps, **which is very time consuming.**\n\nIn our paper, we propose **an lightweighted and fast algorithm that allows to directly minimize the KL-divergence KL** $(\\pi^*||\\pi\\_{\\theta})$ between the parametric approximation $\\pi\\_{\\theta}$ and the solution to the entropic OT $\\pi^*$ without knowing $\\pi^*$ and even without needing to have samples of it (Proposition 3.1). Furthermore, we show that by using Gaussian mixture parameterization, this KL minimization can be performed by a standard gradient descent algorithm without the need to use time-consuming Langevin dynamics. Moreover, we proved that such parameterization has a universal approximation property (Theorem 3.4) similar to the Gaussian mixture model.\n\nWe believe this is a sufficient amount of novelty to demonstrate that our work is not just a combination of prior works.\n\n**(2) Even on the proof of the universal theorem of the Gaussian mixture model, one can see that half of it is straightforward calculation from the two aforementioned paper.**\n\nWe respectfully disagree. The only thing where the proof may intersect with the above-mentioned papers [1,2] is the usage of the duality form from weak OT. This generic known result just serves an intermediate step to guarantee the existence of the nearly-optimal continuous dual variable (stage 1 of the proof). The main (*the trickiest and the longest*) part of the proof (stage 2) is to demonstrate that the dual form can be approximated arbitrarily well with the Gaussian mixtures. This is highly **non trivial** due to the necessity to simultaneously control the approximation error for the potential and its $C$-transform. To our knowledge, this principal analysis is completely novel.\n\n*We emphasize that, to our knowledge, our Theorem 3.4 is the first ever universal approximation result for the SB.*\n\n**(3) Unclear benefit the of LightSB solver in realistic setting: for the single cell dataset using W1 distance as a metric, LightSB's performance leaves some gaps with the two best methods, however I do not see author's comment about this.**\n\nThe main benefit of our solver is the simplicty and its speed. First, it is as simple as the well-celebrated Gaussian mxiture for the density estimation. Second, our solver converges on CPU in 1 minute while the rest methods require longer training times. As one may see, e.g., from Tables 3 and 4, despite its simplicity, *our method already beats many existing complex approaches.* Please also see **Q1 in the general response** to all the reviewers.\n\n**(4) The other benchmark on unpaired image-to-image translation is very hard to judge (...). I do not know why the authors omit unconditional image generation tasks (...).**\n\nIn our paper, we emphasize that we develop a solver for SB applications in moderate-dimensional spaces, not for large-scale generative modeling. Just like the basic Gaussian mixture model is **not used** for generative modeling, our solver is **not aimed** for learning large-scale generative models. Still the basic Gaussian mixture is a very easy-to-use baseline for many data analysis tasks, so we do believe that our solver will play an analogous role in the field of SB. Specifically, we already see that despite its simplicity it outperforms many methods."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692693047,
                "cdate": 1700692693047,
                "tmdate": 1700692693047,
                "mdate": 1700692693047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fTLBejAhzf",
                "forum": "WhZoCLRWYJ",
                "replyto": "8paFuziIJC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_cSx4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_cSx4"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "My concern regarding the proof of the universality is cleared. I am therefore raise the score to 5, as I see the paper's merit and clear motivation for solving the SB problem fast and requires less resources than other baselines, but still have concern over the novelty of this paper and the lack of unconditional image generation task.\n\nI have a quick quetion regarding the new benchmarks in Section 5.3 however: why did the authors use different metric (energy distance) than before, which is a variant of Bures-Wasserstein metric, that in my opinion is fairer to compare (assumed) Gaussian distributions? For three different benchmarks now I see the authors used three different distances as metrics. \n\nNote that since the author submitted the rebuttal very late into the discussion phase, I have not time to review the additional theoretical section in the Appendix yet (regarding statistical property)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707716085,
                "cdate": 1700707716085,
                "tmdate": 1700707716085,
                "mdate": 1700707716085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nie7wyjszo",
            "forum": "WhZoCLRWYJ",
            "replyto": "WhZoCLRWYJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_o9rU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_o9rU"
            ],
            "content": {
                "summary": {
                    "value": "Current SB solvers often have complex neural parameterization and time-consuming training procedures due to minimax. This work proposed a novel parameterization technique and a non-minimax training method to bypass the issues above, with the aim at offering a lightweight and easy-to-use SB baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strong motivation: the complex neural parameterization and time-consuming training procedures do hinder the application of current SB methods.\n\nClear presentation: the background knowledge and preliminaries are clearly presented. The connections with important references, such as (Tong et al., 2023), are highlighted and summarized in Table 1. The narrative is highly structural, with words and paragraphs to offer overviews of important sections.\n\nSoundness: the work seems highly self-contained: learning objectives, training/inference strategies, and theoretical properties. Limitations are also well discussed in appendix, some of which are examined through experiments."
                },
                "weaknesses": {
                    "value": "1. Equation (5) seems incomplete or not well-defined. The equation is a crucial part of the methodology, and its clarity is essential for both understanding the method and replicating the results.\n\n2. It is pointed regarding other SB solvers that `they expectedly require time-consuming training/inference procedures.`. The authors state that their method avoids `time-consuming max-min optimization, simulation of the full process trajectories, iterative learning, and MCMC techniques,` which are commonly employed in existing solvers. While this claim holds conceptual interest, empirical evidence to substantiate this would significantly strengthen the paper.\n\nIn summary, although there exist some issues with this work, most of them seem correctable. I look forward to seeing the author feedback and the revised manuscript."
                },
                "questions": {
                    "value": "See the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Reviewer_o9rU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841615803,
            "cdate": 1698841615803,
            "tmdate": 1700631073011,
            "mdate": 1700631073011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "74DTckDBzI",
                "forum": "WhZoCLRWYJ",
                "replyto": "nie7wyjszo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal answer"
                    },
                    "comment": {
                        "value": "Dear Reviewer o9rU, thank you for your comments. Here are the answers to your questions.\n\n**(1) Equation (5) seems incomplete or not well-defined.**\n\nIt seems like we do not completely understand your comment. Equation (5) just explains that we would like to find the best approximation (in terms of KL divergence) of optimal plan $\\pi^{*}$ in a given set of distributions $\\pi_{\\theta}$ indexed by a parameter $\\theta$. The particular parameterization is introduced in the following narrative (equations 7 and 9).\n\n**(2) It is pointed regarding other SB solvers that they expectedly require time-consuming training/inference procedures. (...) While this claim holds conceptual interest, empirical evidence to substantiate this would significantly strengthen the paper.**\n\nFollowing your question, we considered several new high-dimensional setups, provided the qualitative comparisons with several EOT/SB solvers and compared their training time. Please see **Q1 in the general answer** to all the reviewers.\n\n**Concluding remarks**. \nWe would be grateful if you could let us know if the explanations we gave have been satisfactory in addressing your concerns about our work. We are also open to discussing any other questions you may have."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692395805,
                "cdate": 1700692395805,
                "tmdate": 1700692395805,
                "mdate": 1700692395805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9GxQXRmJp8",
                "forum": "WhZoCLRWYJ",
                "replyto": "74DTckDBzI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_o9rU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_o9rU"
                ],
                "content": {
                    "comment": {
                        "value": "Hi authors,\n\nThanks for the detailed explaination and I have read all of them. I really appreciate you taking the time to answer my question. I will keep my rating and I do think this is an interesting and insightful paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723118289,
                "cdate": 1700723118289,
                "tmdate": 1700723118289,
                "mdate": 1700723118289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iEOwitaSaN",
            "forum": "WhZoCLRWYJ",
            "replyto": "WhZoCLRWYJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_rvYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5284/Reviewer_rvYk"
            ],
            "content": {
                "summary": {
                    "value": "Summary\n1) The authors propose a novel light solver for continuous SB with the Wiener prior, i.e., EOT with the quadratic transport cost. The solver has a straightforward non-minimax learning objective and uses the Gaussian mixture parameterization for the EOT/SB, which avoids the time-consuming max-min optimization, simulation of the full process trajectories, iterative learning, and MCMC techniques that are in use in existing continuous solvers.\n2) The authors show that their novel light solver provably satisfies the universal approximation property for EOT/SB between the distributions supported on compact sets.\n3) The authors demonstrate the performance of the light solver in a series of synthetic and real-data experiments, including the ones with the real biological data considered in related works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- The authors provide a light solver for SB that does not rely on neural network parametrization.\n- This work provides a universal approximation for the solver which seems to be rather non-trivial. But to be honest, I'm not familiar with the proof so I'm not sure of the technical depth.\n- A minor thing but I really appreciate the authors giving a very clear description of the limitations of the solver.\n\nFollowing the rebuttal, the authors also provided statistical guarantees for the method."
                },
                "weaknesses": {
                    "value": "Weaknesses:\nThis work does not have a finite-time nor finite-sample convergence guarantee. I believe that with additional assumptions one can obtain convergence guarantees as in [1]. Furthermore, there have been multiple works on the sampling complexity of quadratic cost EOT, and given the equivalence between EOT and SB, I believe that obtaining guarantees should be feasible.\n\nI also think that the experiments are not comprehensive enough. While the results are nice, I'm wondering what is the runtime performance of the light solver against other solvers. I cannot vouch for the method if I don't know how the method would improve in terms of the quality of the result and the computational cost.\n\nThe authors did mention that the work relies on Gaussian mixture parameterization and the entropic cost. It is indeed a limitation but personally, I think it is fine. Nevertheless, I will still raise this as a potential weakness.\n\nAll and all, I like the paper but it does not convince me enough to vouch for its acceptance yet.\n\nMinor comments:\nI think the literature on EOT is not sufficient. The authors should include gradient-based EOT solvers such as [2] (both of these methods achieved the optimal O(n^2/eps) complexity, which is stronger than that of APDAGD and seems to have good performance) and a gradient-based entropic UOT solver [3].\n\nReferences:\n[1] Chen, Y., Deng, W., Fang, S., Li, F., Yang, N. T., Zhang, Y., \u2026 Nevmyvaka, Y. (2023). Provably Convergent Schr\u00f6dinger Bridge with Applications to Probabilistic Time Series Imputation. Retrieved from http://arxiv.org/abs/2305.07247\n[2] Xie, Y., Luo, Y., & Huo, X. (2023). An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem. Retrieved from http://arxiv.org/abs/2203.00813\n[3] \"On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error\".\nQuang Minh Nguyen, Hoang Huy Nguyen, Lam Minh Nguyen, Yi Zhou"
                },
                "questions": {
                    "value": "Questions:\nHow is this method compared to Langevin dynamics/Langevin Monte Carlo methods in terms of runtime and empirical results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5284/Reviewer_rvYk",
                        "ICLR.cc/2024/Conference/Submission5284/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133715716,
            "cdate": 1699133715716,
            "tmdate": 1700705422953,
            "mdate": 1700705422953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OqpYY1vdjk",
                "forum": "WhZoCLRWYJ",
                "replyto": "iEOwitaSaN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal answer"
                    },
                    "comment": {
                        "value": "Dear Reviewer rvYk, thank you for your comments. Here are the answers to your questions and comments.\n\n**(1) This work does not have a finite-time nor finite-sample convergence guarantee. I believe that with additional assumptions one can obtain convergence guarantees as in [1].**\n\nThanks for the comment, we added the suggested paper to Section 4 (related works). To be honest, we do not think that the results of paper [1] are of close relevance to our paper. The mentioned paper estimates the errors of the approximate Iterative Proportional Fitting (IPF) procedure. One of the key advantages of **our light solver** is it is free from IPF-like procedures, it *directly optimizes KL* w.r.t. the optimal plan in the certain class of parameterized plans *without any IPF-like iterations*. That is, it seems like there is no need to analyze convergence in the sence of [1].\n\n**(2) ... Furthermore, there have been multiple works on the sampling complexity of quadratic cost EOT, and given the equivalence between EOT and SB, I believe that obtaining guarantees should be feasible.**\n\nFollowing your question, we added a new **Appendix H** where we demonstrated that our solver experiences a usual **parametric** convergence rate in the statistical error  ($\\sim \\frac{1}{\\sqrt{N}}+\\frac{1}{\\sqrt{M}}$, where $N,M$ are sample sizes), please see **Q2 in the general response** to all the reviewers. In the future, it would be interesting to further study the trade-off between the statistical and approximation errors, their dependence on $K$ (number of components) and $D$ (dimension).\n\n**(3) (...) I'm wondering what is the runtime performance of the light solver against other solvers. (...).**\n\nFollowing your question, we considered several new high-dimensional setups, provided the qualitative comparisons with several EOT/SB solvers and compared their training time. Please see **Q1 in the general answer** to all the reviewers.\n\n**(4) The authors should include gradient-based EOT solvers [2, 3].**\n\nThanks for noting, we added [2,3] to the discussion of OT solvers in **Appendix D**. Please note that these solvers are discrete solvers and they are of limited relevance to our continuous setup (Section 2).\n\n**Concluding remarks**. \nWe would be grateful if you could let us know if the explanations we gave have been satisfactory in addressing your concerns about our work. If so, we kindly ask that you consider increasing your rating. We are also open to discussing any other questions you may have.\n\n**References.**\n\n[1] Chen, Y., Deng, W., Fang, S., Li, F., Yang, N. T., Zhang, Y., \u2026 Nevmyvaka, Y. (2023). Provably Convergent Schr\u00f6dinger Bridge with Applications to Probabilistic Time Series Imputation. Retrieved from http://arxiv.org/abs/2305.07247\n\n[2] Xie, Y., Luo, Y., Huo, X. (2023). An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem. Retrieved from http://arxiv.org/abs/2203.00813\n\n[3] \"On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error\". Quang Minh Nguyen, Hoang Huy Nguyen, Lam Minh Nguyen, Yi Zhou\n\n[4] Mokrov et. al. (2023). Energy-guided Entropic Neural Optimal Transport. arXiv preprint arXiv:2304.06094."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692295621,
                "cdate": 1700692295621,
                "tmdate": 1700692295621,
                "mdate": 1700692295621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G47nO84DRr",
                "forum": "WhZoCLRWYJ",
                "replyto": "OqpYY1vdjk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_rvYk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5284/Reviewer_rvYk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your detailed response. I think the results are very nice, from both a theoretical and practical standpoint. Although the added experimental result in Appendix B is not too impressive, I understand that it is due to a shortage of time and it does not undercut the main selling point of speed. That being said, I hope that in later iterations of the paper, the authors will add more experimental results and discuss more on the run time of the method.\n\nOn the other hand, I like the theoretical results of this paper and the authors did provide additional theoretical guarantees as requested and addressed all of my concerns. Thus, I will increase my score. However, I must note that the authors should add and improve the experimental results should this paper get accepted.\n\nBest wishes,"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705371028,
                "cdate": 1700705371028,
                "tmdate": 1700705371028,
                "mdate": 1700705371028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]