[
    {
        "title": "Unified Medical Image Pre-training in Language-Guided Common Semantic Space"
    },
    {
        "review": {
            "id": "4DcXvDydek",
            "forum": "XZGklkaOsL",
            "replyto": "XZGklkaOsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_vK4s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_vK4s"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a visual-language pre-training method that can handle both 2D and 3D medical image data. It aligns different modalities of image data with their corresponding diagnostic reports and enhances the correlation between different modalities using MIM-based self-distillation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-organized and clearly described, and the figures are intuitive. Creating a unified model that can effectively handle various kinds of image data is a valuable problem. The strengths of the paper include:\n\n1. The paper is well-written. The organization is clear, and the paper is easy to follow.\n2. The studied problem is meaningful. The paper offers a unified framework to integrate multi-modal medical language-guided images into semantic space, facilitating the analysis and interpretation of complicated imaging data.\n3. The approach obtains superior performance in the downstream classification and semantic segmentation tasks."
                },
                "weaknesses": {
                    "value": "The weaknesses of the paper include:\n1. The novelty appears to be somewhat incremental, as the method may seem to be a direct application of Attentive Mask CLIP proposed by Yang et al. (2023).\n2. Although the method is capable of handling 3D image data, it treats them as separate 2D images and does not consider the structural aspects of 3D data.\n3. Some implementation details need to be further explained. For instance, the structures of the segmentation decoder."
                },
                "questions": {
                    "value": "1. As mentioned above, Please provide the decoder structure used for the segmentation task.\n2. The medical semantic segmentation task on the RSNA and BCV datasets appears to involve predicting bounding boxes that indicate evidence of pneumonia, thus making it more akin to a detection problem rather than a segmentation problem.\n3. Please elucidate the distinctions between the proposed method and Attentive Mask CLIP proposed by Yang et al. (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Reviewer_vK4s"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698572479087,
            "cdate": 1698572479087,
            "tmdate": 1699636483406,
            "mdate": 1699636483406,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CWPZTuMYLX",
                "forum": "XZGklkaOsL",
                "replyto": "4DcXvDydek",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vK4s (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments, and we address the questions below:\n\n>(**W1**). The novelty appears to be somewhat incremental, as the method may seem to be a direct application of Attentive Mask CLIP proposed by Yang et al. (2023).\n\n>(**Q3**) Please elucidate the distinctions between the proposed method and Attentive Mask CLIP proposed by Yang et al. (2023).\n\n\nWe re-elaborate more here on the novelty of our method, which is quite different from Attentive Mask CLIP (ACLIP).\n\nFirst, we target an unified VL framework capable of handling various medical modalities (e.g. 2D X-rays, 3D CT), and Figure 2 shows that merging representations of different medical image modalities into a language-guided common semantic space is challenging. Naively using language guidance and VL contrastive learning (Figure 2a) and simply unifying in one model (Figure 2b) cannot achieve the goal of merging. To this end, we introduce several designs for UniMedI, and *all of the designs aim at better merging 2D and 3D medical images into a language-guided common semantic space*. To best of our knowledge, we are the first one merging 2D and 3D medical images into a unified VL framework (we also provide a table to summarize the difference between our work and existing methods in this field at the end of this answer). Particularly, ACLIP cannot handle multi-modal images, and it can only be used for single modal images. So, *ACLIP cannot be directly applied to our problem*.\n\n\nThe first design is incorporating the guidance from report via VL contrastive learning. Generally, the $[CLS]$ token in the image side is optimized with representation of the paired report. And this image $[CLS]$ token is used in the following parts of UniMedI to ensure that the integrating is guided by language. We also have a novel motivating observation here for our problem (shown in Figure 1 and introduction). We see that despite big differences, medical images from various modalities share a common semantic latent space, which captures the underlying features of an individual's health status, and such status are reflected in medical reports via language. *This motivating observation is insightful in our problem and has not been mentioned in previous works as far as we know.*\n\nThe second design is our pipeline. Since no paired 2D and 3D image data exists, so we first select informative 2D slice from 3D volume as the bridge via the proposed attentive selection strategy. And then 2D slices, 2D X-ray and 3D volume are sent to the vision encoder together, enabling better merging. *ACLIP does not have this pipeline and the attentive selection strategy, since bridging multimodal images is not handled in ACLIP.* We have updated our paper by including a subsfigure in Figure 3 to show this pipeline.\n\nThe third design is the auxiliary task of mask and recovery implemented by self-distillation. Since 2D and 3D tokens are sent to the visual encoder at the same time and a large proportion of tokens are masked, this auxiliary task can help to enhance interactions among 2D and 3D tokens, facilitating merging representations. *The mask in ACLIP is used for removing redundant information, while the auxiliary task in UniMedI is enhancing dimensional interactions.*\n\nBesides, the vision encoder has *separate tokenizer for 2D and 3D data and a shared backbone*, which is also designed for representation integration, and not used by ACLIP.\n\nIn summary, the problem we addressed in this paper is quite novel, and all designs in UniMedI aim at better solving the targeted problem. Maybe some losses are similar with other methods, but they are introduced with different purpose here, and thus brings different effects. And there are lots of difference compared with ACLIP, as we listed above.\n\n| Method | Vision-Language | Unify | Downstream Task | Input Medical Image type |\n| :------: | :---------------: | :-----: | :---------------: | :------------------------: |\n|COVIRT [1] | $\\checkmark$ | --- | Classification, Segmentation, Detection | 2D Images |\n|GLoRIA-MIMIC [2] | $\\checkmark$ | --- | Classification, Segmentation, Detection | 2D Images |\n|MGCA [3] | $\\checkmark$ | --- | Classification, Segmentation, Detection | 2D Images |\n| Joint [4]   | ---| $\\checkmark$| Classification, Segmentation  | 2D Images, 2D slices  |\n| UniMiss [5] | ---| $\\checkmark$| Classification, Segmentation  | 2D Images, 3D volumes, 2D slices |\n| UniMedI     | $\\checkmark$ | $\\checkmark$| Classification, Segmentation, Detection(Appendix)          | **2D Images, 3D volumes, 2D slices** |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725103525,
                "cdate": 1700725103525,
                "tmdate": 1700725103525,
                "mdate": 1700725103525,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DqQJGfTIMY",
            "forum": "XZGklkaOsL",
            "replyto": "XZGklkaOsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_8U1f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_8U1f"
            ],
            "content": {
                "summary": {
                    "value": "The UniMedI framework presents an innovative strategy for unifying the processing of diverse medical image modalities, especially 2D and 3D images, by employing diagnostic reports as a common semantic foundation. This approach facilitates the creation of consistent representations for different types of medical images. By harnessing the guidance provided by text, UniMedI excels in extracting pertinent visual information. It adeptly identifies impacted areas in 2D X-ray images and locates slices with lesions in more intricate 3D CT scans, thereby markedly improving coherence across various medical imaging formats."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic is new and clinically relevant.\nIt successfully develops unified representations for a variety of medical images, notably addressing both 2D and 3D image formats.\nThe framework's performance is thoroughly evaluated across 10 different datasets, encompassing a broad spectrum of medical imaging tasks including classification, segmentation, and retrieval.\nThe paper is well-structured and clearly written, making it accessible and easy to understand for readers."
                },
                "weaknesses": {
                    "value": "The framework appears to be a multi-stage learning process rather than a true end-to-end 2D/3D multi-modal learning framework. It seems to involve selecting high-attention slices from 3D images and then training a 2D image-text encoder. If this understanding is correct, it is not as useful as a unified 2D/3D multi-modal learning framework.\n\nWhile the use of t-SNE for problem definition is interesting, the paper lacks a comparative t-SNE plot in the results section to illustrate the impact of 2D and 3D co-learning. This could have provided clearer visual evidence of the model's effectiveness.\n\nThe paper does not explore straightforward alternative approaches, such as using a DENO or MAE, for learning all 2D sections from 3D volumes in conjunction with all 2D X-ray data together in a single self-supervised learning model.\n\nThere is a lack of clarity on why certain metrics, like AUC (Area Under Curve) and ACC (Accuracy), are chosen and reported in different sections. Particularly, I would suggest to use AUC over ACC in COVIDx, which could affect the clarity of the performance evaluation.\n\nMany of the results, as shown in tables such as Table 4 (AUC), Table 5, and Table 7, indicate only marginal improvements. This raises questions about the practical significance and real-world applicability of the proposed framework."
                },
                "questions": {
                    "value": "See the Weakness section. My further final decision will be decided based on the author's rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763911656,
            "cdate": 1698763911656,
            "tmdate": 1699636483299,
            "mdate": 1699636483299,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9daFc2syKl",
                "forum": "XZGklkaOsL",
                "replyto": "DqQJGfTIMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8U1f  (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed feedback on our paper!\n\n>(**W1**) The framework appears to be a multi-stage learning process rather than a true end-to-end 2D/3D multi-modal learning framework. It seems to involve selecting high-attention slices from 3D images and then training a 2D image-text encoder. If this understanding is correct, it is not as useful as a unified 2D/3D multi-modal learning framework.\n\nWe apologize for causing misunderstanding here. UniMedI is not a multi-stage learning process and it takes 2D slices and 3D images at the same time. Specifically, after extracting 2D slices, the 3D volume and 2D slices are sent to the vision encoder as inputs at the same time. With separate tokenizers, 2D and 3D tokens exists in one pass. In this way, we can use the mask and recovery task to enhance dimensional interactions. We do not have separate training stage for attentive slice selection and the whole model is updated with backprogration once each batch. To clarify more clearly, we provide explanations of UniMedI's pipeline below. In our revised paper, we have included a subfigure in Figure 3 to illustrate, and also adjusted corresponding descriptions.\n\n### Pipeline of UniMedI\nAs we have described in the first paragraph of Section 3, to overcome the challenges that no paired 2D and 3D image data exists, during the training process, UniMedI first extracts a portion of informative 2D slices from a 3D volume according to language guidance via the proposed **attentive slice selection strategy (shown in Figure 4)**. This creates 2D-3D data pairs to bridge the disparity between the two modalities. Then, selected 2D slices and 2D X-rays, together with original 3D data, are sent to **a unified vision encoder (shown in Figure 3)** to obtain image representations. \n\nOverall, UniMedI is a vision-language pre-training framework. The paired report of each X-ray/CT is encoded by the text encoder, and VL contrastive learning (shown in Figure 3) are employed to obtain a language-guided common semantic space for both 2D and 3D medical images.\n\n>(**W2**) While the use of t-SNE for problem definition is interesting, the paper lacks a comparative t-SNE plot in the results section to illustrate the impact of 2D and 3D co-learning. This could have provided clearer visual evidence of the model's effectiveness.\n\nWe would like to clarify that the impact of 2D and 3D co-learning is already shown in Figure 2. Generally, UniMedI target the goal of mapping data from various medical image modalities into the shared semantic space, which is guided by language in reports, and Figure 2 (visualizations of representations) is used to show that this goal is hard to achieve. We provide the information in Figure 2 in the following table. In summary, Figure 2 shows that naively using language guidance (2a) and simply unifying in one model (2b) cannot achieve the our challenging goal. In contrast, UniMedI, which involves 2D and 3D co-learning, can better integrate different medical image modalities.\n\n|  | How the models are trained | What information is shown |\n| -------- | -------- | -------- |\n| Figure 2a    |   Two models for 2D and 3D images are trained individually in two VLP processes, respectively.   |  There is a **significant gap** in the representation of 2D and 3D medical modality data.    |\n| Figure 2b    |  One model for 2D and 3D images are trained in one VLP process.       |    The gap between the modalities have been **narrowed compared with 2a**, but overall, the 2D X-ray representations are distributed in the upper part of the figure, while the 3D CT representations are distributed in the lower part of the figure.      |\n| Figure 2c    |  One model for 2D and 3D images are trained in UniMedI, where the connection between image modalities are largely enhanced.        |    **The representations of different modalities are distributed together and closer compared with 2b**.\n\nMoreover, per suggestion from Reviewer zUjR, **we have added visualization of data representations of two different classes (i.e., pneumonia and cardiomegaly) in Figure 6 in Appendix C.4**. As can be seen, *even data coming from different classes, UniMedI can bring together representations for medical data of different modalities, while not making features of all classes similar, well preserving semantic information from reports in the representation space*. This nicely demonstrates the effectiveness of UniMedI on integrating medical multi-modal images into a language-guided common semantic space."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732287255,
                "cdate": 1700732287255,
                "tmdate": 1700732287255,
                "mdate": 1700732287255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Iwo8L7Elea",
            "forum": "XZGklkaOsL",
            "replyto": "XZGklkaOsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_EXCm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_EXCm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a pre-training method to jointly learn 2D and 3D medical images with radiology reports. The method uses diagnostic reports as a common semantic space to create unified representations for 2D X-ray and 3D CT scans. To jointly incorporate 2D and 3D data, an attentive slice selection method was designed to select the disease-relevant 2D slices from 3D CT scans."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed UniMedI framework is designed to handle different imaging modalities (e.g., 2D X-rays, 3D CT scans), which is significant because medical imaging is inherently diverse, and most existing models are limited to single-dimension data.\n- By using associated diagnostic reports, the framework can tap into the rich semantic information that is typically underutilized in image-only models, potentially leading to more context-aware representations."
                },
                "weaknesses": {
                    "value": "- The description of the framework is not clear. It would be better if you could mark the (1)-(4) in Figure 3. \n\n- Based on the methodology description, it seems that the proposed framework is a combination of the existing methods except for the attentive slice selection. The motivation for introducing the auxiliary task is not clear. Directly saying \u201cinspired by\u2026\u201d is not a good writing style. Please explicitly present the main motivation of all your methodology components. \n\n- The medical classification experiments are weak since most selected datasets are old and many other dedicated methods already achieved great performance. Please use the latest RSNA datasets for validation as well. \n\n- For the 3D segmentation tasks, the small BCV dataset cannot provided statistically significant results (other previous works also used this BCV dataset is not a good excuse). Please use larger datasets like MICCAI FLARE and AMOS."
                },
                "questions": {
                    "value": "- The authors designed an attentive slice selection method to select the disease relevant 2D slices from 3D volume. What\u2019s the accuracy of the method? It is not validate yet but it could be easily done by testing it on some lung/abdomen tumor CT datasets that have tumor segmentation masks, e.g., check whether the selected slices cover all the tumors and compute the accuracy. \n\n- Sec 3. \u201cselected 2D input is fed into the network along with the 3D tokens..\u201d, You already covert the 3D images into 2D key slices. What are the 3D tokens here? \n\n- Please explicitly clarify the unique motivations of EMA and auxiliary tasks rather than just saying \"inspired by\"\n\nMinors: Please polish the writing:\nPage 5. Sec. 3.3 \u201cIn Section 3.1\u2026. Then in Section 3.1\u2026\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Reviewer_EXCm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771875545,
            "cdate": 1698771875545,
            "tmdate": 1700688773171,
            "mdate": 1700688773171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1i9ZIiAAWQ",
                "forum": "XZGklkaOsL",
                "replyto": "Iwo8L7Elea",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EXCm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing insightful comments on our paper. We provide clarifications to the concerns below:\n\n>(**W1**) The description of the framework is not clear. It would be better if you could mark the (1)-(4) in Figure 3.\n\n\nThank you for your valuable suggestion. We apologize if the description of our framework was unclear in the initial submission.  We have revised Figure 3 in our paper to give a clearer presentation without changing the content. The left of Figure 3 demonstrates the overall pipeline of UniMedI, and the right part of Figure 3 and Figure 4 present details on the modules used in the pipeline. Also, we have marked (1)-(4) in Figure 3 upon your request. To further address your concerns, we would like to briefly review UniMedI here.\n\n### Pipeline of UniMedI\nAs we have described in the first paragraph of Section 3, to overcome the challenges that no paired 2D and 3D image data exists, during the training process, UniMedI first extracts a portion of informative 2D slices from a 3D volume according to language guidance via the proposed **attentive slice selection strategy (shown in Figure 4)**. This creates 2D-3D data pairs to bridge the disparity between the two modalities. Then, selected 2D slices and 2D X-rays, together with original 3D data, are sent to **a unified vision encoder (shown in Figure 3)** to obtain image representations. \n\nOverall, UniMedI is a vision-language pre-training framework. The paired report of each X-ray/CT is encoded by the text encoder, and VL contrastive learning (shown in Figure 3) are employed to obtain a language-guided common semantic space for both 2D and 3D medical images.\n\n### Key module 1: the unified vision encoder (Figure 3)\n\nFor the vision encoder, first, 2D data (selected 2D slices and 2D X-rays) and 3D data (original 3D volumes) are processed by 2D tokenizer $T_{2D}$ and 3D tokenizer $T_{3D}$, respectively. Seperate tokenizers encode input data from different modalities into a series of tokens. 3D CT tokens and corresponding 2D slices tokens are concatenated together for further procesing by a shared backbone. *Note here input contains both 2D and 3D data, so we have 2D and 3D tokens at the same time.* 2D X-rays tokens directly processing by a shared backbone.\n\nThen, to enhance the cross-dimensional communication among 2D and 3D tokens, we adopt **an auxiliary task, i.e., mask and recovery**, and this task is implemented by the **self-distillation method**. Specifically, our visual encoder contains the teacher network $\\overline{E}_v$ and the student network $E_v$, where $\\overline{E}_v$ is updated by exponential moving averaged (EMA) over $E_v$. Tokens, once encoded by the tokenizer, are input into the teacher network. This process yields global and local embeddings and simultaneously outputs attention scores for each patch of the X-rays or CT scans, as outlined in **Equation 1** in main text. Depending on the scores of each patch, we mask the less significant areas (those with lower scores) for the input into the student network. Since **the student network only inputs important areas of the medical images**, the resulting visual encoding is more closely matched with the semantics included in the report. Finally, loss of the self-distillation method is applied to both the global $[CLS]$ token (i.e., $L_{icl}$) and local patch tokens (i.e., $L_{pcl}$).\n\n### Key module 2: the attentive slices selection strategy (Figure 4)\n\nThe aim of attentive slice selection is selecting informative 2D slices from the 3D CT volume. The 3D CT volume is processed by the tokenizer $T_{3D}$ and the teacher network, and thus, attention scores for patches in the CT can be obtained. Then, the scores of all tokens within one slice are averaged to derive the score of the corresponding slice. This process is referred to as the *Inter-Slice Average*, as outlined in **Equation 2** in main text. Slices with top $k$ score are selected as the connection between 2D and 3D data.\n\nNote that these scores are attention weights between the $[CLS]$ token and other patch tokens, and the $[CLS]$ token is used for VL contrastive learning. Therefore, this slice selection process is guided by text information in the report, and 2D slices containing important information in the report can be selected accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685933545,
                "cdate": 1700685933545,
                "tmdate": 1700685933545,
                "mdate": 1700685933545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1XQIWmZa67",
                "forum": "XZGklkaOsL",
                "replyto": "yneHVtuWgQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Reviewer_EXCm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Reviewer_EXCm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. Most of my concerns have been addressed. I raised my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688805494,
                "cdate": 1700688805494,
                "tmdate": 1700688805494,
                "mdate": 1700688805494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2SBq0tD9Va",
            "forum": "XZGklkaOsL",
            "replyto": "XZGklkaOsL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_zUjR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4965/Reviewer_zUjR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a unified framework for pre-training, unifying 2D (X-ray) and 3D (CT) modalities. The unification is realized by introducing language embeddings, which align the features of two unpaired images, but with the same pathological condition (e.g., pneumonia), into a similar space. Results show improved transfer learning performance in both 2D and 3D medical imaging tasks when pre-trained with language (pathological reports)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Table 4 demonstrates that leveraging a composite of 2D and 3D datasets during pre-training enhances performance.  This ablation analysis is clear and important.\n+ Introducing language-vision framework into pre-training appears promising direction since the availability of pathological reports along with X-ray and CT images."
                },
                "weaknesses": {
                    "value": "- The motivation of the paper, bridging features in 2D and 3D, is not validated (see Q1).\n- The details of the method are confusing (see Q2).\n- Lack of baseline methods for 2D and 3D pre-training (see Q3)."
                },
                "questions": {
                    "value": "1. From Figure 1, I fail to observe (c) is particularly better than (b) in terms of feature quality. I understood that the authors try to present the features of pneumonia get closer between 2D and 3D images, but this needs features of another class (e.g., nodule) as a comparison. It is possible that the proposed UniMedI makes features of all classes similar. I think the discrepancy between 2D and 3D modalities could be much larger than that among classes.\n\n2. The illustration of Figure 3 and Figure 4 is unclear to me. What are the light orange boxes used for? What is the operation after T_3D and T_2D? The main text in the method section is not corresponding to Figure 3 or Figure 4. It is unclear how Equations (1-2) were integrated into the framework.\n\n3. As a paper for pre-training, the authors did not compare with the many existing pre-trained models, neither 3D or 2D. For 2D X-ray imaging, there are many pre-trained models publicly available [e.g., 1-3]. For 3D CT imaging, a variety of public models are also missing [e.g., 4-6].\n\n4. The authors did not provide sufficient reference for the baseline methods presented in Table 1. \n\n**Reference**\n\n[1] Ma, DongAo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Nahid UI Islam, Fatemeh Haghighi, Michael B. Gotway, and Jianming Liang. \"Benchmarking and boosting transformers for medical image classification.\" In MICCAI Workshop on Domain Adaptation and Representation Transfer, pp. 12-22. Cham: Springer Nature Switzerland, 2022.\n\n[2] Yan, Ke, Jinzheng Cai, Dakai Jin, Shun Miao, Dazhou Guo, Adam P. Harrison, Youbao Tang, Jing Xiao, Jingjing Lu, and Le Lu. \"SAM: Self-supervised learning of pixel-wise anatomical embeddings in radiological images.\" IEEE Transactions on Medical Imaging 41, no. 10 (2022): 2658-2669.\n\n[3] Zhang, Xiaoman, Chaoyi Wu, Ya Zhang, Weidi Xie, and Yanfeng Wang. \"Knowledge-enhanced visual-language pre-training on chest radiology images.\" Nature Communications 14, no. 1 (2023): 4542.\n\n[4] Tang, Yucheng, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20730-20740. 2022.\n\n[5] Haghighi, Fatemeh, Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, and Jianming Liang. \"DiRA: Discriminative, restorative, and adversarial learning for self-supervised medical image analysis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20824-20834. 2022.\n\n[6] Chen, Sihong, Kai Ma, and Yefeng Zheng. \"Med3d: Transfer learning for 3d medical image analysis.\" arXiv preprint arXiv:1904.00625 (2019)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4965/Reviewer_zUjR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844233887,
            "cdate": 1698844233887,
            "tmdate": 1699636483131,
            "mdate": 1699636483131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0XIyg7uGPm",
                "forum": "XZGklkaOsL",
                "replyto": "2SBq0tD9Va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zUjR"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments on our work. We appreciate the opportunity and have addressed all your concerns point by point as follow, and also revised our manuscript accordingly.\n\n> (**W1**) The motivation of the paper, bridging features in 2D and 3D, is not validated (see Q1).\n\n> (**Q1**) From Figure 2, I fail to observe \\(c\\) is particularly better than \\(b\\) in terms of feature quality. I understood that the authors try to present the features of pneumonia get closer between 2D and 3D images, but this needs features of another class (e.g., nodule) as a comparison. It is possible that the proposed UniMedI makes features of all classes similar. I think the discrepancy between 2D and 3D modalities could be much larger than that among classes.\n\nFirst, in the revised version, we have highlighted the differences among each subfigures of Figure 2 as much as possible. In 2(b), overall, the 2D X-ray representations are distributed in the upper part of the figure, while the 3D CT representations are distributed in the lower part of the figure. In 2\\(c\\), representations of 2D and 3D images are generally merged together and closer compared with 2(b). In summary, Figure 2 shows that naively using language guidance (2a) and simply unifying in one model (2b) cannot achieve the our challenging goal, which is using text information in reports as the guidance to map data from various medical image modalities into the shared semantic space. In contrast, UniMedI can better integrate different medical image modalities.\n\nAlso, to further address your concerns, **we have visualized data representations of two different classes (i.e., pneumonia and cardiomegaly) in Figure 6 in Appendix C.4**. As can be seen, *even data coming from different classes, UniMedI can bring together representations for medical data of different modalities, while not making features of all classes similar, well preserving semantic information from reports in the representation space*.\n\nWe really agree with the reviewer on the viewpoint that discrepancy between 2D and 3D modalities could be much larger than that among classes. Therefore, Figure 6 in Appendix address this point and nicely demonstrate the effectiveness of UniMedI on integrating medical multi-modal images into a language-guided common semantic space, since Figure 6 shows semantic meaning is preserved while modalities are merged in the representation space, as we expected.\n\n> (**W2**) The details of the method are confusing (see Q2).\n\n> (**Q2**) The illustration of Figure 3 and Figure 4 is unclear to me. What are the light orange boxes used for? What is the operation after T_3D and T_2D? The main text in the method section is not corresponding to Figure 3 or Figure 4. It is unclear how Equations (1-2) were integrated into the framework.\n\nWe appreciate the reviewer's suggestion to depict our method more clearly. Firstly, we would like to elaborate more on the pipeline of UniMedI to make the whole process clear. Based on the pipeline, we further explain contents in Figure 3 and Figure 4 clearly. After that, we address your question point-to-point in order to clarify any confusion. Lastly, **we have adjusted Figures 3 and 4 without changing the content in the updated version of our paper, including clearer annotations and some accompanying explanations**. We believe these enhancements will greatly improve the clarity and comprehensibility of our framework.\n\n### Pipeline of UniMedI\nAs we have described in the first paragraph of Section 3, to overcome the challenges that no paired 2D and 3D image data exists, during the training process, UniMedI first extracts a portion of informative 2D slices from a 3D volume according to language guidance via the proposed **attentive slice selection strategy (shown in Figure 4)**. This creates 2D-3D data pairs to bridge the disparity between the two modalities. Then, selected 2D slices and 2D X-rays, together with original 3D data, are sent to **a unified vision encoder (shown in Figure 3)** to obtain image representations. \n\nOverall, UniMedI is a vision-language pre-training framework. The paired report of each X-ray/CT is encoded by the text encoder, and VL contrastive learning (shown in Figure 3) are employed to obtain a language-guided common semantic space for both 2D and 3D medical images. To make this pipeline clear, we add a subfigure to Figure 3 for illustration."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685036291,
                "cdate": 1700685036291,
                "tmdate": 1700685036291,
                "mdate": 1700685036291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eVgPiKUrP1",
                "forum": "XZGklkaOsL",
                "replyto": "2SBq0tD9Va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zUjR"
                    },
                    "comment": {
                        "value": "Most listed models [1, 2, 4, 5] are trained via self-supervised learning (SSL). Comparing SSL models with the model by vision-language (VL) pre-training is not very reasonable, since VL pre-training is dealing with some multi-modality problem, which is quite different with the SSL problem. Moreover, UniMedI also handles multimodal medical images at the same time. Therefore, the comparison might not be as fair as what we listed in the paper. Nevertheless, we provide compared results in the below table (CheXpert$^\\ast$ denoting full backbone fine-tuning and CheXpert$^\\diamond$ representing linear classification, only fine-tuning the classification head). Here, we additionally include some latest SOTA SSL methods in the general domain, further demonstrating the superiority of our method.\n\n\n| Method | Perfomance | Description |\n| :--------: | :--------: | :--------: |\n| SimMIM [1]    | 88.07 (CheXpert$^\\ast$, 100%) | Benchmark of SSL in Medical Image Classification |\n| SAM [2] | --- | Designed specific backbones for 2D and 3D data, only focus on landmark detection, Lesion matching |\n| Swin UNETR[4] | --- | Designed for specific task,  semantic segmentation of brain tumors in MRI Images |\n| DiRA [5] | 87.59 (CheXpert$^\\ast$, 100%) | Unites discriminative, restorative, and adversarial learning in a unified manner |\n| Moco v2 [7]     |  84.90 (CheXpert$^\\diamond$, 100%)    | General domain SSL method     |\n| Moco v3 [8]     |  82.02 (BCV, 100%) | General domain SSL method |\n| DINO [9]        |    82.61(BCV, 100%) | General domain SSL method |\n| UniMedI | **90.50** (CheXpert$^\\diamond$, 100%), **85.40** (BCV, 100%) |---|\n\n\nIt is evident that UniMedI outperforms all other methods, even without fine-tuning for the evaluation on CheXpert. Other methods not included in the above table are specially designed for some downstream tasks. SAM [2] is for landmark detection and Lesion matching with designed backbone, and it use separate models for 2D and 3D data, respectively. Swin UNETR [4] is designed for semantic segmentation of brain tumors in MRI Images, which cannot be applied to the same downstream tasks as our method. KAD [3] is designed for classification tasks, incorporating external knowledge and a classification head related to the disease. Med3D [6] is for transfer learning with a coarse-to-fine structure. Comparing with these specially designed methods is not a fair setting. Besides, all these designs for specific tasks are orthogonal with our methods, and can be involved in our framework for further performance improvement. Unifying all these designs can be our future work.\n\n>(**Q4**) The authors did not provide sufficient reference for the baseline methods presented in Table 1.\n\nThanks for pointing out. We have added the references in the updated version.\n\n[1] Ma, DongAo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Nahid UI Islam, Fatemeh Haghighi, Michael B. Gotway, and Jianming Liang. \"Benchmarking and boosting transformers for medical image classification.\" In MICCAI Workshop on Domain Adaptation and Representation Transfer, pp. 12-22. Cham: Springer Nature Switzerland, 2022.\n\n[2] Yan, Ke, Jinzheng Cai, Dakai Jin, Shun Miao, Dazhou Guo, Adam P. Harrison, Youbao Tang, Jing Xiao, Jingjing Lu, and Le Lu. \"SAM: Self-supervised learning of pixel-wise anatomical embeddings in radiological images.\" IEEE Transactions on Medical Imaging 41, no. 10 (2022): 2658-2669.\n\n[3] Zhang, Xiaoman, Chaoyi Wu, Ya Zhang, Weidi Xie, and Yanfeng Wang. \"Knowledge-enhanced visual-language pre-training on chest radiology images.\" Nature Communications 14, no. 1 (2023): 4542.\n\n[4] Tang, Yucheng, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20730-20740. 2022.\n\n[5] Haghighi, Fatemeh, Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, and Jianming Liang. \"DiRA: Discriminative, restorative, and adversarial learning for self-supervised medical image analysis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20824-20834. 2022.\n\n[6] Chen, Sihong, Kai Ma, and Yefeng Zheng. \"Med3d: Transfer learning for 3d medical image analysis.\" arXiv preprint arXiv:1904.00625 (2019).\n\n[7] Chen X, Fan H, Girshick R, et al. Improved baselines with momentum contrastive learning[J]. arXiv preprint arXiv:2003.04297, 2020.\n\n[8] Chen X, Xie S, He K. An empirical study of training self-supervised vision transformers. In 2021 IEEE[C]//CVF International Conference on Computer Vision (ICCV). 9620-9629.\n\n[9] Caron M, Touvron H, Misra I, et al. Emerging properties in self-supervised vision transformers[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 9650-9660."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685677664,
                "cdate": 1700685677664,
                "tmdate": 1700686130500,
                "mdate": 1700686130500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]