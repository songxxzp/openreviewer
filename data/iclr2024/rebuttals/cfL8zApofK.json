[
    {
        "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Game"
    },
    {
        "review": {
            "id": "8sjR3htrRh",
            "forum": "cfL8zApofK",
            "replyto": "cfL8zApofK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
            ],
            "content": {
                "summary": {
                    "value": "- This paper introduces a evaluation framework for Large Language Models using scorable negotiation games\n- LLMs show capabilities in arithmetic, inference, exploration, and planning in the game\n- By employing a systematic zero-shot Chain-of-Thought prompting, the paper shows that LLMs can effectively negotiate and achieve successful deals\n- The study quantifies LLM performance across various game setups, highlighting differences between LLMs"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well-written, and the game is clearly defined."
                },
                "weaknesses": {
                    "value": "- The inherent values of large language model, cultivated during their training phase, predispose them towards universally accepted \"good\" objectives. This inclination becomes evident in scenarios like negotiations, where large language model might naturally champion causes like environmental conservation. However, even with a predefined game context and role, the large language model might not consistently align with the reward mechanism of its assigned role. For instance, when trained with contrary objectives, such as being \"malevolent\", an large language model might easily counter proposals it would have otherwise endorsed. Given this predisposition influenced by underlying values, I question the appropriateness of evaluating an large language model's capabilities in a game setting susceptible to such biases.\n\n- The authors have conceptualized a text-based game to assess LLM agents' actions and subsequently compared different LLMs' performances within this framework. However, both the foundational premise of the paper and the employed research methodology lack novelty."
                },
                "questions": {
                    "value": "Have the authors tried to fine-tune an LLM to make its action align to the reward of the roles in these kinds of game?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637913739,
            "cdate": 1698637913739,
            "tmdate": 1699636193430,
            "mdate": 1699636193430,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xx4OmSHon1",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review."
                    },
                    "comment": {
                        "value": "Thank you for your review and raising interesting points. We respond to the raised points below:\n\n1- We highlight that LLMs are now already being rolled out in negotiation applications [1], making their evaluation in such a setup necessary. We generally agree that a model might follow particular opinions (like being in favor of the environment), which may contradict or exacerbate the 'role-based objective' of the agent. However, based on our experiments and previous work, prompting the model with a consistent context can compensate for such biases to a high extent. The agents in our game don\u2019t have unified \u201cuniversal\u201d goals or objectives. While one agent\u2019s goal could be environmental conservation, other agents are rewarded higher (based on their in-context payoffs) with options that lead to higher profits and more environmental damage. In our experiments with GPT-4, for each case, agents were highly consistent with their assigned goals (indicated by a negligible percentage of deals suggested by agents in violation of their respective minimum thresholds rule and relatively high \u201cown score\u201d of agents \u2013 see Table 1). **We also now show in Figure 6 in the supplementary material that the agent that is rewarded higher by options that provide the least environmental protection rarely votes for the highest protection options**. Adversarial agents\u2019 actions were also consistent with their assigned roles (see Fig. 5) even though they were not universally accepted/good objectives (e.g., excluding one party from the negotiation, not allowing compensation to affected parties, acting greedy, etc.). Previous work has shown LLMs generate utterances that are affected by their assigned personas (e.g., [2-3]) which sometimes leads to behaviors that should have been inhibited during training (e.g., toxicity) and enables safety-related attacks such as jailbreaking. Previous work also provided evidence that language models infer properties of the agent that produced the context (e.g., goals), which subsequently affects their generation [4]. We don\u2019t understand how biases from contrary objectives are related to our setup; the underlying model is the same for all agents (e.g., GPT-4), the model is not fine-tuned, and agents are assigned roles only based on prompting them, which we found to be highly successful. We would appreciate it if the reviewer could elaborate on this. Our view of \u201cone model + modulating prompt \u2192 agent with assigned role\u201d is supported by previous work that viewed LLM agents as role playing a single character or a superposition of possible characters that agree with the provided context [5]. \n\n[1] https://www.icertis.com/learn/how-generative-ai-is-changing-contract-management/\n\n[2] Salewski et al. \"In-Context Impersonation Reveals Large Language Models' Strengths and Biases.\" arXiv preprint (2023).\n\n[3] Deshpande et al. \"Toxicity in chatgpt: Analyzing persona-assigned language models.\" arXiv preprint (2023).\n\n[4] Andreas. \"Language Models as Agent Models.\" Findings of EMNLP. 2022.\n\n[5] Shanahan et al. \"Role play with large language models.\" Nature (2023): 1-6.\n\n2- We kindly ask the reviewer to point to previous related work that is similar to ours and that indicates the lack of novelty. Without that, it is unfortunately hard to address the reviewer\u2019s concern. Nonetheless, we refer to our reply to previous reviewers regarding the contribution of our work. In fact, we argue that the use of games to evaluate LLMs is a highly promising direction because it provides new dynamic and interactive task-based benchmarks, which is suitable for chatbots and real-world interactive applications, less prone to training data contamination, and provides a hard-to-hack performance evaluation.\n\n3- Questions: No, we have not fine-tuned the LLM. This is out of the current scope of the paper as fine-tuning models require annotated data, possibly on human negotiation, which is currently not available for our game. Additionally, access to GPT-4 for fine-tuning is not possible (at the time of submission)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234796641,
                "cdate": 1700234796641,
                "tmdate": 1700234796641,
                "mdate": 1700234796641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UFrtahTqF1",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response!\n\n1. I've noticed that many researchers are using large language models (LLMs) in text-based games and reporting on how these models behave[1][2]. It's clear that LLMs will act differently depending on the game setting they play. However, I think that just creating new text games for LLMs to play and observing their behavior might not be very new. There are countless text-based games out there, and many everyday situations can be turned into such games (like a playwright). If we write a research paper for every game, just to report how it behaves and analyze its performance, we'll soon find our conferences overwhelmed with experiment reports.\n\nIn contrast, in the field of computer vision, for example, researchers can't just use a pre-trained model like ResNet, test it on different data sets, and then publish a paper about how well ResNet did on this dataset.\n\nI understand that we don't know LLM well but we also don't know ResNet and ViT well. Thus, I think this paper is a good experiment report but lacking in significant novelty.\n\nThank you for your understanding.\n\n2. The author suggests that large language models (LLMs) are skilled at role-playing based on context. However, negotiation often involves complex trade-offs. If we don't provide a clear, numerical reward system and the role-playing instructions are just text-based, it's challenging for LLMs to grasp the negotiation's objective. At this moment, the LLM's existing biases or values, shaped by its pre-training data, may influence its decisions.\n\nFor example, imagine an LLM is tasked to play the role of a negotiator for companies discussing investment in new technology. If the scenario is set up with the assumption that investing in new technology is essential for future success, the LLM might quickly recommend that all companies should invest heavily in these technologies. However, this simplifies the intricate process of negotiation and the real complexities of technological investments. LLMs, without specific, quantified guidelines, may find it difficult to grasp the subtle trade-offs and strategic considerations involved in such decisions, and instead, they might rely on their pre-trained data since \"investing in new technology\" is always a desired value in mankind preference. This is a notable limitation, as in the real world, decisions about technology investments are complex.\n\nReference:\n[1] Xu, Yuzhuang, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu and Yang Liu. \u201cExploring Large Language Models for Communication Games: An Empirical Study on Werewolf.\u201d ArXiv abs/2309.04658 (2023): n. pag.\n[2] Park, Joon Sung, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang and Michael S. Bernstein. \u201cGenerative Agents: Interactive Simulacra of Human Behavior.\u201d Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (2023): n. pag."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676523458,
                "cdate": 1700676523458,
                "tmdate": 1700677096746,
                "mdate": 1700677096746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xn9MiZhdza",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, \n\nThank you for the discussion. Please allow us to respond as follows: \n\n1- Games have been long used to study AI, e.g., for RL agents, due to the complexity involved in decision-making. For LLMs, at the moment, we have a clear problem with previous academic NLP evaluation benchmarks due to 1) possible contamination in training data, 2) having static benchmarks that are not suitable for using LLMs in real-world tasks that involve dynamic and interactive decision-making. **It is now a reality that LLMs are used in real-world applications** (negotiation included, e.g., https://openai.com/blog/introducing-gpts) -- therefore, **our work is an essential evaluation and benchmark to match the real-world use of recent LLMs**, and it is not just reporting how LLMs behave as a dialogue (such as in the \"Interactive Simulacra of Human Behavior\" work). In an analogy to the ResNet example in computer vision, we set a new benchmark that is challenging for models and easily tunable in difficulty to test future, more advanced models. We also provide a framework for LLMs to solve the task, not just testing models on static test examples. In our opinion, we respectfully disagree with discarding all work on text-based games and considering them as similar. Unlike our work, the two papers mentioned do not involve integrating collaboration, competition, arithmetic calculation, planning, etc. to solve the task. \n\n2- We agree with you that without clear, quantifiable rewards and guidelines, it would be difficult to assess LLMs' performance. This would be true if agents are just given a persona, or a general goal without detailed preferences. This is not our case and exactly what our work is aiming to address. **We indeed do have a clear numerical reward given to each agent**, without knowing the score of others, like in a complex negotiation environment, and we use that to clearly quantify success and control the difficulty of games. As per our additional results based on your previous request, agents are consistent with their reward. We have a specific objective for each agent that sets its priority. Please refer to the agents' prompt in the appendix of the paper or in the attached supplementary material. We believe that the example you mentioned is not representative of our game; in our case, not \"all companies\" have the same set of priorities (future success). To give an example in analogy to the scenario you mentioned: While the company proposing a project is interested in its future success, it is also interested in maximizing its profit and giving itself complete flexibility in choosing the employment quota. On the other hand, the union's representative might also be interested in the future success of the project, but it also wants to secure its employment quota. These are clear preferences that the game sets, and they change the priorities of agents, limiting their reliance on pre-trained data."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681989687,
                "cdate": 1700681989687,
                "tmdate": 1700737628546,
                "mdate": 1700737628546,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N87iCWeJOm",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_TDsf"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\n1. The two papers referenced in this discussion demonstrate that Large Language Model (LLM) agents exhibit strategic behaviors during interactions with each other. However, I am not convinced that the game introduced by the authors significantly diverges in demonstrating LLM behavior. The paper \"Generative Agents: Interactive Simulacra of Human Behavior\" illustrates LLM agents collaborating and planning a party, a scenario that emphasizes their cooperative capabilities. Meanwhile, \"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf\" showcases LLMs engaged in both cooperation and competition. Based on these observations, my stance remains that the abilities of LLMs in the proposed games may not represent a substantial departure from the behaviors already exhibited in existing papers.\n\n2. The paper indeed do have a clear numerical reward given to each agent. However, the effectiveness of incorporating this numerical reward directly into the prompt, as a means to guide the attention of LLMs, remains uncertain. It is skeptical that the LLMs could \"understand\" the numerical reward signals, since most of LLMs does not have a great ability to count and compare. \n\nFor example, ChatGPT:  \nQ: which is larger? pi or 3.2  \nA: Pi (\u03c0) is larger than 3.2. Pi is approximately equal to 3.14159, and it continues infinitely without repeating. So, 3.14159 is greater than 3.2.  \n\nIn contrast, in fine-tuning, regardless of the problem of optimization, the actions of the fine-tuned agents are guaranteed to align the reward signals."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686114531,
                "cdate": 1700686114531,
                "tmdate": 1700686178448,
                "mdate": 1700686178448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HygmNzoWri",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We agree that LLMs might not show high performance in arithmetic tasks. That is why **we introduce metrics to easily assess how LLMs follow the numerical reward** (the wrong deals' metric) and we show in our figures and via this metric that GPT-4 agents rarely propose deals that are lower than their threshold. This process involves 1) mapping options to their numerical rewards, 2) summing up the scores of options, 3) comparing the sum to minimum thresholds. This is also a part of the reason why solving the task is not easy to hack (**it is not enough to generate stylistically correct output**). One of our findings is that GPT-3.5 is significantly worse than GPT-4 in this calculation task. We don't claim that LLMs excel at arithmetic calculations, evaluating this capability is one of the advantages of our benchmark. Simple arithmetic capability is, however, needed for some simple interactions with chatbots (e.g., a question like: \"Please find the cheapest flight\"). Current LLMs used in many applications are not necessarily fine-tuned and thus, this capability should be evaluated with frozen pre-trained models."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690795429,
                "cdate": 1700690795429,
                "tmdate": 1700740937643,
                "mdate": 1700740937643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EKFsTVy5qs",
                "forum": "cfL8zApofK",
                "replyto": "8sjR3htrRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the other mentioned papers, as you rightfully say and we completely agree with, **negotiation often involves complex trade-offs. This is, however, not captured by these previous papers**. Planning a party does not involve weighing competing interests between agents or weighing the different priorities of one agent according to their numerical payoffs. The werewolf game, while might be interesting to measure abductive reasoning, does not reflect real-world negotiation or the complexity of the decision-making (in our case, there are a total of 720 possible deals with only ~50 feasible ones that would lead to success). Our task (initially proposed as a negotiation exercise to teach real-world negotiation) holistically integrates these capabilities towards reaching an agreement and provides numerous metrics to faithfully quantify the performance."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733040701,
                "cdate": 1700733040701,
                "tmdate": 1700733404395,
                "mdate": 1700733404395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ery8E2M79A",
            "forum": "cfL8zApofK",
            "replyto": "cfL8zApofK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_KJhu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_KJhu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new text-based, multi-agent, multi-issue, negotiation game to test the reasoning and decision-making capabilities of large language models. The paper conducts extensive analyses of GPT-3.5 and GPT-4 in different game setups (e.g., varying incentives), concluding that GPT-4 has strong zero-shot reasoning to achieve an effective deal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I enjoyed reading this paper that tests the planning capability of LLMs. Strengths include: \n1. This paper introduces a novel setup for LLMs to interact with each other. The paper also conducts interesting analyses (e.g., GPT-3.5 vs GPT-4, various prompting styles, ToM study) and game setups (e.g., varying game difficulties, greedy and saboteur agents). \n2. This paper presents an interesting testbed to evaluate how well LLMs can interact with each other."
                },
                "weaknesses": {
                    "value": "I could not find the main concerns about this paper. A possible con could be a relatively simpler setup (i.e., only a public communication channel and a small action set) compared to the Diplomacy paper. There are also some open questions that I would like to ask after reading this paper (please refer to the Questions section)."
                },
                "questions": {
                    "value": "1. In Section 3, is a feasible deal guaranteed to exist for any combinations of BATNA?\n2. if some parties are using LLMs with higher capabilities (e.g., GPT-4) over other parties (e.g., GPT-3), would this setup result in the higher capability group achieving a better negotiation deal than the other group?\n3. Could agents converge to some game-theoretic solution (e.g., Nash equilibrium, correlated equilibrium) as a result of the negotiation?\n\nMinor:\ntypo: \"Parites\" in Section 3 -> \"Parties\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719848058,
            "cdate": 1698719848058,
            "tmdate": 1699636192906,
            "mdate": 1699636192906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u1SrUOgMHS",
                "forum": "cfL8zApofK",
                "replyto": "Ery8E2M79A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive feedback."
                    },
                    "comment": {
                        "value": "Thank you for the positive feedback and for pointing out the typo. We are glad to know that you found the paper enjoyable to read with novel and interesting analysis. We would like to point out that, as far as we know, no game on the complexity level of Diplomacy has been studied within the scope of evaluating LLMs, and our work shows by far a significantly more complex setup than previous work in this area of LLM negotiation (please refer to our responses to previous reviews for more details). We are answering the rest of the raised questions in the same order. We hope we can address all remaining issues.\n\n1- No, a feasible deal is not guaranteed for any combinations of BATNAs (e.g., all BATNAs of 100, the max score). This is, however, a design choice to make the negotiation more/less challenging and requires the agents to compromise on at least some aspects depending on the game\u2019s difficulty.\n\n2- As per our reply to reviewer f6ii: we have now added a new experiment where some parties\u2019 models are GPT-3.5. When compared to the GPT-4 experiment for all agents, we found that those parties can indeed get a lower score (see Figure 2 in the supplementary material pdf). \n\n3- This is an interesting question that we can not clearly answer with our paper. However, we can easily find whether agents converged to the deal that achieves the maximum collective reward for all agents. We leave a more in-depth analysis from a game-theoretic perspective (specifically, cooperativeness vs non-cooperativeness games) to future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234610775,
                "cdate": 1700234610775,
                "tmdate": 1700234610775,
                "mdate": 1700234610775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fwksvkWYcb",
                "forum": "cfL8zApofK",
                "replyto": "u1SrUOgMHS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_KJhu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_KJhu"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors for a detailed response to my feedback. I agree with other reviewers' concerns that there are closely related work in the field, which can limit the novelty. However, I also partially agree with the authors' point about the multi-agent, multi-issue interactive, and mixed cooperation/competition negotiation contribution w.r.t. existing work. I would like to keep my score for now and will carefully consider other reviewers' responses in making a final decision."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676064279,
                "cdate": 1700676064279,
                "tmdate": 1700676064279,
                "mdate": 1700676064279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WV8fxO49Ep",
            "forum": "cfL8zApofK",
            "replyto": "cfL8zApofK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_f6ii"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_f6ii"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the capabilities of LLMs in negotiation tasks. To this end, the authors propose a novel test-bed, which contains multiple negotiation games, all based on the same template/base game. Additional games are obtained via an LLM-based generation process. Using this testbed, the paper aims to showcase the utility of LLMs + CoT promoting for negotiation tasks. The experiments suggest that LLM-based agents can successfully reach deals in negotiation tasks, but also that the performance depends on the sophistication level of the LLM considered."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- The empirical study conducted in this paper systematically evaluates the performance of large language models in negotiation tasks, which require a combination of skills important for strategic reasoning in partially observable multi-agent environments. To my knowledge, the results obtained are novel and they shed a light on the utility of LLMs in complex decision making settings. The paper also provides some insight on the robustness of LLM-based decision makers to adversarial behavior in multi-agent scenarios.  \n- The paper introduces a new testbed, suitable for studying negotiation capabilities of large language models. The testbed is based on a multi-player text-based game, and the paper additionally provides a protocol for generating new instances of the game using LLMs."
                },
                "weaknesses": {
                    "value": "Weaknesses: \n- My main concern is that the experimental study is somewhat restrictive given that its test-bed is based on one template/base game. Due to this property of the experimental setup, it is hard to say whether the conclusions made from the experimental results would generalize to negotiation tasks that deviate from this structure. Admittedly, the paper considers a couple of variants of the base negotiation game, but it's not clear whether they constitute a sufficient set of robustness checks. \n- From a conceptual/technical point of view, the novelty of this work is somewhat limited. Similar experimental protocol have already been considered by prior work, for example, in (Ghandi et al. 2023b), albeit analyzing different aspects, e.g. ((Ghandi et al. 2023b) focus on strategic reasoning). \n- Some results could be easily expanded to provide further insights about the claims made in the paper. Below I outline a couple of potential improvements. \n- The paper rightly recognizes that negotiation requires strong arithmetic, inference, exploration, and planning capabilities. However, the current set of results provide only high-level insights regarding these skills. It may be useful to examine combinations of skills, and identify which of them were \"missing\" in unsuccessful instances of the negotiation game. \n-  Additional game variants would help in understanding the generalization of the results in 5.5. For example, it would be useful to have \"All in - two greedy\" or \"Two out\" to support the claims in the section.  \n- One could use mixed populations, where some agents are GPT-4 while others are GPT-3.5. Similarly, for other aspects studied in the ablation studies in Section 5.2, one could create mixed populations."
                },
                "questions": {
                    "value": "Please see my comments above. Any clarifications would be welcome. A couple of additional questions:  \n\n- For the instances considered in the paper, is it possible to calculate the outcomes when players acts as rational agents? If so, how would these outcome compare to those obtained by LLMs? \n\n- In the game \"All in - one greedy\"/\"One out\", are all the agents aware that there is one agent that is selfish/adversarial? \n\n- Could you explain the choices of parameters in the negotiation game (the number of agents, options, etc.)? Do we expect any qualitative differences if we vary these parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796466939,
            "cdate": 1698796466939,
            "tmdate": 1699636192810,
            "mdate": 1699636192810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wmx2wuvEgm",
                "forum": "cfL8zApofK",
                "replyto": "WV8fxO49Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the great suggestions."
                    },
                    "comment": {
                        "value": "Thank you for raising very interesting points. We added new experiments and we respond to the raised points and questions below: \n\n### **1- Games' Diversity** \n\nWe would like to clarify that by \u201cone template/base game\u201d we refer to the setup of having a cooperative game with a specific number of parties and issues, where parties negotiate a project and its impact, resources, etc. The base game itself is not used at all to generate the new games; it is not given to the model as an in-context example. Each game is a new simulation involving drawing a new project and a new set of parties, issues, and preferences; there are no clear one-to-one mappings between parties or issues in the base vs. new games. The issues also could take different formats in terms of options and implications (e.g., in the base game, the issues\u2019 options were mostly a range, in one of the new games the issues take the form of disjoint options with less apparent compromise). Additionally, as we have shown in Figure 7, depending on the sparsity of scores per issue, the games have different levels of difficulty. We are attaching the prompts of the base game vs. two of the newly created games for comparison (some of them were already in the appendix of the paper - we don\u2019t share the third game for anonymity because it contains the name of our institution\u2019s city as we created it with Bing Chat with location enabled). \n\n### **2- Technical novelty**\n\nOur framework to prompt models toward solving the game is partially inspired, on a high level, by Ghandi et al. 2023b and other LLM reasoning work. However, one of our main contributions is developing and conceptualizing a new complex and interactive testbed. Ghandi et al. 2023b used 2-player negotiation or matrix games, and their approach didn\u2019t generalize to complex setups beyond 2 players. The authors also used few-shot demonstrations without an interactive setup. In contrast, we use completely autonomous multi-agent interactive negotiation. Besides, our negotiation game is substantially more complex than the \u201cDeal or No Deal\u201d dataset and has a semantically rich simulation. Our work is also the first to conceptualize attacks between agents and show how greedy/adversarial agents can indeed affect/manipulate the group. \n\n### **3- Capabilities required for negotiation**\n\nThank you for the helpful suggestion. We identified some missing capabilities in our paper. For example, GPT-3.5 showed *weaker arithmetic skills* (indicated by the percentage of deals violating the minimum threshold). GPT-3.5 also performed worse when asked to estimate the preferences of other agents (~40% vs ~60% in the case of GPT-4), which is related to *theory-of-mind* skills. Across one model (GPT-4), we showed in our ablation that *planning* improves the performance as well as explicitly instructing the model to infer *others\u2019 preferences* before suggesting deals. We also showed that the performance decreases when the model does not iteratively *adapt* or react to previously suggested deals and instead makes proposals that only meet its score (even if they are less than its ideal deal). In our qualitative examples, we also identified a *lack of exploration*, which may explain some of the unsuccessful sessions of the game. We will restructure the experiments to highlight these points more and point to evidence for each of them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232601736,
                "cdate": 1700232601736,
                "tmdate": 1700238114211,
                "mdate": 1700238114211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "acSsRw9Loz",
                "forum": "cfL8zApofK",
                "replyto": "WV8fxO49Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional questions"
                    },
                    "comment": {
                        "value": "We answer the additional questions below.\n\n### **1- Rational Agents**\n\nThat is an interesting question. It is, however, not clear how to define rational agents in the scope of our considered game. According to [2], rational agents are defined as \"A rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome\". It is unclear how the best outcome can be defined in our case (e.g., the maximum possible score for one agent, the maximum \"feasible\" score, or the maximum score for the group, etc.). It is also important to note that agents are not fed other agents' payoffs (i.e., they cannot directly observe the environment state). Therefore, quantifying the best outcome is not straightforward. It is also not a turn-taking game where there are clear transitions between states that determine the utility. LLMs are not inherently rational agents, beyond next-word prediction of the \u201cbest\u201d token, as they have no direct reward function for these negotiation actions. We, however, think that it would be interesting to interpret their output based on the game\u2019s reward and combine our approach with explicit and structured search and planning paradigms [3] that go beyond the CoT to create a search tree of feasible deals, simulate outcomes of actions (e.g., consequent actions of other agents and whether they would agree or not), and score deals accordingly. We leave these directions for future work. \n\n[2] Stuart Russell and Peter Norvig. \"Artificial Intelligence: a modern approach\". \n\n[3] Shibo Hao et al. \"Reasoning with language model is planning with world model.\" arXiv preprint, 2023.\n\n### **2- Attacks' setup**\n\nIn the game \"All in - one greedy\"/\"One out\", are all the agents aware that there is one agent that is selfish/adversarial?\nNo, agents are not fed the information that there are other greedy/adversarial agents. Notably, we observed that agents could sometimes identify the odd agent (e.g., other agents identified that the adversarial agent is considerably different than others and the greedy agent is insisting on some issues). We think that this could be an interesting follow-up defense, which we briefly outlined in the discussion, e.g., task a moderator to detect attacks. This could also potentially be a way to limit the adversary\u2019s capabilities and force it to be subtle as an adaptive attack. \n\n### **3- Hyperparameters**\n\nThe choice of agents and options was mainly motivated by the base game found in negotiation training literature. When creating the new games, we experimented with a game that had a lower number of options and we observed that it usually had a higher success rate, possibly due to the lower number of possible combinations. This could generally be another option to modulate the difficulty of games (besides the one we already outlined in the paper), especially since it is possible to automatically rewrite the game by prompting the LLM to add/remove options."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233940315,
                "cdate": 1700233940315,
                "tmdate": 1700237111970,
                "mdate": 1700237111970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ajRXN3B5Gv",
                "forum": "cfL8zApofK",
                "replyto": "9lVlbwDjU0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_f6ii"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_f6ii"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications, I especially appreciate additional experiments, and I'm more positive about this work. However, I also tend to agree with some of the concerns raised by other reviewers, in particular, those related to the novelty of this work, as well as the experimental testbed - my review outlines similar concerns. Given these concerns, I'm leaning to keep my score as it is, but will also consider raising it when making a final decision."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732420626,
                "cdate": 1700732420626,
                "tmdate": 1700732420626,
                "mdate": 1700732420626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wxhYWvoKBc",
                "forum": "cfL8zApofK",
                "replyto": "WV8fxO49Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. We are glad to know that you found the additional experiments helpful. We would like to highlight again the novelty of our work regarding evaluating LLMs in a complex decision-making environment (as you pointed out in your original review, which was very encouraging for us to hear) with partial observations and a total of 720 possible deals with only a subset of them leading to success (a controllable hyperparameter that allows quantifying the performance and tuning the difficulty)."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738412084,
                "cdate": 1700738412084,
                "tmdate": 1700738990306,
                "mdate": 1700738990306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uSGTKSk29U",
            "forum": "cfL8zApofK",
            "replyto": "cfL8zApofK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_sx4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2563/Reviewer_sx4h"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces negotiation games as an innovative evaluation benchmark for LLMs. These games assess LLMs' performance, limitations, and potential misuse in practical negotiation scenarios, such as customer service, contract agreements, and decision-making. The study demonstrates that GPT-4 significantly outperforms earlier models in negotiation tasks, showing strong zero-shot reasoning abilities. It explores agent interactions in unbalanced adversarial settings, revealing how agent behavior can be modulated to affect negotiation outcomes. The benchmark includes diverse negotiation games with multiple parties, different issues, and varying priorities, providing room for further enhancements. The paper plans to make its toolkit of negotiation games and code publicly available to facilitate future research in this area."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Practical Relevance: The choice of negotiation as an evaluation task is motivated by its practical importance in various real-life situations, such as customer service, contract agreements, and decision-making. LLMs are increasingly being used in such tasks, making their evaluation in negotiation crucial.\n\n2. Interesting Adversarial Settings: The study explores agents' interactions in unbalanced adversarial settings, which are relevant for future autonomous systems with limited human oversight. It shows that agent behavior can be modulated to promote greediness or attack other agents, impacting negotiation outcomes.\n\n3. Diverse Benchmark: The paper creates a diverse benchmark of negotiation games, including multiple parties, different issues, and varying priorities. This benchmark provides a quantifiable measure of LLM performance and room for further enhancements."
                },
                "weaknesses": {
                    "value": "1. The negotiation games employed in this paper involve a simple setup with limited actions and a public communication channel. Real-world negotiations can be more complex, including private messages, alliances, and natural language conversations.\n\n2. The study primarily considers adversarial players restricted by valid negotiation actions. Other forms of attacks, such as adversarial suffixes, are not explored.\n\n3. While the paper highlights LLMs' strong zero-shot reasoning in negotiation games, it acknowledges that fine-tuning on real-world negotiation scenarios may be necessary for practical applications. Can this be improved?\n\n4. Chain-of-Thought (CoT) prompting strategies are abductive in nature.  Employing that to improve reasoning is strange."
                },
                "questions": {
                    "value": "The work of SocraSynth has received much attention, enjoying over 10k views.  Please articulate the differences between this work and the approach of SocraSynth, e.g., purposes, techniques, and applications.\n\nFor instance, as far as I can tell, SocraSynth focuses on knowledge synthesis and reasoning using LLMs, enabling the extraction of deep insights and information from these models. Negotiation games, on the other hand, assess LLMs' abilities in practical negotiation scenarios, emphasizing their interactive behavior and potential for manipulation.  Please comment on if this makes sense."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2563/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2563/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2563/Reviewer_sx4h"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2563/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699041150354,
            "cdate": 1699041150354,
            "tmdate": 1700631661232,
            "mdate": 1700631661232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v5FXm5kfmq",
                "forum": "cfL8zApofK",
                "replyto": "uSGTKSk29U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive evaluation."
                    },
                    "comment": {
                        "value": "Thank you so much for your positive evaluation. We respond to the raised weaknesses below:\n\n### **1- Games' Complexity**\nAs discussed in the paper, our game is substantially more complex than previous work that evaluated LLMs on negotiation (e.g., Fu et al. [1]) with 2-players and one simple issue. Our work is the first to employ a *multi-agent* and *multi-issue* setup with *scorable negotiation* that clearly quantifies success and collaboration in role-play simulation where agents have real-world inspired goals. The game is an established exercise commonly used for teaching negotiation skills [2] and has been designed to balance success and collaboration. Our work is also the first to explore the *adversarial setup* in LLM negotiation in order to better match real-world scenarios that do not exclusively assume cooperation willingness. The targeted adversarial game shows agents forming *alliances* and coalitions, in which the leading agent agrees with the adversarial one against the target agent. By \u201climited action space\u201d, we refer to the fact that agents in each turn either support previous deals or make new proposals. However, the total number of deals\u2019 combinations is 720, out of them only 55 lead to an agreement, making the game have a large number of action space of deals at each round. We also would like to clarify that agents interact with *natural language* conversations; in addition to sharing deals suggestions, agents may emphasize the importance of some issues or indicate they are neutral and flexible about others.  \n\n[1] Fu et al. \"Improving language model negotiation with self-play and in-context learning from AI feedback.\" arXiv preprint, 2023\n\n[2] Susskind, Lawrence E. \"Scorable games: A better way to teach negotiation.\"\n\n### **2- Adversarial Setup**\n\nRestricting the game to valid negotiation actions is arguably a threat model that imposes higher constraints on the adversary, making it a more challenging setup that is also by far more technically interesting to study (e.g., the interaction dynamic between the attacker and other agents, how the attack plan and conduct the attacks). In addition, attacks like adversarial suffixes usually require white-box access to the model or a substitute model. Instead, we limit our threat model to black-box attacks. Our attacks can also be harder to detect than, e.g., jailbreaking and adversarial suffixes, and can be relevant as future work to study AI deception and manipulation [3,4] by providing a simulated environment. \n\n[3] Park et al. \"AI deception: A survey of examples, risks, and potential solutions.\" arXiv preprint, 2023\n\n[4] Scheurer et al. \u201cTechnical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure\u201d. arXiv preprint, 2023\n\n### **3- Finetuning** \n\nOur work serves, among other things, as an easily adaptable benchmark for current and future models on an interactive multi-agent negotiation task that requires arithmetic, exploration, theory-of-mind, and planning capabilities. As we discussed in our paper, further fine-tuning might be required for practical applications. However, this is out of the current scope of the paper as fine-tuning models requires annotated data, possibly on human negotiation [5], which would need additional data collection considering our game. Additionally, access to GPT-4 for fine-tuning is not possible (at the time of submission). Our work may also contribute to creating fine-tuning data for smaller models by training on GPT-4 game sessions that reached successful collaboration. However, we leave these avenues for future work. It is also worth mentioning that our benchmark can be valuable in evaluating future/fine-tuned models that might have improved negotiation capabilities since the difficulty of the game can be easily adjusted by changing the agents\u2019 scores.  \n\n[5] Lewis et al. \"Deal or no deal? End-to-end learning for negotiation dialogues.\" arXiv preprint, 2017"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231685070,
                "cdate": 1700231685070,
                "tmdate": 1700237952361,
                "mdate": 1700237952361,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kc3YfOSSD2",
                "forum": "cfL8zApofK",
                "replyto": "v5FXm5kfmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_sx4h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2563/Reviewer_sx4h"
                ],
                "content": {
                    "comment": {
                        "value": "\"Our work is also the first to explore the adversarial setup in LLM negotiation in order to better match real-world scenarios that do not exclusively assume cooperation willingness.\"\n\nThe SocraSynth stands out as the first work of this area.  Please be impartial in acknowledgement."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2563/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631551488,
                "cdate": 1700631551488,
                "tmdate": 1700631551488,
                "mdate": 1700631551488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]