[
    {
        "title": "IMP: Benchmarking Image Polysemy in Vision-Language Models"
    },
    {
        "review": {
            "id": "7UjeP7NiIq",
            "forum": "RIbH5ekQpr",
            "replyto": "RIbH5ekQpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_VqT9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_VqT9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a benchmark, IMP, to evaluate understanding of image polysemy in vision-language models. The IMP benchmark includes diverse captions that range from descriptive to conceptual. A semi-automatic pipeline was constructed to select the captions, with use of human annotators. A variety of existing vision-language models are benchmarked on the IMP dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The dataset is novel, and is the first to study image polysemy in the context of image-text matching / retrieval. The experiment evaluation covers a wide range of models and is sufficient."
                },
                "weaknesses": {
                    "value": "I am concerned about the difficulty of the task. \"Vanilla\" image-text retrieval itself is not straightforward to evaluate, given the high rate of false negative caused by plausible but unrecorded matches [1,2]. The subjectivity of image polysemy amplifies all these difficulties. I wonder to what degree this task is even _possible_ to evaluate objectively. For example, it was not obvious to me that the caption and images in Fig 4. Col1 & Col 4 go together. Similarly for many of pairs in Fig 3. \n\nThe subjectiveness of this benchmark calls for a human accuracy check, given that this benchmark is so heavily based on human judgements of image meaning. The human judgements of similarity can then be used to create similarity judgements similar to [1,2] or correct false negatives. Given the dataset construction technique, I'm skeptical whether the evaluations are meaningful given that the captions are abstract enough that many of the might match plausibly match to other images. This could be corrected by verifying human accuracy on the dataset and modifying the benchmark so that correlation with human accuracy is being assessed instead.\n\nNote that the annotation process _does not_ guarantee this, since the annotation procedure does not exclude the possibility that there are 10-15 other captions somewhere in the dataset that describe the image equally well, and vice versa for any image. You could do this by using CLIP to rank _all_ the captions in the dataset w.r.t to a particular image (and vice versa), and having humans rerank the top 50 from each cluster. This could be used to measure both human accuracy, and capture more plausible measurements of human similarity. \n\n\n[1] ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO\n[2]  Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for ms-coco"
                },
                "questions": {
                    "value": "Please see the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718191711,
            "cdate": 1698718191711,
            "tmdate": 1699636119111,
            "mdate": 1699636119111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ENtgE0m0RA",
                "forum": "RIbH5ekQpr",
                "replyto": "7UjeP7NiIq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VqT9 (R5)"
                    },
                    "comment": {
                        "value": "We thank the reviewer $\\textcolor{purple}{R5}$ for insightful comments and valuable suggestions on the subjectiveness of IMP.\n\n## Task difficulty and false negatives\n\nWe thank the reviewer $\\textcolor{purple}{R5}$ for starting the discussion on the difficulty of the task and false negatives as this goes directly to the core of the issue we are studying in this paper. Indeed, considering image polysemy amplifies the difficulties concerning evaluation, yet we argue that when dealing with real-world examples these difficulties are unavoidable. \n\nIncreasingly datasets are collected by webscraping image-text pairs, and inherently these will contain pairs with varying levels of 'subjectivity' - some image-text pairs may exist solely because a single person decided to pair them. As such, even what is considered a 'true positive' is in the eye of the beholder (as exemplified by the reviewer's comments on the pairings in Fig 3 & 4). However, as humans we can deal with such ambiguity - or perhaps better, with such polysemy - yet, vision-language models trained with a contrastive loss presumably cannot. \n\nIn our paper we test to what extent VLM trained with contrastive loss cannot deal with the challenge of polysemy, by proposing a dataset that makes explicit the presence of polysemy. Since IMP-5k is human annotated we have reliable ground truth for the true positives, making it possible to get better insight into the polysemy challenge. However, as pointed out by the reviewer we do not have annotations for true negatives - and whilst the procedure specified by the reviewer is viable we must also recognise that this would be a very costly exercise. \n\nWe strongly believe that even with the existing setup, IMP dataset provides a useful testbed for getting a better understanding of image polysemy in VLM. While this understanding may at this early stage of research into image polysemy in VLM be somewhat coarse, we do believe that any model that explicitly handles image polysemy would perform significantly better on IMP. \n\n>   the captions are abstract enough that many of the might match plausibly match to other images\n\nDuring annotation we aimed to excluded captions which are very non-specific. \n\nFor example:\n\u201cI took this photo with my tablet.\u201d -- Adds no contextual value to the image and can basically used as caption for any image in this dataset.\n\nThe annotators are also instructed to avoid \u201cover-thinking\u201d. Annotators will first see the caption and then the image. The judgement is based on their first and second impression (directly finding the link or after a few seconds of thinking) of whether the caption and image can be a good pair. We find that this instruction helps reduce far-fetched pairings, but leaves the possibility for creativity and highly polsemic pairings."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576596726,
                "cdate": 1700576596726,
                "tmdate": 1700576596726,
                "mdate": 1700576596726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OxLRFkgDW5",
                "forum": "RIbH5ekQpr",
                "replyto": "ENtgE0m0RA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_VqT9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_VqT9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It does not address my main question. \n\nThe annotation and evaluation procedures are asymmetric. The annotator sees a small set of $n_1$ viable captions only. The model being evaluated sees $n_2>> n_1$ captions, many of which could also match the image. \n\nThe evaluations in the paper would make sense if the evaluation was done one-by-one (use a model with an image-text matching head like BLIP-1/OFA/BLIP-2 or CLIP with a threshold for match / no match) for each image and human-annotated true positive caption. \n\nThe evaluations as they are currently done do not exclude the possibility that the model understands polysemy perfectly well and it is simply digging up other captions in the dataset that could plausibly match the image, especially since many of the captions are so abstract. It's possible that humans would even agree with the other captions the model selected. \n\nIt's difficult to draw any claims about the ability of a model to understand polysemy, given the problem above. \n\nI will keep my rating, but I emphasize that I think the dataset is interesting. But you need a better evaluation protocol, or a comparison to humans under the current protocol."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718201459,
                "cdate": 1700718201459,
                "tmdate": 1700718201459,
                "mdate": 1700718201459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2s6D7OA5jw",
            "forum": "RIbH5ekQpr",
            "replyto": "RIbH5ekQpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
            ],
            "content": {
                "summary": {
                    "value": "The following work presents a dataset for benchmarking polysemy in text-image pairings. Examples of difficult pairings include meme-like pairings that require drawing higher level and more abstract connections between the text and image content. Dataset curation uses a combination of google vision API and CLIP image embeddings to collect website titles as potential captions, combined with candidate captions from existing datasets. Annotator's select captions either based on descriptiveness or level of conceptual match, the latter being a more abstract linking. Diversity is encouraged by clustering captions candidates by their sentenceBERT embeddings, then selecting 1 caption candidate per cluster. Existing pretrained models are then benchmarked on this dataset through both zero-shot testing and fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Tackles an important barrier in higher level image-text comprehension\n- Contributes another fairly large scale dataset that can be of use to community"
                },
                "weaknesses": {
                    "value": "- The claim for having higher diversity in captions in this work is slightly problematic because diversity is never clearly defined. Based on their method, it seems to refer specifically to whatever euclidean distance can be associated with in sentenceBERT's embedding representation, which I am not entirely sure how to interpret.\n- Some concerns regarding methodology which I elaborate on in the next section.\n- While this work certainly provides a useful benchmark for polysemy, I don't think the contributions of this work sufficiently shed light on any new aspects of the problem that the community was already aware of."
                },
                "questions": {
                    "value": "- The MPL2D metric measures the mean euclidean distance between image and caption embeddings, meant to capture the semantic diversity to justify the aforementioned diversity claims. There may be some concerns regarding this formulation which I would like to give the authors a chance to verify any possible misconceptions on my end:\n    - If these are the embeddings used in the CLIP contrastive cosine distance loss, then it should be specified whether they are normalized inner products. The CLIP contrastive loss encodes distances only in the angular difference of the embeddings, so the embedding norms should not contribute to the analysis unless there's a good reason to.\n    - If the used embeddings correspond to CLIP's normalized inner product space, then we also have to be careful about applying any sort of euclidean analysis, because the clustering from the learned representation probably lies on the surface of the N-dimensional normalized sphere.\n- Is it possible to verify that there is no dataset leakage in the zero-shot analysis? Based on my understanding of the dataset collection pipeline, there should be existing captions from previous datasets such as MSCOCO and FLICKR30K included in this dataset, with possibly different image pairings. Can we confirm that none of the pre-trained models have been exposed to MSCOCO or FLICKR30k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I'm not sure if this is a serious concern, but the free-use license for unsplash.com images does not cover the use of images with brands, works of art, or recognizable faces. The images in the paper certainly include examples of recognizable faces, which may be in breach of the license.\n\nUpdate: Author responded and this is no longer a concern."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720729034,
            "cdate": 1698720729034,
            "tmdate": 1700697992414,
            "mdate": 1700697992414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3uZZBbRKhC",
                "forum": "RIbH5ekQpr",
                "replyto": "2s6D7OA5jw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KVAJ (R4)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{blue}{R4}$ for valuable comments on dataset collection process and the core challenge in this paper. We respond to the reviewer's concerns and questions in detail below.\n\n## MPL2D and Maximize Diversity among Captions\n\nWe thank reviewer $\\textcolor{blue}{R4}$ for critical question on MPL2D and diversity maximization using sentenceBERT. We first apologize for the confusion we have made in the paper, and any potential misunderstanding of reviewer's question.\n\n> If the used embeddings correspond to CLIP's normalized inner product space\n\nWe indeed used the normalized version for computing further statistics. We appreciate the concern when applying euclidean analysis with normalized inner product space. The aim of this metric is to measure how well each caption is paired with its image. We directly use the image enbedding as the pseudo center of its caption embeddings. This is because we argue that an image can have (potentially) infinitely many captions, which the collected captions are just a small subset of them.\n\n> whatever euclidean distance can be associated with in sentenceBERT's embedding representation\n\nTo clarify the clustering process during caption selection we add further details about what is described in appendix A. We first extract the text embedding of all captions (including noisy captions) using sentenceBERT. We then proceed all the text embeddings to UMAP to learn a low-dimentional projection of the text embedding while maximally maintaining the structure of those embeddings in the original high-dimentional space. Noisy captions are also included to have more samples for better UMAP learning. \n\nAfter we obtain the UMAP projection, for each caption we select those captions which passed human-evaluation, and performing clustering in the low-dimentional 2D space. This is the commonly used visualization pipeline being used with different encoders and dimentionality reduction techniques. The results are also checked by human-annotators with randomly selected images. For image with more than 100 captions, we can see that each cluster represent different meanings. While for those with fewer captions (still larger than 10), the captions are still selected with maximal potential diversity.\n\n## Contributions\n\nWe thank reviewer $\\textcolor{blue}{R4}$ for comments on our contributions.\n\n> I don't think the contributions of this work sufficiently shed light on any new aspects of the problem that the community was already aware of.\n\nTo the best of out knowledge there are no works that address image polysemy in-depth. Our aim is to not only make the community aware of the issue, but to also make it possible to be able to quantify to what extent it is a problem. Our results show that polysemy has the potential to heavily degrade retrieval performance, which affects VLM applied to real-world data at large.\n\n## Data Leakage\n\nWe thank reviewer $\\textcolor{blue}{R4}$ for valuable concern on the data leakage problem.\nWe would like to clarify first that we did not include captions from MSCOCO or Flickr30k in IMP.\nWe can only verify that image-caption pair in IMP does not exist in CC3M and CC12M.\nWe observe that there exists handful cases of very similar captions (but not identical) existing in IMP and MSCOCO, but the image is not identical.\nWe are not able to test data leakage of whether pre-trained models have been exposed to MSCOCO or Flickr30k, either due to the private dataset, or the massive public dataset which might need a lot of time to do the leakage test."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576550291,
                "cdate": 1700576550291,
                "tmdate": 1700576550291,
                "mdate": 1700576550291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JWUpOkvkcC",
                "forum": "RIbH5ekQpr",
                "replyto": "3uZZBbRKhC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for clarifying several details!\n\nI think the MPL2D metric, when computed on normalized vectors, is probably ok when the angular difference is small, because the squared euclidean distance is at least proportional to the cosine angle (I'm not sure if squared or unsquared euclidean is being used here). A more principled approach would have been to perhaps compute the intrinsic means and variances using the geodesics along the surface of the hypersphere [1,2]. Regardless, I suspect the current results would still hold, but I imagine the numbers would come out quite differently, and that the current approach probably wouldn't be accurate enough to assess fine-grained differences in such statistics.\n\nRegarding the use of sBERT, I agree that the overall formulation is reasonable and the additional quality control step suggests that the results are probably good enough.\n\nI would also like to thank the authors for checking the issue regarding data leakage. There's certainly a limited extent to which one can perform this test these days on large pretrained models, but it's still important to take note of when leakage might be an issue.\n\nFinally, as the authors indicated they intend to publicly release the data, I noticed that the Unsplash license (https://unsplash.com/terms) does not apply to:\n- Trademarks, logos, or brands that appear in Photos\n\n- People\u2019s images if they are recognizable in the Photos\n\n- Works of art or authorship that appear in Photos\n\nHave the authors accounted for this? \n\n\n\n[1] Oldfield et al. Parts of Speech-Grounded Subspaces in Vision-Language Models. Neurips2023\n[2] Fletcher et al. Principal geodesic analysis for the study of nonlinear statistics of shape. IEEE Transactions on Medical Imaging ( Volume: 23, Issue: 8, August 2004)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670193632,
                "cdate": 1700670193632,
                "tmdate": 1700670193632,
                "mdate": 1700670193632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zxh9nvaPC3",
                "forum": "RIbH5ekQpr",
                "replyto": "Ph9fHLsufH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Reviewer_KVAJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your very much prompt response on this, and sorry for being last-minute in my feedback. All immediate concerns have been addressed and I have no further questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697908465,
                "cdate": 1700697908465,
                "tmdate": 1700697908465,
                "mdate": 1700697908465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qesICnTjcU",
            "forum": "RIbH5ekQpr",
            "replyto": "RIbH5ekQpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_FPXN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_FPXN"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Image Polysemy dataset (IMP). IMP is designed to require models to understand that there are different plausible ways of describing an image that goes beyond the approach of crowdsourcing captions from the internet. IMP is collected in a multi-stage process starting from images sourced from Unsplash that includes human annotation and cleaning. The dataset is used to evaluate a broad collection of VLM in a zero-shot and fine-tuned setting. The results show that surprisingly good performance is possible in the zero-shot setting, and that fine-tuning various versions of the CLIP model can indeed improve performance. The experiments also include ablations on different CLIP-style models trained on different amounts of data, different architectures, and on different data sources."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Extensive zero-shot experiments across a large collection of models.\n2. Should prove to be a useful resource for evaluating VLMs."
                },
                "weaknesses": {
                    "value": "1. I didn't feel like both Figure 1 and Figure 3 needed to exist in the main body of the paper because it feels like they are duplicating information. \n2. Not entirely clear why the dataset needed to be based on \"high-quality stock photography from Unsplash\"."
                },
                "questions": {
                    "value": "1. Why is the high-quality stock photography from Unsplash a good source for evaluating models in different scenarios?\n2. Which steps did you take to ensure that none of the models had pretrained on the photos that you used from Unsplash?\n3. Why did you use captions that described other images as the \"human-authored\" versions of the captions? Is CLIP similarity really good enough for this?\n4. Why is RSUM a meaningful measure to report? It doesn't seem like it gives a better understanding of model performance than the original set-based retrieval measures.\n5. What is the purpose of the qualitative analysis in Section 4.3? I couldn't fully understand the contribution of this section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1888/Reviewer_FPXN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699022137258,
            "cdate": 1699022137258,
            "tmdate": 1699636118947,
            "mdate": 1699636118947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3nlEuGO72Z",
                "forum": "RIbH5ekQpr",
                "replyto": "qesICnTjcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FPXN (R3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer $\\textcolor{green}{R3}$ for insightful comments and recognition of IMP as a useful resource for evaluating VLM.\n\n## Modification of the figure 1 and 3\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for suggetion on better organizing the main text.\nWe follow the reviewer's suggestion, now Figure 1 has been moved to appendix so there is no duplicated information.\n\n## Choice of Unsplash Images\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for insightful question on database we select.\nWe would like to give two main reasons we chose Unsplash:\n\n1.   These images from Unsplash, due to their nature (high resolution, less noise, aesthetic value), are more likely to be used as wallpapers or figures in blogs, forums, news, etc. This allows us to search for the use of this exact image over the internet and see how real humans use the image. On the other hand, images from other datasets are mostly likely to only appear once on the internet.\n2.   We would like to make our dataset public, and Unsplash can be openly shared. So researchers can easily use the dataset for experiments or extend the dataset. For each image we have at least 10 perfect-matched image found by google-vision web entity search.\n\n## Pre-training data, which contains Unsplash images\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for valuable comment on potential data leakage.\nWe can confirm that in CC3M and CC12M there is no image-text pair exists from IMP-5k. Since many models either pre-trained on private dataset or massive public dataset, we cannot test the data leakage for all these models. However, since our pipeline can get new image and captions from the web, the leakage problem should be at most a handful number.\n\n\n## Human-authored captions\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for critical comment on details of our caption gather process. \nWhile CLIP similarity is not sufficient to find semantically identical images fully, it did enable us to select candidate captions which we could then use within the further pipeline, including human annotation. Due to our pipeline we do finally end up with high quality captions. Our aim with this procedure was to increase the number of conceptual captions in our dataset, i.e., if a caption can be transferred from one image to a similar image then it is presumably less descriptive.\n\n\n## Evaluation Metric\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for valuable comment on evaluation metric. \nWe are interested in learning how well the model can retrieve all its corresponding captions, which means the ability to deal with image polysemy. We largely agree with the reviewer that RSUM can not fully reflect this aspect. We have added medr and meanr for more analysis and more insights into the experiments, which is included in the shared response and in Appendix C.\n\n\n## Qualitative analysis\n\nWe thank the reviewer $\\textcolor{green}{R3}$ for insightful question on qualitative analysis.\nIn the qualitative analysis section, we have reported several \u201chard captions\u201d and the false-negative rate from the image-text matching task. These hard captions passed the human evaluation, and the image-text pairs make sense to most annotators, but they failed to be matched by the model either zero-shot or after fine-tuning. Assuming we have a candidate image, there can be captions from other images which can be paired with the candidate image. However, since the candidate image and its captions has passed the human check, we would like to evaluate this by the false-negative rate, which should highlight the core difficulties for when models learn from polysemic images."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576477984,
                "cdate": 1700576477984,
                "tmdate": 1700576477984,
                "mdate": 1700576477984,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QJ7gqzGpuB",
            "forum": "RIbH5ekQpr",
            "replyto": "RIbH5ekQpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_6sQn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_6sQn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes IMP, a dataset for image-text pairs in which texts capture polysemy: diverse types of correspondences between each image and its (multiple) captions. The main difference from previous datasets is the curation and inclusion of multiple non-descriptive and naturally-occuring captions for a single image. The paper also reports results on this benchmark on image and text retrieval tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- S1: I think that the research question studied in this paper is significant and interesting. This benchmark would be really beneficial to the community. As far as I can tell, no benchmark like this exists. Previous benchmarks also suffer from losing images over time.\n\n- S2: Overall, both the approach for collecting the dataset and experimental design are sound."
                },
                "weaknesses": {
                    "value": "- W1: Analysis of this dataset can be improved significantly from what is reported in Table 1. My major concern is in the characterization of polysemy, which is not much beyond \u201cnon-descriptive\u201d. For instance, does any kind of image-text correspondence ontology emerge? Perhaps this needs human classification of (a subset) captions into a pre-defined categories, or automatically clustering captions, etc. This is especially important since quantitatively MPL2D cannot distinguish noise from diversity. Finally, I would also like to see further statistics beyond word lengths (word clouds, token/type ratio, etc. that would lead us to have a clearer picture of how this dataset is more diverse than existing ones. \n\n- W2: The tasks that perform on this dataset are standard image- and text- retrieval tasks with zero-shot and fine-tuning evaluation. Further, an adaptation of models that address image polysemy does not lead to improved performance (see, e.g., SE models in Table 4). These experiments are a good start but the paper would be stronger with additional tasks that focus on measuring model\u2019s capability in addressing polysemy such as image captioning generation given a \u201ccaption sense\u201d.\n\n- W3: Besides improving the analysis in W1 above, the discussion of image-text datasets can also be further expanded to include additional datasets in the analysis, including RedCaps and SBU captions. \n\n- W4: Clarity on the data collection process is quite obscure (under Table 4). IMO, it is important to include the details on the database used to retrieve captions (CC3M and CC12M) in the main text so the reader is well-informed about the bias of these captions. In addition, it is important to include the details on how the authors optimize for diversity and quality control."
                },
                "questions": {
                    "value": "At this point, what would change my mind the most is a much more thorough analysis of the dataset that focuses on polysemy (W1)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699164168456,
            "cdate": 1699164168456,
            "tmdate": 1699636118869,
            "mdate": 1699636118869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kFMM3Yajgo",
                "forum": "RIbH5ekQpr",
                "replyto": "QJ7gqzGpuB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6sQn (R2) (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer $\\textcolor{orange}{R2}$ for insightful comments and recognition of the benefit of this dataset to the community. In addition to what listed in the common response, we would like to response:\n\n## Characterisation of polysemy \n\nWe purposefully do not try to strictly define polysemy as any such definition would quickly exclude valid examples. In essence, image polysemy emerges when considering images in real-world scenarios: each person who observes an image may have a different interpretation, the existence of these different interpretations (or meanings) is what defines image polysemy. Polysemy does not necessarily have to be non-descriptive - however, we found that existing captions for images are mainly descriptive and by adding non-descriptive captions we were able to increase the diversity of our dataset. We do not that descriptive versus non-descriptive is not a binary, but rather a spectrum, and we did not find that any natural ontology emerges from the captions.\n\nDistinguishing between noise from diversity is indeed an open challenge, and MPL2D cannot do this in a straightforward manner indeed. However, both MSCOCO and IMP/5K consist of human annotated or human verified captions, which significantly lowers the noise present in these datasets. For a comparison between MSCOCO and IMP/5K we can thus conclude that differences in MPL2D are due to differences in diversity.\n\nFor a large-scale webscraped dataset like Conceptual Captions the MPL2D score is most likely strongly influenced by noise - yet it gives us an understanding of the range of MPL2D and what a possible upper-bound may be. \n\n## Experiments and future work\n\nWe thank the reviewer $\\textcolor{orange}{R2}$ for critical comments on the cross-modal retrieval task, evaluation, and suggestions.\n\nThe intuition of having these experiments is given in the shared reply. We would like to answer further regarding:\n\n>   Further, an adaptation of models that address image polysemy does not lead to improved performance (see, e.g., SE models in Table 4).\n\nThe SetEmbedding (SE) model uses slot attention to have different slots focusing on different regions of the given input. According to the SetEmbedding paper, it can be treated more as a multi-view approach and can achieve SOTA performance on MSCOCO and Flickr30k. Due to the nature of object-centric learning, slot-attention modules would enable the model to link different combinations of objects in the image to different captions. However, we have performed small-scale experiments of how the SE model learns to utilise the slots. For example, when given four descriptive captions and one conceptual caption, it would learn to link the conceptual caption to the background noise (one slot always appears to be dedicated to 'noise'). Increasing the number of slots can somehow mitigate the problem, but we argue that an image can potentially have infinitely many different meanings. So this would need further modification on the SE model to deal with image polysemy, as well as moving beyond the multi-view focus of current SE models, which is out of the scope of this paper.\n\n>   These experiments are a good start but the paper would be stronger with additional tasks that focus on measuring model\u2019s capability in addressing polysemy such as image captioning generation given a \u201ccaption sense\u201d.\n\nWe appreciate the reviewers recognition of the benefits our current experiments. The central question in our work is to show that current contrastive learning-based models fail to deal with the polysemy challenge. Whilst testing other types of models may give further insight into the polysemy challenge, it also adds complexity due to the difference in how these models are trained. Retrieval directly maps onto contrastive-learning and the alignment between images and text representations - and we believe that if the model cannot overcome this challenge in image-to-text retrieval then image captioning would suffer a similar fate. Additional \"sense\" prompts may be used during generation, however, as we cannot determine the set of possible prompts apriori this would leave the polysemy challenge unsolved still.\n\n\n## Expand the discussion to additional datasets and Data Collection Details\n\nWe thank the reviewer $\\textcolor{orange}{R2}$ for valuable suggestions on including more datasets for comparison and more details of diversity and quality control.\nWe have included SBUcaptions in the table, the analysis is based on the entire SBUcaptions (849k). And due to the nature of RedCaps, doing analysis on only one topic will be biased. Instead we select all topics of year 2020, and perform the analysis. We still need some time to get statistics and analysis for RedCaps, once we finished we will add them to the main text."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576377326,
                "cdate": 1700576377326,
                "tmdate": 1700576377326,
                "mdate": 1700576377326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pf1gBanuVp",
            "forum": "RIbH5ekQpr",
            "replyto": "RIbH5ekQpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_9GYX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1888/Reviewer_9GYX"
            ],
            "content": {
                "summary": {
                    "value": "The paper argues that current models are mostly trained on datasets where an image has a single caption and collects a dataset covering both descriptive and conceptual captions and each image could have multiple captions. Experiments show that current models struggle on the dataset even after fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Most prior work seeks to have cleaner and more descriptive captions; it is interesting to see efforts on including more conceptual captions.\n\nThe evaluation shows that current models struggle with retrieving more abstract and conceptual captions, which raises an interesting and challenging problem."
                },
                "weaknesses": {
                    "value": "**A.** It seems that the \u201cimage polysemy\u201d considered in this paper mainly means: the model should have the ability to match images to both \u201cdescriptive\u201d and \u201cconceptual\u201d captions; previous datasets such as COCO contains mainly descriptive captions.\n\nHowever, I am not fully convinced that the collected datasets have more \u201cconceptual\u201d captions than the web image-text data such as CC3M, since many of the captions in the dataset come from web data. The MPL2D also does not indicate that the collected dataset is more \u201cconceptual\u201d.\n\nThe only advantage of the dataset seems to be that it has multiple captions per image while CC3M/12M does not. But if the final goal is to teach a model to match an image to descriptive / conceptual captions, then it is not clear why it is necessary to have multiple captions per image for training; as long as CC3M has a lot of conceptual & descriptive captions, then the model can learn to retrieve both types captions. \nE.g., say there is a dataset A with 1K images each with 5 captions; suppose a dataset B with 5K images each with 1 caption. The captions in A and B are identical. Then I do not see the necessity of training on A if we have dataset B.\n\nIn sum, it would be better if the paper could illustrate either a) why / how the dataset has more diverse / conceptual captions than CC3M or b) the importance of having multiple captions per image. \n\n**B.** The experiments are not very insightful. While the problem of image polysemy is interesting, the paper simply evaluates/fine-tunes current models with image-text retrieval on the collected datasets. The take-away conclusion seems to be that the task is hard and larger models perform better. \nI would expect more analysis and discussions on why and how studying image polysemy could benefit future vision-language models and how to model such a phenomena. For example, does explicitly modeling image polysemy benefit other tasks that could require high-level conceptual understanding (e.g., understanding actions, events, memes, etc)?\n\n**C.** For the image-to-text retrieval evaluation, how does this test model\u2019s ability to handle image polysemy? If I understand it correctly, as each image has 5 matching captions, as long as the model retrieves 1 of the matching captions, then it is counted as correctly retrieved? Then the evaluation protocol does not test whether the model handles polysemy; if for most images, at least 1 of the captions is \u201cdescriptive\u201d, then a model that only \u201cunderstands\u201d descriptive captions will still score high on image-to-text retrieval.\n\n**D.** For comparing the dataset with CC3M/CC12M on MPL2D, why not treat the collected dataset as a single caption dataset (either downsample to one caption per image or just \u201cduplicate\u201d the images)? \n\nIn addition, I am not sure about the takeaway message by comparing MPL2D: is higher MPL2D score better or lower score better? On the one hand, if there are more diverse / conceptual captions, the score is higher; on the other hand, if the captions are noisier, the score is also higher. Thus, a lower/higher score could be attributed to these two possible factors and we cannot make a conclusion."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699523814510,
            "cdate": 1699523814510,
            "tmdate": 1699636118791,
            "mdate": 1699636118791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bxfFX6Fs7u",
                "forum": "RIbH5ekQpr",
                "replyto": "pf1gBanuVp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9GYX (R1) (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer $\\textcolor{red}{R1}$ for highlighting the importance of conceptual captions and their distinction from descriptive captions in our dataset. The reviewer's points raise crucial aspects of our work, which we aim to clarify further in addition to the shared response:\n\n## Definition of \"Image Polysemy\":\n\nWe thank reviewer $\\textcolor{red}{R1}$ for valuable comments on \"Image Polysemy\".\n\n>   It seems that the \u201cimage polysemy\u201d considered in this paper mainly means: the model should have the ability to match images to both \u201cdescriptive\u201d and \u201cconceptual\u201d captions\n\nIt\u2019s correct that in our dataset, we can say captions are \"descriptive\" or \"conceptual\". A more structured way to classify captions is possible, and we would like to leave this to be discovered by models instead of human design. \n\nThe concept of image polysemy in our work revolves around the inherent capacity of an image to convey multiple, diverse meanings. This multifaceted nature of images presents a significant challenge for current vision-language models, particularly those based on contrastive learning. \n\nAs noted in our shared reply, the central aim of this paper is not to develop or expect models to match both descriptive and conceptual captions accurately. Instead, we focus on demonstrating that contrastive learning-based models struggle with the presence of multiple meanings expressed across captions.\n\n>   However, I am not fully convinced that the collected datasets have more \u201cconceptual\u201d captions than the web image-text data such as CC3M. \n>\n>   The MPL2D also does not indicate that the collected dataset is more \u201cconceptual\u201d.\n\nWe would like to clarify that we are not expecting more \u201cconceptual\u201d captions than CC3M. We would expect a mixture of captions with different conceptual levels. The use of MPL2D is to illustrate the level of diversity per dataset instead of more \u201cconceptual\u201d. Moreover, we emphasise that our 5K test set has gone through human annotation, whereas datasets like CC3M have not. Any diversity in our IMP/5K thus comes from polysemy, whereas in CC3M it may still be noise. \n\n## Importance of multiple captions per image:\n\nWe thank reviewer $\\textcolor{red}{R1}$ for insightful questions on the structure of the dataset.\n\n>   b) the importance of having multiple captions per image\n\nWe believe that multiple captions differs from having a larger dataset with single captions per image. We show in the paper that when a model is exposed to multiple interpretations of the exact same image, it fails to recognise the multifaceted nature of visual content. The model could learn to match only one caption to the image if we use a single caption dataset like CC3M. To get a deep understanding of the challenge of image polysemy we believe it is necessary to explicitly model it in the dataset. We see this is a crucial step in advancing vision-language models beyond their current limitations, and beyond the typical paradigm of associating a single image with a single meaning.\n\nTraining models on datasets with single captions per image, even if they include both descriptive and conceptual captions, does not provide the same learning opportunity. It does not explicitly teach the model that multiple equally valid captions can describe a single image, an essential aspect of understanding image polysemy.\n\nMoreover, creating a dataset with multiple captions per image allows us to control the experimental variables, thereby bridging datasets like CC3M and MSCOCO. By choosing a multiple-caption style, we facilitate a more balanced and reasonable comparison of how well models can handle diverse challenges not adequately addressed by existing datasets.\n\nIn conclusion, the multiple captions per image in our dataset are not merely about increasing the quantity of data but are a strategic decision to advance the study of image polysemy. This approach provides a unique opportunity to understand how vision-language models handle real-world challenges such as image polysemy."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576215866,
                "cdate": 1700576215866,
                "tmdate": 1700576215866,
                "mdate": 1700576215866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]