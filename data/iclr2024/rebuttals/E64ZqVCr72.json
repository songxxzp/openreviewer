[
    {
        "title": "Active Domain Adaptation Of Medical Images Using Feature Disentanglement"
    },
    {
        "review": {
            "id": "oTb2t639wv",
            "forum": "E64ZqVCr72",
            "replyto": "E64ZqVCr72",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for combining active learning techniques with domain adaptation. They propose an algorithm for learning task-specific and task-shared features, along with several metrics which are supposed to quantify informativeness of samples for active learning. They evaluate their method on 2 datasets from the medical imaging domain, showing an improvement over the baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Authors performed ablation studies for all proposed modifications\n- The proposed approach achieves (slightly) better results than baselines"
                },
                "weaknesses": {
                    "value": "1. The paper seems written in a rush and it\u2019s difficult to read at times\n2. Using features of a pre trained classifier for L_base does not need to identify the correct task-specific features. Nothing prohibits the model from a) extracting task-independent features and assigning them zero weights in the final classification layer and b) collapsing to the target variable already in the hidden layer (or the logit distribution)\n3. The density estimation approach is incorrect. You cannot reason about probability densities by comparing cosine similarities of **arbitrarily distributed** vectors (e.g., imagine the case where several dimensions are strongly correlated).\n4. In fact most of your objectives suffer from that same problem - the similarities can easily be inflated if multiple dimensions are not independent. Consider using a more probabilistically sound approach, e.g., by incorporating models such as normalizing flows"
                },
                "questions": {
                    "value": "1. \u201cGiven source and target domains S and T, an ideal domain independent feature\u2019s classification accuracy on domain S is close to those obtained using the original images\u2019 features.\u201d - I do not understand this sentence\n2. How did you select the hyperparameters (e.g., the percentiles for similarity cutoffs)? Actually looks like you just fitted the hyperparameters to the final results? Which is incorrect? How did you select hyperparameters for the baselines ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697716338127,
            "cdate": 1697716338127,
            "tmdate": 1699636894856,
            "mdate": 1699636894856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aR5QuHxglA",
                "forum": "E64ZqVCr72",
                "replyto": "oTb2t639wv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Official Review of Submission7450 by Reviewer Hvpz"
                    },
                    "comment": {
                        "value": "Weaknesses 1: The paper seems written in a rush and it\u2019s difficult to read at times\n\nOur Response: We regret that our writing was not clear to the Reviewer.We will proofread the manuscript again and make sure to improve the readability of the final version. \n\nWeakness 2: Using features of a pre trained classifier for L_base does not need to identify the correct task-specific features. Nothing prohibits the model from a) extracting task-independent features and assigning them zero weights in the final classification layer and b) collapsing to the target variable already in the hidden layer (or the logit distribution)\n\nOur Response: We disagree with the reviewer.It is known in domain generalization that when finetuning models on a small amount of data from a different domain the output features tend to collapse in diversity.See section 2 of: Wuyang Chen et. al, \"CONTRASTIVE SYN-TO-REAL GENERALIZATION\".Using L_base helps us prevent such feature collapse.While using M_base we assume that the classifier is the optimal one. Thus the resulting set of features from M_base\u2019s penultimate layer (not the logit distribution) are expected to be the optimal features.  \n\nWhile reviewer's point (b) can possibly occur, the use of dropout after the feature layer mitigates such issues. L_base is not the only criterion for getting task specific and domain specific features. Combined with the other loss functions L_1,L_2,L_3, we ensure that the final z_task is not prone to inaccuracies.\n\nWeakness 3: The density estimation approach is incorrect. You cannot reason about probability densities by comparing cosine similarities of arbitrarily distributed vectors (e.g., imagine the case where several dimensions are strongly correlated).\n\nOur Response: We believe there is a misunderstanding by the reviewer. We are not using density estimation/probability values in any of our proposed loss functions. The only instance is use of predictive entropy for Q_Unc in Eqn. 7, which is a standard technique. \nCan the reviewer be more specific to which equation they are referring to? \n\nWeakness 4 : In fact most of your objectives suffer from that same problem - the similarities can easily be inflated if multiple dimensions are not independent. Consider using a more probabilistically sound approach, e.g., by incorporating models such as normalizing flows\n\nOur Response: We like to clarify that  we use an autoencoder, as described in the text and different loss functions- e.g. reconstruction loss is used for autoencoder training. An autoencoder is a better option than normalizing flows for feature disentanglement since we do not need density distributions.  (A note is that in Fig 1a of the paper I show VAE encoder. In reality it is simple autoencoder since I use only th reconsrucion loss. Perhaps the reviewer is confused because of that?) \n\nQuestions 1: \u201cGiven source and target domains S and T, an ideal domain independent feature\u2019s classification accuracy on domain S is close to those obtained using the original images\u2019 features.\u201d - I do not understand this sentence\n\nOur Response: Let us consider an optimal classification network (M_optimal) which gives the best results for a particular dataset which is part of the source domain S. Our motivation is to obtain domain independent features that give a similar performance on this source domain dataset.  We can rephrase the sentence as \u201cGiven source and target domains S and T, an ideal domain independent feature's classification accuracy on domain S is close to those obtained using features from an optimal classifier.\u201d \n\nQuestion 2: How did you select the hyperparameters (e.g., the percentiles for similarity cutoffs)? Actually looks like you just fitted the hyperparameters to the final results? Which is incorrect? How did you select hyperparameters for the baselines ?\n\nOur response: The percentile based values for eta_1 and eta_2 were chosen based on steps similar to what one would choose for other parameters. Insead of having a fixed value for these parameters we observe that a good threshold is highly dependent on the actual similarity values. Hence we experimented with different percentile values as threshold varying them from 5% to 60% for eta_1, and 20% to 90% for eta_2, across all datasets. We found that eta_1=30% and eta_2=75% gives the best results across all datasets.  \n\nWe have also selected the hyperparameters after careful consideration. We confirm that as per standard practices in machine learning the parameter values were set using a separate validation set which was not part of training or test datasets. We agree that this is a high number of parameters that need to be optimized due to the multiple loss terms in our method. Hence we have taken extra care to ensure that the optimization steps were done with utmost care and we have cross checked them multiple times. Reproducible results were ensured with fixed seed values. The high number of hyperparameters leads to much improved results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331331672,
                "cdate": 1700331331672,
                "tmdate": 1700331331672,
                "mdate": 1700331331672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NlMvD1HRP0",
                "forum": "E64ZqVCr72",
                "replyto": "aR5QuHxglA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
                ],
                "content": {
                    "comment": {
                        "value": "Weakness 2:\n\nSection2 of the linked paper just shows a \u201cmotivating example\u201d on a toy dataset, it is by no means a proof that NNs converge to diverse features in the penultimate layer. Even when using dropout the model can in principle just learn to duplicate a feature N times.\n\nWeakness 3 and 4:\n\nYou are not using probabilities, yes, but you reason about similarities of vectors with your proposed approach. I do not see how your approach can actually guarantee you will select truly similar samples."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482738964,
                "cdate": 1700482738964,
                "tmdate": 1700482738964,
                "mdate": 1700482738964,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mOB9Ei1liR",
                "forum": "E64ZqVCr72",
                "replyto": "oTb2t639wv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Comment by Reviewer Hvpz"
                    },
                    "comment": {
                        "value": "Thank you for the comments.\n1. The strategy of turning off connections in dropout makes it less likely that the learned features will be duplicates of other features.\n\n2. Although no method can guarantee the selection of highly similar samples, the cosine similarity coupled with our multiple constraints can be expected to do better than state of the art methods. This hypothesis is supported by our method's performance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732618287,
                "cdate": 1700732618287,
                "tmdate": 1700737192724,
                "mdate": 1700737192724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8BvTLRq8m",
                "forum": "E64ZqVCr72",
                "replyto": "mOB9Ei1liR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_Hvpz"
                ],
                "content": {
                    "comment": {
                        "value": "1. I do not see how this is true, at least without a proper proof. I believe the opposite is true, since features can be turned off at random, the important ones will have to be duplicated in order to retain performance.\n\n2. Again, I do not think this is true. If you were to use a probabilistic approach for example you would have theoretical guarantees. \nYour improved empirical performance (in terms of predictions) doesn\u2019t necessarily guarantee you are selecting actually similar  samples. This might still happen of course, but is not guaranteed, as you did not provide any theoretical justification for it"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737955106,
                "cdate": 1700737955106,
                "tmdate": 1700737955106,
                "mdate": 1700737955106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UuCvHAsrI3",
            "forum": "E64ZqVCr72",
            "replyto": "E64ZqVCr72",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel strategy for performing domain adaptation in an active learning scenario. \nThe method is based on learning disentangled representations referred to domain and task, from which an informative score is computed on samples from the target domain. The most informative samples (below a certain available budget) are chosen and added to the training set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is interesting and seems to provide good performance\n\n- The problem tackled is very relevant in the medical field due to the cost of annotating data"
                },
                "weaknesses": {
                    "value": "- The proposed method is not simple in terms of optimizations: 5 different losses are used, each with its own hyperparemeter. Also the informative score introduces many hyperparamers. Choosing many hyperparameters is not trivial, and authors report some arbitrary value for them. How were they chosen? It is not completely clear \n\n- The tables are hard to read, best results are not highlighted. I also suggest author report a standard devation, especially in the higher p-value cases\n\n- I think the experimental validation is somewhat lacking, as only two settings were explored (histology and cxr images). I suggest authors also include other modalities or tasks. For example brain MRI with the task of brain age regression (particularly relevant for domain shift), or image segmentation."
                },
                "questions": {
                    "value": "See weaknesses.\n\nAdditional questions: \n\n- Could your framework be adapted to other tasks such as segmentation or regression (as mentioned in the weaknesses)?\n\n- In your experimental protocol (Sec. 4.1) you select at each step 10% of the size of the training data from the target set. This means that the added samples will be in minority in the training set. Have you also tried reweighting them in the classification loss? Can this help in reducing the number of samples required for better AUC? \n\n- For improving the readability of this work, I think that the results could be presented in from of plots of AUC vs size rather then big tables"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7450/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD",
                        "ICLR.cc/2024/Conference/Submission7450/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763484103,
            "cdate": 1698763484103,
            "tmdate": 1700679922363,
            "mdate": 1700679922363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cYWzcV46PR",
                "forum": "E64ZqVCr72",
                "replyto": "UuCvHAsrI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Official Review of Submission7450 by Reviewer FaUD"
                    },
                    "comment": {
                        "value": "Weaknesses 1:\nThe proposed method is not simple in terms of optimizations: 5 different losses are used, each with its own hyperparemeter. Also the informative score introduces many hyperparamers. Choosing many hyperparameters is not trivial, and authors report some arbitrary value for them. How were they chosen? It is not completely clear\n\nOur Response: We would like to clarify that we select the hyperparameters after careful consideration. We confirm that as per standard practices in machine learning the parameter values were set using a separate validation set which was not part of training or test datasets. We agree that this is a high number of parameters that need to be optimized due to the multiple loss terms in our method. Hence we have taken extra care to ensure that the optimization steps were done with utmost care and we have cross checked them multiple times. Reproducible results were ensured with fixed seed values. The high number of hyperparameters leads to much improved results.\n\nWeakness 2:  The tables are hard to read, best results are not highlighted. I also suggest author report a standard deviation, especially in the higher p-value cases\n\nOur Response: Due to space constraints we did not report standard deviations. However we will include them in the final manuscript. The standard deviations among close performing methods is such that there is negligible overlap in their values, especially methods with higher p-values \n\nWeakness 3: I think the experimental validation is somewhat lacking, as only two settings were explored (histology and cxr images). I suggest authors also include other modalities or tasks. For example brain MRI with the task of brain age regression (particularly relevant for domain shift), or image segmentation.\n\nOur Response: We believe that we have done exhaustive validation on 3 different datasets for medical image classification, which has been acknowledged by other reviewers. Our method may work for brain age regression without major changes, but will need major changes for segmentation. Since domain adaptive segmentation  would require the training step to learn structural information, we need to change our method.\n\nAdditional Question 2: In your experimental protocol (Sec. 4.1) you select at each step 10% of the size of the training data from the target set. This means that the added samples will be in minority in the training set. Have you also tried reweighting them in the classification loss? Can this help in reducing the number of samples required for better AUC?\n\nOur Response: We believe the reviewer is alluding to importance sampling. While updating the classifier we use the labels of the newly added samples to finetune the classifier. There is no explicit reweighting, although that can be explored in future work. \n\nAdditional Question 2: For improving the readability of this work, I think that the results could be presented in from of plots of AUC vs size rather then big tables\n\nOur Response: We thank the Reviewer for their suggestion and agree this change might be a good way to represent our results.  We will add the figures to the final version\u2019s supplementary material."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330885836,
                "cdate": 1700330885836,
                "tmdate": 1700330885836,
                "mdate": 1700330885836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R4VZbckGbI",
                "forum": "E64ZqVCr72",
                "replyto": "cYWzcV46PR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. However I still think that more empirical validation is needed on different medical tasks, hence I confirm my current rating"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585505482,
                "cdate": 1700585505482,
                "tmdate": 1700585505482,
                "mdate": 1700585505482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XNsBNJMUU7",
                "forum": "E64ZqVCr72",
                "replyto": "UuCvHAsrI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_FaUD"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional experiments and results. I have read the supplementary material and I think the experimental validation is now more convincing. The new experiments should be mentioned in the revised main text. I gladly raise my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679898558,
                "cdate": 1700679898558,
                "tmdate": 1700679963892,
                "mdate": 1700679963892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IB5TjgNkYC",
            "forum": "E64ZqVCr72",
            "replyto": "E64ZqVCr72",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_nM77"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_nM77"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an innovative active learning method for domain adaptation. The problem at hand involves two data domains: the source and target domains, with a distribution shift between them. The algorithm comprises two key steps. In the first step, data (images) in both domains are transformed into a latent space using two separate autoencoders, and the feature representations in the latent space are disentangled into domain-specific and task-specific representations. It is assumed that the domain-specific representations account for the distribution shift. In the second step, criteria were designed to select informative unlabeled image samples in the target domain for labeling. The labeled image samples are then added to the labeled image samples from the source domain to update the classification model trained on the labeled data from the source domain. This process can be repeated multiple times. The proposed method was evaluated on two public image analysis datasets and outperformed several state-of-the-art active learning methods and a couple of domain adaptation algorithms."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Incorporating active learning, feature disentanglement, and domain adaptation all together seems to be an innovative idea.\n2. The proposed metrics for identifying informative image samples based on disentangled feature representations appear to be effective.\n3. The proposed algorithm was evaluated on two relatively large medical imaging datasets, and it achieved superb results."
                },
                "weaknesses": {
                    "value": "1. The two medical image datasets are quite large. In the experimental setting, at least 10% of the unlabeled data samples in the target domain were selected for labeling and added to the source domain's labeled data to update the classifier. While 10% still represents a significant number, considering the constraints in the research setting\u2014such as limited and expensive expertise in the medical field\u2014it would be more valuable to evaluate the effectiveness of the proposed method with much fewer labeled samples from the target domain, for example, 0.1% or 1%. Unfortunately, this aspect is missing in the paper.\n\n2. The discussion section or the presentation of the paper could be improved. For instance, in the ablation study, each component of the loss function and the metrics for identifying informative data samples was thoroughly examined, and their contributions were reported in the tables. However, there is a lack of in-depth discussion and no clear claims have been made regarding which component contributes more. It is not readily apparent to readers which component has the most significant impact."
                },
                "questions": {
                    "value": "In the algorithm description, the authors initially state that feature disentangling was performed jointly using data samples from both the source and target domains. However, they later mention that the process was performed using data solely from the source domain. Which statement is correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804185415,
            "cdate": 1698804185415,
            "tmdate": 1699636894610,
            "mdate": 1699636894610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gp34cNgj5W",
                "forum": "E64ZqVCr72",
                "replyto": "IB5TjgNkYC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Official Review of Submission7450 by Reviewer nM7"
                    },
                    "comment": {
                        "value": "Weaknesses 1:\nThe two medical image datasets are quite large. In the experimental setting, at least 10% of the unlabeled data samples in the target domain were selected for labeling and added to the source domain's labeled data to update the classifier. While 10% still represents a significant number, considering the constraints in the research setting\u2014such as limited and expensive expertise in the medical field\u2014it would be more valuable to evaluate the effectiveness of the proposed method with much fewer labeled samples from the target domain, for example, 0.1% or 1%. Unfortunately, this aspect is missing in the paper.\n\nOur Response: We thank the Reviewer for raising this point. Although we show results for every 10% increment to the training data, it is done over batches. We use a batch size of 96 in determining the informative samples. On an average in every batch 2-3 samples  of each class are chosen  to add to the training set. This represents 0.1% or less for each class in the NIH dataset. The increase of performance is very low over batches and becomes prominent after every 10% increment of dataset size. In the final manuscript we will provide results for 0.1% and 1% labeled target samples. \n\nWeakneess 2: The discussion section or the presentation of the paper could be improved. For instance, in the ablation study, each component of the loss function and the metrics for identifying informative data samples was thoroughly examined, and their contributions were reported in the tables. However, there is a lack of in-depth discussion and no clear claims have been made regarding which component contributes more. It is not readily apparent to readers which component has the most significant impact.\n\nOur Response: We agree that our discussions could be improved and we should emphasize the most important components of our method. We have addressed this issue in the revised manuscript and  will elaborate further in the final manuscript.\n\nQuestion 1: In the algorithm description, the authors initially state that feature disentangling was performed jointly using data samples from both the source and target domains. However, they later mention that the process was performed using data solely from the source domain. Which statement is correct?\n\nOur Response: We apologize for the confusion. The feature disentanglement network was trained on source and target data. The confusion possibly arises from the text in Sec 4.1. We would like to clarify that in Sec 4.1 we refer to those target domain  samples that were not part of the training process."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330373924,
                "cdate": 1700330373924,
                "tmdate": 1700330634673,
                "mdate": 1700330634673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tkahx21EYp",
                "forum": "E64ZqVCr72",
                "replyto": "Gp34cNgj5W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_nM77"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Reviewer_nM77"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your response. \n\nReviewer: nM77"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676884628,
                "cdate": 1700676884628,
                "tmdate": 1700676884628,
                "mdate": 1700676884628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "otA710Y9d0",
            "forum": "E64ZqVCr72",
            "replyto": "E64ZqVCr72",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_S5xV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7450/Reviewer_S5xV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method that uses feature disentanglement for active learning. The method is demonstrated under domain shift, i.e. actively selecting examples for further training so as to adapt to a shifted target domain. Results are given on multi-centre histopathology and chest x-ray datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method involves an informativeness score that combines measures of uncertainty, \u201cdomainness\u201d, density, and novelty. There is some novelty in this.\nFairly extensive experiments are reported incorporating 6 methods from the literature on two medical applications using public datasets. Overall the performance seems promising.\nUnder \u201cAblation Studies\u201d each of the free parameters in the loss and informativeness score (Eqns (10 and (11)) is set to zero in turn and the effect on performance measured. This is a useful experiment to show that each term has an effect (although in a few cases removal of L_1 or Q_unc seems to have helped, and that could be commented upon)."
                },
                "weaknesses": {
                    "value": "My main criticism is that the method has 4 free parameters in the loss function (Equation (1)) and another 4 in the informativeness score (Equation (11)). This is a high number of hyperparameters to set empirically and it needs to be clear that this has been done carefully and reproducibly. For the histopathology and CheXpert experiments, values are stated without any explanation of how these values were arrived at. This needs some comment, and in particular we need to know for certain that these values were determined without using test data in any way. For the NIH ChestXray experiment, subsection 4.4 describes a greedy hyperparameter search; again it needs to be clarified that test data were not used in this search (presumably). If test performance was used in this search then the results would be invalid. Hopefully this is not the case."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864392570,
            "cdate": 1698864392570,
            "tmdate": 1699636894509,
            "mdate": 1699636894509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Flz9j7C6V7",
                "forum": "E64ZqVCr72",
                "replyto": "otA710Y9d0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Official Review of Submission7450 by Reviewer S5xV"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their comments. Indeed we can confirm that as per standard practices in machine learning the parameter values were set using a separate validation set which was not part of training or test datasets. \n\nWe agree that this is a high number of parameters that need to be optimized due to the multiple loss terms in our method. Hence we have taken extra care to ensure that the optimization steps were done with utmost care and we have cross checked them multiple time. Reproducible results were ensured with fixed seed values.\n\nTo get the values for Histopathology dataset (CAMELYON17) and CheXpert, we follow the same procedure as the NIH dataset. We have mentioned these points in the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330035392,
                "cdate": 1700330035392,
                "tmdate": 1700330035392,
                "mdate": 1700330035392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]