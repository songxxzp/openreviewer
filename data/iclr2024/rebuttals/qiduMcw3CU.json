[
    {
        "title": "Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning"
    },
    {
        "review": {
            "id": "BbNsbmFARo",
            "forum": "qiduMcw3CU",
            "replyto": "qiduMcw3CU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_38Qb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_38Qb"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes skill machines to encode skill primitives of all high-level sub-tasks. Then zero-shot and few-shot learning is used to help train a policy which aims to satisfy specifications composing sub-tasks temporally and logically."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Proposed a principled way to transfer learned task policies to related temporal logic specification.\n2. Conducted thorough experiments to validate the approach."
                },
                "weaknesses": {
                    "value": "1. The definition of (linear) temporal logic is not provided.\n\n2. I would suggest the authors to briefly mention how to convert LTL to reward machines, e.g., through buchi automata, to make it more clear.\n\n3. This work only considers some simple temporal operations such as \"until\" and \"eventually\", but no continuous time interval. Extension to other temporal logic like signal temporal logic (STL) can be more interesting and useful practically -- also more challenging for sure.\n\n4. There are some related work to compare. For instance, I find the following two papers relevant to this paper, the authors are encouraged to compare with them.\n[1]. Luo, X. and Zavlanos, M.M., 2019, December. Transfer planning for temporal logic tasks. In 2019 IEEE 58th Conference on Decision and Control (CDC) (pp. 5306-5311). IEEE.\n[2]. Le\u00f3n, B.G., Shanahan, M. and Belardinelli, F., Systematic Generalisation of Temporal Tasks through Deep Reinforcement Learning."
                },
                "questions": {
                    "value": "1. In Sec 2.2, the authors say \"Consider the multitask setting, where all tasks share the same state and action space, transition\ndynamics and discount factor, but differ in reward function. The agent is required to reach a set of desired terminal states in some goal space G \u2286 S.\" Does this mean all subtasks can be represented as reaching a some goal states? If yes, this should be remarked more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635912755,
            "cdate": 1698635912755,
            "tmdate": 1699636294811,
            "mdate": 1699636294811,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6tVcT7wiGZ",
                "forum": "qiduMcw3CU",
                "replyto": "BbNsbmFARo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate the reviewer\u2019s time and effort spent in reviewing our paper and providing useful feedback. We hope the reviewer\u2019s concerns are fully addressed with our response here, and are happy to clarify any further ones during the discussion period.\n\n**W1:** \n- We have added the definition for LTL in Section 2.1 page 3. We thank the reviewer for this suggestion. Due to space constraints,  we refer the reader to Camacho et al. (2019) for an expanded explanation of LTL and its interaction with RL.\n\n\n**W2:** \n- Similarly, we have added a brief description of how to convert LTL to reward machines in the caption of Figure 1. Due to space constraints,  we again refer the reader to Camacho et al. (2019) for an expanded explanation of how to convert LTL to reward machines.\n\n\n**W3:** \n- We focus on LTL because of its relative simplicity and popularity in the literature, which is integral to not obfuscate the focus of this paper\u2014which is about addressing the spatial and temporal curses of dimensionality by composing sub-tasks logically and temporally. Extending this work to other types of temporal logic like STL is definitely an interesting direction for future works. Since such an extension is not trivial, it is outside the scope of this work, but we believe that this work forms a good foundation for such extensions (since STL is a generalisation of LTL). \n\n\n**W4:** \n- We thank the reviewer for the suggested works, and we are happy to include them in our related works. We will further discuss them here and hope that this clarifies why we omitted them from our baselines.\n\n\n- To the best of our understanding, Luo et al. (2019) assume the usual planning/optimal-control setting, which is a very different setting to ours and all of the prior works cited in this paper. Mainly, they assume that the environment dynamics are known and amenable to planning. Here, they are more interested in how to efficiently obtain and transfer plans from previous LTL tasks to new ones. Hence, they also do not compare against reward machines but instead compare against a prior work on temporal logic optimal-control. Note that in our work and prior works, the environment dynamics are not known (hence why RL is used), hence we are unsure how the suggested baseline can be applied in this setting (since planning at the level of dynamics is not possible). In our work and prior works, planning (if used) only happens at the level of the reward machines.\n\n- We believe that CRM is representative of Le\u00f3n et al. (2019) in our Figure 3 experiments. Since our work is the first to attempt and achieve both spatial and temporal skill composition in temporal logic tasks provably (to the best of our knowledge), the aim of this experiment was to demonstrate the importance and benefit of our approach compared to the other popular ones considered in prior works. To elaborate slightly:\n\n  -  In Le\u00f3n et al. (2019), assuming that the tasks are specified using TTL, they propose using a combined or separate task and observation module (LSTMs and CNNs) to end-to-end learn an embedding of \u201cTTL progressions\u201d. They then demonstrate empirically that their approach can learn given tasks and also achieve some generalisation to new ones, with varying success depending on the architectures used. However, CRMs and Le\u00f3n et al. (2019) do not leverage spatial or temporal skill composition. Hence, they rely on function approximation (e.g neural networks) to generalise both spatially and temporally. Hence, considering the spatial and temporal curses of dimensionality described in our paper (and the arbitrarily long temporal sequence of sub-tasks present in each task), they have to learn every new task with high probability. Consequently, CRMs provide a baseline representative of the class of algorithms without spatial or temporal composition that instead leverages the compositional structure of temporal logic to learn optimal policies efficiently.\n\n\n- Thus, we are uncertain what benefit a comparison with Le\u00f3n et al. (2019) would provide which is not already achieved by comparing with CRMs, and would appreciate the reviewer's thoughts on this point. We do still agree that these are important citations and will gladly include them as such.\n\n\n**Q1:** \n- Yes. We thank the reviewer for the suggestion and have clarified this sentence in the background (Sec 2.2 Page 3)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510049380,
                "cdate": 1700510049380,
                "tmdate": 1700511847138,
                "mdate": 1700511847138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BrYRvfiKxT",
            "forum": "qiduMcw3CU",
            "replyto": "qiduMcw3CU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces skill machines, which are finite state machines derived from reward machines, enabling agents to tackle complex tasks involving temporal and spatial composition. These skills are encoded in a specialized goal-oriented value function. By combining these learned skills with the value functions, downstream tasks can be solved without additional learning. Importantly, this method guarantees that the resulting policy aligns with the logical task specification. The behavior generated is provably satisficing, with empirical evidence indicating performance close to optimality. The paper suggests that further fine-tuning can enhance sample efficiency if optimal performance is required."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper showcases the development of an agent with the ability to flexibly compose skills both logically and temporally to provably achieve specifications in the regular fragment of linear temporal logic (LTL). This approach empowers the agent to sequence and order goal-reaching skills to satisfy temporal logic specifications. Zero-shot generalization is important in this context is important due to the vastness of the task space associated with LTL objectives,  rendering training across every conceivable scenario intractable."
                },
                "weaknesses": {
                    "value": "The paper raises concerns about the optimality of the policy resulting from composition, as the task planning algorithm lacks consideration for the cost of sub-tasks within a skill machine. Consequently, the generated behavior can be suboptimal due to this oversight. Although the paper presents a method for few-shot learning to create new skills, it remains unclear why learning a new skill is preferred over recomposing existing ones, raising questions about the approach's efficiency. Furthermore, it does not seem a learned new skill in few-shot learning is consistent with Definition 3.1.\n\nAn empirical comparison between the paper's few-shot learning strategy and the Logical Options Framework (LQF) baseline is lacking. The LQF baseline employs a meta-policy for choosing options to achieve subgoals within the finite state automaton representation of an LTL property. This approach integrates value iteration over the product of the finite automaton and the environment, ensuring that options can be recombined to fulfill new tasks without necessitating the learning of entirely new options. \n\nFurthermore, the absence of a comparison with LTL2Action in the continuous safety gym environment raises questions. The paper concludes that the zero-shot agent's performance is nearly optimal. It is crucial to assess this claim by comparing the zero-shot agent's performance in terms of discounted rewards with that of the LTL2Action agent in the safety gym environment.\n\nThere is room for improvement in the execution of the paper, particularly in the clarity of its formalization. Following the formalization proved to be challenging, and below, I have outlined my specific concerns regarding this aspect.\n\nBrandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan A. DeCastro, Micah J. Fry, and Daniela Rus. The logical\noptions framework. CoRR, abs/2102.12571, 2021"
                },
                "questions": {
                    "value": "* In Sec 2.2, $\\pi^\\ast(s) \\in arg max_a max_g \\bar{Q^\\ast}(s, g, a)$.  I would appreciate clarification regarding the reward function associated with the policy $\\pi^\\ast$. Does this imply that the policy is considered successful whenever it reaches any goal $g$ within the goal space?\n\n* In Definition 3.1, what is the purpose of tracking $c$ as a history of safety violation to constraints in $\\mathcal{C}$?\n\n* In the work by Nangue Tasse et al. (2020), the reward function for a skill specifies that if the agent enters a terminal state corresponding to a different task, it should incur the most substantial penalty possible. I am curious why this aspect is omitted in Definition 3.1.\n\n* At state $u$ in a skill machine, it computes a skill $\\delta Q(u)(\\langle s, c\\rangle,\\langle a, 0 \\rangle) \\mapsto max_g \n\\bar{Q}^\\ast_u (\\langle s, c \\rangle, g, \\langle a, 0 \\rangle)$ that an agent can\nuse to take an action $a$.  Is it necessary for this skill to enumerate the entire goal space? At every state within the skill machine, you have already derived a Boolean combination of primitive Q functions concerning specific goals. For instance, in $u_0$ from Fig. 1, the relevant goals include the green button and the blue region. Could we potentially use the green button directly as the goal for Q_button and the blue region as the goal for Q_blue, thereby bypassing the need to exhaustively enumerate all possible goals?\n\n* Relatedly, in the scenario where the goal space is continuous, how does the algorithm determine the optimal value for $g$?\n\n* As per Definition 3.1, each skill is associated with a sub-goal. However, in the context of few-shot learning, what constitutes the goal for a newly acquired skill?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3429/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg",
                        "ICLR.cc/2024/Conference/Submission3429/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728284351,
            "cdate": 1698728284351,
            "tmdate": 1700684791585,
            "mdate": 1700684791585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h2xnUzBcNe",
                "forum": "qiduMcw3CU",
                "replyto": "BrYRvfiKxT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 1/2)"
                    },
                    "comment": {
                        "value": "We really appreciate the reviewer\u2019s time and effort spent in reviewing our paper and providing useful feedback. We hope the reviewer\u2019s concerns are fully addressed with our response here, and are happy to clarify any further ones during the discussion period.\n\n> Weaknesses\n\n**W1: [Few-shot learning]** \n- Please note that we do not claim that our few-shot approach is the best way to improve our zero-shot policy. In fact, by using Q-learning to learn a new skill, our aim here was to use a relatively simple approach to not obfuscate the main point of this section (which is that our zero-shot policy can be improved in a way that is guaranteed to converge to the optimal policy).\n \n- Investigating other few-shot methods for improving our zero-shot policy is definitely an interesting direction for future works. For example, by designing efficient algorithms for learning the best compositions of skill primitives to use.\n\n- (also from Q6) In the few-shot learning case, the new skill learned is not a skill primitive, but just a regular action-value function for the specific task. We have clarified this point on page 6 (the highlighted text).\n\n**W2: [Baselines]** This is an important concern from the reviewer, and we hope our discussion here fully clarifies why we omitted the 2 suggested papers from our baselines (despite citing them).\n\n- We believe that HRM [Icarte et al., 2022] is representative of LOF [Araki et al., 2021] and that CRM [Icarte et al., 2022] is representative of LTL2Action [Vaezipoor et al., 2021] in our Figure 3 experiments. Since our work is the first to attempt and achieve both spatial and temporal skill composition in temporal logic tasks provably (to the best of our knowledge), the aim of this experiment was to demonstrate the importance and benefit of our approach compared to the other popular ones considered in prior works. To elaborate slightly:\n\n  - While approaches that only leverage temporal skill composition like HRMs and LOF are able to learn tasks fast, they all still require learning the sub-task options for most new tasks. They are also all hierarchically optimal, while our few-shot approach is globally optimal (Figure A5 and A6 shows how the performance of a hierarchically optimal policy can be vastly lower than that of a globally optimal policy).\n\n  - In LOF, the authors learn options for each subgoal proposition of the reward machine, then use value iteration or Q-learning to learn a meta-policy over options to solve the **current** task. These options can then be reused in new tasks (i.e new reward machines) where the same sub-goal propositions appear. As described in the introduction and observed by Liu et al. (2022), in all these works (similarly to HRMs), the options learned from a previous task can not be transferred satisfactorily to some new tasks (also see the example on page 4). For example in the office world, if in a previous task the agent learns an option for \u201cgetting coffee\u201d, it can reuse it to \u201cget coffee then deliver it to the office\u201d, but it cannot reuse it in a new task where it needs to \u201cget coffee without breaking decorations\u201d. In contrast, Skill Machines (our work) are able to bypass this issue by learning skill primitives that can then be composed zero-shot to get coffee with or without constraints. Thus, HRMs and LOF omit spatial compositionality and HRMs provide an appropriate baseline which is representative of the limitations of all such algorithms without spatial compositionally that instead leverages temporal compositionally to learn hierarchically-optimal policies. \n\n  - In LTL2Action, assuming that the tasks are specified using LTL, they propose using a separate LTL module (a graph neural network) to end-to-end learn an embedding of \u201cLTL progressions\u201d. They then demonstrate empirically that their approach converges to more optimal policies than a myopic baseline and also achieves comparable or better generalisation. However, CRMs and LTL2Action do not leverage spatial or temporal skill composition. Hence, they rely on function approximation (e.g neural networks) to generalise both spatially and temporally. Hence, considering the spatial and temporal curses of dimensionality described in our paper (and the arbitrarily long temporal sequence of sub-tasks present in each task), they have to learn every new task with high probability. Consequently, and similarly to the relationship between HRMs and LOF, CRMs provide a baseline representative of the class of algorithms without spatial or temporal composition that instead leverages the compositional structure of temporal logic to learn optimal policies efficiently.\n\n- Thus, we are uncertain what benefit a comparison with LOF or LTL2Action would provide which is not already achieved by comparing with HRMs and CRMs, and we would appreciate the reviewer's thoughts on this point. We do still agree that these are relevant citations, reflected by the fact that we did cite them in the original version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508990353,
                "cdate": 1700508990353,
                "tmdate": 1700511795143,
                "mdate": 1700511795143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0tlVbvcBB5",
                "forum": "qiduMcw3CU",
                "replyto": "h2xnUzBcNe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "content": {
                    "title": {
                        "value": "Comparison with LOF"
                    },
                    "comment": {
                        "value": "I believe there is a misunderstanding about LOF. LOF assumes safety propositions in an RL environment with LTL objectives, learning options to reach subgoals while ensuring safety propositions are never met by the options. It then learns a meta-policy via value iteration to choose options for achieving subgoals within the finite state automaton representation of an LTL property. Thus, LOF can combine options to fulfill new tasks without needing to learn new options. I disagree with the statements that \"LOF omits spatial compositionality\".\n\nIn this manuscript, safety constraints are similarly introduced, and I don't see the setup of safety constraints versus safety propositions as different. The proposed approach also uses value iteration to learn to combine options (or primitives, as the authors term them) but requires few-shot learning of a new \"skill\" to ensure optimality.\n\nThe key difference, in my opinion, is that during value iteration, LOF considers the performance of an option to choose the best combination, whereas the proposed approach does not consider the reward performance of the primitives in this process, necessitating the learning of a new \"skill\" to compensate. This is why I consider LOF a relevant baseline for understanding the tradeoffs of these different approaches.\n\nRegarding the comparison with LTL2Action, I agree with the authors that this approach differs significantly. However, I suggest considering LTL2Action as an upper bound on policy performance. If the authors claim that their few-shot learning approach achieves global optimality, it would be beneficial to see if their reward performance indeed matches or is close to that of LTL2Action.\n\nBased on these points, my stance remains on the negative side.\n\n[LOF] Brandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan A. DeCastro, Micah J. Fry, and Daniela Rus. The logical options framework. ICML 21."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670709209,
                "cdate": 1700670709209,
                "tmdate": 1700670709209,
                "mdate": 1700670709209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IdSnpNGLzf",
                "forum": "qiduMcw3CU",
                "replyto": "BrYRvfiKxT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarification. Still unconvinced."
                    },
                    "comment": {
                        "value": "**1. To demonstrate the avoidance behavior, the system designers must include as a part of the safety propositions in LOF, otherwise LOF policies will not prevent avoidance behavior at execution time.**\n\nIndeed, LOF does require safety propositions in advance. However, skill machines also need predefined safety constraints to learn the skill primitives (as per Definition 3.1).\n\n**2. Thus for 1 sub-task proposition, and $n$ safety propositions, the LOF library must train $2^n$ options for each avoidance prosposition.**\n\nAccording to \"Algorithm 1 Learning and Planning with Logical Options\" in the LOF paper, it only needs to learn one option per sub-task proposition, regardless of the number of safety propositions.\n\n**3. The difference in behavior will only be uncovered if the task environment is configured such that in satisfying the reach proposition, the safety proposition will be along the way.**\n\nThis is precisely why I find the LTL2Action environments to be valuable.\n\n**4. SKill machine can demonstrate avoidance behaviors without explicitly training for it through logical composition**\n\nI agree demonstrating avoidance behaviors without explicitly training would be a valuable feature. However, I have reservations about whether skill machines indeed have this feature. From the paper, before Definition 3.1, \"By setting the blue region proposition as a constraint, the agent keeps track (in its cross-product state) of whether or not it has reached a blue region in its trajectory when learning a primitive\". It seems that this aligns exactly with what LOF does for learning options to avoid safety propositions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674153895,
                "cdate": 1700674153895,
                "tmdate": 1700674611786,
                "mdate": 1700674611786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aHLUy6Wwy7",
                "forum": "qiduMcw3CU",
                "replyto": "qqzlLJflZO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you. I now realize that the ability for compositional avoidance is an advantage of skill machines over LOF. My initial concern was focused on learning a new skill versus learning a new combination of existing skills, as in the LOF approach. The discussion on compositional avoidance might be slightly off the intended track. My concern remains after this discussion. However, it is good to know that this paper contributes to the advancement of existing methods, at least in one aspect. Based on this, I increased the score to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684761946,
                "cdate": 1700684761946,
                "tmdate": 1700684761946,
                "mdate": 1700684761946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qcIOVR7BiW",
            "forum": "qiduMcw3CU",
            "replyto": "qiduMcw3CU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out that a large number of combinations of high-level goals can lead to the curse of dimensionality. To address this issue, this paper presents skill machines, which flexibly compose a set of skill primitives both logically and temporally to obtain near-optimal behaviors. This paper demonstrates that skill machines can map complex temporal logic task specifications to near-optimal behaviors, which are sufficient for solving subsequent tasks without further learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation for this paper is quite sound. The agent begins by converting the LTL task specification into a reward machine (RM) and proceeds to determine the suitable spatial skill for each temporal node through value iteration. It also composes its skill primitives into spatial skills, effectively forming a skill machine. Finally, the agent applies these skills to solve the task without requiring additional learning."
                },
                "weaknesses": {
                    "value": "One weakness of the paper lies in its relatively complex non-end-to-end training process. More precisely, it necessitates reinforcement learning training for acquiring skill primitives and additional value iteration for the selection of appropriate spatial skills. Another potential limitation is its applicability, which may be more suited to navigation scenarios. It requires different linear temporal logic (LTL) task specifications tailored to specific application scenarios, potentially leading to challenges in generalization across various scenarios. What\u2019s more, the descriptions of definitions 3.1 and 3.2 are not clear enough and lack some corresponding explanations."
                },
                "questions": {
                    "value": "1. How are different tasks specifically set in each benchmark? Why does the agent achieve zero-shot performance?\n\n2. The policy generated by Skill Machine (SM) may be locally optimal, so Theorem 3.4 assumes global reachability and the policy is satisficing. There may be enough combinations of skills to approach this assumption, but it will limit the agent's exploration ability. And what is the approximate performance gap when certain states are unreachable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3429/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3429/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826868649,
            "cdate": 1698826868649,
            "tmdate": 1700701813182,
            "mdate": 1700701813182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n4J9ZXJSzu",
                "forum": "qiduMcw3CU",
                "replyto": "qcIOVR7BiW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate the reviewer\u2019s time and effort spent in reviewing our paper and providing useful feedback. We hope the reviewer\u2019s concerns are fully addressed with our response here, and are happy to clarify any further ones during the discussion period.\n\n> Weaknesses\n\n**W1:** \n- Please note that the use of RL to acquire skills is standard in the literature. Hence, while the need for RL to learn the skill primitives is indeed a weakness of this work, it is also a weakness of all prior works that do not assume known environment dynamics (e.g [Vaezipoor et al., 2021; Jothimurugan et al., 2021; Liu et al., 2022; Icarte et al. 2022]). \n\n- However, investigating approaches for acquiring skill primitives without RL is still an interesting direction for future works. For example, by using planning when the environment dynamics are known or behaviour cloning/imitation learning/offline RL when appropriate trajectories datasets are available. \n\n- Please note that the planning (value iteration) done in this work is only over the reward machines (it does not include the environment dynamics), and hence uses no additional environment samples. Value iteration is also a standard algorithm in RL [Sutton & Barto, 2009].   \n\n**W2:** \n- The focus on LTL (and regular languages in general) indeed limits the applicability of this work, and to the best of our knowledge, similarly limits the applicability of all prior works in this litterature (e.g [Vaezipoor et al., 2021; Jothimurugan et al., 2021; Liu et al., 2022; Icarte et al. 2022]). \n\n- There is however strong literature justifying the benefit of specifying tasks using LTL (and formal languages in general) rather than scalar rewards (e.g [Li et al., 2017; Littman et al., 2017]). The main ones are the ease of specifying desired/undesired behaviours and sample efficiency.\n\n- We use LTL for navigation tasks in our experiments since they are a canonical scenario used in the literature, and are easier to understand (e.g [Vaezipoor et al., 2021; Jothimurugan et al., 2021; Liu et al., 2022; Icarte et al. 2022]). LTL can also be used to specify other types of tasks, like robot reach&manipulation tasks (e.g [Li et al., 2017; Jothimurugan et al., 2021; Araki et al., 2021;])\n\n**W4:** \n- We have added Figures A7 and Figure A8 to further improve the understanding of Definitions 3.1 and Definition 3.2. We hope these help clarify the reviewer\u2019s concern here, and are happy to further clarify any aspect of these definitions.\n\n\n>  Questions\n\n**Q1:** \n- For each task in each benchmark, the RM is obtained by converting the LTL expression into an FSM using Duret-Lutz et al. (2016) (https://spot.lre.epita.fr/app/), and then giving a reward of 1 for accepting transitions and 0 otherwise. We then use the code base of Icarte et al. (2022) (https://github.com/RodrigoToroIcarte/reward_machines) to set the tasks for each RM.\n\n- Our agent achieves zero-shot performance because at each FSM state, the learned skill primitives can be composed to achieve the Boolean expression obtained from value iteration (if it is achievable). We have added Figure A8 to make this process clearer.\n\n**Q2:**  \n- We have added two experiments (Figure A5 and Figure A6 on page 18) where we evaluate the performance of our approach in different cases when the reachability assumption does not hold. Please see the highlighted text in Section A.4 for the discussion. In brief, we observe that the performance of the zero-shot agent may significantly decrease when the reachability assumption does not hold in some states (Figure A5), and may lead to complete failures in the worst-case scenario when the reachability assumption does not hold in all states (Figure A6). In both cases, we observe that the few-shot agent can still quickly improve on the zero-shot performance by learning the optimal policy."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508715047,
                "cdate": 1700508715047,
                "tmdate": 1700508715047,
                "mdate": 1700508715047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ReFxhkSJIi",
                "forum": "qiduMcw3CU",
                "replyto": "n4J9ZXJSzu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for providing the response to address my questions, which has partially alleviated my concerns. While the authors have cited several prior studies tested on manipulation tasks, it appears that this paper itself has not conducted such evaluations. Additionally, the issue of generalization still seems to be unresolved. I will temporarily withhold my score, but I also believe that this paper has a certain value and its quality is close to the acceptance standards of ICLR. If this paper is accepted, I will not hold any objections. Meanwhile, I will refer to the opinions of other reviewers and maintain consistency with the majority of them."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632896884,
                "cdate": 1700632896884,
                "tmdate": 1700632896884,
                "mdate": 1700632896884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MYVFDsYflZ",
                "forum": "qiduMcw3CU",
                "replyto": "qcIOVR7BiW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the further explanation"
                    },
                    "comment": {
                        "value": "Now I realize the applicability and generalizability of this framework. After considering the other reviewer's discussion, I believe this paper is valuable for the community. I hope to see more experimental results, such as manipulation task, in the camera-ready version if possible. I will raise my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702236762,
                "cdate": 1700702236762,
                "tmdate": 1700702236762,
                "mdate": 1700702236762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1opaO6tCk8",
            "forum": "qiduMcw3CU",
            "replyto": "qiduMcw3CU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the skill machines formalism. This is a variant of the reward machine formalisms where the agent learns Q-functions to satisfy individual task propositions, and then plans a temporal sequence of proposition states to be achieved through value iteration over the skill machine. Once the optimal path through the reward machine is found, the authors propose to use logical Q-function approximation from prior work to initialize the global semi-markov decision process Q-function. The initial policy can serve as a good zero-shot approximation to a satisficing policy. While the Q-function can also be optimized through exploration to generate a hierarchically optimal solution for the overall problem. \n\nThe key assumption in this problem is that all reward machine transitions are possible from all given reward machine states. Such as assumption is usually satsified when the domain involved proposition encoded as occupying certain regions of the state-space. It is definitely true if the propositions cover non-overlapping regions, but I am less certain if the assumption holds when there are overlapping regions as well. Nevertheless, the authors do clearly state the requisite assumption for the validity of their proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality, and significance**: The paper makes a significant contribution by providing an approach to compose skills both through proposition logic and temporal operators. This is quite a unique capability, and many works have proposed partial solutions for the same. The ideas, and evaluations presented in this work are quite compelling, and are theoretically sound. There are some claims that need to be examined in a greater detail as listed in the weakness and limitations sections, but nonetheless the paper is a significant advance.\n\n**Evaluations**: The authors decision to push the complexity of the task specification must be commended. This evaluation explicitly tests for upward generalization capabilities. This can and should be strengthened by randomly sampling LTL goals from predefined distributions over specifications with a rejection sampling approach for unsatisfiable specifications, however, I am convinced regarding the validity of the approach from the theoretical arguments presented in the paper, and the scrupulous evaluations over varying complexity formulas.\n\n**Comparison to prior LTL-based transfer approaches**: The comparison with prior approaches and the explanations of the key differences in capabilities is much appreciated."
                },
                "weaknesses": {
                    "value": "**Environment validity:** The assumption of task satisfiability, and proposition transition reachability are quire strong. It is unclear what the test for whether an environment satisfies the assumptions required by the approach. The authors can ameliorate this by either providing domains where these assumptions always hold (environments where propositions encode visiting specific regions, and there being no overlap between propositions is one such environment), and they can also specifically search for a violating environment specification pair, and demonstrate what their approach outputs in that case. This information will be valuable to practitioners who would like to utilize skill machines.\n\n**Relative expressivity of RMs vs LTL**: I believe that only a fragment of LTL can be translated into a reward machines, and there exist reward machines that cannot be expressed through an LTL formula. I would suggest the authors to appropriately restrict the LTL formula class they allow as input to the model to a fragment of LTL that is known to be expressible as a reward machine. (I believe the obligations segment)"
                },
                "questions": {
                    "value": "Please refer to the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885102315,
            "cdate": 1698885102315,
            "tmdate": 1699636294472,
            "mdate": 1699636294472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yby70jW3DJ",
                "forum": "qiduMcw3CU",
                "replyto": "1opaO6tCk8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate the reviewers' positive outlook on our paper, their time spent reviewing it, and providing useful feedback. We have updated the paper to reflect the reviewer\u2019s suggestions regarding the weaknesses:\n\n\n**W1 [Environment validity]:** \n- We have added two experiments (Figure A5 and Figure A6 on page 18) where we evaluate the performance of our approach in different cases when the reachability assumption does not hold. Please see the highlighted text in Section A.4 for the discussion. In brief, we observe that the performance of the zero-shot agent may significantly decrease when the reachability assumption does not hold in some states (Figure A5), and may lead to complete failures in the worst-case scenario when the reachability assumption does not hold in all states (Figure A6). In both cases, we observe that the few-shot agent can still quickly improve on the zero-shot performance by learning the optimal policy.\n- It would be interesting to have an efficient test for whether the satisfiability and reachability assumptions hold for a given environment-specification pair. Unfortunately, this is non-trivial. Consider for example environments where propositions encode visiting specific regions and there is no overlap between propositions, as suggested by the reviewer. The Office GridWorld with a unique proposition for each decoration and coffee location is such an example. However, if the entrance to the office and mail rooms are blocked (say by a wall), then the office and mail propositions can never be satisfied when starting from positions outside of those rooms. That said, investigating efficient ways of checking those assumptions is an interesting avenue for future work.\n\n\n**W2 [Relative expressivity of RMs vs LTL]:** \n- Indeed, only some fragments of LTL can be translated into reward machines. We restrict ourselves to such fragments, such as co-safe LTL as shown in Camacho et al. (2019). We have stated this more explicitly in Section 2.1 page 3. We thank the reviewer for this suggestion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508135418,
                "cdate": 1700508135418,
                "tmdate": 1700508135418,
                "mdate": 1700508135418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vuRkrTmOEk",
                "forum": "qiduMcw3CU",
                "replyto": "0tlVbvcBB5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "content": {
                    "title": {
                        "value": "In Defense of Author's LOF claims"
                    },
                    "comment": {
                        "value": "I wish to clarify the LOF claims of the authors hopefully with some examples. Reviewer pDWg is indeed correct in stating that LOF can handle the avoidance tasks through the use of safety propositions by ensuring that all propositional policies avoid satisfying all safety propositions throughout their execution, thus demonstrating avoidance behavior. However the author's claims are slightly different here:\n\n1) To demonstrate the avoidance behavior, the system designers must include $decorations$ as a part of the safety propositions in LOF, otherwise LOF policies will not prevent avoidance behavior at execution time. Also as a side effect the safety proposition avoidance will be seen for all downstream tasks if that policy is used. Therefore if an avoidance behavior is not necessary, then LOF must be pre-trained with options to turn avoidance behaviors on and off\n2) Thus for 1 sub-task proposition, and $n$ safety propositions, the LOF library must train $2^n$ options for each avoidance prosposition. However skill machines Q-function composition will require training only $n+1$ options and initializing the Q-function for any requisite avoidance through logical composition.\n3) This is an important capability difference between LOF and skill machines. This is also a difference that may not be uncovered simply through a comparative evaluation and comparing the task completion rates. The difference in behavior will only be uncovered if the task environment is configured such that in satisfying the reach proposition, the safety proposition will be along the way. \n4) The author's argument is not that LOF cannot demonstrate avoidance behaviors, but rather it can demonstrate avoidance behaviors without explicitly training for it through logical composition approach presented by the authors.\n\nThus I would argue that the there is enough differences between LOF, LTL2Action and skill machines. I also believe that qualitatively skill machines handles tasks that LOF cannot, and while I think the reverse is true as well, this is not a disqualifying criterion for skill machines to be accepted for publication."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672466732,
                "cdate": 1700672466732,
                "tmdate": 1700672466732,
                "mdate": 1700672466732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KpoUyYjuQE",
                "forum": "qiduMcw3CU",
                "replyto": "Yby70jW3DJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "content": {
                    "title": {
                        "value": "Happy with modifications"
                    },
                    "comment": {
                        "value": "I retain my strong positive evaluation of this paper. Having read the other reviewer's comments, I think the concerns are as follows:\n1) Comparisons with prior works in transfer of LTL formulas to novel tasks: I largely agree with the author's assessments that HRM and CRM are conceptual standins for prior approaches as mapped in their rebuttal, and while there might be implementation details, requiring the authors to run them would be running baselines for the sake of running baselines. However it is important that the authors clarify the prior approaches and their mappings to the baselines in the sections where they define these baselines. Running these baselines explicitly also helps to provide direct empirical evidence that they are conceptually similar to the selected baselines, but again I do not feel that this should be a disqualifying criterion.\n2) Choice of LTL as a specification language: I believe that utility of LTL as a specification tool is an open question with no settled answers. Whether scalar rewards or STL are more beneficial as specification frameworks for RL is outside the scope of the discussion of this paper. As a reviewer my belief about LTL or Reward machines not being an appropriate tasks specification should not be the basis of assessing the soundness of the paper.\n\nThe authors have also tried to respond to the soundness and clarity concerns throughout their responses, and I believe that there are no outstanding correctness concerns. The remaining clarity concerns can be addressed without substantially changing the rhetorical claims of the paper.\n\nIn view of this I retain my positive assessment, and invite the other reviewers to revisit their assessments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673340982,
                "cdate": 1700673340982,
                "tmdate": 1700673340982,
                "mdate": 1700673340982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqzlLJflZO",
                "forum": "qiduMcw3CU",
                "replyto": "IdSnpNGLzf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
                ],
                "content": {
                    "title": {
                        "value": "Contd."
                    },
                    "comment": {
                        "value": "Thank you for the clarification. I agree that LTL2Action environments are valuable, and that LTL2Action is an applicable baseline but do not believe that it would be a disqualifying factor simply because of how distinct these approaches are. While we disagree on whether this might be a disqualifying weakness, I think we agree in principle.\n\nOn LOF:\n\n1) I think the paper indeed makes the claim that compositional avoidance behavior can be achieved without explicitly training for it, case in point the moving targets domain where the set of subgoals and the set of constraints is identical (Section. \n2) In contrast in the LOF paper, the statement of Algorithm 1 holds only for a given fully defined safety automaton. The authors further clarify this in Section (A.3) (https://arxiv.org/pdf/2102.12571.pdf), and I quote the authors here:\n> Option policies are learned by training on the product of the environment and the safety automaton, $\\mathcal{E}\u00d7\\mathcal{W}_{safety}$ \nand\n> This is because in LOF, safety properties are not composable, so using a learning algorithm that is satisfying and optimal but not composable to learn the safety property is appropriate.\n\nFinally, the authors also mention that the safety property must not change throughout the task (depending on the liveness state), whereas skill machines places no such restrictions. Again, from the LOF authors:\n\n>Note that since the options are trained independently, one limitation of our formulation is that the safety properties cannot depend on the liveness state. In other words, when an agent reaches a new subgoal, the safety property cannot change. However, the workaround for this is not too complicated. First, if the liveness state affects the safety property,\nthis implies that liveness propositions such as subgoals may be in the safety property. In this case, as we described above, the subgoals present in the safety property need to be substituted with \u201csafety twin\u201d propositions. Then during option training, a policy-learning algorithm must be chosen that will learn sub-policies for all of the safety property states.\n\nNote that the work around that the authors suggest is specifically learning a different property for all possible safety constraints (or safety automata) that the agent would encounter during test execution. At face value it may seem that defining constraints the way the authors have done here is similar to the way LOF authors define safety propositions, however the key difference here is that the LOF authors require safety and subgoal propositions to be mutually exclusive sets with some work arounds proposed, whereas skill machines does not require this assumption to hold. \n\n3) On the other hand Paragraph 2 page 4 explicitly motivates the need to compose avoidance constraints on the fly, and the evaluations in the moving target domain are geared towards test-time safety constraint composition. Which leads me to believe that skill machines is capable of such compositional behavior.\n\nHopefully I have successfully conveyed my understanding of the way LOF and skill machines define avoidance behaviors, and please feel free to point out any gaps."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3429/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680165290,
                "cdate": 1700680165290,
                "tmdate": 1700680165290,
                "mdate": 1700680165290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]