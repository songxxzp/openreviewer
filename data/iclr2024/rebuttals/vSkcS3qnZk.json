[
    {
        "title": "Emergent Corpus Pretraining Benefits Vision Language Modeling"
    },
    {
        "review": {
            "id": "eIDXpXmpwL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_DU5f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_DU5f"
            ],
            "forum": "vSkcS3qnZk",
            "replyto": "vSkcS3qnZk",
            "content": {
                "summary": {
                    "value": "The paper explores the problem of generalization in vision-language modeling. To enhance the generalizability of vision-language models, the authors propose to use emergent communication (EC) between a listener and a speaker agent. Experiments showcase several potential benefits of EC for the visual entailment, visual question answering, and visual referring expression tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper includes the following strengths:\n\n- The proposed method is intuitive and interesting.\n\n- The methodology is well-written and understandable with accurate notations and coherent explanations.\n\n- The experiment section lucidly states the details regarding the settings and results."
                },
                "weaknesses": {
                    "value": "The paper contains some minor but serious weaknesses:\n\n- The motivation for EC is not convincing. For example, in the introduction, the author declares that EC is a promising approach without providing any evidence / explaining why this is promising. Moreover, even though self-supervised learning (SSL) is a well-known solution to tackle the limit of labeled data, the introduction lacks the the discussion towards SSL.  \n\n- The intuition of emergent language is not evident. Why do some unintelligible tokens, e.g. in Figure 2, can benefit the vision-language models, which primarily work with natural language? The paper does not provide an intuitive discussion towards this aspect.\n\n- The experiments are also not comprehensively conducted. There is not an ablation study to investigate the effect of each component and also an analysis for better understanding the EC framework, e.g. why the authors choose OFA as the base model for EC?"
                },
                "questions": {
                    "value": "- In the intuitive sense, why does emergent language can benefit vision-language modeling, which is mainly about natural language?\n\n- Why do you choose OFA as the base model? Does EC perform effectively with other models, such as UniVL, CLIP, etc.?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8721/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8721/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8721/Reviewer_DU5f"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697270837514,
            "cdate": 1697270837514,
            "tmdate": 1699637093454,
            "mdate": 1699637093454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "x29spc610S",
            "forum": "vSkcS3qnZk",
            "replyto": "vSkcS3qnZk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_PktS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_PktS"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a methodology for pre-training Vision Language models on images paired with emergent communication (EC) strings prior to fine-tuning on downstream tasks. The EC strings are derived from a speaker model trained for an image reference game task in an emergent communication paradigm (where the speaker and listener agents must converge on a communication protocol). \n\nThe main claim of the paper is that pre-training under this paradigm can provide useful inductive biases during pre-training that can help improve performance when fine-tuning on downstream tasks. \n\nAs a testbed, the paper uses the architecture of the One For All (OFA) VLM model from Wang et al. 2022, evaluated on three downstream vision-language tasks: Visual Referring Expression (VRE, where the model must generate a bounding box for an object in being referred to by a natural language expression), Visual Question Answering (VQA), and Visual Entailment (VE, where an agent must classify a natural language string as being (1) entailed, (2) contradicted, or (3) neither in relation to a given image). \n\nThe core of the experimental results compare three cases to each other: \n\n(Base) A pre-trained OFA model that is not fine-tuned on downstream tasks. \n(+EC) A pre-trained OFA model, further pre-trained on a corpus of EC token/image pairs, fine-tuned on downstream tasks. \n(+NL) A pre-trained OFA model, further pre-trained on a corpus of natural language / image pairs, fine-tuned on downstream tasks. \n\nOn VRE and VE, the +EC model improves over the baseline while achieving lower (or comparable in some cases with more fine-tuning data in the VE task) performance than +NL. In VQA the +EC model is outperformed by the other two variants."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* I find the idea of pre-training on emergent communication strings to be compelling, and the paper motivates potential reasons to expect this benefit well (in particular, the idea that the structural properties of a learned EC protocol could yield useful learning signal is intuitive). \n\n* To my knowledge, the proposed methodology of pre-training a VLM model on EC data and the experiments evaluating this are novel. \n\n* The paper is well written, generally clear, and easy to follow.\n\n* The experimental results show promise for the method (however, I have reservations about whether the claims are fully supported by the results, which I've listed under weaknesses)."
                },
                "weaknesses": {
                    "value": "My main concern, and the main reason for my ratings, is related to the experimental setup. I am concerned that the presented results do not fully support the conclusions of the paper: \n\nTo my understanding the paper argues that EC pre-training may yield benefits for VLM model performance in cases where there may otherwise not be more data containing natural language / image pairs to train on. \n\nWith this in mind, I believe the paper could be much stronger with comparison against the following experimental conditions (in addition to the Base, +EC, and +NL conditions already presented): \n\n(1) A Base model that is also fine-tuned on downstream task data, but *not* additionally pre-trained on either EC nor NL data. This would simulate the case where one only has access to (a) the original pre-training data, and (b) the downstream fine-tuning data, and could potentially improve model performance by additionally pre-training on EC data. Without this comparison, I do not believe that it is clear from the presented results if the improved performance of +EC over Base is due to the EC data itself or if it's due to the downstream fine-tuning. \n\n(2) Downstream task performance of a model pre-trained on the original pre-training data as well as EC data, but *not* fine-tuned on downstream tasks. This would simulate the case where one only has the original pre-training dataset and an evaluation set for the downstream task and could potentially improve performance by augmenting pre-training with EC data."
                },
                "questions": {
                    "value": "1. My main question is if there are results available for either of the two conditions I described in Weaknesses? The lack of these results is the main factor affecting my rating. \n\n2. I'm wondering if the token vocabulary used for images is the same between the OFA model and the EC models? Or do they each learn their own independent tokenizer?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797023031,
            "cdate": 1698797023031,
            "tmdate": 1699637093311,
            "mdate": 1699637093311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "iZpQXyJJ6J",
            "forum": "vSkcS3qnZk",
            "replyto": "vSkcS3qnZk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_2A8R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8721/Reviewer_2A8R"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the use of Emergent Communication (EC) for knowledge transfer in the Vision Language Pretrained Models. It pretrains a model on an EC corpus, then experiments on three tasks, including Visual Referring Expression (VRE) and Visual Entailment (VE). Empirical experiments show that pretrained on EC corpus can improve the performance on the downstream tasks, and highlight the transferability and generalization capabilities of EC pretraining on VL domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Empirical results show the VL models pretrained on EC corpus can be transferred to downstream tasks with improved gain in controlled settings."
                },
                "weaknesses": {
                    "value": "1. Experiment part is pretty weak, lacks of baselines for comparison, especially strong baselines, to verify the effectiveness of the proposed approach.\n2. The novelty of the method / approach is also a big concern."
                },
                "questions": {
                    "value": "The major issue of this work is experiment part is too weak, needs more baselines for comparison to show the effectiveness of the approach, given the main approach also lacks of novelty."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699574167732,
            "cdate": 1699574167732,
            "tmdate": 1699637093188,
            "mdate": 1699637093188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]