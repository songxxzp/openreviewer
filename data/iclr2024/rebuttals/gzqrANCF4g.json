[
    {
        "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation"
    },
    {
        "review": {
            "id": "pVbccFjRLl",
            "forum": "gzqrANCF4g",
            "replyto": "gzqrANCF4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel visual tokenizer designed to enhance large language models in producing high-quality images and videos. Experimental results show that, when integrated with the proposed tokenizer, LLM surpasses diffusion models in standard benchmarks such as ImageNet, UCF-101, and Kinetics-600. Additionally, the paper presents promising results in video compression and representation learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis paper presents the first evidence of large language models surpassing diffusion models on the ImageNet benchmark.\n2.\tThe paper proposes a novel lookup-free quantization approach, providing a promising direction for expanding vocabulary size in LLM-based visual generation.\n3.\tThe motivation is clear, and the overall presentation is coherent and easy to follow.\n4.\tGood results on visual generation video compression, and video representation learning."
                },
                "weaknesses": {
                    "value": "1.\tWhile the presented method is tailored for masked LM, many of the prevailing and powerful LLMs, such as LLaMA [A], employ an autoregressive approach. Incorporating results from AR-LM would greatly enhance the paper's relevance to the community.\n2.\tIn Table 4, despite the good action recognition performance showcased by the proposed method, it doesn't conclusively establish its efficacy as a viable self-supervised pre-training target. Notably, some pivotal baselines, like pixel colors and the image descriptor from MaskFeat [B], are missing.\n\n[A] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n\n[B] Wei, C., Fan, H., Xie, S., Wu, C. Y., Yuille, A., & Feichtenhofer, C. Masked feature prediction for self-supervised visual pre-training. In CVPR 2022."
                },
                "questions": {
                    "value": "1.\tIn Figure 1, it's highlighted that the VQ generation FID sees a pivotal change at a vocabulary size of 2^14, while the LFG generation FID consistently improves. I'm curious to understand how the LFG generation FID would respond to even larger vocabulary sizes.\n2.\tRegarding Table 3, what could be the reason behind the proposed method's PSNR and MS-SSIM values being inferior to those of the standard video codec?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Reviewer_k12R"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638358535,
            "cdate": 1698638358535,
            "tmdate": 1699636771481,
            "mdate": 1699636771481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mchIgcNW9b",
                "forum": "gzqrANCF4g",
                "replyto": "pVbccFjRLl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k12R"
                    },
                    "comment": {
                        "value": "We appreciate the thoughtful comments from reviewer k12R and the acknowledgment that `with the proposed tokenizer, LLM surpasses diffusion models in standard benchmarks`, and the tokenizer also shows `promising results in video compression and representation learning.`  \n\n> W1: Incorporating results from AR-LM.\n\nIn the Appendix, we have added experiments on the autoregressive language model (AR-LM) where the proposed tokenizer outperforms the prior work MAGVIT [R3-A]. The results, also provided in Table R3.1 below, show our tokenizer can also work with the AR-LM.\n\nIt is worth noting that we have observed that achieving optimal performance requires a much larger AR-LM model size. But due to time constraints, we have not been able to train a larger AR-LM model with an adequate number of training steps. That also partly explains our current choice of not building our work on the autoregressive language model, and we leave it as future work.\n\n*Table R3.1: Video generation results: class-conditional generation on UCF-101 with AR-LM models. We use the same transformer configuration as MLM experiments but without vocabulary factorization and weight tying. As a result, the AR-LM with our tokenizer uses more parameters in the embedding table and the softmax layer.*\n\n| Tokenizer | FVD\u2193 | # Params | # Steps |\n|---|:---:|:---:|:---:|\n| MAGVIT | 265 | 306M | 1024 |\n| *This paper* | **109** | 840M | 1280 |\n\n> > [R3-A] [MAGVIT: Masked Generative Video Transformer. In CVPR 2023.](http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.html)\n\n> W2: Efficacy as a viable self-supervised pre-training target compared to baselines like pixel colors and image descriptors.\n\nFollowing the reviewer\u2019s suggestion, we have added pixel colors and HoG descriptors from MaskFeat [B] as baseline self-supervised pre-training targets into revised Table 4, with a distilled version in Table R3.2 below. As shown, pre-training with tokens from our model as the target achieves the best performance compared to the previous leading tokenizer MAGVIT as well as other baselines, including raw pixels, 3D VQ-VAE, and HoG descriptors. \n\n*Table R3.2 Video action recognition performance (classification accuracy\u2191 $\\times$100) with different self-supervised pre-training targets as the transformer output.*\n\n| Target | SSv2 |\n|---|:---:|\n| 3D VQ-VAE | 64.13 |\n| MAGVIT | 67.22 |\n| *This paper* | **67.38** |\n| Raw pixel | 64.83 |\n| HoG descriptor | 65.86 |\n\n> Q1: Generation performance with LFQ at larger vocabularies?\n\nIn Table R3.3, we showcase an instantiation of LFQ with a much larger vocabulary size at $2^{40}$. As shown, it yields consistent improvement over $2^{18}$ in both reconstruction and generation FID metrics.\n\n*Table R3.3 Class-conditional image generation results on ImageNet 512$\\times$512 with larger LFQ vocabularies. We use an MLM model and 12 decoding steps.*\n\n| Vocabulary | Groups | # Params | rFID\u2193 | FID\u2193 |\n|:---:|:---:|:---:|:---:|:---:|\n| $2^{18}$ | 2 | 307M | 1.22 | 5.93 |\n| $2^{40}$ | 4 | 312M | **0.80** | **5.15** |\n\n\n> Q2: PSNR and MS-SSIM compared to standard video codec.\n\nOur model uses GAN and perceptual losses to improve the reconstruction quality especially in terms of the realism of the generated video. At a similarly low bit rate, standard codecs based on block-level Fourier-related transformations may preserve better local details but introduce inter-block artifacts. As pointed out by [R3-B], traditional distortion metrics such as PSNR and MS-SSIM, \u201cfor very low bitrates \u2026 these distortion metrics **lose significance** as they favor pixel-wise preservation of local structure over preserving texture and global structure.\u201d [R3-B]\nAs a result, while standard codecs might appear to outperform on metrics like PSNR and MS-SSIM, they fall short in the gold-standard human rater study (see Figure 6). We have revised Section 4.3 to clarify this point.\n\n> > [R3-B] [Generative adversarial networks for extreme learned image compression](http://openaccess.thecvf.com/content_ICCV_2019/html/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.html)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711077708,
                "cdate": 1700711077708,
                "tmdate": 1700711077708,
                "mdate": 1700711077708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XeMEGPcs1v",
            "forum": "gzqrANCF4g",
            "replyto": "gzqrANCF4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on learning a video / image tokenizer to discretize video / images so that they can be modeled using a Language Model. They specifically introduce one innovation in this setting: A lookup free quantizer. They show that in this limit of using a large vocabulary and no lookup, the tokenizer reconstruction and LM generation quality both increase with vocabulary size. The authors also show that the learned tokenizer performs very well as a compression scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper does a good job of motivating the core thesis of the paper: How to design a tokenizer for image / video? The authors also do a good job of presenting this idea to the uninitiated. It is also pretty clear that the work is significant to academia and industry given that it helps unify image generation with image understanding and natural language generation and understanding techniques. The ideas in the paper are well explained. The authors also do a thorough job of running experiments to substantiate many claims including numerous ablations. Some of the important technical insights like the lookup free quantizer (and in general lower dimensional code words) helping in generation quality are substantiated by experimental results"
                },
                "weaknesses": {
                    "value": "My main concern is the completeness of the exposition in the paper. The authors assume that the reader is familiar with the state of the art in video tokenization and details do get rather buried in the many \u201cdeltas\u201d relative to the baseline. I do understand the space limitations but it might be helpful if the authors try to make the core system / model design more explicit. Lot of the ideas like factorization of the output space in the decoder (and associated weight tying) for example are just mentioned in passing."
                },
                "questions": {
                    "value": "* The question \u201cWhy masked LM and not AR LM for image / video generation?\u201d for evaluating the tokenizer was not clearly answered.\n* No explicit definition of the objective for training the tokenizer (loss function)\n* No mention of decoder in VQ-VAE and VQ-VAE loss used when we use no lookup\n* More motivation needed on why the authors choose to use a causal encoder for tokenizer when doing masked LM for image / video generation\n* It\u2019s not clear why the authors tackle video generation if the aim was to understand the fundamentals of tokenization. It may be desirable for them to clearly motivate why they study videos and not images alone?\n* It may be interesting for the reader to understand the computational complexity of both the tokenizer (encoder) and the detokenizer (decoder) and how they compare with video or audio codecs"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6714/Reviewer_sfJe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783954186,
            "cdate": 1698783954186,
            "tmdate": 1699636771350,
            "mdate": 1699636771350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sDmpOnILt5",
                "forum": "gzqrANCF4g",
                "replyto": "XeMEGPcs1v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sfJe"
                    },
                    "comment": {
                        "value": "We appreciate the constructive comments from reviewer sfJe and the positive assessment. We provide responses to each individual question below.\n\n> W1: the completeness of exposition in the paper; assuming the reader is familiar with the state of the art in video tokenization.\n\nThank you for your understanding regarding the space limitations. To enhance the method's readability and self-containment, we have implemented the following changes. First, we have added a paragraph in Section 3.1, detailing all the training losses, which includes those from prior works. Second, we have added a section to the Appendix, which provides additional details on model design and hyperparameters. Third, a paragraph has been included in Section 3.2 to clarify the weight tying approach in the factorized prediction. We hope these can improve the completeness of the exposition, and we are open to additional recommendations from the reviewer.\n\n> Q1: Why masked LM and not AR LM for image / video generation?\n\nWe selected the masked language model (MLM) due to its competitive performance on benchmark datasets [R2-A, R2-B] compared to the autoregressive language model (AR-LM), where MLM also uses much fewer decoding steps than AR-LM. In Table R2.1, we also show experiments on the AR-LM where the proposed tokenizer outperforms the prior work MAGVIT [R2-A]. The results show our tokenizer can also work with the AR-LM, but the AR-LM lags behind the state-of-the-art. We have included this rationale in the revised Section 4.2 and added the new result to the Appendix.\n\nIt is worth noting that we have observed that achieving optimal performance requires a much larger AR-LM model size. But due to time constraints, we have not been able to train a larger AR-LM model with an adequate number of training steps. That also partly explains our current choice not to build our work on the autoregressive language model. We are investigating the integration of the tokenizer with the large AR-LM and will study this in our future research.\n\n*Table R2.1: Video generation results: class-conditional generation on UCF-101 with AR-LM models. We use the same transformer configuration as MLM experiments but without vocabulary factorization and weight tying. As a result, the AR-LM with our tokenizer uses more parameters in the embedding table and the softmax layer.*\n\n| Tokenizer | FVD\u2193 | # Params | # Steps |\n|---|:---:|:---:|:---:|\n| MAGVIT | 265 | 306M | 1024 |\n| *This paper* | **109** | 840M | 1280 |\n\n> > [R2-A] [MAGVIT: Masked Generative Video Transformer. In CVPR 2023.](http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.html)  \n> > [R2-B] [Discrete Predictor-Corrector Diffusion Models for Image Synthesis. In ICLR 2023.](https://openreview.net/forum?id=VM8batVBWvg)\n\n\n> Q2: No explicit definition of the objective for training the tokenizer (loss function).\n\nFollowing the reviewer's recommendation, we have added a description of the tokenizer's training objective at the end of Section 3.1, detailing all the training losses including those from prior works. In addition to the entropy penalty in Equation 5, the overall training objective involves the standard combination of reconstruction, GAN, perceptual, and commitment losses from VQGAN [R2-C], excluding the codebook loss. In addition, we follow [R2-A] in using LeCAM regularization [R2-D] for improved stability. To provide better clarity on the details, we have added Appendix A.2 listing all relevant hyperparameters.\n\n> > [R2-C] [Taming Transformers for High-Resolution Image Synthesis. In CVPR 2021.](https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html)  \n> > [R2-D] [Regularizing Generative Adversarial Networks under Limited Data. In CVPR 2021.](http://openaccess.thecvf.com/content/CVPR2021/html/Tseng_Regularizing_Generative_Adversarial_Networks_Under_Limited_Data_CVPR_2021_paper.html)\n\n> Q3 No mention of decoder in VQ-VAE and VQ-VAE loss used when we use no lookup.\n\nInitially, the decoder was only briefly mentioned in Section 2 and in the caption of Figure 2. We have improved the clarity by elaborating on the additional details on model design and hyperparameters in Appendix A.2 with an architecture diagram in Figure 7, as well as by providing a discussion of the loss functions in Section 3.1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711097431,
                "cdate": 1700711097431,
                "tmdate": 1700711676758,
                "mdate": 1700711676758,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6Q12UZEOwW",
            "forum": "gzqrANCF4g",
            "replyto": "gzqrANCF4g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_rVyC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6714/Reviewer_rVyC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel visual tokenizer based on lookup-free quantization (LFQ). With the growth of the vocabulary size LFQ consistently improve both reconstruction and generation quality, which is in stark contrast with Vector Quantization (VQ) where an increased vocabulary size reduces reconstruction error but hurts generation results. The tokenizer can be integrated with MAGVIT and achieves state-of-the-art performance on video generation. The tokenizer can also improve video compression and video recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Starting from an interesting finding that enlarging the vocabulary improves reconstruction quality but hurts generation results, the paper proposes solutions (LFQ) to tame both reconstruction and generation simultaneously.\n\n2. A detailed study of architecture modifications that improves up MAGVIT supported by extensive ablations. \n\n3. The tokenizer is proved to benefit video generation, compression and recognition. It will potentially have a huge impact on the general audience of video understanding."
                },
                "weaknesses": {
                    "value": "1. For video compression results, it would be better if there is a PSNR/LPIPS/MS-SSIM-bpp curve comparing the performance across different bpps.\n\n2. In the video recognition setup, it seems unnecessary to detokenize the visual tokens back to pixels since BEVT and BEIT can work with tokenized input. I understand one of the main reasons is that the underlying recognition model is the ViViT which takes raw pixels as input (as stated in the draft). However, you may also have a comparison with BEVT."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699078850701,
            "cdate": 1699078850701,
            "tmdate": 1699636771173,
            "mdate": 1699636771173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U1wDppM9bM",
                "forum": "gzqrANCF4g",
                "replyto": "6Q12UZEOwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rVyC"
                    },
                    "comment": {
                        "value": "We appreciate the valuable feedback from Review rVyC and the acknowledgment of our method that `is proved to benefit video generation, compression and recognition` and `potentially has a huge impact on the general audience of video understanding`. Please see our response below.\n\n> W1: Video compression metric curves.\n\nFollowing the suggestion, we have added Figure 9 in the Appendix showing the curves of compression metrics, including LPIPS, PSNR, and MS-SSIM at various bpp levels. The results are consistent with the findings from Table 3 that `our model outperforms MAGVIT on all metrics and outperforms all methods on LPIPS`. It is worth noting that LPIPS may `correlate more closely with subjective assessments`, which we have shown in the human rater study in Figure 6, than PSNR or MS-SSIM, according to [R1-A].\n\n> > [R1-A] [Generative adversarial networks for extreme learned image compression](http://openaccess.thecvf.com/content_ICCV_2019/html/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.html)\n\n\n> W2: Necessity of detokenization in video recognition.\n\nWe appreciate your understanding that ViViT `takes raw pixels as input`. The detokenization experiments used the **frozen** ViViT model that was trained on RGB pixels. For proper inference, the detokenizer is needed to convert the tokens back into pixel inputs, aligning with the input requirements of the frozen ViViT model. As shown in Table 4, our model shows superior performance compared to MAGVIT [R1-B] and is very close to raw pixels while only using ~1/500 data bandwidth. This setting was mentioned in Appendix A.3 of the original submission. And we have revised Section 4.4 to clarify this.\n\nFollowing the reviewer\u2019s suggestion, we also train a BEVT model with tokens as both the input and the output for action recognition. The result is shown in Table R1.1. We find that directly using token inputs yields suboptimal performance in part because the current architecture and pre-training setups are tailored for pixel inputs. Further research is needed to investigate this token-in-token-out pretraining paradigm for action recognition, where both the input and output are based on discrete tokens.\n\n*Table R1.1: Video action recognition performance (classification accuracy\u2191 $\\times$100) on SSv2 dataset.*\n\n| Tokenizer | Input+Output | Output only |\n|---|:---:|:---:|\n| MAGVIT | 60.86 | 67.22 |\n| *This paper* | **60.99** | **67.38** |\n\n> > [R1-B] [MAGVIT: Masked Generative Video Transformer. In CVPR 2023.](http://openaccess.thecvf.com/content/CVPR2023/html/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.html)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711054342,
                "cdate": 1700711054342,
                "tmdate": 1700711054342,
                "mdate": 1700711054342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]