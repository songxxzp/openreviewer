[
    {
        "title": "Training Diffusion Models with Reinforcement Learning"
    },
    {
        "review": {
            "id": "tXmkJsrWRj",
            "forum": "YCWjhGrJFD",
            "replyto": "YCWjhGrJFD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_rYFt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_rYFt"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a policy gradient algorithm to finetune text-to-image diffusion models for downstream tasks using only a reward function. Specifically, they reframe the denoising process as a multi-step MDP and introduce two variants of denosing diffusion policy optimization. In addition, they validate the proposed method on different downstream tasks, such as aesthetics and prompt-image alignment. In particular, they propose a automated VLM-based reward function for prompt-image alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The proposed method is effective and validated with various experiments. Besides, this paper is simple and easy to implement. \n+ The proposed automated prompt alignment is somewhat practical and provide an alternative for prompt-image alignment."
                },
                "weaknesses": {
                    "value": "- The contribution of the proposed method is minor, though it is effective on different downstream tasks. Compared with DPOK, the modification of policy optimization is somewhat incremental. In the related work, the authors claims that training on many prompts is one of two key advantages over DPOK. But, DPOK also can train with multiple prompts and provides quantitative results for that experiment. It is better to conduct this experiment with the same setting to support this point. Besides, the authors only show that the proposed method outperforms simple RWR methods. It is essential to compare with previous works. For example, show different methods\u2019 ability to handle prompts involving color, composition, counting and location. \n-  As shown in Appendix B, there is a subtle bug in the implementation. This bug maybe affect the quantitative comparisons and lower the support for the effectiveness of the proposed method. It is one of my major concerns about this paper. \n- In the experiment related to aesthetic quality, the finetuned model produce images with fixed style, as shown in Figure 3 and Figure 6. Is it only attributed to the bug in the implementation? Based on this phenomenon, I raise a concern: does the proposed method compromise the diversity of text-to-image diffusion models?"
                },
                "questions": {
                    "value": "- In the related work, \u2018Reward-Weighted Regression\u2019 should be used before using its abbreviation (RWR). If not, it maybe a misleading phrase for the readers, especially ones without context."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Reviewer_rYFt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753524675,
            "cdate": 1698753524675,
            "tmdate": 1700636344163,
            "mdate": 1700636344163,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ulqc3jhOma",
                "forum": "YCWjhGrJFD",
                "replyto": "tXmkJsrWRj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal + new experiments"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review. It seems that your primary concerns are novelty with respect to DPOK and the subtle bug affecting the aesthetic score. Regarding the bug, we have re-run the primary quantitative comparison with the fixed reward function. Regarding DPOK, as we explain in the top-level rebuttal, our paper was posted to arXiv 3 days before DPOK, and as such the work should be considered concurrent.\n\nThat said, we have added an experiment directly comparing our method to DPOK in Appendix C of the updated PDF ([screenshot of quantitative results;](https://ibb.co/Jc0LmS0) [screenshot of qualitative results](https://ibb.co/z4shhBd)). Our method significantly outperforms the numbers reported by DPOK on the same task, even without KL regularization.\n\n> **As shown in Appendix B, there is a subtle bug in the implementation. This bug maybe affect the quantitative comparisons and lower the support for the effectiveness of the proposed method. It is one of my major concerns about this paper.**\n\nApologies for any confusion, but this bug only affected the leftmost chart in Figure 4 (primary comparison) and the leftmost chart in the RWR interleaving ablation in the appendix. No other results, qualitative or quantitative, were affected. While the affected results still represented a fair scientific comparison, we decided to mention the bug in the appendix in the interest of full transparency.\n\nWe have re-run the primary comparison in Figure 4 and updated the submission. Unfortunately, it is infeasible to re-run the extra interleaving baseline/ablation in the appendix due to compute limitations. However, the leftmost chart still provides a fair comparison, since all methods were tested on the same reward function, and the rightmost chart supports the same conclusion while being unaffected by the bug.\n\n> **does the proposed method compromise the diversity of text-to-image diffusion models**\n\nYes, we do notice convergence on a similar style for the aesthetic quality experiments. A similar phenomenon has been observed with RLHF for language models ([Kirk et. al., 2023](https://arxiv.org/abs/2310.06452)). While reduction of diversity is a concern, we do not believe that this is fatal to the practical applicability or intellectual contribution of our method, and could be alleviated in future work. We will be sure to add a discussion of sample diversity to the limitations section for the camera-ready.\n\n> **In the related work, \u2018Reward-Weighted Regression\u2019 should be used before using its abbreviation (RWR).**\n\nThanks for the catch! We have fixed this in the updated submission."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287511663,
                "cdate": 1700287511663,
                "tmdate": 1700529884635,
                "mdate": 1700529884635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZHKlRiZox8",
                "forum": "YCWjhGrJFD",
                "replyto": "tXmkJsrWRj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi reviewer rYFt,\n\nWe hope we have addressed all of your stated weaknesses, as well as answered your questions about the paper. Most notably, we have clarified that DPOK is concurrent work and thus the DDPO algorithm should be considered an original intellectual contribution. We have also fixed the issue with the aesthetic predictor, which was one of your major concerns.\n\nAs such, we would greatly appreciate it if you could take a look at our rebuttal and let us know if you still think our paper is below the acceptance threshold for ICLR.\n\nRegards,\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529795026,
                "cdate": 1700529795026,
                "tmdate": 1700530224820,
                "mdate": 1700530224820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5fAIBdMCoX",
                "forum": "YCWjhGrJFD",
                "replyto": "ulqc3jhOma",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_rYFt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_rYFt"
                ],
                "content": {
                    "comment": {
                        "value": "I have read all reviews and authors\u2019 rebuttal. The authors resolve my concerns. Thus, I raise my score to 6."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636331823,
                "cdate": 1700636331823,
                "tmdate": 1700636331823,
                "mdate": 1700636331823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wlw0IfLk6E",
            "forum": "YCWjhGrJFD",
            "replyto": "YCWjhGrJFD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method that poses the diffusion process as an MDP and applies reinforcement learning to text-to-image diffusion models. It uses different reward functions, such as image compressibility, prompt alignment and image quality. The authors formulate three different approaches to fine-tune image diffusion models, two involving multi-step reinforcement learning and one that re-weights the original inverse diffusion objective. The authors evaluate their method on relevant datasets and discuss relevant shortcomings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the paper proposes a novel work (other work mentioned in related works section is concurrent work as it will only be published at Neurips in December)\n- the approach is simple yet effective\n- a variety of reward functions are explored and all yield visually pleasing results"
                },
                "weaknesses": {
                    "value": "- The proposed method does not consider the problem of overoptimisation, instead the authors argue that early stopping from visual inspection is sufficient. This makes the method applicable to problems where visual inspection is possible (which is likely the case for many image tasks). However, it renders the method inapplicable to problems where the needed visual inspection is not possible. (E.g. one might not be able to apply this method to a medical imaging task where visual inspection by a human supervisor does not allow to determine when to stop the optimisation process).   \n- Further, the reliance on visual inspection negatively impacts scalability of the method.  \n- The evaluation is missing a simple classifier guidance baseline, where samples are generated from the diffusion model with classifier guidance according to the given reward function. (Classifier guidance is mentioned in the related work but not evaluated as a baseline)  \n - The hypothesis that cartoon-like samples are more likely after fine-tuning because no photorealistic images exist for the given prompt category could be verified by fine-tuning on categories for which photo-realistic images do exist .\n- the generalization study in 6.3. is purely qualitative, a quantitative analysis would be more adequate\n- similarly, overoptimization could be quantitatively analysed"
                },
                "questions": {
                    "value": "- I am wondering why the lacking quantitative experiments have not been conducted (or why they cannot be conducted)\n- Similarly, I am wondering how the authors think about the early stopping mechanism"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773557931,
            "cdate": 1698773557931,
            "tmdate": 1700597545501,
            "mdate": 1700597545501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A1zKR9JGQj",
                "forum": "YCWjhGrJFD",
                "replyto": "wlw0IfLk6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal + new experiments"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review. It seems that your primary concern is about overoptimization. It\u2019s worth noting that we found it reasonably straightforward to run our method for a fixed number of iterations (e.g., 15k reward queries) and recover good generations; we also did not observe completely degenerated images in the VLM or aesthetic quality experiments, only in compressibility and incompressibility (when run for long enough). We have added an additional quantitative experiment that we hope addresses your concern, which we discuss more below.\n\nWe would also like to point out that overoptimization is an open research problem that is well outside the scope of this paper. While we agree that it is certainly a major issue that hinders many real-life applications (such as the provided medical imaging example), the issue of overoptimization also exists in language modeling to an equivalent degree ([Gao et. al., 2022](https://arxiv.org/abs/2210.10760)), yet it has certainly not prevented RLHF from having a huge impact in that domain ([Ouyang et. al., 2022](https://arxiv.org/abs/2203.02155); [Bai et. al., 2022](https://arxiv.org/abs/2212.08073)). Current techniques for combating overoptimization, such as KL-regularization or automated evaluation, do not completely eliminate the need for human evaluation in real-life systems.\n\n> **the generalization study in 6.3. is purely qualitative, a quantitative analysis would be more adequate**\n\nA quantitative analysis of generalization was already provided in Appendix D, which is referenced in the caption of Figure 6.\n\n> **overoptimization could be quantitatively analysed**\n\nWe have provided a quantitative analysis of overoptimization in Appendix C of the updated submission ([screenshot](https://ibb.co/Jc0LmS0)). We find that both the optimized metric (ImageReward) and held-out metric (Aesthetic Quality) increase for quite some time, during which the model produces high-quality generations ([screenshot](https://ibb.co/z4shhBd)). Overoptimization \u2014 reflected by a decrease in the held-out metric \u2014 does begin to occur for one prompt near the end of training, but even then it is not fatal. Such held-out metrics could also be used to automatically decide a stopping point.\n\n> **The evaluation is missing a simple classifier guidance baseline**\n\nWe have added a comparison to classifier guidance in Appendix B of the updated submission ([screenshot](https://ibb.co/dfJbVTw)). We note that only the aesthetic predictor reward function is differentiable, meaning it is the only one where classifier guidance is applicable. See the top-level rebuttal for more information."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287347700,
                "cdate": 1700287347700,
                "tmdate": 1700287347700,
                "mdate": 1700287347700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JlOxSRM5Lx",
                "forum": "YCWjhGrJFD",
                "replyto": "wlw0IfLk6E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi reviewer EgHv,\n\nWe hope we have addressed most of your weaknesses, as well as answered your questions about our paper. To be specific, we have added the requested quantitative evaluation of overoptimization and a new classifier guidance baseline, as well as provided some clarification about overoptimization, which seemed to be your main concern. As such, we would greatly appreciate it if you could take a look at our rebuttal and let us know if you still think our paper is below the acceptance threshold for ICLR.\n\nThanks,\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529445737,
                "cdate": 1700529445737,
                "tmdate": 1700529946023,
                "mdate": 1700529946023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W0mwW8vy0Y",
                "forum": "YCWjhGrJFD",
                "replyto": "JlOxSRM5Lx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for your thorough rebuttal, which addressed most of my concerns and answered most of my questions.\n\nI am still curious about testing the hypothesis that cartoon-like samples are more likely after fine-tuning because no photorealistic images exist for the given prompt category by fine-tuning on categories for which photo-realistic images do exist. Whats your take on this?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597520177,
                "cdate": 1700597520177,
                "tmdate": 1700597520177,
                "mdate": 1700597520177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SzZaTRcIrV",
                "forum": "YCWjhGrJFD",
                "replyto": "W0mwW8vy0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "content": {
                    "comment": {
                        "value": "I agree with the authors that DPOK is concurrent work. Hence it should not be used for assessing this paper in any way."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598430066,
                "cdate": 1700598430066,
                "tmdate": 1700598430066,
                "mdate": 1700598430066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "du3Gr7xVoc",
                "forum": "YCWjhGrJFD",
                "replyto": "wjeBKbL7IC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_EgHv"
                ],
                "content": {
                    "comment": {
                        "value": "Hi, thanks for the response. In all fairness, I wanted to point out that this is not a new request that I just brought up a day before the rebuttal deadline, but that I had written exactly this in my initial review of the paper."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643391863,
                "cdate": 1700643391863,
                "tmdate": 1700643391863,
                "mdate": 1700643391863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BEujamuBbq",
            "forum": "YCWjhGrJFD",
            "replyto": "YCWjhGrJFD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_sWSo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_sWSo"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Denoising Diffusion Policy Optimization (DDPO), where they fine-tune diffusion models by reinforcement learning (RL). In specific, given the scalar reward function that takes an image and conditions (e.g., text prompts), DDPO considers the reverse generative process as Markovian Decision Process (MDP), where the reward is only given at the zeroth timestep. The authors implement DDPO via policy gradient (i.e., REINFORCE) or together with importance sampling (i.e., PPO-like approach). Through experiments they show that DDPO can be used in improving aesthetic quality, image-text alignment, and compression (or incompression) of an image, and verify its effectiveness in comparison to reward augmented fine-tuning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is clearly written and easy to follow.\n- The problem of solving diffusion generative processes as solving MDP is clearly stated, and the proposed method generalizes prior works (i.e., reward-weighted regression (RWR)) for the multi-step MDP case.\n- The experiments clearly validate the efficiency of DDPO over prior diffusion model tuning with reward functions as well as detailed algorithmic choices are provided."
                },
                "weaknesses": {
                    "value": "- After RL fine-tuning, the generated images seem to be saturated. For example, the fine-tuned models generate images with high aesthetic scores, but they seem to generate images with similar backgrounds of sunset. For prompt alignment experiments, the models generate cartoon-like images. \n- I think one of the main contributions of the paper is on utilizing VLMs for optimizing text-to-image diffusion models. In this context, the discussion on the choice of reward function should be discussed more in detail. For example, I think different instruction prompts for the LLaVA model would make different results, but the paper lacks details in the choice of reward function."
                },
                "questions": {
                    "value": "- Regarding the capability of DDPO, is it possible to control the image generation to have predicted reward? For example, Classifier-Free guidance provides control over sample fidelity and diversity. It seems like DDPO improves prompt fidelity at the expense of sample diversity, so I wonder if there is a way to control the amount of reward during generation. \n- It seems like the reward function for different tasks have different ranges, and thus DDPO uses running mean and variance for normalization. I guess the difficulty of solving MDP varies across different prompts. Could the author elaborate on the optimization analysis with respect to the difficulty of prompts? \n- In implementation, the author chooses T=50 throughout experiments. What if the timesteps become smaller or larger? Definitely the denser timestep sampling would require more cost, but I believe it can reduce the variance of gradient. Could the author elaborate on this point? \n- Do the authors try different reward models for image-text alignment? How robust is DDPO algorithm with respect to different reward functions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Reviewer_sWSo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775672619,
            "cdate": 1698775672619,
            "tmdate": 1700533595538,
            "mdate": 1700533595538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QY80rgyYGk",
                "forum": "YCWjhGrJFD",
                "replyto": "BEujamuBbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal + new experiments"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. It seems that your primary concerns are the saturation of images during finetuning and the lack of discussion of VLM reward functions. Regarding the first point, we would like to point out that this is a well-known phenomenon that also affects RL finetuning of language models ([Kirk et. al., 2023](https://arxiv.org/abs/2310.06452)), yet it has certainly not prevented RLHF from having a huge impact in that domain. Regarding the second point, we would like to emphasize that the primary intellectual contribution of this paper is the algorithm itself, and we believe the thorough analysis of 4 reward functions in the main paper meets the bar for ICLR acceptance. That said, we have added an additional experiment that uses ImageReward in Appendix C ([screenshot](https://ibb.co/Jc0LmS0)).\n\n> **After RL fine-tuning, the generated images seem to be saturated**\n\nYes, we do notice convergence on a similar style for the aesthetic quality experiments. A similar phenomenon has been observed with RLHF for language models ([Kirk et. al., 2023](https://arxiv.org/abs/2310.06452)). While reduction of diversity is a concern, we do not believe that this is fatal to the practical applicability or intellectual contribution of our method, and could be alleviated in future work. We will be sure to add a discussion of sample diversity to the limitations sections for the camera-ready.\n\n> **the discussion on the choice of VLM reward function should be discussed more in detail**\n\nWe briefly discuss the choice of LLaVA prompt in the third paragraph of Section 5.3, where we mention that the design space of VLM reward functions is virtually unlimited and that we look forward to future work that explores this direction. In the camera-ready, we will elaborate on why we chose LLaVA (it was the best open-source VLM at the time) and the evaluation scheme that we did (we chose to stick with the simplest, most general prompt, but found that BERTScore was necessary to increase reward granularity).\n\n> **is it possible to control the image generation to have predicted reward?**\n\nThe most direct way would be early stopping \u2014 earlier checkpoints in the training process have lower reward, in exchange for being closer to the distribution of the pretrained model.\n\n> **Could the author elaborate on the optimization analysis with respect to the difficulty of prompts?**\n\nSome prompts may be naturally more difficult for the pretrained model than others. For example, the  model may easily generate \u201ca man riding a horse,\u201d but struggle to generate well-aligned images for \u201ca horse riding a man.\u201d Figure 5 provides a quantitative example of this: the \u201criding a bike\u201d prompts achieve the highest average reward, followed by \u201cplaying chess,\u201d and then \u201cwashing dishes.\u201d\n\n> **In implementation, the author chooses T=50 throughout experiments. What if the timesteps become smaller or larger?**\n\nUsing more generation timesteps would certainly improve policy gradient estimates, as well as improve the base generation quality of the model. 50 timesteps is a standard choice in the literature, and we found it to work well in our experiments, so we did not ablate this hyperparameter. We suspect that one would find diminishing returns if the number of timesteps was increased much beyond 50.\n\n> **Do the authors try different reward models for image-text alignment?**\n\nWe have included an additional experiment with ImageReward in Appendix C ([screenshot](https://ibb.co/Jc0LmS0)). This and LLaVA are the only two models for image-text alignment that we have personally tried."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287735980,
                "cdate": 1700287735980,
                "tmdate": 1700287735980,
                "mdate": 1700287735980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zoFyhy0lRk",
                "forum": "YCWjhGrJFD",
                "replyto": "BEujamuBbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi reviewer sWSo,\n\nWe hope we have addressed both of your stated weaknesses, as well as answered all of your questions. As such, we would greatly appreciate it if you could take a look at our rebuttal, letting us know if you find the responses adequate, and if so, if you would consider raising your score.\n\nThanks,\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529189163,
                "cdate": 1700529189163,
                "tmdate": 1700529189163,
                "mdate": 1700529189163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q5PXlqvuDr",
                "forum": "YCWjhGrJFD",
                "replyto": "zoFyhy0lRk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_sWSo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_sWSo"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your response, most of my concerns are addressed. I believe this is a primary work on fine-tuning the diffusion model with reward models (with concurrent work DPOK). However, as other reviewers also noted, DDPO might cause the saturation problem and the choice of reward function (e.g., LLaVA score) could be better elaborated. I am generally positive about accepting this paper. I update my score to 8."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533581830,
                "cdate": 1700533581830,
                "tmdate": 1700533581830,
                "mdate": 1700533581830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7QT1jjqoGe",
            "forum": "YCWjhGrJFD",
            "replyto": "YCWjhGrJFD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for fine-tuning large-scale diffusion-based text generation models using reinforcement learning. It views the reverse process of diffusion models as a reinforcement learning process with time steps T (T is the denoising steps), where the output of the diffusion model at each step serves both as an action and the observation for the next moment. The authors describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, referred to as denoising diffusion policy optimization. And the authors also employ an open-source image quality evaluator as the reward function and also devise an automated prompt-image alignment method using feedback from a vision-language model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is well-written, with clear logic and beautifully crafted figures, making it easy to follow the authors' line of reasoning. Additionally, the paper is well-structured, presenting an easy-to-follow approach to fine-tuning diffusion models using reinforcement learning for alignment. The narrative is straightforward, and the methods described are, in my opinion, sensible."
                },
                "weaknesses": {
                    "value": "1) In terms of image generation quality, the paper lacks a quantitative and qualitative comparison with recent works. It fails to provide experimental support for its effectiveness. Specifically, in the absence of comparisons in image quality with all methods related to \"Optimizing diffusion models using policy gradients,\" it is challenging to discern the improvements this paper offers over baseline approaches. This makes it difficult to evaluate the paper's contribution to the community.\n\n2) Regarding originality and community contribution, compared to DPOK and \"Optimizing ddpm sampling with shortcut fine-tuning,\" I did not observe significant differences between this paper and DPOK from the introduction to the Method section. The structural similarity in writing is quite evident. The most noticeable difference is the introduction of importance sampling in method 4.3. However, this alone does not sufficiently support the paper\u2019s claims of innovation, especially without experimental evidence backing its effectiveness (if such evidence exists, please inform me during the Rebuttal phase). The second noticeable difference is the automated generation of rewards using BLIP, which is already a standard engineering practice and has been claimed by RLAIF. I do not believe the methodological contributions and community impact of this paper meet the acceptance standards of the ICLR conference.\n\n3) The experimental evaluation criteria are unreasonable. Most pretrained reward evaluation models are not designed for robust data manifold, especially for treating it as a \"fine-tuning teacher.\" Consequently, out-of-domain evaluation is necessarily needed. For example, the authors use Aesthetic Quality as a reward function to fine-tune the Diffusion model and employ the same Aesthetic Quality for scoring during evaluation. This approach does not allow for assessing whether an improvement in the Aesthetic Quality Score correlates with an enhancement in image generation quality. A more reasonable evaluation, as exemplified by DPOK, would involve fine-tuning with one Reward model and then evaluating it against both this model and a new, out-of-domain Reward model. DPOK's evaluating approach might provide a more substantial basis for assessment.\n\n4) Lack of code. The lack of open-source code, compounded by the absence of comparisons with recent works, makes it difficult to assess the practical feasibility of the approach."
                },
                "questions": {
                    "value": "My main points have been outlined in the Strengths and Weaknesses sections. If the authors can provide satisfactory responses and additional experiments addressing these points, I would consider revising my review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8548/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698983420795,
            "cdate": 1698983420795,
            "tmdate": 1700314415609,
            "mdate": 1700314415609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bCKybLlHPY",
                "forum": "YCWjhGrJFD",
                "replyto": "7QT1jjqoGe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal + new experiments"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review. It seems the novelty of the contribution with respect to DPOK is your primary concern; as we explain in the top-level rebuttal, our paper was posted to arXiv 3 days before DPOK, and as such the work should be considered concurrent. The overall algorithm, narrative, and writing structure of our paper have not changed since that initial posting, and as such, our imitation of DPOK is impossible.\n\nThat said, we have added an experiment directly comparing our method to DPOK in Appendix C of the updated PDF ([screenshot of quantitative results;](https://ibb.co/Jc0LmS0) [screenshot of qualitative results](https://ibb.co/z4shhBd)). Our method significantly outperforms the numbers reported by DPOK on the same task, even without KL regularization.\n\n> **out-of-domain evaluation is necessarily needed**\n\nOur comparison to DPOK includes the out-of-domain evaluation you recommended; namely, training using ImageReward and evaluating using Aesthetic Score. We find that both metrics increase across all prompts ([screenshot](https://ibb.co/Jc0LmS0)).\n\n> **Lack of code**\n\nApologies for the oversight! Open-source code has been available for the past 6 months, but we forgot to provide an anonymized link in our submission. You can find anonymized code [here](https://anonymous.4open.science/r/ddpo-pytorch-1D5C/).\n\n> **the second noticeable difference is the automated generation of rewards using BLIP, which is already a standard engineering practice and has been claimed by RLAIF**\n\nWe are not aware of any prior work that uses feedback from vision-language models to improve image-generative models. If you do know of such work, please let us know and we will add appropriate discussion!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287066082,
                "cdate": 1700287066082,
                "tmdate": 1700287066082,
                "mdate": 1700287066082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RjgNPfKDHl",
                "forum": "YCWjhGrJFD",
                "replyto": "bCKybLlHPY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the responses."
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal, which has enhanced the article's validity and authenticity, and I am willing to update my rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314381762,
                "cdate": 1700314381762,
                "tmdate": 1700314381762,
                "mdate": 1700314381762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Vi7FR6TQ0",
                "forum": "YCWjhGrJFD",
                "replyto": "7QT1jjqoGe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi reviewer qqX7,\n\nTo clarify, we believe that our rebuttal has thoroughly mitigated all of your stated weaknesses. Weaknesses (1) and (2) were about DPOK, which we have clarified is concurrent work, meaning that the DDPO algorithm should be considered an original intellectual contribution. Weakness (4) was about code, which we have provided. Weakness (3) was about out-of-domain reward evaluation, for which we have provided an additional experiment based on your recommendation.\n\nAs such, if you still think this work is below the acceptance threshold for ICLR, we would greatly appreciate it if you could elaborate on why so that we can further improve our paper!\n\nThank you,\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528948147,
                "cdate": 1700528948147,
                "tmdate": 1700543410799,
                "mdate": 1700543410799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Og9yk9sHpQ",
                "forum": "YCWjhGrJFD",
                "replyto": "9Vi7FR6TQ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8548/Reviewer_qqX7"
                ],
                "content": {
                    "comment": {
                        "value": "I am grateful to the author for the experimental responses during the Rebuttal period, which resolved some of my concerns. As a result, I have raised my score to 5. After a detailed reading and confirmation of the paper, I received two persistent inquiries from the author. Therefore, I will elaborate on my reasons for scoring as follows:\n\nCompared to concurrent work, this paper is so similar to DPOK in terms of innovation and paper writing but significantly weaker in contributions to reinforcement learning theory and insights than DPOK. Other reviewers, including rYFt, also raised questions about the similarity of this work. The paper was submitted after DPOK, and besides early convergence due to Important Sampling (I am not convinced by just one curve to claim this is a sufficient quantitative experiment, I am not satisfied with the experiment's sufficiency), I am not convinced that the paper has made sufficient innovative contributions to the community. Therefore, I have raised my score to 5, just below the acceptance threshold of ICLR.\n\nThe authors repeatedly emphasize the simultaneity of the paper (not only in my thread) but fail to prove its superiority over DPOK through sufficient quantitative indicators. In the work of Lee et al., 'Optimizing DDPM Sampling with Shortcut Fine-Tuning', the method of using RL to fine-tune DDPM has been detailed, and MDP was fully introduced. This paper was published on Arxiv on 31 Jan 2023, well before its publication date. In my view, Lee et al. first introduced RL into DDPM in 'Optimizing DDPM Sampling with Shortcut Fine-Tuning,' and then in DPOK, systematically and rigorously proved the validity of the practices in 'Optimizing DDPM Sampling with Shortcut Fine-Tuning.' A similar work to this paper should be 'Optimizing DDPM Sampling with Shortcut Fine-Tuning,' not DPOK. Also, I believe the authors had sufficient time (about six months) to extend DPOK to better performance, yet claim it as concurrent work, and today, after six months, with insufficient experiments, prove its validity compared to DPOK. In my view, DPOK is a theoretical extension, while 'Optimizing DDPM Sampling with Shortcut Fine-Tuning' is **the similar work** of this paper.\n\nRegarding serious errors in the experiments. As the author mentioned in the appendix, there was a serious bug in the experiments of this paper, but they claimed that such a bug does not affect the fairness of the comparison. This bug was only responded to after community inquiries. That is, some results of the paper can only be achieved on customized devices like TPU, and I am skeptical about whether such bugs exist in other parts, so I reserve my judgment on the completeness of this paper.\n\nI do not think repeatedly asking and unilaterally resolving all my doubts is an effective Rebuttal method. Therefore, I will keep my rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8548/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582169871,
                "cdate": 1700582169871,
                "tmdate": 1700582169871,
                "mdate": 1700582169871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]