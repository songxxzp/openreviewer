[
    {
        "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images"
    },
    {
        "review": {
            "id": "0dKIxthvr0",
            "forum": "RJDjSXNuAZ",
            "replyto": "RJDjSXNuAZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_KB1H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_KB1H"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of expensive and time-consuming annotation requirements for training state-of-the-art object detection models. The authors propose a domain-specific weakly supervised object detection algorithm that leverages image-level annotations instead of annotated bounding boxes. By distilling the knowledge of a pre-trained model focused on virus presence/absence prediction, the proposed approach generates a set of pseudo-labels that can be used to train an object detection model effectively. The method utilizes an optimization approach with a shrinking receptive field, enabling the extraction of virus particles directly without relying on specific network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Addressing Expensive Annotation Requirement: The paper tackles the challenge of acquiring costly bounding box annotations by proposing a weakly supervised approach that relies on image-level annotations. This significantly reduces the manual labor and time required for annotation, making the training process more efficient.\n- Extensive Comparative Studies: The authors conduct comprehensive studies to evaluate the effectiveness of the proposed pseudo-labels. The results demonstrate that the generated pseudo-labels outperform other weak labeling methods and even ground truth annotations in scenarios where annotation time is limited. This indicates the superiority and practical value of the proposed approach."
                },
                "weaknesses": {
                    "value": "- Limited Model Exploration. The paper primarily focuses on using Faster-RCNN with a ResNet-101 backbone as the detection model. It would be beneficial for the authors to consider exploring other models, such as DETR, to evaluate their effectiveness in the proposed approach.\n- Lack of Discussion on Low Signal to Noise Ratio (SNR) in EM Images: While the authors mention that low SNR in EM images can impact the performance of methods designed for other imaging modalities, there is a lack of in-depth discussion, algorithm design, and experiments addressing how to mitigate the low SNR problem and how it specifically affects the capacity of weakly supervised object detection (WSOD) methods in the EM scenario. Further exploration and discussion of this property would be valuable to enhance the understanding and applicability of the proposed approach."
                },
                "questions": {
                    "value": "In general, I find this work to be commendable. However, there are certain limitations that should be addressed for further improvement. Specifically, it is crucial to include testing with DETR to evaluate the effectiveness of transformer-based architectures. Additionally, at least providing a thorough discussion on the low SNR problem would significantly enhance the quality of the paper. If these questions are well solved, I would like happy to raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698557709615,
            "cdate": 1698557709615,
            "tmdate": 1699636586583,
            "mdate": 1699636586583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ZFsaajg2I",
                "forum": "RJDjSXNuAZ",
                "replyto": "0dKIxthvr0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer KB1H,\nwe highly appreciate your thoughtful comments. In the following, we will respond to every comment.\n\n## Limited Model Exploration\nWe agree, that our model exploration is limited. We motivate our decision to use CNN-based architectures by the limited data set sizes for virus detection in EM. However, we agree that a more thorough model exploitation can give additional insights, specifically as ViTs have shown great success in weakly supervised setups. We hence include the comparison of Ours(Opt) based on the ResNet-101 and Ours(Opt) based on a ViT backbone in the updated version of our paper (appendix A.2). \nNevertheless, we refrained from delving deeper into the exploration of alternative detection models such as DETR. Our ablation study, employing ViT backbones for Ours(Opt), suggests that DETR might face challenges and likely underperform, particularly given the constraints posed by the relatively small dataset sizes. However, we acknowledge that leveraging different detection models has the potential to enhance the performance of all methods, including our proposed approach (Ours(OD)).\n\n## Lack of Discussion on Low Signal to Noise Ratio (SNR) in EM Images\nWe agree that the low SNR in EM images can lead to the current state-of-the-art methods not performing well on EM images. Additionally, there are more challenges that are being posed by the nature of virus detection in EM like the occurrence of multiple instances of the same object in one image, as well as small annotated dataset sizes.\nWe hence include a more thorough discussion of the topic in relation to our updated evaluation in the paper (see section 4.4)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674646519,
                "cdate": 1700674646519,
                "tmdate": 1700674646519,
                "mdate": 1700674646519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hZspqTp3hb",
            "forum": "RJDjSXNuAZ",
            "replyto": "RJDjSXNuAZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a class activation map (CAM)-based weakly supervised learning method for virus particle detection in electron microscopy images. Specifically, it first uses a pre-trained classifier to obtain an initial position of a virus using GradCAM (Selvaraju et al. 2017), and then iteratively refines the position with a Gaussian mask with a dynamic standard deviation. It repeats this process for each virus until all the viruses are detected in the input image. The proposed method is evaluated on 5 electron microscopy image datasets, and the experimental results are promising."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a simple yet promising method for weakly supervised object detection. Meanwhile, it conducts extensive ablation studies to show that the proposed weakly supervised method can outperform other more fine-grained annotation-based approaches (e.g., bounding box and point annotations), given a certain time budget.\n\n2. The paper designs a specific user study to demonstrate the effectiveness and efficiency of the proposed method."
                },
                "weaknesses": {
                    "value": "1. In the experiments, the proposed method is not compared with other state-of-the-art weakly supervised learning methods, such as Zeng et al. 2019, Wei et al. 2022, and Lu et al. 2020. In addition, it is not compared with other CAM-based weakly supervised object detection methods in the experiments, such as Xu et al. 2022. Without a comparison with recent state of the art, it is difficult to determine the superiority of the proposed methods over other approaches.\n\n2. The method requires the object size to be known in advance. This needs additional effort to estimate the size of target objects before applying the method. It would be helpful to provide an in-depth discussion on this design (probably also including the effects of using different estimated object sizes).  \n\n3. It seems that the proposed method needs to repeat the optimization process (i.e., solving Equation (2)) for each virus. The time cost may be high if there is a large number of viruses in the input image.\n\n4. The method is evaluated on only virus detection in electron microscopy images, where viruses do not overlap. Thus, the method may not generalize to object detection (e.g., cell or nuclei detection) in other microscopy imaging modalities, such as hematoxylin and eosin (H&E) or immunohistochemistry (IHC) stained brightfield microscopy images, and fluorescence mages, which often have touching or overlapping cells or nuclei. In addition, the repeated optimization for each object would be expensive for H&E or IHC images that typically have thousands of or even more cells/nuclei."
                },
                "questions": {
                    "value": "1. The proposed method is based on the GradCAM method. What if the GradCAM does not provide good initializations or even wrong saliency maps? What are the effects of inaccurate saliency map creation on the quality of the pseudo-labels generated by the proposed method? \n\n2. During the optimization process of the proposed method, i.e., solving Equation (2), is the classifier C fixed and not updated? If so, is the optimization of Equation (2) simply to find the position that has the highest value in the prediction map C(I * M(p_t)) at each time step? But if not, what algorithm is used to optimize Equation (2)? \n\n3. During the postprocessing, the method uses non-maximum suppression to eliminate virus particles that have low detection scores. Are the detection scores of virus particles obtained from the initial CAM map or the prediction map from the Gaussian-filtered input, C(I * M(p_t))?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5642/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5642/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5642/Reviewer_zGyY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613919139,
            "cdate": 1698613919139,
            "tmdate": 1699636586484,
            "mdate": 1699636586484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7RTCXkqEtf",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer zGyY, \nWe highly appreciate your comments and will discuss them in detail in the following. We are looking forward to further discussions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674221425,
                "cdate": 1700674221425,
                "tmdate": 1700674221425,
                "mdate": 1700674221425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K1kr5RDdIX",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison to other WSOL methods"
                    },
                    "comment": {
                        "value": "We acknowledge this concern and regret that certain methods, such as [1] and [2], lacked publicly available training code. Other methods like GC-Net [3] lack the ability to detect more than one object, which is why we chose not to include it in the comparisons. We opted to include alternative state-of-the-art methods \u2014 specifically, LayerCAM [4], TS-CAM [5], and Reattention [6]. We further included GradCAM [7], as we make use of this method as an initialization schema. As ViTs have shown great success in current state of the art WSOL methods, we include the ViT as well as ResNet backbones for the application of GradCAM and LayerCAM.\nIn the interest of a more equitable comparison, we opted to integrate the known virus size into the existing methods. Through an extensive ablation study (appendix A.5), we systematically determined the most suitable approach for informing these methods (section 4.4). \n\n| Method                        | Herpes                | Adeno                   | Noro                 | Papilloma                      | Rota                 |\n|-------------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n| $\\mathrm{GradCAM}$ ResNet      | 78.79 $\\pm$ 2.04        | 19.17 $\\pm$ 0.78        | 05.54 $\\pm$ 2.99        | 11.57 $\\pm$ 4.17        | 31.78 $\\pm$ 21.58       |\n| $\\mathrm{LayerCAM}$ ResNet     | 78.44 $\\pm$ 2.73        | 16.48 $\\pm$ 9.34        | 05.04 $\\pm$ 1.91        | 10.87 $\\pm$ 5.33        | 31.22 $\\pm$ 20.07       |\n| $\\mathrm{GradCAM}$ ViT        | 61.87 $\\pm$ 11.87       | 08.00 $\\pm$ 2.12        | 19.31 $\\pm$ 13.64       | 04.03 $\\pm$ 4.52        | 13.12 $\\pm$ 7.37        |\n| $\\mathrm{LayerCAM}$ ViT       | 68.33 $\\pm$ 6.59        | 09.18 $\\pm$ 5.64        | 10.82 $\\pm$ 11.78       | 17.41 $\\pm$ 11.33       | 09.74 $\\pm$ 2.42        |\n| $\\mathrm{TS-CAM}$             | 32.06 $\\pm$ 1.02        | 39.25 $\\pm$ 4.13        | 14.64 $\\pm$ 4.66        | 07.11 $\\pm$ 3.85        | 43.53 $\\pm$ 3.93        |\n| $\\mathrm{Reattention}$        | 68.85 $\\pm$ 0.62        | **58.49** $\\pm$ 2.22        | 55.09 $\\pm$ 8.92        | 35.60 $\\pm$ 13.01       | 59.05 $\\pm$ 11.40       |\n| **$\\mathrm{Ours (Opt)}$** | 86.98 $\\pm$ 1.92  | 47.85 $\\pm$ 11.82 | 54.65 $\\pm$ 4.94  | 70.02 $\\pm$ 2.85  | 71.73 $\\pm$ 3.51 |\n| **$\\mathrm{Ours (OD)}$**  | **91.20** $\\pm$ 0.24 | **58.28** $\\pm$ 5.91 | **74.32** $\\pm$ 1.18 | **78.33** $\\pm$ 2.40 | **78.34** $\\pm$ 2.15 |\n\nAs shown in the table above, we found that other weakly supervised methods fail to reliably detect virus particles in EM images, eventhough we also incorperate the virus size in existing methods. Our methods usually outperforms all others by a large margin, except in the adeno virus, where we are en par with Reattention. Please find a detailed discussion in the paper (section 4.4, appendix A.5)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674251436,
                "cdate": 1700674251436,
                "tmdate": 1700674251436,
                "mdate": 1700674251436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "93VYycsBa4",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Known object size"
                    },
                    "comment": {
                        "value": "We regret we were not able to communicate it properly in the paper. Usually, the virus size can be found in literature. Moreover, as the standard deviation of virus particles is small, the size can easily be approximated by measuring only a few instances. We will include a discussion in the updated version of our paper in appendix A.4.\nWe also agree, that the correct approximation of the objects size is important for our method in order to work well. To communicate this, we added an additional experiment in appendix A.4 about the influence of approximation error in the updated version of our paper. For a quick overview please see the provided table:\n\n| Error               | $0\\%$                   | $10\\%$                  | $20\\%$                  | $30\\%$                  |\n|---------------------|-------------------------|-------------------------|-------------------------|-------------------------|\n| $\\mathrm{Ours (Opt)}$ | **86.98** $\\pm$ 1.92   | 82.38 $\\pm$ 02.34       | 79.14 $\\pm$ 03.10       | 67.31 $\\pm$ 02.03       |\n\n\nAs expected, we found that the performance reduces with increased error in the size approximation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674294715,
                "cdate": 1700674294715,
                "tmdate": 1700674378188,
                "mdate": 1700674378188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CScHYQ2uL9",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Repetition for multiple objects"
                    },
                    "comment": {
                        "value": "We agree that the computation time increases with the amount of visible virus particles in the image.\nThis is however a specific design choice for dealing with the non-object centered nature of EM images.\nOur evaluation shows that current state of the art methods lack to reliably detect multiple instances of the same object as they only have been trained on image level labels, and hence are able to only focus on the most prominent virus in the image (see appendix C.2). Our approach is able to side step this limitation to be applicable in the context of EM."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674434778,
                "cdate": 1700674434778,
                "tmdate": 1700674434778,
                "mdate": 1700674434778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G1K3puMfBC",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Generalization to other microscopy modalities"
                    },
                    "comment": {
                        "value": "In general, the detection of overlapping instances of the same object in a weakly supervised setup is no trivial task and remains a major challenge. We would like to emphasize that we specifically designed an approach for the detection of virus particles in EM images. In our extended evaluation we can show that current state of the art approaches fail to reliably detect virus particles in EM, which highlights the need of a specific method. \nAddressing the broader challenge of detecting overlapping instances of the same object within a weakly supervised setup is inherently complex and represents a substantial hurdle in general weakly supervised setups. Nonetheless, we maintain our confidence in the adaptability of our approach to diverse microscopy modalities. Qualitative results presented in the appendix (section C) illustrate instances where virus particles form clusters and exhibit touching behavior. Notably, our current method focuses exclusively on non-overlapping viruses, employing non-maximum suppression with a strict threshold to mitigate overlap.\nWe believe that relaxing this threshold would likely enable our method to detect overlapping instances. However, a thorough evaluation and extension of our method in this regard are reserved for future research."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674465117,
                "cdate": 1700674465117,
                "tmdate": 1700674465117,
                "mdate": 1700674465117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mseVgj1vzM",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "GradCAM initialization"
                    },
                    "comment": {
                        "value": "In the Appendix A.1.1 we investigate different methods for initialization and the power of the optimization process. The experiment shows, that even with non-optimal initialization (like random initialization) the optimization process is able to converge to an optimum, when enough iterations are used. In our extended evaluation, we found that GradCAM based WSOL failed to reliably detect the virus capsids in negative stain images (see appendix C.1), as the classifier was focusing on the boarder of the virus. However, the inclusion of our optimization procedure enables a more robust detection."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674504400,
                "cdate": 1700674504400,
                "tmdate": 1700674504400,
                "mdate": 1700674504400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2CLCi7sp2U",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Optimization of Equation 2"
                    },
                    "comment": {
                        "value": "Yes, the classifier is fixed during the optimization. We will emphasize this more in the updated version of our paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674530854,
                "cdate": 1700674530854,
                "tmdate": 1700674530854,
                "mdate": 1700674530854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OOAQCUvG4N",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Computation of detection scores"
                    },
                    "comment": {
                        "value": "We kindly refer to Section 3.5 in our paper. Here we explain, that the detection score is computed individually for each detected virus in a postprocessing step. To do so, we mask all other detected virus particles by a circular disk with radius of the known virus size and derive the score of the remaining virus by forwarding the masked image to the pretrained classifier."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674549404,
                "cdate": 1700674549404,
                "tmdate": 1700674549404,
                "mdate": 1700674549404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQMGGTyV5U",
                "forum": "RJDjSXNuAZ",
                "replyto": "hZspqTp3hb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[1] Xu, J., Hou, J., Zhang, Y., Feng, R., Zhao, R.W., Zhang, T., Lu, X. and Gao, S., 2022. Cream: Weakly supervised object localization via class re-activation mapping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9437-9446).\n\n[2] Wei, J., Wang, S., Zhou, S.K., Cui, S. and Li, Z., 2022, October. Weakly supervised object localization through inter-class feature similarity and intra-class appearance consistency. In European Conference on Computer Vision (pp. 195-210). Cham: Springer Nature Switzerland.\n\n[3] Weizeng Lu, Xi Jia, Weicheng Xie, Linlin Shen, Yicong Zhou, and Jinming Duan. Geometry constrained weakly supervised object localization. In European Conference on Computer Vision (ECCV), 2020\n\n[4] Jiang, Peng-Tao, et al. \"Layercam: Exploring hierarchical class activation maps for localization.\" IEEE Transactions on Image Processing 30 (2021): 5875-5888.\n\n[5] Gao, Wei, et al. \"Ts-cam: Token semantic coupled attention map for weakly supervised object localization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[6] Su, Hui, et al. \"Re-attention transformer for weakly supervised object localization.\" arXiv preprint arXiv:2208.01838 (2022).\n\n[7] Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" Proceedings of the IEEE international conference on computer vision. 2017."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674563232,
                "cdate": 1700674563232,
                "tmdate": 1700674563232,
                "mdate": 1700674563232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jI2Mu3gUrf",
            "forum": "RJDjSXNuAZ",
            "replyto": "RJDjSXNuAZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_6zGs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_6zGs"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a method for weakly-supervised object detection (WSOL) of virus capsids in EM images, which can be used for rapid curation of bounding boxes. The overview of this method (presented in Figure 1) uses an iterative process in which: 1) Grad-CAM from a pretrained encoder is use to output saliency maps of the highest-scoring virus location, 2)  gradient descent is used to optimize the location of the virus, 3) virus is masked out using known information about the virus size. This process repeats until all viruses are removed (referenced as Ours (Opt)), with the bounding boxes created using this process usable for developing weakly-supervised object detectors (Ours (OD)). Comparisons against human annotators (weakly-supervised binary annotation, location, bounding box) and self-supervised detectors were performed, with comparison against human annotators (with and without time constraints) also performed."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, this work presents a very unique methodology and study design for curating bounding boxes in EM images. A contribution not emphasized in this work is the simplicity of the method, using a very intuitive heuristic that outperforms current unsupervised, deep learning-based detectors such as SAM and CUTLER. Though specific to EM, I believe the uniqueness and simplicity of this work would still be of interest to the computer vision community. The related work section is all comprehensive, and the authors of this work reference related works in weakly-supervised and self-supervised object detection very well."
                },
                "weaknesses": {
                    "value": "- Though the related work section provides a comprehensive overview of current progress in WSOL methods, was there a reason why this work does not compare against other WSOL methods such as Xu et al. [1] (CREAM), Wei et al. [2] (ISIC), and other more recent works such as LOCATE [3] and GenPromp [4]? Though specific to EM, many other works in the WSOL domain can also be readily adapted.\n- In addition to lack of comparisons, one of the main limitations of this work that may prevent broader interest in the ICLR community is that the proposed method is too specific to EM and is not evaluated on diverse tasks. Though EM is unique compared to natural images which are generally more object-centric, other modalities such as histopathology and multiplexed imaging share similar characteristics (as noted in [5]), with the image scale is objective with units per pixel being fixed. Specifically, the contributions of this work would be strengthened if shown that a simpler heuristic can also be created for other imaging domains.\n- Following other works which have found pretrained Vision Transformers (ViTs) to be strong in WSOL [4,5,6], was there as a reason why a ResNet-101 was used for classification instead of a ViT? Moreover, was the DINO-ViT used in CUTLER trained using EM images, or was it using a pretrained checkpoint from ImageNet? As ViTs have been also found to have natural fit for microscopy images [5], it would be interesting to explore how the a DINO-ViT for EM images would: 1) improve the WSOL results of this work, and 2) improve the CUTLER baseline reported in this work.\n- Though this work is well-written, it was difficult to understand the training dataset and the downstream dataset for evaluation and annotator labeling. Though described in text, including a table with the distribution of labels for train and test may be simpler to communicate.\n\nOverall, I found this work to have a unique contribution computer vision in proposing simpler techniques that outperform more complicated solutions that are more complex to train and underperform in domain-specific areas. At the same time, I feel that the scope of the work is too narrow for ICLR, and lacks comparisons to other WSOL works. I still feel that this work has many strengths, and believe that it would make a timely contribution in conferences specific to computer vision such as CVPR and ECCV where approaches for solving domain-specific challenges would be more broadly appreciated.\n\n1. Xu, J., Hou, J., Zhang, Y., Feng, R., Zhao, R.W., Zhang, T., Lu, X. and Gao, S., 2022. Cream: Weakly supervised object localization via class re-activation mapping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9437-9446).\n2. Wei, J., Wang, S., Zhou, S.K., Cui, S. and Li, Z., 2022, October. Weakly supervised object localization through inter-class feature similarity and intra-class appearance consistency. In European Conference on Computer Vision (pp. 195-210). Cham: Springer Nature Switzerland.\n3. Li, G., Jampani, V., Sun, D. and Sevilla-Lara, L., 2023. LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10922-10931).\n4. Zhao, Y., Ye, Q., Wu, W., Shen, C. and Wan, F., 2023. Generative prompt model for weakly supervised object localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6351-6361).\n5. Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D., Krishnan, R.G. and Mahmood, F., 2022. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16144-16155).\n6. Murtaza, S., Belharbi, S., Pedersoli, M., Sarraf, A. and Granger, E., 2023. Discriminative sampling of proposals in self-supervised transformers for weakly supervised object localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 155-165)."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822789511,
            "cdate": 1698822789511,
            "tmdate": 1699636586390,
            "mdate": 1699636586390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DOR2vdygvB",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer 6zGs,\nwe highly appreciate your thoughtful comments. In the following, we will respond to every comment. We are looking forward to further discussions.\n\n## Comparison to other WSOL methods\nWe acknowledge this concern and regret that certain methods, such as [1] and [2], lacked publicly available training code. Other methods like GC-Net [3] lack the ability to detect more than one object, which is why we chose not to include it in the comparisons. We opted to include alternative state-of-the-art methods \u2014 specifically, LayerCAM [4], TS-CAM [5], and Reattention [6]. We further included GradCAM [7], as we make use of this method as an initialization schema. As ViTs have shown great success in current state of the art WSOL methods, we include the ViT as well as ResNet backbones for the application of GradCAM and LayerCAM.\nIn the interest of a more equitable comparison, we opted to integrate the known virus size into the existing methods. Through an extensive ablation study (appendix A.5), we systematically determined the most suitable approach for informing these methods (section 4.4). \n\n| Method                        | Herpes                | Adeno                   | Noro                 | Papilloma                      | Rota                 |\n|-------------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n| $\\mathrm{GradCAM}$ ResNet      | 78.79 $\\pm$ 2.04        | 19.17 $\\pm$ 0.78        | 05.54 $\\pm$ 2.99        | 11.57 $\\pm$ 4.17        | 31.78 $\\pm$ 21.58       |\n| $\\mathrm{LayerCAM}$ ResNet     | 78.44 $\\pm$ 2.73        | 16.48 $\\pm$ 9.34        | 05.04 $\\pm$ 1.91        | 10.87 $\\pm$ 5.33        | 31.22 $\\pm$ 20.07       |\n| $\\mathrm{GradCAM}$ ViT        | 61.87 $\\pm$ 11.87       | 08.00 $\\pm$ 2.12        | 19.31 $\\pm$ 13.64       | 04.03 $\\pm$ 4.52        | 13.12 $\\pm$ 7.37        |\n| $\\mathrm{LayerCAM}$ ViT       | 68.33 $\\pm$ 6.59        | 09.18 $\\pm$ 5.64        | 10.82 $\\pm$ 11.78       | 17.41 $\\pm$ 11.33       | 09.74 $\\pm$ 2.42        |\n| $\\mathrm{TS-CAM}$             | 32.06 $\\pm$ 1.02        | 39.25 $\\pm$ 4.13        | 14.64 $\\pm$ 4.66        | 07.11 $\\pm$ 3.85        | 43.53 $\\pm$ 3.93        |\n| $\\mathrm{Reattention}$        | 68.85 $\\pm$ 0.62        | **58.49** $\\pm$ 2.22        | 55.09 $\\pm$ 8.92        | 35.60 $\\pm$ 13.01       | 59.05 $\\pm$ 11.40       |\n| **$\\mathrm{Ours (Opt)}$** | 86.98 $\\pm$ 1.92  | 47.85 $\\pm$ 11.82 | 54.65 $\\pm$ 4.94  | 70.02 $\\pm$ 2.85  | 71.73 $\\pm$ 3.51 |\n| **$\\mathrm{Ours (OD)}$**  | **91.20** $\\pm$ 0.24 | **58.28** $\\pm$ 5.91 | **74.32** $\\pm$ 1.18 | **78.33** $\\pm$ 2.40 | **78.34** $\\pm$ 2.15 |\n\nAs shown in the table above, we found that other weakly supervised methods fail to reliably detect virus particles in EM images, eventhough we also incorperate the virus size in existing methods and dataset sizes are the same. Our methods usually outperforms all others by a large margin, except in the adeno virus, where we are en par with Reattention. Please find a detailed discussion in the paper (section 4.4, appendix A.5)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673973582,
                "cdate": 1700673973582,
                "tmdate": 1700675002707,
                "mdate": 1700675002707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hpe2RnzUvT",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Limited applicability of the method"
                    },
                    "comment": {
                        "value": "Our proposed method does not aim to reach a large range of applications and object types, but it is designed for the detection of particles in electron microscopy images. In our extensive evaluation, we are able to show that the proposed method is able to outperform current state-of-the-art methods, even though we also include the known virus size in existing approaches. We believe that this is most likely due to the fact, that current state-of-the-art methods highly benefit from large dataset sizes, object-centric datasets, and high SNR. These large amounts of annotated data are not available for virus detection in EM. Additionally, EM images aren't object-centric but usually contain multiple instances of the same object in one micrograph. Finally, low SNR can hinder the performance of these methods in EM. \nThis finding underlines the need for methods that are specifically designed to work well for virus detection in EM. \nDespite this, we believe that the proposed method holds significance for the broader ICLR community. Firstly, its simplicity renders it accessible and intuitive, making it potentially valuable for a wide audience. Secondly, its adaptability to a broader spectrum of applications is noteworthy. By replacing the circle with ellipses and incorporating size optimization, as suggested by [3], the approach demonstrates versatility.\nWhile a comprehensive evaluation of these adaptations is beyond the scope of this paper, we recognize their potential impact and plan to delve into a detailed assessment in future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673992623,
                "cdate": 1700673992623,
                "tmdate": 1700673992623,
                "mdate": 1700673992623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fe8S1JC4fL",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ViTs in WSOL"
                    },
                    "comment": {
                        "value": "We agree that ViTs have shown great success in WSOL. However, due to the small data set sizes in virus detection for EM, we decided to use basic CNN architectures, as CNNs often exhibit strong data efficiency, particularly when dealing with relatively small datasets.\nWe still agree that the use of ViT as a backbone is an interesting ablation, which is why we included it in the updated version of our paper. For details please see appendix A.2. \n\n| Method                            | Herpes              | Adeno                 | Noro               | Papilloma                    | Rota               |\n|-----------------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|\n| $\\mathrm{Ours (Opt)}$ ResNet      | **86.98** $\\pm$ 1.92   | **47.85** $\\pm$ 11.82  | **54.65** $\\pm$ 4.94   | **70.02** $\\pm$ 2.85   | **71.73** $\\pm$ 3.51   |\n| $\\mathrm{Ours (Opt)}$ ViT         | 48.66 $\\pm$ 07.44      | 07.46 $\\pm$ 05.21      | 10.44 $\\pm$ 09.06      | 05.67 $\\pm$ 08.05      | 04.50 $\\pm$ 3.51       |\n\n\nAs presented in the table above, the results show that the ViT especially underperforms for the very small negative stain data sets of the Adeno, Noro, Papilloma and Rota virus."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674019631,
                "cdate": 1700674019631,
                "tmdate": 1700674019631,
                "mdate": 1700674019631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jXelj8aP9g",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "DINO-ViT in CutLer"
                    },
                    "comment": {
                        "value": "In our experiments, we did not train a DINO-ViT for CutLer on EM images.\nIn our experience with training models on EM images, we found that most often fine-tuning ImageNet pre-trained weights can lead to superior performance compared to fine-tuning models based on EM pre-trained weights [8].  \nThis can be due to the natural fit of ViTs to EM images (as you've mentioned) but also to the usually much bigger size of the pretraining dataset (exemplified by ImageNet with 1.2 million training images as opposed to CEM500k [8] with 500,000 training images).\nFine-tuning the DINO-ViT on EM images is, therefore, a non-trivial task, warranting a more thorough investigation. The intricacies involved in achieving optimal performance in this context necessitate a comprehensive exploration of the challenges and potential adaptations for ViTs in the domain of EM image fine-tuning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674048171,
                "cdate": 1700674048171,
                "tmdate": 1700674048171,
                "mdate": 1700674048171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hQZ5LdBFkQ",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Label Distribution"
                    },
                    "comment": {
                        "value": "We agree that the distribution of the training data is difficult to understand, especially given the evaluation using different data set sizes based on annotation times. We hence include a more informative table in appendix D."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674096310,
                "cdate": 1700674096310,
                "tmdate": 1700674096310,
                "mdate": 1700674096310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CqrqO1dkip",
                "forum": "RJDjSXNuAZ",
                "replyto": "jI2Mu3gUrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "[1] Xu, J., Hou, J., Zhang, Y., Feng, R., Zhao, R.W., Zhang, T., Lu, X. and Gao, S., 2022. Cream: Weakly supervised object localization via class re-activation mapping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9437-9446).\n\n[2] Wei, J., Wang, S., Zhou, S.K., Cui, S. and Li, Z., 2022, October. Weakly supervised object localization through inter-class feature similarity and intra-class appearance consistency. In European Conference on Computer Vision (pp. 195-210). Cham: Springer Nature Switzerland.\n\n[3] Weizeng Lu, Xi Jia, Weicheng Xie, Linlin Shen, Yicong Zhou, and Jinming Duan. Geometry constrained weakly supervised object localization. In European Conference on Computer Vision (ECCV), 2020\n\n[4] Jiang, Peng-Tao, et al. \"Layercam: Exploring hierarchical class activation maps for localization.\" IEEE Transactions on Image Processing 30 (2021): 5875-5888.\n\n[5] Gao, Wei, et al. \"Ts-cam: Token semantic coupled attention map for weakly supervised object localization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[6] Su, Hui, et al. \"Re-attention transformer for weakly supervised object localization.\" arXiv preprint arXiv:2208.01838 (2022).\n\n[7] Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n[8] Conrad, Ryan, and Kedar Narayan. \"CEM500K, a large-scale heterogeneous unlabeled cellular electron microscopy image dataset for deep learning.\" Elife 10 (2021): e65894."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674175615,
                "cdate": 1700674175615,
                "tmdate": 1700674175615,
                "mdate": 1700674175615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pcnJGVhMZM",
            "forum": "RJDjSXNuAZ",
            "replyto": "RJDjSXNuAZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_xKLD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5642/Reviewer_xKLD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a domain-specific weakly supervised object detection method that relies on image-level annotations instead of bounding boxes. They use a pre-trained model to generate pseudo-labels for training, showing that these labels outperform other weak labeling methods and even ground truth labels in time-constrained scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper is well-written and easily to follow. \n- it proposed an relevantly simple but effective method for an impactful task. In their experimenst, aurthors sucessfully demonstrated the supriority over the consider baselines, includig supervised method as well as zero-shot learning with large scale pretrained models. \n- the authors utilized the spatial information and explored an novel way to refine the localization neural networks provide."
                },
                "weaknesses": {
                    "value": "The proposed method has potential to work for not only electron microscope images but other medical images. It will be interesting and also brings broader impact if authors can provide discussions around this."
                },
                "questions": {
                    "value": "The current setup with Gaussian as a prior assumes that the object to detect is in a round shape. How easily it can be extended to different objects and how accurately it will work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5642/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699284788420,
            "cdate": 1699284788420,
            "tmdate": 1699636586310,
            "mdate": 1699636586310,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pjZ5GqtWt3",
                "forum": "RJDjSXNuAZ",
                "replyto": "pcnJGVhMZM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5642/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer xKLD,\nThank you for your valuable comments. We would like to address your comments one by one, as seen below. We are looking forward to your feedback.\n\n## Broader impact\nWe agree, that the proposed method has potential to work not only for the detection of virus particles in EM but other medical images, or other standard CV datasets. \nHowever, our approach is strategically crafted for the detection of particles in EM images, rather than aiming for a broad range of applications and object types. In our expanded evaluation, we demonstrate the efficacy of our method by surpassing the performance of more intricate state-of-the-art methods, even when incorporating virus size information into their frameworks.\nThe observed superiority of our method can be attributed to the unique challenges posed by virus detection in EM, which diverge from the characteristics addressed by current state-of-the-art methods. Specifically:\n\n1. Limited Annotated Data: Current state of the art approaches highly benefit from large data set sizes. These large amounts of annotated data are not available for virus detection in EM.\n2. Non-Object-Centric Nature of EM Images: Current state of the art approaches are usually designed for more object-centered approaches. This does not apply to EM images where multiple instances of the same virus occur in a single micrograph.\n3. Low Signal-to-Noise Ratio (SNR) in EM: Current state of the art approaches usually do not need to deal with low SNR images, as it is the case for EM.\n\nThese observations underscore the significance of developing methods explicitly designed to excel in the specialized task of virus detection in electron microscopy. Our approach addresses these unique challenges, positioning it as a valuable contribution to the field by outperforming existing methodologies specifically tailored for this demanding context. We will include a more in depth discussion about this in the updated version of our paper in relation to our extended evaluation (see section 4.4)\n\n\n## Round shape as a prior and discussion on braoder impact\nWe believe that the extension of the proposed method to a wider range of applications is possible. In [1] the authors use a generator to predict ellipses which are then used to mask the input image for training of a detector with image-level labels only. Substituting the proposed circle by ellipses will open the possibility to detect objects with different aspect ratios. They use an additional size penalty as a loss to additionally optimize for the size. \nCombining the proposed method with [1] will most likely achieve a broader application of the approach. \nHowever, as discussed above, our evaluation shows the need for more specific approaches in the field of virus detection in EM. We hence leave the extension of the approach to a wider range of applications for future work.\n\n\n[1] Lu, Weizeng, et al. \"Geometry constrained weakly supervised object localization.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVI 16. Springer International Publishing, 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5642/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673732235,
                "cdate": 1700673732235,
                "tmdate": 1700673732235,
                "mdate": 1700673732235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]