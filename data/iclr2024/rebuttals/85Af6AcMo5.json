[
    {
        "title": "SciRE-Solver: Accelerating  Diffusion Models Sampling by Score-integrand Solver with Recursive Difference"
    },
    {
        "review": {
            "id": "Kx4boPosz0",
            "forum": "85Af6AcMo5",
            "replyto": "85Af6AcMo5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_zqdo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_zqdo"
            ],
            "content": {
                "summary": {
                    "value": "The work investigates accelerating diffusion model sampling. Compared with existing works, the authors propose a new gradient estimation method,  named recursive difference, and show improvement compared with existing works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The experiments are very comprehensive for image experiments in terms of FID. \n2. Based on experiments in the main paper and appendix, the proposed new method shows improvements."
                },
                "weaknesses": {
                    "value": "1. After reading the main paper, I have difficulties in understanding why the proposed recursive difference works better. The authors claim \" This method recursively extracts the hidden lower-order derivative information of the higher-order derivative terms in the Taylor\nexpansion of the score-integrand at the required point, as illustrated in Figure 2.\" The recursive difference trick is key contribution of the work, authors should consider rewriting the above high-density sentence into an easy-to-understand paragraph and highlight why it works, presenting more analysis.\n\n2. Can the author show some comparison in terms of numerical accuracy (MSE against ground truth solution) besides FID? How fast of various methods converge to ground truth solution? \n\n3. After reading the main paper and appendix, it is unclear to me why the chosen coefficient C6 is better than C5. Besides the empirical experiments, do authors have more principled math analysis for them? \n\n4. Similar to Q2, can authors present evidence that the proposed method can better estimate score gradients?\n\n5. A recent work investigates a similar problem and shares a similar algorithm. Can authors comment on the connection and difference[1]? \n\n[1] Zhang et al. Improved order analysis and design of exponential integrator for diffusion models sampling"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4270/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646059189,
            "cdate": 1698646059189,
            "tmdate": 1699636394452,
            "mdate": 1699636394452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmg7kAqh3O",
                "forum": "85Af6AcMo5",
                "replyto": "Kx4boPosz0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer zqdo,\n\nThank you for the detailed review and valuable feedback. Your comments are helping improve our work, many thanks. Below, we address specific questions you are concerned about.\n\n### W1: ... The recursive difference trick is key contribution of the work, authors should consider rewriting the above high-density sentence... and highlight why it works, presenting more analysis.\n\nThanks for the detailed review and valuable suggestions. We highly value your suggestions and have rephrased the relevant portion in main paper to provide a detailed explanation and context. For example, we indicate that in algorithms based on the exponential integrator (such as DPM-Solver [1] and DEIS [2]), in order to achieve fast sampling, the original coefficient $\\frac{e^h-h-1}{h^2}$ of the difference term is replaced by $\\frac{e^h-1}{h}$. Although this substitution is based on the concept of equivalent infinitesimals, it simultaneously reveals that after the replacement of the original coefficients, the acceleration effect becomes more pronounced. Therefore, we speculate that utilizing the conventional FD method directly to evaluate the derivative of the score function may be a suboptimal choice. Then, based on Taylor expansion, we recursively applied finite differences to handle higher-order terms and derived a new equivalent infinitesimal. We refer to this new equivalent infinitesimal as the recursive difference technique. For specific details, please refer to pages 4-5 of the main paper and Appendix E.\n\n[1] Lu et al., Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps, NeurIPS 2022.\n\n[2] Zhang et al., Fast sampling of diffusion models with exponential integrator, ICLR 2023.\n\n### W2-W4 ...why the chosen coefficient C6 is better than C5... How fast of various methods converge to ground truth solution? \u00a0... can authors present evidence that the proposed method can better estimate score gradients?\n\nThanks for the thoughtful questions. In fact, these issues may be equivalent. In fact, the main characteristic of the RD method is that this novel estimation incorporates low-order derivative information hidden in the higher-order derivative terms of the Taylor expansion. Refer to the demonstration in Eq. $(3.5)$ and the summary in Figure 2 for details. Compared to FD method, the RD method incorporates additional information $\\frac{1-\\phi_1(m)}{\\phi_1(m)}\\frac{\\Gamma (\\tau_{t_{i-1}}) - \\Gamma (\\tau_{t_i})}{h_{t_i}}$ from other higher-order derivative terms. Naturally, we have reason to believe that this additional term may counterbalance to a certain level with these higher-order terms from the Taylor expansion. This explanation may address your concerns. The details are included in the newly added Appendix E, especially in E2.\n\nIn E2, to address the concerns you raised, we have analyze RD method from three perspectives as comprehensively as possible.\n\n- Firstly, we indicate that a commonality among the coefficients of RD method and the coefficients used in the exponential integrator is the amplification of the original coefficients of FD.\n  \n- Secondly, we compare the coefficients of EI and RD method from the perspective of analyzing these three different coefficient functions (FD, EI, RD).\n  \n- Finally, we attempt to theoretically substantiate our viewpoint regarding the offsetting of higher-order terms. We demonstrate that under certain conditions, RD methods for derivative estimation can achieve better truncation errors. We feel that these conditions may be relaxed, but after several days of derivation, we have not yet found a suitable approach. Nevertheless, these conditions all affirm the observed phenomenon.\n  \n\n### W5: A recent work investigates a similar problem and shares a similar algorithm. Can authors comment on the connection and difference[3]?\n\nThanks for the interesting question. In fact, we believe that the exponential integrator fundamentally utilizes an equivalent infinitesimal substitution, amplifying the ratio of difference terms (as indicated in the previous answer). Upon your prompt, we noticed that paper [3] is also under review at ICLR 2024. Wishing them good luck. Based on my reading and learning, it appears that paper [3] also involves the adjustment of coefficients for difference terms. The difference lies in the fact that in reference [3], a method similar to the Butcher tableau is employed to adjust the coefficients of the original EI within the context of EI.\n\n[3] Zhang et al., Improved order analysis and design of exponential integrator for diffusion models sampling."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403309598,
                "cdate": 1700403309598,
                "tmdate": 1700404500873,
                "mdate": 1700404500873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XOYbH9lzUv",
                "forum": "85Af6AcMo5",
                "replyto": "cmg7kAqh3O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Reviewer_zqdo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Reviewer_zqdo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my questions. I will maintain my rating. \n\n\nRegarding \"present evidence that the proposed method can better estimate score gradients\", \n\nCan the author provide empirical results based on MSE between various solvers and ground truth solutions, which can be obtained with a large NFE solution?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680535281,
                "cdate": 1700680535281,
                "tmdate": 1700680535281,
                "mdate": 1700680535281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FdMgV3hTzb",
                "forum": "85Af6AcMo5",
                "replyto": "Kx4boPosz0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response.\n\nWe have conducted a comparison of MSE on the bedroom dataset under the same basic settings. \nSpecifically, we used the uniform-time trajectory with the termination time set at $1e-3$.  \nDue to time constraints, we have included these results in the attachment. Please refer to the report file \"report_MSE.txt\".  We sincerely hope that our endeavors garner your support.\n\nThanks again."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740624333,
                "cdate": 1700740624333,
                "tmdate": 1700741793925,
                "mdate": 1700741793925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "svCHRAlOJs",
            "forum": "85Af6AcMo5",
            "replyto": "85Af6AcMo5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_MTm2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_MTm2"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a new method, the recursive difference method, to improve the speed of Diffusion models by efficiently calculating score function derivatives. Their SciRESolver technique significantly accelerates DM sampling, achieving state-of-the-art FID scores with fewer NFEs. The method demonstrates remarkable performance on tasks like text-to-image generation, requiring few NFEs for high-quality samples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In diffusion models, the NFE required for sampling has always been the main computational overhead, and reducing NFEs is very important for efficiency."
                },
                "weaknesses": {
                    "value": "The improvement is not consistent, which makes the interpretation a little challenging."
                },
                "questions": {
                    "value": "Given the current literature, as the distillation of score-based models can reduce NFE significantly. How can your algorithm be combined with distillation?\n\nHow can you explain the behavior of SciRE-V1-2 and SciRE-V1-3 from a theoretical perspective?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4270/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789888945,
            "cdate": 1698789888945,
            "tmdate": 1699636394346,
            "mdate": 1699636394346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K2sWBuIkub",
                "forum": "85Af6AcMo5",
                "replyto": "svCHRAlOJs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer MTm2,\n\nThank you for the detailed review and thoughtful feedback. Your comments are helping improve our work, many thanks. Below, we address specific questions you are concerned about.\n\n### W: The improvement is not consistent, which makes the interpretation a little challenging.\n\nThanks for the detailed review. We carefully examined our entire paper, and perhaps the inconsistency you mentioned is reflected in the performance of the SciRE-V1-3-agile on the pre-trained model of continuous-time CIFAR-10. After closely inspecting all the procedures on continuous-time CIFAR-10, we found a lack of setting for random seeds in the codebase we downloaded. Upon fixing the random seed, we observed that the experimental results no longer exhibit fluctuations like the blue dashed line in sub-figure b of Figure 7. \n\n### Q: Given the current literature, as the distillation of score-based models can reduce NFE significantly. How can your algorithm be combined with distillation? How can you explain the behavior of SciRE-V1-2 and SciRE-V1-3 from a theoretical perspective?\n\nThanks for the thoughtful question. The SciRE-Solver sets up numerical algorithms from the perspective of numerical solutions to differential equations, eliminating the need for any additional training and optimization and offering strong flexibility.\n\nFrom my current perspective, while the distillation method of score-based can significantly reduce the NFE, the quality of the generated samples only achieves \"a comparable level\", falling short of the optimal performance (FID) of its base model (undistilled).\n\nWe believe that training-free algorithms, can not only accelerate but also enhance the generative capabilities of models. SciRE-Solver has validated this perspective. Regarding how to further reduce NFE, research will become very challenging, but SciRE-Solver at least gives us a glimpse of such hope.\n\nThe response regarding how to integrate with distillation and the theoretical analysis of SciRE-V1 is as follows:\n\n- The combination of an ODE Solver and distillation is exemplified in the Consistency Distillation algorithm for consistency models (see [1], [2]). As for how to integrate them more effectively, I don't have a definitive answer.\n  \n- In Appendix F, we provide the convergence (convergence order) proofs for SciRE-V1-2 and SciRE-V1-3. Additionally, in Appendix E, we further analyze the recursive difference method. \n  \n\n[1] Song et al., Consistency Models, ICML 2023.\n\n[2] Song et al., Improved Techniques for Training Consistency Models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388069002,
                "cdate": 1700388069002,
                "tmdate": 1700388069002,
                "mdate": 1700388069002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aKXZeZUBKL",
            "forum": "85Af6AcMo5",
            "replyto": "85Af6AcMo5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_eocv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_eocv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new training-free sampling algorithm for diffusion models, called SciRE-solver. SciRE-solver is based on Taylor expansion and uses the proposed Recursive Difference (RD) method to estimate the derivative of the score model. The authors conduct extensive experiments on various datasets such as CIFAR10, CelebA, ImageNet, LSUN, showing that the proposed SciRE-solver consistently outperforms existing numerical samplers."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written with sufficient details and comprehensive ablation studies. \n2. The paper clearly explains its relationship to and differences from the related work to be well-placed in the literature. \n3. SOTA performance compared to existing numerical sampling algorithms on various datasets and diffusion models."
                },
                "weaknesses": {
                    "value": "1. The paragraph above Figure 2 needs revision for clarity. The score approximation error is inevitable. How can the proposed sampling method mitigate this issue? The third sentence is also vague without explaining what the \"additional variables\" means. It is not clear how these considerations lead to the hypothesis either. \n2. While the authors demonstrate generated samples of pre-trained DMs on high-resolution datasets such as ImageNet, LSUN-bedroom, and stable diffusion model, there is lack of quantitative results on these datasets except for ImageNet128. Can you also add quantitative results on ImageNet256 and LSUN-bedroom? \n3. In Table 1, SciRE-solver uses pre-trained model from EDM. But the results of the original EDM sampler is missing from the comparison. \n4. Minor: in the abstract, \"Experiments demonstrate also that demonstrate that\" -> \"Experiments also demonstrate that\"."
                },
                "questions": {
                    "value": "1. While SciRE-solver outperforms its counterpart DPM-solver in the experimental results, can you elaborate more on why it is better than DPM-solver numerically? Does SciRE-solver provide more accurate higher-order derivative estimation than DPM-solver theoretically?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4270/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4270/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4270/Reviewer_eocv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4270/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798071087,
            "cdate": 1698798071087,
            "tmdate": 1699636394255,
            "mdate": 1699636394255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NKrNiNUOn3",
                "forum": "85Af6AcMo5",
                "replyto": "aKXZeZUBKL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer eocv,\n\nThank you for the detailed review and thoughtful feedback. Your comments are helping improve our work, many thanks. Below, we address specific questions you are concerned about.\n\n### W1: The paragraph above Figure 2 needs revision for clarity. The score approximation error is inevitable ...\n\nThanks for the valuable feedback. Taking into full consideration this feedback, as well as the inevitability of approximation errors, we have revised the expression of this paragraph from the perspective of the characteristics of existing literature. Specifically, in order to accelerate sampling, existing literature has replaced the coefficients of finite difference terms with the concept of equivalent infinitesimals during algorithm design. We believe that such replacement fundamentally alters the way derivatives are estimated, and it also potentially indicates that using finite difference to estimate derivatives may not lead to further acceleration effects. This analysis supports our hypothesis.\n\n### W2: ...demonstrate generated samples of pre-trained DMs ... such as ImageNet, LSUN-bedroom, and stable diffusion model, there is lack of quantitative results on these datasets except for ImageNet128. Can you also add quantitative results on ImageNet256 and LSUN-bedroom?\n\nThank you for your suggestions. Due to time and server constraints, computing FID on large datasets has become challenging for us. Nonetheless, we have provided the main file of our algorithm, \"scire_solver.py\", in the supplementary materials. We welcome any explorers to try our algorithm on these large datasets. Subsequently, we will also open source all of our code.\n\n### W3: In Table 1, SciRE-Solver uses pre-trained model from EDM. But the results of the original EDM sampler is missing from the comparison.\n\nThanks for the detailed review. We have included the experiments for EDM in Table 1.  When invoking the Heun iteration algorithm for EDM, we utilized the trajectory with the EDM type.\n\n### W3: Minor: in the abstract, \"Experiments demonstrate also that demonstrate that\" -> \"Experiments also demonstrate that\".\n\nThanks for the detailed review. We have thoroughly examined our paper and made the necessary revisions. Many Thanks.\n\n### Q: While SciRE-Solver outperforms its counterpart DPM-Solver in the experimental results, can you elaborate more on why it is better than DPM-solver numerically? Does SciRE-Solver provide more accurate higher-order derivative estimation than DPM-solver theoretically?\n\nThanks for the thoughtful review. This question has prompted deep contemplation for me. I have attempted to theoretically derive some insights, but it has proven exceptionally challenging (so far). Due to time constraints, we discuss and analyze the commonalities and differences among the difference coefficients used in various estimation methods in the context of the exponential integrator. The details refer to Appendix E, esp. E2."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700381255772,
                "cdate": 1700381255772,
                "tmdate": 1700471448419,
                "mdate": 1700471448419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D4MY8uBvoi",
            "forum": "85Af6AcMo5",
            "replyto": "85Af6AcMo5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_gjHa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4270/Reviewer_gjHa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Recursive Difference (RD) method to calculate the derivative of the score function network. Based on the RD method and the truncated Taylor expansion of score-integrand, the authors propose SciRE-Solver to accelerate diffusion model sampling. \nThe core of their algorithm relies on evaluating higher-order derivatives of the score functions, which cannot be done by conventional finite difference methods, as errors can propagate easily. The RD method is proposed to tackle this problem. They provide extensive experiments on variant benchmark datasets to demonstrate the effectiveness of their approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors propose a fast sampling algorithm for diffusion models based on using an RD approach to evaluate higher-order derivatives of the score function. They clearly introduce the intuiotion and the background story. Extensive experiments on various datasets are conducted to support the use of their proposed algorithm. Compared to existing algorithms, the proposed SciRE-based algorithm in many cases achieve lower FID score with a fewer number of NFEs."
                },
                "weaknesses": {
                    "value": "I have two major concerns:\n\n1. Their main result, the RD procedure is not presented clearly enough. This algorithm is only described in words, and Figure 2 is hard to parse. From Equation (3.7) I see that to evaluate first order derivative at $s$, we need both the first and the second order derivatives at $t$. Then why the authors say in the caption of Figure 2 that we can evaluate the first order derivative at $s$ with only zero order derivative at $t$? I would suggest present the most general form algorithm in a pseudo-code format like Algorithm 1 and 2. \n\n2. This paper might contain some critical typos that affect the entire proposal (see my second question). \n\nI would love to increase my score if these issues are well-addressed."
                },
                "questions": {
                    "value": "1. Is there any acceleration algorithm for diffusion SDE as well? If yes, I would love to see the authors providing a discussion. If no, could the authors elaborate a bit on why training-free acceleration is mostly for diffusion ODE? \n2. I thought $\\alpha_t = \\prod_{i = 1}^t \\beta_i$ is piecewise constant?  Then how do you define $f(t)$ as the derivative of $\\log \\alpha_t$ with respect to $t$? It is unclear whether the authors use $t$ as a index for discrete time step or continuous time. \n3. In Eq (3.1), $h(r)$ should be $h_1(r)$. \n4. I understand that $NSR$ is monotone, but why is it strictly monotone? Namely, how to guaratee the existence of its inverse function. \n5. Why the authors say in Figure 1 that the proposed algorithm outperforms DDIM and DPM solver? \n6. Where is $t_i$ defined? \n7. Maybe this is a dumb question. Why can we assume the neural network is differentiable? I would imagine this is not the case when the activation function is ReLU. \n8. This sentence is hard to parse. \"This method recursively extracts the hidden lower-order derivative information of the higher-order derivative terms in the Taylor expansion of the score-integrand at the required point, as illustrated in Figure 2\". I would suggest the authors present their most general form algorithm in the format of Algorithm 1 and 2. \n9. Could the authors elaborate on what Figure 2 is trying to illustrate? In particular, why some blocks are colored in red and the others in blue? Why do you call the first row Taylor series and the second row Weighted sum? \n10. In Theorem 3.2, $m$ has to be larger than 2 or 3? \n11. The legend in Figure 3 is a bit misleading. I assume the first four ones are for dashed lines, but it is not apparent at first glance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4270/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4270/Reviewer_gjHa",
                        "ICLR.cc/2024/Conference/Submission4270/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4270/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133187417,
            "cdate": 1699133187417,
            "tmdate": 1700668585636,
            "mdate": 1700668585636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Is9AT2nkUe",
                "forum": "85Af6AcMo5",
                "replyto": "D4MY8uBvoi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer gjHa,\n\nThank you for the detailed review and valuable feedback. Your comments are helping improve our work, many thanks. Below, we address specific questions you are concerned about.\n\n### Q1: Is there any acceleration algorithm for diffusion SDE as well? ... to see the authors providing a discussion.\n\nThanks for providing such interesting topic. We observed that 'dpm-solver++' has introduced a dedicated accelerated iterative scheme specifically designed for diffusion SDE.\n\nThe most significant difference between diffusion SDE and diffusion ODE lies in the presence of stochastic term. In diffusion SDE, once the coefficient function of the stochastic term is determined, the remaining integral term is equivalent to solving an ODE, because the integration of stochastic term can be estimated using the mean formula and variance formula of It\u00f4 integration. Therefore, there are two key aspects for diffusion SDEs. \n- The first involves finding a balance between the coefficient function of the stochastic term and the coefficient function of the Score function term. \n- The second aspect pertains to devising an algorithm capable of accommodating diffusion ODEs with deterministic variance perturbations.\n\n[1] Lu et al., DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models.\n\n### Q2: how do you define $f(t)$ ..., It is unclear...use\u00a0$t$ as a index for discrete time or continuous time.\n\nThanks for the detailed review. We have corrected the expression in the Background and provided detailed explanations in Appendix A.1. We clarify that $\\alpha_t=\\prod_{i=1}^t \\beta_i$ serves as the schedule for DDPM. From a discrete perspective, your thought is correct. For $f(t)$, we follow the generalized schedule defined by Kingma et al.; the details refer to Appendix A.1. Our theoretical framework is established in continuous time. Nevertheless, our algorithms can also be applied to discrete-time diffusion models, thanks to the groundwork laid by DPM-Solver. Details of the conversion from continuous time to discrete time are provided in Appendix G.\n\n[2] Kingma et al., Variational Diffusion Models, NeurIPS 2021.\n\n[3] Lu et al., Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps, NeurIPS 2022.\n\n### Q3: In Eq (3.1),\u00a0$h(r)$\u00a0should be\u00a0$h_1(r)$.\n\nThanks for the very detailed review. We carefully examined the entire text and corrected it.\n\n### Q4: ...why $NSR$ is it strictly monotone?\n\nUnder the variance-preserving setting, $\\sigma_t^2 +\\alpha_t^2 = 1$. Since $\\sigma_t$ is strictly monotonic, $NSR(t)=\\frac{\\sigma_t}{\\alpha_t} $ is also strictly monotonic. The inverse of $NSR(t)=\\frac{\\sigma_t}{\\alpha_t}$ is provided in Appendix G3.\n\n### Q5: Why ... in Figure 1 that the proposed algorithm outperforms DDIM and DPM solver?\n\nThanks for the detailed question. For a single image, we often lack a reasonable method to compare its generation quality, as each algorithm's generated image may not necessarily be the optimal image under that specific noise. In such cases, we can only evaluate based on the details and clarity of the generated images. Therefore, in Figure 1, we can assess based on the details and clarity of the generated samples. For instance, the samples produced by our algorithm at 15 steps are clearer than those generated by DDIM at 15 and 20 steps, and are even comparable to those produced by DDIM at 50 steps.\n\n### Q6: Where is\u00a0$t_i$\u00a0defined?\n\n$t_i \\in[0,1]$ represents the time at the $i$-th moment.\n\n### Q7:\u00a0Why can we assume the neural network is differentiable? ... is not the case when the activation function is ReLU.\n\nThanks for the thoughtful question. The differentiability allows us to compute the gradient of the loss function with respect to the network parameters, enabling parameter updates. ReLU is indeed non-differentiable. We have provided a review study that we hope will be helpful to you for more details.\n\n[4] Dubey et al., Activation functions in deep learning: A comprehensive survey and benchmark,\u00a0Neurocomputing\u00a0(2022).\n\n### Q8-Q9: This sentence is hard to parse. \"...\". \u00a0I would suggest ...\n\nThanks for the valuable suggestion. In order to facilitate reader comprehension, we have rephrased the sentence into a paragraph and provided a detailed explanation. In the revised version, we illustrate this with an example using Eq. (3.7), where the colors annotated in the equation correspond to the colors in Figure 2. For the general form of the algorithm, we showcase it in the attached scire-solver.py.\n\n### Q10: In Theorem 3.2,\u00a0has to be larger than 2 or 3?\n\nIn fact, $m-1$ represents the number of recursive operations performed on how many higher-order derivatives. We recommend performing at least two recursive operations, so m should be greater than 2.\n\n### Q11: The legend in Figure 3 is a bit misleading. ... for dashed lines, but it is not apparent...\n\nThanks for the detailed review. We have made appropriate adjustments to make it look more apparent."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369250309,
                "cdate": 1700369250309,
                "tmdate": 1700470343523,
                "mdate": 1700470343523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HxRgAPcOFN",
                "forum": "85Af6AcMo5",
                "replyto": "Is9AT2nkUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4270/Reviewer_gjHa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4270/Reviewer_gjHa"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for the detailed response. A good proportion of my concerns are addressed, hence I increase my rating from 5 to 6. \n\nHowever, I still feel hiding the general form of the algorithm in Python script causes a lot of confusion, maybe the authors should consider improving their manuscript by presenting a the algorithm instead in the main paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4270/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668895404,
                "cdate": 1700668895404,
                "tmdate": 1700668895404,
                "mdate": 1700668895404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]