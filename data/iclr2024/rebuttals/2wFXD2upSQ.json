[
    {
        "title": "A Demon at Work: Leveraging Neuron Death for Efficient Neural Network Pruning"
    },
    {
        "review": {
            "id": "Os7FOjnGpk",
            "forum": "2wFXD2upSQ",
            "replyto": "2wFXD2upSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_tiWA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_tiWA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \"Demon's Pruning\" (DemP), an innovative method that utilizes dying neurons for model optimization, and provides a comprehensive exploration of hyperparameters influencing neuron mortality. The authors demonstrate DemP's superior performance over existing structured pruning techniques and its competitive results with unstructured methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Insightful exploration of dying neurons, establishing their utility in structured pruning algorithms.\n2. Introduction of the simple yet broadly applicable DemP, which outshines current structured pruning techniques.\n3. Extensive empirical validation of DemP's effectiveness across multiple benchmarks, with comparative analysis against other pruning methods."
                },
                "weaknesses": {
                    "value": "1. The paper's motivation is somewhat unconvincing. Encouraging neuron death during training may compromise the expressivity of neural networks, potentially leading to performance degradation. The authors, however, propose accelerating neuron death and subsequent pruning. What is the motivation to accelerate the neuron death? (I understand the authors' motivation, i.e., pruning, but why just have a \"normal\" speed of pruning as the neurons will die at the end of training with a high probability.) Besides, It's worth noting that several existing works already prune networks by removing small-weight \"dead\" neurons without the need for prompting [1, 2] in structured pruning and unstructured pruning, e.g., magnitude. \n2. the paper lacks direct evidence that dead neurons **remain inactive** during training, despite the existence of previous works validating this hypothesis, such as the overlap coefficient [3]. \n3. The experiments in Fig 2 suggest that noise may contribute to the accumulation of dead neurons, but its role appears to be more like an amplifier than a key driver. Moreover, the effect seems to depend on the noise type. The section seems more focused on expanding the word count which makes this paper inconsistent, and the experimental details on noisy updating are insufficient. Additionally, \n\nMinor:\n1. The unit of Dead Neurons Variation in Fig. 3 is unclear.\n\nReferences:\n1. Sokar, G., Agarwal, R., Castro, P. S., & Evci, U. (2023). The dormant neuron phenomenon in deep reinforcement learning.\u00a0_arXiv preprint arXiv:2302.12902_.\n2. Wang, H., Qin, C., Zhang, Y., & Fu, Y. (2020). Neural pruning via growing regularization.\u00a0_arXiv preprint arXiv:2012.09243_.\n3. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., & Zhang, C. (2017). Learning efficient convolutional networks through network slimming. In\u00a0_Proceedings of the IEEE international conference on computer vision. (pp. 2736-2744)."
                },
                "questions": {
                    "value": "see the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Reviewer_tiWA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698433061665,
            "cdate": 1698433061665,
            "tmdate": 1700686278979,
            "mdate": 1700686278979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "slyQg0pcoZ",
                "forum": "2wFXD2upSQ",
                "replyto": "Os7FOjnGpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tiWA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the accurate summary of our work and their positive feedback on the soundess and usefulness of our approach and the simplicity and  effectiveness  of our method. We also appreciate their concrete suggestions to better articulate the motivation of our approach and to strengthen some of claims with further evidence. \n\n************************Weaknesses:************************\n\n1.  **On the motivation for accelerating neural death** \n\nThank you for pointing out this confusing aspect of our paper. The motivation to accelerate and promote neural death is threefold:\n\n *  It alleviates the need for a score function and pruning intervention because the learning dynamics reach directly a sparser solution. It is also a notable difference from previous existing works. We now discuss further this point in the introduction.\n * Actively promoting neural death and dynamically removing the dead units can lead to training speedup. We added training speedup results in Table 1 to better highlight this aspect.\n * Because the updates are proportional to the loss, the update size diminishes as the model converges, significantly affecting the dying speed. Actively promoting neural death ensures that dying occurs in a fixed number of times steps. We added Appendix D1 to clearly expose this aspect.\n\n2. **Evidence that inactive neurons remain inactive**\n\nThank you for pointing out this omission and the great suggestion. As per  the reviewer\u2019s suggestion, we complemented our previous results justifying dynamic pruning (now Appendix H1) with measurements of the overlap ratio in a new Appendix C (Fig. 8). We have added a reference to the latter to section 3.1.\n\n3.  **On the role of noise** \n\nThank you for highlighting the apparent inconsistency. Section 3.1 introduces \u2014 and tries to provide intuition behind \u2014 the driving hypothesis of our paper: Does the noise level during optimization affect the solution sparsity via dead neuron accumulation?  Section 3.1 and Figure 2 show that SGD noise structure has the potential to amplify dead neuron accumulation, motivating further investigations. The comparison with Gaussian noise highlights the asymmetric difference between the two noise structures, an important aspect that is also part of our thought experiment.\n    \nWe reworked thoroughly the intuition in section 3.1 and brought further clarification in the Figure 2 caption as well to better justify this section.\n    \n\n******Minor:******\n\n1. Thank you, we clarified Figure 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542544041,
                "cdate": 1700542544041,
                "tmdate": 1700542544041,
                "mdate": 1700542544041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uq513dlC5c",
                "forum": "2wFXD2upSQ",
                "replyto": "slyQg0pcoZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Reviewer_tiWA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Reviewer_tiWA"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments from Reviewer tiWA"
                    },
                    "comment": {
                        "value": "Thanks for the response! Most of my questions are efficiently addressed. However, regarding motivation: I still hold the belief that neuron death is a trade-off in sparse training and performance improvement because the plasticity loss will incur the decreased performance.  \n\nIn other words, you can not accelerate the neuron death and maintain the performance over the training process at the same, where I also found an interesting paper that maintains this intuition in submission of this conference, but with some interesting techniques to ensure performance improvement during the sparse training from the perspective neuron deaths. I would like to increase to 5 points but a lower confidence."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686264028,
                "cdate": 1700686264028,
                "tmdate": 1700686264028,
                "mdate": 1700686264028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s56ZE2exKx",
                "forum": "2wFXD2upSQ",
                "replyto": "Os7FOjnGpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further discussion with Reviewer tiWA"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful reconsideration and adjustment of our score.\n\nWe want to emphasize our agreement with the reviewer's point about the inherent sparsity-performance tradeoff. Achieving a favorable balance, where high sparsity levels are attained with minimal impact on performance, is a central challenge faced by all pruning methods. Our paper extensively evaluates and compares these tradeoffs, as illustrated in Figures 4-7. Our results demonstrate a more advantageous tradeoff for our method compared to the strongest competitors in the recent  literature of structured pruning, despite the simplicity of our pruning criterion.\n\nWe appreciate the reference to another current submission at ICLR, but given the inherent variability in pruning methods' performance across different sparsity levels, a comprehensive quantitative comparison based on sparsity-performance curves would be needed for a fair assessment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695153957,
                "cdate": 1700695153957,
                "tmdate": 1700695241425,
                "mdate": 1700695241425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2GtXDOs0RI",
            "forum": "2wFXD2upSQ",
            "replyto": "2wFXD2upSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_i9Pr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_i9Pr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method \u201cDemon\u2019s Pruning\u201d (DemP) for structured pruning, which removes dead neurons during training. The paper studied the phenomenon of dying neurons during training and how the choices of hyperparameter configurations impact how dying neurons occur in neural networks. Experiment results on CIFAR-10 and ImageNet show the advantages of the proposed method DemP."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studied the setting of dying neurons during training, which is missing in existing structured pruning approaches.  \n2. The perspective of learning a structured sparse network during training is interesting and promising."
                },
                "weaknesses": {
                    "value": "1. No speedup evaluation is conducted. As a method for network pruning, it is expected to conduct real inference speedup evaluation over the original dense model.  \n2. It seems that the method is essentially a structured version of existing sparse training methods, e.g. RigL. It is not clear to me the technical contribution of this work.  \n3. The theory in section 3.2. seems not useful and redundant. It is not related to the method DemP itself."
                },
                "questions": {
                    "value": "1. It is not clear to me why we should compare a structured pruning method with unstructured pruning methods, e.g. in Table 1. Since DemP is a structured pruning method, we should compare with possibly more structured pruning baselines?\n2. Could the authors provide speedup evaluation of structured pruned models? \n3. Could the authors illustrate clearly the difference of the \u201cdynamic pruning\u201d procedure in the paper and existing sparse training methods, e.g. RiGL and iterative magnitude pruning? It looks to me they are essentially the same except DemP is removing neurons while sparse training methods remove individual weights.\n4. Adjust the theory part in section 3.2 and explain how it is related to DemP. It seems right now they are just some fancy equations which is not helpful for understanding the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635202613,
            "cdate": 1698635202613,
            "tmdate": 1699636579487,
            "mdate": 1699636579487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lqxDnBJ7r9",
                "forum": "2wFXD2upSQ",
                "replyto": "2GtXDOs0RI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i9Pr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback on the presentation, soundness and novelty of our approach. More importantly, we thank the reviewer for their questions and concrete suggestions to clarify our contribution and improve the paper's presentation. \n\n********************Questions:********************\n\n1.  **Comparison with unstructured methods**. \n\nWe appreciate the reviewer's valuable input regarding the comparison of DemP with unstructured pruning methods in Table 1. Our initial decision to include unstructured baselines was motivated by a desire for consistency with existing literature [1, 2], aiming to highlight the trade-off between structured and unstructured pruning approaches. However, we acknowledge the importance of contextualizing our comparisons within the domain of structured pruning.\n\nTo address this concern, we have revised our manuscript in Section 5 to explicitly clarify the rationale behind including unstructured baselines. Additionally, in response to the reviewer's observation, we have relocated Figure 6 to Appendix I to maintain focus on the main results. Importantly, we want to emphasize that the structured baselines chosen for comparison represent, to the best of our knowledge, the top-performing dynamics pruning algorithms available.\n\nWe refrained from introducing additional baselines that may offer subpar performance to prevent overcrowding the figures, ensuring a clear presentation of the most relevant and competitive comparisons. We believe these adjustments enhance the overall clarity and relevance of our comparisons, aligning more closely with the structured pruning context.\n\n 2.  **More speedup results**\n\nIn line with the reviewer\u2019s suggestion,  we added in Table 1 extended results for training speedup as well as training/inference FLOPs for the ResNet-50 architecture on ImageNet. We intend to run analogous experiments for the smaller networks as well and add the corresponding results to the camera-ready version.  \n\n3.  **Difference with existing methods**\n\nThank you for this important question. Dynamic pruning is an overloaded expression, that we use in accordance to how it is used in RiGL paper. It consists of methods that gradually prune neural nets to recover a sparse model at the end of training under a fixed trained budget. It contrasts with methods doing single-shot pruning at the end of training before fine-tuning, or with iterative magnitude pruning that repeats multiple cycles of single-shot pruning after initial complete training. Apart from the structured nature of DemP, further differences are:\n\n * DemP acts on the learning dynamics to force the optimization process to generate a sparse solution. It does not require single or multiple pruning interventions during training.\n  *  It alleviates the need to design a score function to identify the neurons to remove, monitoring the activations retrieved during the forward pass is sufficient.\n    \n We appreciate the opportunity to further highlight the originality of our approach. We made the necessary modifications in our paper\u2019s introduction and in Section 4.\n\n4. **On the relation between DemP and the theorerical analysis**\n\nWe appreciate this important feedback.  Viewing the optimization process as akin to a biased random walk suggests that we can act on the learning dynamics to learn sparser solutions instead of relying on pruning intervention. DemP explores a single direction to affect the learning dynamics (regularization) while Section 3.2 was an attempt to further understand the impact of various hyperparameters on sparsity. We tried to clarify this connection by emphasizing this point of view in the introduction and section 4 and reworking the intuition. However we acknowledge that since the mathematical analysis is not central to the paper,  it tends to distracts away from the main point of the paper.  Following your suggestion, we recentered the discussion in section 3 on empirical evidence and moved the discussion of the Brownian motion model to Apppendix B, while  incorporating a reference to it in the intuition subsection of Section 3.1. We appreciate the opportunity to improve the flow and clarity of this analysis section. \n\n[1] John Rachwan, Daniel Z\u00fcgner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, Stephan G\u00fcnnemann: **Winning the Lottery Ahead of Time: Efficient Early Network Pruning.** ICML\u00a02022:\u00a018293-18309\n\n[2] Stijn Verdenius, Maarten Stol, Patrick Forr\u00e9: **Pruning via Iterative Ranking of Sensitivity Statistics.** CoRR\u00a0abs/2006.00896 (2020)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541846139,
                "cdate": 1700541846139,
                "tmdate": 1700541893955,
                "mdate": 1700541893955,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ywjvKAptfI",
            "forum": "2wFXD2upSQ",
            "replyto": "2wFXD2upSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_k7gu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_k7gu"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the dying neurons phenomenon in network pruning. By employing the random walk model for network parameters, it reveals that neurons that become inactive during training may be challenging to recover. The study also explores how different hyperparameter configurations impact the occurrence of dying neurons. The authors thus introduce the \"Demon\u2019s Pruning\" (DemP) to remove dead neurons in real time as they arise. This method dynamically prunes networks during training and outperforms some existing structured pruning techniques as shown in the experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Offers some insights into mechanisms of neuron mortality through the lens of network sparsity and pruning and provides analysis into the influence of gradient noise/learning rate/regularization. Experimental results support its findings to some extent.\n- The proposed pruning method seems simple, computationally efficient, and straightforward to implement."
                },
                "weaknesses": {
                    "value": "- The concept of pruning neural networks by eliminating inactive neurons based on their activation doesn't appear very novel. Several prior papers have explored the notion of dead neurons in sparse neural networks or proposed activation-based pruning methods [1,2,3].\n- The analysis section seems to oversimplify the problem. It remains a question whether this analysis, built on Brownian motion model of weights rather than some model of activation, can effectively explain the activation-based pruning method.\n- The paper is generally easy to follow, but certain sections could benefit from additional details to make it more self-contained and less confusing. For instance, providing background information on Brownian motion, discussing implicit assumptions when using absorbing Brownian motion model, and offering a clear definition of 'dead neurons' in the context of convolutional neural networks would enhance clarity.\n\n[1] Hu, Hengyuan, et al. \"Network trimming: A data-driven neuron pruning approach towards efficient deep architectures.\" *arXiv preprint arXiv:1607.03250* (2016).\n\n[2] Whitaker, Tim, and Darrell Whitley. \"Synaptic Stripping: How Pruning Can Bring Dead Neurons Back To Life.\" *arXiv preprint arXiv:2302.05818* (2023).\n\n[3] Liu, Shiyu, Rohan Ghosh, and Mehul Motani. \"AP: Selective Activation for De-sparsifying Pruned Networks.\" *Transactions on Machine Learning Research* (2023)."
                },
                "questions": {
                    "value": "- I'm curious about the definition of 'dead neurons' in convolutional neural networks and what specific structures DemP will remove.\n\n- I'm also interested in understanding why a one-dimensional absorbing Brownian motion can effectively represent the weight dynamics of neural networks. Does the behavior of weights align with the assumptions of this model?\n\n- The experiments employ a one-cycle scheduler for the regularization parameter. Is the comparison with other baseline methods also using the same regularization? It's important to consider that regularization may impact model performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Reviewer_k7gu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687799266,
            "cdate": 1698687799266,
            "tmdate": 1701071705157,
            "mdate": 1701071705157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b3919CSFl1",
                "forum": "2wFXD2upSQ",
                "replyto": "ywjvKAptfI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k7gu"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the accurate summary of our work and for recognizing the simplicity and efficiency of the method we propose. We also appreciate the questions and constructive feedback to improve the flow of the paper. \n\n**Novelty of Our Approach:**\nWe appreciate Reviewer K7GU's observation that the concepts of dead neurons and activation-based pruning methods have been explored in prior literature. However, we want to emphasize the distinctiveness of our contribution. To the best of our knowledge, our work is the first to demonstrate the viability and effectiveness of pruning based solely on units inactive across the entire dataset. This novel approach stems from our insights into the phenomenon, where we identified specific hyperparameters that make dead neurons prevalent enough for a successful pruning algorithm.\n\nUnlike previous methods [1,2,3], our approach uniquely ensures that the pruning action does not disrupt the learning dynamics, eliminating the need for recovery interventions. While [2] and [3] focused on restoring capacity through pruning negative weights, their emphasis aligns more closely with studies addressing plasticity loss and subsequent restoration [4,5].\n\nIn reponse to the reviewer\u2019s comment,  we further highlighted  these distinctive aspects in the revision,  in both the introduction and Section 4, providing a comprehensive context for the originality of our approach. We appreciate the opportunity to clarify these nuances.\n\n********************Questions:********************\n\n1. **Dead neurons definition for convolutional layers**. Thank you for pointing out this oversight.  We have added an explicit definition in Appendix A,  referenced in section 3 when we introduce the general definition. In convolutional layers, ReLU is applied element-wise to the pre-activation feature map. We consider an individual neuron (filter) dead if all elements of the feature map post-activation are 0.\n2. **Brownian motion model**. The purpose of this model is to highlight in a simple setting the specific role played by the *noise structure* in the weights dynamics. Specifically,  the absorbing Brownian motion models a system subject to noise with ReLU activation structure;  and our analysis highlights how such a structured noise induced sparsity, as the weight ends up in the inactive region with probability 1. \n    \n    We believe that this insight is important and reveals the pivotal role played by the stochasticity of the learning algorithm in the occurrence of dead neurons, as also illustrated in our empirical observations in Fig 2. However we acknowledge this is not central to the paper. As per Reviewer\u2019s feedback,   (also per Reviewer i9Pr\u2019s suggestion) we\u2019ve moved the discussion of the Brownian motion model to Apppendix B and incorporated a reference to it in the intuition subsection of Section 3.1. We appreciate the opportunity to improve the flow and clarity of this analysis section.\n    \n\n1. **On the use of DemP\u2019s regularization schedule for the baselines**. Thanks for highlighting this confusing aspect. Our baselines trained for comparison do not use the one-cycle regularization schedule used by DemP, but rather the constant weight decay suggested by the training recipes. We clarified this important aspect in Section 5 and in Appendix J1-J4.\n\n[1] Hu, Hengyuan, et al.: **Network trimming:** **A data-driven neuron pruning approach towards efficient deep architectures.**\u00a0*arXiv preprint arXiv:1607.03250*\u00a0(2016).\n\n[2] Whitaker, Tim, and Darrell Whitley: **Synaptic Stripping: How Pruning Can Bring Dead Neurons Back To Life.**\u00a0*arXiv preprint arXiv:2302.05818*\u00a0(2023).\n\n[3] Liu, Shiyu, Rohan Ghosh, and Mehul Motani: **AP: Selective Activation for De-sparsifying Pruned Networks.**\u00a0*Transactions on Machine Learning Research*\u00a0(2023).\n\n[4] Ghada Sokar,\u00a0Rishabh Agarwal,\u00a0Pablo Samuel Castro,\u00a0Utku Evci: **The dormant neuron phenomenon in deep reinforcement learning.**\u00a0ICML\u00a02023:\u00a032145-32168\n\n[5] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, Marlos C. Machado: **Loss of Plasticity in Continual Deep Reinforcement Learning.** CoRR\u00a0abs/2303.07507 (2023)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540326158,
                "cdate": 1700540326158,
                "tmdate": 1700540326158,
                "mdate": 1700540326158,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OaIxoZ2gm0",
            "forum": "2wFXD2upSQ",
            "replyto": "2wFXD2upSQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_YsTs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5614/Reviewer_YsTs"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript provides mathematical and empirical support for the authors' argument that neuronal death is governed by settings of common hyperparameters (batch size, regularization strength, etc.). They leverage this observation by choosing hyperparameter settings that prompt neuron death, allowing them to prune such neurons to obtain training speedups. The results improve on those of other structured pruning algorithms on ResNet18, VGG16, and ResNet50."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Relative to other structured pruning algorithms, the proposed pruning approach (DemP) is more accurate and providing of stronger speedups. \n\nThe idea to prune dead neurons for efficient training is, as far as I know, original. If it can be shown to be helpful in more contexts (see \"Questions\" below), this method's simplicity and intuitive justification will make it impactful. \n\nThe manuscript provides a mathematical analysis in Section 3.2 that provides further intuition for the neuron death problem and its relationship to training hyperparameters, which is generally helpful. \n\nThe experiments (those analyzing neuron death and those analyzing DemP) are well designed and clear -- e.g., baseline structured pruning methods that are compared to DemP are thoughtfully chosen."
                },
                "weaknesses": {
                    "value": "The experiments left unclear the practical relevance of DemP. As discussed more thoroughly below (see \"Questions\"), training in a wider variety of contexts and using more competitive training setups as baselines will help clarify whether DemP provides speedups that are relevant to readers' work/research."
                },
                "questions": {
                    "value": "Score-affecting:\n\n1. More results on speedups from DemP would be great to see. For instance, using your method, can you improve on the Mosaic ML ResNet50 ImageNet training time result? (See https://docs.mosaicml.com/projects/composer/en/stable/tutorials/train_resnet50_on_aws.html.) \n2. ResNet18 typically gets ~95% accuracy on CIFAR10 but does not in your experiments. Could you please explain the hyperparameter choices causing the gap? \n   - If the decrease in accuracy is caused by the regularization approach you use to encourage sparsity, that is seemingly a limitation of the proposed method that should be stated clearly. \n3. In the main text, please make clear that the baseline methods compared to (e.g., EarlyCroP) are not using the (potentially suboptimal) regularization schedule required by DemP. I believe they are not based on my reading of Sections F.1 and F.3, but please correct me if I am wrong.\n4. Is DemP effective on more modern architectures like ViTs or language models? Exploring transformer models need not require a significant increase in compute (e.g. results on GPT-2 Small would be interesting and that model is not too much larger than ResNet50).\n5. The fact that unstructured pruning baselines aren't well tuned, which is noted in the manuscript, should coincide with more cautious claims about performance relative to unstructured pruning. \n   - A well tuned magnitude pruning algorithm on ResNet-18 using CIFAR10 data actually improves baseline accuracy (95% accurate) at 95% sparsity (96% accurate); see Figure 1 of Diffenderfer et al. (2021).\n   - Consider complementing Figure 6 with more details (in the main text) on the unstructured algorithms to provide needed context for their performances. \n   - Relative to unstructured pruning, the benefit of DemP to emphasize is probably its ability to provide speedups (as opposed to its ability to match weakly-tuned unstructured pruning algorithms on accuracy).\n6. It would be great to see ImageNet accuracy at a few different DemP sparsity levels in a figure that supplements Table 1 (i.e., show the accuracy-efficiency frontier created by DemP). \n7. In Table 1, train your own baselines (at least for the dense model). \n   - Right now, it's unclear what to attribute the gap between Dense and DemP to (training code differences or DemP's effect).\n\n\nImportant:\n1. Figures should be closer to where they are discussed. E.g., Figure 4 is two pages away from where it's explained. \n\nMinor:\n\n1. The second paragraph of Section 3.1 should be made clearer -- where are the 3904 neurons that are referenced coming from?\n2. The Maxwell's demon analogy in the intuition section is slightly unclear, perhaps clean this up a bit to avoid confusion. \n   - If I understand correctly, the demon in that thought experiment is needed because there's otherwise a lack of a mechanism for entropy decreasing. When it comes to a neuron's transition to death/life, however, there are actual mechanisms (e.g., learning rate size) that govern transitions; no hypothetical demon is needed. Perhaps the function of the analogy is to clarify that, before the present manuscript, neuron death transition was treated too much like a black box (or demon). In any case, I suggest revising the \"Intuition\" paragraph that discusses this analogy. \n3. At the top of page 5, a simplified version of Equation 4 is referenced, I think \"3\" is meant.\n4. In equation 5, is eta in the denominator or numerator of the exponent? I think it's the latter but the typesetting leaves this unclear. \n5. The last sentence of the third to last paragraph on page 5 is unclear, I think an \"=0\" is missing. \n6. Figure 7 has an incorrect caption.\n7. On page 8, you reference Table 5, I think you mean Table 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5614/Reviewer_YsTs",
                        "ICLR.cc/2024/Conference/Submission5614/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829766524,
            "cdate": 1698829766524,
            "tmdate": 1700706167717,
            "mdate": 1700706167717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SPC7FU6Fgo",
                "forum": "2wFXD2upSQ",
                "replyto": "OaIxoZ2gm0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YsTs (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their accurate summary of our work. We also thank the reviewer for insightful comments and concrete suggestions to improve the manuscript. We try to address as best as we can the issues raised below:\n\n1.  **More speedup results**.  In line with the reviewer\u2019s suggestion,  we added in Table 1 extended results for training speedup as well as training/inference FLOPs for the ResNet-50 architecture on ImageNet. We intend to run analogous experiments for the smaller networks as well and add the corresponding results to the camera-ready version.  Regarding the reviewer\u2019s suggestion to refer to Mosaic ML\u2019s results,  it is worth noting that these leverage a number of methods and tricks, such as  FFCV [1], in addition to pruning, so that fair,  direct comparisons are not straightforward. Moreover, the provided framework requires more computational resources than what we use in our experiments. Finally, it is only available in Pytorch where our code relies on JAX.\n\n2. **Performances on CIFAR 10**. We appreciate the reviewer\u2019s feedback, which highlights the need for clarifications in our manuscript.  First, it\u2019s important to note that we report the test accuracy at the end of the training, while multiple works report the best accuracy reached during training (e.g., [3] sec. 5).  Second, while SGD+momentum typically gives better accuracies, we focused our experiments on ADAM because of its popularity and its inherent alignment with our method (see Appendix E). It is also the choice made by our strongest  structured pruning baseline [2].  The performance of our experiments seems aligned with the reported results in [3] (Fig. 5) for the ADAM optimizer. \n\n3. **On the use of DemP\u2019s regularization schedule for the baselines**. You are correct, our baselines do not use the (potentially suboptimal) regularization schedule used by DemP. We clarified this important aspect in Section 5 and in Appendix J1-J4.\n\n4. **DemP on transformers**. While we acknowledge the importance of investigating the application of our approach to transformer-based models, we would like to highlight that our benchmarking was deliberately aligned with recent prior work on the topic (e.g., [2,4]). This alignment allows for a meaningful comparison with prior work, especially given our focus on gradual pruning during training to reduce computational costs.\n\n5. **On our comparison with unstructured pruning baselines**. We thank the reviewer for these important comments and suggestions. We note that Diffenderfer et al. (2021) are targeting a different setup where pruning is done in a single shot after regular training, with additional fine-tuning to recover performance afterward. The goal we pursue with our method is to do dynamic pruning under a fixed training budget to reduce the computational load for learning the model. Also, they achieved this performance with SGD + momentum, while we used ADAM (see reply to question 2).\n    \n We agree with the reviewer that, relative to unstructured methods, the benefit of DemP is its ability to provide speedup. We included a quick comparison to unstructured methods that also prune the model throughout training to expose that the perceived trade-off between structured and unstructured methods (unstructured achieve better performance) is not always obvious and can require extensive fine-tuning. It also aligns with prior work [2].\n    \n Following your suggestion, we clarified further Fig. 6 with the points raised above. As per Reviewer i9Pr\u2019c comment (question 1), we also moved this figure  (Figure 16 in the new revision) to Appendix I to focus the content on structured methods.\n    \n6. **More ImageNet results at different DemP sparsity levels**  Thank you for this great suggestion. We complemented Table 1 with further results, adding a comparison between methods at 90% sparsity as well. We also included the suggested figure in the revision (see Figure 7). \n\n7.  **Implementation of Dense model**  Thanks again for pointing this inconsistency to our attention. Following your suggestion,  we have trained our own dense baseline and added the results to Table 1.\n\n**Please continue to the comment below**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536465304,
                "cdate": 1700536465304,
                "tmdate": 1700536465304,
                "mdate": 1700536465304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WREg1g6bag",
                "forum": "2wFXD2upSQ",
                "replyto": "OaIxoZ2gm0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5614/Reviewer_YsTs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5614/Reviewer_YsTs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for adding the new results and clarifications! The experiments appear more convincing now, and the speedups are particularly nice to see. \n\nIn main text Figures like 4b, 5b, 6b, and 7b (especially Figure 6b), I think a plot of training speedup vs. \"neuron sparsity\" would be more compelling than plotting Accuracy vs. \"weight sparsity\" (inference speedup results would be great too). For example, if you show a scenario where accuracy is not significantly harmed (<1% difference) and training time is improved (e.g., by 5+%), that would increase the relevance of this paper to the efficient training literature (e.g., see \"Compute-Efficient Deep Learning\", Bartoldson et al., 2023). Relatedly, extending the experiments to show speedups on transformer training could greatly raise the contribution of this manuscript.\n\nI have raised my score. Going forward, I believe this paper can attain a significantly broader impact by expanding its focus in the suggested ways (improving on the MosaicML or another benchmark ResNet50 time, showing applications to transformer training, etc.)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706152217,
                "cdate": 1700706152217,
                "tmdate": 1700706829279,
                "mdate": 1700706829279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]