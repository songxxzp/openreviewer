[
    {
        "title": "Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations"
    },
    {
        "review": {
            "id": "JPpiGbNaQQ",
            "forum": "6pPYRXKPpw",
            "replyto": "6pPYRXKPpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_DMVD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_DMVD"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new benchmark for imitation learning with a focus on evaluating diverse behaviors. The authors perform an extensive comparison of different imitation learning algorithms, ranging from deterministic to stochastic algorithms using MLPs and transformers along with interesting insights about state-based and image-based policies and other algorithmic aspects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a new benchmark for imitation learning along with appropriate metrics to quantitatively evaluate diverse behaviors.\n- The paper performs ablation studies and provides good insights about state-based vs image-based policies, the impact of history and action prediction horizon, and learning with less data.\n- The paper provides some results that are consistent across methods - (1) transformers improve performance over using MLPs, (2) historical inputs enhance the performance of transformer-based policies, and (3) transformers exhibit superior performance in the low data regime.\n- The paper has tasks of varying difficulty with a task like stacking-3 which is not satisfactorily solved by any of the existing algorithms. This provides a scope for improvement."
                },
                "weaknesses": {
                    "value": "- Based on the results, it seems like all tasks can be solved by existing methods except one. Though this gives some scope for improvement on the algorithmic side, I believe just a single variant of a task remaining unsolved might not be a very useful for future works considering this benchmark for evaluations. It would be great if the authors could include other tasks or provide functionalities for adding new tasks.\n- It would be great if the authors could provide code since a benchmark is only useful if the code is available."
                },
                "questions": {
                    "value": "It would be great if the author\u2019s could address the points mentioned in \u201cWeaknesses\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6147/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6147/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6147/Reviewer_DMVD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648817011,
            "cdate": 1698648817011,
            "tmdate": 1700671914860,
            "mdate": 1700671914860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DtpwGjoKOj",
                "forum": "6pPYRXKPpw",
                "replyto": "JPpiGbNaQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their feedback on our paper. We would like to address the mentioned weaknesses:\n\n>Based on the results, it seems like all tasks can be solved by existing methods except one. Though this gives some scope for improvement on the algorithmic side, I believe just a single variant of a task remaining unsolved might not be a very useful for future works considering this benchmark for evaluations. \n\nWe kindly express some uncertainty regarding the specific definition the reviewer uses to characterize a task as solved. Assuming the criteria for considering a task as solved is a success rate exceeding 0.8, we acknowledge that two tasks, Sorting (T4) and Stacking (T5), remain unsolved for both state- and image-based policies in our study.\n\nIt's important to highlight that one of the strengths of our work lies in introducing an additional performance metric, namely *behavior entropy*. This metric provides additional dimensions for improvement across tasks.\n\n---\n\n>It would be great if the authors could include other tasks [\u2026]\n\nWe appreciate the reviewer's suggestion and concur with the idea that the inclusion of more tasks enhances the quality of our work. Accordingly, we have introduced two additional tasks, *Inserting (T6) and Arranging (T7).*\n\n*Inserting (T6)* is conceived as a more challenging variant of the Pushing (T3) task, encompassing more blocks, increased behavior-level multi-modality, and demanding more dexterity. We have collected over 800 demonstrations using a gamepad controller. Preliminary results indicate that this task presents room for improvement across current methods.\n\n*Arranging (T7)* involves organizing objects on a table, such as flipping a cup. We are actively engaged in data collection utilizing our augmented reality system, and while preliminary results might not be available immediately, we aim to provide them during the rebuttal phase.\n\nWe included task descriptions and the preliminary results in a revised version of our paper which are highlighted with blue color and can be found in Appendix D. We are committed to incorporating these tasks, along with a comprehensive evaluation of all methods, in the final camera-ready version of our work.\n\n---\n\n\n>It would be great if the authors could provide code [\u2026]\n\nWe have included the code base as a .zip file in the supplementary material. Additionally, we have provided a link to an anonymous GitHub repository, accessible here: https://github.com/d3iltest/Temporary_D3IL.\n\n---\n\n\n>[\u2026] or provide functionalities for adding new tasks.\n\nThe code base is accompanied by a comprehensive readme. This readme provides detailed instructions on how to add new tasks to the framework. Specifically, it guides users through the process of setting up a new environment and recording data using a gamepad.\n\n\nWe express our gratitude to the reviewer for their valuable comments and suggestions. We are pleased to address any additional questions or concerns that may arise.\n\n---\n\n[1] Diffusion Policy: Visuomotor Policy Learning via Action Diffusion, RSS \u201823\n\n[2] Goal-Conditioned Imitation Learning using Score-based Diffusion Policies, RSS \u201823"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160317616,
                "cdate": 1700160317616,
                "tmdate": 1700160317616,
                "mdate": 1700160317616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UxQN5qgxSW",
                "forum": "6pPYRXKPpw",
                "replyto": "JPpiGbNaQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6147/Reviewer_DMVD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6147/Reviewer_DMVD"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal. My concerns have been sufficiently addressed and I am raising my score from 5 -> 8."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671872825,
                "cdate": 1700671872825,
                "tmdate": 1700671929193,
                "mdate": 1700671929193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZsSqiOxA1g",
            "forum": "6pPYRXKPpw",
            "replyto": "6pPYRXKPpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_S4Qj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_S4Qj"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a benchmarking for imitation learning from human demonstrations. Compared to other released benchmark datasets, the authors place emphasis on using human demonstrations and those demonstrations covering diverse behaviors.  The argument for doing so is because human demonstratinos inherently have some noise if the teleoperator differs, if people have different levels of expertise, etc. The goal is also to propose a quantitative measure of diverse behavior.\n\nThe proposal for this is to assume there exists a space $\\mathcal{B}$ of discrete behavior descriptions $\\beta \\in \\mathcal{B}$ (i.e. for pushing, whether we are pushing red to red or red to green and in what order). Our demonstrations define a $p(\\beta)$ distribution of how often different behaviors appear. A learned policy $\\pi$ will induces its own $\\pi(\\beta)$ distribution of behaviors, and similarity is measured by $KL(\\pi(\\beta) || p(\\beta))$. This is simplified to a uniform distribution for $p(\\beta)$ in all experiments, with entropy scaled to lie in range $[0,1]$. This reduces to $H(\\pi) = - \\sum \\pi(\\beta) \\log_{|B|}\\pi(\\beta)$\n\nDiversity is further defined as achieving many behaviors from the same initial state $s_0$, giving the conditional behavior entropy of\n\n$$\nH(\\pi(\\beta|s_0)) \\approx -\\frac{1}{S_0} \\sum_{s_0} \\sum_{\\beta} \\pi(\\beta|s_0) \\log_{|B|} \\pi(\\beta|s_0)\n$$\n\nSince the $p(s_0)$ distribution is unknown, this is approximated by Monte Carlo estimates using $S_0$ samples of the initial state.\n\nThe proposed benchmark is implemented in MuJoCo using a Panda robot and mostly consists of block moving tasks. A variety of imitation learning methods are tried, varying from pure MLPs to history-aware methods and diffusion policies. Experiments are also conduted on history length and on size of dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors run an extensive series of benchmark methods over their proposed environments. The environments are described in sufficient detail to reproduce them. The evaluation protocol and model selection criteria is documented well, a rarity in robot learning papers. The paper is also written quite clearly."
                },
                "weaknesses": {
                    "value": "The fact that different behaviors must be enumerated ahead of time is a large limitation of the proposed behavior-level entropy measure. I also found the KL-divergence to be a bit unmotivated. This paper assumes the demonstration dataset is always uniformly distributed among all behaviors, but in cases where the demonstration dataset is not uniformly distributed, it's not clear to me if KL divergence is the right measure to use. (We would expect a good learning method to have low KL, but if the demo dataset is skewed, we may still prefer a policy that is uniform across behaviors, even if this has higher KL than identicall matching the skewed distribution.) It seems like the entire discussion about KL is pointless and it would be more straightforward to just use the behavior entropy definition.\n\nSetting aside this for the moment, the paper also does not ever make a claim that multimodal policies would be good. Success rate need not be correlated with high behavior entropy - as argued by the experiment results, deterministic policies can still achieve okay success rate without diverse behavior. And hypothetically, you could have a 100% success rate policy that only follows a singular behavior $\\beta$. Such a policy may even be preferred (i.e. in factory automation, repeatable behavior given initial conditions is desired.)\n\nArguably, the paper is just about measuring this quantity, rather than arguing why it matters, but I would have appreciated some argument on this front."
                },
                "questions": {
                    "value": "Overall I feel the paper is okay, despite the flaws it does make some strides towards focusing on diversity of behavior. But could the authors comment on where conditional imitation learning falls into the picture. In the pushing task for example, if the 4 behaviors are known ahead of time, you could imagine conditioning the policy on a 1-hot with 4 values for \"push X1 to Y1, push X2 to Y2\", and that would allow a deterministic policy to achieve any of the behaviors assuming a perfect learning method. What is the argument for why we cannot or should not do something where we provide additional context to the policy on how we want the task to be performed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710336111,
            "cdate": 1698710336111,
            "tmdate": 1699636666298,
            "mdate": 1699636666298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4uwWckCUxG",
                "forum": "6pPYRXKPpw",
                "replyto": "ZsSqiOxA1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work and the many helpful comments and suggestions. We are committed to addressing your questions and concerns.\n\n---\n\n> The fact that different behaviors must be enumerated ahead of time is a large limitation of the proposed behavior-level entropy measure.\n\nWe acknowledge the reviewer's concern about the limitation imposed by the prerequisite to enumerate different behaviors in advance for the proposed behavior-level entropy measure. However, quantifying diverse behavior is a challenging problem, for which, to the best of our knowledge, exists no approach that circumvents this requirement. Moreover, there exists no task suite that provides any metric for quantifying diverse behaviors as highlighted in Table 1 of the paper. In light of this, we believe that our work makes a valuable contribution to the field.\n\n---\n\n> I also found the KL divergence to be a bit unmotivated [...] It seems like the entire discussion about KL is pointless and it would be more straightforward to just use the behavior entropy definition.\n\nWe agree with the reviewer that directly introducing the entropy improves the paper\u2019s clarity and readability. We thank the reviewer for their assessment and updated a revised version of our manuscript that does not introduce the KL divergence. These changes are colored in magenta and can be found on page 3.\n\n---\n\n> [...] the paper also does not ever make a claim that multimodal policies would be good [...]\n\nWe thank the reviewer for their suggestion to enhance the motivation for policies capable of learning diverse behavior.  In response, we have incorporated additional points and references into the introduction of our paper. These changes are colored in magenta and can be found on page 1. \n\nAdditionally, we summarized these points below:\n\n1. **Improving Generalization:** If the learned policy overfits to a specific set of demonstrated behaviors, it may not generalize well to new situations. By exposing the model to diverse behaviors, the risk of overfitting is reduced, and the learned policy is more likely to capture the underlying principles of the task rather than memorizing specific trajectories [1, 2, 3].\n2. **Enhancing Skill Transfer:** Learning diverse behaviors facilitates better skill transfer across different but related tasks. If the agent can imitate a wide range of behaviors, it is more likely to possess a set of skills that can be applied to various tasks, making it a more versatile and capable learner [1, 3].\n3. **Unpredictability in competitive Games:** Predicting an opponent\u2019s strategy in competitive games, such as table tennis, becomes much harder if the adversary has a diverse set of skills [4].\n\n---\n\n> What is the argument for why we cannot or should not do something where we provide additional context to the policy on how we want the task to be performed?\n\nThe primary argument against adopting goal-conditioned imitation learning (GCIL) [5, 6, 7] lies in the labor-intensive nature of context provision. Integrating contextual information often entails human annotation, a resource-intensive and challenging process to scale, particularly with extensive, unannotated datasets. We believe that the findings from our research contribute to mitigating this dependence on additional labeling efforts by highlighting methods that can learn diverse behaviors from datasets without human annotations.\n\n\nWe would like to thank the reviewers again for assessing our work. We would be delighted to address any additional questions or concerns they may have.\n\n---\n\n[1] Neural Probabilistic Motor Primitives for Humanoid Control, ICLR \u201819\n\n[2] InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations, NeurIPS \u201817\n\n[3] One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL, NeurIPS \u201820\n\n[4] Specializing Versatile Skill Libraries using Local Mixture of Experts, CoRL \u201821\n\n[5] Goal-conditioned Imitation Learning, NeurIPS \u201819\n\n[6] Goal-Conditioned Imitation Learning using Score-based Diffusion Policies, RSS \u201823\n\n[7] From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data, ICLR \u201823"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140032001,
                "cdate": 1700140032001,
                "tmdate": 1700140032001,
                "mdate": 1700140032001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0XPxKvL8h7",
            "forum": "6pPYRXKPpw",
            "replyto": "6pPYRXKPpw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_K6Tb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6147/Reviewer_K6Tb"
            ],
            "content": {
                "summary": {
                    "value": "As the interest in learning behaviors from natural human data raises, importance of imitation learning algorithms that can successfully learn from diverse and potentially multi-modal human behavior raises similarly. However, such algorithms are only recently gaining traction, and such there does not exist many benchmarks for properly comparing them. This situation is what D3IL aims to resolve, by creating a new benchmark to evaluate and compare imitation learning algorithms capable of learning multi-modal behavior.\n\nThe paper is divided the following sections: first the authors introduce the diversity metric used to evaluate the algorithms, which is an important component since the diverse, multi-modal behavior require a good notion of \"coverage\" of the behaviors. Then, they introduce the environments, baseline algorithms, and follow up by showing the performance of the algorithms and architectures on the tasks, both on terms of success rate and behavior diversity. Finally, they run a host of ablation experiments, such as limited data and impact of historical information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is a timely work since learning from diverse human data has shown major success in other fields such as natural language processing, and evaluation of candidate algorithm that can learn from diverse datasets is of vital importance at this moment. Here are the things this paper did well:\n\n1. The benchmark is quite principled, and both the success metric and the diversity metric are well justified while being intuitive and implementable.\n2. The list of baseline algorithms is also quite comprehensive, and covers the list of recent important developments in the space.\n3. The set of ablation experiments run covers the primary points of interest, such as dataset size, visual/state based models, and impact of history.\n\nOverall, this is a paper with a straightforward mission that achieves its goals well."
                },
                "weaknesses": {
                    "value": "The paper, while quite strong on the execution, has some major shortcomings that can be improved in the future.\n- A benchmark paper is useless without the environment codes and the data, which is absent from the supplementary materials. This is a major negative for this paper because we are being asked to judge it without being able to understand how easy it may be to run new algorithms against this benchmark.\n- Another primary criticism is that all five environments are very simple tabletop environments, and thus the complexity of the algorithm needed to solve that may not be quite high. A better benchmark would involve multiple kinds of environments, involving 2D/3D environments, potentially with different intractable elements.\n- One critical component missing from the evaluation is the required forward pass time or control frequency of the algorithms, which to my understanding is one of the largest disadvantages of diffusion-based models."
                },
                "questions": {
                    "value": "What is the action space used by the environments? In the diffusion policy paper they show that diffusion policies are better for some absolute action spaces while being worse for others relative action spaces. Clarification as to that would be great."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6147/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783162361,
            "cdate": 1698783162361,
            "tmdate": 1699636666184,
            "mdate": 1699636666184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LuzxrAoegG",
                "forum": "6pPYRXKPpw",
                "replyto": "0XPxKvL8h7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6147/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": ">This paper is a timely work since learning from diverse human data has shown major success in other fields [\u2026]\n\nWe thank the reviewer for their positive feedback. We aim to thoroughly address the concerns and questions raised by the reviewers.\n\n---\n\n>A benchmark paper is useless without the environment codes and the data [\u2026]\n\nWe have included the codebase as a .zip file in the supplementary material. Furthermore, we have supplied a link to an anonymous GitHub repository, which can be accessed here: https://github.com/d3iltest/Temporary_D3IL\n\n---\n\n>Another primary criticism is that all five environments are very simple tabletop environments, and thus the complexity of the algorithm needed to solve that may not be quite high. A better benchmark would involve multiple kinds of environments, involving 2D/3D environments, potentially with different intractable elements.\n\nWe appreciate the reviewer's consideration and understand the concern about the simplicity of the tabletop environments. As Reviewer DMDV shared a similar concern, we kindly guide the reviewer to our reply to Reviewer DMVD. In that response, we addressed the concerns and pointed to Appendix D of the revised version of the paper which elaborates on the new tasks we plan to introduce in the camera-ready version.\n\n---\n\n>One critical component missing from the evaluation is the required forward pass time or control frequency of the algorithms, which to my understanding is one of the largest disadvantages of diffusion-based models.\n\nWe acknowledge the importance of providing forward pass/inference times to offer a more comprehensive understanding of the strengths and weaknesses of the different methods. Consequently, we have revised our manuscript and incorporated this information. You can find the results in Appendix B, highlighted in green.\n\n---\n\n>What is the action space used by the environments? In the diffusion policy paper they show that diffusion policies are better for some absolute action spaces while being worse for others relative action spaces. Clarification as to that would be great.\n\nIn our experiments, we opted for velocity control, representing relative action spaces, as opposed to position control (absolute action spaces). A revised version of our manuscript has been uploaded, encompassing a thorough comparison between absolute and relative action spaces. However, our findings differ from those presented in the diffusion policy paper, as we observed consistently better performances with relative action spaces. The cause for this discrepancy is currently unknown. These additional experiments can be found in Appendix B and are highlighted in green.\n\nWe welcome the opportunity to address any additional concerns or questions that the reviewers may have."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6147/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160674549,
                "cdate": 1700160674549,
                "tmdate": 1700160674549,
                "mdate": 1700160674549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]