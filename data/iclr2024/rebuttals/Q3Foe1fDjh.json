[
    {
        "title": "Expected Probabilistic Hierarchies"
    },
    {
        "review": {
            "id": "sCtrkA8rSo",
            "forum": "Q3Foe1fDjh",
            "replyto": "Q3Foe1fDjh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a differentiable optimization approach for solving the hierarchical clustering problem. Specifically, for the Dasgupta\u2019s objective and TSD objective. To this end, the authors begin by encoding the tree structure as two 0-1 matrices $\\hat{A}, \\hat{B}$. Then they relax the integer constraint and obtain two continuous matrices $A,B$, which can be interpreted as the probability distribution over the discrete tree structure. Consequently, the goal is transformed into optimizing the expectation over the distribution represented by $A$ and $B$. Moreover, the authors prove that the optimal value of the expectation is equivalent to the discrete version. \n\nIn the optimization procedure, the authors replace the expectation with its appriximation, the mean computed from sampled hierarchies. Due to the high complexity of computing Dasgupta\u2019s objectives, they employ subgraph sampling techniques to accelerate the evaluation process.\n\nFinally, the proposed method is extensively evaluated on diverse datasets to assess its performance and effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The overall completeness of this work is good. It demonstrates clarity in writing and provides detailed explanations.\n- In my opinion, the expected objectives presented in this paper (eq. 4) are more reasonable compared to previous work. For instance, considering $Das(\\tilde{T})$, $c(v_i\\wedge v_j)$ is the number of leaves under $LCA(v_i, v_j)$ and it should be computed as $\\sum_{z\\in Z}\\sum_{v\\in V}\\Pr(z=LCA(v_i,v_j)\\ and \\text{ v is under z})$ in the probabilistic tree. The two conditions in the formula are not independent hence eq. 13 is more accurate than eq. 9.\n- The new scaling method proposed by this work has better explainability than its counterpart in [1].\n- The experiment results demonstrate that EPH method is competitive in practical application.\n\n[1] End-to-End Learning of Probabilistic Hierarchies on Graphs"
                },
                "weaknesses": {
                    "value": "I have not found obvious weaknesses of this paper. However, there is a concern about the novelty contribution when it is compared with [1]. The overal strategy and presentation closely resemble the scheme of [1]. Both the expected objectives and scaling trick appear to be modifications of the results presented in [1]."
                },
                "questions": {
                    "value": "- The example $K_4$ graph illustrates the advantage of EPH over FPH. Can you provide an example graph whose edges weights are not the same? As in the context of hierarchical clustering, unweighted graph has no real hierarchy. An instance with non-uniform weights would be more persuasive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599845353,
            "cdate": 1698599845353,
            "tmdate": 1699636545561,
            "mdate": 1699636545561,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GaWhAhP9iy",
                "forum": "Q3Foe1fDjh",
                "replyto": "sCtrkA8rSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We updated the manuscript based on the feedback with important text modifications highlighted in blue. In the following, we address their comments.\n\n\n**Comment:** Similarity to FPH.  \n**Response:** While our work builds upon the work of Z\u00fcgner et al., it addresses multiple drawbacks of FPH. We prove that Soft-Das is a lower bound of the discrete Dasgupta cost (Sec. 4.2) and present a simple example where it fails to find the minimizing hierarchy. Based on these observations, we propose our cost functions, the expected metrics (Sec. 4.1). We motivate them by showing that the optimal hierarchies align with their discrete counterparts, unlike Soft-Das (Sec. 4.2). Furthermore, we introduce an unbiased subgraph sampling scheme, allowing us to employ both, FPH and EPH, to vector datasets. In an exhaustive empirical evaluation, we demonstrate the superiority of EPH on graph and vector datasets.\n\n**Comment:** Weighted example where FPH fails.  \n**Response:** We decided to show the unweighted $K_4$ graph as an example since it is the most simple case. We included another weighted example with four nodes in Appendix A.7. As in the previous case, EPH finds the minimizing hierarchy, while FPH fails to do so. \n\nWe hope that we have satisfactorily addressed the reviewer's questions and concerns. We are happy to address any remaining remarks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233203919,
                "cdate": 1700233203919,
                "tmdate": 1700246800031,
                "mdate": 1700246800031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0aUNw6FhMR",
                "forum": "Q3Foe1fDjh",
                "replyto": "sCtrkA8rSo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I have further questions about the subgraph sampling procedure. \n\nThe process of sampling edges from the edge distribution $P(v_i, v_j)$ entails an $\\Omega(n^2)$ complexity (for vector data). Is this approach less scalable compared to the node-sampling method employed in [1]? Intuitively, the node-sampling technique does not require the preparation of weights for all edges, potentially having a much better scalability.\n\n[1] End-to-End Learning of Probabilistic Hierarchies on Graphs"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494474110,
                "cdate": 1700494474110,
                "tmdate": 1700494522347,
                "mdate": 1700494522347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YjOsAnrxWI",
                "forum": "Q3Foe1fDjh",
                "replyto": "APGzap1uDl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_geHj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarifications."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710083798,
                "cdate": 1700710083798,
                "tmdate": 1700710083798,
                "mdate": 1700710083798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jjpbua8SIJ",
            "forum": "Q3Foe1fDjh",
            "replyto": "Q3Foe1fDjh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_gKno"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_gKno"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a probabilistic model to learn a hierarchical clustering by optimizing the expectation of a quality metric (TSD or Dasgupta score).\n\nThey show that if this criterion has the same optimal value as the discrete counterpart (unlike say a continous relaxation based approach), and so is a reasonable target to optimize for. They use a end-to-end gradient based optimizer to optimize for this target.\n\nExperiments were performed to show that their proposed method outperforms reasonable baselines, including a simple relaxation based method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Use of hierarchical sampling to enable end-to-end differentiable optimization instead of the more obvious relaxation approach is interesting."
                },
                "weaknesses": {
                    "value": "While the resulting clustering does seem to be improved (judged by the improvement in the target criteria), it is unclear how much more expensive this process is compared to the baseline, or how the quality changes with changes in the number of samples used."
                },
                "questions": {
                    "value": "Please include some more details around 1) how much the results vary with the number of samples used 2) speed comparisons"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763736086,
            "cdate": 1698763736086,
            "tmdate": 1699636545460,
            "mdate": 1699636545460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UKuoH96SLa",
                "forum": "Q3Foe1fDjh",
                "replyto": "Jjpbua8SIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for their feedback and questions. We updated the manuscript based on the feedback with important text modifications highlighted in blue. In the following, we address their comments.\n\n**Comment:** How much more expensive is this process compared to the baseline?  \n**Response:** We include the runtimes for the experiments in Tab. 18 and Tab. 19 in the Appendix. While EPH is slower than the heuristic-based baselines, it performs similarly to the optimization-based baselines HypHC and FPH. EPH has a shorter runtime than HypHC on the graphs and large vector datasets. Furthermore, it matches the runtime of FPH for large vector datasets, as the number of sampled hierarchies is set to 1.\n\n**Comment:** How does the quality change with changes in the number of samples used?  \n**Response:** We show the normalized Dasgupta cost across a different number of sampled hierarchies and edges in Fig.3. As expected, the Dasgupta cost decreases when the number of samples increases. For the edges, we can already observe an improvement towards the average linkage algorithm when sampling $\\sqrt{n}$ many edges. For the number of sampled hierarchies, we observe that on most of the datasets, 20 samples are sufficient. However, even when using only one sample, EPH outperforms FPH, the second-best method:\n\n\n| Method | Brain | Citeseer | CoraML | Genes | OpenFlight | PolBlogs | WikiPhysics |\n| ------ | ----- | -------- | ------ | ----- | ---------- | -------- | ----------- |\n| FPH    | 466.94|  77.16   | 257.42 | 183.63| 355.61     |   262.48 |    538.47   |\n| EPH    | 437.12|  74.35   | 244.29 | 182.09| 332.03     |  253.39  |    494.23   |\n\nWe hope that we have satisfactorily answered the questions by the reviewer. If so, we kindly ask the reviewer to consider raising their scores. We are happy to address any remaining concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233080414,
                "cdate": 1700233080414,
                "tmdate": 1700246812690,
                "mdate": 1700246812690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "baaDpHyaMK",
                "forum": "Q3Foe1fDjh",
                "replyto": "dfwkF8IawF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_gKno"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_gKno"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707317084,
                "cdate": 1700707317084,
                "tmdate": 1700707317084,
                "mdate": 1700707317084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eIJApdGQhr",
            "forum": "Q3Foe1fDjh",
            "replyto": "Q3Foe1fDjh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents methods for hierarchical clustering using gradient-based methods, in particular a optimizing the expected score (e.g., Dasgupta cost) over a distribution of tree structures. \n\nThe authors demonstrate that the proposed approach has several nice properties (e.g., global optimal corresponds to optima of discrete cost). \n\nThe proposed approach performs well empirically compared to a variety of other approaches, compared to classic approaches such as average/ward linkage agglomerative methods and other gradient-based methods.."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper presents an interesting approach for gradient-based hierarchical clustering. Strengths include:\n\n* **Well-Written & Thorough** - The paper is quite complete, it is a pleasure to read and provides a clear outline of the approach, provides analysis of the empirical results (e.g., Fig 3,4,5, + much of supplement), and helpful remarks about the technical details of the approach (e.g., page 5 limitations)\n* **Methodological Approach** - While the parameterization of tree structures is similar to Z\u00fcgner et al (2022), the details of the sampling based approach seem to be distinct and the core contribution of the paper. While these are based on existing methods, the application here is intriguing.\n* **Empirical Results** - The proposed approach performs well empirically, outperforming most other methods in terms of these cost functions. There is thorough analysis which investigates the performance of the method throughout the supplemental material.\n\nMinor:\nPage 23 cuts off"
                },
                "weaknesses": {
                    "value": "Limitations of the paper include:\n* I think that the paper could have benefited from discussion of how the proposed cost functions relate to down stream tasks of clustering (e.g., evaluation against target labels, target hierarchies, etc.) and how continuous cost functions compare to discrete ones in this setting.\n* Similarly, discussion about when one prefers such methods in practice could be interesting (e.g., are there end-to-end applications?)\n* More details about the line: \"To obtain these for EPH and FPH, we take the most likely edge for each row in A and B, as Z\u00fcgner et al. \u00a8 (2022) proposed\""
                },
                "questions": {
                    "value": "Apologies, if I have missed something, are the trees produced by EPH binary? Do you convert them into binary trees for Dasgupta cost evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5391/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799082861,
            "cdate": 1698799082861,
            "tmdate": 1700707557713,
            "mdate": 1700707557713,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kDpWGOkJfy",
                "forum": "Q3Foe1fDjh",
                "replyto": "eIJApdGQhr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for their constructive and valuable feedback. We updated the manuscript based on the feedback with important text modifications highlighted in blue. In the following, we address their remarks and questions.\n\n**Comment:** How do the proposed cost functions relate to downstream tasks of clustering, and how do the continuous cost functions relate to the discrete ones?  \n**Response:** To analyze the relationship between the different cost functions and downstream tasks, we performed an external evaluation, as detailed in Appendix B.2. Here, we flatten the hierarchies derived from the different methods and compare the resulting clusters with the available ground-truth labels (see Table 12). \n\nTo perform an external evaluation on graph datasets, we use two synthetic HSBMs. Our goal was to answer two questions: First, how well do the inferred hierarchies align with the ground-truth hierarchies? And second, how well do the distances correlate between graphs and hierarchies? To address the first question, we calculated the Normalized Mutual information (NMI) across the different levels of the hierarchies. For the second question, we compute the Cophenetic correlation based on the shortest path and Euclidean distances of DeepWalk embeddings (see Table 4). Exp-Das and Exp-TSD almost perfectly recover the first three levels of the ground-truth hierarchy and achieve a notably high cophenetic correlation comparable to the ground truth. In both cases, Exp-TSD achieves slightly better scores, implying a better capability to recover the ground truth. \n\n**Comment:** When are such methods used in practice? E.g., are there End-to-end applications?   \n**Response:** End-to-end trainable hierarchical clustering methods are particularly useful in an offline setting where results are not required in real-time. Practical use cases include jet physics and genomics [1] or clustering of products in E-commerce [2], where linkage algorithms are commonly used.\n\n**Comment:** More details about the line: \"To obtain these for EPH and FPH, we take the most likely edge for each row in **A** and **B**...\"  \n**Response:** After the training process, EPH yields two probabilistic hierarchies, represented as matrices $\\mathbf{A}\\in[0,1]^{n\\times n'}$ and $\\mathbf{B}\\in[0,1]^{n'\\times n'}$. In these matrices, each row corresponds to a categorical distribution, i.e., $\\mathbf{A}$ and $\\mathbf{B}$ are row-stochastic matrices. As we are interested in discrete hierarchies, we convert these into discrete counterparts, $\\mathbf{\\hat{A}}\\in\\{0,1\\}^{n\\times n'}$ and $\\mathbf{\\hat{B}}\\in\\{0,1\\}^{n'\\times n'}$, respectively. While one approach is to sample an entry in each row weighted by their probability, we select the entry with the highest probability, i.e., the most likely edge. We refined this in the updated manuscript.\n\n**Comment:** Are the trees produced by EPH binary?  \n**Response:** EPH allows non-binary hierarchies, providing more flexibility. While the Dasgupta cost favors binary hierarchies, TSD does not. We can observe these in our qualitative evaluation in Fig. 4. While the hierarchies inferred using Exp-TSD have multiple branches, the hierarchies inferred by Exp-Das tend to be binary.\n\nWe thank the reviewer again for their comments and feedback. We hope that we have satisfactorily answered the reviewer's questions. We are happy to address any remaining concerns.\n\n[1]: Macaluso, Sebastian, Craig Greenberg, Nicholas Monath, Ji Ah Lee, Patrick Flaherty, Kyle Cranmer, Andrew McGregor, and Andrew McCallum. \"Cluster trellis: Data structures & algorithms for exact inference in hierarchical clustering.\" In International Conference on Artificial Intelligence and Statistics, pp. 2467-2475. PMLR, 2021.\n\n[2]: Brinkmann, Alexander, and Christian Bizer. \"Improving Hierarchical Product Classification using Domain-specific Language Modelling.\" IEEE Data Eng. Bull. 44, no. 2 (2021): 14-25."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233018281,
                "cdate": 1700233018281,
                "tmdate": 1700246751991,
                "mdate": 1700246751991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TvBGVaVZte",
                "forum": "Q3Foe1fDjh",
                "replyto": "kDpWGOkJfy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for your detailed and thoughtful reply. \n\n> Response: To analyze the relationship between the different cost functions and downstream tasks, we performed an external evaluation, as detailed in Appendix B.2. Here, we flatten the hierarchies derived from the different methods and compare the resulting clusters with the available ground-truth labels (see Table 12).\n\nGreat! I think this analysis strengthens the paper.\n\n> Response: End-to-end trainable hierarchical clustering methods are particularly useful in an offline setting where results are not required in real-time. Practical use cases include jet physics and genomics [1] or clustering of products in E-commerce [2], where linkage algorithms are commonly used.\n\nIt is not obvious to me that the jet physics applications need end-to-end methods. The generative process as I understood for the discovery of the jet reconstruction also are similar to, yet distinct from Dasgupta style costs. I believe that [1] shows only approximations to commonly used models for reconstruction that fit into the Dasgupta-like cost framework. Still I see how your method could be well suited for representing say uncertainty over of tree structures for such applications. \n\nI was expecting end-to-end learning of trees may matter for something where you might train an encoder of data + learn tree structure together (e.g. perhaps more like [Cattan et al, 2021](https://arxiv.org/pdf/2104.08809.pdf)?). Not asking for more experiments, just speaking out loud.\n\n> Response: After the training process, EPH yields two probabilistic hierarchies, ...\n\nThank you, I think the updated version is improved. For further revisions, I would suggest reminding readers again in this section why taking the most likely edge is a good idea wrt to the properties of the method. \n\n> Response: EPH allows non-binary hierarchies, providing more flexibility. While the Dasgupta cost favors binary hierarchies, TSD does not. We can observe these in our qualitative evaluation in Fig. 4. While the hierarchies inferred using Exp-TSD have multiple branches, the hierarchies inferred by Exp-Das tend to be binary.\n\nThanks yes. It is interesting that even without a strict constraint, you achieve sota Dasgupta costs. IIRC, you could improve your dasgupta costs by simply randomly partitioning any non-binary splits into binary splits? Unless I missed it (sorry), I think adding some discussion more discussion on these points would be beneficial.\n\nOverall, I think this is a nice and complete paper. I am considering my original score and thinking about whether or not I would like to increase it. I will post further thoughts / comments when I make such decisions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406444509,
                "cdate": 1700406444509,
                "tmdate": 1700406444509,
                "mdate": 1700406444509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x62MYGQBIk",
                "forum": "Q3Foe1fDjh",
                "replyto": "rVWr0BgI5K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5391/Reviewer_jgu2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your additional clarifications."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5391/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708094070,
                "cdate": 1700708094070,
                "tmdate": 1700708094070,
                "mdate": 1700708094070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]