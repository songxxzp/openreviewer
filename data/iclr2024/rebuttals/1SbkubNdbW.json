[
    {
        "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks"
    },
    {
        "review": {
            "id": "HiO1OYVHot",
            "forum": "1SbkubNdbW",
            "replyto": "1SbkubNdbW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript analyzes the potential risk of increased privacy leakage associated with traditional positive Label Smoothing in the context of Model Inversion Attacks. Additionally, it provides an analysis on how utilizing negative values can counter this risk."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors have for the first time considered the potential privacy leakage risks associated with the use of positive label smoothing under model inversion attacks, and have provided insights into the privacy benefits that may arise from using negative label smoothing. Additionally, the authors have presented a framework for analyzing the exposure of privacy in model inversion attacks due to label smoothing, and have examined how label smoothing may affect the sampling and optimization phases of model inversion attacks. Furthermore, the authors have provided a geometric intuition in the EMBEDDING SPACES to explain this phenomenon, all of which are relatively novel contributions. The quality of writing in the article is excellent, the presentation is clear, the experiments are comprehensive, and the work holds significant importance."
                },
                "weaknesses": {
                    "value": "1. The paper lacks a theoretical analysis framework. The authors claim that negative label smoothing performs better than state-of-the-art defenses, but in fact, it only performs better under model inversion attacks within the analysis framework of this paper. However, privacy protection mechanisms are often capable of resisting multiple types of attacks (or even arbitrary attacks), and it is still unknown whether the approach of negative label smoothing is effective in protecting privacy against other types of attacks.\n\n2. It is generally considered that achieving both privacy and efficiency in Euclidean space through privacy protection mechanisms is an NP-hard problem. The paper does not discuss whether the performance improvement of using negative label smoothing is significantly less than that of positive label smoothing. It is noted that when negative label smoothing was previously proposed, the range of parameter choices was from negative infinity to 1. In this case, good performance does not mean that always choosing negative label smoothing will result in good performance. The authors do not discuss in the article how the performance of using negative label smoothing compares to that of positive LS or even not using this regularization measure at all, as well as whether negative LS is not applicable in many tasks."
                },
                "questions": {
                    "value": "1. Can an analysis be provided comparing the performance of using negative label smoothing to the performance under positive label smoothing?\n\n2. Are there any known vulnerabilities introduced by using negative label smoothing?\n\n3. How generalizable are the findings of this paper? Are the observed effects of label smoothing on privacy specific to the datasets and models used in the experiments, or can they be applied to other domains and architectures as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This article might inspire people to use model inversion attacks to target some existing models trained with positive label smoothing."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Reviewer_EAeB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698306422130,
            "cdate": 1698306422130,
            "tmdate": 1699636044979,
            "mdate": 1699636044979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9tg8DjTDSz",
                "forum": "1SbkubNdbW",
                "replyto": "HiO1OYVHot",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "title": {
                        "value": "clarification of term \"performance\""
                    },
                    "comment": {
                        "value": "Dear Reviewer EAeB,\n\nthank you for your detailed review. We are currently working on the rebuttal to answer your questions and provide additional results. To provide a satisfying answer for you, we would like to ask for a more clear definition, of what you mean by \"good performance\" as in \"Can an analysis be provided comparing the performance of using negative label smoothing to the performance under positive label smoothing?\". \n\nDo you mean a model's prediction performance on an unseen dataset (classification test accuracy)? If not, a clarification of the term \"performance\" would make it easier for us to prepare our answer.\n\nBest regards,\nthe authors"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882628441,
                "cdate": 1699882628441,
                "tmdate": 1699882628441,
                "mdate": 1699882628441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7jO4Ojr9sd",
            "forum": "1SbkubNdbW",
            "replyto": "1SbkubNdbW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how label smoothing (during model training) can affect the performance of the model inversion attacks.\nSpecifically, the authors find that a positive label smoothing factor would facilitate the inversion attacks, while a negative factor would suppress the attacks.\nThis phenomenon underlines the importance of delving more into factors that influence machine learning privacy leakage."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper reveals a novel phenomenon that the positivity/negativity of label smoothing factors can affect privacy leakage in model inversion attacks. Actually, the mechanism behind this can also be explained as the robustness-privacy trade-off: [r1] first find that label smoothing with positive factor can improve adversarial robustness, and [r2] first find that there is a trade-off between adversarial robustness and membership inference attacks. Nevertheless, although the relationship between label smoothing and model inversion attacks is forecastable based on the aforementioned works, this paper is the first that empirically demonstrates the relationship. As a result, I think the results of this paper are fundamental and important.\n\n2. The paper is easy to follow and provides sufficient experiments details for reproducing its results.\n\n\n**References:**\n\n[r1] Shafahi et al. \"Label smoothing and logit squeezing: A replacement for adversarial training?\" arXiv 2019.\n\n[r2] Song et al. \"Privacy risks of securing machine learning models against adversarial examples\". CCS 2019."
                },
                "weaknesses": {
                    "value": "1. In Section 3 and Figure 1, the authors provide an intuitive explanation of why label smoothing can affect model inversion attacks based on a simple experiment on 2D data. The explanation is based on the hypothesis that a more clear decision boundary would make training data less likely to be leaked through model inversion attacks. This hypothesis is odd (at least for me) and I think the authors may need to put more effort into explaining why the hypothesis makes sense.\n\n\n2. Negative-factor label smoothing would result in the smoothed label no longer a probability simplex. I think the authors may need to justify why this type of label smoothing is appropriate.\n\n\n3. Suggestion: As explained in Section \"Strengths\", the found phenomenon can be seen as a robustness-privacy trade-off. Since this paper finds that negative-factor label smoothing can mitigate privacy leakage by model inversion attacks, I suspect it could also harm the adversarial robustness of the model. Therefore, I suggest the authors include a discussion on the potential harm to models' robustness when protecting training data privacy."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698512663272,
            "cdate": 1698512663272,
            "tmdate": 1700391527511,
            "mdate": 1700391527511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Uu07xY4mj8",
                "forum": "1SbkubNdbW",
                "replyto": "7jO4Ojr9sd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer uLpV,\nthank you very much for reviewing our paper. We are happy that you think our presented results are fundamental and important. We try to address all your remarks appropriately.\n\n**1.) Why would a clear decision boundary make training data less likely to be leaked?** We think there might be a misunderstanding here. Figure 1 indeed visualizes the decision boundaries (which are quite similar for all three models) together with the classifiers' confidence scores for each point in the input space. However, the paper's argumentation is not that negative label smoothing (Fig. 1c) defends against model inversion attacks because the model has clear decision boundaries. We rather argue that negative label smoothing leads to models that provide no clear guidance signal for the model inversion attack algorithm. In this toy example, the model predicts high confidence scores for almost all inputs, excepts those close to the decision boundary. Therefore, not much information about the positioning of the training distribution in the input space is leaked, and the attack is not able to move close towards the training data. Positive label smoothing, on the other hand, places the training data in a high confidence area, therefore, making it easier for the attack algorithm to guide towards this position in the input space. \n\n**2.) Negative label smoothing creates no valid probability simplex:** Indeed, the labels smoothed with a negative factor define no valid probability distribution due to the negative probabilities (if the labels were interpreted as probabilities). We already brought this point up in Section 2.2, and it was already discussed by Wei et al. [1], who originally proposed negative label smoothing for tackling label noise. The interesting effect arises from putting negatively smoothed labels into the cross-entropy loss, which avoids a gradient saturation effect during training. Consequently, negative label smoothing gives the model an incentive to increase its logit output for the true label while reducing the logits for all other classes. We already provide a more comprehensive, formal analysis of label smoothing with positive and negative smoothing factors in **Appx. A**.\n\n[1] Wei et al., To Smooth or Not? When Label Smoothing Meets Noisy Labels"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700062368249,
                "cdate": 1700062368249,
                "tmdate": 1700062793197,
                "mdate": 1700062793197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XEu7qyFty3",
                "forum": "1SbkubNdbW",
                "replyto": "7jO4Ojr9sd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_uLpV"
                ],
                "content": {
                    "title": {
                        "value": "Score has been raised"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their detailed respone.\nGiven that all my concerns have been addressed, I have raised my score to 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391687192,
                "cdate": 1700391687192,
                "tmdate": 1700391719743,
                "mdate": 1700391719743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J62rMx5tHM",
            "forum": "1SbkubNdbW",
            "replyto": "1SbkubNdbW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the impact of label smoothing, a commonly used regularization techniques in deep learning, on the privacy vulnerability of models to model inversion attacks (MIAs), which aim to reconstruct the characteristic features of each class. It is shown that traditional label smoothing with positive factors may inadvertently aid MIAs, increasing the privacy leakage of a model. The paper also finds that smoothing with negative factors can counteract this trend, impeding the extraction of class-related information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- MIAs represent a major threat to the privacy of machine learning models. This seems to be the first paper studying the relationship between label smoothing and MIAs. \n- The work provides both empirical and analytical justification for the connections between label smoothing and MIAs. \n- The ablation study is interesting to show that label smoothing impacts MIAs mainly in the optimization stage."
                },
                "weaknesses": {
                    "value": "- The findings are not very surprising. Given that label smoothing helps the model generalize, leading to better representation of each class (e.g., more smooth decision boundaries). It is intuitive that more smooth decision boundaries allow gradient-based method to better optimize the representation of each class. \n- There is trade-off between the model's accuracy and its vulnerability to MIAs. Label smoothing with negative factors reduces the privacy risks at the cost of model accuracy (see Table 1). Intuitively, a poorly trained model is more robust to MIAs."
                },
                "questions": {
                    "value": "How does label smoothing impact the model's vulnerability to other attacks (e.g., adversarial attacks, backdoor attacks, etc)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688661406,
            "cdate": 1698688661406,
            "tmdate": 1699636044810,
            "mdate": 1699636044810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rxZC8XLvnn",
                "forum": "1SbkubNdbW",
                "replyto": "J62rMx5tHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer  wJ7G,\nthank you very much for reviewing our paper. We are glad that you find our paper tackling an interesting and important problem. We try to address all your remarks and questions.\n\n**1.) Findings are not very surprising:** Please excuse that we kindly disagree here. To our knowledge, no previous research has investigated the impact of label smoothing on a model's privacy leakage or even discussed such a possibility when training with positive label smoothing. Particularly the effects of negative label smoothing have generally been barely investigated, and its impact on a model's privacy is important to examine. For people familiar with the standard label smoothing literature, some of our findings might be intuitive (which is not a bad thing at all), but the general community seems not to be aware of its effects on a model's privacy. We also think that the impact of negative label smoothing is not trivial to see, even for people familiar with the topic. We believe that a comprehensive study on the impact of label smoothing on a model's privacy leakage under the lens of model inversion is critical and very relevant to the privacy research community and will inspire future efforts in this direction.\n\n**2.) Accuracy-vulnerability trade-off:** While there definitely exists an inherent trade-off between a model being able to recognize certain features in inputs required to make good predictions and its privacy leakage, we stress that the slightly reduced prediction accuracy of the models trained with negative LS is not the main reason for their defensive effects. To demonstrate this effect, we re-trained the models with positive LS and standard cross-entropy loss and stopped each training run as soon as the model's test accuracy matched that of the model trained with negative LS. We then repeated the attacks against these models, which all achieve comparable prediction accuracy, to see if the attack success of the models drops. Our results demonstrate that even under comparable test accuracy, the attacks still perform well and substantially better than on the negative LS models. This clearly indicates that reducing a model's prediction accuracy is not the driving factor behind negative LS as a defense mechanism. This can also be seen when comparing with BiDO and MID, which both achieve lower test accuracy than negative label smoothing but still are more vulnerable to the attack.\n\n|  FaceScrub |  \u03b1 |  \u2191 Test Acc |  \u2191 Acc@1 | \u2191 Acc@5 | \u2193 \u03b4face | \u2193 \u03b4eval |  \u2193FID | \n| -------- | ------- |------- |------- |------- |------- |------- |------- |\nNo Label Smoothing |0.0 |91.74%| 89.68%| 99.00% |0.7382 |133.12 |50.43|\nPositive Label Smoothing| 0.1 |92.13%| 87.16% |97.20% |0.7766 |135.73| 49.56|\nNegative Label Smoothing |\u22120.05| 91.45% |14.34%| 30.94% |1.2320 |239.02 |59.38 |16.45% |15.73%|\n\n|  CelebA |  \u03b1 |  \u2191 Test Acc |  \u2191 Acc@1 | \u2191 Acc@5 | \u2193 \u03b4face | \u2193 \u03b4eval |  \u2193FID | \n| -------- | ------- |------- |------- |------- |------- |------- |------- |\n|No Label Smoothing |0.0 |84.02%| 69.36% |90.60%| 0.7899| 344.82| 49.60|\n|Positive Label Smoothing |0.1 |86.78% |76.64%| 92.84%| 0.785| 332.53| 50.05|\n|Negative Label Smoothing |\u22120.05 |83.59%| 26.41%| 49.96%| 1.0420| 441.67 |61.30 |7.08%| 5.89%|\n\nOur results including a more detailed experimental setting and more elaborated discussion are also stated in **Appendix C.6**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700061132874,
                "cdate": 1700061132874,
                "tmdate": 1700062823327,
                "mdate": 1700062823327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ltDsT4EtGo",
                "forum": "1SbkubNdbW",
                "replyto": "J62rMx5tHM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3.) How does label smoothing impact the model's vulnerability to adversarial attacks and backdoor attacks?** We stress that the paper focuses on the domain of model inversion attacks, and adversarial examples and backdoor attacks are out of the general scope of the paper. However, we agree that the impact of label smoothing on security attacks is indeed a very interesting question. We conducted various additional experiments in the setting of adversarial attacks to provide additional insights.\n\nFor adversarial attacks, we used four attack algorithms (FGSM, PGD, BIM, One-Pixel Attack) in two modes (targeted + untargeted) to measure the robustness of models trained with hard labels, positive label smoothing, or negative label smoothing. Please refer to **Appendix C.8** for the detailed hyperparameters and attack results. Overall, we found that training with positive label smoothing makes the models slightly more robust to adversarial examples. Interestingly, training with negative label smoothing markedly increased the models' robustness to adversarial attacks. So, as a first result in this research direction, we can conclude that negative label smoothing not only helps mitigate privacy leakage but can also make a model more robust to adversarial examples. We will look further into this direction of adversarial attacks in future work.\n\n*Columns 4-7 state the attack success rate for untargeted adversarial attacks, columns 8-11 the success rate for targeted attacks.*\n\n*Columns 3-6 state the attack success rate for untargeted adversarial attacks, columns 7-10 the success rate for targeted attacks.*\n\n**FaceScrub**\n| \u03b1| \u2191 Clean Test Acc | \u2193 FGSM | \u2193 PGM |\u2193 BIM |\u2193 One-Pixel |\u2193 FGSM |\u2193 PGM | \u2193 BIM | \u2193 One-Pixel|\n| ------- |-------- | ------- |-------- | ------- |-------- | ------- |-------- | ------- |-------- | \n|ResNet-152|\n|0.1 |97.39%|70.09%| 100.00% |100.00%| 3.93%| 7.76% |92.42% |93.51%| 0.00%|\n|0.0| 94.93% |96.09%| 100.00% |100.00%| 7.63%| 55.62%| 99.95% |99.97% |0.03%|\n|\u22120.05| 91.45% |13.38% |13.94% |13.44% |10.43% |22.60% |98.52%| 98.89%| 0.08%|\n|ResNeXt-50|\n|0.1| 97.54%| 59.21% |100.00% |100.00% |3.54% |6.84% |83.58%| 85.56%| 0.03%|\n|0.0| 95.25% |97.36%| 100.00%| 100.00%| 7.79% |52.46% |100.00%| 99.97%| 0.05%|\n|\u22120.05 |92.40% |11.17% |11.38%| 11.11% |9.74% |24.00%| 99.55% |99.74%| 0.08%|\n|DenseNet-121|\n|0.1| 97.15%| 81.76%| 100.00%| 100.00% |3.83%| 9.16% |93.11%| 94.59% |0.03%|\n|0.0 |95.72%| 97.02%| 100.00%| 100.00% |5.44%| 39.73%| 100.00%| 100.00%| 0.05%|\n|\u22120.05| 92.13% |31.84% |32.74%| 32.15%| 9.61%| 18.72% |86.17% |88.52%| 0.08%|\n  \n   \n  \n**CelebA**\n| \u03b1| \u2191 Clean Test Acc | \u2193 FGSM | \u2193 PGM |\u2193 BIM |\u2193 One-Pixel |\u2193 FGSM |\u2193 PGM | \u2193 BIM | \u2193 One-Pixel|\n| ------- |-------- | ------- |-------- | ------- |-------- | ------- |-------- | ------- |-------- | \n|ResNet-152|\n|0.1| 95.11%| 97.44% |100.00%| 100.00% |6.49%| 17.64%| 99.53%| 99.90% |0.00%|\n|0.0| 87.05%| 98.20% |100.00%| 100.00%| 20.67% |31.59%| 100.00%| 99.97%| 0.03%|\n|\u22120.05| 83.59% |28.13%| 30.16% |28.16%| 20.67%| 8.16%| 92.08% |93.81%| 0.03%|\n|ResNeXt-50|\n|0.1 |95.27% |94.81% |100.00% |100.00% |6.29% |11.68% |98.84% |99.17% |0.00%|\n|0.0 |87.85% |96.84% |100.00%| 100.00% |18.88%| 28.56% |100.00% |99.90% |0.03%|\n|\u22120.05| 84.79%| 23.20%| 24.40% |23.34% |20.07% |9.02%| 95.27%| 96.74% |0.03%|\n|DenseNet-121|\n|0.1 |92.88% |98.67%| 100.00% |100.00%| 8.22%| 14.02% |98.04%| 99.10% |0.00%|\n|0.0 |86.05%| 98.57%| 100.00% |100.00%| 15.78% |20.01%| 99.90% |99.93% |0.03%|\n|\u22120.05| 86.48% |67.48%| 68.54%| 68.18% |15.65%| 8.96%| 67.54% |69.41%| 0.00%|\n\nResults  including a more detailed experimental setting and more elaborated discussion are also stated in Appendix **C.8**.\n\nExperiments investigating the backdoor setting are currently being implemented. We will provide the results as soon as they become available."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700062076056,
                "cdate": 1700062076056,
                "tmdate": 1700062689836,
                "mdate": 1700062689836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YWwqHYzpc0",
                "forum": "1SbkubNdbW",
                "replyto": "td2cKAoJtD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_wJ7G"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response and added experiments, which have addressed some of my questions. My concern about the interestingness of the findings remains. I'm inclined to increase my score and will do so after discussing with fellow reviewers and AC."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683711916,
                "cdate": 1700683711916,
                "tmdate": 1700683711916,
                "mdate": 1700683711916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ge0NsER78l",
            "forum": "1SbkubNdbW",
            "replyto": "1SbkubNdbW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the problem of defending against Model Inversion Attacks in white-box settings. The major contributions of this work are:\n\n1) In the context of model inversion attacks, this paper observes that positive label smoothing increases a model\u2019s privacy leakage and negative label smoothing counteracts such effects.\n\n2) Consequently, negative label smoothing is proposed as a proactive measure to defend against Model Inversion Attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper is written well and it is easy to follow.\n\n2) To my knowledge, this is the first work to explore label smoothing in the context of defense against model inversion attacks."
                },
                "weaknesses": {
                    "value": "At a high level, Model Inversion attack procedures can be decomposed into 2 stages: 1) Classifier training, and 2) Model Inversion attack on the classifier. My review below addresses weaknesses of this work corresponding to each stage.\n\n**Classifier training:**\n\n1) Section 3. Analysis is only remotely related to contemporary Model Inversion setups. In particular, it is unclear as to how observation from Section 3 lays groundwork to the remainder of the paper.\n\n- What is the optimization objective for both classifier training and model inversion attack for Figure 1? A clearly formulated problem definition with relevant equations is critical to understand this part.\\\n\n- How would Figure 1 change if the number of iterations = 5K for all setups. \n\n2) Section 4.3. A large number of observations from Sec 4.3 for Standard and Positive Label Smoothing has already been thoroughly investigated in prior works. \n\n- Muller et al. [A] and Chandrasegaran et al. [B] have already shown that positive label smoothing erases some relative information in the logits resulting in better class-wise separation of penultimate layer representations under positive LS compared standard training.\n\n- Figure 4, column 3 is unclear. What are the Training and Test accuracies of the classifiers used in Figure 4?  Recall that if the classifier trained with negative label smoothing is good, penultimate layer representations should be linearly separable. Therefore, does column 3 correspond to a poor classifier trained with negative label smoothing?\n\n3) Can the authors explain why standard training is required in the first few iterations before \u201cgradually\u201d increasing the negative label smoothing?\n\n\n\n**Model Inversion Attacks:**\n\n1) Limited empirical study. This work only studies one attack method for evaluating defense (No comparison against MID and BiDO defense setups even in Table 8 in Supp.). I agree that PPA works well in high-resolution setups, but SOTA attacks in well established test beds are required to understand the efficacy of the proposed defense.\n\n- It is important to include GMI, KEDMI, VMI [C], LOMMA [4] and PLG-MI [5] attacks to study the efficacy of the proposed defense (against other SOTA defense methods). Currently it is not possible to compare the proposed method with results reported in the MID and BiDO defense papers.\n\n2) There is no evidence (both qualitative and quantitative) to establish that unstable gradient directions during Model Inversion attack is due to negative label smoothing. Is it possible that such shortcomings could be due to the PPA attack optimization objectives? Addressing 1) above can answer this question to some extent. \n\n3) User studies are necessary to show the defense/ leakage of privacy shown by the inversion results. Since this work focuses on private data reconstruction, it is important to conduct user study to understand the improvements (See [F]).\n\n4) Significant compromise in model utility when using negative label smoothing questioning the findings/ applicability of this approach. Table 1 and Table 8 results suggest that Neg. LS reduces the Model Accuracy (model utility) by huge amounts, i.e.: A 3.5% reduction in Test Accuracy for CelebA (Table 1) compared to Standard training could be serious. Recall that lower model accuracy leads to lower MI attack results. I agree that generally some compromise in model utility might be required for defense, but large reduction in model utility makes this approach questionable, i.e., In practice, no one would deploy/ attack a weaker model.\n\n5) Error bars/ Standard deviation for experiments are missing.\n\n6) Missing related works [C, D, E].\n\n\nOverall I enjoyed reading this paper. But in my opinion, the weaknesses of this paper outweigh the strengths. But I\u2019m willing to change my opinion based on the rebuttal. \n\n===\n\n[A] M\u00fcller, Rafael, Simon Kornblith, and Geoffrey E. Hinton. \"When does label smoothing help?.\" Advances in neural information processing systems 32 (2019).\n\n[B] Chandrasegaran, Keshigeyan, et al. \"Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?.\" International Conference on Machine Learning. PMLR, 2022.\n\n[C] Wang, Kuan-Chieh, et al. \"Variational model inversion attacks.\" Advances in Neural Information Processing Systems 34 (2021): 9706-9719.\n\n[D] Nguyen, Ngoc-Bao, et al. \"Re-thinking Model Inversion Attacks Against Deep Neural Networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[E] Yuan, Xiaojian, et al. \"Pseudo Label-Guided Model Inversion Attack via Conditional Generative Adversarial Network.\" AAAI 2023 (2023).\n\n[F] [MIRROR] An, Shengwei et al. MIRROR: Model Inversion for Deep Learning Network with High Fidelity. Proceedings of the 29th Network and Distributed System Security Symposium."
                },
                "questions": {
                    "value": "Please see Weaknesses section above for a list of all questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe",
                        "ICLR.cc/2024/Conference/Submission1183/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789725658,
            "cdate": 1698789725658,
            "tmdate": 1700683982523,
            "mdate": 1700683982523,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wvy10e3p4c",
                "forum": "1SbkubNdbW",
                "replyto": "ge0NsER78l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer dNRe,\nthank you very much for reviewing our paper. We are happy that you enjoyed reading our paper. We hope we can clear up any ambiguities, provide the requested details and results, and convince you more of our research.\n\n### Classifier Training\n\n**1.) Additional Details for Figure 1:** To provide more details on the hyperparameters and optimization goals in Figure 1, we added Section B.6. to the Appendix. Specifically, each model was trained with a cross-entropy loss, as defined by Eq. (3) in the main paper. Each training only differs in the label smoothing factor used ($\\alpha \\in \\{0, 0.05, -0.05\\}$). So the classifier training does conceptually not differ from the training of the face recognition models. For the attack, we used the identity loss proposed by Zhang et al.[1], which corresponds to a simple cross-entropy objective. Since no image prior is used, there is no additional prior loss. Optimization itself was done using standard SGD. The attack goal is to adjust the features of the initial sample from the green circles class to recover features from the orange pentagons class. This setting simulates a simple model inversion attack with the goal to recover features from the target class, i.e., in the simple 2D setting moving the sample as close as possible to the target distribution. For more detailed descriptions and parameters, please have a look at **Appendix B.6**.\n\nAs requested, we also repeated the experiment and set the number of optimization steps to 5,000 without any stopping mechanism applied. The result, which we state in **Figure 7 in Appx. B.6**, draws a similar picture. Again, the attack against the model trained with positive LS moves the optimized sample close to the target distribution's center, whereas the optimization on the model trained without LS moves closer to the target distribution but stays still on the surface of it. For the negative LS model, not much has changed. The optimized sample moves a bit further away from the decision boundary but still is far away from the training distribution.\n\nOverall, the toy example motivates our research into the effect of label smoothing on model inversion attacks. Particularly, the setting shows that models trained with positive LS provide better guidance signals for model inversion attacks, leading the optimization closer to the true training distribution, whereas training with negative smoothing complicates the optimization process and hinders the recovery of sensitive features. These effects are then later shown on face recognition models, for which the FaceNet distance might be the most important metric to demonstrate that attacks against positive LS models recover more features of the target distribution, whereas attacks against negative LS models revert this effect."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063177423,
                "cdate": 1700063177423,
                "tmdate": 1700063177423,
                "mdate": 1700063177423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hOoabifyRy",
                "forum": "1SbkubNdbW",
                "replyto": "ge0NsER78l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear dNRe,  \nwe just added additional model inversion experiments for PLG-MI, as well as for MID and BiDO as defense mechanisms. For BiDO and MID, we trained two models each with different defense parameter strengths. For VMI, we are also currently working on getting the code running, which is a bit tricky due to undetailed documentation and missing requirements and config files.\n\nOverall, as our results on the 7 attacks demonstrate, negative LS beats MID and BiDO as defense in most attacks (at comparable test accuracy). For PLG-MI, all defenses perform comparably. Given that the defense improvement of negative LS is even higher in the high-resolution setting of PPA, we think our empirical results have sufficiently demonstrated the defensive impact of negative LS.\n\n|  Attack Type |  Training Parameter |  \u2191 Test Acc |  \u2191 Acc@1 |  \u2191 Acc@5 |  \u2193 \u03b4KNN  | \u2193 FID | \n| -------- | ------- |  ------- |  ------- |  ------- |  ------- |  ------- |\n| GMI | \n| |  Baseline \u03b1 = 0.0 |  85.74% |  16.00% \u00b1 3.75 |  36.60% \u00b1 4.37 |  1043.22 |  52.90 | \n| |  LS \u03b1 = 0.1 |  88.10% |  25.40% \u00b1 5.06|  47.40% \u00b1 5.09 |  1067.55 |  51.12 | \n| |  LS \u03b1 = \u22120.05 |  80.02% |  5.92% \u00b1 2.31 |  19.80% \u00b1 3.91 |  1078.57 |  70.87|  \n| | MID \u03b2 = 0.003 |77.56% | 14.60% \u00b1 2.80 | 30.00% \u00b1 4.15 | 1079.17 | 56.52 |\n| | MID \u03b2 = 0.01 | 67.45% | 12.56% \u00b1 3.79 | 31.00% \u00b1 3.69 | 1085.05 | 59.41 |\n| |  HSIC \u03b2 = (0.05, 0.5) |  79.06%  |  7.40% \u00b1 2.79 |  17.80% \u00b1 4.55 |  1142.14 |  65.36|  \n| |  HSIC \u03b2 = (0.05, 1.0) |  70.18%  |  2.76% \u00b1 1.36 | 8.60% \u00b1 2.21 |  1227.64 |  81.60|  \n| KED | \n| |  Baseline \u03b1 = 0.0 |  85.74% |  43.64% \u00b1 3.67 |  71.80% \u00b1 3.41 |  897.54 |  42.59 | \n| |  LS \u03b1 = 0.1 |  88.10%  |  68.88% \u00b1 3.23  | 86.20% \u00b1 2.43 | 791.11 | 24.10| \n| | LS \u03b1 = \u22120.05 | 80.02%  | 24.10% \u00b1 3.06  | 54.80% \u00b1 2.88 |  953.74 |  43.56 | \n| | MID \u03b2 = 0.003 | 77.56% | 60.40% \u00b1 2.49 | 87.80% \u00b1 2.32 | 797.99 | 27.58 |\n| | MID \u03b2 = 0.01 | 67.45% | 50.44% \u00b1 2.21 | 79.20% \u00b1 1.80 | 821.01 | 27.02 |\n| |  HSIC \u03b2 = (0.05, 0.5) | 79.06%  | 42.72% \u00b1 4.25 | 71.60% \u00b1 2.67 |  865.94 |  29.98| \n| |  HSIC \u03b2 = (0.05, 1.0) |  70.18% |  29.00% \u00b1 5.13  |  58.20% \u00b1 2.60 |  932.78 |  31.95|  \n|  LOMMA (GMI) | \n| |   Baseline \u03b1 = 0.0 |  85.74%|   53.64% \u00b1 4.64|   79.60% \u00b1 3.55 |  878.36 |  42.28| \n| | LS \u03b1 = 0.1 | 88.10%| 50.96% \u00b1 3.52  | 71.80% \u00b1 4.48 |  955.44 |  47.44| \n| | LS \u03b1 = \u22120.05 |  80.02% |  39.16% \u00b1 4.25 |  68.00% \u00b1 4.49 |  854.10 |  39.24| \n| | MID \u03b2 = 0.003 | 77.56% | 32.92% \u00b1 3.59 | 57.40% \u00b1 3.16 | 961.51 | 51.98 |\n| | MID \u03b2 = 0.01 | 67.45% | 17.16% \u00b1 3.71 | 36.80% \u00b1 6.47 | 1044.63 | 59.07 |\n| | HSIC \u03b2 = (0.05, 0.5) |  79.06%  |  47.84% \u00b1 4.32|  74.40% \u00b1 4.51 |  892.00 |  41.76 | \n| |  HSIC \u03b2 = (0.05, 1.0) |  70.18%|  30.84% \u00b1 4.27 |  54.20% \u00b1 4.68 |  963.49 |  44.36|  \n|  LOMMA (KED)| \n| |  Baseline \u03b1 = 0.0 |  85.74% |  72.96% \u00b1 1.29 |  93.00% \u00b1 0.83 |  791.80 |  33.39|  \n| |  LS \u03b1 = 0.1 |  88.10%  |  76.52% \u00b1 1.31 |  92.40% \u00b1 1.05 |  780.76 |  33.01 | \n| |  LS \u03b1 = \u22120.05 | 80.02% |   63.60% \u00b1 1.37 |  86.60% \u00b1 0.70 |  784.43 |  40.97 | \n| | MID \u03b2 = 0.003 77.56% | 63.56% \u00b1 1.11 | 90.60% \u00b1 0.68 | 792.74 | 39.69 |\n| | MID \u03b2 = 0.01 67.45% | 50.68% \u00b1 1.49 | 76.60% \u00b1 1.58 | 831.31 | 36.87 |\n| |  HSIC \u03b2 = (0.05, 0.5) |  79.06% |  65.68% \u00b1 1.41 |  86.60% \u00b1 0.67 |  810.61 |  35.95 | \n| |  HSIC \u03b2 = (0.05, 1.0) |  70.18%  |  47.64% \u00b1 1.28 |  73.80% \u00b1 0.94 |  860.08 |  38.77| \n| PLG-MI | \n| |  Baseline \u03b1 = 0.0 |  85.74% |  71.00% \u00b1 3.31 |  92.00% \u00b1 3.16 |  1358.56 |  22.43| \n| | LS \u03b1 = 0.1 | 88.10%|  80.00% \u00b1 4.47 |  92.00% \u00b1 3.16 | 1329.05|  21.89| \n| | LS \u03b1 = \u22120.05 | 80.02%  | 72.00% \u00b1 2.50| 89.00% \u00b1 2.00 | 1544.82|  78.98| \n| | MID \u03b2 = 0.003 | 77.56% |72.00% \u00b1 6.08  | 89.00% \u00b1 3.00  | 1378.50 | 20.74 |\n| | MID \u03b2 = 0.01 | 67.45% | 59.00% \u00b1 2.45 | 81.00% \u00b1 3.87 | 1487.30 | 20.64 |\n| | HSIC \u03b2 = (0.05, 0.5) | 79.06%  | 70.00% \u00b1 4.00  | 85.00% \u00b1 3.16 | 1433.48 | 25.37| \n| | HSIC \u03b2 = (0.05, 1.0) | 70.18%  | 42.00% \u00b1 4.80 |  62.00% \u00b1 5.10 | 1585.35|  30.52| \n| RLB-MI| \n| | Baseline \u03b1 = 0.0 | 85.74%|  52.00% | 75.00%|  - | -| \n| | LS \u03b1 = 0.1|  88.10%|  65.00% |  84.00% | - | -| \n| | LS \u03b1 = \u22120.05 | 80.02%  | 19.00%|  48.00%|  -|  -| \n| | MID \u03b2 = 0.003 | 77.56% | 27.00% | 41.00% | - | - |\n| | MID \u03b2 = 0.01 | 67.45% | 20.00% | 42.00% | - | - |\n| | HSIC \u03b2 = (0.05, 0.5) | 79.06% |  35.00% | 57.00%| -| -| \n| | HSIC \u03b2 = (0.05, 1.0)|  70.18% | 25.00% | 49.00%| -| -| \n| BREP-MI| \n| |  Baseline \u03b1 = 0.0 |  85.74% |  49.00%|  73.67%|  -|  -| \n| | LS \u03b1 = 0.1 | 88.10%  | 56.33%  | 77.00% | -|  -| \n| | LS \u03b1 = \u22120.05|  80.02% |  48.70% |  70.50%|  - | -| \n| | MID \u03b2 = 0.003 | 77.56% | 53.43% | 76.05% | - | - |\n| | MID \u03b2 = 0.01 | 67.45%| 43.17% | 69.25% | - | - |\n| | HSIC \u03b2 = (0.05, 0.5) | 79.06% |  42.60% | 66.90%| -| -| \n| | HSIC \u03b2 = (0.05, 1.0) | 70.18%  | 27.10%|  50.30%| -| -| \n\nFor more details, please visit **Appx. C.5.** of the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469025876,
                "cdate": 1700469025876,
                "tmdate": 1700582019677,
                "mdate": 1700582019677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FsPHh4uIWe",
                "forum": "1SbkubNdbW",
                "replyto": "ge0NsER78l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Reviewer_dNRe"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you authors for the extensive rebuttal. I appreciate the detailed response, additional results and user studies. I also like to remark that this paper is entirely empirical.\n\nAfter reading the rebuttal and the other reviews, below I include my response for each portion:\n\n1. **User Study**: The human preference results presented are supportive and provide valuable insights. However, I suggest a potential refinement to the user study's design for future work. The central question of the study could be more effectively framed as \u201cWhich image resembles the person above?\u201d This approach would directly assess the comparative effectiveness of BiDO/MID against your method. Such a framing could offer a more objective understanding of the efficacy of negative label smoothing. Nonetheless, I acknowledge that the current results are satisfactory.\n\n$~$\n\n2. **Figure 4, column 3**. I\u2019m still confused by the penultimate visualization of Figure 4, column 3. In fact, if the classifier accuracy =91.45% for the model with $\\alpha=-0.05$, why are the penultimate representations of different classes mixed up? I.e.: Did you project the representations before dimensionally reducing them similar to [A, B] prior to plotting.\n\n$~$\n\n3. **Contradictory rebuttal results reported for LOMMA and PLG-MI.** The rebuttal results reported in Table 8 actually do not support the thesis of the paper:\n$~$\n- Why does positive label smoothing provide better defense compared to no label smoothing for LOMMA (GMI), despite having higher test accuracy.. I copy the reported rebuttal results below:\n\n| LOMMA (GMI)              | Test Acc | $\\alpha$ | Top1 Attack Acc |\n| ------------------------ | -------- | --------- | ----------------- |\n| Baseline                 | 85.74%   | 0.0         | 53.64% \u00b1 4.64     |\n| Positive Label Smoothing | 88.10%   | 0.1       | 50.96% \u00b1 3.52     |\n\n$~$\n- Why does negative label smoothing provide worse defense compared to no label smoothing for PLG-MI, despite a significantly lower test accuracy? I copy the reported rebuttal results below:\n\n| PLG-MI                   | Test Acc | $\\alpha$ | Top1 Attack Acc |\n| ------------------------ | -------- | --------- | --------------- |\n| Baseline                 | 85.74%   | 0.0         | 71.00% \u00b1 3.31   |\n| Negative Label Smoothing | 80.02%   | \\-0.05    | 72.00% \u00b1 2.50   |\n\n\n**Given that this paper is entirely empirical, critical results using state-of-the-art model inversion algorithms do not support this paper\u2019s thesis.**\n\n$~$\n\n4. **Lack of Theoretical/Analytical Support.** A major limitation of this work is the absence of solid theoretical/ analytical backing for the claims made. Beyond the toy experiment illustration in Section 3, the rationale behind how positive and negative label smoothing impacts model inversion attacks as suggested in the paper remains unclear. This concern is further compounded by the contradictory results for PLG-MI and LOMMA (GMI) in the rebuttal, which further obscures the paper's claims.\n\n\n$~$\n\n\n**Therefore, while I sincerely appreciate the authors' comprehensive rebuttal and the efforts put into it, I find myself still hesitant to vote for the acceptance of this paper in its current form (I have increased my rating from 3->5 as the rebuttal has addressed some of my concerns).**"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682567591,
                "cdate": 1700682567591,
                "tmdate": 1700684112245,
                "mdate": 1700684112245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "soowZUKynq",
            "forum": "1SbkubNdbW",
            "replyto": "1SbkubNdbW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_22v2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1183/Reviewer_22v2"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows that model trained with label smoothing can be more vulnerable to model inversion attacks while negative label smoothing can be a defense for the attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper looks at an interesting and important problem of how model training can affect model\u2019s vulnerability to MIAs.  \nThe experimental results in general justifies the statements well.\nThe presentation is clear."
                },
                "weaknesses": {
                    "value": "I\u2019m still wondering how much we can conclude the relation between vulnerability of MIAs and training methods (normal / smoothing / negative smoothing) from the empirical results shown here. For example,\n\n- different training methods lead to different model test accuracy (Table 1) especially for CelebA. I think it\u2019s fairer to compare the attack accuracy under the same model accuracy (e.g. early stop the label-smoothed model at a lower accuracy iteration), as it might be normal for a higher accuracy model leak more. It might also be interesting to look at training accuracy of the models to understand how well they generalize.\n\n- we\u2019re looking at one particular attack algorithm here. I think it\u2019s natural to ask whether another algorithm, or maybe an adjusted version this this algorithm can achieve a different result. For example, if the attacker knows that a model is trained with negative label smoothing (and thus has a different calibration than a normally-trained model), can they possibly sample more initial latent embeddings in the first stage, or adjust their objective function to incorporate with the calibration of this model in the second stage?"
                },
                "questions": {
                    "value": "In Fig 6b, why is the gradient similarity for label smoothing lower & with higher variance than that of hard label? I must admit that I don\u2019t have a good intuition here but I was kind of expecting the lines of smoothing and negative smoothing to stay on two sides of the hard label\u2019s line.\n\n(And those mentioned above.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244033466,
            "cdate": 1699244033466,
            "tmdate": 1699636044678,
            "mdate": 1699636044678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gsnXihIRIK",
                "forum": "1SbkubNdbW",
                "replyto": "soowZUKynq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 22v2,\nthank you very much for reviewing our paper. We are glad that you find our paper interesting and important. We will try to answer all your questions sufficiently.\n\n**1.) Attack comparison under same model accuracy:** We re-trained the models with positive LS and standard cross-entropy loss and stopped each training run as soon as the model's prediction accuracy on the holdout test set matched that of the model trained with negative LS. We then repeated the attacks against these models, which all achieve comparable prediction accuracy, to see if the attack success against these models drops. Our results clearly demonstrate that even under comparable test accuracy, the attacks still perform well and substantially better than on the negative LS models. This indicates that reducing a model's prediction accuracy is not the driving factor behind negative LS as a defense mechanism. We note that for FaceScrub, the attack against the model trained with positive LS performs slightly worse than for the model trained without any LS. However, the positive LS model already achieves the target accuracy after only 4 epochs, while the model without LS was trained for 19 epochs. Consequently, the positive LS model has seen much fewer samples and underwent fewer parameter updates to incorporate individual class features.\n\n|  FaceScrub |  \u03b1 |  \u2191 Test Acc |  \u2191 Acc@1 | \u2191 Acc@5 | \u2193 \u03b4face | \u2193 \u03b4eval |  \u2193FID | \n| -------- | ------- |------- |------- |------- |------- |------- |------- |\nNo Label Smoothing |0.0 |91.74%| 89.68%| 99.00% |0.7382 |133.12 |50.43|\nPositive Label Smoothing| 0.1 |92.13%| 87.16% |97.20% |0.7766 |135.73| 49.56|\nNegative Label Smoothing |\u22120.05| 91.45% |14.34%| 30.94% |1.2320 |239.02 |59.38 |16.45% |15.73%|\n\n|  CelebA |  \u03b1 |  \u2191 Test Acc |  \u2191 Acc@1 | \u2191 Acc@5 | \u2193 \u03b4face | \u2193 \u03b4eval |  \u2193FID | \n| -------- | ------- |------- |------- |------- |------- |------- |------- |\n|No Label Smoothing |0.0 |84.02%| 69.36% |90.60%| 0.7899| 344.82| 49.60|\n|Positive Label Smoothing |0.1 |86.78% |76.64%| 92.84%| 0.785| 332.53| 50.05|\n|Negative Label Smoothing |\u22120.05 |83.59%| 26.41%| 49.96%| 1.0420| 441.67 |61.30 |7.08%| 5.89%|\n\nOur results including a more detailed experimental setting and more elaborated discussion are also stated in **Appendix C.6**."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060356662,
                "cdate": 1700060356662,
                "tmdate": 1700060356662,
                "mdate": 1700060356662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]