[
    {
        "title": "FusionViT: Hierarchical 3D Object Detection via Lidar-Camera Vision Transformer Fusion"
    },
    {
        "review": {
            "id": "ZQDfXMml1q",
            "forum": "sGd02fkoAE",
            "replyto": "sGd02fkoAE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FusionViT, a 3D object detection framework that fuses 2D images with point cloud data. This framework consists of three components, including a 2D image model, a point cloud model, and a fusion model. All three components are based on vision transformer architectures. The method is evaluated on Waymo Open Datset and KITTI benchmarks for 2D & 3D object detections. The results show FusionViT can achieve performance that is competitive with latest works in 2D/3D object detections on those datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* This paper is an interesting exploration to use \"pure\" ViT architectures for 3D object detection. This is a sound research objective as ViT has demonstrated very strong performance in image classification and as a very strong model for visual embeddings. It is generally useful to explore adoption of this backbone in dense prediction in 3D tasks.\n\n* The paper presents a good amount of details and illustrations of the method. For the most part, concepts and algorithms are defined using precise language, assisted with helpful illustrations. \n\n* The proposed method is simple and clean, yet it shows strong performance against baselines that is a reasonable sample of recent works. The benchmarks are done on Waymo dataste and KITTI. Both are popular datasets suitable for evaluation of a 3D object detector."
                },
                "weaknesses": {
                    "value": "* It seems to be an exaggeration to claim that this work is \"the first study to investigate the possible pure-ViT based 3D object detection framework\".  Transformer based architectures for 3D tasks seem to have been explored extensively in the literature, see [1] for a survey.\n\n* The literature survey in this work is unfortunately not effective. While it lists some of the recent and classical fields, it fails to clearly define the relevance of the current work against the prior approaches. It will help position this work better if such an explicit analysis is presented.\n\n* The paper does not seem to provide much justifications on the various design choices. There are a few well-known 3D object detection paradigms in the literature, voxel based, pillar based and projection based. The paper focuses on the voxel based paradigm. It does not seem to be particularly favorable for a pure ViT architecture. Compared to pillar based approaches it is likely slower and harder to train due to the larger complexity in the attention layers. Compared to a projection based approach it is likely harder to align with the image features (being an 3D detection framework that fuses 2D and 3D). Given this large design space and obvious concerns, I think the paper should provide more rationale and ablation studies to justify the design choices.\n \n* While ViT architectures are shown to be very capable for classification/embedding tasks, it does have a few significant shortcomings in practice. For example, due to lack of the inductive bias it typically requires a large dataset for training to achieve competitive performance. Also, the quadratic complexity in attention layers makes it much harder to be used in dense prediction tasks. These are particularly relevant for 3D object detection using LiDAR as an input modality. It would be reasonable to expect a paper that set out to explore a \"pure-ViT based 3D object detection framework\" should provide deep analysis on those issues and propose effective mitigations of those shortcomings. It would also be reasonable to expect the same paper to demonstrate the superiority of ViT based architecture versus other sensible architecture choices, such as Swin based methods, despite the possible shortcomings. However, neither is presented in this work. \n\n* The comparison in Table 1 and Table 2 shows promising result of FusionViT compared to some reported numbers of prior works. But as an object detection system, it is critical to provide additional contexts of the accuracy achieved as more expensive systems typically have an edge in terms of accuracy. In this case, I think it is a minimum requirement to compare prior works at similar FLOPs. Better still, latency measured in clearly defined hardware platform should be provided.\n\n[1]Transformers in 3D Point Clouds: A Survey  https://arxiv.org/pdf/2205.07417.pdf"
                },
                "questions": {
                    "value": "* Beyond high level number, is there a detailed complexity/accuracy tradeoff provided for the reported results in Table 1 & Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v",
                        "ICLR.cc/2024/Conference/Submission8882/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806557854,
            "cdate": 1698806557854,
            "tmdate": 1700716405282,
            "mdate": 1700716405282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2AuQumKsP7",
                "forum": "sGd02fkoAE",
                "replyto": "ZQDfXMml1q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions. For weaknesses and questions mentioned, let us explain them in detail:\n\n1. As [1] Section 4.1.2 indicates, current transformer-based 3D Object Detection Frameworks are primarily restricted to DETR-based methods, that is, the frameworks with both encoder and decoder (such as 3DETR, CT3D, CAT-Det mentioned in [1]). Designing an encoder-based framework in 3D Object Detection is still a fairly unexplored area. \n\n2. While to the best of our knowledge it is the first study to investigate the possible pure-ViT-based 3D object detection framework, we accept the suggestion to conduct more exploration on ViT-related issues, such as large datasets for training and the quadratic complexity. We may consider conducting several mathematical analyses and ablation discussions and proposing more strategies to resolve these ViT common issues in future work. We could also report the FLOPs and inference time for a more comprehensive performance comparison. \n\n3. We clarify that due to space limitations, we cut down several literature surveys of this work. Nevertheless, we tried to outline the connection, pros and cons among the previous related works, as well as their relevance to the current work.\n\n4. As for comparing with other 3D object detection paradigms, we conducted serval experiments on them. For example, we compared our model with PointPillars and FasterPillars (Pillar-based methods), CenterPoint and Part-{A^2}-free (Point-based methods). For some assumptions such as `Compared to pillar-based approaches it is likely slower and harder to train due to the larger complexity in the attention layers`, and `Compared to a projection-based approach it is likely harder to align with the image features`, my opinion is that we cannot do such general categories' comparison (we could never prove that $a-$based framework must have a good performance (faster, more accurate, etc.) than $b-$based framework, since these framework categories are quite probably. There is no mathematical formula to restrict one method belonging to a group of frameworks. In other words, these frameworks are just categories by Point Cloud input feature preprocessing ways. After all, besides feature preprocessing, it is also important to design an efficient backbone, neck, detection head, and loss function. These efficient models could exist in any kind of data preprocessing category. Therefore, it is not reasonable to do justifications on the various design choices. One could only report methods performance from different such categories.\n\nIn light of our clarification and additional results, we would like to ask if the reviewer will reconsider the rating. Thank you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739987201,
                "cdate": 1700739987201,
                "tmdate": 1700739987201,
                "mdate": 1700739987201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1rWXDQbK5",
            "forum": "sGd02fkoAE",
            "replyto": "sGd02fkoAE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a vision transformer based lidar and camera fusion for 3D object detection. \nMulti-modal data provides different views of the same scene which makes it more feature-rich compared to single modality models. The paper is motivated by a lack of \u201cpure-ViT\u201d based 3D object detectors and proposes a model that uses independent ViT per modality (CameraViT and LidarViT) to extract single-modality features and fuses them using another ViT (MixViT) and performs bounding box regression and classification on the final output. The CameraViT operates on mini-patches and the LidarViT branch uses voxelization followed by filtering empty voxels and sampling to address the input size problem. The MixViT module operates on the concatenated features to address feature misalignment and modality differences. Experiments are performed on the Waymo Open and KITTI datasets, and show improvement over existing multimodal fusion works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a ViT-only approach for single modality representation learning and  multi-modal fusion in the context of 3D object detection and achieves comparable performance with other camera-lidar fusion based approaches. The lidar-only ViT branch uses voxelization to reduce dimensionality and shows good performance compared to existing lidar-only 3D detectors. The paper also shows ablation studies for the different components which indicates that all the proposed model components are contributing to the performance."
                },
                "weaknesses": {
                    "value": "The paper is motivated by the absence of \u201cpure-ViT\u201d based multi-modal 3D detection models. However, the paper doesn\u2019t explain why such a ViT-only approach is expected to be beneficial for the task.\n\nThe overall architecture is not quite novel in that most multi-modal fusion approaches, e.g., DeepFusion, Transfusion, DeepInteraction and any of the BEV fusion based approaches use single modality representation learning followed by multi-modal fusion. It\u2019s unclear what the novelty of the architecture shown in Fig. 1 is.\n\nMixViT uses a large MLP on the concatenated feature which is similar to how DeepFusion uses a localized MLP to learn alignment. The large MLP approach is still learning similar alignment but inefficient in terms of feature utilization.\n\nThere\u2019s comparison missing with DeepInteraction (NeurIPS 2022) around the same time as other papers which performs camera-lidar fusion for 3D object detection.\n\nThe paper doesn\u2019t show any robustness experiments with lidar-camera spatio-temporal misalignment or robustness of MixViT in the presence of single modality failures. \n\nPerformance on NuScenes is also missing in the paper.\n\nMinor\n-------\nThere are a several typos in the paper. For example,\n1. \"Swim\" Transformer written in several places\n2. Section 3.5, what is \"Multi-Level Perceptions\"?"
                },
                "questions": {
                    "value": "1. Why is pure-ViT expected to perform better than other approaches in the context of multi-modal 3D object detection? \n2. How well does the approach generalize to different data domains. For example, LidarViT to different types of lidar sensors.\n3. How does the approach perform under single modality failure and spatio-temporal misalignment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813349068,
            "cdate": 1698813349068,
            "tmdate": 1699637117640,
            "mdate": 1699637117640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2o1BdkzBbd",
                "forum": "sGd02fkoAE",
                "replyto": "E1rWXDQbK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions.  For weaknesses and questions mentioned, let us explain them in detail:\n\n1. We clarify that, as explained in detail in the Fourth paragraph of Section 1, designing a pure-ViT framework in 3D Object Detection is still a fairly unexplored area. To fill in this gap, we conducted extensive design discussions (Section 3) and experiments (Section 4.3) on it. In the end, we concluded what we found and proposed the presented FusionViT framework. \n\n2. We clarify that while multi-modal fusion is a general approach that utilizes more than one kind of input for CV tasks (detection), its following structures that embed and process these input features could be much different and require lots of innovation. For instance, DeepFusion emphasizes its two novel techniques: InverseAug and LearnableAlign. Transfusion highlights its soft-association mechanism and image-guided query initialization module. DeepInteraction advocates its modality interaction strategy. Our framework on the other hand promotes our pure-ViT hierarchical architecture. \nWe also note that while Transfusion and DeepInteraction use BEV as Lidar input, our method and DeepFusion directly use Point Cloud as the Lidar input. \n\n3. We agree that the ablation study could be sounder if we add more robustness experiments for MixViT (add `Without LidarViT and MixViT` and `Without LidarViT and MixViT` experiments in Table 4) and for lidar-camera spatio-temporal misalignment. Adding additional comparing experiments for DeepInteraction could also make our paper more convincing. We will add these in the revised version.\n\n4. KITTI, Waymo Open, and NuScenes are the three most popular datasets for 3D object detection in traffic scenarios. KITTI was first proposed in 2012, it has about 100GB of size which has been well suited for academic research. NuSense and Waymo Open are two industry-level datasets that contain more than 1TB of size for each. They are almost similar in data size, tasks, popularity, and resolution. Due to limited time and devices, we chose to conduct experiments on KITTI and Waymo Open to report the performance under a broader range we could do.\n\n5. We will correct the typos to have a nicer presentation to readers. Thanks for pointing out!\n\nIn light of our clarification and additional results, we would like to ask if the reviewer will reconsider the rating. Thank you!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730566804,
                "cdate": 1700730566804,
                "tmdate": 1700740032400,
                "mdate": 1700740032400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5OMflU3XzJ",
            "forum": "sGd02fkoAE",
            "replyto": "sGd02fkoAE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_Jv34"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8882/Reviewer_Jv34"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces FusionViT, a transformer architecture that fuses lidar and camera inputs for 3D object detection. \nTheir model is broadly composed of three components - CameraViT, LidarViT and MixViT.  \nThey train the components in three stages - CameraViT and LidarViT perform 2D and 3D detection, respectively,  \nand MixViT is trained to fuse the multi-modal features for 3D detection.  \nThey perform experiments on the KITTI and Waymo datasets and show their pure-transformer architecture can outperform other fusion baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* LidarViT and lidar transformers are a fairly unexplored area (to my knowledge) and is a solid contribution. \n* The high-level idea is straightworward and can be more-or-less summarized by Figure 1.\n* Numbers are strong considering the proposed work pivots to a full transformer architecture compared to the baselines."
                },
                "weaknesses": {
                    "value": "Experiment section, besides main results on KITTI and Waymo, not sufficient.  \nAblations are not that informative. The paper currently has two ablations now - one which is comparing sum vs. concat for fusion and the other compares removal of LidarViT, CameraViT, MixViT  \nModel runtime is key for object detection, and with these heavy transformer models, it would good to see some numbers on this.  \nThe authors also introduce a \"corner loss\" in Section 3.5, which I would have liked to see in the ablations.\nWhat other key design choices were made?\n\nWriting needs improvement. The writing in Sections 3.2, 3.3 is clear enough to understand,  \nbut the mathematical notation is overloaded and actually makes it harder to understand.  \nAn alternative is to summarize Equations 1, 2, 3 with a figure."
                },
                "questions": {
                    "value": "Table 1: from the writing, I assume the first row is performing 2D detection (comparing DETR, Swin, CameraViT).  \nThe second two rows are 3D detection with lidar and lidar-camera fusion, respectively. Why is the first group being compared to the second two?\n\nIs there any difference in the camera/fusion architecture when dealing with single-view (KITTI) and multi-view (Waymo) camera images?  \nHow/is the camera pose information being used in the positional embeddings?\n\nSection 3.6: the authors state they train the model in 3 stages due to \"some potential issues of large memory consumption\", but still run the model  \nend to end for training the MixVit? Are the subsequence layers frozen in this stage?\n\nThe authors use the word \"cubic\" to describe the 3D representation of the scene - is there any difference between this term and \"voxel\"?\n\nMinor typos:\n* Swim-transformer\n* hungingface"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699471264803,
            "cdate": 1699471264803,
            "tmdate": 1699637117526,
            "mdate": 1699637117526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zQd4iwJIa4",
                "forum": "sGd02fkoAE",
                "replyto": "5OMflU3XzJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions, and they are exceedingly helpful for us to improve our paper. We will improve the writing structure in Section 3 to have a nicer presentation to readers. \n\nFor the Experiment section, you mentioned it is not sufficient, could you explain it in more detail? We compared our model with several baselines. For the baseline choosing, we selected both classical and SOTA models, typically for Object Detection tasks in camera-only, lidar-only, and camera-lidar fusion input. In addition, we followed the experiment settings and reported the full evaluation metrics from both KITTI and WOD. \n\nAs for Ablations, we chose not to report `Model Runtime`, `The Loss Choices`, and other potential choices since they either do not have a great variation or not being the main research object in this experiment. `Model Runtime` is the first situation. While we will report the general `Model Runtime` in our experiment, it varies nearly negligible under different model structures. `The Loss Choices` is the second situation. The main contribution of this paper is to introduce a novel vision transformer-based 3D object detection model with pure-ViT-based hierarchical architecture. While it could be a future work, we focused on the model structure design, rather than the specific loss function that could match our models best.\n\nFor the Questions:\n1. For the experiments, as mentioned, we selected both classical and SOTA models, typically for Object Detection tasks in camera-only, lidar-only, and camera-lidar fusion input. We put these three groups of experiments together, to show the necessity of using 3D object Detection as well as using fusion. By comparing groups `1 and 2`, we should that 3D Object Detection frameworks generally perform better than pure 2D Detection. From groups `1 and 3` or `2 and 3` we show fusion strategies generally outperform single-modal strategies.\n\n2. We first clarify that while both KITTI and Waymo have multi-view camera images, we use just a single view for each dataset. For KITTI we used `camera2` and for Waymo we used `front`. Therefore, we do not need to encode the camera pose information.\n\n3. The subsequence layers of MixVit are not frozen in this stage. For the pertaining, we pre-train CameraViT and LidarViT on the training set. In the fine-tuning, only parameters in CameraViT and LidarViT are frozen.\n\n4. We clarify that `cubic` is a specification of `voxel`. Firstly introduced from VoxelNet, `voxel` is a general concept that discretizes point clouds. Our LidarViT is a voxel-based framework, which specifically uses `cubic` as the minimal feature representation. The structure and how it is got is explained in detailly in Section 3.3.\n\nIn light of our clarification and additional results, we would like to ask if the reviewer will reconsider the rating. Thank you!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720435810,
                "cdate": 1700720435810,
                "tmdate": 1700740058407,
                "mdate": 1700740058407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]