[
    {
        "title": "DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption"
    },
    {
        "review": {
            "id": "OW1Wv8UZtK",
            "forum": "4olqbTBt1Y",
            "replyto": "4olqbTBt1Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
            ],
            "content": {
                "summary": {
                    "value": "### Summary:\nThe paper introduces a novel exploration approach for reinforcement learning (RL) called Dream Dual Structured Exploration (DREAM). The main focus is to address the challenges of efficient exploration in sparse reward environments.\n\n#### Key Contributions:\n1. **Dual Structured Exploration:** The authors propose a two-pronged exploration strategy. The first component involves traditional intrinsic motivation where agents receive rewards for novel behaviors. The second component is a \"dream\" mechanism, where agents hallucinate or simulate possible future scenarios to guide their exploration.\n  \n2. **DREAM Model:** This model is central to the paper. It's a generative model that simulates future trajectories based on the agent's past experiences. By imagining potential future outcomes, the agent can better decide where to explore next. This approach aims to improve exploration efficiency, especially in environments where rewards are sparse and hard to find.\n  \n3. **Empirical Evaluation:** The authors validate the effectiveness of DREAM through a series of experiments in various RL environments. The results demonstrate that DREAM outperforms several state-of-the-art exploration strategies, especially in challenging sparse-reward settings.\n\n4. **Scalability and Flexibility:** The DREAM approach is shown to be scalable and can be combined with other RL algorithms. This adaptability makes it a promising tool for a wide range of applications."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "#### 1. Originality:\nThe paper introduces several original concepts and techniques that add value to the domain of reinforcement learning (RL) exploration.\n\n- **Dual Structured Exploration:** The combination of traditional intrinsic motivation and a \"dream\" mechanism is a unique and innovative approach. While intrinsic motivation is a well-established concept in RL, the idea of agents simulating or hallucinating future scenarios (dreaming) to guide their exploration is a fresh take on the exploration challenge.\n  \n- **DREAM Model:** The generative model that simulates future trajectories based on past experiences is a novel concept. It effectively bridges the gap between traditional exploration techniques and forward-thinking strategies, allowing agents to anticipate potential outcomes.\n\n#### 2. Quality:\nThe paper demonstrates high quality in both its theoretical constructs and empirical evaluations.\n\n- **Theoretical Foundation:** The underlying principles of the DREAM model and dual structured exploration are well-justified and rooted in established RL concepts.\n  \n- **Empirical Evaluation:** The experiments conducted are comprehensive, covering multiple RL environments. The results not only validate the efficacy of the DREAM approach but also provide insights into its potential advantages over other state-of-the-art methods.\n\n#### 3. Clarity:\nThe paper is well-structured and presents its concepts in a clear and organized manner.\n\n- **Presentation:** The flow of the paper, from introducing the problem to detailing the solution and its evaluation, is logical and easy to follow.\n  \n- **Figures and Diagrams:** The included visual aids, such as graphs and flowcharts, effectively complement the textual content, aiding in the understanding of the proposed concepts and results.\n\n- **Mathematical Formulations:** The mathematical representations and formulations, particularly those related to the DREAM model, are clearly articulated. While they require a foundational understanding of RL, they are accessible to the target audience.\n\n#### 4. Significance:\nThe contributions of this paper have considerable significance in the domain of RL exploration.\n\n- **Addressing a Crucial Challenge:** Efficient exploration in sparse reward environments is a longstanding challenge in RL. The DREAM approach offers a potential solution, making it a valuable contribution to the field.\n  \n- **Scalability and Flexibility:** The adaptability of the DREAM approach, which can be combined with other RL algorithms, broadens its applicability and potential impact. This adaptability implies that DREAM could be foundational for future RL research and applications.\n\n- **Potential for Further Research:** The concepts introduced open up avenues for further exploration, refinement, and application in other RL scenarios or even beyond RL."
                },
                "weaknesses": {
                    "value": "#### 1. Generality of the DREAM Model:\nWhile the DREAM model shows promise in the explored environments, the paper could benefit from a deeper discussion on its generality across diverse environments. \n**Actionable Insight:** Test the DREAM model in a broader set of RL environments, particularly those that have different dynamics or complexities than the ones currently evaluated. This would provide a more comprehensive understanding of where the model excels and where it might face challenges."
                },
                "questions": {
                    "value": "How does the DREAM model scale with increasing complexity of the RL environment, especially in terms of computational resources and time? Does the \"dream\" mechanism become more resource-intensive in more complex scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698303776420,
            "cdate": 1698303776420,
            "tmdate": 1699636763727,
            "mdate": 1699636763727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KBYCnXRO7O",
                "forum": "4olqbTBt1Y",
                "replyto": "OW1Wv8UZtK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ysQV"
                    },
                    "comment": {
                        "value": "We are truly grateful for the time you have taken to review our paper and your insightful review. Here we address your comments in the following.\n\n> Q1: While the DREAM model shows promise in the explored environments, the paper could benefit from a deeper discussion on its generality across diverse environments. Actionable Insight: Test the DREAM model in a broader set of RL environments, particularly those that have different dynamics or complexities than the ones currently evaluated. This would provide a more comprehensive understanding of where the model excels and where it might face challenges.\n\nA1: Thanks for your question. Firstly, we add a more complicated scenario with diverse environments. In particular, we have four different target domains and one source domain for each dataset. The performance comparison of different methods is shown as follows. From the results, we can conclude that our method has better generalization capacity compared with baselines. Secondly, thank you for bringing RL into our attention. In future works, we would extend the DREAM model to a broader set of RL environments with dynamic graphs.\n\n|Methods|MSRC_21|MSRC_21|MSRC_21|MSRC_21|Letter-high|Letter-high|Letter-high|Letter-high|\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n-|P1->P2|P1->P3|P1->P4|P1->P5|P1->P2|P1->P3|P1->P4|P1->P5|\n|RIGNN|60.8|61.4|60.3|59.7|45.1|44.5|43.9|42.6|\n|DREAM|63.2|63.8|61.1|62.1|47.3|45.9|46.8|44.6|\n\nWe have also added your suggestion about future works into our revised version. \n\n> Q2: How does the DREAM model scale with increasing complexity of the RL environment, especially in terms of computational resources and time? Does the \"dream\" mechanism become more resource-intensive in more complex scenarios?\n\nA2: Thanks for your question. We have analyzed the computational complexity of our method. The computational complexity primarily relies on two different branches for graph representations. Given a graph $G$, $||A||_0$ denotes the number of nonzeros in the adjacency matrix, $d$ denotes the feature dimension, $L$ denotes the number of layers, $T$ denotes the number of views, $R$ denotes the number of clusters. The graph-level representation learning branch takes $O(L||A||_0d+L|V|d^2+Td|V|+Td^2)$. The subgraph-view representation learning branch takes $O(L||A||_0d+L|V|d^2+R^2d)$. In our case, $R^2\\ll||A||_0$, $T\\ll d$ and $T\\ll|V|$. Therefore, the complexity of the proposed DREAM and GraphCL are both $O(L||A||_0d+L|V|d^2)$ for each graph sample, which is linearly related to $||A||_0$ and $|V|$. We would further explore the complexity of our method in more complicated scenarios such as graph-based reinforcement learning in our future works. \n\nWe have also added your suggestion about future works into our revised version. Thanks again for appreciating our work and for your constructive suggestions. Please let us know if you have further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291179661,
                "cdate": 1700291179661,
                "tmdate": 1700291179661,
                "mdate": 1700291179661,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V3kD75WGIb",
                "forum": "4olqbTBt1Y",
                "replyto": "KBYCnXRO7O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply."
                    },
                    "comment": {
                        "value": "Thanks for your reply. I think this is an interesting work, and it provides an extension view of future research. I still have a question: the status and reward of samples are an important basis for model training in RL, what is the main basis for judging that a sample belongs to a new class in this article?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634127430,
                "cdate": 1700634127430,
                "tmdate": 1700634127430,
                "mdate": 1700634127430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n4rhqCPShB",
                "forum": "4olqbTBt1Y",
                "replyto": "ErLWocuxxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_ysQV"
                ],
                "content": {
                    "comment": {
                        "value": "The idea of the manifold mixup seems interesting and enlightening. Overall, I think this is a good paper and can inspire the community."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636051168,
                "cdate": 1700636051168,
                "tmdate": 1700636051168,
                "mdate": 1700636051168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8LtKwctGty",
            "forum": "4olqbTBt1Y",
            "replyto": "4olqbTBt1Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_Pqqy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_Pqqy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for open-set graph adaptation, called DREAM. It combines attention mechanisms to enhance features at the graph level. Additionally, the network also includes a subgraph-enhanced branch. To address the open-set scenario, a special classifier and manifold mixup techniques are employed. In terms of adaptation, this paper utilizes the k-nearest neighbor and multi-sample mixup method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results have shown improvement, and the charts and figures in the experiments are clear and easy to understand."
                },
                "weaknesses": {
                    "value": "The contribution of the article is not clear. While the author elaborates on their method in detail, it is not evident how this work contributes in comparison to others.\nThere are some issues with the symbols in certain equations in this article. Should 'v' in Equation (1) be adjusted or corrected to 'h'? In Equation (4), 'g' and 'h' represent different entities but appear in the same position as 'p_\\theta(y| )'. Equation (11) seems unrelated to the subsequent formulas, and it maybe appear unnecessary.\nThe paper does not compare with methods of graph domain adaptation or methods for open-set graph classification. It is insufficient to only compare with graph classification and open-set classification algorithms."
                },
                "questions": {
                    "value": "Please refer to \u2018Weaknesses\u2019 section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665180839,
            "cdate": 1698665180839,
            "tmdate": 1699636763578,
            "mdate": 1699636763578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRaupxlGB5",
                "forum": "4olqbTBt1Y",
                "replyto": "8LtKwctGty",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pqqy"
                    },
                    "comment": {
                        "value": "We are truly grateful for the time you have taken to review our paper and your insightful review. Here we address your comments in the following.\n\n> Q1: The contribution of the article is not clear. While the author elaborates on their method in detail, it is not evident how this work contributes in comparison to others.\n\nA1: Thanks for your comment. The contribution of this paper is summarized as follows:\n\n* **New Problem** We introduce a novel problem of open-set graph domain adaptation, which accommodates unlabeled in-the-wild target graphs from unseen classes. \n\n* **Two branches to learn graph semantics in a complementary way**. Our graph-level representation branch employs the message passing and attention mechanism to explore topological knowledge while our subgraph-view branch focuses on key local functional parts using graph clustering and constructs a hierarchical GNN architecture.\n\n* **EM-style interaction between two branches**. We combine the advantages of two branches by posterior regularization to enhance the consistency between the predictions from the two branches.\n\n* **Well-designed Mixup for domain alignment and open-set classification**. We not only utilize linear interpolation to generate virtual novel samples in the hidden space, but also combine multiple cross-domain neighboring samples to generate virtual samples for domain alignment.\n\n* **Extensive Experiments.** Extensive performance comparisons to competing methods and ablation studies validate the superiority of our proposed DREAM over state-of-the-art methods.\n\n\n\n> Q2: Should 'v' in Equation (1) be adjusted or corrected to 'h'? In Equation (4), 'g' and 'h' represent different entities but appear in the same position as 'p_\\theta(y| )'. Equation (11) seems unrelated to the subsequent formulas, and it maybe appear unnecessary. \n\nA2: Thanks for your comment. We have corrected typos as below.\n\n- As for Equation (1): We have modified it to $h_i^{(l)}=COMBINE^{(l)}(h_i^{(l-1)},n_i^{(l)})$;\n- As for Equation (4): We have modified it into $L_s=-E_{(G,y) \\in D^s}[log p_\\theta (y|G)]-E_{(\\bar{h},y) \\in D^v} [log \\phi_\\theta^g (\\bar{h})[C+1]]$.\n- As for Equation (11): $A_{ij}$ appears in Equation (12) to guide the generation of virtual samples.\n\n\n\n> Q3: The paper does not compare with methods of graph domain adaptation or methods for open-set graph classification. It is insufficient to only compare with graph classification and open-set classification algorithms.\n\nA3: Thanks for your comment. We have added two graph domain adaptation methods: DEAL [1] and CoCo [2], and three open-set methods: RIGNN [3], OpenWGL [4], and OpenWRF [5] for comparison. The results are shown as below. From the results, we can validate the superiority of the proposed DREAM. \n\n\n|Methods|MSRC_21|MSRC_21|Letter-high|Letter-high|COIL-DEL|COIL-DEL|COIL-RAG|COIL-RAG|\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n-|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|\n|DEAL|$68.3\\pm3.8$|$67.9\\pm3.2$|$50.7\\pm2.9$|$47.3\\pm2.3$|$31.4\\pm4.3$|$27.7\\pm2.4$|$58.1\\pm2.3$|$54.2\\pm2.7$\n|CoCo|$69.8\\pm4.7$|$68.3\\pm2.1$|$50.3\\pm3.4$|$49.8\\pm1.4$|$33.7\\pm3.1$|$19.8\\pm1.7$|$61.3\\pm1.4$|$55.7\\pm1.6$|\n|RIGNN|$72.6\\pm4.3$|$73.8\\pm2.4$|$51.1\\pm3.2$|$43.8\\pm2.1$|$34.2\\pm2.9$|$33.5\\pm3.3$|$57.6\\pm2.1$|$57.2\\pm3.1$|\n|OpenWGL|$70.7\\pm4.2$|$69.5\\pm3.3$|$48.2\\pm2.8$|$40.9\\pm2.3$|$36.4\\pm2.1$|$33.8\\pm1.9$|$56.3\\pm3.3$|$54.8\\pm2.3$|\n|OpenWRF|$73.0\\pm3.7$|$70.4\\pm2.5$|$42.6\\pm2.6$|$40.4\\pm1.9$|$31.5\\pm1.8$|$30.2\\pm2.8$|$55.3\\pm2.4$|$54.8\\pm2.4$|\n|DREAM|**74.3**$\\pm$**5.4**|**75.2**$\\pm$**3.7**|**58.7**$\\pm$**3.5**|**53.3**$\\pm$**0.8**|**44.0**$\\pm$**2.6**|**40.2**$\\pm$**0.5**|**65.4**$\\pm$**1.7**|**62.5**$\\pm$**1.9**|\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider raising your score. If there are any additional notable points of concern that we have not yet addressed, please do not hesitate to share them, and we will promptly attend to those points.\n\n**Reference**\n\n[1] Hoffmann M, Galke L, Scherp A. Open-World Lifelong Graph Learning. IJCNN 2023.\n\n[2] Yin N, Shen L, Li B, Wang M, Luo X, Chen C, Luo Z, Hua XS.. DEAL: An Unsupervised Domain Adaptive Framework for Graph-level Classification. ACM MM 2022.\n\n[3] Yin N, Shen L, Wang M, Lan L, Ma Z, Chen C, Hua XS, Luo X. CoCo: Let Weisfeiler-Lehman Kernel Improve Unsupervised Domain Adaptive Graph Classification. ICML 2023.\n\n[4] Luo X, Zhao Y, Mao Z, Qin Y, Ju W, Zhang M, Sun Y. RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification. TMLR 2023.\n\n[5] Wu M, Pan S, Zhu X. Openwgl: Open-world graph learning. ICDM 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291029220,
                "cdate": 1700291029220,
                "tmdate": 1700310883092,
                "mdate": 1700310883092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fnlkTzY0oe",
                "forum": "4olqbTBt1Y",
                "replyto": "8LtKwctGty",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The deadline for the author-reviewer discussion phase is approaching!"
                    },
                    "comment": {
                        "value": "Dear reviewers,\n\nWe sincerely appreciate your valuable feedback.\n\nAs the deadline for the author-reviewer discussion phase is approaching, we would like to check if you have any other remaining concerns about our paper. If our responses have adequately addressed your concerns, we kindly hope that you can consider increasing the score.\n\nWe sincerely thank you for your dedication and effort in evaluating our submission. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\nBest Regards,\n\nAuthors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553233695,
                "cdate": 1700553233695,
                "tmdate": 1700553233695,
                "mdate": 1700553233695,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CVFnSewRyU",
                "forum": "4olqbTBt1Y",
                "replyto": "8LtKwctGty",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\n\nThank you for your invaluable feedback. As the deadline for the author-reviewer discussion phase is approaching, we hope to make sure that our response sufficiently addressed your concerns regarding the contribution, as well as the revised version of our paper. We hope this could align with your expectations and positively influence the score. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\n\n\nBest Regards,\n\nAuthors"
                    },
                    "title": {
                        "value": "Thank you for your invaluable feedback!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737422730,
                "cdate": 1700737422730,
                "tmdate": 1700741118790,
                "mdate": 1700741118790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1tHtN364yc",
            "forum": "4olqbTBt1Y",
            "replyto": "4olqbTBt1Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_qwA4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_qwA4"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of open-set graph domain adaptation. The proposed method extracts graph structure representations using complementary branches. The graph-level representation branch uses a MPNN followed by attention layer for aggregation. The subgraph branch split each graph into several subgraphs using graph clustering and extract representations with GNNs. The method also includes dissimilar source samples in the latent space, and a k-nearest neighbor-based graph where nodes represent graph samples and are combined to generate new samples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies open-set graph learning, which is an interesting and practical setting.\n\n2. The paper presentation includes rich contents, with tables and figures well organized.\n\n3. The conducted experiments look correct and include analysis from multiple views including ablation studies and sensitivity analysis."
                },
                "weaknesses": {
                    "value": "1. Novelty overclaimed and related works not well addressed. The authors claim that \"we are the first to study open-set graph domain adaptation\". However, the problem studied is no difference with the existing open-world graph classification, such as [1], where the task is to classify each unlabeled graph example into either one of the known classes or a corresponding novel class. Moreover, it also closely resembles the open-world graph learning works like [2,3], where the learning goal is to classify nodes belonging to seen classes into correct groups, but also classify nodes not belonging to existing classes to an unseen class. The paper lacks a thorough review of related literature. In addition to open-world graph works, fields such as (graph) OOD detection is also closely related and should be discussed in the related works.\n\n2. Following the above point, the experiments should include open-world graph learning related baselines. Currently only general graph classification methods are compared.\n\n3. The method design include a lot of modules but lack support and motivations. Why is the attention mechanism necessary for aggregation? Why can the graph-of-graph design generate plausible cross-domain virtual features? For the objective why are $L_S, L_T, L_{DA}$ added without weights? The overall method seems complex and farraginous and unclear why it works.\n\n\n\n[1] RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification\n\n[2] Openwgl: Open-world graph learning\n\n[3] Open-World Lifelong Graph Learning"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779772120,
            "cdate": 1698779772120,
            "tmdate": 1699636763461,
            "mdate": 1699636763461,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6Fe5uXEknE",
                "forum": "4olqbTBt1Y",
                "replyto": "1tHtN364yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qwA4 (I)"
                    },
                    "comment": {
                        "value": "We are truly grateful for the time you have taken to review our paper and your insightful review. Here we address your comments in the following.\n\n> Q1: Novelty overclaimed and related works not well addressed. The authors claim that \"we are the first to study open-set graph domain adaptation\". However, the problem studied is no difference with the existing open-world graph classification, such as [1], where the task is to classify each unlabeled graph example into either one of the known classes or a corresponding novel class. Moreover, it also closely resembles the open-world graph learning works like [2,3], where the learning goal is to classify nodes belonging to seen classes into correct groups, but also classify nodes not belonging to existing classes to an unseen class. The paper lacks a thorough review of related literature. In addition to open-world graph works, fields such as (graph) OOD detection is also closely related and should be discussed in the related works.\n\nA1: Thanks for your comment. Compared with open-world graph classification [1], our problem **also considers potential distribution shifts** across labeled and unlabeled data. Out-of-distribution unlabeled data [4] is always ubiquitous in practical graph learning and our problem aims to align the representations of source and target data besides OOD detection. Compared with node-level open-world graph learning [3,4] which studies a single graph with extensive nodes, our problem involves **a large number of graphs** and graph-level labels. We have included the discussion of open-set graph domain adaptation, node-level open-world graph learning and graph OOD detection into our related works as follows:\n\n\"Recently, graph neural networks have also been studied in different OOD settings. Semi-supervised open-world graph classification involves partial unlabeled graphs belonging to unknown classes [1]. Graph OOD detection aims to detect OOD graph samples without using ground-truth labels [2]. Node-level open-world graph learning aims to find OOD nodes on a single graph [3,5]. Compared with these problem settings, we not only detect OOD graph samples, but also overcome distribution shifts across source and target domains.\"\n\n\n> Q2: Following the above point, the experiments should include open-world graph learning related baselines. Currently only general graph classification methods are compared.\n\nA2: Thanks for your question. We adapt more open-world graph learning methods, i.e., **RIGNN [1], OpenWGL [3] and OpenWRF [5]** in our setting for comparison. The results are shown as below. We can observe that our proposed DREAM surpasses the performance of baseline models, highlighting the superiority of the proposed method.\n\n|Methods|MSRC_21|MSRC_21|Letter-high|Letter-high|COIL-DEL|COIL-DEL|COIL-RAG|COIL-RAG|\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n-|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|\n|RIGNN|$72.6\\pm4.3$|$73.8\\pm2.4$|$51.1\\pm3.2$|$43.8\\pm2.1$|$34.2\\pm2.9$|$33.5\\pm3.3$|$57.6\\pm2.1$|$57.2\\pm3.1$|\n|OpenWGL|$70.7\\pm4.2$|$69.5\\pm3.3$|$48.2\\pm2.8$|$40.9\\pm2.3$|$36.4\\pm2.1$|$33.8\\pm1.9$|$56.3\\pm3.3$|$54.8\\pm2.3$|\n|OpenWRF|$73.0\\pm3.7$|$70.4\\pm2.5$|$42.6\\pm2.6$|$40.4\\pm1.9$|$31.5\\pm1.8$|$30.2\\pm2.8$|$55.3\\pm2.4$|$54.8\\pm2.4$|\n|DREAM|**74.3**$\\pm$**5.4**|**75.2**$\\pm$**3.7**|**58.7**$\\pm$**3.5**|**53.3**$\\pm$**0.8**|**44.0**$\\pm$**2.6**|**40.2**$\\pm$**0.5**|**65.4**$\\pm$**1.7**|**62.5**$\\pm$**1.9**|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290848284,
                "cdate": 1700290848284,
                "tmdate": 1700311050509,
                "mdate": 1700311050509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3U2VXvYIqP",
                "forum": "4olqbTBt1Y",
                "replyto": "1tHtN364yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qwA4 (II)"
                    },
                    "comment": {
                        "value": "> Q3: The method design includes a lot of modules but lack support and motivations. Why is the attention mechanism necessary for aggregation? Why can the graph-of-graph design generate plausible cross-domain virtual features? For the objective why are L_s, L_T, L_DA  added without weights? The overall method seems complex and ferruginous and unclear why it works.\n\nA3: Thanks for your question. We will explain the motivations of the modules in detail:\n\n* **The Attention Mechanism**: We have added a model variant (DREAM w/o A), which replaces the attention mechanism with the global pooling. The compared results are shown below. From the results, we can observe that our full model outperforms DREAM w/o A. The reason is that a global pooling operator cannot capture the task relevance of nodes and structural dependencies while our attention mechanism can generate super-nodes for semantic exploration. \n\n\n* **Graph-of-Graph**: Our graph-of-graph connects graph samples to identify similar cross-domain graphs for each graph. Then mixing these similar graphs in the embedding space would also generate virtual representations with similar semantics and encourage the consistency between original samples and their cross-domain virtual representations would thus enhance the domain alignment. In addition, we have included a model variant (DREAM w/o MM), which removes the graph-of-graphs for domain alignment and the following Mixup. The compared results are shown below. From the results, we can observe that DREAM w/o MM performs much worse, which validates our graph-of-graph is important for domain alignment. \n\n* **The Weights of Objective Loss**: We have included the weights by modifying the overall loss to $L=L_S+\\alpha L_T+\\beta L_DA$. To determine the hyper-parameters of $\\alpha$ and $\\beta$, we study the sensitivity analysis with $\\alpha$ and $\\beta$ in the range of {0.6, 0.8, 1, 1.2, 1.4}, which is shown in the Table. We can find that our performance is robust to both $\\alpha$ and $\\beta$. Therefore, we directly set the default parameter to $1$.\n\n\n|Methods |Letter-high|Letter-high|COIL-DEL|COIL-DEL|\n|:---|:--:|:--:|:--:|:--:|\n-|P1->P2|P2->P1|P1->P2|P2->P1|\n|DREAM w/o A|55.4|51.0|40.3|38.7|\n|DREAM w/o MM|56.1|51.3|41.0|39.1|\n|DREAM|**58.7**|**53.3**|**44.0**|**40.2**|\n\n|Methods|MSRC_21|MSRC_21|Letter-high|Letter-high|COIL-DEL|COIL-DEL|COIL-RAG|COIL-RAG|\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n-|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|\n|$\\alpha$=0.6|74.9|75.5|57.8|53.2|42.7|38.4|64.6|61.7|\n|$\\alpha$=0.8|75.8|76.1|58.4|53.7|43.9|40.2|65.3|62.2|\n|$\\alpha$=1|76.7|76.9|59.2|54.4|44.2|41.3|64.8|62.3|\n|$\\alpha$=1.2|75.4|76.2|59.3|54.0|44.7|40.9|64.2|61.6|\n|$\\alpha$=1.4|75.1|75.6|58.5|53.4|43.5|39.6|63.3|61.1|\n\n|Methods|MSRC_21|MSRC_21|Letter-high|Letter-high|COIL-DEL|COIL-DEL|COIL-RAG|COIL-RAG|\n|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n-|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|P1->P2|P2->P1|\n|$\\beta$=0.6|74.0|75.1|57.9|51.9|43.3|38.7|64.8|61.5|\n|$\\beta$ =0.8|74.6|75.5|58.7|53.4|44.2|40.4|65.7|62.8|\n|$\\beta$ =1|73.8|75.4|58.1|53.7|44.1|40.7|65.1|63.3|\n|$\\beta$ =1.2|73.0|74.7|57.4|52.5|43.9|39.6|65.3|63.4|\n|$\\beta$ =1.4|72.4|73.8|56.8|51.4|43.5|39.1|64.4|62.8|\n\nIn light of these responses, we hope we have addressed your concerns, and hope you will consider raising your score. If there are any additional notable points of concern that we have not yet addressed, please do not hesitate to share them, and we will promptly attend to those points.\n\n\n**Reference**\n\n[1] Luo X, Zhao Y, Mao Z, Qin Y, Ju W, Zhang M, Sun Y. RIGNN: A Rationale Perspective for Semi-supervised Open-world Graph Classification. TMLR 2023.\n\n[2] Liu Y, Ding K, Liu H, Pan S. GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection. WSDM 2023\n\n[3] Wu M, Pan S, Zhu X. Openwgl: Open-world graph learning. ICDM 2020.\n\n[4] Gui S, Li X, Wang L, Ji S. GOOD: A Graph Out-of-Distribution Benchmark. NeurIPS 22\n\n[5] Hoffmann M, Galke L, Scherp A. Open-World Lifelong Graph Learning. IJCNN 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290932398,
                "cdate": 1700290932398,
                "tmdate": 1700311215211,
                "mdate": 1700311215211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g8jY1lU9VN",
                "forum": "4olqbTBt1Y",
                "replyto": "1tHtN364yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The deadline for the author-reviewer discussion phase is approaching!"
                    },
                    "comment": {
                        "value": "Dear reviewers,\n\nWe sincerely appreciate your valuable feedback.\n\nAs the deadline for the author-reviewer discussion phase is approaching, we would like to check if you have any other remaining concerns about our paper. If our responses have adequately addressed your concerns, we kindly hope that you can consider increasing the score.\n\nWe sincerely thank you for your dedication and effort in evaluating our submission. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\nBest Regards,\n\nAuthors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553216014,
                "cdate": 1700553216014,
                "tmdate": 1700553216014,
                "mdate": 1700553216014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oM0MJDQaRM",
                "forum": "4olqbTBt1Y",
                "replyto": "g8jY1lU9VN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_qwA4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_qwA4"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed explanations, which adds to the completeness of the work. To me, the contribution and novelty of this paper is not prominent. Given this work's similarity with open-world graph works, it's hard to claim open-set graph domain adaptation as a new problem. The graph-of-graph technique has been explored in previous open-world works. Also, regarding the question on plausible cross-domain virtual representations, there's no guarantee that mixing similar graphs would generate virtual representations with similar semantics. This assumption is rather strong without additional supervision. I choose to maintain my original rating of this work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724039555,
                "cdate": 1700724039555,
                "tmdate": 1700724039555,
                "mdate": 1700724039555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jPE6gC6ATQ",
                "forum": "4olqbTBt1Y",
                "replyto": "1tHtN364yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your feedback!"
                    },
                    "comment": {
                        "value": "Thanks for your feedback and we are happy to resolve your further concerns as follows:\n\n> Q1. Given this work's similarity with open-world graph works, it's hard to claim open-set graph domain adaptation as a new problem.\n\nA1. The difference between our problem and traditional open-world graph problems is the introduction of distribution shifts between training data and test data. Given that **extensive works [1,2] focus on distribution shifts**, we believe that our problem is more challenging than the classic open-world graph problems. We have also revised our manuscript accordingly. \n\n> Q2. The graph-of-graph technique has been explored in previous open-world works.\n\nA2. Thanks for your problem. You may refer to RIGNN [3]. Actually, graph-of-graph is not our contribution while one of our related contributions is **Well-designed Mixup for domain alignment and open-set classification**. We not only utilize linear interpolation to generate virtual novel samples in the hidden space, but also combine multiple cross-domain neighboring samples based on graph-of-graph to generate virtual samples for domain alignment. \n\nMoreover, we summarize the differences between RIGNN and our method as follows:\n\n- **Different methodology.** RIGNN constructs a graph-of-graph to connect unlabeled graphs with labeled graphs while our DREAM focuses on the exploration of cross-domain relationships between graphs. \n- **Different motivations** RIGNN builds a graph-of-graph to enhance the representation learning while our DREAM aims to align graph representation from different sources. \n- **Different objectives**. RIGNN constructs a graph-of-graph for contrastive learning while our DREAM utilizes the graph-of-graph to guide our multi-sample Mixup for domain alignment. \n\n> Q3. regarding the question on plausible cross-domain virtual representations, there's no guarantee that mixing similar graphs would generate virtual representations with similar semantics. This assumption is rather strong without additional supervision.\n\nA3. Thanks for your comment. We have three reasons to support this:\n\n- A recognized fact is that the features of samples with the same semantic information should lay on a high-dimensional manifold as in [4,5]. Therefore, the mixed representations should be still in the high-dimensional manifold, which indicates the similar semantics in most cases. \n\n- Mixing representations from different samples have been introduced in manifold Mixup [6]. This paper [6] shows that mixing in the latent space can produce virtual samples with similar semantics in meaningful regions. Therefore, the assumption is not strong. Moreover, our method extends manifold Mixup into our new setting for domain alignment and open-set classification. \n\n- Our ablation studies (comparison between DREAM w/o Multi-sample Mixup and DREAM) have been shown in Table 3. From the results, we can observe that introducing our Mixup performs much better, which validates our component is reasonable and crucial for domain alignment.\n\nWe have also revised our manuscript accordingly.\n\n> Q4. To me, the contribution and novelty of this paper is not prominent. \n\nA4. Thanks for your comment. We want to emphasize our contribution as follows:\n\n* **New Problem** We introduce a novel problem of open-set graph domain adaptation, which accommodates unlabeled in-the-wild target graphs from unseen classes. \n\n* **Two branches to learn graph semantics in a complementary way**. Our graph-level representation branch employs the message passing and attention mechanism to explore topological knowledge while our subgraph-view branch focuses on key local functional parts using graph clustering and constructs a hierarchical GNN architecture.\n\n* **EM-style interaction between two branches**. We combine the advantages of two branches by posterior regularization to enhance the consistency between the predictions from the two branches.\n\n* **Well-designed Mixup for domain alignment and open-set classification**. We not only utilize linear interpolation to generate virtual novel samples in the hidden space, but also combine multiple cross-domain neighboring samples to generate virtual samples for domain alignment.\n\n* **Extensive Experiments.** Extensive performance comparisons to competing methods and ablation studies validate the superiority of our proposed DREAM over state-of-the-art methods.\n\n\nThanks again for your constructive suggestions. Please let us know if you have further questions.\n\n**Reference**\n\n[1] GOOD: A Graph Out-of-Distribution Benchmark, NeurIPS 2022\n\n[2] SizeShiftReg: a Regularization Method for Improving Size-Generalization in Graph Neural Networks, NeurIPS 2022\n\n[3] RIGNN: A Rationale Perspective for Semi-supervised Open- world Graph Classification, TMLR 2023\n\n[4] Conditional Adversarial Domain Adaptation, NeurIPS 2018\n\n[5] CIMON: Towards High-quality Hash Codes, IJCAI 2021\n\n[6] Manifold Mixup: Better Representations by Interpolating Hidden States, ICML 2018"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727043591,
                "cdate": 1700727043591,
                "tmdate": 1700728607508,
                "mdate": 1700728607508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YuB7ePMXEB",
                "forum": "4olqbTBt1Y",
                "replyto": "1tHtN364yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\n\nThank you for your invaluable feedback. As the deadline for the author-reviewer discussion phase is approaching, we hope to make sure that our response sufficiently addressed your concerns regarding the contribution, as well as the revised version of our paper. We hope this could align with your expectations and positively influence the score. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\n\n\nBest Regards, \n\nAuthors"
                    },
                    "title": {
                        "value": "Thank you for your invaluable feedback!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737367511,
                "cdate": 1700737367511,
                "tmdate": 1700741109707,
                "mdate": 1700741109707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FIv8Kn93Ec",
            "forum": "4olqbTBt1Y",
            "replyto": "4olqbTBt1Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_9UFe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_9UFe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method called DREAM for open-set graph domain adaptation, which aims to accurately classify target graphs into their respective categories under domain shift and label scarcity. DREAM incorporates a graph-level representation learning branch as well as a subgraph-enhanced branch, which jointly explores graph topological structures from both global and local viewpoints. The method also amalgamates dissimilar samples to generate virtual unknown samples belonging to novel classes and establishes a k nearest neighbor-based graph-of-graphs to alleviate domain shift. Extensive experiments demonstrate the superiority of DREAM over state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novelty: The paper introduces a new problem of open-set graph domain adaptation, which accommodates unlabeled in-the-wild target graphs from unseen classes. The proposed method, DREAM, is a novel approach that employs two branches to investigate structural semantics and integrates them into a trustworthy and domain-invariant framework. \n2. Effectiveness: The paper demonstrates the remarkable effectiveness of DREAM when compared to state-of-the-art methods in various challenging scenarios. In particular, the performance gain of DREAM over the best existing method is up to an impressive 15.5%. \n3. Flexibility: DREAM is a flexible method that can handle open-set scenarios and mitigate domain shift. It generates virtual unknown samples belonging to novel classes for additional supervision in the open-set scenarios and constructs a k nearest neighbor-based graph-of-graph to generate cross-domain counterparts using multi-sample mixup, which helps to improve cross-domain consistency. \n4. Clarity: The paper is well-written and easy to understand. The authors provide clear explanations of the problem formulation, methodology, and experiments, making it accessible to a wide range of readers."
                },
                "weaknesses": {
                    "value": "1.There seems a lot of modules in the DREAM, it\u2019s better to analysis the complexity of the proposed method.\n2.What I am concern is the scalability of this model, i.e., whether this method can be applied into the dynamic graph scenario for learning."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699072816194,
            "cdate": 1699072816194,
            "tmdate": 1699636763360,
            "mdate": 1699636763360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fgOrzcp0ji",
                "forum": "4olqbTBt1Y",
                "replyto": "FIv8Kn93Ec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9UFe"
                    },
                    "comment": {
                        "value": "We are truly grateful for the time you have taken to review our paper, your insightful comments and support. Your positive feedback is incredibly encouraging for us! In the following response, we would like to address your major concern and provide additional clarification.\n\n> Q1: There seem a lot of modules in the DREAM, it\u2019s better to analysis the complexity of the proposed method. \n\nA1: Thanks for your comment. We have analyzed the computational complexity of our method. The computational complexity primarily relies on two different branches for graph representations. Given a graph $G$, $||A||_0$ denotes the number of nonzeros in the adjacency matrix, $d$ denotes the feature dimension, $L$ denotes the number of layers, $T$ denotes the number of views, $R$ denotes the number of clusters. The graph-level representation learning branch takes $O(L||A||_0d+L|V|d^2+Td|V|+Td^2)$. The subgraph-view representation learning branch takes $O(L||A||_0d+L|V|d^2+R^2d)$. In our case, $R^2\\ll||A||_0$, $T\\ll d$ and $T\\ll|V|$. Therefore, the complexity of the proposed DREAM and GraphCL are both $O(L||A||_0d+L|V|d^2)$ for each graph sample, which is linearly related to $||A||_0$ and $|V|$.  We have added this into the revised version.\n\n> Q2: What I am concern is the scalability of this model, i.e., whether this method can be applied into the dynamic graph scenario for learning.\n\nA2: Thanks for your comment. We have included the complexity analysis, showing that our method has a comparable scalability as popular GraphCL. In future works, we would try to extend the work to more complicated scenarios such as dynamic graphs.\n\nWe have also added your suggestion about future works to our revised version. Thanks again for appreciating our work and for your constructive suggestions. Please let us know if you have further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290706765,
                "cdate": 1700290706765,
                "tmdate": 1700308550070,
                "mdate": 1700308550070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GBcovJt3Rj",
                "forum": "4olqbTBt1Y",
                "replyto": "fgOrzcp0ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_9UFe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Reviewer_9UFe"
                ],
                "content": {
                    "title": {
                        "value": "More questions."
                    },
                    "comment": {
                        "value": "Thanks for your reply. I think the authors have addressed my majority question. The setting of graph open-set domain adaptation is novel, can the author provide some circumstances in real-world applications?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634940697,
                "cdate": 1700634940697,
                "tmdate": 1700634940697,
                "mdate": 1700634940697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kw0VlizUWP",
                "forum": "4olqbTBt1Y",
                "replyto": "FIv8Kn93Ec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\n\nThank you for your invaluable feedback. As the deadline for the author-reviewer discussion phase is approaching, we hope to make sure that our response sufficiently addressed your concerns regarding the real-world applications, as well as the revised version of our paper. We hope this could align with your expectations. Please do not hesitate to let us know if you need any clarification or have additional suggestions.\n\n\n\nBest Regards,\n\nAuthors"
                    },
                    "title": {
                        "value": "Thank you for your invaluable feedback!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739532989,
                "cdate": 1700739532989,
                "tmdate": 1700741100202,
                "mdate": 1700741100202,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OYnzYuH1IL",
            "forum": "4olqbTBt1Y",
            "replyto": "4olqbTBt1Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_SVBJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6668/Reviewer_SVBJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes DREAM for open-set graph domain adaptation, which incorporates a graph-level representation learning branch as well as a subgraph-enhanced branch to jointly explores graph topological structures from both global and local viewpoints."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe problem of open-set graph domain adaptation is novel.\n2.\tThe paper is well organized and clearly written.\n3.\tThe proposed method is clever and interesting."
                },
                "weaknesses": {
                    "value": "1.\tThe format of references is not uniform, such as [4] and [5].\n2.\tWhat\u2019s the difference between the open-set graph domain adaptation and universal domain adaptation? Can the proposed model be extended to UDA?"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6668/Reviewer_SVBJ"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137742171,
            "cdate": 1699137742171,
            "tmdate": 1699636763260,
            "mdate": 1699636763260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "akWgaFfdMA",
                "forum": "4olqbTBt1Y",
                "replyto": "OYnzYuH1IL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SVBJ"
                    },
                    "comment": {
                        "value": "We are truly grateful for the time you have taken to review our paper, your insightful comments and support. Your positive feedback is incredibly encouraging for us! In the following response, we would like to address your major concern and provide additional clarification.\n\n> Q1: The format of references is not uniform, such as [4] and [5].\n\nA1: Thanks for your comment. We have unified the reference format.\n\n> Q2: What\u2019s the difference between the open-set graph domain adaptation and universal domain adaptation? Can the proposed model be extended to UDA?\n\nA2: Thanks for your comment. Universal domain adaptation allows two domains to own their private categories, which is a more generalized problem including open-set domain adaptation, partial domain adaptation and open-partial domain adaptation. In contrast, we only focus on open-set domain adaptation on graphs. We would extend our DREAM to more generalization problems such as universal graph domain adaptation in our future work.\n\nWe have also added your suggestion about future works to our revised version. Thanks again for appreciating our work and for your constructive suggestions. Please let us know if you have further questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289757247,
                "cdate": 1700289757247,
                "tmdate": 1700290675453,
                "mdate": 1700290675453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]