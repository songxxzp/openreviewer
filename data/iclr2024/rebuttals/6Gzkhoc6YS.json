[
    {
        "title": "Personalize Segment Anything Model with One Shot"
    },
    {
        "review": {
            "id": "1RXddyEvWI",
            "forum": "6Gzkhoc6YS",
            "replyto": "6Gzkhoc6YS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes PerSAM, a novel training-free method, to customize the general-purpose SAM for personalized object segmentation by using a single image with a reference mask. Additionally, the paper introduces PerSAM-F, an efficient variant that enhances performance by tuning just two parameters within 10 seconds. The effectiveness of the proposed method is demonstrated by comprehensive experiments and ablation studies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* This paper is well-written and easy to understand.\n\n* This paper first studies an interesting task of customizing a general-purpose segmentation model for personalized scenarios. And the paper presents a highly effective method to address this task.\n\n* The method is simple and easy to follow. The proposed PerSAM can guide SAM to segment target objects by three effective training-free techniques.  By tuning 2 parameters within 10 seconds, PerSAM-F efficiently alleviates the mask ambiguity issue and improves the performance.\n\n* This paper has comprehensive discussions and experiments and shows good performances on various tasks."
                },
                "weaknesses": {
                    "value": "The feature semantics of SAM might be limited due to SAM's class-agnostic training. While PerSAM and PerSAM-F demonstrate promising performance in personalized object segmentation, their effectiveness may be constrained by SAM's feature semantics in scenarios involving multiple different objects. This may require additional training to enable better transfer of SAM's features to downstream tasks. Alternatively, introducing other representations with stronger semantics, such as CLIP."
                },
                "questions": {
                    "value": "* Can PerSAM be extended to do few-shot segmentation with more reference masks for achieving better performance? In the real scenario, utilizing 5-shot or 10-shot examples does not significantly increase the cost compared to 1-shot, but it does offer more precise visual information.\n\n* Is the fine-tuned PerSAM-F generalized only to a specific object? Can PerSAM generalize to different objects using the same parameters? What about using different fine-tuning methods such as LoRA for multi-objects generalization performance?\n\n* Can PerSAM help DreamBooth achieve more complex multi-object customization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583535079,
            "cdate": 1698583535079,
            "tmdate": 1699636160097,
            "mdate": 1699636160097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p5ZX946A86",
                "forum": "6Gzkhoc6YS",
                "replyto": "1RXddyEvWI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer inm4"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable reviews and recognition of our work. We hope our response can address your concerns.\n\n---\n> **Q1: PerSAM and PerSAM-F may be constrained by SAM's limited semantics by class-agnostic training**\n\n>\n1. Indeed, due to SAM's inherent class-agnostic training, the visual features extracted by SAM's encoder contain limited category-level semantics. This might constrain the category-level discriminative capability for complex multi-object scenes. Observing this limitation, we locate the target object among other objects in test images entirely by ***feature matching***, i.e., the location confidence map. Such a matching strategy only considers the appearance-based class-agnostic similarity, without category semantics.\n\n2. As you suggested, we can leverage other semantically rich image encoders, e.g., CLIP and DINOv2, for PerSAM(-F) to improve the multi-object performance. We conduct an ablation study of different image encoders on DAVIS 2017 dataset for video object segmentation, which contains multiple similar objects within a video. As shown in the table below, applying CLIP and DINOv2 with more sufficient semantic knowledge can improve the results of PerSAM-F for more challenging multi-object segmentation.\nEncoder|J&F|J|F\n-|:-:|:-:|:-:\nSAM|73.4|70.9|75.9\nCLIP|74.9|72.8|77.0\nDINOv2|76.1|73.2|78.9\n\n\n---\n> **Q2: Can PerSAM be extended to few-shot segmentation for better performance?**\n\n>\nThanks for your advice! Yes, our approach is not limited to one-shot segmentation, and can accept few-shot references for improved results. As an example, given 3-shot references, we independently calculate 3 location confidence maps for the test image, and adopt a pixel-wise max pooling to obtain the overall location estimation. For PerSAM-F, we regard all 3-shot data as the training set to conduct the scale-aware fine-tuning.\n\nWe respectively conduct experiments for 3-shot segmentation on ***PerSeg*** dataset and 5-shot segmentation on ***FSS-1000*** dataset. The results are shown in the following two tables. By providing more visual semantics in few-shot data, both our training-free PerSAM and the fine-tuned PerSAM-F can be further enhanced.\nMethod|Shot|mIoU|bIoU\n-|:-:|:-:|:-:\nSegGPT|1-shot|94.3|76.5\nSegGPT|3-shot|96.7|78.4\nPerSAM|1-shot|89.3|71.7\nPerSAM|3-shot|90.2|73.6\nPerSAM-F|1-shot|95.3|77.9\nPerSAM-F|3-shot|97.4|79.1\n\nMethod|Shot|mIoU\n-|:-:|:-:\nSegGPT|1-shot|85.6\nSegGPT|5-shot|89.3\nPerSAM|1-shot|81.6\nPerSAM|5-shot|82.3\nPerSAM-F|1-shot|86.3\nPerSAM-F|5-shot|89.8\n\n---\n> **Q3: Is the fine-tuned PerSAM-F generalized only to a specific object?**\n\n>\nOur PerSAM-F can not only be personalized by a specific object, but also generalize to a certain category with the same amount of parameters. As visualized in ***Figure 13 of Appendix***, given a reference cone/armour/crocodile in FSS-1000 dataset, our PerSAM-F can well segment other similar cones/armours/crocodiles in test images. This is because objects of the same category can contain similar hierarchical structures, so the learned scale weights of PerSAM-F by one sample can also be applicable to different objects within the same category. In contrast, for different categories, one needs to fine-tune two sets of scale weights to respectively fit their scale information.\n\n---\n> **Q4: What about using different fine-tuning methods, e.g., LoRA, for multi-object generalization?**\n\n>\nThanks for your advice! As shown in ***Table 3 of the paper***, we have conducted an ablation study using different fine-tuning methods for single-object segmentation on PerSeg. In the table below, we further investigate different fine-tuning methods for multiple objects on DAVIS 2017 dataset. We respectively tune different parameters for different objects within a video. As shown, our proposed scale-aware fine-tuning performs the best with only 2 parameters.\nMethod|Parameter|J&F\n-|:-:|:-:\nPerSAM|0|66.9\nAdapter|196K|50.4\nLoRA|293K|75.8\nPerSAM-F|2|76.1\n\n---\n> **Q5: Can PerSAM help DreamBooth achieve better multi-object customization?**\n\n>\nYes. Similar to single-object personalization, we only calculate the loss within foreground regions for DreamBooth with multi-object training samples. As visualized in ***Figure 20 of Appendix***, we show the improvement for two-object customization assisted by our PerSAM. The backgrounds within images generated by DreamBooth are severely disturbed by those within few-shot training images, while the PerSAM-assisted DreamBooth can accurately synthesize new backgrounds according to the input language prompts."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235532659,
                "cdate": 1700235532659,
                "tmdate": 1700235532659,
                "mdate": 1700235532659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I5Qv0KRCW8",
                "forum": "6Gzkhoc6YS",
                "replyto": "p5ZX946A86",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
                ],
                "content": {
                    "title": {
                        "value": "Thanks a lot for the clarification!"
                    },
                    "comment": {
                        "value": "Thank you for your response, and pardon my late reply. I have read the rebuttal and other reviewers' comments and decided to keep my original score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554167082,
                "cdate": 1700554167082,
                "tmdate": 1700554167082,
                "mdate": 1700554167082,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u064BYcFlG",
            "forum": "6Gzkhoc6YS",
            "replyto": "6Gzkhoc6YS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a training-free Personalisation approach for SAM. For PerSAM, it uses a reference mask for obtaining the positive-negative location prior and then enable SAM for segmentation by: 1) target-guided attention and 2) target-semantic prompting. To reduce the ambiguity of segmentation scales, PerSAM-F is further proposed to fine-tuning SAM. PerSeg dataset is also constructed to evaluate the personalisation segmentation performance. The paper is also evaluated on the one-shot image and video segmentation benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well organised with clear motivation and easy to understand. The illustration and visualisation figures are well presented.\n\n2. PerSAM is training-free and computationally efficient, where the ablation experiment for PerSAM in Table 4, 5 and 6 are extensive.\n\n3. The paper demonstrates good performance not only on the constructed PerSeg benchmark, but also on many image/video segmentation benchmarks."
                },
                "weaknesses": {
                    "value": "1. In the appendix, the author mentioned using dinov2 features. Can the authors also provide the results in Table 2 and 3 by using the default image encoder features of SAM?\n\n2. What is the running speed/ memory consumption of PerSAM comparing to SAM?\n\n3. In Table 2, can the author provide performance comparison to SAM-PT [a]? [a] is a related work in adapting SAM for video object segmentation.\n\n[a] SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. arXiv, 2023.\n\n4. On the video object segmentation benchmark, how dose PerSAM differentiate objects with similar appearances? Can PerSAM also work on self-driving scenario with many similar vehicles/cars in the dense traffic?"
                },
                "questions": {
                    "value": "Can the paper provide more failure cases visualisation for PerSAM? I am generally positive about this paper. If my concerns in the weakness section can be addressed, I will consider to further upgrade my rating."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679476553,
            "cdate": 1698679476553,
            "tmdate": 1699636160017,
            "mdate": 1699636160017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NAiANayWPW",
                "forum": "6Gzkhoc6YS",
                "replyto": "u064BYcFlG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fp1Y (Part #1)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your careful and constructive reviews. We hope our response can address your concerns.\n\n---\n> **Q1: What are the results in Tables 2 and 3 by using the default image encoder features of SAM?**\n\n>\nThanks for your advice! The detailed results of the two image encoders in Tables 2 and 3 are respectively as follows.\nMethod|Encoder|J&F|J|F\n-|:-:|-|-|-\nPainter|-|34.6|28.5|40.8\nSEEM|-|58.9|55.0|62.8\nSegGPT|-|75.6|72.5|78.6\nPerSAM|SAM|62.8|58.6|67.0\nPerSAM|DINOv2|66.9|63.4|70.4\nPerSAM-F|SAM|73.4|70.9|75.9\nPerSAM-F|DINOv2|76.1|73.2|78.9\n\n\n\nMethod|Encoder|FSS-1000|LVIS-92$^i$|PASCAL-Part|PACO-Part\n-|:-:|:-:|:-:|:-:|:-:\nPainter|-|61.7|10.5|30.4|14.1\nSegGPT|-|85.6|18.6|-|-\nPerSAM|SAM|74.9|12.9|31.3|21.2\nPerSAM|DINOv2|81.6|15.6|32.5|22.5\nPerSAM-F|SAM|79.4|16.2|32.0|21.3\nPerSAM-F|DINOv2|86.3|18.4|32.9|22.7\n\nAs DINOv2 is particularly pre-trained by large-scale contrastive data, it produces more discriminative image features than SAM's encoder. This contributes to a more precise positive-negative location prior for better segmentation results, especially on the challenging FSS-1000 dataset. Despite this, with SAM's original encoder, our PerSAM-F and the training-free PerSAM still obtain better segmentation accuracy than Painter or SEEM on different datasets, demonstrating the effectiveness of our approach.\n\n---\n> **Q2: What's the running speed/memory compared to SAM?**\n\n>\n\nWe test the running efficiency on a single NVIDIA A100 GPU with batch size 1. As shown in the table below, our PerSAM and PerSAM-F bring marginal latency and GPU memory consumption over SAM.\nMethod|FPS$\\uparrow$|Memory (MB)$\\downarrow$\n-|:-:|:-:\nSAM|2.16|5731\nPerSAM|2.08|5788\nPerSAM-F|1.98|5832\n\n---\n> **Q3: How's the performance comparison to SAM-PT?**\n\n>\n\nThanks for pointing out! We will add the discussion and comparison in the revised version.\n\n1. ***Method Comparison.*** Although both our PerSAM(-F) and SAM-PT are developed based on SAM, our approach can be generalized to most one-shot segmentation tasks (personalized/video/semantic/part segmentation), while SAM-PT specifically aims at video object segmentation. One key difference between our approach and SAM-PT is how to locate and associate objects from the previous to the current frame, i.e., propagating the location prompt for SAM across frames. In detail, our PerSAM(-F) simply calculates a location confidence map by feature matching, while SAM-PT relies on an external point tracking network, PIPS [1].\n\n2. ***Performance Comparison.*** As shown in the table below, on DAVIS 2017 dataset, SAM-PT performs slightly better than the original PerSAM-F. However, inspired by SAM-PT, we can also incorporate its point tracking strategy (the PIPS tracker) with PerSAM(-F)  to propagate the positive-negative point prompt, which effectively enhances the segmentation performance. This demonstrates the flexible extensibility of our approach for applying more advanced trackers in a plug-and-play way.\nMethod|Propagation|J&F\n-|-|:-:\nSAM-PT|Point Tracking|76.6\nPerSAM|Feature Matching|66.9\nPerSAM|+Point Tracking|68.2\nPerSAM-F|Feature Matching|76.1\nPerSAM-F|+Point Tracking|77.2\n\n#### Reference:\n#### [1] Particle Video Revisited: Tracking through Occlusions using Point Trajectories. ECCV 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235528282,
                "cdate": 1700235528282,
                "tmdate": 1700235528282,
                "mdate": 1700235528282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3wpsE1afcg",
                "forum": "6Gzkhoc6YS",
                "replyto": "u064BYcFlG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fp1Y (Part #2)"
                    },
                    "comment": {
                        "value": "> **Q4: How does PerSAM differentiate objects with similar appearances in video object segmentation?**\n\n>\n\nFor video object segmentation, our approach tries to accurately locate the target object among similar ones by the following three aspects.\n\n1. ***Discriminative features from the encoder.*** Due to large-scale pre-training, the SAM's image encoder, or the more powerful DINOv2, can already produce discriminative enough visual features for different similar objects, which is fundamental to the subsequent calculation of location confidence map.\n\n2. ***Comprehensive location confidence map.*** We calculate a set of confidence maps for all foreground pixels within the target object, such as the head, the body, or the paws of a dog, and then aggregate them to obtain an overall location estimation. This strategy can comprehensively consider the slight differences in any local parts between similar objects.\n\n3. ***Temporal cues between adjacent frames.*** To better leverage the temporal consistency along the video, we prompt SAM's decoder additionally with the object bounding box from the last frame. As different objects have different trajectories, such temporal constraints can better differentiate similar objects by spatial locations.\n\nAs visualized in ***Figure 12 of Appendix***, our method can precisely segment the dancing man in front of a crowd (the 2$^{nd}$ row) and differentiate different fishes within a group (the last row).\n\n---\n> **Q5: Can PerSAM also work on self-driving scenarios with many similar vehicles/cars?**\n\n> \n\nYes. In most cases, our model can segment the designated cars with distinctive appearances in dense traffic. \n\n1. As visualized in ***Figure 15 of Appendix***, for the user-provided target (e.g., a red car, a truck, and a bus), our PerSAM-F can well locate and segment them under severe occlusion or surrounded by similar cars.\n\n2. We conduct an experiment for one-shot segmentation on ***Tokyo Multi-Spectral-4$^i$*** [2] dataset, which contains 16 classes within outdoor self-driving scenarios. As shown in the table below, our PerSAM-F can surpass existing methods that require in-domain training, indicating our generalization capacity in city traffic scenes.\nMethod|In-domain Train|mIoU\n-|:-:|:-:\nPFENet|$\\checkmark$|14.0\nPGNet|$\\checkmark$|17.5\nV-TFSS|$\\checkmark$|26.1\nPerSAM|-|18.4\nPerSAM-F|-|25.6\n\n---\n> **Q6: More failure cases visualization of PerSAM**\n\n>\n\nThanks for your advice! There are mainly two kinds of failure cases of PerSAM(-F).\n\n1. As shown in ***Figures 2 and 8 of the paper***, the training-free PerSAM might fail to segment objects with hierarchical structures, e.g., the hat on top of a teddy bear, or the head of a robot toy. Such scale ambiguity can be effectively alleviated by the fine-tuning of PerSAM-F.\n\n2. After solving the scale ambiguity issue, the three types of failure cases of PerSAM-F are shown in ***Figure 16 of Appendix***: (a) different people with the same clothes, indicating our approach is not very sensitive to fine-grained human faces; (b) the key appearance of the target object is occluded by in test images (the red chest of the bird), indicating that we still need to improve our robustness when there is too large appearance change in test images; (c) discontinuous objects that SAM cannot tackle, for which we can replace SAM with stronger segmentation foundation model for assistance.\n\n#### Reference:\n#### [2] Visible and Thermal Images Fusion Architecture for Few-shot Semantic Segmentation. JVCIR 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235578874,
                "cdate": 1700235578874,
                "tmdate": 1700235578874,
                "mdate": 1700235578874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "946Bwticfd",
                "forum": "6Gzkhoc6YS",
                "replyto": "3wpsE1afcg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal clarification"
                    },
                    "comment": {
                        "value": "Most of my concerns have been well addressed after the rebuttal. I also read the comments by other reviewers. Thus, I keep my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570959675,
                "cdate": 1700570959675,
                "tmdate": 1700570959675,
                "mdate": 1700570959675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5SFIujlmBv",
            "forum": "6Gzkhoc6YS",
            "replyto": "6Gzkhoc6YS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a lightweight method to leverage the Segment Anything Model (SAM), to perform single shot image segmentation. SAM is a powerful image segmentation framework, however it is becomes challenging to use it to perform personalized image segmentation by manually tuning its input prompts. In the approach proposed in the paper, the authors present Personalized approach for SAM (PerSeg) which is training free and can be used to segment a particular object (class) across images given only a single example image along with its binary segmentation mask. To do this, the authors present two approaches: (1) target-guided attention, and (ii) target-semantic prompting. Additionally, to alleviate the problem of objects being present in different scales the paper presents a simple fine-tuning technique that keeps SAM frozen, to combine information from multi-scale masks and improve the final segmentation outputs. \n\nA new dataset PerSeg is also introduced to test the proposed method, and finally the paper also shows how PerSAM can be used to improve DreamBooth [1], for the task of custom text-to-image synthesis. \n\nThe paper quantitatively and qualitatively demonstrates the efficacy of the proposed method across multiple datasets, and achieves competitive performance as compared to existing methods.\n\n[1] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem of single-shot image segmentation is an important problem to solve. This has many downstream utilities in real-world applications ranging from design to healthcare. And the paper introduces a simple but effective technique to solve this by leveraging the powerful Segment Anything Module (SAM) [1]. \n\nThe introduced method is called Personalization approach for SAM (PerSAM), and it takes as input a single example image of the desired object we want to segment, and its corresponding segmentation mask. This is then used to segment out the given object across multiple images automatically. To do this PerSeg involves 2 approachs:\n(1) target-guided attention\n(2) target semantic prompting\n\nAdditionally to handle the object occurring in different scales, the paper introduces a lightweight fine-tuning technique that keeps the SAM model frozen, and aggregates information across multiple scales for finer segmentation.\n\nA new testing dataset is also introduced called PerSeg. \n\nThe work also shows the utility of the proposed method for improving text-to-image synthesis [2].\n\nAnd finally the authors show qualitative and quantitative results for the proposed method, and it performs competitively as compared to existing approaches.\n\nOverall a nicely written paper, with a simple idea and good results.\n\n\n[1] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023\n[2] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022"
                },
                "weaknesses": {
                    "value": "Overall it is a nicely written paper, with good results.\n\nHowever, it is somewhat lacking in it's quantitative evaluation. The choice of evaluation datasets is limited.\n\nIt would be worthwhile to also see the performance of the proposed method for one-shot segmentation on additional (more challenging) datasets like- MS-COCO, AED20K, CityScapes to also compare with more powerful existing state of the art models.\n\nAlso the comparison is lacking. It would be nice to compare against methods that do zero-shot or text guided segmentation like DenseCLIP [1].\n\n[1]. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting, Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, https://arxiv.org/abs/2112.01518"
                },
                "questions": {
                    "value": "How comparable is this work or results against methods that do zero-shot or text guided segmentation like DenseCLIP [1]? And why / why not does it make sense to compare against such methods?\n\n[1]. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting, Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, https://arxiv.org/abs/2112.01518"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2273/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2273/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823909650,
            "cdate": 1698823909650,
            "tmdate": 1699645872460,
            "mdate": 1699645872460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OH1htyqeJS",
                "forum": "6Gzkhoc6YS",
                "replyto": "5SFIujlmBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XA1U"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your detailed and insightful reviews. We hope our response can address your concerns.\n\n---\n> **Q1: Experiments for one-shot segmentation on additional (more challenging) datasets**\n\n>\nThanks for your advice! It is very necessary to evaluate PerSAM in more challenging scenarios. \n\nIn our paper, we have experimented on ***six datasets*** of different tasks, i.e., ***PerSeg*** (personalized object segmentation), ***DAVIS 2017*** (video object segmentation), ***FSS-1000*** and ***LVIS-92$^i$*** (semantic segmentation), ***PASCAL-Part*** and ***PACO-Part*** (part segmentation). On different benchmarks with various domains, our PerSAM(-F) without in-domain training can attain competitive performance to existing specialist models.\n\nAs you suggested, we further present the performance of our approach on two additional datasets for one-shot segmentation: ***COCO-20$^i$*** [1] and ***Tokyo Multi-Spectral-4$^i$*** [2]. (Note that, ADE20K and CityScapes have no corresponding few-shot benchmarks.)\n\n1. ***COCO-20$^i$*** is constructed from MS-COCO by dividing the diverse 80 classes evenly into 4 folds. We directly test our method on the validation set without specific in-domain training. As shown in the below table, our PerSAM(-F) achieves favorable segmentation performance over a wide range of object categories, comparable to previous in-domain methods.\nMethod|In-domain Train|mIoU\n-|:-:|:-:\nFPTrans|$\\checkmark$|47.0\nSCCAN|$\\checkmark$|48.2\nHDMNet|$\\checkmark$|50.0\nPerSAM|-|47.9\nPerSAM-F|-|50.6\n\n2. ***Tokyo Multi-Spectral-4$^i$*** is sampled from Tokyo Multi-Spectral containing 16 classes within outdoor city scenes, similar to CityScapes. Different from existing methods, we only take as input the RGB images without the paired thermal data, and do not conduct in-domain training. As shown in the table below, our approach still exhibits good generalization capacity in street scenarios.\nMethod|In-domain Train|mIoU\n-|:-:|:-:\nPFENet|$\\checkmark$|14.0\nPGNet|$\\checkmark$|17.5\nV-TFSS|$\\checkmark$|26.1\nPerSAM|-|18.4\nPerSAM-F|-|25.6\n\n---\n> **Q2: Comparison with zero-shot or text-guided segmentation methods, like DenseCLIP**\n\n>\n\nThanks for your advice! It would be beneficial to discuss the relationship between PerSAM and existing zero-shot text-guided segmentation models. However, please note that DenseCLIP cannot perform zero-shot text-guided segmentation for comparison with our method. We first explain the reason, and then show the comparison with other zero-shot text-guided methods.\n\n1. ***Why DenseCLIP is not comparable?*** This is because DenseCLIP is a conventional closed-set method, which *cannot conduct zero-shot or text-guided segmentation*. Specifically, DenseCLIP only utilizes CLIP's text embeddings as axillary semantics for image features, and still requires fully supervised training for segmentation on limited \"seen\" classes. Therefore, it cannot be directly utilized for text-guided zero-shot segmentation on \"unseen\" classes, or one-shot segmentation given reference data.\n\n2. ***Comparing with OVSeg [3] and Grounded-SAM [4].*** We select two popular text-guided open-set segmentation methods for comparison with our PerSAM(-F) on three benchmarks: ***PerSeg*** and ***COCO-20$^i$***. ***OVSeg*** leverages MaskFormer to first generate class-agnostic mask proposals, and then adopts a fine-tuned CLIP for zero-shot classification. ***Grounded-SAM*** utilizes a powerful text-guided detector, Grounding DINO [5], to generate object bounding boxes, and then utilize them to prompt SAM for segmentation. Instead of giving a one-shot reference, we directly prompt them by the category name of the target object for text-guided segmentation, e.g., \"cat\", \"dog\", or \"chair\". As shown in the table below, our PerSAM-F consistently achieves competitive results in the two scenarios. This indicates that utilizing PerSAM with a class-agnostic one-shot reference is on par with recognizing the category and then segmenting it with text-guided methods.\nMethod|Prompt|PerSeg|COCO-20$^i$\n-|:-:|:-:|:-:\nOVSeg|Category Name|76.5|37.8\nGrounded-SAM|Category Name|93.2|49.9\nPerSAM|One-shot Data|89.3|47.9\nPerSAM-F|One-shot Data|95.3|50.6\n\n\n#### Reference:\n#### [1] Feature Weighting and Boosting for Few-shot Segmentation. ICCV 2019.\n#### [2] Visible and Thermal Images Fusion Architecture for Few-shot Semantic Segmentation. JVCIR 2021.\n#### [3] Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP. CVPR 2023.\n#### [4] https://github.com/IDEA-Research/Grounded-Segment-Anything.\n#### [5] Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235519369,
                "cdate": 1700235519369,
                "tmdate": 1700235519369,
                "mdate": 1700235519369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GDWCtI2aFA",
                "forum": "6Gzkhoc6YS",
                "replyto": "OH1htyqeJS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors."
                    },
                    "comment": {
                        "value": "Thanks a lot for the detailed response. I have analyzed the review in light of the response, and decide to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2273/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570195426,
                "cdate": 1700570195426,
                "tmdate": 1700570195426,
                "mdate": 1700570195426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]