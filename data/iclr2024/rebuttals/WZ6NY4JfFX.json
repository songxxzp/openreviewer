[
    {
        "title": "Revisiting the Role of Language Priors in Vision-Language Models"
    },
    {
        "review": {
            "id": "rzhDL7fbvb",
            "forum": "WZ6NY4JfFX",
            "replyto": "WZ6NY4JfFX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the generative vision-language models, which have been the focus of many recent works. The authors introduce a novel approach, VisualGPTScore, for employing these generative models in discriminative tasks, particularly image-text alignment and retrieval. Empirical experiments suggest that blind language models occasionally outperform established methodologies. Building on this insight, the authors propose an additional post-processing step during testing to control \"language bias.\" In essence, this paper presents a promising avenue for harnessing generative model confidences effectively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper introduces a probabilistic approach to generative model prediction confidences, exhibiting superior performance in comparison to Image Text Matching (ITM) formulations.\n* Rigorous experimentation and ablation analyses showcase the efficacy of VisualGPTScore.\n* The paper maintains a well-structured and articulate presentation."
                },
                "weaknesses": {
                    "value": "* The paper's results under standard training and testing assumptions ($\\alpha = 0$) are exceptional, but the rationale for deviating from this assumption lacks proper motivation. Measuring $\\alpha^*$ demands test-time privileged information, which raises concerns about the approach's validity.\n\n* Addressing language bias is a crucial aspect, yet the method employed for debiasing, measured by $\\alpha$, appears to operate at the dataset level ($P_{test}$ vs. $P_{train}$) rather than the instance level. Evaluating the total effect, as $P(t|i) - P(t|i=\\phi)$, would provide a more meaningful approach to remove the \u201clanguage bias\u201d.\n\t\n* While the analysis is extensive in the context of I-to-T retrieval tasks, it falls short in terms of assessing a broader range of downstream tasks. Incorporating analyses of zero-shot classification, VQA2.0, and GQA tasks would offer a more comprehensive perspective."
                },
                "questions": {
                    "value": "* If privileged information is employed to measure $\\alpha^*$, how does this impact individual biases such as \"black apple\" vs \"red apple\"? Shouldn't the value of  $\\alpha^*$ vary based on the specific target ($t$)?\n* What is the performance of VisualGPTScore on zero-shot classification? This would contribute to a more comprehensive evaluation of the proposed approach.\n* In the case of \"blind models,\" how is the measurement of $P(t^i_{positive})$ with respect to $P(t^i_{negative})$ conducted for each $i^{th}$ instance? Elaboration on the methodology for evaluating individual test instances in \u201cblind models\u201d is needed.\n* Is it plausible that negative captions rarely occur in web corpora? This factor might be affecting the performance of \"blind models\" and deserves further investigation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission721/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET",
                        "ICLR.cc/2024/Conference/Submission721/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682934353,
            "cdate": 1698682934353,
            "tmdate": 1700627907989,
            "mdate": 1700627907989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqzCN2WSaK",
                "forum": "WZ6NY4JfFX",
                "replyto": "rzhDL7fbvb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer AoET's feedback. We begin by clearing up some potential misunderstandings:\n\n> **Question 3: In the case of \"blind models\", how is the measurement of $P(t^i_{positive})$ with respect to $P(t^i_{negative})$ conducted for each instance? Elaboration on the methodology for evaluating individual test instances in \"blind models\" is needed.**\n\n$P_{train}(text)$ is measured for each individual text independently using Monte Carlo estimation (Equation 9): $P_{train}(text) \\approx \\frac{1}{n} \\sum_{k=1}^n P_{train}(text | image_k)$, where $k$ represents the number of random Gaussian noise images sampled.\n\n> **Weaknesses 1: Measuring $\\alpha^{*}$ demands test-time privileged information, which raises concerns about the approach's validity.**\n\nOur method **does not demand test-time privileged information**. In practice, one only needs a small validation set from the target task to search for optimal alpha, or can simply make practical assumptions without tuning alpha, e.g., choosing $\\alpha=1$. We show in Table 2-a that both consistently improve performance without using test-time privileged information.\n\n> **Weaknesses 2 + Question 1: Addressing language bias is a crucial aspect, yet the method employed for debiasing, measured by a, appears to operate at the dataset level ($P_{train}$ vs. $P_{test}$) rather than the instance level. Evaluating the total effect, as P(t|i) - P(t|i =$\\phi$), would provide a more meaningful approach to remove the \"language bias\". Shouldn't the value of $a^{*}$ vary based on the specific target (t)?**\n\nOur $\\alpha-$debiasing method ($\\frac{P_{train}(text | image)}{P_{train}(text)^\\alpha}$) is motivated by the widely existing **train-test shift** from VLM\u2019s training set (web corpora) to testing benchmarks (such as SugarCrepe), as thoroughly discussed in Section 3. Without additional training, it is unclear how to measure an $\\alpha$ that varies based on specific target texts. As our paper focuses on training-free debiasing, we leave learning instance-specific $\\alpha$ to future work.\n\n> **Weaknesses 3 + Question 2: While the analysis is extensive in the context of I-to-T retrieval tasks, it falls short in terms of assessing a broader range of downstream tasks. Incorporating analyses of zero-shot classification, VQA2.0, and GQA tasks would offer a more comprehensive perspective.**\n\nOur score and debiasing method are designed to calculate the similarity score between an image and a caption. Thus, it does not naturally support VQA tasks. \n\nWhile BLIP was neither designed nor benchmarked on zero-shot classification, we follow your suggestion to evaluate our methods on ImageNet1K and show that our $\\alpha$-debiasing solution (using a fixed $\\alpha=1$) can nearly double the performance from 18.6% to 36.2%. We also try to search for alpha by sampling an additional one-shot validation set (and repeat using 3 random seeds). This extremely low-shot validation set can lead us to a near-optimal alpha, achieving 40.0% without retraining or finetuning the model:\n\n\n| Method                                  | Result      |\n|-----------------------------------------|-------------|\n| ITCScore                                | 31.7       |\n| ITMScore                                | 37.4       |\n| VisualGPTScore ($\\alpha=0$)             | 18.6       |\n| VisualGPTScore ($\\alpha=1$)             | 36.2       |\n| VisualGPTScore ($\\alpha^*_{val}=0.65\\pm0.01$) | 40.0$\\pm$0.1  |\n| VisualGPTScore ($\\alpha^*_{test}=0.69$)  | 40.2       |\n\nIn this experiment, by sampling only a single Gaussian noise image for the calculation of $P_{train}(t)$, we incur a negligible inference cost, equivalent to the testing of one additional image.\n\n> **Question 4: Is it plausible that negative captions rarely occur in web corpora? This factor might be affecting the performance of \"blind models\" and deserves further investigation.**\n\nYou are right that negative captions in benchmarks such as ARO rarely occur in web corpora. This explains why even \u201cblind models\u201d are able to outperform SOTA methods on many recent popular benchmarks. As said by Reviewer ANt2, our experiments suggest that *\u201cexisting benchmarks are in some sense, still not \u201chard enough\u201d and contain correlations that can be exploited or can be solved by using language priors\u201d*. We hope our experiments are useful for everyone designing new benchmarks for vision-language tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699921957294,
                "cdate": 1699921957294,
                "tmdate": 1699921957294,
                "mdate": 1699921957294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B5ReoKdkWp",
                "forum": "WZ6NY4JfFX",
                "replyto": "LqzCN2WSaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. This clarifies several doubts and misunderstandings. Also, appreciate the additional experiments on the zero-shot classification task. \n\nOverall I like the insights from this paper. If we fix the value of $\\alpha^*$, then still results are still consistent and better.\nBut my understanding of the train-test bias is still different. \n\nFor weakness 1, my understanding is that we need to measure the $\\alpha$ only when the test domain is different than i.i.d. So, what kind of distribution shifts are we observing across these datasets? Some examples of such differences might give the proper motivation. \nIf I consider the Fig.1 as a motivation example then trying to solve the train-test distribution shift needs to be further justified. Scenario 2 focuses on the issue of \"language prior\" and calculating $\\alpha^*$ does not justify whether this bias is removed or not.\n\nIs it possible to provide some before/after probability distribution examples -- success & failure cases? \n\n\nNote: Increased the score to 5. I will decide the final score based on the needed clarification on the above question."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629129817,
                "cdate": 1700629129817,
                "tmdate": 1700629129817,
                "mdate": 1700629129817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dOuQGfS6Od",
            "forum": "WZ6NY4JfFX",
            "replyto": "WZ6NY4JfFX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission721/Reviewer_ANt2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission721/Reviewer_ANt2"
            ],
            "content": {
                "summary": {
                    "value": "This is a scientific work that empirically analyzes language bias in image-text retrieval tasks and generative vision-language models (not image generation models, but text generation models). They first characterize the ability of generative vision-language models to match images to text in a zero-shot manner by measuring the probability that a textual sequence may be generated from an image. They then turn to a benchmark-centric view, empirically showing that several benchmarks can be solved even by blind LLMs in this manner, simply by their ability to flag linguistically unlikely captions from language priors. They show that with postprocessing, generative approaches can outperform handcrafted discriminative approaches on image-text retrieval tasks, even highly compositional ones."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "It is an open question to what degree vision-language models are doing the task of image-text retrieval as opposed to exploiting spurious correlations. This is dependent on the degree to which benchmarks themselves can be beaten by exploiting correlations. The major strength of this paper is that they (at least partially) answer this question. The experiments are convincing and cover a broad range of vision-language models. They show that even blind LLMs and VLMs do surprisingly well on these benchmarks, suggesting that existing benchmarks are in some sense, still not \"hard enough\" and contain correlations that can be exploited or can be solved by using language priors. \n\nThe proposed method for debiasing vision-language models works well, given the simplicity. I consider the simplicity and generality a strength. \n\nThe scientific conclusions of this paper are novel and useful for everyone designing new benchmarks for vision-language tasks."
                },
                "weaknesses": {
                    "value": "A minor weakness of the paper is that they do not compare with the recent crop of truly \"large\" generative vision-language models like BLIP-2, LLAVA, etc. However, this is a minor weakness and I do not think it needs to be really addressed in this work, since these models are still new enough that training them is extra engineering work. Also, if anything, the language prior should be worse in LLM-based VLMs."
                },
                "questions": {
                    "value": "I have no questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692215474,
            "cdate": 1698692215474,
            "tmdate": 1699635999321,
            "mdate": 1699635999321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fxDFgY5WTF",
                "forum": "WZ6NY4JfFX",
                "replyto": "dOuQGfS6Od",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer ANt2's positive and insightful review. Your understanding and articulation of our work's core contributions are highly encouraging. We agree that exploring the extent to which benchmarks can be solved by exploiting language priors is a critical question, and we are glad that our work contributes meaningfully to this discussion. We are also grateful for your recognition of the simplicity and generality of our debiasing approach. \n\nWe address your (minor) concern below:\n> **A minor weakness of the paper is that they do not compare with the recent crop of truly \"large\" generative vision-language models like BLIP-2, LLAVA, etc. However, this is a minor weakness and I do not think it needs to be really addressed in this work, since these models are still new enough that training them is extra engineering work. Also, if anything, the language prior should be worse in LLM-based VLMs.**\n\nWe have included the results of the state-of-the-art captioning model BLIP-2 (both stage-1-QFormer and stage-2-FlanT5) in Appendix D. For example, Table 9 shows that our debiasing solution, even with a fixed alpha = 1, consistently improves performance on balanced benchmarks across all model variants. For your convenience, we attach the image-to-text retrieval results (text score) for Winoground and EqBen below: \n\n| Winoground                          | $\\alpha=0$ | $\\alpha=1$ | $\\alpha=\\alpha^*$ |\n| ------------------------------ | ---------- | ---------- | ----------------- |\n| BLIP-1                         | 27.0       | 33.0       | 36.5              |\n| BLIP-2 (stage-1-QFormer)       | 24.3       | 29.3       | 33.0              |\n| BLIP-2 (stage-2-FlanT5)        | 25.3       | 31.5       | 34.3              |\n\n| EqBen                          | $\\alpha=0$ | $\\alpha=1$ | $\\alpha=\\alpha^*$ |\n| ------------------------------ | ---------- | ---------- | ----------------- |\n| BLIP-1                         | 9.6       | 19.8       | 19.8              |\n| BLIP-2 (stage-1-QFormer)       | 12.2       | 21.9       | 22.2              |\n| BLIP-2 (stage-2-FlanT5)        | 8.5       | 22.0       | 22.0              |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699921532856,
                "cdate": 1699921532856,
                "tmdate": 1699921532856,
                "mdate": 1699921532856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p5k7CtlbQe",
            "forum": "WZ6NY4JfFX",
            "replyto": "WZ6NY4JfFX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission721/Reviewer_85bY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission721/Reviewer_85bY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simple method to use the class of generative VLM for image to text similarity tasks. Although the proposed method is quite general, the paper evaluates it on the BLIP model and, as previously observed in the literature, shows that some Vision-Language benchmarks can be solved by only looking at the text modality. The authors also propose to reduce the reliance of VLMs on the language prior by using a probabilist post-processing calibration technique aimed at controlling the amount of linguistic bias of generative VLMs which is shown to improve image-to-text search results on the proposed benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea of calibrating a pre-trained generative VLM models to adapt to the \u201cformat\u201d of the test dataset is interesting and impactful."
                },
                "weaknesses": {
                    "value": "1. **(Lack of novelty)**: Measuring similarity using the average log likelihood of strings given a fixed context (an image+prompt in this case) is not new. For example in the language community it has been previously used multiple times to provide alternative similarity measures to using encoder models (based on dot product similarity). Furthermore, such idea has already been extended to VLMs in [1], where it has been shown that \u201cmodels trained on captioning can perform on-par with models trained with the usual contrastive image-text matching loss.\u201d What is particularly novel about \"VisualGPTScore\"?\n2. **(Soundeness)** The paper briefly points out a connection between the proposed calibration approach and Mutual Information based approaches. How does this connection help the reader? What is the intuition that motivates using point-wise mutual information to improve the calibration of the deployed generative VLMs? Can the authors comment more on this? As of now, this seems more an afterthought rather than a clear and sound motivation.\n\n\n**Minor:**\n\nWhile the proposed VisualGPTScore is more efficient to be computed than next-token generation it is fair to point out that it is much slower than computing similarity scores based on dot products (e.g. CLIP) especially when the size of the retrieval index grows. See for example, [1, 2] and their techniques to limit the computational cost of performing image-to-text search over large databases. Can the authors comment more on this in the manuscript?\n\n[1] Antonie Miech et al. \u201cThinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers\u201d\n\n[2] J. Li et al., \u201cAlign before fuse: Vision and language representation learning with momentum distillation\u201d"
                },
                "questions": {
                    "value": "1. The proposed trick to estimate the marginal over text p(t) is not sound. Why should averaging Gaussian noise fed as input to the VLM work more efficiently than averaging over the distribution of natural images? Is there any theoretical guarantee that this is the correct thing to do? Especially given the fact that Gaussian noise has never been used during training of the VLM and is therefore out of distribution for the model.\n    - I suggest the authors to perform the experiments using a more recent VLM like (LLaVA or BLIPv2, also BLIP is not SoTA) which both can be directly used to compute the language marginals.\n2. If VLMs are deployed as Zero Shot models why do we care about the gap between test and training (p_te vs p_tr) since the model do not use p_tr? \n3. I am not fully convinced by the empirical evaluation. Isn\u2019t it obvious that the proposed method with the optimal alpha performs better than any other method since it has been optimized (through Cross Validation) to find the best possible alpha for each dataset independently? I\u2019d expect a comparison with other baseline methods that calibrate the model\u2019s predictions before inference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728699414,
            "cdate": 1698728699414,
            "tmdate": 1699635999248,
            "mdate": 1699635999248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xQFhdYfeQ3",
                "forum": "WZ6NY4JfFX",
                "replyto": "p5k7CtlbQe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "After reading through the comments, we believe Reviewer 85bY\u2019s concerns may stem from some misunderstandings about our work. We first clarify:\n\n> **Question 2: If VLMs are deployed as zero-shot models, why do we care about the gap between test and training (p_te vs p_tr) since the model does not use p_tr?**\n\nAs we focus on zero-shot applications of generative VLMs, $P_{train}$ in our paper refers to **VLMs\u2019 pretraining datasets**, such as LAION for BLIPv1 and BLIPv2. In fact, most of the recent popular benchmarks such as ARO and SugarCrepe are designed to evaluate VLMs in a zero-shot fashion, and thus do not even provide a training split.\n\n> **Question 1.1: I suggest the authors perform the experiments using a more recent VLM like (LLaVA or BLIPv2, also BLIP is not SoTA) which both can be directly used to compute the language marginals.**\n\nThe language marginal ($P_{train}(t)$) refers to **VLMs\u2019 pretraining captions**. Therefore, without sampling any images, one simply cannot approximate this marginal from the language decoder of LLaVA or BLIPv2, whose language models are pretrained on text corpora different from VLMs\u2019 pretraining captions.\n\nAlso, our paper already includes SOTA captioning-model BLIPv2 results (both stage-1-Q-Former and stage-2-FlanT5-XL). For instance, in Table 9, we show that our $\\alpha$-debiasing solution consistently improves all BLIP model variants even with a fixed $\\alpha$=1.\n\n> **(Lack of novelty) Because this generative score has been applied before, what is particularly novel about our work (e.g., compared to paper [1])?**\n\nWe agree that the generative score has been applied in prior art such as [1]. In fact, our goal is **not** to introduce a new method. Instead, we aim to **revisit** the language priors and biases in generative VLMs (as noted by Reviewer ANt2). This issue has been **neglected** in recent popular image-text retrieval benchmarks until now. \n\nWe also carefully review the paper [1] and note two crucial differences:\n\n\n- While [1] focuses on training this score from scratch, we focus more on its zero-shot application, especially on how to improve its performance under a substantial distribution (marginal) shift between VLMs\u2019 pretraining data and downstream tasks.\n- [1] only applies this generative score to text-to-image retrieval tasks (as we also show in Table 3-b). However, it chooses **not** to report image-to-text retrieval performance, although it uses the same COCO and Flickr30K benchmarks. We think this omission is likely due to the issue of language biases, which our paper thoroughly addresses. Concretely, we also provide algorithms for debiasing the generative score from [1], which dramatically improves performance on challenging benchmarks (such as COCO and Flickr30K).\n\n> **(Soundness) Is our calibration approach motivated by pointwise mutual information (PMI)? How does the connection between our approach and PMI help the reader?**\n\nTo clarify, our approach (dividing by the marginal $P_{train}(t)^\\alpha$) is motivated by our analysis in Section 3 (not by PMI). Here is a concrete example (cf. Figure 1) for ease of understanding: in some recent compositionality benchmarks such as SugarCrepe, a negative caption such as \u201c*people are cooking in a kitchen*\u201d is more common in the VLM\u2019s trainset (thus a higher $P_{train}(t)$) than the positive caption \u201c*people are posing in a kitchen*\u201d. As such, dividing by $P_{train}(t)$ forces the model to select the caption that matches better to the image (rather than the caption with a larger marginal prior).\n\nOur Appendix A shows that our approach $\\frac{P(t|i)}{P(t)^\\alpha}$ happens to be the same as smoothed estimates of pointwise mutual information (PMI$^k$), which also reduces the effect of marginal priors from training data. We include this connection because some readers may find it interesting.\n\n> **(Minor: Efficiency) VisualGPTScore is slower than CLIPScore especially when the size of the retrieval index grows. [1,2] show some methods for reducing this cost. Can the authors comment more on this in the manuscript?**\n\nWhile efficiency is not the focus of our work, we are happy to cite [1,2] that suggest re-ranking or distillation to reduce the inference cost. This could be a promising future direction. \n\n> **Question 1: The proposed trick to estimate the marginal over text p(t) is not sound. Why should averaging Gaussian noise fed as input to the VLM work more efficiently than averaging over the distribution of natural images? Is there any theoretical guarantee that this is the correct thing to do? Especially given the fact that Gaussian noise has never been used during training of the VLM and is therefore out of distribution for the model.**\n\nOur empirical trick is motivated by [3] that approximates P(answer) using a null prompt \u201cN/A\u201d. Yet, the language models used in [3] also are never trained on P(answer | \u201cN/A\u201d). Studying its theoretical guarantee could be an interesting future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699921396571,
                "cdate": 1699921396571,
                "tmdate": 1699921396571,
                "mdate": 1699921396571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "azxSEdvutt",
            "forum": "WZ6NY4JfFX",
            "replyto": "WZ6NY4JfFX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the zero-shot performance of generative VLMs in image-text retrieval tasks. A novel metric, VisualGPTScore, is introduced to compute the match score by generating a specific text string based on an image. Notably, the authors identify the train-test distribution shift and present a probabilistic post-processing method. This approach enables the regulation of linguistic bias in generative VLMs during testing without necessitating model retraining or fine-tuning. The proposed method sets new state-of-the-art results on several image-to-text retrieval benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Applying generative VLMs to image-to-text retrieval tasks is an intriguing endeavor.\n2. Comprehensive experiments have been conducted, achieving state-of-the-art results.\n3. The problem's formulation as a train-test distribution shift, followed by a probabilistic derivation, leading to the adjustable parameter alpha, is both logical and intriguing."
                },
                "weaknesses": {
                    "value": "1. My primary concern is the application's real-world viability. Image-text retrieval, often utilized in search engines, demands high time efficiency. With this method, for every new image, the VLM must process all texts, resulting in substantial computational costs. In contrast, traditional methods like CLIP pre-compute text embeddings and only require a dot product with each image embedding. Hence, while the experimental results are impressive, I question this method's practical value.\n2. As demonstrated in Table 7 in the appendix, as the dataset size increases, the OTS scores progressively deteriorate, and the gap with ITM widens even when using the optimal alpha. How can this be explained? Might this indicate an inherent limitation of the method?"
                },
                "questions": {
                    "value": "1. Following the first weakness, could you provide a time-efficiency assessment comparing various methods?\n2. The paper mentions, \"To apply this to our generative VLMs, we choose to sample 'null' inputs as Gaussian noise images.\" Why are Gaussian noise images suitable as \"null\" inputs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission721/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission721/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811199144,
            "cdate": 1698811199144,
            "tmdate": 1699635999134,
            "mdate": 1699635999134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E6PCFYGCKH",
                "forum": "WZ6NY4JfFX",
                "replyto": "azxSEdvutt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer eKtF\u2019s feedback. We will address your concerns below:\n\n> **My primary concern is the application's real-world viability. Image-text retrieval, often utilized in search engines, demands high time efficiency. With this method, for every new image, the VLM must process all texts, resulting in substantial computational costs. In contrast, traditional methods like CLIP pre-compute text embeddings and only require a dot product with each image embedding. Hence, while the experimental results are impressive, I question this method's practical value.**\n\nWhile our proposed score is slow for *large-scale* retrieval, it can still be practical for other applications like evaluating text-to-image generation, e.g., measuring the similarity score between a text prompt and a generated image. As an illustration, our method can complement the widely used CLIPScore [1,2], which struggles with understanding complicated texts that involve compositions of objects, attributes, and their relations. In fact, recent image-text retrieval benchmarks, such as ARO and SugarCrepe, are designed specifically to evaluate VLMs\u2019 compositional reasoning capabilities.\n\nSince our primary focus is on studying the issues of linguistic priors in both generative VLMs and vision-language benchmarks, we leave it to future work to adapt our method for more efficient large-scale retrieval tasks. For instance, one could use established re-ranking technique or distill the VisualGPTScore into CLIPScore to improve inference speed [3].\n\n> **Following the first weakness, could you provide a time-efficiency assessment comparing various methods?**\n\nWe provide a time-efficiency comparison between ITCScore (CLIPScore), ITMScore, and VisualGPTScore, using the same BLIP model for large-scale inference (1000 images x 5000 texts):\n\n| Metric          | Inference Cost     | Time on Flickr30K |\n|-----------------|--------------------|----------------------------------------------------|\n| ITCScore        | O(\\|image\\| + \\|text\\|) | 2 minutes                                         |\n| ITMScore        | O(\\|image\\| x \\|text\\|) | 3 hours                                           |\n| VisualGPTScore  | O(\\|image\\| x \\|text\\|) | 3 hours                                           |\n\n\n> **As demonstrated in Table 7 in the appendix, as the dataset size increases, the OTS scores progressively deteriorate, and the gap with ITM widens even when using the optimal alpha. How can this be explained? Might this indicate an inherent limitation of the method?**\n\nGood question! We include Table 7 (retrieval performance on training set) to motivate a scientific discussion on the inherent linguistic priors of VisualGPTScore, which make it a biased estimator of $P_{train}(image | text)$. For a thorough discussion, please refer to Appendix C, titled *\u201cIs VisualGPTScore a biased estimator?\u201d*. Intuitively, such estimators can suffer from well-known \u201clong-tailed\u201d issues that arise when learning from imbalanced datasets (i.e., in our case, certain sentences are more common than others). We hope that our initial discussion on this issue can serve as a useful reference point for future work.\n\n> **The paper mentions, \"To apply this to our generative VLMs, we choose to sample 'null' inputs as Gaussian noise images.\" Why are Gaussian noise images suitable as \"null\" inputs?**\n\nOur empirical findings echoes [4], which uses a language model to approximate P(text) = P(text | \u201cN/A\u201d). Their null text prompt \u201cN/A\u201d is also not seen during training, but can empirically help calibrate language models. Also, we try both gaussian noise images and training set images, and find that they show similar performance (Table 4).\n\n\n**References**:\n\n[1] Ruiz et al. \u201cDreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\u201d. \n\n[2] Brooks et al. \u201cInstructPix2Pix: Learning to Follow Image Editing Instructions\u201d.\n\n[3] Antonie Miech et al. \u201cThinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers\u201d. \n\n[4] Zhao et al. \u201cCalibrate Before Use: Improving Few-Shot Performance of Language Models\u201d"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699921125609,
                "cdate": 1699921125609,
                "tmdate": 1699921125609,
                "mdate": 1699921125609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JRpbFFcuwl",
                "forum": "WZ6NY4JfFX",
                "replyto": "E6PCFYGCKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! \n\n1. As for the \"null\" inputs, you mentioned that \"Their null text prompt \u201cN/A\u201d is also not seen during training\". However, in natural language corpora, N/A is likely to appear, and I didn't find any relevant expressions in [4] (please correct me if I overlooked it). Additionally, Table 3 in [4] demonstrates that the choice of content-free input does affect accuracy. Therefore, conducting more in-depth analyses and experiments on this is highly recommended. \n\n2. You mentioned that this method is practical for other applications, such as complementing the widely used CLIPScore\". I suggest conducting experiments on more tasks to prove that the method has performance improvements, as only text-image retrieval tasks are evaluated in the paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission721/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480421283,
                "cdate": 1700480421283,
                "tmdate": 1700480421283,
                "mdate": 1700480421283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]