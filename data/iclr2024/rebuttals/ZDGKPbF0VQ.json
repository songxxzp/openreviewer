[
    {
        "title": "Improving Language Models with Advantage-based Offline Policy Gradients"
    },
    {
        "review": {
            "id": "S4SuC0Y8KJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
            ],
            "forum": "ZDGKPbF0VQ",
            "replyto": "ZDGKPbF0VQ",
            "content": {
                "summary": {
                    "value": "The widely used RLHF algorithmic backbone, PPO, can incur additional computational overhead mode collapse due to its online learning nature. Viewing this problem, this paper proposes an offline policy gradient method, Advantage-Leftover Lunch RL (A-LOL), which optimizes LMs towards desired rewards using only static data. Specifically, A-LOL considers the entire output sequence as a single action, and calculates training data advantage before filtering unfavorable instances.\nThe proposed A-LOL is easy to implement over standard cross entropy loss by adding sequence-level reward-weighting and importance-sampling weights.\nIn experiments, the proposed method shows competitive results and data efficiency on our different language generation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is clear and easy to implement, with relatively few assumptions. Therefore, it may have practical merits.\n2. The experiments are relatively throughout and the results are promising.\n3. Human study helps demonstrating the efficacy of the proposed method. \n4. The paper is generally easy to follow."
                },
                "weaknesses": {
                    "value": "1. Unclear method contribution: the proposed method is nearly, if not exactly, a special case of TRPO/PPO method in the bandit setting. Such a special bandit instantiation has been widely considered in classical RLHF works, such as [1,2].\n\n2. Advantage-weighted policy optimization is also a well-studied method in offline RL. For example, Eq. 5 in this paper is very similar to Eq. 4 in AWAC [3], except for the importance weighting that basically comes from TRPO/PPO.\n\n3. The formulation of considering the entire output sequence as a single action step may suffer from exponentially large action space, which may make policy training harder and less stable. See for example [4, 5]. As an aside, recent works have already tried to learn a per-token reward function that incorporates arbitrary human-designed scoring function(s), which may better cope with the large action space in NLG problem, see, e.g., [6].\n\n4. Weighted behavior cloning has been quite extensively used in prior NLP papers, e.g., [6,7,8,9,10,11]. It will make the algorithmic contribution of this paper more clear if the authors can have a paragraph discussing and comparing with such related works, instead of only citing the CRR paper from offline RL.\n\n5. In Table 2, the comparison with PPO may not be fair, because the reward for PPO is a good-or-bad classifier. The offline RL methods, on the other hand, are fitted towards the original responses that themselves show high linguistic diversity, which would implicitly guide the offline RL methods, especially A-LoL, towards generating longer and more diverse sequences. In short, there is no guiding signal for PPO to generate such sequences, while the offline RL methods implicitly have the guidance.\n\n6. There are several well-established exogenous components in the proposed method, such as (1) importance clipping, (2) discarding negative advantage datapoints, (3) prioritized sampling. It is unclear how each of those exogenous components contribute to the overall performance. It is also unclear if the baselines can also benefit from such exogenous components, e.g., (2) and (3). This again muds the algorithmic contribution of the proposed method and make the experiment results less convincing.\n\n[1] Stiennon, Nisan, et al. \"Learning to summarize with human feedback.\" Advances in Neural Information Processing Systems 33 (2020): 3008-3021.\n\n[2] Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744.\n\n[3] Peng, Xue Bin, et al. \"Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.\" arXiv preprint arXiv:1910.00177 (2019).\n\n[4] Guo, Han, et al. \"Text Generation with Efficient (Soft) $ Q $-Learning.\" (2021).\n\n[5] Snell, Charlie, et al. \"Offline rl for natural language generation with implicit language q learning.\" arXiv preprint arXiv:2206.11871 (2022).\n\n[6] Yang, Shentao, et al. \"Preference-grounded Token-level Guidance for Language Model Fine-tuning.\" arXiv preprint arXiv:2306.00398 (2023).\n\n[7] Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, and Caiming Xiong. 2022. [CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 92\u2013102, Dublin, Ireland. Association for Computational Linguistics.\n\n[8] Feng, Y., Yang, S., Zhang, S., Zhang, J., Xiong, C., Zhou, M., & Wang, H. (2023). Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems. arXiv preprint arXiv:2302.10342.\n\n[9] Norouzi, Mohammad, et al. \"Reward augmented maximum likelihood for neural structured prediction.\" Advances In Neural Information Processing Systems 29 (2016).\n\n[10] Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, and Shashank Srivastava. How helpful is inverse reinforcement learning for table-to-text generation? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 71\u201379, 2021.\n\n[11] Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, and Kenneth Heafield. Approaching neural grammatical error correction as a low-resource machine translation task. arXiv preprint arXiv:1804.05940, 2018."
                },
                "questions": {
                    "value": "1. Is the proposed method an offline RL method or (online) off-policy RL method? I am a bit confused by some citations, e.g., \"Degris et al., 2012\" in the second paragraph of Section 1, which is titled as \"Off-Policy Actor-Critic\". \n\n2. How does the learned $\\pi_\\theta$ significantly improve over  $\\pi_{ref}$? With the clipping technique in PPO, the learned  $\\pi_\\theta$ will be constrained within a \"$\\epsilon$-neighbourhood\" around $\\pi_{ref}$, which limits the room for possible improvement.\nNote that in PPO $\\pi_{ref}$ is constantly changing and hence allows the continuous improvement of $\\pi_\\theta$, but in this paper $\\pi_{ref}$ is never updated during training (Appendix A). \n\n3. From Line 2 in Algo. 1, it's unclear how do the authors train the value function $V(x)$. Do the authors sample multiple $y'$ from $\\pi_{ref}$ for *each* $x$? If yes, with this multi-sequence sampling, how would the proposed method save compute compared to standard PPO-style LM training? If no, then Line 2 in Algo. 1 will only regress to the reward $R$ of $y'$, which is a crude and high variance estimate of the state value $V(x)$.\n\n4. Will \"discarding the data points with negative advantages\" worsen data scarcity and harm the quality of the LM generations? For example, even though those data points may be less advantageous with regard to the given reward, they may still be helpful for generating fluent text sequences.\n\n5. Maybe Line 4 in Algo. 1 is \"... not converge\", instead of \"... converges\"?\n\n6. How would you justify the approximate importance weight in A-LOL sequence? How is it different from the standard per-step importance sampling in RL? Given that it is the best performing method, it would be important if one can justify it.\n\n7. Could you explain the term $\\frac{\\ln \\pi_\\theta}{\\ln \\pi_{ref}}$ in Eq. 6? Would it be better and easier to optimize if we use the log importance weight $\\ln \\frac{ \\pi_\\theta}{ \\pi_{ref}} = \\ln \\pi_\\theta - \\ln \\pi_{ref}$?\n\n8. Will the offline RL methods converge if you only allocate one epoch of training steps?\n\n9. In Fig. 2, is NLL trained on the same number of steps and plotted on the same step counts as other methods that are trained on top of the NLL-based reference policy? \n\n10. How is the training time of value function compared to the training time of policy $\\pi_\\theta$? Is it fair compared to NLL?\n\n11. Why are preferences inherently unidimensional and cannot be extended to tasks where more than one aspect is important? Why couldn't humans make judgement based on multiple axes?\n\n12. Would the success of A-LoL sequence contradict the single-action assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697414407719,
            "cdate": 1697414407719,
            "tmdate": 1700621188145,
            "mdate": 1700621188145,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fbXA8LyHca",
                "forum": "ZDGKPbF0VQ",
                "replyto": "S4SuC0Y8KJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed feedback. To clarify, our primary goal is to effectively train language models by exclusively using existing language data and sequence-level language classifiers. Thus, we compare *offline* RL [22] methods that exclusively learn from a static dataset, to reduce training costs and improve efficiency.\n\n__Response to the weaknesses:__\n1. *\"a special case of TRPO/PPO in the bandit setting\"*  \nWe believe this comparison is an oversimplification. PPO/TRPO are online RL that assume the training trajectories are generated by constant interaction with the environment. Our method is offline and assumes access to only static data. Accordingly, value estimate, advantage, and the training process differs significantly from the online methods.  \nThank you for references to PPO in single action bandit setting; we will update Table 1 and the intro. Most modern implementations of PPO for LMs [12,13,14] use per-token action assumption with trajectory level sparse reward (only the last token gets reward). This makes the value estimation of intermediate tokens difficult. Instead of on-policy updates, we use a sequence-level value estimate of reference policy that we cheaply train from its validation performance. To our knowledge, such an easy-to-compute sequence-level value estimate has not been used before for offline RL training of LMs. We also show a combination of algorithmic modifications to improve training efficiency and performance (point 6).\n2. *\"Eq. 5 is very similar to Eq. 4 in AWAC [3],\"*  \nWhile it is true that Advantage-weighted Regression [3] is a well-studied method in offline RL, there are other differences from A-LoL. AWR maintains two identical models for value estimate and policy and it trains both models on samples from D. Here, D can collect additional off-policy trajectories. In contrast, A-LoL estimates sequence-level value on top of a frozen LM\u2019s validation performance only once before target policy training. This allows A-LoL to discover a subset of positive advantages within the static training data, since negative advantage instances are detrimental to performance (point 6). \n3. *\"suffer from exponentially large action space\"*  \nLarge action space is not an issue for A-LoL due to its use of offline data (that can be human-written). In contrast, both online on-policy and online off-policy methods use LM-generated data with high variance due to the large action space. While there exist a few studies with per-token [6] and phrase-level [15] reward functions, most prominent RLHF implementations use a sequence-level reward [1,2,12,13,14] and thus, we compare with the same setting.\n4. *\u201cWeighted behavior cloning has been used in prior NLP papers\u201d*  \nThank you for sharing the related works! We use weighted behavior cloning as a reward-based offline RL baseline and will add a paragraph discussing these additional related works.\n5. *\u201creward for PPO is a good-or-bad classifier\u201d*  \nCommon RLHF benchmarks and PPO implementations [12, 13, 14] use sequence-level preference classifiers as reward models. For a fair comparison, we keep the same initial reference LM and reuse data, and reward models [21] from PRO\u2019s [20] experimental setup (a preference-based offline RL baseline). In the PPO baseline, we use nucleus sampling (top-p=0.95) and also include entropy in the loss term to encourage diversity.  \nAll offline RL baselines use the same data, but vary in diversity. In particular, DPO [18] loses diversity, whereas all reward-based baselines and A-LoL methods show better diversity. \n6. *\"exogenous components, such as (1) importance clipping, (2) discarding negative advantage datapoints, (3) prioritized sampling.\u201d*  \nThank you for pointing this out. We compare the impact (1) and (3) in the ablations in Appendix B.3 (Figure 6). We find (1) importance clipping is crucial for A-LoL; it degenerates to NaN losses without it. We find (3) priority sampling is notably better than random sampling. We added a new ablation for (2) and found that negative advantage is detrimental. \nFor a fair comparison with reward-based baselines, we employed reward-based priority sampling (page 4), but do not discard the negative advantage data as they do not have an internal value estimate like A-LoL."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533458214,
                "cdate": 1700533458214,
                "tmdate": 1700533458214,
                "mdate": 1700533458214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LelS6rFN7U",
                "forum": "ZDGKPbF0VQ",
                "replyto": "v0myJ7RxP2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for the detailed response. \n\nAfter reading other reviewers' comments and your respective rebuttal. I have some additional questions and comments.\n\n\n1. > ... such an easy-to-compute sequence-level value estimate has not been used before for offline RL training of LMs\n\nIf I understand correctly, sequence-level value estimate has been used in RL4LMs [14]. In fact, Line 2 of your Algorithm 1 is the same as Line 9 of Algorithm 1 of RL4LMs.\n\n2. > ... show a combination of algorithmic modifications to improve training efficiency and performance\n\nWhile I appreciate the experiment results, I have to say that those modifications are very standard in previous offline RL and NLP papers, as indecated by the papers listed in my main review. Therefore, I tend to not counting those modifications as the algorithmic contribution of this paper, and would encourage the authors to clearly cite and state the origin of them in the paper.\n\n3. I don't quite understand your comparision with AWAC in the rebuttal. My understanding is that the main differences are with problem setting: offline v.s. online off-policy, and stepwise v.s. sequence-wise; rather than algorithmic differences. \n\n4. > We sample one output per input in the validation... with the mean squared error loss.\n\nThanks for the clarification. Then, as discussed in my main review, how is the estimated $V(\\cdot)$ different from $R(\\cdot)$?\n\n5. > It is quite common to have noisy, incoherent, or toxic language in datasets.\n\nAs far as I understand, low advantage means $R - V < 0$. In other words, it means that the quality of those texts is \"below average\". It does not mean that those texts are \"noisy, incoherent, or toxic language\", which is just a sufficient condition but by no means necessary.\nMy understanding is that the findings in Appendix B.4, Table 12 are tied to the specific datasets that you are working on.\n\n6. > In the HHA task, humans rate A-LoL model\u2019s responses (with scalar importance weight) more helpful than the A-LoL sequence (per-token importance weight).\n\nMaybe I misunderstand something, but it looks to me that in Table 3, the best performing method in terms of Human evaluation is \"**A-LOL seq.**\", which is also bolded.\n\n\nA small suggestion: for your *Additional references* in the rebuttal, it would be more readable if you could include the name of the paper (like what I did in the review)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545491131,
                "cdate": 1700545491131,
                "tmdate": 1700545491131,
                "mdate": 1700545491131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5WVo3x2QNB",
                "forum": "ZDGKPbF0VQ",
                "replyto": "S4SuC0Y8KJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. *\u201csequence-level value estimate has been used in RL4LMs [14]. In fact, Line 2 of your Algorithm 1 is the same as Line 9 of Algorithm 1 of RL4LMs.\u201d*  \nIt is not correct that RL4LMs use a sequence-level value estimate. RL4LMs makes the per-token action assumption [12,13] and the sequence-level reward arrives at the end of the episode (Section 3.1 in RL4LMs[14], [official code](https://github.com/allenai/RL4LMs/blob/main/rl4lms/envs/text_generation/reward.py#L179-L205)). The per-token reward definition (eq (1) in RL4LMs) also includes a per-token KL penalty. Subsequently, Line 9 in RL4LMs Algorithm 1 is a per-token value estimate from the samples drawn from the masked LM policy and is updated throughout the training. In contrast, our Line 2 in Algorithm 1 estimates a scalar value for the entire input (single-action assumption) on the validation performance of reference LM only once before the training. This reference LM value estimate allows the removal of negative advantage in offline training and enables prioritized sampling.\n2. *\u201calgorithmic modifications are very standard in previous offline RL and NLP papers\u201d*  \nWe acknowledge that clipping and reward-based prioritized sampling are modifications suggested by two independent works and are respectively cited as inspiration in Section 2.2 (page 4). However, computing the cheap sequence-level value estimate in an offline setting and removal of negative advantages is novel in our technique. We are able to use advantage-based prioritized sampling because the reference LM advantages stay static. Overall, we tried multiple combinations of these optimizations and developed an offline RL training recipe for LMs that works very well in practice.\n3. *\u201ccomparison with AWAC \u2026 the main differences are with problem setting: offline v.s. online off-policy, and stepwise v.s. sequence-wise; rather than algorithmic differences.\u201d*  \nThere are key algorithmic distinctions in the value estimate of A-LoL and AWAC along with the other mentioned differences in the problem setting. Algorithm 1 lines 5,6 in AWAC [3] uses the samples from off-policy trajectories in $D$ to continually update both the value model and policy model which are two separate networks with identical architectures (Appendix C in AWAC). Instead, A-LoL reuses the bulk of frozen parameters of $\\pi_{ref}$ and trains its sequence-level value estimate only once before target policy training and, therefore, is not only more computationally efficient than AWAC but is also able to remove negative advantage data and employ prioritized sampling. Our derivation also naturally yields importance weight in our main equation, which across all our experiments, has shown improvement in the performance and stability.\n4. *\u201chow is the estimated $V(.)$ different from $R(.)$?\u201d*  \nConcretely, we define a value layer (MhA + MLP) on top of the frozen reference LM which only uses the hidden states of any given input $V_{\\pi_{ref}}(x)$ and emits a scalar value as output. To train the value estimate, we employ MSE loss on the rewards ($R(x, y\u2019)$) obtained from one reference LM output ($y\u2019 \\sim \\pi_{ref}(x)$) for all the inputs in the validation set $D_v$ (280 instances in HHA task). Our value function gets a crude estimate of how well the reference LM is expected to behave on any input. In our experiments, the MSE loss of this value estimate approaches 0 but is never equal to 0. Since, the reference LM cannot perfectly reproduce the performance of all the static trajectories in $D_{tr}$ and there is some delta between the rewards of the training set and the value estimate of the reference LM (line 3 of Algorithm 1 in our work).\n5. *\u201clow advantage means $R - V < 0$. In other words, it means that the quality of those texts is \"below average\"\u201d*  \nInstead of \u201cbelow average\u201d, we view the negative advantages as instances where reference LM is expected to generate better quality outputs than the ones in the static training set. Intuitively, there should be no benefit of training on these poor quality train trajectories and subsequently removing negative advantage instances yields benefits in all the tasks. We refer to these instances as suboptimal or noisy trajectories. In experiments with known good and bad data splits, A-LoL automatically finds a larger subset of negative advantages in the bad data split (48% of Reddit downvoted comments compared to 36% of Reddit upvoted comments).\n6. *\u201cin Table 3, the best performing method in terms of Human evaluation is \"A-LOL seq.\"\u201d*  \nThis is not correct. A-LoL seq. is considered more safe and helpful according to GPT-4, but humans rate A-LoL more helpful than A-LoL seq. (53.5% win vs 49.3% win) and A LoL seq. more safe than A-LoL (63.3% win vs 46.7% win). The discrepancy between GPT-4 and human eval is expected as we saw only 72% alignment in their labels. However, human evaluation should be considered more trustworthy."
                    },
                    "title": {
                        "value": "Response to the additional questions"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611368902,
                "cdate": 1700611368902,
                "tmdate": 1700611744267,
                "mdate": 1700611744267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WsJggmVRKJ",
                "forum": "ZDGKPbF0VQ",
                "replyto": "arluNVQR7F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for your reply. Below are my additional comments.\n\n\n1. > ...removal of negative advantages is novel in our technique\n\nIt seems to me that this technique comes from Eq. (3) of the CRR paper [24], except that you are dealing with the bandit setting and have added several other techniques.\n\n\n2. > ... we view the negative advantages as instances where reference LM is expected to generate better quality outputs than the ones in the static training set\n\nIt is unclear to me the rationality and development of this view. I think the practical efficacy of this view is very likely tied to the specific datasets and tasks that you are working on, though it does work in this regard.\n\n3. > humans rate A-LoL more helpful than A-LoL seq. (53.5% win vs 49.3% win) and **A LoL seq. more safe than A-LoL (63.3% win vs 46.7% win).**\n\nThank you for pointing out this interesting comparison.\nIf I understand correctly, this comparison coincides with the concern in my original review that the success of A-LoL sequence may *contradict the single-action assumption* --- as we can see, A LoL seq. is more safe than A-LoL. \n\n[24] Wang, Ziyu, et al. \"Critic regularized regression.\" Advances in Neural Information Processing Systems 33 (2020): 7768-7778.\n\n***\nNevertheless, I appreciate the authors' trial of multiple combinations of RL-optimization techniques and the development of an offline RL recipe that practically works well for LM training. I will increase my rating to 5."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621139358,
                "cdate": 1700621139358,
                "tmdate": 1700621139358,
                "mdate": 1700621139358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AGYPV44pfM",
                "forum": "ZDGKPbF0VQ",
                "replyto": "S4SuC0Y8KJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional comments"
                    },
                    "comment": {
                        "value": "We thank the reviewer for considering our responses to the previous concerns and improving the score. However, we would like to address the additional comments raised.\n\n1. *\u201cremoval of negative advantage comes from Eq. (3) of the CRR paper [24], except that you are dealing with the bandit setting and have added several other techniques.\u201d*  \nThe eq (3) in CRR uses a sign operator on *target* policy advantage as follows $\\mathbb{1}[A_{\\theta}(s,a) > 0]\u200b$. The authors of CRR mention that *\"Intuitively, Eq. (3) entails BC on a filtered dataset where the filtering increases the average quality of the actions we learn from\u201d*. This is different from A-LoL, which uses reference policy advantage estimates and also naturally includes an importance weight term. Thus, the sum total of various components and training procedure of A-LoL is very different from CRR. Moreover, CRR is developed for continuous control tasks where reward signals and data assumptions are very different from the language domain. A-LoL is specifically designed for the large action space and sparse reward signal of language tasks and thus, creates a unique offline training recipe for LMs.\n2. *\u201cI think the practical efficacy of this view is very likely tied to the specific datasets and tasks that you are working on\u201d*  \nWe didn\u2019t apply any special criteria in our task selection that would make the removal of negative advantages particularly beneficial. In our experiments, we tested 4 different language generation tasks each with a different set of rewards and data sources - HHA uses an unknown 52B LLM generated responses (Sec 4), Reddit uses human-written internet comments (Sec 5), Commonsense reasoning uses GPT-3 generated data (Appendix C.1) and Knowledge-grounded dialog uses crowd-collected conversational data (Appendix C.2). Depending on the quality of the data source, reward distribution, its supervised reference LM, A-LoL finds 10% to 55% of the data as negative advantage. \n3. *\u201cthe success of A-LoL sequence may contradict the single-action assumption\u201d*  \nWe would like to reiterate that, although A-LoL seq. improves in diversity and safety, it is not the best-performing method in all the metrics (since humans rate A-LoL more helpful than A-LoL seq. in HHA evaluation Table 3). The net benefit of these different variants of importance weight (full-trajectory, per-token, and KL) is minor compared to the benefit brought by the entire suite of A-LoL methods: changing the reward term with reference LM advantage (as evidenced by all 4 task results in Tables 2, 4, 5, and 6)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675349688,
                "cdate": 1700675349688,
                "tmdate": 1700677246528,
                "mdate": 1700677246528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TKoZbo5U3f",
                "forum": "ZDGKPbF0VQ",
                "replyto": "AGYPV44pfM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_u5W8"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for the reply. Below are my comments to some of your misunderstandings.\n\n1. > The eq (3) in CRR uses a sign operator on target policy advantage...\n\nAs far as I know, $\\mathbb{1}[\\cdot]$ stands for the indicator function, not the sign operator.\n\n2. > although A-LoL seq. improves in diversity and safety, it is not the best-performing method in all the metrics\n\nI did not claim that A-LoL seq. is the best. I only said that it is successful compared to A-LoL, other A-LoL variants, and the baselines. As I keep saying, the per-token nature of \"A-LOL sequence\", together with its good empirically results relative to A-LoL, may contradict to the single-action assumption, which is a central backbone to your paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721177828,
                "cdate": 1700721177828,
                "tmdate": 1700721177828,
                "mdate": 1700721177828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bIEMVkp07E",
            "forum": "ZDGKPbF0VQ",
            "replyto": "ZDGKPbF0VQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces advantage-leftover lunch RL (A-LoL) which is a class of offline policy gradient algorithms. Crucially, different from most past work in RL+NLP literature, the assumption is that the entire LM output is a single action. \n\nThe algorithm is essentially MLE loss (standard cross entropy loss for text generation) but multiplied by sequence-level advantage and importance weight. Algorithm 1 contains the pseudo code. A few tricks are used, including clopping importance weight, advantage priority sampling (simply discard examples with negative advantage). A few variants are proposed\n- Regular\n- \u201cRef free\u201d variant (using 1 as importance weights)\n- \"Sequence\u201d variant (see my concern below)\n- \u201cKL\u201d variant (replacing importance weights with KL)\n\nBaselines include PPO (on-policy RL), direct preference optimization (DPO; recent non-RL approach), preference ranking optimization (PRO), and GOLD (offline RL / learning from demonstrations). The generation models are based on llama-7b architecture. The reward models are taken from Pythia (trained on Open Assistant; see footnote 8 and surrounding footnotes). The algorithm is tested on four text generation tasks. The main text includes the helpful harmless assistant task (Anthropic\u2019s dataset), and a Reddit response generation task. There\u2019s improvement over PPO and approaching DPO performance on the helpful harmless assistant task. Different tasks used different baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I appreciate the direction of exploring objectives that are not on-policy RL, which is the hot topic these days in fine-tuning LLMs. \n\nI\u2019m glad the discussion of comparison with GOLD exists (Section 2.4), because the motivation and derivation are extremely similar to GOLD (except for a few differences like the per-token action vs. single action distinction, the different treatment of importance weights, etc., as described in Section 2.4). I think it\u2019s totally fine even if it\u2019s similar to GOLD \u2013 there are design differences, a few tricks are used, and more experiments are done. \n\nRelated to the above point: for experiments, I especially appreciate the experiments on the helpful-harmless assistant task."
                },
                "weaknesses": {
                    "value": "Approximations don\u2019t seem justified mathematically. Maybe it\u2019s alright given RL+NLP research has too many approximations in general \u2013 I\u2019ll need to see what other reviewers think. \n- For the \u201cref free\u201d variant: Is it justified to use 1 as importance weights, in the reference free variant of the algorithm? I can\u2019t wrap my head around whether that\u2019s an acceptable approximation, or whether that's making the derived Equation (3) or Equation (4) simply incorrect. \n- For the \u201csequence\u201d variant: The approximation of importance rule is a bit strange. See line 6 of the \u201cvariants with alternative importance weight\u201d paragraph on page 4. it\u2019s essentially saying a1 * a2 * \u2026 * aT * (b1 + b2 + \u2026 + bT) = a1 * b1 + a2 * b2 + \u2026 + aT * bT. But this seems wrong? Am I understanding this correctly? Perhaps an explanation of why this is a good approximation will be helpful. But at the same time, the empirical results aren\u2019t really impacted much, so I\u2019m conflicted on how much I should treat this approximation seriously. \n\nA major issue: did the authors train PPO methods for more training steps (more than 1.3 times the training steps of offline methods)? If for more training steps, PPO results improve but your results stay stable, then we can\u2019t say PPO is worse. \n\nPhrasing of the main question in paragraph 1 \u2013 the main question seems to be the italicized sentence at the end of the first paragraph: \u201ccan we perform rewarded learning, similar to PPO, while only using pre-existing data\u201d but the answer is already yes given the literature in the past few years. \n- Cringe loss (https://aclanthology.org/2023.acl-long.493.pdf), as well as the older director loss (https://arxiv.org/pdf/2206.07694.pdf) and unlikelihood loss are relevant. The other algorithms the authors cited are also examples where we can learn from rewards while only using pre-existing data. I think the authors\u2019 research question can be more specific & take prior work into account. \n- In addition, I\u2019m also confused about what \u201csimilar to PPO\u201d means: do the authors mean that PPO is a form of \u201crewarded learning\u201d or do the authors mean \u201ccan we perform rewarded learning such that the results on X benchmark is similar to PPO performance?\u201d\n\nNo on-policy RL performance on Reddit generation task. Is PPO helpful here (given that it\u2019s so popular)?"
                },
                "questions": {
                    "value": "Can you elaborate what leftover lunch means? \n\nWhat amount of training trajectories have importance weights that are larger than 1+epsilon or smaller than 1-epsilon (given the bounds on page 4 in \u201cclipping importance weight\u201d)?\n\nFor the HHA task, the authors say that they filtered out 20K out of ~160K of training data with responses that abruptly end in a colon. Can the authors explain why this is helpful/necessary, or give an example? \n\nWhat kind of tasks would this method not make a difference or fail on? Or does this method work on any text generation task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697869843860,
            "cdate": 1697869843860,
            "tmdate": 1699637102942,
            "mdate": 1699637102942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BnI7JRZ4wB",
                "forum": "ZDGKPbF0VQ",
                "replyto": "bIEMVkp07E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are encouraged that they find our discussion about online vs offline methods that use preference or rewards useful and our motivations and technical contributions clear.\n\n__Responses to the Weaknesses:__\n1. *\u201cFor the \u201cref free\u201d variant: Is it justified to use 1 as importance weights?\u201d*  \nYes. A-LoL reference free is effectively the supervised LM advantage multiplied with negative log-likelihood loss. Equation-wise, it is very similar to the weighted behavior cloning baseline which is the reward multiplied with the log-likelihood. Another way to look at reference-free is to use the clipping parameter $\\epsilon = 0$. A-LoL reference free should be viewed as an ablation as we mainly compare with it to investigate the impact of importance weight.\n2. *\u201cFor the \u201csequence\u201d variant: it\u2019s essentially saying a1 * a2 * \u2026 * aT * (b1 + b2 + \u2026 + bT) = a1 * b1 + a2 * b2 + \u2026 + aT * bT\u2026 an explanation of why this is a good approximation will be helpful\u2026\u201d*  \nTo clarify, the sequence variant importance weight is an approximation and not mathematically equal. Intuitively, it tries to reweight each token\u2019s likelihood with its token importance weight while still using trajectory-level advantage values. Empirically, we find this approximation to improve diversity in all the tasks while also showing better training convergence than other A-LoL variants. We also find quantitative differences in how the sequence variant affects the importance weights (more in answer to question 2 below).\n3. *\u201cdid the authors train PPO methods for more training steps.\u201d*  \nIn order to compare the training efficiency of each method, we allocated a similar number of gradient updates to each baseline method including PPO. Our primary goal is to get the best possible policy from existing data while reducing computing costs. We ran for 2.6 times the training steps with PPO but didn\u2019t notice any significant improvement.\n4. *\u201cPhrasing of the main question in paragraph 1 \u2026 is already yes given the literature in the past few years\u2026  I\u2019m also confused about what \u201csimilar to PPO\u201d means\u201d*  \nWe thank you for sharing more related work. The main distinction with previous works is that A-LoL doesn\u2019t need model-generated data during target policy training and can use arbitrary classifiers as rewards. Both cringe loss [1] and unlikelihood training [2] require contrastive trajectories from the learner model during its training loop. Director [3] needs to train the classifier alongside the target LM and cannot use arbitrary pre-existing classifiers as rewards. By \u201csimilar to PPO\u201d, we mean the ability of PPO to incorporate arbitrary non-differentiable rewards while using pre-existing language data. We will update the text to highlight these distinctions.\n5. *\u201cNo on-policy RL performance on Reddit generation task\u201d*  \nWe thank you for this suggestion. We conducted additional experiments with PPO on the Reddit task with a sum of five classifiers as a reward and found that the best model achieved total reward of 3.06 (about 0.1 less than our best A-LoL method). However, their corpus diversity turned out to be very bad with most responses reading like \u201cThis is my favorite \u2026\u201d or \u201cI don\u2019t know \u2026\u201d. Overall, PPO gets stuck in a local maxima by optimizing three out of five rewards (fluency, safety, and tf-idf scores) and ignored the other two (engagement and upvote probability). We included this discussion with qualitative examples in Appendix C.3 (Tables 7 and 8)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540543201,
                "cdate": 1700540543201,
                "tmdate": 1700540543201,
                "mdate": 1700540543201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1j4usFKx5f",
                "forum": "ZDGKPbF0VQ",
                "replyto": "BnI7JRZ4wB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer responses to \"responses to the weaknesses\""
                    },
                    "comment": {
                        "value": "Q1 and Q2: Yeah I understand they are approximations. The question is how liberal (& potentially theoretically unjustified?) of an approximation is acceptable. But good to know that the results are good. \n\nQ3: \"We ran for 2.6 times the training steps with PPO but didn\u2019t notice any significant improvement.\" -- That's encouraging if true. It's great that the authors' approach outperforms PPO on the chosen tasks. \n- I think it's very important to include some results in the paper on longer training using the proposed approach vs. PPO. \n\nQ5: Thank you for the on-policy RL results on PPO."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722077578,
                "cdate": 1700722077578,
                "tmdate": 1700722077578,
                "mdate": 1700722077578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lw21ABvCVY",
                "forum": "ZDGKPbF0VQ",
                "replyto": "ovZf3OwkrA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Reviewer_KNWL"
                ],
                "content": {
                    "title": {
                        "value": "Responses to \"answers to questions\""
                    },
                    "comment": {
                        "value": "Q1: I see. I wouldn't have got that. Please emphasize why you named your algorithm A-LoL in a conspicuous spot in the paper. Otherwise I think many readers may be confused. \n\nQ2: Interesting. So the vast majority of the sequences are out of the (1-epsilon, 1+epsilon) region. \n\nQ4: Interesting. Another quick question: Would the method perform well on tasks that require longer sequences? I'm just curious what the authors' intuition is.\n\n\nI'm keeping my score at the moment."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722265867,
                "cdate": 1700722265867,
                "tmdate": 1700722265867,
                "mdate": 1700722265867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HvaC6wmQGY",
            "forum": "ZDGKPbF0VQ",
            "replyto": "ZDGKPbF0VQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_3aK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_3aK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a better off-policy policy-gradient based algorithm called Advantage Leftover Lunch RL (A-LoL) in the context of Reinforcement Learning from Human Feedback (RLHF). The proposed A-Lol significantly simplifies PPO by using a fixed advantage function and treating each response as a single action. Experiments are carried out mostly in the HHA benchmark and the reddit response generation task, showing the effectiveness and stability of A-lol."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The single-action assumption made here is very reasonable in the RLHF setting because the transition kernel in natural language generation is deterministic and trivial, such that the standard RLHF is in effect a contextual bandit problem instead of an RL problem. RL algorithms like PPO are unnecessarily complicated in the standard RLHF setting, so it's nice to see a more stable contextual bandit problem algorithm. The proposed method is different enough from other alternatives.\n\nIn the common HHA benchmark, A-LOL beats other recent preference-based offline RL baselines such as DPO and PRO and other common baselines such as weighted behavior cloning and PPO. Experiments on reddit generation task also shows the advantage of the proposed method and its flexibility in terms of optimizing for versatile rewards."
                },
                "weaknesses": {
                    "value": "A-LoL does not seem to perform better than DPO on the HHA benchmark with the common reward function. In particular, A-LoL seems to be less \"Helpful\" compared to DPO. Is there any explanation on that? The paper seems to be suggesting that there is an issue of reward hacking with the common reward function, is there a concrete example supporting this claim?\n\nIn the offline setting where all the data comes from existing offline datasets, the best that we can do seems only to be as good as the best trajectories in the offline datasets. Is it possible to modify A-LoL such that it can continue to improve itself with online data generated by itself and labeled by the reward model?\n\n\nminor - The single-action assumption might not always hold especially when the dialogue involves multi-step interaction with the users."
                },
                "questions": {
                    "value": "Why is this method called advantage leftover lunch RL?\n\nI'm curious if other contextual bandit algorithms (such as those from https://arxiv.org/abs/1802.04064) can work well too in the RLHF setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8769/Reviewer_3aK6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698107341084,
            "cdate": 1698107341084,
            "tmdate": 1699637102822,
            "mdate": 1699637102822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d3ZLaekkpN",
                "forum": "ZDGKPbF0VQ",
                "replyto": "HvaC6wmQGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and are encouraged to see that they find our method more straightforward in language settings compared to modern PPO implementations for RLHF. We also appreciate that the reviewer found our method \u201cdifferent enough from other alternatives\u201d and allows \u201cflexibility in terms of optimizing for versatile rewards\u201d.\n\n__Response to weaknesses:__\n1. *\u201cThe paper seems to be suggesting that there is an issue of DPO reward hacking with the common reward function, is there a concrete example supporting this claim?\u201d*  \nDuring our analysis of high-reward responses in the helpful subset of HHA task, we found that responses with list of items were assigned unusually high rewards. For example, even an empty list (\u201c\\n- \\n- \\n- \u2026\u201d) generated by DPO received 0.84 reward. Subsequently, we found DPO to generate a total of 294 responses with lists where A LoL seq. only generated 51 responses. In comparison, the test set only contained 24 responses with lists. \n2. *\u201c... Is it possible to modify A-LoL such that it can continue to improve itself with online data generated by itself and labeled by the reward model?\u201d*  \nInteresting question! In the future, we certainly want to investigate better exploration strategies than a random sampling of current RLHF implementations and eventually improve A-LoL further with high-quality on-policy generations. In this study, we wanted to benchmark and highlight the interesting properties of A-LoL in the offline setting.\n3. *\u201cThe single-action assumption might not always hold especially when the dialogue involves multi-step interaction with the users.\u201d*  \nWe agree with this issue and will include it in our limitations section in Appendix D.\n\n__Answers to questions:__\n1. *\u201cWhy is this method called advantage leftover lunch RL?\u201d*  \nA-LoL has the unique feature of using supervised finetuned LM\u2019s value estimate to detect the negative advantage training instances. By exclusively training from positive advantage data, our method is robust to noisy instances (for instance, Reddit response generation from downvoted comments - Section 5). To capture this essence of selecting the leftover positive advantage data, we call our method Advantage Leftover Lunch.\n2. *\u201cI'm curious if other contextual bandit algorithms (such as those from https://arxiv.org/abs/1802.04064) can work well too in the RLHF setting?\u201d*  \nThank you for this interesting suggestion. We will explore these methods in our future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541185052,
                "cdate": 1700541185052,
                "tmdate": 1700541185052,
                "mdate": 1700541185052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "esu8qcqbw1",
            "forum": "ZDGKPbF0VQ",
            "replyto": "ZDGKPbF0VQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_hcnY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8769/Reviewer_hcnY"
            ],
            "content": {
                "summary": {
                    "value": "Large language models have shown performance improvement due to training them with reinforcement learning (RL) from human feedback. But unlike other paradigms of learning, RL is very data-inefficient. The authors propose to address this issue by introducing a class of algorithms called Advantage-Leftover Lunch RL (A-LOL). The idea of these algorithms is to utilize better the SFT and the underlying data that the RL algorithm uses. In particular, A-LOL algorithms are offline algorithms that do not require any online samples, making their algorithm more sample-efficient than RL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- The motivation of the paper and the technical contributions are clear.\n- The authors perform a thorough empirical investigation of their proposed technique across several tasks. The authors conducted various ablation experiments of the proposed idea to show why the algorithm performed well (e.g., R-LOL versus A-LOL).\n- The author studies an important question of comparing online and offline policy gradient algorithms.\n- The authors also discuss an interesting issue around a subset of RLHF techniques needing human preference data, whereas other techniques do not."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- Given that in language, the transition function is trivial - it is unclear why offline algorithms are more sample-efficient than online algorithms. Offline algorithms assume access to a lot of quality data, while online algorithms can work with small amounts of data and well-designed reward functions.\n- The paper relies on the assumption that each token is not an action but instead a sequence is an action, but it is unclear why this assumption matters. Most RLHF techniques optimize policies on sequence-level losses, not token-level losses because the reward functions are defined on a sequence.\n- For the A-LOL algorithms to work, there is a set of assumptions that are not explicitly mentioned in the paper that could have a big impact on performance. In particular, if you don't have good data coverage and a good initial policy, then A-LOL will fail, which means that A-LOL and PPO in RLHF have the same assumptions.\n- The experiments did not include PPO due to a seed collapsing, but there has been evidence in the literature that this does not happen, which means the authors did not tune this baseline algorithm properly [1]. \n- The authors claim that their proposed approach is more data-efficient because they filtered out 33% of good responses, but the same procedure can be done for other techniques. However, a similar procedure was not conducted for the baseline algorithms to show that with less data, their proposed approach performs better.\n\n[1] PAIRWISE PROXIMAL POLICY OPTIMIZATION: HARNESSING RELATIVE FEEDBACK FOR LLM ALIGN- MENT  https://arxiv.org/pdf/2310.00212.pdf"
                },
                "questions": {
                    "value": "- There seems to be a typo in equation (2) $\\mathbb{E}_{\\boldsymbol{x} \\sim d^{\\pi^{ref}}, \\boldsymbol{y} \\sim \\pi\\_\\theta} [R(\\boldsymbol{x}, \\boldsymbol{y}, \\star)]$ because $y \\sim \\pi\\_\\theta$ in the expectation.\n- What is the difference between a single action step and trajectory-base RL? Most RLHF algorithms assume we are performing trajectory-based RL and not token-based RL. The reward function is only defined on the trajectory, not the token level.\n- Why is PPO, an online policy algorithm, only able to represent action on a token level, but all the offline policy algorithms can represent action on a sequence level?\n- Why can you not optimize the PPO  objective with multiple rewards? If so, then how does PPO perform?\n- Are you saying that equation (3) and equation (4) are equal? \n- In equation (3) are you ignoring the derivative concerning $\\pi\\_\\theta$ in the ratio ($ \\nabla_\\theta \\frac{\\pi\\_\\theta}{\\pi\\_{ref}}$)?\n- Why are the inputs $D_x$ in $D_{tr}$ satisfying this $D_x \\subset d^{\\pi^{ref}}$? If you are assuming that $\\boldsymbol{x}$ is indepdnent of $\\pi_{ref}$ then that seems to mean that $D_x = d^{\\pi^{ref}}$.\n- What is M, h and $A(\\pi\\_{ref})$ in algorithm 1 line 1?\n- Did you run experiments with R-LOL with the advantage of the learner policy instead of the reward?\n- Why can't you sum $\\pi\\_{ref}$ log probabilities and compute the reward for GOLD the baseline? \n- Why is A-LOL more data-efficient? You could also filter the data based on pairs with low rewards for the baseline algorithms and train them. But it is hard to understand if your algorithm is more data-efficient without training the baseline algorithm with the same data. \n- The average length of ppo is very odd compared to other algorithms. Do you have qualitative outputs to share? Did you include the kl-penalty into the objective? \n\n\n\nMissing citations:\n- Pairwise Proximal Policy Optimization: Harnessing Relative Feedback For LLM Alignment Wu et al. 2023\n- Learning to Generate Better Than Your LLM by Chang et al. 2023\n- Calibrating Sequence Likelihood Improves Conditional Language Generation by Zhao et al. 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8769/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699332282346,
            "cdate": 1699332282346,
            "tmdate": 1699637102698,
            "mdate": 1699637102698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8WY36mtjyJ",
                "forum": "ZDGKPbF0VQ",
                "replyto": "esu8qcqbw1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8769/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are encouraged that they find our discussion about online vs offline methods that use preference or rewards useful and our motivations and technical contributions clear.  \n__Response to the weaknesses:__\n1. *\u201cwhy offline algorithms are more sample-efficient than online algorithms \u2026 online algorithms can work with small amounts of data and well-designed reward functions.\u201d*  \nThis is not always the case. Online RL is only sample efficient if exploration yields high-quality data, and the reward function is well-designed. Exploration with LMs is tricky due to exponentially large output space [2,3] and reward arrives at the end of the output [4]. Instead, offline RL operates on static training data. A-LoL can further filter low-quality instances using negative advantage. In the HHA task we allocate a similar number of gradient updates to all baselines and PPO shows slower growth (Fig 3) compared to offline RL methods (Fig 2).\n2. *\u201cThe paper relies on the assumption that each token is not an action but instead a sequence is an action, but it is unclear why this assumption matters\u201d*  \nMost prominent implementations of PPO in RLHF use per-token as action and assign the full sequence reward to the last token [5,6,7]. This makes the value estimate of intermediate actions tricky and also affects the loss values at each token. Thus, concurrent works are investigating entire output as single-action in offline RL [1,8] for LMs. Using the single-action assumption, our method trains the reference LM value-estimate of the entire sequence and freeze it before training target policy, which allows for stable training (Fig 2).\n3. *\u201cif you don't have good data coverage and a good initial policy, then A-LOL will fail\u201d*  \nThank you for highlighting this point. We agree that a bad initial policy or lack of good data coverage can be detrimental to the performance of A-LoL. We will clarify this in the limitations section (Appendix D). \n4. *\u201cdid not include PPO due to a seed collapsing, but there has been evidence in the literature that this does not happen[1].\u201d*  \nWe include the average PPO performance for the two out of three seeds (Table 2). We followed the experiment setup from the PRO baseline [10] which used a 1.4B parameter reward model [11] for training and a 6.9B parameter model for evaluation. The reward model used in the provided reference [1] is a 6B parameter and even the reference policy is different from what we used in our experiment, which could explain the difference in PPO\u2019s performance.  \nFor hyperparameter tuning, we selected one seed and tested with PPO rollout batches in {16, 128} and learning rates in {2e-4, 2e-5, 5e-6, 5e-7} and found the best configuration be batch size=16 and lr=2e-5. Extending these hyperparameters to 3 seeds resulted in one of the seeds collapsing which highlights the instability in on-policy RLHF [1,2,3]. To verify the correctness of our PPO implementation, we also include the PPO checkpoint from an [external source](https://huggingface.co/reciprocate/ppo_hh_pythia-6B), but found our PPO implementation to work better.\n5. *\u201cauthors claim that their proposed approach is more data-efficient because they filtered out 33% \u2026 but the same procedure can be done for other techniques\u201d*  \nThe reward-based and preference-based offline RL baselines do not come with any principled method to filter data on their own. To filter data with rewards, one would need to know the reward landscape of each training dataset and also analyze the low reward instances to pick appropriate thresholds [9], which is out of the scope of this study. A-LoL is unique in its ability to automatically detect the subset of positive advantage in any training set using the sequence-level value estimate of the initial supervised LM. The amount of negative advantage varies for each task, initial LM, and data split, for example, 33% of the response in HHA task (Section 4), 48% of the downvoted, and 36% of upvoted reddit comments (Section 5) and 10%, 39% and 55% of different data splits of knowledge-grounded dialog (Appendix C.2). \n\n__Answers to the Questions:__\n1. *\u201cThere seems to be a typo in equation (2)  because in the $y \\sim \\pi_{\\theta}$ in expectation.\u201d*  \nThe equation (2) does have $y \\sim \\pi_{\\theta}$ but we rearrange the probabilities to convert it to $y \\sim \\pi_{ref}$ as follows:  \n$\\nabla_{\\theta} J(\\theta) = E_{x \\sim d^{\\pi_{\\text{ref}}}} [\\sum_{y \\in \\mathcal{Y}}R(x, y, \\star) \\nabla_{\\theta}\\pi_{\\theta}(y|x)]$  \n$= E_{x \\sim d^{\\pi_{\\text{ref}}}}[\\sum_{y \\in \\mathcal{Y}} \\pi_{ref}(y|x)\\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)} R(x, y, \\star) \\frac{\\nabla_{\\theta}\\pi_{\\theta}(y|x)}{\\pi_{\\theta}(y|x)}]$  \n$= E_{x \\sim d^{\\pi_{\\text{ref}}}, y \\sim \\pi_{\\text{ref}}}[R(x, y, \\star)\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\text{ref}}(y|x)}\\nabla_{\\theta}\\ln \\pi_{\\theta}(y|x)]$"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8769/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538271528,
                "cdate": 1700538271528,
                "tmdate": 1700538271528,
                "mdate": 1700538271528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]