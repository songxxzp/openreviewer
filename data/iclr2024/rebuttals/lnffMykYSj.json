[
    {
        "title": "On the Long Range Abilities of Transformers"
    },
    {
        "review": {
            "id": "PX3BzsKutO",
            "forum": "lnffMykYSj",
            "replyto": "lnffMykYSj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_nsrb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_nsrb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an architectural modification for the transformer's attention mechanism, which allows it to generalize better to longer inputs. The authors argue that the key principles for long-range tasks are (1) inductive bias toward smoothness; and (2) locality. By modifying the transformer's attention with these principles in mind, the authors achieve empirical gains in the Long-Range Arena (LRA) benchmark."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Modifying the transformer architecture to better support long-range inputs is an important direction of research.\n2. The empirical gains look significant, but I am not sure they were compared to the right baselines."
                },
                "weaknesses": {
                    "value": "1. The paper contains many assumptions that are inaccurate or unjustified. For example:\n>The lack of effectiveness of transformers in this setting [long context]\n\nimplies that transformers are ineffective, in general, on long inputs. I am not sure that this is the case - I agree that transformers could be improved, but the main problem seems to me that most transformers cannot even process long inputs. When the input *does* fit in the transformer's context window, most results that I've seen are not that bad. In other words, claiming that \"transformers are ineffective in long context\" requires some experiments and justification, to show exactly what the authors mean, instead of relying on one paper who said so a few years ago.\n\nAnother inaccurate and unjustified claim appears in Section 3:\n>the observed sub-optimal performance of transformers on long-range tasks does not arise necessarily from issues of optimization or expressiveness, which are inherent to the architecture. Rather, it is a matter of generalization\n\nThis claim assumes that transformers are sub-optimal (compared to what?) on long-range tasks (which?), and claims that the issue is generalization. This is a bit of a vacuous claim, since \"Generalization\" can contain anything. If the model reaches a zero training loss but is worse at test time, is that generalization? Is that the kind of \"generalization\" that the authors refer to?\n\nThe paper contains further inaccurate claims like:\n>Long-range dependencies are often associated with optimization issues, such as exploding and vanishing gradient problems.\n\nwhich were not justified and references were not provided. I agree that in the past RNN era, this was indeed the common belief. But can this claim be said on \"long-range dependencies\" in general?\n\nAs a final example for inaccurate and unjustified claims, Section 3 says that:\n> on the LRA benchmarks including the validation set, large transformers can achieve near 100% accuracy,\n\nbut then says that:\n> the existing transformers underfit the long-range data.\n\nDon't these two claims contradict each other? If not, there should be provided some justification, numbers, and references.\n\n2. Motivation - the authors motivate their solution with the assumption that:\n>smooth and exponentially decaying kernels are associated with a long-range inductive bias\n\nThis \"axiom\" isn't clear - I am not sure what exactly the authors mean by \"smoothness\". And in general, why? Why would *decaying* any kind of attention would improve any long-range modeling? \nI was not convinced that these are indeed related to the problem/solution.\n\n3. Evaluation - the experiments section does not provide any details regarding the underlying model - number of layers, number of parameters, whether it was pretrained, etc. I am also not sure what is the right baseline - Table 1 lists a variety of baselines, but are they comparable in terms of sizes? Which of them is the baseline that has the exact same number of parameters and layers, but without the proposed attention modification?\n\n4. Clarity - there are many parts of the paper which are unclear. For example, the introduction says:\n>We discern two simple yet significant conditions (i) an exponential decaying positional structure, and (ii) a regularized smooth global operator. \n\n>furthermore (iv) present an SL-chunk variation\n\nAt this point in reading the paper, this is meaningless to me.\nAs another example, the next Background section mentions many terms, but completely meaningless for the uninformed reader. For example:\n\n> An emerging approach implicitly defines the convolution kernel via a learnable function (Romero et al., 2021). Namely, the kernel kh\ni (filter) at position i and channel h is defined by a function fh such that fh(i) = ki.\n\nFor a reader who did not read the paper by Romero et al. (2021), this doesn't mean anything.\nThe rest of the Background section continues to cite dozens of papers, while these citations *hurt* the readability of the paper if the reader had not read the referenced papers. Later, Alibi (Press et al., 2021)  is mentioned, along with its equations (Equation 1), without any elaboration, leaving the reader to try to understand the equations and their notations, while Alibi is completely irrelevant to the proposed approach.\n\nFurther, some figures in the paper are not explained nor elaborated. Figure 1, for example, shows \"Examples of random kernels of several long-range layers\", without mentioning (1) what does the x axis mean; (2) what is the y axis; (3) what are the different colored curves, and what should the reader understand from this figure."
                },
                "questions": {
                    "value": "### Questions\n\n1. What are the number of layers, number of parameters, was the model pretrained, etc.?\n2. All tasks in the experimental section are somewhat synthetic. Do the authors' conclusions hold for **text**-based tasks as well, e.g., long-document summarization, long-document QA, etc?\n3. Which of the baselines in Table 1 is the baseline that has the exact same number of parameters and layers, but without the proposed attention modification?\n4. The models that \"rely on global convolutions\" in Table 1 perform significantly better than the proposed LaS attention model. If so, what is the benefit of using the proposed LaS model? Who is expected to benefit from using LaS?\n\n### Summary\n\nWhile the high-level idea is interesting, I feel like this paper suffers from too many weaknesses (detailed above). I thus recommend rejection at this time, and hope that the authors would improve writing and justify their motivation in the future."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697563165790,
            "cdate": 1697563165790,
            "tmdate": 1699637153580,
            "mdate": 1699637153580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z2PmlzUsyF",
                "forum": "lnffMykYSj",
                "replyto": "PX3BzsKutO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive review and the constructive feedback.\n\n> The paper contains many assumptions that are inaccurate or unjustified\n\nWe disagree that the paper contains many inaccurate or unjustified assumptions. It appears the reviewer may not be fully cognizant of recent developments in the domain, which our paper discusses in the introduction and background sections. \n\n> For example: The lack of effectiveness of transformers in this setting [long context].. implies that transformers are ineffective, in general, on long inputs. I am not sure that this is the case - I agree that transformers could be improved, but the main problem seems to me that most transformers cannot even process long inputs. When the input does fit in the transformer's context window, most results that I've seen are not that ba,  In other words, claiming that \"transformers are ineffective in long context\" requires some experiments and justification, to show exactly what the authors mean, instead of relying on one paper who said so a few years ago. \n\n\nThe statement that the reviewer extracted from our manuscript (\"transformers are ineffective in long context\") is immediately substantiated in the next two paragraphs. \n\nIt seems that the reviewer may not be aware of critical and well-established results in long-range modeling, which are also discussed in our introduction and background. Specifically, state-space layers such as S4 [S4] (ICLR 2022), DSS [DSS] (NeurIPS 2022), H3 [H3] (ICLR 2023), S5 (ICLR 2023), and other variants such as Gated state-space [GSS] and Liquid state-space [LSS] are known to achieve significantly better results than transformers on long-range tasks (please see full reference list in the shared response).\n\n Notably, these layers outperform the best-known transformer variants by 20 points on the Long Range Arena Benchmark, a key benchmark in the field, and they do so with fewer parameters and even when compared to transformers trained on the entire sequence length. For more details, please refer to Table 1. \n\nAdditionally, studies such as [H3, GSS] have shown that the interleaving of such layers enhances performance on long-range language modeling tasks. This trend is also evident in RL [RL1,RL2], speech processing [S1,S2,S3], and other areas [F1,HY2] (see citations in the shared response).\n\n\n > This claim assumes that transformers are sub-optimal (compared to what?) on long-range tasks (which?), and claims that the issue is generalization. This is a bit of a vacuous claim, since \"Generalization\" can contain anything. If the model reaches a zero training loss but is worse at test time, is that generalization? Is that the kind of \"generalization\" that the authors refer to? \u201ccompared to what? ..\u201d :\n\nPlease refer to the first sentence in the section you cited: \u201cThis research starts with a systematic attempt to understand the reasons behind the inferior performance of transformers in long-range modeling, **compared to state-space layers and long convolution layers**\u201d.  \n\n*Additionally, please consider the third paragraph in the introduction:** \u201cMotivated by recent advances in deep long sequence modeling, we delve into the question of **why long-range layers such as state-space layers** (Gu et al., 2021b;a; Gupta et al., 2022a) and long convolutions (Li et al., 2022; Fu et al., 2023) perform well on the LRA benchmark and other long-range tasks\u201d. \n\nMoreover, the first sentence in the abstract states \u201c.. transformer architectures exhibit sub-optimal performance on long-range tasks **compared to recent layers that are specifically designed for this purpose**. In this work, drawing inspiration from **key attributes of long-range layers, such as state-space layers, linear RNN layers, and global convolution layers\u201d.** \n\nFor more discussion regarding these questions, please see point #4 in the shared response.\n\n\n> Long-range tasks (which?) : \n\nThe main focus of the paper is the Long Range Arena benchmark. However, the findings are also applicable to other long-range tasks, including long-range NLP (as seen in [H3,DSS]), RL [RL1,RL2], speech processing [S1,S2,S3], and more [F1,S4,HY2] (please see citations in the shared response).\n\n> \u201cGeneralization\" can contain anything\u201d\n\nGeneralization is a well-defined term widely used in machine learning papers. It encompasses various issues such as the absence of regularization, inadequate inductive bias, or a small training dataset. It does not pertain to everything. For instance, it does not cover multiple aspects of expressiveness or optimization (despite being related). \n\n> \u201cIf the model reaches a zero training loss but is worse at test time, is that generalization?\u201d \n\nYes, this is an example of a lack of generalization.\n\n> Is that the kind of \"generalization\" that the authors refer to? \n\nYes.\n\n> \u201cthe existing transformers underfit the long-range data\u201d\n\nThanks for catching this! This is a very confusing typo which is fixed in the revised manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734993208,
                "cdate": 1700734993208,
                "tmdate": 1700734993208,
                "mdate": 1700734993208,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PO2UIn8lC0",
            "forum": "lnffMykYSj",
            "replyto": "lnffMykYSj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_tj55"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_tj55"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates how transformers use long-range dependencies.\n\nThe thesis of the paper is that transformers suffer from lack of generalization. They propose several inductive biases to help. Namely, smooth and exponentially decaying kernels. They are inspired by prior work in transformer variants like state-space layers.\n\nThey invent a method called Local and Smooth attention (LaS) to test their hypothesis and achieve strong results in Long Range Arena (LRA) and Sequential MNIST. The local comes from weighting with what they call Exponentially Locally Decaying (ELD). The smooth comes from a convolution operator that does average pooling.\n\nThey do various ablations to further bolster their claims about generalization and the various operations in LaS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The empirical part of the paper is strong. The method does well and the ablations support the hypothesis. They investigate the generalization claim by manipulating context length and dataset size.\n\nThe refutation that transformers lack expressiveness or suffer from optimization problems is convincing.\n\nFigure 2 makes the method very clear."
                },
                "weaknesses": {
                    "value": "The section on identifying common design choices of prior work feels a bit handy wavy. Perhaps could be better presented in a table.\n\nThere is an error either in Equation 3 or Figure 2. They transpose the softmax and average pooling. My guess is the error is in Equation 3.\n\nThere seems to be only empirical evidence about why seemingly \"unintuitive\" methods work. Some theoretical justification would make the paper stronger."
                },
                "questions": {
                    "value": "What is meant by \"necessary conditions for achieving success in long-range tasks\"? Is it meant in the strict mathematical sense? If so, I would expect to see a proof along the lines of success implies these conditions.\n\nTo further support the lack of generalization due to underfitting hypothesis, have you tried getting more data beyond what's in the LRA dataset?\n\nSince this mechanism works in a causal manner, how well does it perform in decoding tasks like language modeling or translation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819605033,
            "cdate": 1698819605033,
            "tmdate": 1699637153458,
            "mdate": 1699637153458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hqpih1Jxau",
                "forum": "lnffMykYSj",
                "replyto": "PO2UIn8lC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive review and the constructive feedback.\n\n> The section on identifying common design choices of prior work feels a bit handy-wavy. Perhaps could be better presented in a table.\n\nThank you for the suggestion. We have added a comparison table and some discussion to Appendix E (in the revised manuscript).\n\n> There is an error either in Equation 3 or Figure 2. They transpose the softmax and average pooling. My guess is the error is in Equation 3\n\nThanks for pointing out this typo. We fixed Figure 2 in the revised manuscript. \n\n> There seems to be only empirical evidence about why seemingly \"unintuitive\" methods work. Some theoretical justification would make the paper stronger.\n\nPlease refer to Theorem 1 in Appendix B of the revised manuscript (see point #1 in the shared response). Transformers are shown to have the capacity to express several long-range layers, such as S4. This indicates that the performance gap in such tasks is not a problem of expressiveness, but rather, a problem of generalization.\n\nRegarding the 'unintuitive' method, while we agree that smoothness and locality may seem counterintuitive, we believe that the rationale for trying such a method becomes apparent after explaining that we took inspiration from long-range layers (see Figure 1 and the first paragraphs in Section 4).\n\nAdditionally, regarding why smoothness and locality enhance the model's ability to manage long-range dependencies, we hypothesize that in long-range tasks across common modalities such as images, text, and speech, long-range dependencies manifest through a hierarchical combination of local and smooth dependencies. As argued in Section 3, the difficulty in capturing these long-range dependencies stems from a generalization challenge, which can be mitigated by an appropriate inductive bias. Hence, such bias, when suitably applied, can be highly effective.\n\n> What is meant by \"necessary conditions for achieving success in long-range tasks\"? Is it meant in the strict mathematical sense? If so, I would expect to see a proof along the lines of success implies these conditions.\n\nIn this context, 'necessary conditions' is not used as a mathematical term. Due to the confusion, we have replaced \"necessary conditions\" with \"desired properties.\"\n\n> To further support the lack of generalization due to the underfitting hypothesis, have you tried getting more data beyond what's in the LRA dataset?\n\nWe acknowledge that incorporating additional samples could strengthen our claims. However, we have not pursued this approach, as utilizing more data could lead to questions about the data's origin, and we aim to maintain a fair comparison with previous transformer models evaluated on the LRA benchmark (as these results have been validated many times, and thus constitute a very stable baseline).\n \nWe believe that reducing the amount of data serves a similar purpose and is a much faster and more efficient method.\n\n> Since this mechanism works in a causal manner, how well does it perform in decoding tasks like language modeling or translation?\n\nIndeed, the current model is causal, but it can be adapted for non-causal tasks by converting it into a bidirectional model. This can be achieved by applying each layer twice, once in each direction, similar to other global convolution layers (like S4). Alternatively, we can adopt an approach similar to that in [A1], which extends Alibi to bidirectional by carefully defining the values in the upper part of the attention matrix.\n\n[A1] Lee, Minchul, Kijong Han, and Myeong Cheol Shin. \"LittleBird: Efficient Faster & Longer Transformer for Question Answering.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734401919,
                "cdate": 1700734401919,
                "tmdate": 1700734401919,
                "mdate": 1700734401919,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gJBKnrNWWX",
            "forum": "lnffMykYSj",
            "replyto": "lnffMykYSj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_LwQf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_LwQf"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests modifying the attention matrix in transformer layers, drawing inspiration from models like S4, with the aim of enhancing the transformer's capacity to generalize more effectively in long-range contexts.\n\nThe authors stress the significance of two main concepts: smoothness and some kind of locality. For smoothness, they implement 1-D average pooling on every row of the attention matrix. For the locality (exponential decaying of attention), an element-wise multiplication is applied between the attention matrix at each head and a locally decaying matrix (not learnable).\n\nLaS the, the proposed method, is evaluated on the LRA benchmark and vectorized MNIST (sequential and permuted) and compared with a few different transformer variants as well as S4, Mega, and LSTM. LaS seem to achieve the best performance among Transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and achieves better results on LRA tasks compared to other transformers.\n- Ablation experiments indicate the importance of both components of LaS attention (exponential decay of attention scores and smoothness)"
                },
                "weaknesses": {
                    "value": "- While experiments that study the impact of context length and sample size are intriguing, interpreting the results is challenging without comparing the patterns to any other baseline. For instance, in the experiment where you restrict the context window size to examine LaS's dependence on long-range dependencies, do we have insights into how a standard transformer might be influenced by a reduced context window size?\n- I believe the arguments about expressivity limitations and optimization challenges might be wrong.\n   - For instance, a model can fit the training data without necessarily capturing long-range dependencies, simply by leveraging spurious features.\n   - For example, I believe there are elements within the transformer block where some kind of diminishing effect might transpire as the context window becomes exceedingly long."
                },
                "questions": {
                    "value": "1. Could you explain how one should read Figure 1? What is the x-axis? What is the y-axis?\n2. Can you elaborate a bit more on the arguments about smoothness and its relationship to improving the model's capability to handle long-range dependencies?\n3. Same question as above about locality! What's the intuition?\n4. How does the Transformer architecture perform on other tasks (e.g., standard language modeling or typical image classification)? What do we sacrifice by biasing the models towards solutions that generalize better in long-range context settings?\n5. Have you considered learning the biases or explored applying different patterns? How does LaS compare to a model like Synthesizer?\n6. What does the final attention score matrix look like?\n7. Are there any interactions or side effects from the positional encoding?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820117329,
            "cdate": 1698820117329,
            "tmdate": 1699637153256,
            "mdate": 1699637153256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ivjqJ3Pal1",
                "forum": "lnffMykYSj",
                "replyto": "gJBKnrNWWX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive review and the constructive feedback.\n\n> While experiments that study the impact of context length and sample size are intriguing, interpreting the results is challenging without comparing the patterns to any other baseline. For instance, in the experiment where you restrict the context window size to examine LaS's dependence on long-range dependencies, do we have insights into how a standard transformer might be influenced by a reduced context window size?\n\nThe results of the full-length vanilla transformer are presented in the first line of Table 1, and they are relatively similar to those of simple local attention (which we have added to Table 1 in the revised manuscript). As can be seen, in all tasks except Pathfinder, LaS-chunk performs much better than both of the requested baselines. Therefore, we omit them from the ablations in Figures 3 and 4 for clarity. Furthermore, Figure 4 was created to strengthen the claims of Section 3, and therefore is not directly related to abating the LaS mechanism.\n\n> I believe the arguments about expressivity limitations and optimization challenges might be wrong.\nFor instance, a model can fit the training data without necessarily capturing long-range dependencies, simply by leveraging spurious features.\n\nWe recognize that certain edge cases may arise from the model's lack of expressiveness or optimization challenges. Examples include an inherent inability to express long-range dependencies or optimization issues such as vanishing gradients that prevent the model from learning these dependencies.\n\nHowever, according to the evidence provided by Theorem 1, the transformer architecture does not suffer from a lack of expressiveness, as it is more expressive than other layers that perform well on this dataset. Additionally, optimization is unlikely to be problematic since the generalization improves as the number of training samples increases (refer to Figure 4 and the detailed analysis in Section 3).\n \nFinally, please note that we have refined our claims based on this discussion (\u201cwe claim that the primary factor behind the suboptimal performance of transformers on long-range tasks is **probably** the lack of generalization\u201d, \u201cRather, it is **likely** a matter of generalization\u201d).  \n\n> For example, I believe there are elements within the transformer block where some kind of diminishing effect might transpire as the context window becomes exceedingly long. \n\nOur solution shows that generalization is possible even without modifying these elements. However, it may be possible that ablating parts of the transformer would also be beneficial.\n\n> Could you explain how one should read Figure 1? What is the x-axis? What is the y-axis?\n\nGiven a kernel k:= (k_1, k_2, .. k_L) of length L, axes y represent the value of the kernel (k_i) on position i. Axes x is the position i.\n\n> Can you elaborate a bit more on the arguments about smoothness and its relationship to improving the model's capability to handle long-range dependencies? Same question as above about locality! What's the intuition?\n\nFirst, it is important to recall that we utilize these fundamentals by drawing inspiration from global convolutional layers. For instance, Figure 1 presents relatively local and smooth kernels.\n\nSecond, regarding how smoothness and locality enhance the model's ability to manage long-range dependencies, we hypothesize that in long-range tasks across common modalities such as images, text, and speech, long-range dependencies manifest through a hierarchical combination of local and smooth dependencies. Since we argue that the problem of capturing long-range dependencies reflects as a problem of generalization, such inductive bias can be very effective.\n\n> How does the Transformer architecture perform on other tasks (e.g., standard language modeling or typical image classification)? What do we sacrifice by biasing the models towards solutions that generalize better in long-range context settings?\n\nRefer to point #3 in the shared response. We evaluate LaS-attention (compared to vanilla attention and additional ablations) on standard language modeling tasks in Appendix C. It appears that the locality does not negatively impact the results, and there might be a slight improvement with further tuning of the B hyper-parameter (see the last paragraph of Section 4). However, the smoothness seems to negatively affect the perplexity, possibly reducing the transformer's ability to model complex dependencies between tokens (As the 1D Avg Pooling regularizes the bias toward the pairwise interactions).\n\n> Have you considered learning the biases or explored applying different patterns? \n\nYes, we have considered such variants (and many more), but we didn't observe any major improvements. Please refer to our code for the exact settings and other variants at  \\src\\models\\baselines\\transformer.py, line 843:\n\n`self.alpha = alpha if not learn_local_param else torch.nn.Parameter(alpha)`"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734123748,
                "cdate": 1700734123748,
                "tmdate": 1700736501564,
                "mdate": 1700736501564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cpN9tLNvDH",
            "forum": "lnffMykYSj",
            "replyto": "lnffMykYSj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_hVaG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9164/Reviewer_hVaG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a modification of the self-attention mechanism in Transformer architectures that facilitates the learning of long-range interactions. As global convolutional layers such as SSMs perform significantly better on long-range tasks, the authors' investigation of these models reveals that the convolution kernel of such models often have an exponentially decaying structure with additional smoothness constraints. This motivates the authors to introduce a modified attention mechanism called LaS-attention, which incorporates exponential decay and smoothing (implemented by average pool) along attention scores to incorporate these inductive biases into the architecture. In this way, the output of each attention layer is mostly only influenced by local interactions with varying degrees of locality, and long-range reasoning is captured hierarchically through compositions. This greatly increases Transformer generalization performance on the LRA benchmark and reduces the gap from global convolutional models. Ablation studies demonstrate the benefit of each proposed component. Additional experiment on sequential MNIST also show improvements with respect to this modification. To summarize the key observation, the paper claims that long-range reasoning is best implemented as compositions of mostly localized interactions as motivated by the SSMs, and demonstrated in the context of transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing of the paper is clear, and original as far as I know.\n- The main strength of the paper is the intuition it provides; the reasoning is easy to follow and well-motivated, and it sheds light on the performance gap between SSMs and transformers on long-range tasks, which is a very important problem faced by Transformers.\n- The hypothesis that long-range reasoning is best implemented as compositions of (mostly) localized interactions with exponentially decaying dependencies is well-motivated as supported by the investigation of the kernels of global convolutions, and then verified in the experiments by implementing a simple fix in the attention mechanism."
                },
                "weaknesses": {
                    "value": "- Methodology: Although the insights are novel, significant, and interesting to read, the methodological novelty is limited. The proposed modification is a simple modification 1) of the pre-softmax linear attention matrix by pointwise multiplication with an exponential decay term, and then 2) smoothing of the activated attention matrix. Especially that a similar modified version appeared in previous work in (Press et al. 2021), and its main difference from 1) is effectively that the distance matrix is exponentiated.\n- Experiments:  I also found the experimental aspect somewhat lacking, as only LRA and sequential MNIST is considered, where on LRA there is improvement but not up to par yet with global convolutions. It would also be interesting to know if the proposed upgrade can provide improvements on other Transformer tasks, where it already performs as SOTA.\n\n\nOverall this is a very borderline paper for me as it is mainly a proof of concept, and while the insight is very valuable, I would have appreciated if the authors took the idea further."
                },
                "questions": {
                    "value": "#### Question 1:\nThe authors claim that \"It is fairly straightforward to show that a single layer of a transformer ... can express any state-space\nlayer\" and \"each channel of the state-space layer incorporates a long convolution kernel K, which can be expressed via the attention matrices\" \n\nI would like to ask whether the authors have any references for this claim or if they could provide a proof in the appendix? This seems highly non-obvious to me. I am aware of the previous work \"On the Expressive Power of Self-Attention Matrices\" Likhosherstov et al. 2021, which demonstrates that attention matrix can approximate sparse patterns, but I am not aware of more general results.\n\n\n#### Question 2:\n\n In another paragraph, \"the lack of generalization, caused by an unsuitable inductive bias that results in an unfavorable hypothesis class. In other words, the existing transformers underfit the long-range data.\" \n\nI have two problems with this sentence:\n1) is that previously the authors said that \" large transformers\ncan achieve near 100% accuracy\", which is not a sign of underfitting but overfitting;\n2) is that, although I am not sure about the claim in the previous question at all, if we assume it's true, then it basically says that the issue is not a problem with the hypothesis class in the classical sense, which is all the possible expressible models (although the set of reachable models by common initialization + training procedure combinations is another question, if this is what is meant then it should be clarified).\nIt seems to me that the issue is either - as the authors stated - that decomposing long-range learning into a series of locality pronounced layers helps in the training procedure for generalizability, i.e. this constraint mitigates overfitting by restricting the hypothesis class rather than enlarging it. I wonder if the authors have any thoughts on this? This is an interesting question.\n\n#### Question 3\nAnother question is whether the authors have any intuition about why the exponentiation of the distance matrix helps the attention compared to Alibi? This seems weird to me because applying the decay matrix before the softmax decreases large query-key dot-product values as intended, but negative dot-product values are actually increased, which actually degrades the ability of the model to reduce the dependence on certain tokens. I wonder if this is a desired effect due to some implicit regularization phenomenon of being less likely to attend to a very small select few tokens, or if this is might be harmful?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9164/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699225154305,
            "cdate": 1699225154305,
            "tmdate": 1699637153140,
            "mdate": 1699637153140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2w7CNlDuOL",
                "forum": "lnffMykYSj",
                "replyto": "cpN9tLNvDH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9164/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive review and the constructive feedback.\n\n>.. the methodological novelty is limited:\nOur method (Local and Smooth) is deliberately simple since we seek minimal modifications to the transformer architecture that could improve its long-range capabilities. We believe that controlled and simple experiments are essential for providing clear insights. \n\nThe main novelty of the paper lies in the insights we offer. We have characterized the main challenge of the long range problem (generalization) and identified fundamental principles that make long-range layers, such as state-space layers, much better than transformers on those tasks. Please refer to \u201cFraming of the Contribution\u201d (point #4)  in the shared response for a more comprehensive description of the insights we provide.\n\n> Experiments: I also found the experimental aspect somewhat lacking, as only LRA and sequential MNIST is considered, where on LRA there is improvement but not up to par yet with global convolutions. It would also be interesting to know if the proposed upgrade can provide improvements on other Transformer tasks, where it already performs as SOTA.\n\nFirst, please see the experiment result on language modeling (point #3 in the main response, Appendix C). Second, we want to highlight that the LRA benchmark is the central benchmark in the domain.\n\n> The authors claim that \"It is fairly straightforward to show that a single layer of a transformer ... can express any state-space layer\" and \"each channel of the state-space layer incorporates a long convolution kernel K, which can be expressed via the attention matrices\" I would like to ask whether the authors have any references for this claim or if they could provide a proof in the appendix? This seems highly non-obvious to me\n\nWe thank the reviewer for this comment. Our initial claim was that the kernel could easily be incorporated into key and query matrices. However, encouraged by your comment, we wrote a comprehensive formal proof, which we have included in Appendix B (Theorem 1). See point #1 in the shared response for more details. Please note that the proof holds in general settings for a large variety of global convolution layers, and it sheds additional light on the relationship between transformers and global convolution layers.\n\n>  \u2026 that previously the authors said that \" large transformers can achieve near 100% accuracy\", which is not a sign of underfitting but overfitting. \n\nThank you for catching this! It was a very confusing typo that was fixed in the revised version!\n\n> Another question is whether the authors have any intuition about why the exponentiation of the distance matrix helps the attention compared to Alibi? This seems weird to me because applying the decay matrix before the softmax decreases large query-key dot-product values as intended, but negative dot-product values are actually increased, which actually degrades the ability of the model to reduce the dependence on certain tokens. I wonder if this is a desired effect due to some implicit regularization phenomenon of being less likely to attend to a very small select few tokens, or if this is might be harmful?\n\nWe investigate this question through experiments. It appears that the model learns to control such phenomena by creating a non-symmetric distribution at the input to the softmax layer. Specifically, when iterating over 1,000 examples with a model trained using Large-Scale (LaS) attention, we observe that the values before the softmax layer are in the range of [-100,000,000 to 100], which could potentially handle your scenario."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9164/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733743361,
                "cdate": 1700733743361,
                "tmdate": 1700733743361,
                "mdate": 1700733743361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]