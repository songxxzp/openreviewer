[
    {
        "title": "Batch normalization is sufficient for universal function approximation in CNNs"
    },
    {
        "review": {
            "id": "QS6rtlFPds",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
            ],
            "forum": "wOSYMHfENq",
            "replyto": "wOSYMHfENq",
            "content": {
                "summary": {
                    "value": "This paper explores the role of normalization techniques, particularly Batch Normalization (BN), in deep convolutional neural networks (CNNs). The authors provide a theoretical analysis to demonstrate that training normalization layers alone is adequate for universal function approximation, assuming a sufficient number of random features. This result applies to various CNN architectures, including those with or without residual connections and different activation functions, such as ReLUs. The authors also explain how this theory can elucidate the depth-width trade-off in network design and the empirical observation that disabling neurons can be beneficial."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper provides a solid theoretical foundation for its claims, offering mathematical proofs and a well-structured argument.\n\n- This paper is written in a clear and easily comprehensible manner, making it easy for readers to follow."
                },
                "weaknesses": {
                    "value": "See Questions."
                },
                "questions": {
                    "value": "- In the introduction, I'm a bit confused about the normalization being explored in this paper. The title mentions batch normalization, but the statement \"we delve into the role of layer normalization\" suggests layer normalization. Is this a typo?\n\n- I'm puzzled by the assertion in Section 2 regarding the existence of $f_t$, which the author claims is due to the universal function approximation. Perhaps the author meant to refer to the Universal Approximation Theorem. However, such approximations in neural networks are typically conditional. Has the author considered these conditions? Generally, these conditions are not mild and are idealized. Does this affect the theory presented in this paper? The author should provide an explanation and discussion on this.\n\n- I'm not entirely sure why throughout the paper, normalization is reduced to just a linear transformation and shift, i.e., $\\gamma \\mathbf{h} + \\beta$. This includes batch norm, layer norm, instance norm, etc. They all have this form, so is their significance solely in the learnable parameters $\\gamma$ and $\\beta$? Of course, I understand that $[(x - \\mu) / \\sigma] \\times \\gamma + \\beta$ can be equivalent to $\\gamma \\mathbf{h} + \\beta$\" in the end, but why emphasize batch normalization in the title? Where does \"batch\" come into play?\n\n- Can the analysis apply to the existing advanced batch normalization improvements like IEBN [1] and SwitchNorm [2]. These missing works should be considered and added to the related works or analysis.\n\n- The author needs to clarify the above questions. If these issues are addressed, I will consider these clarifications along with feedback from other reviewers in deciding whether to raise my score.\n\n\n[1] Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise, AAAI\n\n[2] Differentiable Learning-to-Normalize via Switchable Normalization, ICLR"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1322/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa",
                        "ICLR.cc/2024/Conference/Submission1322/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1322/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697335020007,
            "cdate": 1697335020007,
            "tmdate": 1700732913635,
            "mdate": 1700732913635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4rweMhDeG8",
                "forum": "wOSYMHfENq",
                "replyto": "QS6rtlFPds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. In the following, we address their questions and concerns.\n\n- Indeed, we have used layer normalization when we meant normalization layers. We have revised the manuscript accordingly. \n\n- Universal Approximation Theorems often have mild assumptions on the activation functions (e.g. that they are monotonously increasing and continuous) and the smoothness of the data generating function (usually that it is continuous) [1].\nDepending on the considered architecture (e.g. a three layer neural network or a width-constrained perceptron with ReLUs, the approximation quality of the neural network is derived dependent on the number variable network parameters (e.g. the width of the 3-layer neural network or the depth of the width constrained perceptron) [2].\n\n    * By formulating theorems with respect to a given target network that solves a task of interest, we are flexible in which universal function approximation theorem could be invoked. Our results would inherit the specific assumptions.\n    * Also irrespective of the specific universal function approximation theorem, we can cover typical application cases, for which target networks have been obtained by training the networks in a classical way. We have clarified this argument on page 3.\n \n- Our title refers to batch normalization, as it is one of the first and thus most well known normalization techniques. For that reason, several theoretical investigations have focused on understanding its contribution to successful deep learning. Our title embeds our work in this rich line of research but we would be happy to change the it upon request to highlight the more general scope of our insights.\n\n- The analysis can be extended to IEBN and SwitchNorm. We thank the reviewer for pointing us these relevant references, which we have included as examples in our background section.\n\n[1] G\u00fchring, Raslan, Kutyniok (2022). Expressivity of Deep Neural Networks. In P. Grohs & G. Kutyniok (Eds.), Mathematical Aspects of Deep Learning (pp. 149-199). Cambridge: Cambridge University Press. doi:10.1017/9781009025096.004\n\n[2] Shen, Yang, Zhang (2022). Optimal approximation rate of ReLU networks in terms of width and depth, Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, Volume 157, Pages 101-135, ISSN 0021-7824."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563370685,
                "cdate": 1700563370685,
                "tmdate": 1700563370685,
                "mdate": 1700563370685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PoFIFD19PU",
                "forum": "wOSYMHfENq",
                "replyto": "QS6rtlFPds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for your detailed response. Upon reflecting on your response, I still find myself puzzled about whether normalization can be analyzed solely through $\\gamma \\mathbf{h} + \\beta$. While I understand that more intricate normalization sub-operations, such as \"minus the mean, etc.\", are challenging to analyze, I feel the need for a more reasonable explanation. Furthermore, I appreciate the contributions of this work and look forward to a well-founded explanation specifically addressing this issue. And I tend to raise my score if there are some rational explanations."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730310902,
                "cdate": 1700730310902,
                "tmdate": 1700730398876,
                "mdate": 1700730398876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wSoR6dle0O",
                "forum": "wOSYMHfENq",
                "replyto": "BcI36tmoSL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Reviewer_9YBa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you once again for your response."
                    },
                    "comment": {
                        "value": "Thank you once again for your response. While I'm not entirely satisfied with this explanation (It doesn't take away from the fact that this paper deserves to be accepted), I have gained valuable insights. I recognize the complexity of this issue, as I've been contemplating it for quite some time. Without simplifying it to $\\gamma \\mathbf{h} + \\beta$, normalization becomes challenging to analyze. Additionally, you are correct in noting that $\\gamma \\mathbf{h} + \\beta$ already possesses powerful expressive capabilities. I encourage you to incorporate this perspective into the article and cite the following papers with this perspective:\n\n[1] Stabilize deep resnet with a sharp scaling factor tau, arxiv\n\n[2] Rezero is all you need: Fast convergence at large depth, UAI\n\n[3] ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection, NeurIPS\n\n[4] Fixup initialization: Residual learning without normalization,ICLR\n\n[5] How to start training: The effect of initialization and architecture, NeurIPS\n\n\n\n.\n\n\n.\n\n\nOverall, I tend to raise my score, thank you again."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732900181,
                "cdate": 1700732900181,
                "tmdate": 1700732900181,
                "mdate": 1700732900181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OM7VIKqXEz",
            "forum": "wOSYMHfENq",
            "replyto": "wOSYMHfENq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_pcEF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_pcEF"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows that, under certain conditions, training just the batch normalization (BN) parameters is enough to make a CNN a universal function approximator."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper practices experimental method really well (has hypothesis, describes experiments and shows results)."
                },
                "weaknesses": {
                    "value": "The work describes BN as a subset of a Layer Normalization (LN).\nI'd note that they are both normalization layers, but BN is NOT a type of LN.\n(I'm effectively rejecting the paper because it needs to be rewritten based on this)\n\nBN, LN, GroupNorm, etc are all normalization layers, the only difference is the dimension along which normalization is done; GroupNorm (https://arxiv.org/abs/1803.08494) paper has a pictorial depiction of this.\nFurthermore, given BN aggregates statistics across samples, the neural network (NN) output of BN changes if the set of images changes; during training this makes a NN with BN a statistical operator. LN operates within a sample; the NN output does NOT change depending on input samples; during training this makes a NN with LN a function (not statistical). The Online Normalization paper (https://arxiv.org/abs/1905.05894) does a good job talking about this.\n\n\n\n\n- Reference are broken (there are things like \"Theorem ??\" in the paper).\n- Paper's experiments are really small scale for modern Deep learning leaving the reader wondering if they will scale."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1322/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682112991,
            "cdate": 1698682112991,
            "tmdate": 1699636059505,
            "mdate": 1699636059505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NKIoAWoacO",
                "forum": "wOSYMHfENq",
                "replyto": "OM7VIKqXEz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback and address their concerns below.\n\n* According the reviewers suggestions, we have revised the manuscript and replaced our misnomer layer normalization by normalization or normalization layer.\n\n* We have also addressed the broken Theorem link in the experiment section.\n\n* One of our main contributions is that we have derived the precise scaling requirements. While our experiments primarily serve the validation of our theoretical claims, note that our insights have allowed us to achieve much better performance than previous empirical work [1]. \n    - However, as we discuss on page 7 in Section 3.1 and on page 8, we have to acknowledge that the precision of numerical solvers is limited in solving large scale linear systems of equations. Note that the matrix $M$ in our experiments has already a dimension of up to $90000 \\times 90000$ roughly.\n    - In fact, our theoretical results imply that training only BN parameters for computational savings is not a practically reasonable approach if we want to maintain high expressiveness. Only inductive bias in the weight distributions (e.g. as obtained by foundation models) could enable training success despite a low number of degrees of freedom (see Lemma 2.1).\n\n\n[1] Frankle, Schwab, Morcos. Training BatchNorm and only BatchNorm: On the expressive power of random features in CNNs. ICLR, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563105215,
                "cdate": 1700563105215,
                "tmdate": 1700563105215,
                "mdate": 1700563105215,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RGrTvNDh0e",
            "forum": "wOSYMHfENq",
            "replyto": "wOSYMHfENq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_Zja7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_Zja7"
            ],
            "content": {
                "summary": {
                    "value": "They provide explanation for the normalization by proving that training normalization layers alone is already sufficient for universal function approximation if the number of available, potentially random features matches or exceeds the weight parameters of the target networks that can be expressed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- The effectiveness has been well supported by experiments.\n\n2- Well organized and clearly written.\n\n3- The paper is appropriately placed into contemporary literature."
                },
                "weaknesses": {
                    "value": "I read the whole paper excluding the appendix, I acknowledge the importance of their study and appreciate the detailed information."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1322/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1322/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1322/Reviewer_Zja7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1322/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886969880,
            "cdate": 1698886969880,
            "tmdate": 1699636059419,
            "mdate": 1699636059419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cc3KSEk3Ox",
                "forum": "wOSYMHfENq",
                "replyto": "RGrTvNDh0e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive assessment of our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562583842,
                "cdate": 1700562583842,
                "tmdate": 1700562583842,
                "mdate": 1700562583842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bkCpJG32R5",
            "forum": "wOSYMHfENq",
            "replyto": "wOSYMHfENq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_DqFm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1322/Reviewer_DqFm"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a proof that a convolutional multi-layer perceptron with batch normalization is an universal function approximator even when only the batchnorm parameters are trainable and everything else is fixed to its random initialization.\nThe claim is proved by providing a practical construction for a number of common activation functions and parameter random distributions.\n\nMinimum model widths and depths are provided for the task of reproducing an arbitrary convolutional MLP.\n\nThe approach is validated by experiments on image classification of the CIFAR10 and CIFAR100 datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Interesting theoretical contribution."
                },
                "weaknesses": {
                    "value": "Unclear practical relevance. The contribution is marginal compared to known results about approximation with models with random fixed parameters."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1322/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698958503753,
            "cdate": 1698958503753,
            "tmdate": 1699636059322,
            "mdate": 1699636059322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N2LZkCV2sS",
                "forum": "wOSYMHfENq",
                "replyto": "bkCpJG32R5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1322/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Highlighting practical relevance and theoretical significance"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and effort they put into their review. To address their concern, in the following, we highlight the practical relevance and theoretical significance of our contributions.\n\n**Practical relevance:**\n\n- The practical relevance of our work is based on insights that can guide the design of neural network architectures, in which BN parameters contribute significantly to the expressiveness of the neural networks (like in WideResNets).\n\n- Concretely, we have derived architectural constraints and make specific proposals how training only BN paramters can achieve the same performance as training a target network. \nThis way, we could outperform previous experiments with architectures, where only BN parameters were trained [1].\n\n- We have proven rigorously that a higher depth can compensate for a too narrow width. This is a relevant insight, in particular, since the random networks that were analyzed empirically were not wide enough.\n\n- Our results imply that training only BN parameters for computational savings is not practically relevant if we want to maintain high expressiveness. Only inductive bias in the weight distributions (e.g. as obtained by foundation models) could enable training success despite a low number of degrees of freedom (see Lemma 2.1).\nNote that this insight was not well supported even for fully-connected networks because not all trainable parameters were actually utilized in the construction [2].\n\n**Significance of theoretical contributions:**\n\n- Convolutional layers are an important building block of many contemporary neural network architectures. Therefore, we have to understand their intricacies if we want to gain theoretical insights that are of practical relevance and, up to our knowledge, we are the first to do so in the context of training only training only normalization parameters. \n(Note that our insights actually allow us to outperform previous purely empirical work on that matter and explain their findings.) \n- Convolutional architectures are challenging to analyze theoretically in our context, as the composition of multiple filters induces non-trivial equations that are quadratic (and not linear) in the trainable BN parameters. \nFurthermore, the composition of convolutional source filters has to overlap with a target filter, which induces additional constraints that do not occur in fully-connected layers.\n- Even in comparison to previous work on fully-connected layers [2], we present an improved bound on the source width, which utilizes all trainable parameters (which requires solving a quadratic instead of a linear system of equations).\nNote that this improved bound is crucial for an important insight into the limitations of the overall approach. \nIf we want to maintain full expressiveness, we cannot use fewer parameters.\nThis insight is not fully supported by previous work [2], which does not utilize all trainable parameters.\n- Furthermore, we cover almost arbitrary activation functions and not only linear ones or ReLUs.\n- Conceptually, most challenging and novel is the proof of Theorem 2.5 that solves the depth versus width trade-off in the source network.\n\n[1] Frankle, Schwab, Morcos. Training BatchNorm and only BatchNorm: On the expressive power of random features in CNNs. ICLR, 2021.\n\n[2] Giannou, Rajput, Papailiopoulos. The expressive power of tuning only the normalization layers. COLT, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1322/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562500010,
                "cdate": 1700562500010,
                "tmdate": 1700562500010,
                "mdate": 1700562500010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]