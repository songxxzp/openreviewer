[
    {
        "title": "Multi-Armed Bandits with Abstention"
    },
    {
        "review": {
            "id": "kaT98fke4U",
            "forum": "U5BZcr0H7r",
            "replyto": "U5BZcr0H7r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_Yyub"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_Yyub"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a multi-armed bandit setting which features abstention, meaning that after selecting an arm, the agent has the option to suffer a fixed regret c (called fixed-regret setting) or to gain a fixed reward c (called fixed-reward setting). For each abstention model, authors propose lower bounds, an algorithm tackling the corresponding regret minimization problem, and its associated upper bounds, both in terms of asymptotic and minimax bounds."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality : Algorithm 2, which allows to turn any \u201ccanonical\u201d bandit algorithm into an abstention-featuring algorithm, is original while simple to implement.\n- Quality : I did not check the proofs in Appendix in detail, but the results seem consistent (and good) and the outlines of the proof logical. The baselines and the different experimental settings considered in the experimental study and the Appendix make sense and are convincing.\n- Clarity : I think the paper is very well-written, and distinguishes clearly the different settings, and types of theoretical results. The remarks are relevant and interesting.\n- Significance : The topic of assessing whether adding abstention to an online learning allows to improve the performance of the agent is theoretically interesting."
                },
                "weaknesses": {
                    "value": "- Significance : In terms of real-life applications (clinical trials), I am not totally convinced by the considered abstention model(s). In both fixed-regret and fixed-reward settings, the fact that the agent gets access to the stochastic reward no matter the value of Bt does not seem realistic, as it implies that there is some estimation of the reward/regret from pulling a treatment arm (referring to paragraph 3 on page 1). Please correct me if I am wrong."
                },
                "questions": {
                    "value": "- In the experimental sections (both in the main paper for the fixed-regret setting and in the Appendix for the fixed-reward setting), in the plots at fixed T=10,000, perhaps it would be fairer to report the lines corresponding to the regret incurred by the baselines at T=10,000 in each setting, as the baselines seem to fare (slightly) better than the proposed algorithms for lower values of c (in the fixed-reward setting: ex. c=0.5 in the first instance) and for greater values of c (in the fixed-regret setting: ex: c=0.4 in the first instance) than shown in the empirical regret curves. \n- epsilon(=1/K)-TS is asymptotically and minimax optimal for other one-dimensional exponential families (Poisson, Gamma, etc.). Since your algorithmic contributions partially rely on the analysis of epsilon-TS, how hard would it be to extend your results beyond Gaussian distributions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Reviewer_Yyub"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698078734037,
            "cdate": 1698078734037,
            "tmdate": 1699636168213,
            "mdate": 1699636168213,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3G7cBGQTf2",
                "forum": "U5BZcr0H7r",
                "replyto": "kaT98fke4U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the favorable assessment of our paper and for providing us with constructive feedback. We would like to respond to the questions in the following.\n\n--- \n\n**Weakness (Significance):** \n\nWe do not fully understand concerns raised by the reviewer, and we welcome any additional comments. In particular, we do not fully understand the phrase \"no matter the value of $B_t$ does not seem realistic, as it implies that there is some estimation of the reward/regret from pulling a treatment arm\". The agent indeed observes the stochastic outcome $X_t$ after selecting an arm $A_t \\in [K] $ and the abstention option $B_t\\in \\\\{0,1\\\\}$.   In the example of clinical trials presented in the introduction, if researchers implement costly prearranged measures, the treatment outcomes will not contribute to the optimization objective of the study. However, the researcher can still observe these outcomes, thereby leading to more efficient future experimental designs. If the researcher is unable to observe the outcomes, then conducting medical treatments becomes meaningless. In line with this model assumption, our model well aligns with Neu \\& Zhivotovskiy (2020), which explored the role of abstention in the context of online prediction with expert advice.  \n\nThe abstention decision $B_t$ must be made before observing the outcome $X_t$. Consequently in each time step, the agent utilizes  estimates of the reward/regret from pulling the chosen arm to make the abstention decision. \n\n> Gergely Neu and Nikita Zhivotovskiy. Fast rates for online prediction with abstention. In Conference on Learning Theory, pp. 3030\u20133048. PMLR, 2020.\n\n--- \n\n**Question 1 (Experiments):** \n\n\nWe extend our sincere appreciation to the reviewer for offering constructive suggestions. In the revised manuscript, we have presented the performance of baselines in Figures 3 and 5, and provided a concise summary of the subsequent explanation. When the abstention reward $c$ is lower than the smallest mean reward in the fixed-reward setting, or the abstention regret $c$ exceeds the largest suboptimality gap in the fixed-regret setting, the agent gains no advantage in choosing the abstention option when selecting any arm. Unfortunately, the agent lacks precise knowledge of the bandit instance, leading to the inevitable selection of the abstention option. This unavoidable choice introduces a subtle gap between the performance of our algorithms and the baselines.\n\n--- \n\n**Question 2 (Extension beyond Gaussian distributions):** \n\nEpsilon(=1/K)-TS is asymptotically and minimax optimal for various popular reward distributions, including Gaussian, Bernoulli, Poisson, and Gamma. The extension beyond Gaussian distributions appears to be straightforward based on our current analysis, although careful consideration is necessary for specific \"boundary\" cases. For instance, in Bernoulli bandits where the stochastic rewards are bounded in $[0, 1]$, the behavior of minimax regrets might be different from the canonical case of $O(\\sqrt{KT})$, when the abstention reward approaches 1 in the fixed-reward setting.\n\n---\n\nThank you once more for providing us with your valuable feedback. We sincerely hope that our responses effectively address the raised concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946600134,
                "cdate": 1699946600134,
                "tmdate": 1699946600134,
                "mdate": 1699946600134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4Obor6hDTS",
                "forum": "U5BZcr0H7r",
                "replyto": "3G7cBGQTf2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_Yyub"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_Yyub"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for addressing my concerns. I will keep the positive score as it is.\n\n**Weakness (Significance):** I agree that choosing the abstention option should still provide some quantity that enables learning in the theoretical framework -otherwise, the answer to whether having an abstention option improves learning is straightforward. My comment was instead focusing on applying this framework to clinical trials, and in particular on the nature of the \"costly prearranged measures\" which allow the observation of the output from the *same* stochastic process without having to pay the cost of \"classical\" testing. I don't see how it translates to real life, so I believe that the practical application of the abstention model is unrealistic (or the model assumptions should be relaxed to match a more realistic setting). My comment is merely a counter-argument to the claimed real-life application (an argument which does not remove any of the theoretical value of this work).\n\n**Question 1 (Experiments):** The explanation makes sense, and it is good that a sentence in the main text now refers to this."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064991450,
                "cdate": 1700064991450,
                "tmdate": 1700064991450,
                "mdate": 1700064991450,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JBFPpXsSoq",
            "forum": "U5BZcr0H7r",
            "replyto": "U5BZcr0H7r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a multi-armed bandits problem with the choice of abstention. Specifically, after choosing an arm to pull, the algorithm can choose to abstain. In this case, it will observe the random reward, but not receive it. Instead, its reward at this time is a fixed value $c$ (fixed reward setting), or its regret at this time is a fixed value $c$ (fixed regret setting). In both cases, the authors provide algorithms that achieve tight regret upper bounds. They also use experiments to demonstrate the effectiveness of their algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proofs seem to be correct, and the paper writing is good. The studied problem is interesting."
                },
                "weaknesses": {
                    "value": "My main concern about this paper is that its contribution seems limited. Though the problem setting is interesting, the algorithms (as well as the analysis) seem straightforward, and I do not see novel techniques or insights in it. Besides, no proof sketch is provided in the main text, and there is also no discussion about this paper's theoretical novelty.\n\nI also have concerns about the motivation, e.g., in the example of clinical trials (in the section of the introduction), what is the reason that we do not count the random outcome in reward in the case that we can observe it? In my opinion, the abstention is like that we can pay an extra cost to avoid catastrophic outcomes. This seems more natural than the fixed regret setting or the fixed reward setting. \n\nMy next question is why we apply a Thompson Sampling based algorithm but not a UCB-based algorithm? Would a UCB-based algorithm work in this setting? What's its performance in experiments?"
                },
                "questions": {
                    "value": "See above \"Weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698253206813,
            "cdate": 1698253206813,
            "tmdate": 1700557473838,
            "mdate": 1700557473838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8axVCnLJia",
                "forum": "U5BZcr0H7r",
                "replyto": "JBFPpXsSoq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 of Response to Reviewer 4Y9t"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for providing insightful comments and valuable suggestions on our paper. In response, we have conscientiously attended to all the raised concerns.\n\n--- \n\n**Weakness 1 (Theoretical contribution):** \n\nRegarding the theoretical contribution of our paper, while we submit that the individual ingredients that constitute the algorithms and their analysis are not groundbreaking in the bandit literature, the **adaptation** to  being amenable to incorporating the abstention option (which is a novel setting in the bandit literature) and retaining the asymptotic and minimax optimality of the resultant algorithms is far from straightforward, as acknowledged by both Reviewers n2KC and ahVb. \n\n\nTurning to the significance of our work, our findings offer valuable quantitative insights into the advantages of the abstention option, thereby laying the groundwork for further exploration in other online decision-making problems with such an option. This positive aspect is also recognized by all three other reviewers. Specifically, we have extensively discussed the rationale and insights behind the asymptotic and minimax regret within the main text, offering potential inspiration for addressing analogous challenges in other online learning problems involving abstention.\n\nDue to the density in exposition and comprehensiveness of our results, we struggled with space limitations, which prevent us from incorporating proof sketches in the main text. Nevertheless, we are confident that the proofs presented in the appendix are lucid and easily understandable. For the more intricate fixed-regret setting, we delve into the theoretical challenges associated with designing and analyzing the algorithm in Remark 2, which are decidedly non-trivial and demands meticulous effort.\n\n\n--- \n\n**Weakness 2 (Motivation):** \n\nIn the example of clinical trials presented in the introduction, if researchers incorporate costly prearranged measures such as insurance packages, the stochastic treatment outcomes, whether favorable or unfavorable, will not contribute to the optimization objective of the study. Nevertheless, these outcomes remain observable to the researcher and can be utilized to make more effective future experimental designs. If the researcher is unable to observe the outcomes, then conducting medical treatments becomes meaningless. \n\nWe do not fully comprehend the meaning of \"avoiding catastrophic outcomes\", and would appreciate it if the reviewer could provide further comments so that our response is better tailored to the reviewer. In our model, the abstention option is indeed present to mitigate catastrophic outcomes, as the algorithm(s) can utilize the abstention option if it finds that the reward accrued from the chosen arm at a particular time is unacceptably low. Furthermore, the decision to abstain does not influence the stochastic outcome from the selected arm. Its impact is confined to the instantaneous regret at a specific time step, which is modeled as either fixed regrets or fixed rewards. In both the fixed-regret and fixed-reward settings, the instantaneous regret does not depend on the random outcome when the agent chooses to abstain. Exploring a more sophisticated approach to model the cost associated with the abstention option will be an intriguing avenue for future research.\n\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946429170,
                "cdate": 1699946429170,
                "tmdate": 1699946429170,
                "mdate": 1699946429170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jNOQgg8nTl",
                "forum": "U5BZcr0H7r",
                "replyto": "JBFPpXsSoq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2 of Response to Reviewer 4Y9t"
                    },
                    "comment": {
                        "value": "**Weakness 3 (Selection of the base algorithm):** \n\n\nIn the fixed-reward setting, our algorithm FRW-ALGwA can leverage any base algorithm that is asymptotically and minimax optimal for canonical multi-armed bandits as its input. In Appendix A, we provide comprehensive definitions of asymptotic and minimax optimality within the canonical model. Eligible candidate base algorithms include both Thompson Sampling-based and UCB-based approaches. Due to space limitations, we defer all experimental results pertaining to the fixed-reward setting to Appendix E. In this appendix, we examine the empirical performances of two particular realizations of our algorithm: FRW-TSwA and FRW-UCBwA. The former uses Less-Exploring Thompson Sampling (Jin et al., 2023) as its base algorithm, while the latter employs KL-UCB++ (Menard \\& Garivier, 2017). Notably, while both FRW-TSwA and FRW-UCBwA represent implementations of our general algorithm and share identical theoretical guarantees, FRW-TSwA demonstrates superior empirical performance, as illustrated in Figures 4 and 5.\n\nIn the fixed-regret setting, the sampling rule of our algorithm FRG-TSwA is built upon Less-Exploring Thompson Sampling. The fixed-reward setting is inherently more complex than its fixed-regret counterpart because, in the latter setting, we need to dynamically estimate the suboptimality gaps and quantify the regret that results from inaccurately estimating the gaps. These complexities necessitate a more meticulous exploration into the arm sampling dynamics, and preclude us from formulating a generalized strategy. To achieve both forms of optimality, we suspect that the sampling rule should be *anytime* (i.e., not dependent on the time horizon $T$), a criterion not met by current UCB-based approaches. Nevertheless, we do believe it is possible to design a UCB-based algorithm for our abstention model. Finally, regarding the empirical performance, our focus lies on algorithms with solid theoretical guarantees.\n\n> Pierre Menard and Aurelien Garivier. A minimax and asymptotically optimal algorithm for stochastic bandits. In International Conference on Algorithmic Learning Theory, pp. 223\u2013237. PMLR, 2017.\n\n\n---\n\nThanks again for your valuable review. Hope these replies resolve your concerns, and any further comments are welcome!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946496596,
                "cdate": 1699946496596,
                "tmdate": 1699946496596,
                "mdate": 1699946496596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uONiMyHEoU",
                "forum": "U5BZcr0H7r",
                "replyto": "JBFPpXsSoq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 4Y9t,\n\nWe hope this message finds you well. As the rebuttal period draws to a close on November 22nd, just four days away, we would like to reach out to you and inquire if you have any additional suggestions regarding our paper. We would be immensely grateful if you could kindly review our responses to your comments. This would allow us to address any further questions or concerns you may have before the rebuttal period concludes.\n\nWe sincerely appreciate the time and effort you have put into reviewing our work and providing valuable feedback. Thank you for your contributions towards improving the quality of our research.\n\nBest regards,\n\nAuthors of Submission2358"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379988459,
                "cdate": 1700379988459,
                "tmdate": 1700379988459,
                "mdate": 1700379988459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BnSpsZsvD2",
                "forum": "U5BZcr0H7r",
                "replyto": "uONiMyHEoU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "About W1. I read all the proofs in appendix. They are indeed easily understandable. However, I still do not understand the novelty of the analysis. All the analysis seems straightforward. Could you please explain this part in detail? What are the challenges and why they are challenges?\n\nAbout W2. I really encourage the authors to try some more complicated settings. Including abstention is an interesting approach, but the current settings seem too simple.\n\nAbout W3. Can you be more specific? Where should we use the property of anytime sampling rule? I guess it is Lemma 5, but I think such results can be easy to get in UCB-based algorithms as well."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470636745,
                "cdate": 1700470636745,
                "tmdate": 1700470636745,
                "mdate": 1700470636745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JWo4xspdta",
                "forum": "U5BZcr0H7r",
                "replyto": "Hd6z6nuCs5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_4Y9t"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "For W1. If the algorithm is carefully designed so that it is both asymptotically optimal and minimax optimal (e.g., using LCB of the other arms as well as the empirical mean of the pulled arm when deciding whether to abstain), I suggest the authors to include some detailed explanation about this in the main text. Otherwise it is hard for readers to understand why the task is challenging. \n\n======================\n\nThanks for your rebuttal and all the replies. My score is increased to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557452407,
                "cdate": 1700557452407,
                "tmdate": 1700557452407,
                "mdate": 1700557452407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g7hoeUKWF0",
            "forum": "U5BZcr0H7r",
            "replyto": "U5BZcr0H7r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_ahVb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_ahVb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel extension of the standard MAB problem to include the option of abstention while pulling an arm: at each arm pull, the agent has the option to abstain from obtaining a stochastic reward and instead accept one of the following 2 options: (a) suffer a fixed regret, or (b) gain a guaranteed reward. For both options, the authors designed and analyzed algorithms whose regrets match the  minimax-optimal and asymptotically optimal lower bounds. Numerical simulations are also provided to corroborate the theoretical results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality\n----------------------------------------------------------------------------------------------------------------\n- The algorithms and analysis are non-trivial extensions of the known methods in the MAB literature\n\nQuality\n------------------------------------------------------------------------------------------------------------------\n- The analysis seems to be correct\n\nClarity\n-----------------------------------------------------------------------------------------------------------------\n- The paper is well-written and easy to follow\n- The algorithms and analysis are clearly explained\n- Great intuition provided for the behavior of the algorithms and their analysis\n\nSignificance\n------------------------------------------------------------------------------------------------------------------\n- Incorporating the abstention into the standard MAB model lays the groundwork for understanding complex real-world online learning systems (as evident from the motivating example of clinical trials in the paper)"
                },
                "weaknesses": {
                    "value": "- Insufficient numerical experiments: While the authors have carefully chosen the arm means $\\mu^{\\dagger}$ and $\\mu^{\\ddagger}$ for the experiments, I would like to see the numerical experiments for arbitrary values of arm means and large number of arms.\n- \"Gaussian\" is spelt incorrectly in Lemmas 3 and 4 in Appendix B (independent $\\sigma$-sub-\"Guassian\")\n- While proving $(\\star)_{i}$ in the asymptotic upper bound for the fixed regret setting (Appendix C), $\\beta(b, \\mu, K)$ isn't defined until Lemma 5.\n- It will be great if the authors can provide a proof sketch of Lemma 5 (instead of referring the readers to Korda et. al. (2013)) in the appendix for the sake of completeness."
                },
                "questions": {
                    "value": "Please see the Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2358/Reviewer_ahVb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646803706,
            "cdate": 1698646803706,
            "tmdate": 1699636168025,
            "mdate": 1699636168025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kk8qFQQXeZ",
                "forum": "U5BZcr0H7r",
                "replyto": "g7hoeUKWF0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback, valuable comments, and suggestions on our paper. Please find below our response to the  raised concerns.\n\n--- \n\n**Weakness 1 (Insufficient numerical experiments):** \n\nOur paper is primarily positioned as a theoretical work, with numerical experiments serving to validate our theoretical findings. The selection of instances $\\mu^{\\dagger}$ and $\\mu^{\\ddagger}$ was not deliberate. In Appendix E.2 of the revised manuscript, we have heeded the reviewer's advice and incorporated additional numerical experiments, using random instances with large numbers of arms. The construction of these random instances follows the method in Jin et al. (2023). Due to time constraints and for the sake of simplicity in presentation, we focus on the fixed-regret setting. It can be seen that the results in Figures 6 and 7 closely resemble those in Figures 2 and 3.\n\n\n---\n\n**Weaknesses 2 and 3:** \n\nThank you for bringing the typo and the presentation issue to our attention. In the revision, we have corrected the typo and explicitly clarified that $\\beta(b, \\mu, K)$ is defined in Lemma 5.\n\n\n--- \n\n**Weakness 4 (Proof Sketch of Lemma 5):** \n\nThank you for the constructive suggestion. In response, we have incorporated a proof sketch for Lemma 5 in the revised version. It is highlighted in red within the appendix, focusing on the core ideas."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946221899,
                "cdate": 1699946221899,
                "tmdate": 1699946221899,
                "mdate": 1699946221899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9cdD98wxfJ",
                "forum": "U5BZcr0H7r",
                "replyto": "Kk8qFQQXeZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_ahVb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_ahVb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing the comments in the Weaknesses section. I am satisfied with the responses and keeping my positive score. It would be great if the authors could also provide the numerical results in the fixed reward setting for arbitrary values of arm means and large number of arms in the revised version of their submission."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202805305,
                "cdate": 1700202805305,
                "tmdate": 1700202805305,
                "mdate": 1700202805305,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qLyoAJykyu",
            "forum": "U5BZcr0H7r",
            "replyto": "U5BZcr0H7r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_n2KC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2358/Reviewer_n2KC"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new stochastic multi-armed bandit setting, where instead of just having the option of pulling one of the K arms in each round the learner also has the option to abstain. The authors consider two models: one where abstaining incurs a fixed known reward to the agent and one where abstaining incurs a fixed known regret to the learner. Importantly, in every round the agent specifies both which arm to pull and whether to abstain or not. Even if the agent chooses to abstain, they sill observe a sample of the reward of the arm chosen from the true distribution of the rewards of the arm. \n\nIn the fixed regret setting, the authors provide an algorithm that achieves both a minimax optimal and an asymptotically optimal regret bound. Their algorithm builds upon a variant of Thompson sampling by Jin et al. '23, where a natural criterion is added that determines whether the learner should abstain or not. In order to show the optimality of their algorithm the authors extend well-known lower bounds from the stochastic multi-armed bandit literature to the setting they consider.\n\nSimilarly, in the fixed reward abstention setting the authors provide a transformation from algorithms that perform well in the traditional multi-armed bandit setting to algorithms that perform well in the setting that the paper considers. In particular, the authors show various instantiations of these transformations that obtain optimal minimax and asymptotic regret bound. In order to establish the optimality of their upper bounds, the authors prove matching lower bounds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Learning with the option of abstention is an interesting line of work that the learning theory community is excited about. To the best of my knowledge, this is the first paper that considers this problem in the context of multi-armed bandits.\n\n- The algorithms that are provided are clean and elegant. \n\n- The result are interesting, and even though the techniques are a natural adaptation of known results to the abstention setting, the analysis is not entirely straightforward.\n\n- The authors present their results in a clear manner, without overselling their contributions."
                },
                "weaknesses": {
                    "value": "- I think the main weakness has to do with the abstention model that the authors consider. While both having an option that gives a fixed reward or incurs a fixed regret to the learner seem natural to me, being able to observe the sample from the arm that was chosen when the agent chooses to abstain feels a bit too strong. I think the authors need to elaborate a bit on this assumption they have made.\n\n\nNot weaknesses, but I'm writing some small issues that could be improved.\n\n- It would be nice if the authors could give longer sketches of the proofs in the main body, even though I understand that there isn't enough space. One suggestion would be to move all the experiments to the appendix, since this is mostly a theoretical work and the algorithms are straightforward to implement I don't see what value the experiments add to the paper.\n\n- Since the fixed reward setting is easier to analyze it might make sense to change the order of the presentation between sections 3, 4."
                },
                "questions": {
                    "value": "1) Please see the main weakness.\n\n2) If we assume that when the learner abstains they don't observe the sample from the arm they chose, what is the regret bound they get?\n\n3) Is there a way to state the regret bound of algorithm 2 as a function of the regret of the base algorithm? Since this is a black-box transformation I would expect to see this type of result."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2358/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804187387,
            "cdate": 1698804187387,
            "tmdate": 1699636167940,
            "mdate": 1699636167940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XPrBJpQibU",
                "forum": "U5BZcr0H7r",
                "replyto": "qLyoAJykyu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions on our paper. We have carefully addressed your concerns in the following.\n\n\n--- \n\n**Weakness (The abstention model):**\n\n\nWe consider the assumption that the agent can observe the sample regardless of the abstention decision to be rather natural to our problem setting of bandits with abstentions and in particular to the clinical trials example presented in the introduction. In the example of clinical trials, if researchers implement costly prearranged measures, the treatment outcomes will not contribute to the optimization objective of the study. However, the researcher can still observe these outcomes, thereby leading to more efficient future experimental designs. Furthermore, conducting medical treatments becomes meaningless if the researcher is unable to observe the outcomes. In line with this model assumption, our model well aligns with that of Neu \\& Zhivotovskiy (2020), which explored the role of abstention in the context of online prediction with expert advice.\n\n\nRegarding the learnability of the abstention bandit model, if the agent cannot observe the sample from the chosen arm when opting for abstention, then it is equivalent to skipping the time step, rendering the model trivial. In other words, the learner gains no information from the time step but incurs an instantaneous regret.  Finally, we would like to remark that the above statement holds true only for the stochastic bandit model. The dynamics change in non-stationary or adversarial bandits, providing intriguing avenues for future research.\n\n> Gergely Neu and Nikita Zhivotovskiy. Fast rates for online prediction with abstention. In Conference on Learning Theory, pp. 3030\u20133048. PMLR, 2020.\n\n\n---\n\n**Small Issue 1 (Longer proof sketches):**\n\nWe sincerely thank you for your valuable suggestion and understanding. Due to the density of our results, we, as with many other authors, struggled with space limitations, which prohibits us from incorporating comprehensive proof sketches in the main text. Our paper is positioned as a theoretical work, with numerical experiments serving to validate our theoretical findings. Opinions on the inclusion of experiments in the main text vary, and one reviewer suggests more numerical experiments. In light of this, we have chosen not to relocate all experiments to the appendix. According to the ICLR guidelines, the page limit for the main text is strict, and no additional pages can be added in the final version. Consequently, we are unable to include longer proof sketches in the main body. However, we are confident that the proofs presented in the appendix are lucid and easily understandable.\n\n\n\n\n---\n\n**Small Issue 2 (Order of the presentation):**\n\nWe express our gratitude to the reviewer for providing the constructive suggestion. While preparing the manuscript, we ensured clarity by presenting comprehensive explanations in the first setting and avoiding unnecessary repetitions in the second setting. In particular, in the corresponding proofs, we typically streamlined the common components. Although we appreciate the reviewer's kind suggestion, when preparing the initial manuscript, we have thought through whether to present the fixed-regret or fixed-reward settings first, aiming to ensure that the overall paper is as concise as possible.\n\n\n---\n\n**Question 2 (Regret bound for the new model):**\n\nThis point pertains to the point of weakness above. In the context of stochastic multi-armed bandits, if the agent cannot observe the sample from the chosen arm when opting for abstention, then no information is gained during that time step, rendering it equivalent to skipping the time step altogether. Consequently, the abstention option becomes a means to shorten the time horizon with costs. Regarding the cumulative regret bound during time steps when the agent chooses not to abstain, it is the same as that of the canonical bandit model. Hence, there is nothing to analyze in this setting. \n\n---\n\n**Question 3 (Regret bound of Algorithm 2):**\n\nThank you for the question. Since the base algorithm is asymptotically optimal (see Appendix A for relevant definitions), our model that possesses the abstention option and leverages the base algorithm, *inherits the asymptotic optimality of the base algorithm*. Hence, our regret bound is indeed a function of that of the base algorithm but in a rather unusual way; both of them are asymptotically optimal. \nConcerning the minimax upper bound, we indeed possess a result of that type, specifically expressed as a function of the minimax upper bound of the base algorithm. For further details, please refer to the concluding lines of the proof of Theorem 3 in Appendix D.1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946125112,
                "cdate": 1699946125112,
                "tmdate": 1699946125112,
                "mdate": 1699946125112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e9iAmYhDkC",
                "forum": "U5BZcr0H7r",
                "replyto": "qLyoAJykyu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer n2KC,\n\nWe hope this message finds you well. As the rebuttal period draws to a close on November 22nd, just four days away, we would like to reach out to you and inquire if you have any additional suggestions regarding our paper. We would be immensely grateful if you could kindly review our responses to your comments. This would allow us to address any further questions or concerns you may have before the rebuttal period concludes.\n\nWe sincerely appreciate the time and effort you have put into reviewing our work and providing valuable feedback. Thank you for your contributions towards improving the quality of our research.\n\nBest regards,\n\nAuthors of Submission2358"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379955438,
                "cdate": 1700379955438,
                "tmdate": 1700379955438,
                "mdate": 1700379955438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BHmSSx9Ehp",
                "forum": "U5BZcr0H7r",
                "replyto": "e9iAmYhDkC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_n2KC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2358/Reviewer_n2KC"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for carefully addressing my comments. I do not have any further questions and I remain positive about the paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2358/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679550276,
                "cdate": 1700679550276,
                "tmdate": 1700679550276,
                "mdate": 1700679550276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]