[
    {
        "title": "Put on your detective hat: What\u2019s wrong in this video?"
    },
    {
        "review": {
            "id": "YKwHTmCUeg",
            "forum": "Uj2Wjv0pMY",
            "replyto": "Uj2Wjv0pMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_tDxh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_tDxh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new dataset for error recognition in procedure videos. Table 1 compares this new dataset to existing datasets. The process of data collection is described. Baselines are provided for the two tasks of \"Error Recognition\" and \"Multi Step Localization\"."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well presented: (1) the motivation is clear (2) the data collection process is clear.\n\nThe paper provides baseline methods for the proposed two tasks: (1) error recognition (2) multi-step localization."
                },
                "weaknesses": {
                    "value": "1. The additional benefit of this new dataset is unclear. Looking at Table 1, the added benefit compared to existing work \"Assembly 101\" seems to be limited to adding \"depth\" channel. However, it is unclear whether the \"depth\" channel is really useful in the context of error recognition. The authors need to provide more evidence on why this new dataset is significantly more useful than \"Assembly 101\".\n\n2. The collected sensor data such as \"depth\", \"camera trajectory\", and \"hand joints\" are not used or analyzed by the baselines provided.\n\n3. The AUC score between Table 2 (Error Recognition) & Table 3 (Early Error Prediction) looks surprisingly similar. Shouldn't the task of \"Early Error Prediction\" be much harder than the typical \"Error Recognition\" task?"
                },
                "questions": {
                    "value": "1. In Figure 4, Table 2-3, can you highlight the best performing model?\n\n2. It's better to present the the distribution of different error categories in the main paper, e.g., Figure 16-17?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6873/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698452266380,
            "cdate": 1698452266380,
            "tmdate": 1699636798944,
            "mdate": 1699636798944,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5SriHvZvUg",
                "forum": "Uj2Wjv0pMY",
                "replyto": "YKwHTmCUeg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer tDxh"
                    },
                    "comment": {
                        "value": "> The additional benefit of this new dataset is unclear.\n\nFirstly, Error recognition as a task is quite challenging, and we conjecture that it requires the development of techniques that understand the context, meaning, and cause of various errors. \n\nOur dataset is different from Assembly 101 in the following ways:\n\n- **Domain:** We stress the fact that Assembly 101 explores the domain of Assembly and Disassembly tasks, and we focus on Cooking activities. \n   - In assembly, the majority of the errors performed can be corrected, but cooking as a domain includes activities which do not have corrective actions. \n   - Unlike assembly, where shapes and colours of the objects remain fixed during the whole procedure. Cooking involves continuous changes in the shape and colour of ingredients.\n- **Environment:** Assembly 101 is a dataset that is collected in a lab environment, and we collect our dataset in real kitchen environments.\n- **Modalities:** As we primarily employ HoloLens2 in capturing the data, we leverage the wealth of information it enables to capture. For example, our synchronized dataset enables the complete reconstruction of a 3D map of the environment while the activity is being performed.\n\nIn conclusion, we emphasize that the category of errors we explore is different from the category of errors explored by activities in the assembly domain. \n\n---\n\n> The collected sensor data such as \"depth\", \"camera trajectory\", and \"hand joints\" are not used or analyzed by the baselines provided.\n\nWe believe that the task of error recognition is a challenging task and requires complete awareness of the 3D environment. So, we employed Hololens2 to capture the data. In the current version of the manuscript, we do not have results on incorporating data captured from other modalities into the proposed tasks. We shall add the results incorporating data captured from all the modalities in future versions of this manuscript. \n\n---\n\n> The AUC score between Table 2 (Error Recognition)\n\nNotice that although the AUC scores are roughly the same, the F1 scores are always lower because recall is low.\n\n---\n\n> In Figure 4, Table 2-3, can you highlight the best-performing model?\n\nSure, we have updated the manuscript following your suggestion\n\n---\n\n> It's better to present the distribution of different error categories in the main paper, e.g., Figure 16-17.\n\nSure, we have updated the manuscript following your suggestion"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6873/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707235013,
                "cdate": 1700707235013,
                "tmdate": 1700707235013,
                "mdate": 1700707235013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RIxRVwtqr5",
            "forum": "Uj2Wjv0pMY",
            "replyto": "Uj2Wjv0pMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_bmra"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_bmra"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an Egocentric 4D (RGBD) dataset, for understanding step by step procedural activities and the error recognition. The dataset intends to tackle the problem of fine-grained error detection in activities, also anticipating them. The end goal is to mimic activities error in various activities like medical operation or chemical operation. The dataset consists of kitchen recipe videos (384 videos) for 24 tasks, by 8 different actors. The annotation of the datasets ranges from coarse-grained: correct vs error (binary classification) to fine-grained errors on various time stamps of error. The paper also proposes 3 baseline performance metrics : binary error classification, multi-step localization (TAL) and procedural learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tPaper is very written in simple terms, easy to follow language.\n\n\u2022\tProposed errors like technique error, and measurement error will test models fine-grained understanding of activities\n\n\u2022\tThe work is definitely a constructive work towards Coarse grained & Fine grained errors understanding problem, which definitely seems like relatively new field.\n\n\u2022\tExtensive fine-grained annotation would greatly benefit future works. \n\n\u2022\tHigher ratio of error to normal videos, establishes superiority of the work."
                },
                "weaknesses": {
                    "value": "\u2022\tCompared to Assembly 101 (error detection), the paper seems like an inferior / less complicated dataset. Claims like higher ratio of error to normal videos needs to be validated. \n\n\u2022\tCompared to datasets, the dataset prides itself on adding different modalities especially depth channel (RGB-D). The paper fails to validate the necessities of such modality. One crucial different between assembly dataset is use of depth values. What role does it play in training baseline models? Does it boost the model\u2019s performance if these weren\u2019t present. In current deep learning area, depth channels should be reasonably be producible via the help of existing models. \n\n\u2022\tI\u2019m not convinced that the binary classification is a justifiable baseline metrics. While I agree with the TAL task is really important here and a good problem to solve, I\u2019m not sure how coarse grained binary classification can assess models understanding of fine-grained error like technique error. \n\n\u2022\tTiming Error (Duration of an activity) and Temperature based error, does these really need ML based solutions? In sensitive tasks, simple sensor reading can indicate error. I\u2019m not sure testing computer vision models on such tasks is justifiable. These require more heuristics-based methods, working with if-else statement. \n\n\u2022\tProcedure Learning: its very vaguely defined, mostly left unexplained and seems like an after thought. I recommend authors devote passage to methods \u201cM1 (Dwibedi et al., 2019)\u201d and \u201cM2 (Bansal, Siddhant et al., 2022)\u201d. In Table 5, value of lambda? Is not mentioned. \n\n\u2022\tThe authors are dealing with a degree of subjectivity in terms of severity of errors. It would have been greatly beneficial, if the errors could be finely measured. For example if the person uses a tablespoon instead of teaspoon, is still an error? Some errors are more grave than others, is there a weighted scores? Is there a way to measure level of deviation for each type of error or time stamp of occurrence of error. Is one recipe more difficult than the other recipe."
                },
                "questions": {
                    "value": "1.\tPlease validate claims like higher ratio of error to normal videos \n\n2.\tPlease provide the utility of depth channels in error detection tasks or provide baseline performances. \n\n3.\tI feel like binary classification section can be minimized, and instead procedural activity section needs to be emphasized and described in much more detail. \n\n4.\tShow baseline results for each type of errors. A measure of how difficult certain error would help future work. \n\n5.\tSome form of comparisons of errors needs to be there, or a severalty level of errors.\n\n---------------\n\nPost Rebuttal\n\n-------------\n\n1.\tIn terms of statistics, still not convinced if the proposed dataset is better than Assembly 101. None of the baseline models have been shown to be have any adverse impact of these proposed added difficulties : \u201cShape and color changes ingredients\u201d, \u201creal-kitchen environments\u201d. Higher ratio of error to normal videos (not validated). Reviewer I weakness-1 pointed out the same issue. \n\n2.\tAdded additional modality of Depth is still not resolved. As a reviewer, I\u2019m not sure what role depth channel plays in the error detection task. The authors have mentioned again in the rebuttal of presence of depth channel as a superiority over Assembly 101 dataset, but no proof how it\u2019s useful with the task at hand (i.e. error detection) or does Depth channel play any role in this dataset. Reviewer I weakness-1 & Reviewer 3 weakness-1 (&2) pointed out the same issue.\n\n3.\tBinary classification of the as an error detection baseline metrics is not justified. (Similar issue pointed out by Reviewer I weakness 2). I fully agree that the baselines do not provide many insights.\n\n4.\tI\u2019m not entirely convinced metrics based (Time / Temperature) errors are the right metrics to measure error detection capabilities of via Commuter Vision or Machine Learning algorithms. (Similar issue pointed out by Reviewer I weakness 2)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6873/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6873/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6873/Reviewer_bmra"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6873/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899165278,
            "cdate": 1698899165278,
            "tmdate": 1701057032536,
            "mdate": 1701057032536,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MDjX4UxuT8",
                "forum": "Uj2Wjv0pMY",
                "replyto": "RIxRVwtqr5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer bmra"
                    },
                    "comment": {
                        "value": "> Compared to Assembly 101 (error detection), the paper seems like an inferior / less complicated dataset. Claims like a higher ratio of error to normal videos need to be validated.\n\nFirstly, Error recognition as a task is quite challenging, and we conjecture that it requires the development of techniques that understand the context, meaning, and cause of various errors. Secondly, we respectfully deny the statement that this paper is an inferior dataset compared to Assembly 101. We justify the statement with the following reasons\n\n- **Domain:** We stress the fact that Assembly 101 explores the domain of Assembly and Disassembly tasks, and we focus on Cooking activities.\n  - In assembly, the majority of the errors performed can be corrected, but cooking as a domain includes activities which do not have corrective actions. \n  - Unlike assembly, where shapes and colours of the objects remain fixed during the whole procedure. Cooking involves continuous changes in the shape and colour of ingredients.\n- **Environment:** Assembly 101 is a dataset which is collected in a lab environment, and we collect our dataset in real-kitchen environments.\n- **Modalities:** As we primarily employ HoloLens2 in capturing the data, we leverage the wealth of information it enables to capture. For example, our synchronized dataset enables the complete reconstruction of a 3D map of the environment while the activity is being performed. \n\nIn conclusion, we emphasize that the category of errors we explore is different from the category of errors explored by activities in the assembly domain. \n\n---\n\n> Compared to datasets, the dataset prides itself on adding different modalities especially depth channel (RGB-D). The paper fails to validate the necessities of such modality. \n\nWe believe that the task of error recognition is a challenging task and requires complete awareness of the 3D environment. So, we employed Hololens2 to capture the data. \n\nYou are right that the current monocular depth estimation techniques are indeed quite powerful and can equal the capacity of a low-quality depth sensor. But, we leverage the wealth of information provided by Hololens2, thus enabling the creation of a less noisy 3D map of the environment.  \n\nIn the current version of the manuscript, we do not have results on incorporating depth data into the proposed tasks. We shall add the results incorporating data captured from all the modalities in future versions of this manuscript. \n\n---\n\n> I\u2019m not convinced that the binary classification is a justifiable baseline metrics\n\nWe do agree that binary classification is a preliminary baseline and conjecture that any technique developed for the task of error recognition should interpret the action, understand the context and infer the cause. \n\nAs this manuscript introduces the dataset, we followed the convention of providing a supervised baseline using transfer learning techniques for the specific tasks. \n\n---\n\n> Timing Error (Duration of an activity) and Temperature based error, does these really need ML based solutions? In sensitive tasks, simple sensor reading can indicate error. \n\nIt is an interesting question. In our experiments (see Table 5) on the proposed tasks and categorization of common errors that could occur while cooking, we believe that timing, temperature and measurement are the most challenging errors to detect.  \n\nWhenever there's a conditional transition based on a change of state involving a heating component, sensor-based measurements fall short. For example, \\textit{Saute the tofu on a pan for 3 minutes or until it turns brown}. Here, having a perception component that is capable of judging the state change to brown is necessary. \n\n---\n\n> Procedure Learning: its very vaguely defined, mostly left unexplained\n\nWe implemented your suggestion and added an extra paragraph explaining this section. Please look at the updated section on Procedure Learning.\n\n--- \n\n> The authors are dealing with a degree of subjectivity in terms of severity of errors. It would have been greatly beneficial, \n\nIn the current version of the manuscript, we flag any deviation from the procedure as an error. As stated in the above question, error recognition completely depends on the context; your example of using a tablespoon instead of a teaspoon may not be a significant error in the procedure when the step is to **stir**. However, the step involves **adding an ingredient like salt**, which completely changes the taste of the recipe, and this is indeed very significant. \n\nWe do understand that not all errors have the same impact on the procedure. It is an interesting idea to automatically infer the impact of an error on the current activity.\n\n---\n\n> Show baseline results for each type of error. A measure of how difficult certain errors would help future work.\n\nWe placed this information initially in the supplementary section of the paper and, following your suggestion, transferred it to the main paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6873/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706922915,
                "cdate": 1700706922915,
                "tmdate": 1700706970540,
                "mdate": 1700706970540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WM9MHpNkwH",
            "forum": "Uj2Wjv0pMY",
            "replyto": "Uj2Wjv0pMY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_81qP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6873/Reviewer_81qP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new dataset for Error Recognition in procedure videos. The dataset consists of 384 videos (~94.5 hours) capturing 8 subjects on 10 kitchens, while the subjects are cooking 24 different recipes. The dataset is provided with other modalities such as depth, IMU, camera trajectories, however, baseline experiments use only RGB. The paper provides 3 sets of baselines: Error Recognition (supervised and early prediction), Multi-Step Localization, and Procedure Learning. In term of dataset contribution, the proposed dataset does not provide anything significantly different from previous ones (see detailed explanation in weakness). In term of experiments, the provided baselines do not bring any interesting insights about the new datasets. Written presentation is fair, but not great, some parts are unclear."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper dedicates efforts to build a new benchmark for procedural videos, any effort in dataset building is a great contribution to the community."
                },
                "weaknesses": {
                    "value": "1. The proposed dataset has nothing standing out from previous ones. By looking at Table 1, the proposed dataset are bucked in small or medium compared with the existing ones in terms of number of videos or number of hours, and even number of tasks. The only difference may be that the proposed dataset provides more modalities. However, experiments shown in the paper only used RGB so far. Another claim made by the paper is that it has more error instances (compared by error:normal ratio) than Assembly101, which is true. However, the ways of capturing error videos have some problems: in three scenario of capturing error videos (sec 3.1.2), the first twos were scripted, the last one is instructed. Although it helps providing more error videos, however, those error videos will be intentional (the mistakes won't look realistic). In practice, the unintentional mistakes or errors are more relevant and often happened in reality.\n2. The baselines provide not much insights.\n- For Error Recognition (Table 2 & 3), the observation is Omnivore is the best backbone / embedding for error / normal video classification. The early prediction problem is still formulated as classification with partly-observed data. The same finding is confirmed (Omnivore works best for this, this brings no surprise as both are technically the same problem, the later one is a bit harder).\n- For Multi-Step Localization, the problem is formulated as supervised TAL and the same set of features are used and a ActionFormer head was use for localization. The same finding is that Omnivore works best.\n- For procedure learning: Two baselines (Dwibedi et al. 2019 and Siddhant et al. 2022) were used and provided similar performance. No real insights were observed in this experiments.\n- Since this paper is about the new dataset which is claimed to focus on error recognition, however there not much new insights about the significance of bringing more error videos to procedural video dataset: neither in the way data is captured or significant baselines, experiments to demonstrate why it matters?\n\n3. Writing is not clear in some parts\n- In 4.2, is the supervised TAL trained task-agnostically or task-specifically, meaning TAL is trained for all or per tasks (24 recipes)"
                },
                "questions": {
                    "value": "- Minor comments\nSection 4.2: what does \"refer to 16\" mean?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6873/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699052296280,
            "cdate": 1699052296280,
            "tmdate": 1699636798666,
            "mdate": 1699636798666,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ZG7LNP1m6",
                "forum": "Uj2Wjv0pMY",
                "replyto": "WM9MHpNkwH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6873/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer 81qP"
                    },
                    "comment": {
                        "value": "> The proposed dataset has nothing standing out from previous ones.\n\nFirstly, Error recognition as a task is quite challenging, and we conjecture that it requires the development of techniques that understand the context, meaning, and cause of various errors. Secondly, we respectfully question the statement the proposed dataset has nothing standing out from previous ones. We justify the statement with the following reasons\n\n- **Domain:** We stress the fact that Assembly 101 explores the domain of Assembly and Disassembly tasks, and we focus on Cooking activities. \n  - In assembly, the majority of the errors performed can be corrected, but cooking as a domain includes activities which do not have corrective actions. \n  - Unlike assembly, where shapes and colours of the objects remain fixed during the whole procedure. Cooking involves transformations in the shape or colour of ingredients.\n- **Environment:** Assembly 101 is a dataset that is collected in a lab environment, and we collect our dataset in real kitchen environments.\n- **Modalities:**  As we primarily employ HoloLens2 in capturing the data, we leverage the wealth of information it enables to capture. For example, our synchronized dataset enables the complete reconstruction of a 3D map of the environment while the activity is being performed.\n\nIn conclusion, we emphasize that the category of errors we explore is different from the category of errors explored by activities in the assembly domain. \n\n---\n\n> However, the ways of capturing error videos have some problems\n\nWe respectfully deny this statement.\n- **Intentional videos based on scripts**\n  - It has been a common practice to use scripted videos for activity understanding. Popular dataset [1] is constructed based on pre-prepared scripts.\n  - Since the submission of this manuscript, two works have been published in a similar direction, and both explored assembly tasks [2-3] and both used pre-prepared scripts to capture data.\n- **Unintentional videos**\n  - Although participants followed scripts to perform activities. Due to the complex nature of cooking tasks and the participants' lack of experience in cooking, we do capture unintentional errors.\n  - We have updated the following sections of the manuscript to explicitly state this information Table 1, Figure 3 and Section 3.1.2\n- **Visual Appearance**\n  - It is important to note that the visual outcome of the majority of the errors remains the same i,e independent of errors generated intentionally by following a script or unintentionally. \n\n---\n\n> The baselines provide not much insights.\n\nError recognition and robust multi-step localization with errors are harder tasks which require the development of techniques that semantically understand the environment, perceptually identify the changes in the state of the activity and infer the cause. We do understand that the provided baselines are preliminary, but they provide a starting point for the development of task-specific methods. \n\nThe purpose of this manuscript is to explore and categorize common errors that could occur in the cooking domain and follow the convention of evaluating the dataset using task-specific (error-recognition and multi-step localization) transfer learning techniques. \n\n---\n\n> Writing is not clear in some parts\n\nAs stated in the paper, we extracted features from 4 pre-trained models, used three criteria to construct training, validation and test splits and trained 12 models. In all these scenarios, we train models task-agnostically. \n\n---\n\n> Minor comments Section 4.2: what does \"refer to 16\" mean?\n\nIt is a typo. Thanks for catching it. \nIt should be \\textit{refers to Figure 16}, where figure 16 is from the supplementary material. \n\n---\n\n\n\n- [1] Hollywood in homes: Crowd-sourcing data collection for activity understanding. In European Conference on Computer Vision, 2016.\n- [2] IndustReal: A dataset for procedure step recognition handling execution errors in egocentric videos in an industrial-like setting 2024.\n- [3] Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos.ICCV 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6873/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706432768,
                "cdate": 1700706432768,
                "tmdate": 1700706432768,
                "mdate": 1700706432768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]