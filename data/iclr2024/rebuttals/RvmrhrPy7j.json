[
    {
        "title": "Causal Inference Using LLM-Guided Discovery"
    },
    {
        "review": {
            "id": "dk7WFxQrgr",
            "forum": "RvmrhrPy7j",
            "replyto": "RvmrhrPy7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to use LLMs with majority voting to learn a causal order of the random variables in the underlying data generating process represented by directed acyclic graphs from observed data. The learned causal order is then used to orient the undirected edges in the output of the existing causal discovery algorithms. Additionally, the authors claim that causal graphs are not necessary needed for causal effects estimation, rather, the causal order is sufficient by finding a valid backdoor adjustment set. They further argue that using causal orders is preferable in the case when domain expert knowledge is available."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors demonstrate the utility of LLMs in causal discovery through means of causal orders and use that as a background knowledge for the existing causal discovery algorithms.  The paper also shows that causal structures are not necessarily required for causal effect estimation and causal orders are sufficient. It also shows both empirically and theoretically that SHD is not a good metric to measure the accuracy of predicting correct causal orders. The paper is fairly well-written and the proofs are sound."
                },
                "weaknesses": {
                    "value": "* Taking outputs from LLMs as inputs to causal discovery algorithms is not uncommon [5]. I find the comparison in the experiment is not quite fair to the existing causal discovery algorithms. There are many existing algorithms that incorporate background knowledge of ordering restrictions [1, 2, 3] and they are not reported on the paper. The authors could have randomly sampled from the ground truth and provided that as background knowledge to other algorithms in the experiment especially for graphs that are less than 20 nodes to compare against methods with LLMs. Given that the theoretical contributions are relatively small, I would expect to see more empirical experiments to show the strong motivation and merits of the approach. \n\n* The experimental result could have been highly affected by the popularity of the datasets and domain knowledge on the internet and using LLMs to guide causal discovery can be very limited to those commonly available data. \n\n* It is not clear what the advantages of using LLMs as a source of domain knowledge are as it may have issues with hallucinations unless there are large-scale experiments that show some domain knowledge are impractical to obtain via domain experts and need LLMs to guide such effort. \n\n* It is also not clear to me why the estimation is not compared against with those estimation methods that use causal graphs or simply a Markov equivalence class of DAGs [4] even if there is only the information of causal orders available to show the merits of using only the causal order for estimation. \n\nReferences\n\n* [1] de Campos, Luis M., and Javier G. Castellano. \"Bayesian network learning algorithms using structural restrictions.\" International Journal of Approximate Reasoning 45.2 (2007): 233-254.\n* [2] Cooper, Gregory F., and Edward Herskovits. \"A Bayesian method for the induction of probabilistic networks from data.\" Machine learning 9 (1992): 309-347.\n* [3] Borboudakis, Giorgos, and Ioannis Tsamardinos. \"Towards robust and versatile causal discovery for business applications.\" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016\n* [4] Jung, Yonghan, Jin Tian, and Elias Bareinboim. \"Estimating identifiable causal effects on markov equivalence class through double machine learning.\" International Conference on Machine Learning. PMLR, 2021.\n* [5] Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huanhuan Chen. From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint\narXiv:2306.16902, 2023."
                },
                "questions": {
                    "value": "1. How does using triples helps avoiding cycles in learning the causal order? \n2. Is it possible that the causal order output by LLMs orient a new unshielded collider in the output of other causal discovery algorithms? \n3. Have the authors tried to provide background knowledge to PC and compare that with PC+LLM? For example, randomly sample from the ground truth and provide such background knowledge to PC or other algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698389137128,
            "cdate": 1698389137128,
            "tmdate": 1699637167631,
            "mdate": 1699637167631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wAI8tu9qld",
                "forum": "RvmrhrPy7j",
                "replyto": "dk7WFxQrgr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Third Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We have incorporated their feedback and provided a review wise explanation as follows:\n\n**Response To Weaknesses**\n\n**Comment**: Taking outputs from LLMs as inputs to causal discovery algorithms is not uncommon [5] ..... show the strong motivation and merits of the approach.\n\n**Response**:  Our work focuses more on evaluating how good LLMs are as knowledge bases or domain experts rather than focusing on how well different discovery algorithms incorporate domain knowledge. The work leverages LLMs understanding of real world, to extract relevant information (causal order) which is inputted into causal discovery algorithms as priors to further aid their performance. While we agree past work has covered inputting LLM outputs as priors to discovery algorithms, our work stands out by focusing on probing LLMs for extracting causal order instead of detecting all possible edges, due to causal order\u2019s utility in downstream tasks like effect estimation, prediction, etc. While previous work builds upon pairwise prompting strategies our work proposes a novel triplet based method which overcomes the previous methods drawbacks of having cycles in the final graph and presenting a lower DTop.\n\nTo present an analysis of how close LLMs as knowledge bases are to ground truth information as priors, we conduct an experiment on Child and Asia dataset (refer Table A15).\n\n**Comment**: The experimental result could have been highly...limited to those commonly available data.\n\n**Response**: We agree that there is a possibility that LLMs have seen the BNLearn datasets in its pretraining setup, and highlight this in the limitations section of our paper as well. However to tackle this claim we include the Neuropathic Pain Diagnosis dataset in our updated paper draft and show the effectiveness of our methods on the same. The dataset is less popular and requires a very nuanced medical understanding in order to correctly assess the causal relationship, therefore depicting how LLMs can be employed in real world settings which are not straightforward. That being said, we still agree that memorization of causal discovery datasets is an issue in current evaluation of LLMs which is scope for future research work.\n\n**Comment**: It is not clear what the advantages...need LLMs to guide such effort.\n\n**Response**: One of the practical implications of using LLMs can be to aid human experts as they can look at the LLM generated graphs and edit it further thus saving time as well as efforts. While LLMs do suffer from issues like hallucinations, they still carry an understanding of the world which can be used to assist the human experts to improve and fasten the causal graph creation process. It will be on human experts to accept the suggestions and reasoning provided by LLMs while constructing causal graphs thus removing any hallucinated or false outcome.\n\n**Comment**: It is also not clear to me why the estimation...the causal order for estimation.\n\n**Response**: Thank you for the review, we have added a comparative analysis (Refer Table A16 in Appendix) on Asia dataset to show estimation using backdoor set from the given graph vs given causal/topological order of the graphs.\n\n**Response To Questions**\n\n**Question**: How does using triples helps avoiding cycles in learning the causal order?\n\n**Answer**: Our intuition is that due to dynamic context, i.e. with each iteration for deciding edge wise causal direction between a given pair of nodes, the third variable changes thus providing different context each time. Due to this, the LLM gets an overall understanding of the other nodes present in the graph. Thus, the aggregate of all the decisions LLM makes pertaining to a specific pair followed by tie breaker using GPT-4, incorporates overall understanding plus robustness in the final answer. Higher incorporation of neighboring nodes plus multiple querying of while deciding causal edge between all possible pairs, might be some reasons behind lower number of cycles. On the other hand, since pairwise analysis that has been done in previous literature, prompt LLM only once without extra contextual information, the edge formations are not always robust and might be contributing factors for forming more cycles.\n\n**Question**: Is it possible that the causal order output by LLMs orient a new unshielded collider in the output of other causal discovery algorithms?\n\n**Answer**: Since we do not have a check for this in our current algorithm, there is a slight possibility that LLMs might orient new unshielded colliders when used as priors with PC Algorithm. We will incorporate this check in our pipeline.  \n\n**Question**: Have the authors tried to provide background knowledge to PC...knowledge to PC or other algorithms.\n\n**Answer**:  We have added Table A15 in Appendix to show comparison between LLM and Ground truth prior for causal discovery."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647637267,
                "cdate": 1700647637267,
                "tmdate": 1700647637267,
                "mdate": 1700647637267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DtqW6NrURd",
            "forum": "RvmrhrPy7j",
            "replyto": "RvmrhrPy7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the question if and how LLMs can be utilized for causal discovery tasks. For this, the authors focus on effect estimation and argue that knowledge about the causal order is sufficient. The paper aims at two contributions: 1) Showing that the causal order is sufficient for effect estimation problems and 2) showing how LLMs can be used in addition to statistical approaches, such as PC, to improve the causal discovery performance. The suggested approach has been evaluated with different experiments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses a logical step to combine causal discovery approaches with the domain 'knowledge' of LLMs.  \n- Careful consideration of different approaches on using LLMs.\n- Encouraging results in the experiments."
                },
                "weaknesses": {
                    "value": "The overall idea is a logical next step seeing the recent success of LLMs in the causal context. However, some of my concerns are:\n- The first contribution regarding the sufficiency about knowing the causal order is not novel and a rather straightforward insight seeing that conditioning on any 'upstream' node of a treatment variable in a DAG results in a valid adjustment set. Therefore, it is certainly good to point this out again, but this is not a new contribution by this work.  \n- The paper overall seems rather incremental, seeing that the paper by Kiciman et al. is already providing some significant prior work in this regard for causal discovery. However, I acknowledge the incorporation of LLM generated knowledge with statistical approaches such as PC.\n\nSee the \"Questions\" section for further points."
                },
                "questions": {
                    "value": "My main concern is the rather incremental novelty, especially since the argument that the causal order is sufficient for effect estimation tasks is a well known point. Some other remarks:\n\n- You are focusing on effect estimation tasks, but the general premise of using LLMs for causal discovery can also be helpful for other tasks. Consider formulating it more broadly and then focus only on effect estimation in the experiments as an example.\n- You are arguing that looking at SHD is often the wrong metric. However, these works using SHD typically address the problem of inferring the whole DAG structure without any particular causal task in mind, while you are only concerned with the causal order for effect estimation problems. In that sense, the SHD makes sense as a metric to see how good the inferred DAG structure is.\n- While you reference the work by Kiciman et al., a more direct comparison is missing. In particular, the related open source package https://github.com/py-why/pywhy-llm has several prompting techniques for inferring structural information. That being said, they do not combine it with methods like PC, which is the novel part in your work.\n- Fair discussion of the limitations and potential issues with overfitting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709319068,
            "cdate": 1698709319068,
            "tmdate": 1699637167505,
            "mdate": 1699637167505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hZ07KrbIbq",
                "forum": "RvmrhrPy7j",
                "replyto": "DtqW6NrURd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Second Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable inputs, we have provided review wise explanation as follows: \n\n**Weaknesses**\n**Comment**: The first contribution regarding the sufficiency....but this is not a new contribution by this work.\n**Response**: The technical result may not be a new result for the backdoor criterion. But its application leads to a valid order-based method for causal discovery using LLMs, whereas we argue that existing pairwise-based prompts that elicit edges may not be valid. The pre-existing work has heavily leveraged LLMs for causal edge detection between any given nodes by optimizing on metrics like SHD for full graph discovery. But the identification of direct causal edge between any given pair of nodes heavily depends on the presence of other nodes, therefore we believe probing LLMs for getting the order between any given pair is a better question framing since the pairwise order only depends on the variables in the question. \n\nFor example, consider the data-generating process, lung cancer ->doctor visit -> Positive Xray. If asked, an expert would affirm a causal edge from \u2018Lung cancer\u2019 to \u2018Positive Xray\u2019 (indeed, such an edge exists in the BNLearn Cancer [1]). However, if they are told that the set of observed variables additionally includes doctor visit, then the correct answer would be to not create a direct edge between \u2018Lung Cancer\u2019 and \u2018Positive Xray\u2019, but rather create edges mediated through \u2018doctor visit\u2019. Note that the causal order, \u2018Lung Cancer\u2019 < \u2018Positive Xray\u2019 remains invariant in both settings. To fill this conceptual gap in the existing work, our work focuses on causal order and probes LLMs specifically from this perspective since the causal order is useful for most downstream tasks and in practice has huge applications as it is locally consistent in terms of estimation (refer table 1).\n\n**Comment**: The paper overall seems rather incremental...with statistical approaches such as PC.\n\n**Response**: While our work draws inspiration from prior research to enhance LLM-based pipelines for aiding in causal discovery, our approach stands out by prioritizing the exploration of how LLMs can effectively identify causal order. This emphasis on identifying causal order is more valuable compared to solely focusing on direct and indirect edge connections, which often rely heavily on other variables. But since order between any given pair of variables remains invariant to other present variables and they directly correlate with downstream tasks like Causal effect estimation, we emphasize and build our frameworks towards that. While previous papers have focused on pairwise causal relationship discovery, our work takes inspiration from pre-existing algorithms to propose novel LLM based pipelines (such as the Triplet prompt). Also, we present how LLMs can be easily adapted with different classes of discovery algorithms whether it is score based or constraint based, for enhancing discovery performance. Most importantly, we emphasize on how current LLMs can be used for downstream applications like causal inference, effect estimation, prediction, etc by leveraging them as knowledge base for finding correct causal order whereas previous work (like Kiciman et al.) have focused more on pairwise causal relationship identification for graph discovery. We show how this approach is not suitable and does not leverage LLMs potential to help in causal tasks by helping get the correct order of the graph by aiding discovery algorithms like PC, CamML, etc."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646860641,
                "cdate": 1700646860641,
                "tmdate": 1700646860641,
                "mdate": 1700646860641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4hbSsBXBUI",
            "forum": "RvmrhrPy7j",
            "replyto": "RvmrhrPy7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_vBFn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9268/Reviewer_vBFn"
            ],
            "content": {
                "summary": {
                    "value": "As a method for estimating causal effects, this paper proposes using LLMs as virtual experts to elicit a causal ordering of the variables. With the causal ordering, a valid backdoor set can be determined as the causal effect can be estimated. Different prompting strategies are explored, as well as algorithms that combine these virtual expert judgments with existing causal discovery algorithms."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The results are presented fairly well.\n\nReplacing human experts by LLMs could be considered, though I am not up-to-date on the related work cited for this part of the paper."
                },
                "weaknesses": {
                    "value": "The theoretical contribution is trivial, and contains multiple mistakes."
                },
                "questions": {
                    "value": "* Assumption 3.3 states there is no latent confounding between treatment and target, but you actually need the stronger assumption that there is no latent confounding between any observed variables. Otherwise for instance proposition 4.2 will fail: Suppose we want to find a valid backdoor set for $X \\to Y$, and there is a third observed variable $Z$ that is not a cause or effect of $X$ or $Y$, but there is a latent variable causing $X$ and $Z$, and another causing $Y$ and $Z$. Then a valid topological ordering of the observed variables is $Z < X < Y$, but adjusting for $Z$ actually opens the backdoor path.\n\n* Proposition 4.2 requires the further assumption that $i < j$.\n\n* Paragraph below proposition 4.2, \"causal effect practitioners tend to include all confounders ...\": Can you provide a reference for this claim? Either way, what you propose goes further than including all *confounders*: you also include variables that cause either the target or the treatment but not both.\n\n* The definitions of $E_m, E_f, $E_d$ for SHD are incorrect: a wrongly oriented edge will add one to each of these three variables. Further, I think you mean to add the cardinalities rather than the sets themselves.\n\n* Algorithm in section 5.2: Steps 2 and 3 and the difference between them are unclear from the text. For algorithms, it may be better to use pseudocode, or at least some mathematical notation.\n\n* In the prompts in the appendix, I noticed that often \"causally effects\" is written when \"causally affects\" was meant."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843873598,
            "cdate": 1698843873598,
            "tmdate": 1699637167351,
            "mdate": 1699637167351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6BmMJA1jQr",
                "forum": "RvmrhrPy7j",
                "replyto": "4hbSsBXBUI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable inputs, we have incorporated their comments and provided a review wise explanation as follows:\n\n**Question**: Assumption 3.3 states there is no latent confounding....observed variables is Z<X<Y, but adjusting for Z actually opens the backdoor path.\n\n**Answer**: Thank you for pointing it out and we agree with you. We revised the manuscript to include the line \u201cWe assume that the underlying causal graph has no unobserved confounders''.\n\n\n**Question**: Proposition 4.2 requires the further assumption that i<j.\n\n**Answer**: Thank you for pointing it out. We\u2019ve updated the proposition statement in the revised manuscript.\n\n\n**Question**: Paragraph below proposition 4.2, \"causal effect practitioners tend to include all confounders ...\":....the treatment but not both.\n\n**Answer**: The statement is based on causal inference practice in statistics and econometrics where all observed covariates (assumed to be pre-treatment) are included in the estimation model. As noted by Cinelli and Pearl (2020), variables that cause only the target are \u201cgood\u201d variables to condition on, so they do not lead to any trouble. Including causes of treatment only can lead to estimation issues (e.g., including an instrumental variable) that increase with the strength of the instrument\u2019s causal effect on treatment. In practice, however, strong instruments are rare. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3689437\n\n\n**Question**: The definitions of Em, Ef, E_d$ for SHD are incorrect: a wrongly oriented edge will add one to each of these three variables. Further, I think you mean to add the cardinalities rather than the sets themselves.\n\n**Answer**: Apologies. We\u2019ve removed the mathematical formula for SHD as it is not required to understand the rest of the paper and the detailed formula will affect the readability.\n\n\n**Question**: Algorithm in section 5.2: Steps 2 and 3 and the difference between them are unclear from the text. For algorithms, it may be better to use pseudocode, or at least some mathematical notation.\n\n**Answer**: We\u2019ve updated the Algorithm 2 in the revised manuscript to clearly explain the steps.\n\n\n**Question**: In the prompts in the appendix, I noticed that often \"causally effects\" is written when \"causally affects\" was meant.\n\n**Answer**: We apologize for the typographical mistake, we have updated in the main manuscript"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645643316,
                "cdate": 1700645643316,
                "tmdate": 1700645643316,
                "mdate": 1700645643316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]