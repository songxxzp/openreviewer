[
    {
        "title": "FedSKU: Defending Backdoors in Federated Learning Through Selective Knowledge Unlearning"
    },
    {
        "review": {
            "id": "n9nOREOO8R",
            "forum": "rFCGiFTVyY",
            "replyto": "rFCGiFTVyY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_d1FD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_d1FD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called FedSKU (Federated Selective Knowledge Unlearning) to defend against backdoor attacks in federated learning. Compared to existing coarse-grained defenses that either completely remove suspected malicious models or add noise, FedSKU takes a more fine-grained approach by decomposing the model into the malicious trigger and useful knowledge. It recovers the trigger pattern using a novel pre-aggregation scheme for efficiency. Then it uses a dual distillation process to unlearn the trigger while preserving only clean knowledge in a surrogate model. This allows aggregating the useful knowledge from malicious models.\n\nExperiments on image datasets like CIFAR-10/100 and Tiny ImageNet validate FedSKU. It improves accuracy by up to 6.1% over defenses like FLAME and Krum, with negligible increase in attack success rate (<0.01%). FedSKU also outperforms extensions of other unlearning methods like BAERASER and NAD to federated learning. Overall, FedSKU effectively utilizes knowledge from malicious models to improve accuracy while defending backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Proposes a novel selective unlearning framework FedSKU that decomposes models into triggers and useful knowledge for fine-grained backdoor defense in federated learning.\n2. Designs efficient techniques like pre-aggregation scheme for trigger recovery and dual distillation loss to selectively unlearn triggers while retaining useful knowledge.\n3. Achieves significant accuracy gains over prior defenses like FLAME and Krum on CIFAR and Tiny ImageNet datasets, with marginal increase in attack success rate.\n4. Outperforms extensions of other unlearning methods like BAERASER and NAD to federated learning scenario.\n5. Comprehensive experiments analyzing impact of non-IID data, ratio of malicious clients etc."
                },
                "weaknesses": {
                    "value": "1. Accuracy improvements are higher on CIFAR than Tiny ImageNet - I think more analysis are needed for why FedSKU works better on certain datasets and if this could be a sign of generalization difficulties.\n2. No major limitations of the approach have been discussed."
                },
                "questions": {
                    "value": "1. The pre-aggregation scheme for efficient trigger recovery makes sense intuitively, but more details or intuition could be provided on why aggregating backdoored models retains the malicious triggers reliably.\n2. For unlearning using dual distillation, how sensitive is the performance to the hyperparameters like the distillation temperature? Was there any tuning done to set the hyperparameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698530506754,
            "cdate": 1698530506754,
            "tmdate": 1699636428048,
            "mdate": 1699636428048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q2xDVMnvQR",
                "forum": "rFCGiFTVyY",
                "replyto": "n9nOREOO8R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d1FD"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your helpful and valuable feedback on our paper. We address your comments and questions below.\n\n**Weekness1:** Accuracy Analysis on Different Datasets.\n\n**Answer:** Thanks for your insightful question. We believe that generalization difficulties do not exist, as our method demonstrates performance improvement across all datasets, albeit to varying degrees. We speculate that the difference on the accuracy improvement can be attributed to the inherent complexity of the datasets. For example, the lower resolution and simpler feature space of CIFAR may facilitate more effective unlearning compared to the intricate Tiny-ImageNet, resulting in higher accuracy improvements.\n\n**Weekness2:** Limitations.\n\n**Answer:** First, our method may not extend to untargeted poisoning attacks, as they do not typically rely on a trigger pattern for targeted activation. Additionally, the computational demands of our unlearning process necessitate a server with sufficient resources. Finally, our method is based on the malicious-client detection techniques, and thus the accuracy of these detection methods may affect the performance of our approach. In the future, we will try our best to further address these limitations. The limitations have been added in the revision (details in Appendix E.3).\n\n**Question1:** Intuition on Why Aggregating Backdoored Models Retains the Malicious Triggers Reliably.\n\n**Answer:** The rationale behind pre-aggregation lies fundamentally in the attackers' goals. FL Attackers embed backdoor triggers in their updates with the intention that, once aggregated, the global model will inherit and manifest these backdoor characteristics. Pre-aggregation effectively simulates the aggregation process that would occur on the server, thus inherently preserving the malicious features introduced by the attackers.\n\n**Question2:** Sensitivity of the Distillation Temperature.\n\n**Answer:** We conducted experiments on CIFAR to assess the impact of various distillation temperature values. As illustrated in the following table, the distillation temperature exhibits minimal effects on both ASR and GACC, indicating the robustness of our method.\n\n| temperature | 1    |     | 2    |     | 5    |     | 10   |     |\n|-------------|------|-----|------|-----|------|-----|------|-----|\n|             | ASR  | GACC| ASR  | GACC| ASR  | GACC| ASR  | GACC|\n|   FedSKU    | 3.35% | 75.40%| 3.20%  |73.52%| 2.91% |76.68%| 3.06% |74.92%|\n\nWe have added the experimental results in Appendix E.4.\n\nThank you again for your valuable time spent reviewing our paper!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224758893,
                "cdate": 1700224758893,
                "tmdate": 1700228008447,
                "mdate": 1700228008447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gwTjx6SKHW",
            "forum": "rFCGiFTVyY",
            "replyto": "rFCGiFTVyY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_mcVc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_mcVc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a backdoor defense technique in federated learning (FL) by\nfirst identifying the possible trigger via trigger inversion, and then unlearn\nthe trigger from the model. The unlearning is done by distillation, assuming a\nset of public dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "From writing aspect, the paper is easy to follow and understand. Its method description is clear.\n\nExperiment show that the method is very effective, compared with existing\nmethods."
                },
                "weaknesses": {
                    "value": "The used technique in this paper, trigger inversion and distillation, do to seem\nto be significantly different from existing work. For example, its inversion\nmethod is leveraging MESA (Qiao et al., 2019). I am not sure about the\nnovelty and significance of the technique.\n\nThere is no clear threat model in the paper. For example, both the trigger\nrecovery and unlearning require certain public data. But what types of public\ndata? There are various backdoors that work on a subset of inputs or outputs\nlabels. Does this method work on all these attacks? Based on my understanding, I\ndo not think it can cover all backdoor attacks. However, without a clear threat\nmodel clarifying the assumptions, I have no information to leverage -- so does\nthe paper itself.\n\nWhat does the method guarantee? Namely, will the proposed unlearning method be\n\"exact\" or \"approximate\"? \n\nAnother line of work, e.g., FedRecover, that tries to recover from poisoning\nattacks without the need to recover the trigger (and is also not limited to\nbackdoors), and also guarantees the recovered model is similar to the one\ntrained on non-poisoning data with a practical difference bound. The paper\nshould also include a discussion and comparison on that."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772079423,
            "cdate": 1698772079423,
            "tmdate": 1699636427969,
            "mdate": 1699636427969,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mbKm0tlZSf",
                "forum": "rFCGiFTVyY",
                "replyto": "gwTjx6SKHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mcVc (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and valuable feedback. We hope the following clarification can address your concerns.\n\n**Weekness1:** Technical Significance.\n\n**Answer:** First, we are the first to introduce the idea of utilizing the benign knowledge embedded in the backdoored models to enhance the FL performance. Second, for the proposed techniques, we would like to highlight that we have made substantial modifications to the existing techniques, rather than directly applying them. Specifically, for trigger recovery, we introduce a pre-aggregation scheme to the backdoored models before conducting the trigger recovery, in order to avoid the massive computational consumption. For the distillation techniques, Section 4.3 has explicitly described our key designs (i.e., distillation architecture and distillation loss) to achieve unlearning under the FL scenario, which is fundamentally different from the traditional machine unlearning techniques. We will highlight these distinctions more explicitly in our revision to clarify the unique contributions of our work.\n\n**Weekness2:** Threat Model.\n\n**Answer:** We are truly grateful for your reminder regarding the threat model. Here we formally define it.\n\n*Threat Model:* In our paper, we follow the attack setting of the previous FL defense work [1]. The attacker $\\mathcal{A}$ intends to inject the backdoor pattern into the aggregated model, which will subsequently affect all $N$ participants. The attacker controls $m < \\frac{N}{2}$ clients and utilizes them to upload the elaborate model updates to the server. The attacker has full knowledge of the clients under his control, including the local training dataset, training processes, and model parameters. However, the attacker has no information about all other clients and no control of the server. The attacker intends to poison the global model $G$ and simultaneously makes it behave as if it is a normal model. Specifically, the attacker's goal is:\n\n\\begin{equation}\n\\forall x' \\in T, G(x') = y_{\\text{Att}}\n\\end{equation}\nand\n\\begin{equation}\n\\forall x_i \\in D, G(x_i) = y_i.\n\\end{equation}\n\nHere, $y_{\\text{Att}}$ is the attacker's target label, $y_{i}$ denotes the correct label of input $x_{i}$, $D$ is the clean data and $T$ denotes the data containing the backdoor pattern chosen by the attacker.\n\nIn addition, regarding the public data used in the trigger recovery and unlearning process, our method requires a small subset of clean data with the same data type as our training data. Consistent with the settings used in NAD [2], we only utilize 5% data of the dataset as the public data. \n\nAs for the backdoor types, our method is effective against typical backdoor attacks that target a subset of inputs, meaning that the backdoor is activated only when the input data contains a specific trigger pattern. \n\nWe have also added the threat model in the revision (details in Appendix E.1).\n\n[1] Nguyen, Thien Duc, et al. \"{FLAME}: Taming backdoors in federated learning.\" 31st USENIX Security Symposium (USENIX Security 22). 2022.\n\n[2] Li, Yige, et al. \"Neural attention distillation: Erasing backdoor triggers from deep neural networks.\" ICLR 2021. 2021.\n\n**Weekness3:** Method Guarantee.\n\n**Answer:** Due to the poor interpretability of DNN models, our approach cannot achieve an \"exact unlearning,\" which means that we can accurately locate all the backdoors. Instead, we assure an \"approximate unlearning\" by our proposed dual distillation method."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224629202,
                "cdate": 1700224629202,
                "tmdate": 1700224629202,
                "mdate": 1700224629202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5mKDdfQYgL",
                "forum": "rFCGiFTVyY",
                "replyto": "gwTjx6SKHW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mcVc (2/2)"
                    },
                    "comment": {
                        "value": "**Weekness4:** Discussion on FedRecover [3].\n\n**Answer:** The main differences between FedRecover and FedSKU are as follows:\n\n*Memory Demand:* FedRecover requires substantial memory storage to maintain the historical information necessary for recovery. FedSKU, in contrast, does not impose such memory demands.\n\n*Communication Overhead:* FedRecover necessitates additional communication rounds between clients and the server for update corrections, and FedSKU can operate without requiring extra communication overhead.\n\n*Performance Comparison:* FedSKU can significantly improve accuracy for methods that directly exclude the backdoored models and conduct clean model aggregation, which exceeds the ACC upper bound of a model recovered by FedRecover, whose primary objective is being close to the train-from-scratch clean aggregated models.\n\nIn conclusion, while both FedSKU and FedRecover offer robust defense strategies, they are optimized under different objectives and constraints. FedSKU enhances the accuracy of the aggregated clean model, making it suitable for scenarios where accuracy is of high priority and server resources are abundant. FedRecover, on the other hand, can be applied for more general scenarios (i.e., not limited to backdoors) but will require high memory demand and communication cost for clients.\n\nWe have added the discussion in Appendix E.5.\n\n[3] Cao, Xiaoyu, et al. \"Fedrecover: Recovering from poisoning attacks in federated learning using historical information.\" 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023.\n\n\nThank you again for your valuable time spent reviewing our paper!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224659822,
                "cdate": 1700224659822,
                "tmdate": 1700700368709,
                "mdate": 1700700368709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2oVBmQNoWc",
            "forum": "rFCGiFTVyY",
            "replyto": "rFCGiFTVyY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the vulnerability to backdoor attacks that manipulate model parameters to deceive the aggregation process. Unlike existing defenses that employ coarse-grained methods, this research takes a more nuanced approach. The authors propose a novel technique called FedSKU, which involves decomposing the uploaded model into two distinct components: malicious triggers and useful knowledge."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The concept of selective unlearning represents a novel and compelling advancement in comparison to the coarser-grained defenses commonly used.\n- Through extensive evaluation across various datasets, the method demonstrates superior performance in accuracy compared to state-of-the-art defenses, all while keeping the increase in attack success rate at a negligible level.\n- The inclusion of convergence analysis and ablation studies offers valuable insights into the inner workings of the method."
                },
                "weaknesses": {
                    "value": "- The method proposed hinges on anomaly detection for identifying malicious clients. It's important to note that this approach has its limitations; attackers may find ways to evade detection. The authors should delve deeper into this aspect for a more comprehensive discussion.\n- The paper lacks an in-depth analysis of the computational overhead associated with trigger recovery and unlearning.\n- The experiments conducted on non-iid settings are not as extensive as one might expect."
                },
                "questions": {
                    "value": "- The effectiveness of the unlearning process can be influenced by various factors, including model complexity, available data volume, and the complexity of the information to be forgotten. How can we ensure that unlearning remains effective after anomaly detection?\n- How does the computational overhead of the trigger recovery and unlearning process change as the number of clients increases?\n- Instead of performing unlearning, wouldn't it be more efficient to simply exclude the detected malicious clients from the training process?\n- Given the potentially large number of participating clients in Federated Learning, how can we guarantee that the proposed method remains effective despite the high computation overhead?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4512/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803271714,
            "cdate": 1698803271714,
            "tmdate": 1699636427856,
            "mdate": 1699636427856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y8APkltcJs",
                "forum": "rFCGiFTVyY",
                "replyto": "2oVBmQNoWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FcGK (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and valuable feedback. We hope the following clarification can address your concerns.\n\n**Weakness1:** Discussion on anomaly detection.\n\n**Answer:** We agree with the reviewer that advanced adversaries may manage to bypass detection mechanisms. However, what we want to emphasize is that anomaly detection, as a defense mechanism in Federated Learning, is a topic where researchers continually explore more advanced and effective mechanisms to resist relevant attackers. Many recent detection-based defense mechanisms, such as FLAME[1], DeepSight[2], and FLDetector[3], have demonstrated increased maturity, achieving near-perfect accuracy under specific attack scenarios. Therefore, we believe that with the continuous improvement of anomaly detection mechanisms, our finer-grained enhancements based on it will also remain meaningful.\n\n[1] Nguyen, Thien Duc, et al. \"{FLAME}: Taming backdoors in federated learning.\" 31st USENIX Security Symposium (USENIX Security 22). 2022.\n\n[2] Rieger, Phillip, et al. \"Deepsight: Mitigating backdoor attacks in federated learning through deep model inspection.\" NDSS 22. 2022.\n\n[3] Zhang, Zaixi, et al. \"FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n\n**Weakness2& Question2& Question4:** Analysis of the computational overhead for our approach.\n\n**Answer:** Our approach introduces an unlearning process compared to conventional removal-based defense, which comprises a trigger recovery part and an unlearning part. In the following parts, we analyze the overhead in detail.\n\n*Trigger Recovery:* In the trigger recovery phase, we start by searching for the attacker's target label. We denote the label currently being searched for as $i$ and the number of the label is *num\\_class* (denoted as $N_{c}$). We then apply our recovery process to obtain multiple sub-models, denoted by $G_{i}$, where $i = 1, \\ldots, N_{sub}$. For each sub-model, we train for a number of epochs defined by *num\\_recovery\\_epoch* (abbreviated as $E_{r}$) according to Equation (1). Assuming the dataset size at the server is $V$ and the batch size during training is *recovery\\_batch\\_size* (abbreviated as $BS_{r}$), the complexity of the recovery part can be described as performing $N_{c} \\times E_{r} \\times N_{sub} \\times \\frac{V}{BS_{r}}$ backpropagations.\n\n*Unlearning:* After obtaining the final set of sub-models, $G = [G_{1}, G_{2}, \\ldots, G_{N_{sub}}]$, we proceed to conduct unlearning (i.e., distillation). This involves conducting *distill\\_epoch* (abbreviated as $E_{d}$) training epochs with Equation (4) for unlearning. The batch size for distillation is denoted as *distill\\_batch\\_size* (abbreviated as $BS_{d}$), leading to a distillation complexity of $E_{d} \\times \\frac{V}{BS_{d}}$ backpropagations.\n\n*Total overhead:* Let us denote the number of detected anomalous clients as *abnormal\\_client\\_num* (abbreviated as $N_{a}$). Taking into account the pre-aggregation strategy, the overall complexity of our approach for the detected anomalous clients can be expressed as follows:\n\n\\begin{equation}\n\\text{Total Overhead } = N_{c} \\times E_{r} \\times N_{sub} \\times \\frac{V}{BS_{r}} + N_{a} \\times (E_{d} \\times \\frac{V}{BS_{d}})\n\\end{equation}\n\nGiven that backpropagation is the primary computational cost, our complexity analysis focuses on the count of backpropagation operations. In practice, $E_{r}$ and $E_{d}$ do not have to be large, and for the recovery phase, using only 1-2 sub-models can approximate well and yield satisfactory results. \n\n*Increased Client Number (Question2) & Large Client Scale (Question4):* \nAccording to the analysis of the computation cost, if the proportion of malicious clients remains unchanged while the total number of clients increases, the computational cost of our method would scale linearly. When the number of clients becomes large, the cost is indeed non-negligible. However, we would like to point out that all the operations are conducted on the server side. Considering that the servers usually have sufficient resources, it is acceptable to introduce some overhead on it. If computational resources are indeed limited, we can adopt the following strategy as a trade-off: (1) decreasing $E_{r}$, $E_{d}$, and $N_{sub}$; (2) monitoring the number of malicious clients and skipping some unlearning steps if the number is large.\n\nWe have also added the related description in the revision (details in Appendix E.2)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224473894,
                "cdate": 1700224473894,
                "tmdate": 1700224473894,
                "mdate": 1700224473894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wz9h0cHC45",
                "forum": "rFCGiFTVyY",
                "replyto": "2oVBmQNoWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FcGK (2/2)"
                    },
                    "comment": {
                        "value": "**Weakness3:** Experiments on extensive non-iid settings.\n\n**Answer:** To explore the more extensive non-iid settings, we added an experiment on CIFAR-10 for different methods on the non-iid degree of 0.8, which is considered as a high non-iid situation. As shown in the following table \uff08we also updated Table 4 in our revision\uff09, when the degree reaches 0.8, both ASR and GACC are largely affected for all methods, which means that existing methods cannot cope with an extreme non-iid situation. We would like to highlight that all the defense methods are not specifically designed for the non-iid scenario and if we want to conduct evaluation on this condition, some strategies of addressing the non-iid problem should be combined for each method to make a fair comparison, which is out of our scope.\n\n| Non-iid | 0.2    |      | 0.4  |      | 0.6  |      | 0.8  |      |\n|---------|--------|------|------|------|------|------|------|------|\n|         | ASR    | GACC | ASR  | GACC | ASR  | GACC | ASR  | GACC |\n| BAERASER| 6.97%   | 69.01%| 14.22%| 65.27%| 9.79% | 51.14%| 13.16%| 33.88%|\n| NAD     | 15.75%  | 68.80% | 18.41%| 64.55%| 30.97%| 52.98%| 16.24%| 32.53%|\n| FLAME   | 2.66%   | 67.54%| 3.60%  | 58.94%| 4.74% | 44.93%| 11.82%| 22.83%|\n| Ours    | 2.96%   | 68.12%| 2.84% | 62.44%| 4.20%  | 49.45%| 12.04%| 25.44%|\n\n**Question1:** Effectiveness of the Unlearning Process.\n\n**Answer:** Thanks for your question, We would like to address each of the factors you've highlighted.\n\n*Model Complexity:* We acknowledge that higher model complexity could pose challenges for the unlearning process, potentially requiring more epochs and increased computational overhead. However, it is worth noting that in FL, models are often less complex due to the inherent difficulties of training over distributed resource-constrained clients. Moreover, for attackers, embedding backdoors into highly complex models is equally challenging, which naturally increases the difficulty of the attack. \n\n*Available Data Volume:* In our unlearning process, we follow the NAD method to utilize 5% of the training data, which has proven to be sufficient for maintaining the effectiveness of our approach. We believe that this data volume is acceptable and realistic for most real-world scenarios.\n\n*Complexity of Information to be Forgotten:* We agree that the complexity of information could affect the unlearning process. However, this complexity primarily impacts specific unlearning algorithms, i.e., how to achieve unlearning in the presence of more intricate data information. This paper proposes a design strategy (dual distillation) for unlearning and leverages distillation algorithms for implementation. In the context of more complex data scenarios, we can use more advanced distillation algorithms to align with our approach, such as employing sophisticated distillation loss.\n\n**Question3:** Simple Exclusion of Malicious Clients.\n\n**Answer:** While exclusion might seem straightforward, it becomes ineffective when the proportion of malicious clients is significant. Direct exclusion in such cases would lead to the great loss of substantial useful knowledge embedded in the backdoored models, negatively impacting the global model's accuracy. The baseline FLAME can be considered as a simple exclusion method.\n\nThank you again for your valuable time spent reviewing our paper!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224537990,
                "cdate": 1700224537990,
                "tmdate": 1700228118115,
                "mdate": 1700228118115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VGVIJnJnxA",
                "forum": "rFCGiFTVyY",
                "replyto": "wz9h0cHC45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Reviewer_FcGK"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in addressing my concerns.\n\nI remain unconvinced by the argument favoring unlearning instead of straightforwardly excluding malicious clients. In response to Reviewer mcVc, this study focuses on \"approximate unlearning.\" What if the same malicious clients persistently launch attacks in subsequent rounds, even after being subject to approximate unlearning in the previous round? The impact of the attack's footprint may endure, affecting the training process if an exact unlearning cannot be achieved.\n\nContinuing the training process when a substantial number of malicious clients are present might lead to significant consumption of computational resources without generating meaningful improvements in the model. It could be more resource-efficient to allocate these resources to addressing security concerns and rectifying the compromised learning environment.\n\nRemoving malicious clients stands out as a straightforward and robust solution. This approach adheres to the principle of simplicity and diminishes the complexity of the FL system."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635695705,
                "cdate": 1700635695705,
                "tmdate": 1700635695705,
                "mdate": 1700635695705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mGo0pOefLD",
            "forum": "rFCGiFTVyY",
            "replyto": "rFCGiFTVyY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_wRHA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4512/Reviewer_wRHA"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose FedSKU, a defense mechanism that detects and selectively unlearns harmful backdoors in uploaded models. They introduce a pre-aggregated trigger recovery scheme to efficiently train a trigger pattern generator, reducing training overhead in the FL system. They also designed a dual distillation method for selective knowledge unlearning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper emphasizes that pre-aggregation in the model inevitably preserves certain malicious features. They designed the method based on this insight that can effectively reduce the Federated Learning (FL) training overhead from the trigger generator.\n2. The suggested method extracts valuable knowledge from backdoored clients, providing a novel defense method against backdoors while preserving competitive global accuracy.\n3. The experiments are comprehensive, encompassing varying numbers of malicious clients, initialization parameters, and convergence comparisons among others."
                },
                "weaknesses": {
                    "value": "1. The \"pre-aggregated\" process lacks clarity. Also, it would be better to discuss the federated aggregation method on the main page rather than in the appendix.\n\n2. While it's acceptable that FEDSKU has a marginally lower GACC compared to DNN unlearning methods like BAERASER and NAD, given their excessively high ASR indicates unsuccessful defense, it would strengthen the claim about \u2018'take the essence and discard the dross' if the author could elucidate why DNN unlearning methods have superior GACC."
                },
                "questions": {
                    "value": "1. In Figure 2, it appears that two backdoored models exist, but only one is detected. This setup could be confusing due to the absence of the \"pre-aggregates\" step in the Trigger recovery process. It might be clearer if the framework depicted the detection of multiple backdoored models, thereby illustrating the details of \"pre-aggregates\" and showing that each backdoored model has a corresponding surrogate."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000153795,
            "cdate": 1699000153795,
            "tmdate": 1699636427782,
            "mdate": 1699636427782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oHkqlIldgE",
                "forum": "rFCGiFTVyY",
                "replyto": "mGo0pOefLD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wRHA"
                    },
                    "comment": {
                        "value": "Thank you very much for the positive feedback and valuable comments. We hope the following clarification can address your concerns.\n\n**Weakness1& Question1:** Issues on pre-aggregation and federated methods.\n\n**Answer:** We apologize for the lack of clarity. The \"pre-aggregated\" process mainly includes the following steps: (1) detecting the backdoored models with existing detection methods; (2) employing the aggregation algorithm FedAvg to average the parameters among these backdoored models, such that we can obtain a global model. Note that the global model is merely used as the target for trigger recovery rather than the practical model aggregation. In this way, we can significantly reduce the overhead since only a single model is required to conduct the recovery. At the same time, due to the inherent robustness of FL attacks to the aggregation process, we believe that pre-aggregation will not eliminate the backdoor. \n\nIn addition, we have modified Figure 2 to accurately depict the scenario where multiple backdoored models exist and clearly illustrate the \"pre-aggregation\" step in the trigger recovery process. We sincerely thank you for your observation regarding Figure 2. Furthermore, we have transferred the discussion of the federated aggregation methods in the appendix to the main page. Please refer to our revision.\n\n**Weakness2:** Reasons about why DNN unlearning methods have superior GACC.\n\n**Answer:** Thanks for your valuable suggestion. Generally, there is a significant possibility that unlearning may unlearn some useful features if we adopt an aggressive unlearning strategy, which will result in a reduction in accuracy. In our approach, we employ dual distillation to achieve unlearning. While it effectively eliminates the backdoor, the extent of unlearning is larger than simple distillation methods such as NAD and BAERASER. Consequently, our GACC exhibits a slight reduction compared to baseline methods. However, this dual distillation achieves a significantly lower ASR than NAD and BAERASER, which indicates a more effective defense method.\n\nThank you again for your valuable time spent reviewing our paper!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223923591,
                "cdate": 1700223923591,
                "tmdate": 1700223923591,
                "mdate": 1700223923591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]