[
    {
        "title": "Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning"
    },
    {
        "review": {
            "id": "6SLD93JGIo",
            "forum": "Cc0qk6r4Nd",
            "replyto": "Cc0qk6r4Nd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_ky42"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_ky42"
            ],
            "content": {
                "summary": {
                    "value": "this paper discover three interesting observations based on the exploration between homogeneous and heterogeneous FL settings. Then the authors propose InCo Aggregation methods, inpried by these observation and demonstrate the proposed method can be tailored to accommodate model-homogeneous FL methods and achieve better performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easily understandable, effectively communicating the research in a clear and coherent manner.\n\nThe discovered observations are interesting and valuable for future research, providing a foundation for further investigations and potential advancements in the field.\n\nThe experiments conducted in the paper are sufficient, with an appropriate and comprehensive setup that collects relevant data to support the claims and conclusions."
                },
                "weaknesses": {
                    "value": "1. CKA is an important metric in this paper. The authors should explain it in more details.\n2. Model splitting is proposed to facilitate model heterogeneity. What if the layer-wise gradient sizes of different models are not the same, how do you conduct cross-layer gradients mergence?"
                },
                "questions": {
                    "value": "1. where is CKA from in Fig. 1 (c). Is it the average value of all stages or deep/shallow stage?\n2. Can you provide a more detailed comparison of InCo Aggregation with other state-of-the-art methods for handling system heterogeneity in FL? How does InCo Aggregation compare in terms of performance, communication overhead, and computational complexity?\n3. The paper does not provide a detailed analysis of the computational and communication overhead of InCo Aggregation, which could be a significant factor in large-scale FL applications. This limits the practical applicability of the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "null"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1219/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1219/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1219/Reviewer_ky42"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697686274359,
            "cdate": 1697686274359,
            "tmdate": 1699636048418,
            "mdate": 1699636048418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cMi91VcCBk",
                "forum": "Cc0qk6r4Nd",
                "replyto": "6SLD93JGIo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ky42 [1/1]"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive review. We now address your main concerns as follows.\n\n### Weakness\n\n> **W1.** CKA is an important metric in this paper. The authors should explain it in more details.\n\n**Answer:**\n- Thank you for your comment. We have updated the detailed introduction of CKA in Appendix A to ensure clarity and accuracy in describing the motivation of our method.\n\n> **W2.** Model splitting is proposed to facilitate model heterogeneity. What if the layer-wise gradient sizes of different models are not the same, how do you conduct cross-layer gradients mergence?\n\n**Answer:**\n- Thank you for your suggestion. Currently, InCo Aggregation is focused on addressing the issue of model heterogeneity within the same family of backbones with varying depths and widths. It is not suitable for handling model heterogeneity across completely different model families, such as a combination of CNNs and ViTs. This aspect requires further research. We have updated and discussed these limitations of InCo Aggregation in Appendix I.\n- However, regarding the problem of model heterogeneity across completely different architectures, we have identified two potential directions that could be explored in the context of InCo Aggregation. The first direction involves leveraging public datasets to perform knowledge distillation, where the knowledge from different model architectures is distilled onto one or several server models with the same architecture (more server models can store more knowledge). InCo Aggregation can then be performed on these server models to connect clients with different model architectures. This direction is similar to the approach proposed in FedDF [1].\n- The second method involves utilizing hypernetworks to generate parameters for corresponding layers. Different model architectures can be connected through hypernetworks, similar to how different input dimensions can be transformed into the same dimensions of layer parameters through hypernetworks. Similar to the previous direction, multiple server models can be generated using hypernetworks, followed by InCo Aggregation on the generated server models. Currently, there are several papers focusing on personalized federated learning that utilize hypernetworks for generating weights, such as [2], [3], and [4]. However, we have not yet come across any methods using hypernetworks for addressing model heterogeneity in federated learning.\n- These are preliminary ideas to make InCo Aggregation applicable to model heterogeneity across completely different backbones. However, further in-depth research and experimental analysis are required to explore these directions fully.\n\n### Questions\n\n> **Q1.** Where is CKA from in Fig. 1 (c). Is it the average value of all stages or deep/shallow stage?\n\n**Answer:**\n- Thank you for your comment. In order to obtain a clearer observation of the relationship between CKA and accuracy, we utilized the average value of stage 0. However, it is worth noting that similar relationships exist for other stages as well. To provide a comprehensive analysis, we update the relationships between CKA and accuracy for other stages in Figure 12c and 12d, as well as Appendix G.2. The results show that the positive correlation remains consistent cross different stages. This consistency indicates that this relationship is reliable and stable throughout the various stages.\n\n> **Q2&Q3.** Computational and communication overhead of InCo Aggregation.\n\n**Answer:**\n\n- Thank you for your suggestion. We have updated a comparison of communication overhead and computational resource utilization (FLOPs) in Table 3. We also describe these comparison as follows,\n\n|                 | HeteroFL | +InCo | InclusiveFL | +InCo | FedRolex | +InCo | ScaleFL |  +InCo   |\n| --------------- | -------- | ----- | ----------- | ----- | -------- | ----- | ------- | --- |\n| Comm. Overheads | 4.6M     | 4.6M  | 12.3M       | 12.3M | 4.6M     | 4.6M  | 9.5M    |  9.5M   |\n| FLOPs           | 33.4M    | 33.8M | 75.2M       | 75.6M | 33.4M    | 33.8M | 51.9M   |  52.3M   |\n\n- The computational load of InCo Aggregation arises from the computation of Equation (2). It is noteworthy that InCo Aggregation does not impose any additional communication overhead since there is no requirement for clients to transmit supplementary information. Moreover, InCo Aggregation does not place any burdens on the computation resources of the clients, as the operations involving cross-layer gradients and the gradient alleviation theorem are conducted exclusively on the server side.\n\n\n[1] Ensemble distillation for robust model fusion in federated learning. NeurIPS 2020.\n\n[2] Personalized federated learning using hypernetworks. ICML 2021.\n\n[3] Layer-wised Model Aggregation for Personalized Federated Learning. CVPR 2022.\n\n[4] Efficient model personalization in federated learning via client-specific prompt generation. ICCV 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392142033,
                "cdate": 1700392142033,
                "tmdate": 1700392142033,
                "mdate": 1700392142033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dWZYK3GtsU",
            "forum": "Cc0qk6r4Nd",
            "replyto": "Cc0qk6r4Nd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_ibVy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_ibVy"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the challenge of heterogeneous models within federated learning, uncovering a notable pattern where shallow layers exhibit similar gradient distributions, in contrast to the disparate distributions in deeper layers. The authors further observed that higher gradient similarity corresponds to improved accuracy. Capitalizing on these insights, a novel aggregation method is proposed and substantiated through comprehensive experiments, demonstrating the efficacy of the proposed approach in navigating model heterogeneity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author presents intriguing findings regarding the relationship between layer similarity and model performance.\n2. By refining the direction of deep layers, which exhibit lower gradient similarity compared to shallow layers, the author enhances model performance.\n3. The proposed method demonstrates versatility, adaptable to various Federated Learning (FL) schemas.\n4. The effectiveness of the proposed method is substantiated through extensive experimentation."
                },
                "weaknesses": {
                    "value": "1. The utilization of a model with merely three convolutional layers is unconvincing; larger models should be employed in primary experiments to validate the findings.\n2. The manuscript could benefit from a more coherent narrative, including background on previous works, an introduction to the CAK similarity metric, and a discussion on why the proposed method outperforms state-of-the-art (SOTA) methods, especially in handling model heterogeneity.\n3. Figure 6 lacks clarity; indicating the positions of client and global optima could elucidate the depicted concepts.\n4. It is imperative to delineate the problem definition and notations before introducing the method, ensuring a logical flow and better comprehension.\n5. The manuscript does not adequately explain how cross-layer gradient adjustments ameliorate the effects of model heterogeneity."
                },
                "questions": {
                    "value": "1. I wonder if the proposed approach can be applied to complex models. \n2. I wonder how cross-layer gradient adjustments ameliorate the effects of model heterogeneity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549241467,
            "cdate": 1698549241467,
            "tmdate": 1699636048349,
            "mdate": 1699636048349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T4zFMcx2na",
                "forum": "Cc0qk6r4Nd",
                "replyto": "dWZYK3GtsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ibVy [1/3]"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive review. We now address your main concerns as follows.\n\n### Weakness\n\n> **W1.** The utilization of a model with merely three convolutional layers is unconvincing; larger models should be employed in primary experiments to validate the findings.\n\n**Answer:**\n- Thank you for pointint out this issue, and sorry for the confusion. In our primary experiments, we do not analyze models with only three convolutional layers. In the model homogeneity (homo) setting, we use ResNet18 and ViT-S/12 as client models. In the model heterogeneity setting, we employe five ResNet models obtained by stage splitting from ResNet26, resulting in ResNet10, ResNet14, ResNet18, ResNet22, and ResNet26. Additionally, we use five ViT models, namely ViT-S/8, ViT-S/9, ViT-S/10, ViT-S/11, and ViT-S/12, obtained by layer splitting from ViT-S/12. The reason for focusing on three-layer convolutional layers in Figure 2 and 3 is that we analyzed layers within the same shape in the stages of ResNet. However, ResNet typically contains 3-4 stages, so the analyzed models are not limited to only three convolutional layers. In our primary experiments, we specifically analyzed Stage 2 and Stage 3 in ResNet18.\n- Furthermore, we employed ViT-S models ranging from 8 blocks to 12 blocks, and ViT models are considered relatively complex within the context of federated learning. In Appendix F.2, we added an analysis of blocks in ViTs, demonstrating that blocks in ViTs exhibit similar relationships between gradient distribution and similarity as stages in ResNet. This confirms the suitability of applying InCo aggregation to ViTs as well. Moreover, our experimental results demonstrate the applicability of InCo aggregation to ViTs.\n- Finally, we provide the model size for the architecture we used, which are shown below and are also included in Table 7, 8, and 9 in Appendix H.4:\n\n|      | ViT-S/8 | ViT-S/9 | ViT-S/10 | ViT-S/11 | ViT-S/12    |\n| ---- | ------- | ------- | -------- | -------- | --- |\n| Params | 14.57M($\\times$ 0.67)    | 16.34M ($\\times$ 0.754) | 18.12M ($\\times$ 0.836) | 19.90M ($\\times$ 0.912) | 21.67M ($\\times$ 1) |\n\n|      | ResNet10 | ResNet14 | ResNet18 | ResNet22 | ResNet26 |\n| ---- | ------- | ------- | -------- | -------- | --- |\n| Params | 4.91M ($\\times$ 0.281) | 10.81M ($\\times$ 0.619) | 11.18M ($\\times$ 0.641) | 17.08M ($\\times$ 0.979) | 17.45M ($\\times$ 1)  |\n\n> **W2.** The manuscript could benefit from a more coherent narrative, including background on previous works, an introduction to the CAK similarity metric, and a discussion on why the proposed method outperforms state-of-the-art (SOTA) methods, especially in handling model heterogeneity.\n\n**Answer:**\n- Thank you for your suggestion. Due to the page limitation, we put the most crucial content in the main pages of the paper. The background of previous work is covered in Appendix E to provide a comprehensive understanding of the research context. Additionally, we have updated the detailed introduction of CKA in Appendix A to ensure clarity and accuracy in describing the motivation of our method.\n- For a discussion on why the proposed method outperforms SOTA methods\uff0cwe provide comprehensive analyses of how each component of InCo Aggregation leads to improvements in Section 5.4. Furthermore, we provide a comprehensive explanation of the underlying factors that enable InCo Aggregation to achieve superior results as follows.\n- The improvements introduced by InCo Aggregation originate from two crucial aspects: \n    - (1) The utilization of internal cross-layer gradients facilitates the propagation of generalized knowledge from shallow layers to deep layers, enabling the deep layers to acquire additional knowledge. Consequently, the model becomes more generalized and avoids biases towards local datasets.\n    - (2) The Divergence Alleviation Theorem (Theorem 3.1) alleviates conflicts between cross-layer gradients and gradients from deep layers. \n- In Section 5.4, Figures 9d-9f provide vision representations of how InCoAvg's features exhibit a higher degree of generalization compared to FedAvg and HeteroAvg, which often display biased phenomena stemming from model heterogeneity. Furthermore, Figure 10a quantitatively describes the efficacy of InCoAvg in promoting model generalization using the CKA similarity measure. Specifically, InCoAvg exhibits higher CKA similarity in deep layers compared to FedAvg and HeteroAvg. The results obtained from Figures 9d-9f and 10a validate that the utilization of internal cross-layer gradients effectively facilitates the transfer of generalized knowledge from shallow layers to deep layers. This transfer mechanism helps alleviate the bias issue within models, consequently improving their overall performance.\n\n[Our response continues.]"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391752036,
                "cdate": 1700391752036,
                "tmdate": 1700391752036,
                "mdate": 1700391752036,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sJ08D96EM1",
            "forum": "Cc0qk6r4Nd",
            "replyto": "Cc0qk6r4Nd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_e8EV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_e8EV"
            ],
            "content": {
                "summary": {
                    "value": "The paper highlights the positive correlation between client performances and layer similarities in a federated setting, specifically how similarities are higher in shallow layers of the network and steadily decrease as one gets to the deeper layers as well as how smoother gradient distributions correspond to higher layer similarities. \nKeeping these ideas in mind, the paper proposes InCo Aggregation, which leverages cross-layer gradient similarities to improve the similarity across clients in deeper layers, without the need for additional communication between clients.\nOverall, InCo Aggregation is validated on both model-homogeneous and heterogeneous methods to highlight its impact in improving performances across the board."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The observations on similarity at varying levels of deep networks across clients could potentially establish an observation mechanism for federated settings to analyze the impact of changes across multiple axes like data settings, model heterogeneity, etc.\n- Figures 4 and 6 do a good job at conveying concepts surrounding various model splitting methods and gradient divergence, respectively."
                },
                "weaknesses": {
                    "value": "- \"System level heterogeneity\" is mentioned multiple times and is loosely defined through the early portion of the manuscript (Pg. 1, Paragraph 1). Over the course of reading the paper, one can figure out data and model heterogeneity are the relevant axes along which system level heterogeneity is defined. Defining these ideas earlier and more concisely would allow the reader to contextualize the problem domain and understand how the solution being proposed fits within its scope.\n- While the paper discusses the relationship between gradient similarity and performance/accuracy, and gradient similarity with smoother gradients, the key justifications for choosing to use smoother gradients are (a)  lack of a shared database, (b) Features would increase communication overheads, and (c) correlation between similarity of gradients and smoothness. Are there stronger correlations between the level of similarity (actual value) to the peak density or other statistics of the gradients?\n- There doesn't seem to be a an explicit definition of \"deep\" vs. \"shallow\" layers. Implicitly, within each stage there seem to be shallow and deep layers (Pg. 3, Cross-environment similarity). In general, there are certain experimental settings necessary to fully understand the figures plotted through the course of the manuscript that seem to be missing.\n- Given the specific model heterogeneity settings under which InCo Aggregation is applicable, a discussion on how and where it isn't applicable (e.g., Complete model heterogeneity, where models are restricted to be within the same family of backbones) would be useful."
                },
                "questions": {
                    "value": "- Could the authors define the notion of system level heterogeneity, using model and data heterogeneity, earlier and provide connection points to how the proposed method would address these issues. Having these points described earlier would allow the reader to grasp the importance of the observations described later on.\n- Could the authors provide gradient plots like in Figs 2 and 3 for Stage 2's layers as well? Drawing a parallels to behavior across Stages would be helpful in establishing the consistency of the observations on smoother gradients and how they relate to similarity values.\n- Could the authors provide an explicit definition of which layers can be considered deep vs.shallow? If the nomenclature implicitly defined in \"Cross-environment Similarity\" is to be maintained, could the authors provide an explanation of whether this pattern carries over to ViTs as well?\n- Could the authors discuss further about similarity patterns in ViT's and how this impact the observations and InCo aggregation as a whole?\n- Could the authors provide more detail explanations for the exact setups used to generate Fig. 1, 2, and 3?\n- Could the authors provide more insight into how Fig. 3 would vary when tested across multiple trials? This could help remove the uncertainty caused by SGD noise.\n- Could the authors provide the standard deviation values for InCo-based methods?\n- Given the variation in performances in Table 3 and the original values cited under FedROLEX, ScaleFL, etc., could the authors provide a detailed breakdown of how the experimental settings differ from the original works?\n- A discussion on how and where it isn't applicable (e.g., Complete model heterogeneity, where models are restricted to be within the same family of backbones or in cases where weight matrices do no align) is critical to understand and apply the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784663631,
            "cdate": 1698784663631,
            "tmdate": 1699636048279,
            "mdate": 1699636048279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRh8BfNqtR",
                "forum": "Cc0qk6r4Nd",
                "replyto": "sJ08D96EM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e8EV [1/4]"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive review. We now address your main concerns as follows.\n\nWeaknesses:\n\n> **W1.** \"System level heterogeneity\" is mentioned multiple times and is loosely defined through the early portion of the manuscript (Pg. 1, Paragraph 1).\n\n**Answer:** \n- Thank you for your valuable suggestions. To provide a better understanding of the problem domain, we have updated more comprehensive descriptions and definitions of system heterogeneity and model heterogeneity. System heterogeneity refers to a set of varying physical resources $\\\\{R_i\\\\}\\_{i=1}^n$, where $R_i$ denotes the resource of client $i$, a high-level idea of resource that holistically governs the aspects of computation, communication, and storage. Model heterogeneity refers to a set of different local models $\\\\{M_i\\\\}\\_{i=1}^n$ with $M_i$ being the model of client $i$. Let $R(M)$ denote the resource requirement for the model $M$. Model heterogeneity is a methodology that manages to meet the constraints $\\\\{R(M_i)\\leq R_i\\\\}_{i=1}^n$. We have updated these descriptions in the introduction section.\n- Additionally, in the case study section, we have clarified the level of data heterogeneity to give a clearer context for the analysis. Moreover, the additional details about the problem formulation in Appendix F provide readers with a more in-depth understanding of the problem domain.\n\n> **W2.** Are there stronger correlations between the level of similarity (actual value) to the peak density or other statistics of the gradients?\n\n**Answer:**\n- Thank you for your question. We have attempted various statistics, including the means, variances, differences and covariances between gradients from the same layer within different environments. However, none of these statistics have exhibited a stronger correlation with the similarity of gradients compared to the smoothness. We have updated these results in Figure 16 and Appendix G.5. Therefore, we select correlation with smoothness to reflect that the shallow layers have higher similarity and smoother gradient distributions. Based on this observation, we propose InCo Aggregation.\n- Furthermore, we have updated an analysis for the correlation between the similarity and smoothness for ViTs in Appendix G.3. This supplementary analysis further supports the observation of a correlation between similarity and smoothness in ViTs as well.\n\n> **W3.** There doesn't seem to be an explicit definition of \"deep\" vs. \"shallow\" layers.\n\n**Answer:**\n- Thank you for pointing out this issue. \"A deep layer\" refers to the deep layer within a stage (block) in our paper, with the exception of Figure 1. We would like to clarify that in Figure 1, the term \"deep layer\" is indeed the deep layer of the entire model, because Fig 1 is an illustrative example that facilitates the reader's understanding of the distinct properties exhibited by shallow and deep layers. Except for Figure 1, \"a shallow layer\" denotes the first layer within a stage of ResNets, and \"deep layers\" refers to the rest of layers within this stage. The following analyses in Section 2 indicate that the layers within the same stage exhibit similar patterns to the layers throughout the entire model. Moreover, we have updated the paper and clarified these concepts in Section 2.\n\n\n> **W4.** Given the specific model heterogeneity settings under which InCo Aggregation is applicable, a discussion on how and where it isn't applicable would be useful.\n\n**Answer:**\n- Thank you for your feedback and suggestions. Currently, InCo Aggregation is applicable to CNNs (ResNets) and Vision Transformers (ViTs) architectures, but we have not yet explored its application to other network structures, such as LSTM. Furthermore, InCo Aggregation is primarily focused on addressing the issue of model heterogeneity within the same family of backbones with varying depths and widths. It is not suitable for handling model heterogeneity across completely different model families, such as a combination of CNNs and ViTs. This aspect requires further research. We have updated and discussed these limitations of InCo Aggregation in Appendix I.\n- However, regarding the problem of model heterogeneity across completely different architectures, we have identified two potential directions that could be explored in the context of InCo Aggregation. The first direction involves leveraging public datasets to perform knowledge distillation, where the knowledge from different model architectures is distilled onto one or several server models with the same architecture (more server models can store more knowledge). InCo Aggregation can then be performed on these server models to connect clients with different model architectures. This direction is similar to the approach taken in FedDF [1].\n\n[Our response continues.]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700390969439,
                "cdate": 1700390969439,
                "tmdate": 1700391477420,
                "mdate": 1700391477420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LB5SQSc0QJ",
                "forum": "Cc0qk6r4Nd",
                "replyto": "sJ08D96EM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e8EV [2/4]"
                    },
                    "comment": {
                        "value": "- The second method involves utilizing hypernetworks to generate parameters for corresponding layers. Different model architectures can be connected through hypernetworks, similar to how different input dimensions can be transformed into the same dimensions of layer parameters through hypernetworks. Similar to the previous direction, multiple server models can be generated using hypernetworks, followed by InCo Aggregation on the generated server models. Currently, there are several papers focusing on personalized federated learning that utilize hypernetworks for generating weights, such as [2], [3], and [4]. However, we have not yet come across any methods using hypernetworks for addressing model heterogeneity in federated learning.\n- These are preliminary ideas to make InCo Aggregation applicable to model heterogeneity across completely different backbones. However, further in-depth research and experimental analysis are required to explore these directions fully.\n\n\nQuestions:\n\n> **Q1.** Could the authors define the notion of system level heterogeneity?\n\n**Answer:**\n- Thank you for your question. Please refer to the response regarding weakness 1.\n\n> **Q2.** Could the authors provide gradient plots like in Figs 2 and 3 for Stage 2's layers as well?\n\n**Answer:**\n- Thank you for bringing this issue to our attention. We have updated the results for stage 2 in Appendix G.4. In contrast to the gradient distributions of stage 3, the differences in gradient distributions across different layers are less evident for stage 2. This can be observed from Figure 1a, where the CKA similarity for stage 2 is considerably higher than that of stage 3. The higher similarity indicates that stage 2 is relatively less biased and more generalized compared to stage 3, resulting in less noticeable differences in gradient distributions. This observation further supports the relationship between similarity and smoothness, as higher similarity leads to smoother distributions.\n\n> **Q3.** Could the authors provide an explicit definition of which layers can be considered deep vs.shallow? \n\n**Answer:**\n- Thank you for your question. Please refer to the response regarding weakness 3. We have updated the paper and clarified these concepts in Section 2. \n\n> **Q4.** Could the authors provide an explanation of whether this pattern carries over to ViTs as well? Could the authors discuss further about similarity patterns in ViT's and how this impact the observations and InCo aggregation as a whole?\n\n**Answer:**\n- Thank you for your comments. We have updated the cross-environment similarity for ViTs in Appendix G.3, along with the analysis of their gradient distributions and the relationships with similarity.\n- Similar to the gradient analyses conducted for ResNets, we have performed the analysis of gradient distributions for ViTs. In our investigation, we have analyzed the outputs from the norm1 and norm2 layers within the ViT blocks and have also applied InCo Aggregation to these layers. The selection of norm1 and norm2 layers is motivated by the significance of Layer Norm in the architecture of transformers [5]. Additionally, we have chosen Block7 and Block11 for analysis as, in the context of heterogeneous models, Block7 is the final layer of the smallest ViTs, while Block11 represents the final layer of the largest ViTs.\n- From Figure 12a and 12b, we observe that the cross-environment similarities derived from the shallow layer norm (norm1) are higher compared to those from the deep layer norm (norm2). Moreover, similar to the analysis conducted for ResNets, we discover that the distributions of norm1 in ViTs exhibit greater smoothness compared to norm2, as depicted in Figure 13 and Figure 14. These findings reinforce the notion that InCo Aggregation is indeed suitable for ViTs.\n\n> **Q5.** Could the authors provide more detail explanations for the exact setups used to generate Fig. 1, 2, and 3?\n\n**Answer:**\n- Thank you for your comments. Due to page limitations, we have put the detailed experimental setups for the case study in Appendix F.1. The comprehensive experimental settings are provided below.\n\n[Our response continues.]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391136403,
                "cdate": 1700391136403,
                "tmdate": 1700391486360,
                "mdate": 1700391486360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zsKl8xrL5r",
            "forum": "Cc0qk6r4Nd",
            "replyto": "Cc0qk6r4Nd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
            ],
            "content": {
                "summary": {
                    "value": "The authors observe that layer similarities are related to accuracy for some FL models. They also observe that layer similarities are related to gradient distribution smoothness. Based on these observations, they modify the learning approach to increase similarities between shallow and deep layer gradients. This modification results in improved accuracy for a variety of FL methods and datasets. They also provide for layer splitting and other engineering necessities to evaluate their ideas, but I found the observations and modification above to be the most interesting and novel."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This is an interesting paper to read.\n\n+ The paper combines experimental approaches to scientific discovery and system engineering to use these discoveries to produce improved accuracy.\n\n+ The visualizations of findings and diagrams are clear.\n\n+ The writing is well organized."
                },
                "weaknesses": {
                    "value": "- The reasons for the relationships observed by the authors are not well explained, although Section 5.4 makes a mostly unsuccessful attempt at doing so. The authors find that high-accuracy models tend to have particular properties and push the learning process to produce those properties without well explaining why the properties result in accuracy. They do, however, demonstrate that their approach works so it's a question of depth of understanding, not merit.\n\n- Section 3.3 seems central to enabling improvement but it is relatively short, without much justification for design decisions. It states what is done but now why this is the most appropriate approach."
                },
                "questions": {
                    "value": "1) Is there a fundamental justification for the form of expression 1, or might other expressions perform as well or better?\n\n2) Why did you decide to simply constrain gopt from opposing g0 (with an inequality) instead of imposing a cost that increases with decreasing dot product?\n\n3) What is the relationship between your findings and those regarding the contribution of residual connections in ResNets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "When searching for related work, I found a paper with the same title and content that list the author names. Is this consistent with the double-blind review rules?\n\nEDIT: The authors gave an explanation that appears to show compliance with conference policies. The conference policy is self-defeating, but the authors shouldn't suffer as a result of that."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1219/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW",
                        "ICLR.cc/2024/Conference/Submission1219/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814251374,
            "cdate": 1698814251374,
            "tmdate": 1700674819380,
            "mdate": 1700674819380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8lCcZjrwsa",
                "forum": "Cc0qk6r4Nd",
                "replyto": "zsKl8xrL5r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e8EV [1/3]"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and constructive review. We now address your main concerns as follows.\n\n Weaknesses:\n\n> **W1.** The reasons for the relationships observed by the authors are not well explained.\n\n**Answer:** \n- Thank you for your feedback regarding this matter. First, we introduce the reasons why high CKA similarities imply better model performance. High CKA similarity suggests a high degree of alignment between the feature spaces of different models. This can be validated by looking at the eigendecomposition in the calculation of CKA similarity [1].\nSuppose $X\\in \\mathbb{R}^{n\\times p}, Y\\in \\mathbb{R}^{n\\times q}$ are centered feature vectors (i.e., with column mean 0).\n$$ \n    \\begin{align} \\text{CKA}_{\\text{linear}}(X,Y)&=\\text{CKA}(XX^\\text{T},YY^\\text{T}) \\\\\\\\\n&=\\frac{\\text{tr}(XX^\\text{T}YY^\\text{T})}{\\sqrt{\\text{tr}(XX^\\text{T}XX^\\text{T})\\text{tr}(YY^\\text{T}YY^\\text{T})}} \\\\\\\\\n&=\\frac{\\sum\\_{i=1}^p\\sum\\_{j=1}^q\\alpha_i\\beta_j\\langle\\mathbf{u}_i,\\mathbf{v}_j\\rangle^2}{\\sqrt{\\sum\\_{i=1}^p\\alpha\\_i^2}\\sqrt{\\sum\\_{i=1}^q\\beta\\_i^2}},\n\\end{align}\n$$\nwhere $\\{\\mathbf{u}_i\\}\\_{i=1}^n, \\{\\mathbf{v}_i\\}\\_{i=1}^n$ are the left-singular vectors of $X,Y$, and $\\alpha_j,\\beta_j$ are the $j$-th largest eigenvalues of $XX^\\text{T}, YY^\\text{T}$ (squared singular value of $X,Y$). Hence, by the weighted inner product of the eigenvectors, CKA tells the amount of variance in X that Y explains (and vice versa), suggesting how similarly the two representations of a data point lie in the space. Then, consider two local models $f(x;\\theta_1),f(x;\\theta_2)$, parameterized by $\\theta_1,\\theta_2$ repectively, who update themselves locally with data $X_1, X_2$. High CKA similarity between $f(x,\\theta_1)$ and $f(x,\\theta_2)$ means that for $x^*\\in X_1\\setminus X_2$, it holds that $f(x^*;\\theta_2)$ still approximates $f(x^*;\\theta_1)$ even if $x^*$ is unseen by $f(x;\\theta_2)$ and $X_2\\setminus X_1$ contibutes to $\\theta_2$'s update. Therefore, InCo Aggregation successfully improves the generalizability of local models and eventually achieves better performance.\n- The discussion above indicates that lower CKA similarity reflects lower similarity in the features generated by the models, resulting in increased bias. These properties highlight a key issue, namely the bias introduced by client models in a model heterogeneity setting, which can stem from variations in model architectures. Therefore, in Section 5.4, we address the bias issue and demonstrate how InCo aggregation achieves higher accuracy by mitigating this bias.\n- In Section 5.4, Figures 9d-9f provide vision representations of how InCoAvg's features exhibit a higher degree of generalization compared to FedAvg and HeteroAvg, which often display biased phenomena stemming from model heterogeneity. Furthermore, Figure 10a quantitatively describes the efficacy of InCoAvg in promoting model generalization using the CKA similarity measure. Specifically, InCoAvg exhibits higher CKA similarity in deep layers compared to FedAvg and HeteroAvg. The results obtained from Figures 9d-9f and 10a validate that the utilization of internal cross-layer gradients effectively facilitates the transfer of generalized knowledge from shallow layers to deep layers. This transfer mechanism helps alleviate the bias issue within models, consequently improving their overall performance.\n\n> **W2.** Section 3.3 seems central to enabling improvement but it is relatively short.\n\n**Answer:**\n- Thank you for bringing up this issue. We introduce the inequality constraint to limit the influence of $g_0$ on $g_{opt}$ because we want to ensure that $g_{opt}$ is primarily driven by the original gradients of the current layer, denoted as $g_k$. Our objective is to find the optimal projective gradients, denoted as $g_{opt}$, which strike a balance between being as close as possible to $g_k$ while maintaining alignment with $g_0$. This alignment ensures that $g_k$ is not hindered by the influence of $g_0$ while allowing $g_{opt}$ to acquire the beneficial knowledge for $g_k$ from $g_0$. In other words, we aim for $g_{opt}$ to capture the advantageous information contained within $g_0$ without impeding the progress of $g_k$. Hence, we employ this simple yet effective constraint. We have added an explanation of this aspect in Section 3.3.\n\n\nQuestions:\n\n> **Q1 & Q2.** Is there a fundamental justification for the form of expression 1? Why did you decide to simply constrain gopt from opposing g0 (with an inequality)?\n\n**Answer:**\n\n- Thank you for your concern. We can show that constraining $g_{opt}$ from opposing $g_0$ (with an inequality) is better than imposing a cost that increases with decreasing dot product, as supported by the following theoretical analysis.\n- Denote constraining $g_{opt}$ from opposing $g_0$ (with an inequality) as $L_1$, i.e., $L_1=||g_k-g_{opt}||\\_2^2,\\,s.t.\\,\\langle g_{opt},g_0\\rangle\\geq0$.\n\n[Our response continues.]"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700390467375,
                "cdate": 1700390467375,
                "tmdate": 1700392341486,
                "mdate": 1700392341486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T1CH0Qpfrx",
                "forum": "Cc0qk6r4Nd",
                "replyto": "zsKl8xrL5r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e8EV [2/3]"
                    },
                    "comment": {
                        "value": "- Denote imposing a cost that increases with decreasing dot product as $L_2$, i.e., $L_2=||g_k-g_{opt}||\\_2^2-\\lambda \\langle g_{opt},g_0\\rangle$ for a fixed $\\lambda\\geq0$.\n- Then, we can show that $\\forall \\lambda\\geq0, \\min L_1\\geq \\min L_2$ as follows, which means that $\\min L_1$ always give an upperbound for $\\min L_2$ no matter what the value of $\\lambda$ is. As long as we ensure that $L_1$ is small enough, then $L_2$ is automatically smaller. We abbreviate $g_{opt}$ as $g$.\n\\begin{equation}\n\\begin{aligned}\nL_1&\\geq\\min_{\\substack{g\\text{ s.t. }\\langle g,g_0\\rangle\\geq0}} L_1 \\\\\\\\\n&\\geq \\min_{\\substack{g\\text{ s.t. }\\langle g,g_0\\rangle\\geq0}} ||g_{k}-g||\\_{2}^{2} - \\lambda \\langle g,g_0\\rangle,\\text{ for a fixed }\\lambda\\geq0 \\\\\\\\\n&\\geq \\min_{\\substack{g\\text{ unconstrained}}} ||g_{k}-g||\\_{2}^{2} - \\lambda \\langle g,g_0\\rangle,\\text{ for a fixed }\\lambda\\geq0 \\\\\\\\\n&= \\min_g L_2\n\\end{aligned}\n\\end{equation}\nThe second inequality follows from $\\lambda \\langle g,g_0\\rangle\\geq0$. The third inequality follows from relaxing the constraints of $g$. Specifically, given the inequality constraint $\\min L_1 \\geq \\min L_2$, if we aim to satisfy $\\epsilon \\geq \\min L_2$, where $\\epsilon$ is a suitably small value, it follows that $\\epsilon \\geq \\min L_1 \\geq \\min L_2$. Thus, we can establish the equivalence between $L_1$ and $L_2$ under these conditions.\n- Moreover, the second point is that the form of Expression 1 enables us to determine the value of the leveraging weight $\\lambda$ in the Lagrangian without requiring intricate manual parameter tuning. In fact, the form of $L_2$ corresponds to the form of the Lagrangian of $\\min L_1$, i.e.,\n\\begin{equation}\n\\begin{aligned}\nL(g, \\lambda)=||g_{k}-g||_{2}^{2} - \\lambda g^Tg_0, \\\\\\\\\nL_2(g)=||g_k-g||_2^2-\\lambda g^Tg_0.\n\\end{aligned}\n\\end{equation}\n- In this case, the solution can be derived as $g=g_k+\\lambda g_0/2$. If we use other form, such as $L_2$, the value of $\\lambda$ would become a hyperparameter requiring manual adjustment. However, by utilizing the presented inequality constraint in the paper, we can solve for $\\lambda=-\\frac{2b}{a}$ using the Lagrange dual function, where $a=(g_0)^Tg_0$ and $b=g_k^Tg_0$. This particular approach enables dynamic adjustment of the $\\lambda$ value based on varying gradients, eliminating the need for manual tuning. The more details about the Lagrangian are explained in Appendix C.\n\n\n\n> **Q3.** What is the relationship between your findings and those regarding the contribution of residual connections in ResNets?\n\n**Answer:**\n\n- Thank you for pointing out this issue. The goal of residual connections is to avoid exploding and vanishing gradients to facilitate the training of a single model [2], while cross-layer gradients aim to increase the layer similarities across a group of models that are jointly optimized in federated learning. Specifically, residual connections modify forward passes by adding the shallow-layer outputs to those of the deep layers. In contrast, cross-layer gradients operate on the gradients calculated by back-propagation. We present the distinct gradient outcomes of the two methods in the following. \n- To illustrate the differences in gradient updates between these two methods, let's consider a simple example involving three consecutive layers of a feedforward neural network, indexed as $i-1, i, i+1$. We will use the notation $f(\\cdot;W_k)$ to represent the calculations in the $k$-th layer. Given the input $\\mathbf{x_{i-2}}$ to layer $i-1$, the output of the previous layer serves as the input to the next layer, resulting in sequential outputs $\\mathbf{x_{i-1}}, \\mathbf{x_{i}}, \\mathbf{x_{i+1}}$.\n$$\n\\begin{align}\n\\mathbf{x_{i-1}}&=f(\\mathbf{x_{i-2}}; W_{i-1}),\\\\\\\\\n\\mathbf{x_{i}}&=f(\\mathbf{x_{i-1}}; W_{i}),\\\\\\\\\n\\mathbf{x_{i+1}}&=f(\\mathbf{x_{i}}; W_{i+1}). \n\\end{align}\n$$\n- As illustrated in Figure 11 in Appendix B, there is an additional operation that directs $\\mathbf{x_{i}}$ to $\\mathbf{x_{i+1}}$, formulated as $\\mathbf{x_{i+1}'}=\\mathbf{x_{i+1}}+\\mathbf{x_{i}}$ in the case of residual connections. The gradient of $W_{i}$ is\n\\begin{aligned}\n        g_{W_i}=\\frac{\\partial loss}{\\partial(\\mathbf{x_{i+1}}+\\mathbf{x_{i}})}\n        \\cdot \\left(\\frac{\\partial \\mathbf{x_{i+1}}}{\\partial W_i}+\\frac{\\partial \\mathbf{x_{i}}}{\\partial W_i}\\right).\n    \\end{aligned}\n- In the case of cross-layer gradients, the gradient of $W_{i}$ is \n\\begin{aligned}\n        g_{W_i}=\\frac{\\partial loss}{\\partial \\mathbf{x}_{i+1}}\\cdot \\left( \\frac{\\partial \\mathbf{x\\_{i+1}}}{\\partial W\\_i}+\\frac{\\partial\\mathbf{x\\_{i+1}}}{\\partial W\\_{i-1}}\\right).\n    \\end{aligned}\n- Hence, it becomes evident that distinctions emerge between our proposed method and residual connections employed in ResNets, as demonstrated through this example. We further elaborate this issue with more comprehensive insights in Appendix B.\n\n[Our response continues.]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700390578546,
                "cdate": 1700390578546,
                "tmdate": 1700392355278,
                "mdate": 1700392355278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LIgJp2jLlG",
                "forum": "Cc0qk6r4Nd",
                "replyto": "m6i5mnahU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
                ],
                "content": {
                    "title": {
                        "value": "Understood"
                    },
                    "comment": {
                        "value": "The policy does appear to permit revealing authorship in this manner. It's a rather odd policy, because it defeats the purpose of blind review, i.e., any authors who want to reveal their identities to the reviewers can do so by concurrently releasing the paper under review on arXiv. Competent reviewers are very likely to find that paper when reviewing. However, that is a problem with the conference policy and not with the authors of this paper, so I don't think the authors should suffer any negative consequences in this case."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674652752,
                "cdate": 1700674652752,
                "tmdate": 1700674652752,
                "mdate": 1700674652752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qnP088OdeW",
                "forum": "Cc0qk6r4Nd",
                "replyto": "T1CH0Qpfrx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1219/Reviewer_vLVW"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for all your other clarifications. They are quite helpful."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1219/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674801477,
                "cdate": 1700674801477,
                "tmdate": 1700674801477,
                "mdate": 1700674801477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]