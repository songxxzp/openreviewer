[
    {
        "title": "Ask Your Distribution Shift if Pre-Training is Right for You"
    },
    {
        "review": {
            "id": "jQ3Ro12KLS",
            "forum": "7LZjuA4AB2",
            "replyto": "7LZjuA4AB2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_Uetj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_Uetj"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates why does initialization with a pretrained model improves performance on some tasks but not on others?\nThey argue that a model that is trained from scratch cannot perform well on examples that are not in the support of the training distribution, and argue pretraining can help on such out-of-support instances.\nPretraining, however, as they show cannot help with systematic biases (spurious correlations) in the training data. By combining initialization with a pretrained model and training with a balanced dataset that is free of spurious correlations, we get best of both the worlds. \n\nThe paper is mostly easy to follow but I was not surprised by their results, i.e. it did not improve my understanding of the problem or offer a novel practical advice."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The focus of the paper is practically very relevant.\n- The presentation is somewhat easy to follow"
                },
                "weaknesses": {
                    "value": "- **Unclear Contributions**. The contribution of the paper is unclear. Both theoretical and empirical contributions are mild. See next point for comment on theoretical analysis. The paper suggested to use initialization with a pretrained model and training on a balanced dataset, which is a standard practice anyway.\n- **Limited theoretical contribution.** Theoretical analysis considered a very simple setting with an intuitive result. The result is too simple to inform poor extrapolation of random initialization (or training from scratch) or better extrapolation of pretraining in practice. \n- **Vague or inconsistent definition of support**. Out-of-support and in-support are not formally defined. From their analysis and examples, the definition of out-of-support are examples with zero support (or probability) attributed by the input pdf. However, their experiments in Section 4.2 classified examples in and out-of-support using a classifier that is trained to classify between two distributions instead of estimating and using a PDF. \n- **Presentation issues**. ER is a central measure used in the paper, but is not defined. How does the y axis label of Figure 5 (right) relate to MG defined in (4)?\n- **Practical implications are not well argued**. Their takeaways may not be practically relevant. In practice, the distribution shift is likely a mix of \"out-of-support\" (whatever that means) and minority subpopulation. The takeaway of the paper that pretraining helps on out-of-support examples do not bear any practical significance in how we train and deploy a model on the target distribution."
                },
                "questions": {
                    "value": "**Q1** In Figure 2 (left), if the training data only has pictures of _indoor dogs_ and _outdoor cats_, then are the examples of _outdoor dogs_ and _indoor cats_ inside or outside the support of the training distribution? \n\n**Q2** In Figure 6(b), how does finetuning a pretrained model on female only dataset (i.e. female only examples from the original dataset) compare with the other results?\n\nMinor: In figure 3, best to clarify that the cat and dog thumbnails are only placeholders and not from CIFAR-10."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766281006,
            "cdate": 1698766281006,
            "tmdate": 1699636588654,
            "mdate": 1699636588654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3i8LUKXo19",
                "forum": "7LZjuA4AB2",
                "replyto": "jQ3Ro12KLS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback.\n\n**Unclear contributions**\n\nThe reviewer states that \u201cthe paper suggested to use initialization with a pretrained model and training on a balanced dataset, which is a standard practice anyway.\u201d While training on balanced data has indeed been suggested by previous work, our work (in Section 6) suggests that, as a principle, this balanced dataset can be small and even non-diverse when fine-tuning a pre-trained model. We believe that this intuition is valuable for informing dataset curation processes.\n\nOur work also offers an understanding of why pre-training in conjunction with a method for handling bias (such as balancing a dataset) is effective for robustness, which has not been previously studied to our knowledge.\n\n**Limited theoretical contribution**\n\nPlease refer to the general response.\n\n**Vague or inconsistent definition of support**\n\nAs noted by the reviewer, in-support and out-of-support examples are examples inside and outside the support of the reference distribution, respectively. In other words, an example is in-support if its probability density under the reference distribution $p_\\text{ref}$ is small. To divide a shifted distribution into in-support and out-of-support splits, we instead estimate the probability density ratio $p_\\text{ref}/p_\\text{shift}$ and find examples where this quantity is small. The reviewer was confused that this is a different quantity than $p_\\text{ref}$. As we explain and justify in Appendix B.3.1, we do so because estimating $p_\\text{ref}$ is difficult for high-dimensional inputs (in our case, images) and $p_\\text{ref}/p_\\text{shift}$ is similarly meaningful. \n\n**Presentation issues**\n\nWe are surprised by the reviewer\u2019s comment that *effective robustness* (ER) is not defined, as it is defined in the  Section 2. To further clarify this metric, we added a longer definition in the Appendix in the revision.\n\n**Practical implications are not well argued**.\n\nPlease refer to the general response.\n\n**Q1**\n\nIf the training data only has pictures of indoor dogs\u00a0and\u00a0outdoor cats, then examples of\u00a0outdoor dogs\u00a0and\u00a0indoor cats are out-of-support. Here, we expect that a model could suffer both from biases and from poor extrapolation to this previously unseen group.\n\n**Q2**\n\nWe thank the reviewer for raising this point. In experiments that are not currently included in the paper, we found that fine-tuning a pre-trained model on a female only dataset is ineffective (the model still performs poorly on blond males). This might be because there are other attributes spuriously correlated with hair color even among just the female population, and this is why we opt for counterfactual editing. We will include this ablation in future revisions.\n\nWe would like to note that we are not claiming that counterfactual data is the only or most effective way to de-bias a dataset. The purpose of this experiment is to illustrate that de-biasing a small dataset and fine-tuning a pre-trained model on it can be effective, regardless of the de-biasing method."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529912342,
                "cdate": 1700529912342,
                "tmdate": 1700529912342,
                "mdate": 1700529912342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDOdm4u3b4",
            "forum": "7LZjuA4AB2",
            "replyto": "7LZjuA4AB2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates an interesting problem of the impact of pre-training data on the model's robustness to distributional shifts. The work proposes to characterize the \"failure modes\" of the model under distributional shifts into two types\u2013poor extrapolation and dataset biases. The work then argues that pre-training can help with the first but not the second. This paper proposes two approaches to address these issues\u2013use intervention techniques at pre-training to prevent exploiting biases; and fine-tune on small, non-diverse but debiased datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem being investigated is definitely of interest. With the emergence of foundation models, it is of growing interest to better understand the impact of pre-training data and its implications for downstream processes.\n \nThis paper is well-contextualized. Its technical structure is plausible (intuitions, motivating examples, formal analysis, generalization, empirical verification, etc.)."
                },
                "weaknesses": {
                    "value": "This paper is a difficult read in general. I was quite attracted by the topic of this paper and had high hope until Theorem 1, which I could not understand after several attempts. **Many technical details are missing or inconsistent (e.g., missing key definitions, no details for important procedures, only providing references with no description at all), rendering many arguments ungrounded and hardly convincing.**\n\n- Theorem 1, key error\u2013$w_{ref}$ is undefined, which is a crucial variable. With this, I can only guess about this theorem. Assuming it is correct in its own sense, the conclusion is problematic. The theorem only gives that pre-training/initialization affects ID but not OOD. Yet, this affect can be either positive or negative,meaning pre-training/initialization does not necessarily help in every case\u2013which I think is true in practical cases. This work has then taken it for granted that pre-training WILL HELP downstream robustness and built the arguments on this as a main basis. **I think there is a major gap.** It is necessary to specify the conditions when it helps/when it hurts/and when it does not affect, which is actually the most valuable part.\n\n- Another fundamental issue for Theorem 1 is that the model is considered deterministc. Even if you use random initialization and stochastic gradient solvers, you end up with the same solution. It is natural to use a stylized model as a starting point to build the analysis. Then I would expect to see how this analysis generalizes to the case of non-convex models where there is inherent learning stochasticity. Yet, without providing anything else, the paper jumps to experiments that are all based on neural network models. **This is another major gap.** Actually, there is not really a \"pre-training\" thing for convex models\u2013regardless of the order you feed data to the model, it always converges to the same optimal solution. The resulting model solely depends on the training data and is irrelevant to initialization or the training process. \n\n- I don't understand why the proposed \"in-support shifts\" would change the classification boundary at all. Image a max-margin classifier (e.g., SVM) and a binary classification task for cat and dog images. (Note that Logistic Regression requires the same probability for both classes. You cannot directly apply it to unbalanced classification problems.) Regardless of the relative proportion for cat or dog images, the underlying distribution for cats and dogs is invariant. A proper classifier would find the decision boundary somewhere between the generating distributions for both classes. I don't see why this is considered \"not robust\". Or a more important question\u2013what is the robustness considered in this work? **The definition for the central notion of this work is not provided.**\n\n- I don't understand the splitting methods for partitioning sketch images into ID and OOD subsets w.r.t. ImageNet. The paper only describes it based on whether they \"look like\". This process actually sounds rather non-trivial. The paper refers to Appendix B 3.2 for details, **which does not exist.**\n\n- For the proposed approaches\u2013use intervention techniques at pre-training to prevent exploiting biases; and fine-tune on small, non-diverse but debiased datasets. The paper merely cites existing works for these techniques without any description. **This renders this work not self-contained and incomplete and this cannot be counted as technical contribution of this work.**\n\n- Format: Appendix is not cut from the main paper. The PDF provided for the main paper is this 32-page document."
                },
                "questions": {
                    "value": "- Theorem 1, key variable $w_{ref}$ is undefined.\n\n- Appendix B 3.2, which is referred to in Page 6, does not exist.\n\n- Appendix should not be submitted under the main paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779060650,
            "cdate": 1698779060650,
            "tmdate": 1699636588513,
            "mdate": 1699636588513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gzuKT28dPY",
                "forum": "7LZjuA4AB2",
                "replyto": "sDOdm4u3b4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback.\n\n**Concerns about Theorem 1**\n\nThe reviewer expressed confusion about $w^*_\\text{ref}$ being undefined. We state that $w^*_\\text{ref}$ does not depend on the initialization $w_\\text{init}$. We also state that it is a property of the reference dataset. In the revision, we clarify this when we first refer to $w^*_\\text{ref}$.\n\nThe reviewer correctly points out that pre-training can in fact hurt robustness, which we discuss in Appendix D.2. Indeed, this is reflected by our theoretical analysis, which suggests that pre-training can influence a model\u2019s extrapolation, not that it will necessarily help (this is why we use the language \u201ccan help with extrapolation\u201d). See the general response for more discussion.\n\nWhile the model in Theorem 1 is deterministic, crucially, it depends on initialization (in our case, whether the model is pre-trained or randomly initialized). Even though the loss is convex, there are multiple equally good solutions and the initialization specifies which is selected. Pre-training thus can affect the learned model, even in this convex setting.\n\nSee the general response for concerns about the simplicity of the theoretical analysis.\n\n**Robustness to in-support shifts**\n\nWe are confused by the reviewer\u2019s comments about models being agnostic to in-support shifts. Consider [1], for example.\n\n**Definition of robustness**\n\nIn our work, we give both an intuitive and formal definition of robustness. Intuitively, as described in our introduction, we consider the robustness of a method to be its ability to succeed on a shifted distribution that differs from the reference distribution the model is trained on. \n\nFormally, we define and evaluate the robustness of a model via the *effective robustness* metric [2] (we define this metric in Section 2). To further clarify this metric, we added a longer definition in the Appendix in the revision.\n\n**Splitting Method**\n\nWe are unsure why the reviewer is unable to view Appendix B.3.2. We have opened and download the pdf on multiple browsers, and in each case the Appendix is included in the file. We also note that, as we state in the main text in Section 4.2, we actually give the full details in the splitting method in Appendix B.3.1 and not B.3.2.\n\n**Using Interventions to prevent exploiting biases**\n\nWe would like to point the reviewer to Appendix B.4. Here, we indeed give a description of the *Deep Feature Reweighting* (which we assume the author is referring to) and also carefully describe our implementation of this method. Our contribution here is not a specific approach, but rather a guiding principle of combining pre-training with an intervention that can handle biases. Thus, we did not consider the details of the specific choice of this intervention to be worth including in the main paper.\n\n**Location of the Appendix**\n\nWe refer the reviewer to the ICLR 2024 call for papers and author guidelines:\n\n- The ICLR 2024 author guide states: \u201cWe encourage authors to submit a single file (paper + supplementary text). Please mark the supplementary material clearly.\u201d\n- The ICLR 2024 call for papers states: \u201cAuthors may use as many pages of appendices (after the bibliography) as they wish.\u201d\n\n[1] Chaudhuri et al. \u201cWhy does Throwing Away Data Improve Worst-Group Error?\u201d (2023).\n\n[2] Taori et al. \u201cMeasuring Robustness to Natural Distribution Shifts in Image Classification.\u201d (2020)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529861119,
                "cdate": 1700529861119,
                "tmdate": 1700529861119,
                "mdate": 1700529861119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cVUcKD4k8O",
                "forum": "7LZjuA4AB2",
                "replyto": "gzuKT28dPY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the rebuttal. I have read it in full. My main concern about the theorem and conceptual results remains. I cannot agree with the arguments on convex model having multiple equally good solutions\u2013this only happens in degenerate cases (consider linear program as an example). It is unlikely for convex optimization problems with continuous variables (excluding linear programs) to have multiple GLOBAL optimal solutions which have exact same optimal values. More often than not, multiple optimal solution only happens when the final solution is on the decision boundary\u2013in which case the results are often trivial.\n\nThe case with multiple optimums is for nonconvex optimization, where one cannot find a global optimum in polynomial time and has to reduce to LOCAL optimums. Which local optimum one ends up with will depend on its initialization and solution trajectory."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726468566,
                "cdate": 1700726468566,
                "tmdate": 1700726468566,
                "mdate": 1700726468566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xnFsvCP4xW",
            "forum": "7LZjuA4AB2",
            "replyto": "7LZjuA4AB2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to study the effectiveness of model pre-training under various kinds of distribution shifts. A key insight of the paper is that pre-training can help address poor extrapolation but not dataset biases. The paper motivates this insight theoretically and demonstrates the expected behavior empirically in a variety of experimental setups. These include simulated synthetic and real-life shifts, as well as a case study on dataset de-biasing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses the very interesting topic of exploring whether pre-training helps training performant models under different types of distribution shift. This is an important frontier in the increasingly adopted pre-training fine-tuning training methodology.\n- The ideas presented seem original and the relevant literature is sufficiently discussed.\n- The later sections on developing more robust models were interesting case studies on how to apply the insights from previous sections."
                },
                "weaknesses": {
                    "value": "- Although more details are presented in the Appendix, I don't think that the setup for Theorem 3.1 is presented well in the paper. It is not clear why the logistic regression assumption is needed and the proof is not referenced in the main paper. It is also unclear what $proj_{W_{ref}}$ refers to as it is never formally introduced. Is orthogonal to be interpreted mathematically or figuratively? Also \"[...] while the initialization determines how the model extends outside of $W_{ref}$\": since $W_{ref}$ is contained in $proj_{W_{ref}}$. It appears to me like both terms are influenced by $W_{ref}$, not just the first term.\n- Figure 1's legend is unreadable due to overlapping text. I was still able to get the intuition but the authors should fix this. \n- Although Figure 3 presents two examples of in-support and out-of-support shifts, there is no ablation on the shift intensity. I wonder to what extent the shift type influences shifted accuracy and how the shift intensity for a fixed type of shift would alter the experimental results. In other words: does the strength of the bias or the degree of extrapolation matter? My intuition is that this should also play a role. Connecting to this, both in-support and out-of-support shifts are also not formally defined. The descriptions given at the beginning of section 4 should be made more precise.\n- The negative correlation reported in the middle panel of Figure 5 is negative but at the same time the correlation of -0.112 is weak, suggesting that the margin gains are independent of each other rather than complimentary. \n- Overall, the take-away message from this work is a bit unclear to me. While the main paper suggests that pre-training is always desirable (with larger gains for out-of-support shifts than for in-support shifts), Appendix D.2 discusses the possibility of harmful biases being instilled into the model during pre-training. Without assumptions about which distribution to expect at test time (which is what we would want to fine-tune for), it therefore becomes impossible to understand whether a model should have been pre-trained or not. This makes it hard for this method to be applied in practice."
                },
                "questions": {
                    "value": "Embedded in Weaknesses above.\n\nI am willing to increase my score as part of the discussion phase if the authors can address my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818998569,
            "cdate": 1698818998569,
            "tmdate": 1699636588407,
            "mdate": 1699636588407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v4GMvATRIq",
                "forum": "7LZjuA4AB2",
                "replyto": "xnFsvCP4xW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. Please see general response for a discussion on:\n\n- Takeaways\n- Cases where pre-training hurts robustness\n- Correlation of margin gains\n\nBelow, we address the reviewer\u2019s remaining concerns:\n\n**Notation in Section 3**\n\nTo clarify the reviewer\u2019s confusion about Theorem 1:\n\n1. $proj_{W_\\text{ref}}$ is the projection operator onto the subspace $W_\\text{ref}$. We apologize for the confusion and clarify this in the revision.\n2. Orthogonal is to be interpreted mathematically; $w_\\text{init}-proj_{W_\\text{ref}}w_\\text{init}$ is orthogonal to $W_\\text{ref}$ because we remove the component that lies within $W_\\text{ref}$. This is also why this second term would not influence the model\u2019s output on any point within $W_\\text{ref}$ (because the product of this point and $w_\\text{init}-proj_{W_\\text{ref}}w_\\text{init}$ would be zero).\n3. The proof is in Appendix A and is referenced in the paper.\n\n**Overlapping text in Figure 1**\n\nWe are confused by the author\u2019s comments about the overlapping text in Figure 1. We were unable to reproduce this issue when\n\n1. Downloading the PDF and viewing it locally.\n2. Viewing the PDF in Safari, Chrome, Brave.\n\nWe suggest that the reviewer uses one of the methods above to view the PDF.\n\n**Ablations on shift intensity**\n\nWe thank the reviewer for the suggestion of ablating shift intensity, and intend to include this in the Appendix in future revisions. We already have some intuition of how ablating the fraction of samples from the minority group (e.g., randomly tinted examples in the tint shift) affects the effective robustness of pre-trained models. In our existing experiments, we observe that when the number of samples available from minority groups is small, pre-trained models do have a little effective robustness, even under in-support shifts. However, in Appendix C.1.2, we provide evidence that as we increase the total number of samples (while maintaining the same distribution), these effective robustness benefits vanish. If we were to decrease the fraction of samples from the minority groups while maintaining the total dataset size, we would expect the effective robustness of pre-trained models to increase somewhat, but as we discuss in Appendix C.1.2, this robustness could be attributed to better extrapolation from a small number of samples."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529737874,
                "cdate": 1700529737874,
                "tmdate": 1700626138042,
                "mdate": 1700626138042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iv176G5qA1",
                "forum": "7LZjuA4AB2",
                "replyto": "v4GMvATRIq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal which has clarified some of my concerns. However, I am unfortunately still not convinced that the presented characterization of in-support and out-of-support shifts are the main directions on which to categorize whether pre-training is helpful or not. As a result, I will not be able to change my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710893117,
                "cdate": 1700710893117,
                "tmdate": 1700710893117,
                "mdate": 1700710893117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vF0FkIiMDF",
            "forum": "7LZjuA4AB2",
            "replyto": "7LZjuA4AB2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the robustness of classifiers, based on pre-training, to distribution shifts. It provides with a chacterization for distribution shifts based on whether the network is asked to extrapolate (out-of-support) or generalize in a group-balanced manner when trained with data which are label-imbalanced or are generated from spuriously correlated features (in-support). They claim that pretraining helps with the former type, but not the latter; providing some theoretical insights in a very simplified setting and performing various ablating experiments. In order to deal with the second type, they argue that pretraining needs to be combined with group-robustness methods, such as methods based on loss reweighting or data rebalancing; and that this strategy provides with complementary robustness benefits (to both proposed types). They conclude by encouraging \u201cpractitioners not to treat pre-training as a panacea for robustness\u201d."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow and for the most part well-written. While the paper does not propose novel methods for robustness, it leads an important discussion on the interplay of pre-trained networks with training methods for group-robustness. It provides with a novel characterization of distribution shifts and it argues about the complementary utility of the two approaches around this characterization, with some theoretical and empirical insights. The reviewer believes that such an analysis paper is needed in light of recent advances in the studied literature and that the authors have identified well that this is a topic of interest.\n\nThe reviewer appreciates the careful discussion in defining and designing in-support and out-of-support shifts, and they believe it is a potentially useful axis of discussion."
                },
                "weaknesses": {
                    "value": "While their conclusion might not be incorrect, the design of several experiments is flawed and does not lead to the authors\u2019 claims. The reviewer thinks that these consist a large enough body of the paper to lean towards possible acceptance. In particular:\n\n1.  In **Section 4/Figure 3**, there are multiple variables which are being ablated at the same time. The measured effective robustness is with respect the performance of ResNet18 models, however the pretrained models, which are compared to, have various architectures. It is not clear whether the perceived robustness is due to purely transfer properties from pretraining strategies or from larger model size.\n2. Biased datasets are also susceptible to an *in-support*/*out-of-support* analysis, however one is not given at the study. Specifically, one can imagine the extreme case where some minority group probabilities go to 0, yielding them completely *out-of-support*. For example, imagine the case in **Figure 2 (right)**, where cats are also observed during the night. How does this impact the pretraining algorithms? Current analysis seems to suggest that pretraining would be able to handle the more extreme cases better, which is counter-intuitive. I suggest studying systematic generalization tasks, where some combinations of generative attributes are completely not represented in the training set, but they exist in the test set in a balanced manner. See [1,2].\n3. In **Section 5/Figure 5**, the first correlation estimate is not strong enough to indicate negative correlation of (i) (and thus complementary effects between pretraining and debiasing methods). Please provide with a confidence interval of the correlation estimate, as it seems statistically possible that it is very close to 0. On the other hand, the correlation of (ii) is poorly motivated and it can be tautologically positive, in which case it is of no logical inference value.\n4. In **Section 6**, the proposed form of curated dataset is made so that the spurious feature which is balanced during test-time is completely omitted, namely the \u201cgender\u201d attribute of CelebA. If there is no spurious correlated feature during training, then the transfer problem corresponds to an extrapolation one, for which probably a classifier with simple augmentation/regularization might just work. The construction of counterfactual data, beyond being difficult to achieve in most cases, needs to be ablated to demonstrate that it is a necessary component of the curation process.\n\nFinally, a comparatively minor concern is that the theorerical insight provided corresponds to an overly simplified setting.\n\n[1] Schott, Lukas, et al. \"Visual representation learning does not generalize strongly within the same domain.\" (2021).  \n[2] Tsirigotis, Christos, et al. \u201cGroup Robust Classification Without Any Group Information.\u201d (2023)."
                },
                "questions": {
                    "value": "In **Figure 4**, what % of samples are found to be *in-support* for each of the test sets considered? How is absolute test accuracy (iid and ood) affected as we ablate this? In other words, would a test set comprised 100% of *in-support* samples achieve similar absolute accuracy as an iid test set?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820772561,
            "cdate": 1698820772561,
            "tmdate": 1699636588298,
            "mdate": 1699636588298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "En35Cm1AfB",
                "forum": "7LZjuA4AB2",
                "replyto": "vF0FkIiMDF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback.\n\n**Measuring the robustness of pre-trained models with various architectures with respect to ResNet-18 models trained from scratch (1)**\n\nThe reviewer expressed concerns that we measure the robustness of pre-trained models with various architectures with respect to ResNet-18 models trained from scratch. We would like to point out that the pre-trained models include ResNet-18s (pre-trained on ImageNet) in addition to other, larger architectures, so there is already a direct comparison between models with the same architecture. That said, in future revisions we will augment this result by adding models trained from scratch with ResNet-50 and DenseNet-121 architectures (we also tried ViTs, but these tend to perform poorly when trained from scratch on small datasets [1]).\n\n**Biased datasets with zero minority sub-group probabilities (2)**\n\nWe thank the reviewer for pointing out that there might also be biased datasets in which the minority group probabilities are close to zero. This is an important point that we intend to clarify in future revisions. Indeed, in this extreme case a model would suffer from the bias. However, a model could also suffer from poor extrapolation, since the minority groups represent a previously unseen setting. We do not categorize such a shift as in-support for this reason.\n\nThe reviewer says that \u201ccurrent analysis seems to suggest that pre-training would be able to handle the more extreme cases better, which is counter-intuitive\u201d. We agree that this result as stated by the reviewer would be counter-intuitive; however, this is not what our current analysis suggests. Our analysis suggests that pre-training can help with extrapolation, implying that a pre-trained model might be more robust *than a model trained from scratch under the same extreme case*. Notably, the analysis does not suggest that a pre-trained model would perform better in this extreme case than in the case where data from the minority group is available (since, indeed, a pre-trained model would likely suffer more from biases).\n\n**Margin gain correlations (3)**\n\nIn the general response, we discuss why, in our view, even uncorrelated margin gains imply that interventions are complementary.\n\nWe are confused about why the reviewer believes that the correlation of (ii) is poorly motivated and might be tautologically positive. In our understanding, for something to be tautologically true means that they mean the same thing. In this case, applying two interventions separately and summing their independent gains and applying them together and measuring the gain are not the same. While some correlation is to be expected, the high positive correlation provides evidence that applying the two interventions together yields the combined benefits of applying each individually.\n\n**Counterfactual data in the curation process (4)**\n\nThe reviewer points out that \u201cif there is no spurious correlated feature during training, then the transfer problem corresponds to an extrapolation one\u201d. This is exactly the purpose of the dataset curation process; since our analysis suggests that pre-training can help with extrapolation, if we can set up a dataset where the problem is just extrapolation then a pre-trained and fine-tuned model might do well. While there may be other strategies besides pre-training that can help with extrapolation, we do not believe that \u201csimple augmentations/regularization\u201d would be an effective solution in general.\n\nWe are not claiming that counterfactual data is the only or most effective way to de-bias a dataset. The purpose of this experiment is to illustrate that de-biasing a small dataset and fine-tuning a pre-trained model on it can be effective, regardless of the de-biasing method. We intend to clarify this in future revisions.\n\n**Question about the in-support and out-of-support splits in Figure 4**\n\nWe report the number of samples found to be in-support and out-of-support in Table 1 in Appendix C.2.1. As the reviewer alludes to, the accuracy on the in-support split is higher than the average accuracy under distribution shift (since the in-support split contains \u201ceasier\u201d examples). We conduct an analysis in Appendix C.2.4 illustrating that, even accounting for this difference in difficulty between the in-support and out-of-support splits, pre-training can help substantially out-of-support but not in-support.\n\n[1] Dosovitskiy et al. \u201cAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\u201d (2021)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529669304,
                "cdate": 1700529669304,
                "tmdate": 1700529669304,
                "mdate": 1700529669304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pc0zy8ZsLb",
                "forum": "7LZjuA4AB2",
                "replyto": "En35Cm1AfB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Rebuttal"
                    },
                    "comment": {
                        "value": "**Measuring the robustness of pre-trained models with various architectures with respect to ResNet-18 models trained from scratch (1)**\n\nThank you for acknowledging that. I am looking forward to further experiments using the same models (and generally ablating one variable at the time)!\n\n**Biased datasets with zero minority sub-group probabilities (2)**\n\nOne can imagine \u201cinterpolating\u201d between the case of systematic generalization (suppose $\\lambda = 0$ - where training prob. of some minority groups is exactly zero) and fully unbiased in their factors training sets (suppose $\\lambda = 1$ - where all groups are equally probable in the training set). Everything else in between ($\\lambda \\in (0, 1)$) corresponds to a training set generated by spuriously correlated attributes. We can see that for $\\lambda > 0$ generalization is in-support, and the extreme case $\\lambda = 0$ generalization is out-of-support. I would like to see the effective robustness that pretraining (ideally with the same training distribution and model) can provide as $\\lambda \\to 0$. I mentioned that according to the insights of your study, effective robustness should increase as we get closer to out-of-support (systematic) generalization; which to me seems as counter-intuitive.\n\n**Margin gain correlations (3)**\n\nAuthors are agreeing by saying \u201cWhile some correlation is to be expected\u201c, to which I ask \u201chow much and why?\u201d.\n\n**Counterfactual data in the curation process (4)**\n\nThere seem to be two independent processes here. One is the agressive filtering, the other is the counterfactual. They need to be ablated separately, in order to assess their utility even as practically unattainable, but theoretically interesting, constructs.\n\n**Question about the in-support and out-of-support splits in Figure 4**\n\nThank you for your answer!\n\nOverall, while I appreciate the time you have taken to respond to the review, I choose to maintain my original assessment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5655/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690609778,
                "cdate": 1700690609778,
                "tmdate": 1700690609778,
                "mdate": 1700690609778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]