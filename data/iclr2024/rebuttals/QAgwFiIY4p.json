[
    {
        "title": "Graph as Point Set"
    },
    {
        "review": {
            "id": "TjTKoYDx8H",
            "forum": "QAgwFiIY4p",
            "replyto": "QAgwFiIY4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for representing graph data as a set of\npoints. The representation is obtained based on symmetric rank\ndecomposition, followed by the application of a permutation-equivariant\nfunction. Subsequently, the resulting features are treated as\na 'coordinate representation' of the graph, and a Transformer model is\nused to address downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Finding novel ways to represent graphs, in particular methods that do\n  not make use of message passing, are advantageous since they often\n  pinpoint novel research directions and permit escaping the WL\n  hierarchy for graph expressivity.\n\n- The method is elegant and grounded in strong theory (SRD)."
                },
                "weaknesses": {
                    "value": "The current write-up is suffering from substantial flaws, which cannot\nbe easily rectified during the conference cycle:\n\n1. Lack of clarity: there are some (minor) issues with notation (for\n   instance, $Z$ is used both as a matrix and as\n   a permutation-equivariant function), but the overall description of\n   the method should be clarified. I needed multiple reads to understand\n   just *how* a graph is transformed and, moreover, understand the\n   algorithmic details of this transformation.\n\n   An overview figure or a brief summary would be immensely helpful\n   here.\n\n2. Lack of experimental depth: given the strong claims made in the paper\n   concerning a new state of the art, a much more detailed experimental\n   setup is required. In particular, a comparison of the different\n   models in terms of the number of parameters (of the model itself) and\n   the number of hyperparameters is required. Given recent work on\n   [improvements in the LRGB data\n   set](https://arxiv.org/pdf/2309.00367.pdf), which are only contingent\n   on hyperparameter tuning, a more detailed explanation of the setup\n   and the comparison is required.\n\nThere are also some minor weaknesses:\n\n- Table 1 is hard to read and understand at first glance. Please either\n  highlight the best/second-best method or refrain from highlighting\n  altogether. Also, why are there no standard deviations for the other\n  methods?\n\n- There are some language issues, which would require an additional pass\n  over the paper (this is a minor point but it nevertheless slightly\n  impacts accessibility)."
                },
                "questions": {
                    "value": "1. How does the proposed approach compare to the work by [Kim et\n   al.](https://arxiv.org/abs/2110.14416), which also employs\n   Transformers for graph learning tasks?\n\n2. What are the contributions of the Transformer architecture to\n   predictive performance? Would a simple set function, realised by an\n   MLP with an appropriate pooling function, work as well? I believe\n   that disentangling and ablating the results would strengthen the\n   paper immensely.\n\n3. What is $X$ in Theorem 1 (and the subsequent text)? I assume that\n   this pertains to the features of the graph?\n\n4. To what extent is the method contingent on the SRD? Would it be\n   possible to use another type of decomposition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698176163110,
            "cdate": 1698176163110,
            "tmdate": 1700641808384,
            "mdate": 1700641808384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QcgRvMINEk",
                "forum": "QAgwFiIY4p",
                "replyto": "TjTKoYDx8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8zdN (1/2)"
                    },
                    "comment": {
                        "value": "We are grateful for Reviewer 8zdN's comprehensive feedback and have addressed their concerns as follows:\n\n1. **W1-1:** Lack of clarity: there are some (minor) issues with notation (for instance, $Z$ is used both as a matrix and as a permutation-equivariant function),\n\nWe appreciate this observation. To resolve this confusion, we have changed the notation from $Z$ to $\\mathcal{Z}$ in our revised manuscript.\n\n2. **W1-2:** but the overall description of the method should be clarified. I needed multiple reads to understand just *how* a graph is transformed and, moreover, understand the algorithmic details of this transformation. An overview figure or a brief summary would be immensely helpful here.\n\nTo aid in understanding, Figure 1 provides a comprehensive overview of our method. Additionally, Figure 3 in Appendix G summarizes our Point Set Transformer (PST), which we hope will further clarify our approach.\n\n3. **W2** Lack of experimental depth: given the strong claims made in the paper concerning a new state of the art, a much more detailed experimental setup is required. In particular, a comparison of the different models in terms of the number of parameters (of the model itself) and the number of hyperparameters is required. Given recent work on improvements in the LRGB data set [1], which are only contingent on hyperparameter tuning, a more detailed explanation of the setup and the comparison is required.\n\nThank you for the insightful comment. We have updated our manuscript to include more comprehensive experimental details, as you suggested. The updates are reflected in Appendix E, where we provide information about both the number of parameters in the models and the hyperparameters used in our experiments.\n\n1. Parameter Budget Across Datasets: \n   - For the zinc, zinc-full, and pascalvoc-sp datasets, we adhered to the baseline parameter budget constraint of approximately 500k parameters.\n   - On the peptide-func and peptide-struct datasets, we mistakenly doubled the number of parameters. This occurred because our PST employs two sets of parameters (one for scalar and one for vector), which resulted in twice the parameters with the same hidden dimension and number of transformer layers. We are currently conducting experiments with our PST with smaller hidden dimension and baseline with larger hidden dimensions for these datasets. The results are as followings. Compared with representative graph transformers, our PST performs worse than baselines with parameter 500 K. However, when the PST is scaled to 1M parameters, its performance becomes comparable to, or even surpasses, that of other baseline models like Graph-MLPMixer and GraphGPS. Therefore, our PST can still outperforms baselines with the same parameter budget 1M, and our method is effective on the two datasets.\n\t| | peptide-func | peptide-struct |\n\t| --- | --- | --- |\n\t| \\#param=500K | | |\n\t| Graph-MLPMixer | $0.6970_{\\pm0.0080}$ | $0.2475_{\\pm0.0015}$ |\n\t| GraphGPS | $0.6535_{\\pm0.0041}$ | $0.2500_{\\pm0.0005}$ |\n\t| PST | $0.6344_{\\pm0.0041}$ | $0.2543_{\\pm 0.0014}$ |\n\t| \\#param=1M | | |\n\t| Graph-MLPMixer | $0.7004_{\\pm 0.0040}$ | $0.2503_{\\pm 0.0031}$ |\n\t| GraphGPS | $0.6561_{\\pm 0.0027}$ | $0.2557_{\\pm 0.0010}$ |\n\t| PST | $0.6984_{\\pm0.0051}$ | $\\mathbf{0.2470_{\\pm 0.0015}}$ |\n   - Other datasets we explored do not have explicit parameter constraints, and it's worth noting that our PST has fewer parameters compared to representative baselines in these cases.\n\n2. Hyperparameter Configuration: \n   - Our experiments involved tuning seven hyperparameters: depth (4, 8), hidden dimension (64, 256), learning rate (0.0001, 0.003), number of warmup epochs (0, 40), number of cosine annealing epochs (1, 20), magnitude of Gaussian noise added to input (1e-6, 1e-4), and weight decay (1e-6, 1e-1).\n   - We observed that [1] also used seven hyperparameters in their setup.\n   - Batch size was determined based on GPU memory constraints and was not a parameter that we optimized.\n\n[1] Jan T\u00f6nshoff et al. Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. https://arxiv.org/pdf/2309.00367.pdf\n\n\n4. **W3** Table 1 is hard to read and understand at first glance. Please either highlight the best/second-best method or refrain from highlighting altogether. Also, why are there no standard deviations for the other methods?\n\nIn the revised manuscript, we have clarified that highlighting in Table 1 indicates models capable of counting the corresponding substructure (test loss < 10 units). This is now explicitly stated for better understanding. As for the lack of standard deviations in other methods, those results are directly sourced from their original papers, which did not provide standard deviations."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408900965,
                "cdate": 1700408900965,
                "tmdate": 1700464863278,
                "mdate": 1700464863278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kg3xu0fWqJ",
                "forum": "QAgwFiIY4p",
                "replyto": "Z1gHobB7Fx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the extensive rebuttal and apologise for the delay\u2014a colleague passed away unexpectedly and things have been hectic.\n\nIn any case, I appreciate the detailed responses and, conditional on further improvements to clarity (some restructuring so that the most relevant details are in the main paper), I will raise my score accordingly. I believe this has the potential to be a strong contribution to the literature."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641773745,
                "cdate": 1700641773745,
                "tmdate": 1700641773745,
                "mdate": 1700641773745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GVlZIC9Zlr",
                "forum": "QAgwFiIY4p",
                "replyto": "tlOc4Y579j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_8zdN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind words, they are appreciated. At this point, I have no further questions or comments about the work."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731269589,
                "cdate": 1700731269589,
                "tmdate": 1700731269589,
                "mdate": 1700731269589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y6q9tHKzNF",
            "forum": "QAgwFiIY4p",
            "replyto": "QAgwFiIY4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission729/Reviewer_knXN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission729/Reviewer_knXN"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel graph-to-set conversion that transforms interconnected nodes into a set of independent points, which allows seamless application of set encoder architectures such as the Transformer towards graph representation learning. The key is to perform symmetric rank decomposition on the adjacency or related matrices and process the coordinates through an orthogonal-transformation-equivariant set encoder, for which the paper proposes a new architecture, Point Set Transformer (PST). This approach has the benefit of not requiring any positional encodings used in previous graph Transformer literature, and is also theoretically shown to have stronger short-range and long-range expressivity compared to existing baselines. Experiments on substructure counting, molecular property prediction, and the Long Range Graph Benchmark validate the effectiveness of PST on various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- [S1] The perspective of viewing graphs as point sets and performing graph representation learning via a set encoder is very interesting and original, and has great potential across the graph learning community.\n- [S2] The idea is fairly simple and easy-to-follow, yet empirically effective as shown in the presented experimental results."
                },
                "weaknesses": {
                    "value": "- [W1] There are a few details missing in the methodology/experiments that may help to clarify towards better reproducibility.\n  - For each experiment, how is the rank $r$ chosen? Is it chosen via hyperparameter tuning? For larger graphs, it seems choosing a small $r$ will result in loss of information on the connectivity of the input graph. How is it that PST still performs well on Long Range Graph Benchmark despite this potential loss of information?\n  - For the experiments, which matrix was used to generate the generalized coordinates? The adjacency matrix, or normalized adjacency, or some other matrix? The beginning of Section 4 briefly seems to mention that the method mainly uses the adjacency matrix, but this is not clear from the experimental section. Furthermore, an ablation study on which adjacency matrix works well with PST could be another interesting direction that provides further guidance.\n  - After the graph-to-set conversion, how are edge features such as bond types in molecular graphs incorporated into PST? \n- [W2] Another small concern is the computational cost due to use of symmetric rank decomposition. The paper mentions that each layer in PST runs in $\\mathcal{O}(n^2 r)$-time and $\\mathcal{O}(n^2 + nr)$-space, which can be costly if $n$ is large and $r$ must increase proportionally to $n$ in order to cover sufficient connectivity information. Table 10 in the Appendix shows runtime/memory consumption on ZINC, but since ZINC is consisted of fairly small graphs (~23.2 nodes per graph), it is unclear whether PST is scalable to large graphs (e.g. Long Range Graph Benchmark) as well. Could the authors elaborate on this?"
                },
                "questions": {
                    "value": "- Does the proposed graph-to-set conversion have any implications on graph generation [A, B, C] as well? Considering that the mapping is a bijection, being able to generate graphs via set generation would be another interesting direction, and any comments could further support the significance of the paper.\n- Typo in end of Subsection 7.1: \"(Theorem 6 and Theorem 6)\" -> \"(Theorem 6 and Theorem 7)\"\n\n[A] Kong et al., Autoregressive Diffusion Model for Graph Generation. (ICML 2023)\\\n[B] Vignac et al., Digress: Discrete Denoising Diffusion for Graph Generation. (ICLR 2023)\\\n[C] Jo et al., Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations. (ICML 2022)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission729/Reviewer_knXN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731434520,
            "cdate": 1698731434520,
            "tmdate": 1700471137876,
            "mdate": 1700471137876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "57beWhCOS5",
                "forum": "QAgwFiIY4p",
                "replyto": "y6q9tHKzNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer knXN (1/2)"
                    },
                    "comment": {
                        "value": "We are thankful for Reviewer knXN's comprehensive feedback and address their concerns as follows:\n\n- **W1-1:** For each experiment, how is the rank $r$ chosen? Is it chosen via hyperparameter tuning? For larger graphs, it seems choosing a small $r$ will result in loss of information on the connectivity of the input graph. How is it that PST still performs well on Long Range Graph Benchmark despite this potential loss of information?\n\n\nIn our approach, $r$ is not a hyperparameter but the intrinsic rank of the decomposed matrix, depending solely on the data. This means there is no information loss associated with the choice of $r$. \n\n- **W1-2:** For the experiments, which matrix was used to generate the generalized coordinates? The adjacency matrix, or normalized adjacency, or some other matrix? The beginning of Section 4 briefly seems to mention that the method mainly uses the adjacency matrix, but this is not clear from the experimental section. Furthermore, an ablation study on which adjacency matrix works well with PST could be another interesting direction that provides further guidance.\n\nWe clarify that various matrices, including the adjacency matrix, normalized adjacency, and Laplacian matrix, are theoretically viable for generating generalized coordinates. In our experiments, we primarily use the Laplacian matrix. This will be made clearer in our revised manuscript.\n\nWe have added an ablation study on the choice of matrix for decomposition in the QM9 dataset. The results, displayed in the table below, compare PST variants using different matrices for decomposition against DF, our strongest baseline on the QM9 dataset. The similar performance across PST-Laplacian, PST-Adjacency, and PST-Normalized Adjacency illustrates that our method's effectiveness is not heavily dependent on the specific matrix chosen for decomposition.\n\n|  | $\\mu$ | $\\alpha$ | $\\varepsilon_{\\text{homo}}$ | $\\varepsilon_{\\text{lumo}}$ | $\\Delta\\varepsilon$ | $R^2$ | ZPVE | $U_0$ | $U$ | $H$ | $G$ | $C_v$ |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Unit| $10^{-1}$D | $10^{-1}$$a_0^3$ | $10^{-2}$meV | $10^{-2}$meV | $10^{-2}$meV | $a_0^2$ | $10^{-2}$meV | meV | meV | meV | meV | $10^{-2}$cal/mol/K |\n| PST-Laplacian | $3.19_{\\pm 0.04}$ | $1.89_{\\pm 0.04}$ | $5.98_{\\pm 0.09}$ | $5.84_{\\pm 0.08}$ | $8.46_{\\pm 0.07}$ | $13.08_{\\pm 0.16}$ | $0.39_{\\pm 0.01}$ | $3.46_{\\pm 0.17}$ | $3.55_{\\pm 0.10}$ | $3.49_{\\pm 0.20}$ | $3.55_{\\pm 0.17}$ | $7.77_{\\pm 0.15}$ |\n| PST-Adjacency | $3.16_{\\pm 0.02}$ | $1.86_{\\pm 0.01}$ | $6.31_{\\pm 0.06}$ | $6.10_{\\pm 0.05}$ | $8.84_{\\pm 0.01}$ | $13.60_{\\pm 0.09}$ | $0.39_{\\pm 0.01}$ | $3.59_{\\pm 0.12}$ | $3.73_{\\pm 0.08}$ | $3.65_{\\pm 0.06}$ | $3.60_{\\pm 0.016}$ | $7.62_{\\pm 0.21}$ |\n| PST-Normalized Adjacency | $3.22_{\\pm 0.04}$ | $1.85_{\\pm 0.02}$ | $5.97_{\\pm 0.23}$ | $6.15_{\\pm 0.07}$ | $8.79_{\\pm 0.04}$ | $13.42_{\\pm 0.15}$ | $0.41_{\\pm 0.01}$ | $3.36_{\\pm 0.25}$ | $3.41_{\\pm 0.24}$ | $3.46_{\\pm 0.18}$ | $3.38_{\\pm 0.23}$ | $8.10_{\\pm 0.12}$ |\n| DF(our strongest baseline on QM9) | $3.46$ | $2.22$ | $6.15$ | $6.12$ | $8.82$ | $15.04$ | $0.46$ | $4.24$ | $4.16$ | $3.95$ | $4.24$| $9.01$|"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244626829,
                "cdate": 1700244626829,
                "tmdate": 1700244626829,
                "mdate": 1700244626829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FrmSi9GZLl",
                "forum": "QAgwFiIY4p",
                "replyto": "y6q9tHKzNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer knXN (2/2)"
                    },
                    "comment": {
                        "value": "- **W1-3** After the graph-to-set conversion, how are edge features such as bond types in molecular graphs incorporated into PST?\n\nWe will make it more clear in Appendix F in revision. On the ZINC dataset, where edge features are scalar (representing bond types), these are directly considered in the adjacency and Laplacian matrices. For datasets with multi-dimensional edge features, where the adjacency matrix is represented as $A\\in \\mathbb{R}^{n\\times n\\times d}$, a theoretical approach is to decompose each channel separately, resulting in coordinates with $d$ channels $Q\\in \\mathbb{R}^{n\\times m\\times d}$. However, due to efficiency concerns, we first decompose the adjacency matrix without edge features to obtain coordinates $Q\\in \\mathbb{R}^{n\\times m}$, then compute $A^{(i)}Q$ for each channel $i$ to generate the corresponding channel of the coordinates.\n\n- **W2** Another small concern is the computational cost due to use of symmetric rank decomposition. The paper mentions that each layer in PST runs in $O(n^2r)$-time and $O(n^2+nr)$-space, which can be costly if $n$ is large and $r$ must increase proportionally to $n$ in order to cover sufficient connectivity information. Table 10 in the Appendix shows runtime/memory consumption on ZINC, but since ZINC is consisted of fairly small graphs (~23.2 nodes per graph), it is unclear whether PST is scalable to large graphs (e.g. Long Range Graph Benchmark) as well. Could the authors elaborate on this?\n\nThe scalability of our approach on large graphs, such as those in the Long Range Graph Benchmark (LRGB), is indeed a challenge. For example, on the Pascal-VOC SP dataset with ~400 nodes per graph, we limited the batch size to 6 and compare it with other GNNs. The results are as follows. We will include these results in Appendix I of our revised manuscript. To address scalability, we are exploring methods like linear transformers or sparse transformers for future enhancements of PST. \n\n\n|                           | GCN  | GPS   | SAN  | PST   |\n| ------------------------- | ---- | ----- | ---- | ----- |\n| training time per epoch/s | 77.2 | 246.2 | 270.2| 680.5 |\n| GPU memory/G              | 1.97 | 2.15  | 10.13| 18.95 |\n\n- **Q1:** Does the proposed graph-to-set conversion have any implications on graph generation [A, B, C] as well? Considering that the mapping is a bijection, being able to generate graphs via set generation would be another interesting direction, and any comments could further support the significance of the paper.\n\nIndeed, our graph-to-set conversion offers exciting prospects for graph generation. An orthogonal-transformation-equivariant diffusion model could generate node coordinates, from which the graph structure can be recovered. This approach aligns with techniques in molecule generation, where graphs represent atomic interconnections in 3D space. For instance, [1] utilizes an O(3)-invariant diffusion model for generating atomic coordinates. Our method can extend this idea to a broader range of graphs beyond molecules.\n\n[1] Emiel Hoogeboom et al. Equivariant Diffusion for Molecule Generation in 3D. ICML 2022.\n\n6. **Q2** Typo in end of Subsection 7.1: \"(Theorem 6 and Theorem 6)\" -> \"(Theorem 6 and Theorem 7)\"\n\nWe appreciate this catch and have corrected the typo in the revised manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244644393,
                "cdate": 1700244644393,
                "tmdate": 1700245183430,
                "mdate": 1700245183430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mqVTTDzZcb",
                "forum": "QAgwFiIY4p",
                "replyto": "y6q9tHKzNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_knXN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_knXN"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal by Reviewer knXN"
                    },
                    "comment": {
                        "value": "Thank you authors for your commitment into sharing the rebuttal with additional results and clarifications. \n\nAs shown in the runtime/memory measurements, scaling PST to large networks certainly seems to be a challenge, yet I agree with the authors that this can be addressed in the near future as there exist many approaches designed for making Transformers more efficient. \n\nAfter reading other reviewers' comments (as well as authors' respective responses), I still think incorporating SRD and an orthogonal transformation-invariant architecture such as PST for graph encoding is a neat approach towards maintaining symmetry under differences in eigenspaces, and the empirical results clearly show its effectiveness.\n\nAll of my concerns have been addressed, and thus I retain my score as accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471098052,
                "cdate": 1700471098052,
                "tmdate": 1700471098052,
                "mdate": 1700471098052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dfyGQfqIhI",
            "forum": "QAgwFiIY4p",
            "replyto": "QAgwFiIY4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission729/Reviewer_WPLr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission729/Reviewer_WPLr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a graph representation learning approach by converting interconnected graphs into sets of independent points and encoding them using an orthogonal-transformation-equivariant Transformer. This graph-to-set conversion method, based on Symmetric Rank Decomposition (SRD), eliminates the need for intricate positional encodings used in traditional graph Transformers. The authors theoretically demonstrate that two graphs are isomorphic if and only if their converted point sets are equal up to an orthogonal transformation. They also propose a parameterization of SRD using permutation-equivariant functions for practical implementation. The paper introduces a Point Set Transformer (PST) for encoding the transformed point set."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors provide theoretical foundations for their approach, demonstrating that isomorphic graphs can be perfectly represented using their method. The use of Symmetric Rank Decomposition is well-explained and supported by theorems. They also provide expressivity results regarding the short-range and the long-range abilities of the models.\n\n2. The proposed method seems to outperform recent graph transformer architectures in the datasets used in this study."
                },
                "weaknesses": {
                    "value": "1. One notable weakness in the paper is the tendency to overstate the novelty of certain ideas without providing adequate justification. Specifically, the paper emphasizes the concept of transforming a graph into a set of independent nodes as a novel approach. While the paper introduces a unique method of achieving this transformation through Symmetric Rank Decomposition (SRD), it does not sufficiently acknowledge that the idea of treating graphs as sets of nodes is not entirely novel in the context of graph neural networks. Many existing graph neural network models, including graph transformers, do employ the idea of viewing graphs as sets of nodes for processing. They use various techniques, including positional encodings, to capture and leverage the graph's structural information. While the paper rightfully introduces SRD as a different method for this transformation, it should clarify why this particular approach is advantageous or provides a significant improvement over existing methods. This would help readers understand the specific contributions and benefits of the proposed approach more clearly. Furthermore, the paper mentions the elimination of positional encodings in the proposed method, implying that this is beneficial. However, it does not provide a comprehensive explanation or empirical evidence to support this claim. The paper should clarify why removing positional encodings is advantageous and how this contributes to the overall effectiveness of the proposed approach. Without a clear rationale or evidence, it leaves readers questioning the choice to eliminate positional encodings and the potential impact on performance. In summary, the paper should provide a more balanced perspective on the novelty of its ideas and offer a robust justification for choices such as removing positional encodings to enhance the clarity and credibility of its contributions.\n\n2. The paper does not present empirical results from a wide range of graph benchmarks, which could have provided a more comprehensive assessment of the method's performance and generalizability. TUDatasets, like many other benchmark datasets, cover diverse graph structures and characteristics, and their inclusion in the evaluation process would have added valuable insights into the method's effectiveness across different graph types."
                },
                "questions": {
                    "value": "1. It would be beneficial to see experimental results on a variety of benchmark datasets, including TUDatasets, to assess the method's performance and how it compares to existing approaches. Can the authors provide results on more graph benchmarks to substantiate their claims?\n\n2. The paper suggests that removing positional encodings is beneficial, but the rationale behind this choice is not clearly explained. Can the authors elaborate on why the elimination of positional encodings is advantageous and how it contributes to the overall performance of the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844590065,
            "cdate": 1698844590065,
            "tmdate": 1699635999896,
            "mdate": 1699635999896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p9t61cJF66",
                "forum": "QAgwFiIY4p",
                "replyto": "dfyGQfqIhI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WPLr (1/2)"
                    },
                    "comment": {
                        "value": "We are grateful for Reviewer WPLr's comprehensive feedback and address their concerns as follows:\n\n- **W1:** One notable weakness in the paper is the tendency to overstate the novelty of certain ideas without providing adequate justification.   While the paper introduces a unique method of achieving this transformation through Symmetric Rank Decomposition (SRD), it does not sufficiently acknowledge that the idea of treating graphs as sets of nodes is not entirely novel in the context of graph neural networks. Many existing graph neural network models, including graph transformers, do employ the idea of viewing graphs as sets of nodes for processing. They use various techniques, including positional encodings, to capture and leverage the graph's structural information. While the paper rightfully introduces SRD as a different method for this transformation, it should clarify why this particular approach is advantageous or provides a significant improvement over existing methods. This would help readers understand the specific contributions and benefits of the proposed approach more clearly. \n\nThough we agree that existing graph transformers and ordinary GNNs also encode the nodes, we respectfully argue that our assertion of a \"paradigm shift\" is not an overstatement. Our innovation is not merely SRD but an entirely different architecture. Existing models still perceive nodes as **interconnected**, relying on adjacency matrices or their variants, such as random walk matrices, for input. Our approach represents a significant divergence from this norm. By converting graphs to sets of **independent** nodes and utilizing a set encoder without adjacency inputs, we shift the paradigm from encoding adjacency matrices to crafting set encoders for independent nodes.  This paradigm shift, which completely gives up adjacency input, enables **a novel model design space** too. For example, message passing or attention matrix with adjacency information as relative positional encoding is no longer needed---a set encoder over independent points is enough.\n\nThe paradigm shift is rooted in a fundamental difference between SRD and eigendecomposition (EVD), which is commonly used in prior works. As explicated in our Proposition 1, the symmetric rank decomposition (SRD) of a matrix is uniquely determined up to an orthogonal transformation of the entire coordinate system. In contrast, as elucidated in Section 2 of [1], EVD is uniquely determined up to orthogonal transformations within each eigenspace. This fundamental difference allows SRD-based models to effortlessly maintain symmetry (yielding consistent predictions for isomorphic graphs), while EVD-based methods must address each eigenspace individually, making existing EVD-based methods such as [1] hardly work on graph-level tasks where eigenspaces differ between graphs. In essence, SRD leads to a practical **bijective** conversion of graph problems into set problems where a unified set learner can be applied to different graphs, a property which is not achievable by existing EVD methods. \n\nFurthermore, our method distinctively maintains invariance, producing identical predictions for isomorphic graphs, a feature not consistently achieved by most existing positional encoding methods, such as direct utilization of eigenvectors [2]. This attribute enhances the generalization capabilities of our model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244547859,
                "cdate": 1700244547859,
                "tmdate": 1700481700554,
                "mdate": 1700481700554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "71TosrAafy",
            "forum": "QAgwFiIY4p",
            "replyto": "QAgwFiIY4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission729/Reviewer_h4E8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission729/Reviewer_h4E8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new architecture for graph representation learning. Specifically, the proposed model uses symmetric rank decomposition to obtain coordinates for each node in the graph. Then a transformer model treats the graph as a set of nodes (augmented with coordinates) and encodes the set. Since the coordinates are up to transformations by orthogonal matrices, the transformer model is designed to be invariant to orthogonal transformations. This paper analyses the expressive power of the model, and the model shows good empirical performance compared with existing graph transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A clear presentation of the methodology; easy to follow\n- The design of the architecture is well motivated by the analysis of symmetric rank decomposition, and this paper further provides theoretical analysis of expressiveness\n- The proposed model shows good empirical performance"
                },
                "weaknesses": {
                    "value": "- This paper exaggerates its contributions: this paper claims that the proposed model is a \"paradigm shift\", but the use of symmetric rank decomposition is actually not much different from existing positional encodings based on eigendecomposition. Also, the invariance to orthogonal transformations of positional encoding is discussed and addressed by several related papers (e.g., [1]). I don't think the proposed model is significantly different.\n- Given the above point, the experiment part should also include the performance of [1] and do a comparison, but it's missing.\n- Another recent graph transformer is missing [2] which seems to have better performance.\n\n[1] Sign and Basis Invariant Networks for Spectral Graph Representation Learning. Lim et al. ICLR 2023\n\n[2] Graph Inductive Biases in Transformers without Message Passing. ICML 2023"
                },
                "questions": {
                    "value": "Please see Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission729/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699286896552,
            "cdate": 1699286896552,
            "tmdate": 1699635999832,
            "mdate": 1699635999832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BHcmGjaA3k",
                "forum": "QAgwFiIY4p",
                "replyto": "71TosrAafy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h4E8 (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the detailed feedback from reviewer h4E8 and address their concerns as follows:\n\n- **W1-1:** This paper exaggerates its contributions: this paper claims that the proposed model is a \"paradigm shift\", but the use of symmetric rank decomposition is actually not much different from existing positional encodings based on eigendecomposition. \n\nThank you for the valuable comment. We respectfully argue that our assertion of a \"paradigm shift\" is not an overstatement. Below are detailed reasons.\n\n**Distinct Properties of SRD and EVD:** As explicated in our Proposition 1, the symmetric rank decomposition (SRD) of a matrix is uniquely determined up to an orthogonal transformation of the entire coordinate system. In contrast, as elucidated in Section 2 of [1], eigendecomposition (EVD) is uniquely determined up to orthogonal transformations within each eigenspace. This fundamental difference allows SRD-based models to effortlessly maintain symmetry (yielding consistent predictions for isomorphic graphs), while EVD-based methods must address each eigenspace individually, making existing EVD-based methods such as [1] hardly work on graph-level tasks where eigenspaces differ between graphs. In essence, SRD leads to a practical **bijective** conversion of graph problems into set problems where a unified set learner can be applied to different graphs, a property which is not achievable by existing EVD methods. \n\n**Paradigm Shift:** Owing to this property difference, our model represents a significant departure from existing methods. Contrary to positional encoding methods that require **GNNs with inter-node edges** as input (even Graph Transformers still use adjacency matrices or their variants), our approach converts the graph into a **set of independent nodes** and employs a set encoder **without adjacency inputs**. This shift from encoding adjacency matrices of interconnected nodes to using set encoders for independent nodes, devoid of edge information, signifies a major deviation from traditional methodologies. This paradigm shift, which completely gives up adjacency input, enables **a novel model design space** too. For example, message passing or attention matrix with adjacency information as relative positional encoding is no longer needed---a set encoder over independent points is enough.\n\n**Maintaining Invariance:** Our method distinctively maintains invariance, producing identical predictions for isomorphic graphs, a feature not consistently achieved by most existing positional encoding methods, such as direct utilization of eigenvectors [3]. This attribute enhances the generalization capabilities of our model.\n\n- **W1-2:**  Also, the invariance to orthogonal transformations of positional encoding is discussed and addressed by several related papers (e.g., [1]). I don't think the proposed model is significantly different.\n\nWe agree that both our work and [1] address the concept of invariance to orthogonal transformations. Nevertheless, there are significant differences between the two approaches, encompassing the fundamental properties of the two decompositions, the resulting invariance, the architectural designs, and the overarching paradigms.\n\n**Nature of Orthogonal Transformations:** Although both works refer to orthogonal transformations, the contexts in which they are applied differ markedly. In our approach using symmetric rank decomposition (SRD), the orthogonal transformation is applied to the entire coordinate system. In contrast, the orthogonal transformation in the eigendecomposition (EVD) used in [1] is specific to each eigenspace. \n\n**Addressing Orthogonal Invariance:** This fundamental distinction in decomposition means that [1] **cannot completely solve the issue of orthogonal invariance.** Our model can directly integrate an invariant set encoder with the coordinates generated by our SRD. Conversely, [1] is compelled to address each eigenspace individually, resulting in an architecture (BasisNet) that is only applicable to graphs with a fixed number of multiple eigenvalues. This limitation is impractical for graph tasks with varying numbers of multiple eigenvalues. For real-world datasets, [1] is forced to compromise on orthogonal invariance by assuming a uniform multiplicity of 1 for all eigenvalues, as seen in SignNet.\n\n**Divergent Network Structures:** The above difference also leads to distinct network structures. SignNet and BasisNet in [1] require specialized blocks like invariant graph networks and address eigenvectors in each eigenspace separately. In contrast, our Point Set Transformer (PST) more closely resembles a conventional transformer. In fact, our paradigm only requires a set encoder invariant to orthogonal transformations, thus enabling very flexible model choices not limited to the transformer which we implement in this paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244439914,
                "cdate": 1700244439914,
                "tmdate": 1700245087577,
                "mdate": 1700245087577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmB8xi66qq",
                "forum": "QAgwFiIY4p",
                "replyto": "71TosrAafy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h4E8 (2/2)"
                    },
                    "comment": {
                        "value": "- **W2 & W3:** Given the above point, the experiment part should also include the performance of [1] and do a comparison, but it's missing. Another recent graph transformer is missing [2] which seems to have better performance.\n\nWe thank the reviewer for these constructive suggestions. In response, we have updated our manuscript to include these comparisons.\n\n**Comparison with [1]:** Our PST demonstrates a substantial improvement over [1], with a notable 30% margin in performance on both the ZINC and ZINC-full datasets.\n\n**Comparison with [2]:** Our PST shows competitive performance relative to [2]. On the ZINC dataset, [2] marginally surpasses our model by 9%. However, our PST excels on the larger ZINC-full dataset, outperforming [2] by 33%. Both models achieve nearly identical results on the LRGB datasets.\n\n\nReferences:\n\n[1] Sign and Basis Invariant Networks for Spectral Graph Representation Learning, Lim et al., ICLR 2023.\n\n[2] Graph Inductive Biases in Transformers without Message Passing, ICML 2023.\n\n[3] Benchmarking Graph Neural Networks, JMLR 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244456580,
                "cdate": 1700244456580,
                "tmdate": 1700245120088,
                "mdate": 1700245120088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Fvves9Pdc",
                "forum": "QAgwFiIY4p",
                "replyto": "iNf8BWBF1F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_h4E8"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission729/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission729/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission729/Reviewers",
                    "ICLR.cc/2024/Conference/Submission729/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Reviewer_h4E8"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Reviewer"
                    },
                    "comment": {
                        "value": "Thank you for the further clarification! I understand the difference between PST and Sign/BasisNet, and I would say PST is a reasonable improvement over Sign/BasisNet. However, I still don't agree with the \"Paradigm Shift\" claim. \n\nActually, existing graph transformers don't need adjacency input either. The positional encodings are pre-computed and then added/appended to node features, and then nodes are encoded as a set. PST proposed in this paper needs to pre-compute SRD from the adjacency matrix, and the output of SRD is actually an instantiation of positional encoding.\n\nConsidering the above connection to existing works, I think this paper overstates its contribution. A similar concern is also raised by Reviewer WPLr. Therefore I will keep my rating."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737819238,
                "cdate": 1700737819238,
                "tmdate": 1700737819238,
                "mdate": 1700737819238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rTZiKRVa6F",
                "forum": "QAgwFiIY4p",
                "replyto": "71TosrAafy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission729/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback and for considering our response. We appreciate your acknowledgment of the improvements our proposed methodology brings over existing approaches. However, we would like to further clarify why we believe our work constitutes a \"paradigm shift\" in the context of graph transformers. \n\n* First, we would like to point out that **While it is true that existing graph transformers utilize positional encodings to capture graph structure, they typically still rely on the adjacency input.** Specifically,\n\n1. Grit [1] and graphit [8] uses random walk matrix (normalized adjacency matrix) as relative positional encoding.\n2. Vanilla Graph Transformer [6], Graphormer [2] and SAN [3] uses adjacency matrix as relative positional encoding. [2]'s ablation shows that adjacency is crucial.\n3. GPS [4], Exphormer [5], higher-order transformer [7], Graph Vit/MLP-Mixer [9] even directly incorporating message passing block which use adjacency matrix to guide message passing between node.\n\nTherefore, our method without adjacency input differs from existing models in architecture significantly.\n\n* We emphasize that this architecture distinction is not just a minor improvement but a substantial departure from the prevailing approach. By doing so, we enable **a different design space for GNN, where set encoders can be applied to graph tasks easily**, opening up opportunities for novel applications and addressing potential limitations associated with adjacency-based methods. For example, we can also employ set encoders other than transformer like deepset, as demonstrated in our response to Reviewer 8dzn's \n\nIn summary, we believe that our work, using set encoder without adjacency input for graph task, represents a **significant paradigm shift**, and we hope that you will reconsider our claim in light of this explanation. We value your perspective and look forward to any further discussion or feedback you may have.\n\nBest, Authors.\n\n[1] Liheng Ma et al. Graph Inductive Biases in Transformers without Message Passing. ICML 2023.\n\n[2] Chengxuan Ying et al. Do Transformers Really Perform Badly for Graph Representation? NeurIPS 2021.\n\n[3] Devin Kreuzer et al. Rethinking Graph Transformers with Spectral Attention. NeuIPS 2021.\n\n[4] Ladislav Ramp\u00e1sek et al. Recipe for a General, Powerful, Scalable Graph Transformer. NeurIPS 2022\n\n[5] Hamed Shirzad et al. Exphormer: Sparse Transformers for Graphs. ICML 2023.\n\n[6] Vijay Prakash Dwivedi, Xavier Bresson. A Generalization of Transformer Networks to Graphs. https://arxiv.org/abs/2012.09699.\n\n[7] Jinwoo Kim et al. Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs. NeurIPS, 2021.\n\n[8] Gregoire Mialon et al, GraphiT: Encoding Graph Structure in Transformers. arXiv/2106.05667.\n\n[9] Xiaoxin He et al. A Generalization of ViT/MLP-Mixer to Graphs. ICML 2023."
                    },
                    "title": {
                        "value": "Graph transformers need adjacency input"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission729/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740054948,
                "cdate": 1700740054948,
                "tmdate": 1700740854442,
                "mdate": 1700740854442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]