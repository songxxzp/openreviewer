[
    {
        "title": "Efficient Interactive Preference Learning in Evolutionary Algorithms: Active Dueling Bandits and Active Learning Integration"
    },
    {
        "review": {
            "id": "HqMWniLrAQ",
            "forum": "ILtA2ebLYR",
            "replyto": "ILtA2ebLYR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_5t57"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_5t57"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an algorithm for learning based on pairwise preferences in a dueling bandit setting. The proposed method also includes a module for determining the query times, at which the user is queried for preference feedback. The algorithm is then evaluated empirical using single and multi-objective test functions and the sushi domain. It is also applied to a protein structure prediction problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Most preferential optimization approaches control the query times via batch-size hyperparameters or do not consider this problem at all. Therefore, this is an interesting addition. Furthermore, the number of experiments is quite high, spanning everything from very simple to real-world problems, including a diverse range of comparison algorithms."
                },
                "weaknesses": {
                    "value": "Most importantly, the work lacks clarity:\n- $\\hat{P}$ denotes the predicted preference matrix, but there is no prediction step described. According to Eq. 4, it seems to be the observed preferences.\n- Unclear how $P$, $\\mathbf{w}$ and $\\hat{P}$ are related.\n- $P$ and $\\hat{P}$ are formalized as probability matrix, but \"user preferences are consistent and stationary\", which is even used in the Algorithm 1 by reusing previous results. Resultingly $p_{ij}$ is an indictor function?\n- Eq. 6 is defined over the matrix $\\hat{P}$, but used with scalar values in Eq.5 ?\n- Eq. 6 depends on $P$, but it is not clear how this is obtained.\n- $L_t$ denotes the predicted preference distribution. How is this different from $\\hat{P}$ and $\\mathbf{w}$?\n- $L$ is used as symbol for the loss as well as the preference distribution\n- $p_min$ is never described and only defined within Algorithm 1\n- Fitness is never introduced, but is likely to be assumes as a function of $\\mathbf{F}(x)$. However, it seems to be also assumed that this function is linear, as Section 2.3 mentions \"weight vectors\"?\n- Unclear how preferences are generated, but it does not seem to be possible to employ arbitrary generation processes, as a scalar fitness function is not able to result in cyclic preferences.\n- Virtual fitness function not formally introduced\n\nThis also relates to how the bandit algorithm is embedded into the EA methods. \n- It seems the bandit algorithm is used to evaluate candidate solutions, but in how far is the EA algorithm still working with the multi-objective targets?\n- What is meant with \"runs as normal\"? \n- What is optimized in between the query times? \n- Section 2.3 mentions \"assigning a fitness value\" and \"storing weight vectors\". This suggest, the preferences are used to learn some feedback function that can be used to evaluate candidates. How does this differ from learning a fitness function?\n- Unclear how \"rounds\" are related to the budget, timesteps and queries.\n\nFurthermore, the experiment section also needs some additional polish:\n- Results are not reproducible, as several hyperparameters are not mentioned, especially concerning the EA algorithms.\n- Fig.3: How is it possible that RUCB-AL is outperforming RUCB after a single feedback round?\n- \"Our proposed method achieves the minimum mean in 8 instances\" - This only seems true if one is able to optimize the EA algorithm used in conjunction with RUCB-AL. Of course, in practice this is not really possible. Under a fair comparison (preselected EA algorithm), IEMO/D seems to be the best?\n- Table 1 hardly readable because of the number format. Maybe use relative values (e.g. relative difference to the optimum).\n- Sec 3.4 not meaningful without the appendix, as one can not evaluate if the results are good or bad.\n- PSP trials do not mentioned used budget, rounds, etc.\n\nAdditionally, the work is not sufficiently considering related work. E.g. \"Preference-based Multiobjective Evolutionary Algorithm for Power Network Reconfiguration\" or \"A Preference Based Interactive Evolutionary Algorithm for Multi-objective Optimization\". It should also be mentioned, that a vast number of query selection strategies already exist (e.g. see Bengs 2021), that aim at reducing the number of queries to be performed. Why should this not be considered \"active learning\"? \n\nIn conclusion, besides the clarity issues, two of the claims are not sufficiently substantiated:\n- The approach does not seem to be fitness free, as Sec. 2.3 mentions something resembling learned functions.\n- The difference between the \"AL\" part of algorithm and other query selection strategies."
                },
                "questions": {
                    "value": "See weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698394529864,
            "cdate": 1698394529864,
            "tmdate": 1699636341591,
            "mdate": 1699636341591,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BIYfSNYcsK",
                "forum": "ILtA2ebLYR",
                "replyto": "HqMWniLrAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5t57 (part 1)"
                    },
                    "comment": {
                        "value": "We are thankful for your comments. Your feedback has been immensely helpful in identifying areas for improvement and refinement in our work. We are committed to addressing these points to enhance the quality and comprehensiveness of our research.\n\nRegarding the highlighted weaknesses:\n\n1. We\u2019ve refined the clarity of this paper, the questions are answered as follows:\n    - The definition of $\\hat P, P,\\mathbf{w}$ are introduced in this paper, $P$ in basic definition, $\\hat P, \\mathbf{w}$ in **section 2.2.2**. The calculation of $\\mathbf{w}$ is in **line 21** of **algorithm 1** which means we will increment the corresponding weight vector according to the comparison result. $w_{ij}$ denotes the total time arm i beat arm j. $P$ is the theoretical winning probability of incumbent solutions, the calculation of which is shown in **eq3**. $\\hat P$ is the predicted value on $P$ defined by **eq4**. $\\hat P$ rely on winning time matrix $\\mathbf{w}$.\n    - We changed the reused symbols $L_t$ to $V_t$ (**section 2.4**) to avoid misdirection. The $V_t$ is a probability distribution of the whole population according to the recommended solutions while $\\hat P$ denotes the winning probability of incumbent solutions (i.e., K arms).\n    - The definition of $p_{min}$ is introduced below **eq5** and in **line 5** in **Algorithm 1**.\n    - Firstly, the fitness function is not a function in $\\mathbf{F}(x)$. It helps conduct non-dominated sort in MOEAs, especially dominance-based MOEAs. The fitness function can filter solutions generated by MOEAs and reserve non-dominated solutions. Decomposition-based and indicator-based MOEAs don\u2019t need fitness function, they find solutions in every weight vector direction. So those weight vectors are not utilized to approximate fitness function. It is noteworthy we construct a virtual fitness function. State-of-the-art **preference-based evolutionary multi-objective (PBEMO)** algorithms are all model-based. They need to approximate the whole utility function which may not exist or be hard to express. Our virtual fitness function is only accurate concerning the optimal solution as **eq5** indicates.\n    - In our experiment session, preference information is produced by a **virtual user**, which in specific we will set a reference point as user preference. When comparing two incumbent solutions, we will compare their distance to the reference point.\n    - The virtual fitness function is actually $V_t$ (**section 2.4**).\n2. About the combination of EAs and bandit algorithm:\n    - MOEAs will work with the bandit until they reach the maximum evaluation generation. The **bandit algorithm** performs as an **AI assistant** to guide the MOEAs to converge to user-preferred regions. Without the guidance of the bandit, MOEAs will converge to a PF rather than a certain solution.\n    - \u201cruns as normal\u201d means the MOEAs run as defined in their origin paper. We have refined this sentence.\n    - Between two queries, MOEAs will process the last recommendation. And the whole population will converge to the last recommendation iteratively.\n    - If I\u2019m getting it right, the reviewer may misunderstand that our proposed method also needs to simulate a fitness function representing user preference.\n        - Traditional **PBEMO** algorithms learn user preference by **approximating a utility function** with the help of mathematical tools like GP, RankNet, etc. The utility function in reality may not exist or be hard to express. However, our proposed method learns the **optimal solution** with the help of a bandit. The virtual fitness function is only an **assumption** about preference distribution which is only accurate at global optima.\n        - \u201cassigning a fitness value\u201d means the fitness is assigned to solutions according to the virtual fitness function.\n        - We mentioned \u201cstoring weight vectors\u201d in **section 2.4**. Those weight vectors are not used to approximate a fitness function. They are the corresponding weight vector of recommended solutions concerning **search direction**.\n    - In this paper, round shares the same meaning as timestep. In each round, the bandit algorithm will select two incumbent solutions to make the next query. Budget is the number of times the dueling bandit is allowed to present the query to the Oracle. The relationship between the round and budget is: $round > K\\ge budget$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580311537,
                "cdate": 1700580311537,
                "tmdate": 1700580311537,
                "mdate": 1700580311537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AspGXqwNis",
                "forum": "ILtA2ebLYR",
                "replyto": "HqMWniLrAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5t57 (part 2)"
                    },
                    "comment": {
                        "value": "3. About experiment:\n    - The results are reproducible and hyperparameter settings are introduced in **Appendix A.5.1**.\n    - As to why RUCB-AL outperforms RUCB after one round, this phenomenon is caused by the definition of RUCB-AL. In our proposed method, every element in the winning probability $p_{i,j,i\\ne j}$ start from 0.5 while RUCB from 2 (as they define ${x\\over 0}=1$). This setting makes RUCB run from a larger loss value.\n    - Although from the perspective of $Loss$, IEMO/D seems to have the most time of beating other algorithms. However, empirically we find only calculating $Loss$ is not enough. Because IEMO/D has the poorest convergence among these peer algorithms. The final results of IEMO/D are scattered around the whole PF. So apart from using $Loss$ as a performance metric, we intend to take the total $Loss$ of the whole population into consideration.\n    - We\u2019ve rearranged **Table 1** and made it readable.\n    - **Section 3.4** gives an example of how our proposed method performs on PSP problems. As we can see from **Figure 6 (a)**, the red part denotes the native protein while the blue part denotes the predicted one. If the two overlap, this means our algorithm runs well on this protein structure. In this picture, we can see that our predicted result overlaps with the native one for the most part except at the tail. **Figure 6 (b)** shows our algorithm results from the perspective of energy. Our output population converges around the native protein energy. **Figure 6** suggests our proposed method can be implemented on PSP problems. More results are shown in **Appendix A.6**.\n    - The most important parameter of PSP problems is the native energy of different protein structures. The native energy of the 4 protein are listed in **Table 2**. The other parameter settings are aligned with Zhang et al. (2023). We added this statement to the paper.\n    \n    Zhang, Zhiming, et al. \"Pareto Dominance Archive and Coordinated Selection Strategy-Based Many-Objective Optimizer for Protein Structure Prediction.\"\u00a0*IEEE/ACM Transactions on Computational Biology and Bioinformatics*\u00a0(2023).\n    \n4. As to the three recommended reference papers, we\u2019ve thoroughly read them.\n    - *\u201cPreference-based Multiobjective Evolutionary Algorithm for Power Network Reconfiguration\u201d* utilized a preference-based evolutionary multi-objective (PBEMO) algorithm, namely PD-NSGA-II, to solve power network reconfiguration problem. However, their proposed method is only an easily modified version of NSGA-II concerning the dominance relationship. It didn\u2019t learn preference by approximating a utility function or through pairwise preference from human feedback. So we didn\u2019t take this paper into account.\n    - *\u201cA Preference Based Interactive Evolutionary Algorithm for Multi-objective Optimization: PIE\u201d* introduced an interactive PBEMO algorithm implementing NAUTILUS. However, it is not based on pairwise preference learning which is different from our proposed method and peer algorithms in this paper. So we didn\u2019t take this paper\u2019s method as a peer algorithm.\n    - *\u201cPreference-based Online Learning with Dueling Bandits: A Survey\u201d* listed state-of-the-art dueling bandit algorithms. As far as we investigated, none of these dueling bandit algorithms combine active learning in their process. They referred to active ranking, but this method is not a dueling bandit algorithm.\n5. As to two of the claims:\n    - Our proposed method is fitness-free because we don\u2019t learn a fitness function through the process of preference learning. Compare to those fitness-based PBEMO algorithms (e.g., I-EMO/D-PLVF, I-EMO/D-PPL) that **approximate a fitness (utility) function** in preference learning, our method only cares about the **global optima**. Then construct a **virtual utility function** that is only accurate at global optima. This process avoids approximating a fitness function that may not exist or is hard to express in reality.\n    - Other query strategies balance between exploration and **exploitation from the perspective** of different kinds of **regret**. However, our proposed method balances between **random search and greedy search** from the perspective of **loss (uncertainty)**. With the help of active learning, our proposed method can prompt the dueling bandit\u2019s ability to progressively find the global optima within a limited budget. This is very important in real problems, because users may get impatient and give inconsistent preference after querying too much.\n    \nWe deeply appreciate your meticulous review and insightful suggestions. Should there be additional recommendations or insights you might offer, we would eagerly welcome them. Your time and dedication to evaluating our work are sincerely valued."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580816529,
                "cdate": 1700580816529,
                "tmdate": 1700580816529,
                "mdate": 1700580816529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XDKdubkjXK",
            "forum": "ILtA2ebLYR",
            "replyto": "ILtA2ebLYR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework that firstly learns the objective of human users by dueling bandits, and then the learned objective can be optimized by some zero-th order optimization methods. Specifically, the authors first propose an active dueling bandits algorithm, in which the feedback obtained from the comparisons is used to update the preference probability estimates for each solution, which are then used to construct the user-specific objective function. Based on the objective funciton, the second step is to use the learned objective function to guide the optimization process in a multi-objective evolutionary algorithm. The EA uses the learned objective function to evaluate the fitness of candidate solutions and to guide the search towards the region of interest along the Pareto frontier."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem this paper concerns is clearly defined, and all formalizations are clean and easy to follow\n\n- The paper is well-organized with clear writing.\n\n- Real-world application on protein structure prediction is conducted, which may be closely related to AI for science."
                },
                "weaknesses": {
                    "value": "- My major concern is the novelty of the proposed framwork, as learning objective functions according to human feedback is not new[1][2][3].\n\nRef.\n\n[1] Roijers, Diederik M., Luisa M. Zintgraf, and Ann Now\u00e9. \"Interactive thompson sampling for multi-objective multi-armed bandits.\" Algorithmic Decision Theory: 5th International Conference, ADT 2017\n[2] Ding, Yao-Xiang, and Zhi-Hua Zhou. \"Preference based adaptation for learning objectives.\" Advances in Neural Information Processing Systems 31 (2018).\n[3] Tucker, Maegan, et al. \"Preference-based learning for exoskeleton gait optimization.\" 2020 IEEE international conference on robotics and automation (ICRA). IEEE, 2020.\n\n- The proposed active dueling bandits algorithm is confusing. In traditional active learning, there should be an uncertainty measure, according to which the learner decides whether to query; in active dueling bandits proposed in this paper, if I'm getting it right, whether to query if sorely focusing on if the pair is compared before, which is a noisy feedback that is not trustworthy. I'm not saying it's wrong, considering the Copeland Winner's assumption that the pairwise winning probability is not independent but with some internal structure that can be used, but it should be discussed why such active query mechanism is reasonable under the Copeland setting. Besides, the regret if RUCB-AL is linear of T, which means it does not converge to the Copeland winner."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698425270858,
            "cdate": 1698425270858,
            "tmdate": 1699636341509,
            "mdate": 1699636341509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R36KjcP2Bd",
                "forum": "ILtA2ebLYR",
                "replyto": "XDKdubkjXK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ch6a (part 1)"
                    },
                    "comment": {
                        "value": "We are thankful for your comments. Your feedback has been constructive in identifying areas for improvement and refinement in our work. We are committed to addressing these points to enhance the quality and comprehensiveness of our research.\n\nRegarding the highlighted weaknesses:\n\n1. The reviewer may misunderstand our research field. If I\u2019m getting it right, the reviewer thinks our proposed method is to conduct preference learning from human feedback and also learn an objective function representing user preference. However, we are implementing our proposed method in a **preference-based evolutionary multi-objective optimization (PBEMO)** scenario. \n    - As far as we know, we are the first to adopt fitness-free preference learning in MOEAs. State-of-the-art algorithms learn user preference by approximating a utility function, which in reality may not exist or is hard to express. This kind of method is called fitness-based or model-based because it depends on prior knowledge of the distribution of solutions (e.g., Bradley Terry model). However, our proposed method utilizes the property of dueling bandit to conduct preference learning in MOEAs. Given incumbent solutions, the dueling bandit will not try to approximate a utility function. Conversely, it only focuses on the optimal solution. To utilize the recommended solution $\\hat{\\mathbf{z}}^*$, we construct a virtual utility function. The virtual utility function $V_t$ (**section 2.4**) is only accurate regarding optimal solutions. \n    - As to the three recommended reference papers, we\u2019ve thoroughly read them. The research scenarios are different. **Roijers et al. (2017)** studied preference learning in an online interactive multi-objective reinforcement learning (MORL) scenario. They mapped a policy to an arm and with the help of a dueling bandit the policy can converge to an optimal solution aggressively. **Ding & Zhou (2018)** targeted learning a linear combination loss function that outperforms other loss functions. The different loss ingredients are so-called multi-objectives. Their learned loss function will participate in the optimization process. The preference learning indirectly helps optimization. **Tucker et al. (2020)** proposed a Bayesian dueling bandit method for single-objective preference learning. These three research all converge to **a single solution or an aggregation** at last. However, in our research field, PBEMO, preference learning directly helps the optimization algorithm converge to **a set of favorite solutions**. Our final results are a population that helps our algorithm avoid trapping in local optima.\n2. About the proposed active dueling bandit method: \n    - For our proposed method, we have uncertainty measures to decide whether to query. As can be seen in **eq5**, we decide the next query based on a balance between random and greedy search. Also, our proposed dueling bandit is not just based on whether it has been asked before. The two arms are selected according to the definition of $p_{a_c}$ in **eq5**. Then if the two have been queried before, they will not be presented to the user again. \n    - We understand your concern that user feedback may be noisy and not trustworthy. In our experiment, the user feedback follows the **Bernoulli distribution**. It doesn\u2019t matter in this consultation session user gives the wrong answer. Our preference will be adjusted in several consultations. \n    - About the regret bond, we are confident that our regret bond can be polished up since the experiment results suggest our proposed method works well with multi-objective optimization problems. We will polish it up and reference CCB and SCB structure (Zoghi et al., 2015). \n\nWe deeply appreciate your meticulous review and insightful suggestions. Should there be additional recommendations or insights you might offer, we would eagerly welcome them. Your time and dedication to evaluating our work are sincerely valued."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579816776,
                "cdate": 1700579816776,
                "tmdate": 1700579816776,
                "mdate": 1700579816776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PFxLfBUgfc",
            "forum": "ILtA2ebLYR",
            "replyto": "ILtA2ebLYR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of optimizing multi-objectives.  Instead of calculating the fitness function, authors leverage a human preference-type algorithm, motivated by dueling bandits, to solve their problem. The authors further apply their algorithm to practical applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors propose a novel framework to return the optimized objective by using a dueling bandit framework. This motivation is great since, in reality, human's preferences are easier to obtain than a real-valued reward.\n\n2. The authors derived theoretical guarantee for the dueling bandit algorithm and applied it to protein structure prediction."
                },
                "weaknesses": {
                    "value": "Main weakness:\n\n1. The result of the theories seems weak. The authors derive this algorithm in an online version, but the regret order is O(T), which is too large and not efficient.\n\n2. Weak connections between the dueling bandit algorithm and the main goal of the paper. It seems only this regret is analyzed instead of how well it optimizes the objectives, which are not discussed in a theoretical manner.\n\n3. Lack of comparison with benchmarks from both dueling bandit and the main optimization objective in the paper"
                },
                "questions": {
                    "value": "1. The authors provide a regret for the proposed algorithm with order $O(T)$. This result seems too weak. Could the author further improve this bound? Also, could the authors compare this with some existing benchmarks? Since there are many algorithms that are available in dueling bandit area that can achieve $O(\\sqrt{T})$ upper bound\n\n2. When implementing the dueling bandit, it seems the greed algorithm was implemented. What if we consider an algorithm with an upper confidence interval? Would this give a better regret bound?\n\n3. There is no clear and explicit connection between the dueling bandit algorithm and the key objective of this paper, could the authors provide some theoretical guarantees on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594693337,
            "cdate": 1698594693337,
            "tmdate": 1699636341432,
            "mdate": 1699636341432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FrF4kav64P",
                "forum": "ILtA2ebLYR",
                "replyto": "PFxLfBUgfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NX1i (part 1)"
                    },
                    "comment": {
                        "value": "We are thankful for your comments. Your feedback has been constructive in identifying areas for improvement and refinement in our work. We are committed to addressing these points to enhance the quality and comprehensiveness of our research.\n\nRegarding the highlighted weaknesses and questions:\n1. We are confident that our regret bond can be polished up since the experiment results suggest our proposed method works well with multi-objective optimization problems. We are committed to considering CCB and SCB structure (Zoghi et al., 2015) and further refining the regret bond associated with our proposed method.\n2. We added an elaboration of the optimization module (**section 2.2**) before the preference elicitation module to connect our proposed method with optimization. Since not all feasible solutions are visible to users, MOEAs perform as optimizers to generate incumbent solutions. \n    - In this paper, we haven\u2019t considered MOEAs to utilize the regret. In our future work, we are going to utilize the regret in MOPs (e.g., take the recommendation point $\\hat {\\mathbf{z}}^*$ as a distribution rather than a converging target). \n    - As to why not use the UCB function. Traditional UCB is not active enough, empirically given $K=10$ we will need more than 300 rounds to receive an acceptable accuracy. Our proposed method is inspired by UCB. Traditional RUCB algorithms only balance the mean and uncertainty of winning probability. Our designed active RUCB-AL balances exploration and exploitation from another perspective. We've elaborated on the components of **eq 5**, emphasizing its role in achieving a balance between random and greedy search strategies.\n3. Since the key of this paper is **preference-based multi-objective optimization (PBEMO)**, it is natural we focus more on multi-objective optimization. The single-objective experiment is only designed to prove its convergence. We have sufficient multi-objective experiments. Our proposed method compared with **6 peer algorithms** on **13 benchmark** problems with versatile PF shapes and dimensions ($m=\\{2,3,5,8,10\\}$). Additionally, we conducted our method on a complicated real-world problem, PSP. \n\nWe deeply appreciate your meticulous review and insightful suggestions. Should there be additional recommendations or insights you might offer, we would eagerly welcome them. Your time and dedication to evaluating our work are sincerely valued."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579515679,
                "cdate": 1700579515679,
                "tmdate": 1700579515679,
                "mdate": 1700579515679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mzit7cNv2x",
            "forum": "ILtA2ebLYR",
            "replyto": "ILtA2ebLYR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_Xsp2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3837/Reviewer_Xsp2"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this paper is to guide evolutionary algorithms via human preference feedback, so that there is no need to design the fitness function. The authors propose to achieve this by adopting a modified version of the dueling bandit algorithm, RUCB (Zoghi et al. 2014)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea of combining dueling bandits and evolutionary algorithms is novel and well-motivated, as it naturally allows using human preference feedback to guide evolution."
                },
                "weaknesses": {
                    "value": "1. The presentation of this paper needs significant improvement. Currently, the problem setting, the objective, and the description of the proposed \"human-dominated preference-based EMO\" are not explained clearly, with several important details being vague/missing.\n\nFor example, there lacks a clear and rigorous formulation of the problem illustrated in Figure 1. The author only provided formulation for the \"consultation module\" in Section 2.2.1, which explains when given a set of individuals, the goal of this module is to find the Copeland winner based on human preference feedback. However, it is not clear why we need the evolutionary algorithm to optimize Eq 1. \nI inferred from the context that, the goal of the whole pipeline is to find the optimal solution of Eq 1. But the learner cannot simply choose the solution in set $\\Omega$ to maximize function $F$, i.e., the learner cannot observe the whole set $\\Omega$. Instead, it has to rely on a evolutionary algorithm to create new solutions. Please correct me if I misunderstood anything. \n\nAlso, it is also quite confusing to me that the multi-objective in Eq 1 plays no role in the formulation of dueling bandit, i.e., how to compare two arms under multi-objective setting.\n\n2. RUCB algorithm relies on the assumption that there exists a Condorcet winner. Relaxation of this assumption, i.e., find Copeland winner instead, requires different algorithm design, like the CCB and SCB algorithm in Zoghi et al. (2015). The authors dropped the Condorcet winner assumption in Assumption 1. I would appreciate it if the authors could elaborate on how this can be achieved with the modified RUCB algorithm in Algorithm 1.\n\nZoghi, M., Karnin, Z.S., Whiteson, S. and De Rijke, M., 2015. Copeland dueling bandits. Advances in neural information processing systems, 28.\n\n3. Related to the previous comment, it is not clear to me why the bonus term of standard RUCB algorithm is modified to Eq 5. In addition, if I am not missing anything, the squared loss of $\\hat{p}\\_{i,j}$ in Eq 5 cannot be computed by the algorithm, since the value of $p_{i,j}$ is unknown?"
                },
                "questions": {
                    "value": "The definition of Copeland winner in Eq 2 does not seem to be correct. Moreover, as Copeland winner is guaranteed to exist, it would be better to call this as \"Definition\" instead of \"Assumption\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698905302461,
            "cdate": 1698905302461,
            "tmdate": 1699636341335,
            "mdate": 1699636341335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JXJUF0xy0I",
                "forum": "ILtA2ebLYR",
                "replyto": "Mzit7cNv2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3837/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xsp2"
                    },
                    "comment": {
                        "value": "Thank you sincerely for the invaluable insights and thoughtful comments on our paper. Your feedback has been constructive in identifying areas for improvement and refinement in our work. We are committed to addressing these points to enhance the quality and comprehensiveness of our research.\nRegarding the highlighted weaknesses:\n1. About clarity:\n    - We have included a dedicated **section 2.2** elucidating the optimization module's significance, particularly emphasizing the necessity of employing multi-objective optimization algorithms (MOEAs). Since not all feasible solutions are visible to users. MOEAs play the role of generating solutions that are actively sampled and fed into bandit for preference learning. We've clarified that our research problem operates within multi-objective scenarios, especially **preference-based evolutionary multi-objective optimization (PBEMO)**.\n   - Traditional PBEMO algorithms learn user preference by learning fitness. However, such a fitness function may not exist or be hard to express. Also, traditional methods are model-based for they depend on different mathematical models (i.e., Bradley Terry model) which requires a lot of expert knowledge about the environment. Our proposed fitness-free PBEMO architecture does not need to simulate a fitness function and the virtual fitness function is only accurate about optimal solutions. Also, our method is model-free. We are not based on any mathematical model assumptions.\n    - Additionally, we've explained our utilization of the widely adopted **sigmoid function** to map multi-objective solutions to arms, establishing a link between recommended solutions and a virtual utility function ($V_t$) for preference probability computation $p_{ij}=sigmoid(V_t(\\mathbf{z}_i)-V_t(\\mathbf{z}_j))={1\\over 1+e^{-(V_t(\\mathbf{z}_i)-V_t(\\mathbf{z}_j))}}$.\n2. We have corrected the definition of the Copeland winner (**eq 2**). In order to actively query users, we designed our bandit by choosing arms from the perspective of uncertainty (i.e., MSE loss). We believe by improving the certainty of the winning probability matrix, the bandit algorithm will output a more precise Copeland winner within a limited budget. Empirically, this method proves to be efficient. We are also committed to considering CCB and SCB structure (Zoghi et al., 2015) and further refining the regret bond associated with our proposed method.\n3. Traditional RUCB only balances the mean and uncertainty of winning probability. Our designed active RUCB-AL balances exploration and exploitation from another perspective. The first term in **eq5** refers to a decaying random search process, while the second term refers to a greedy search based on the loss function. We designed the **eq5** to balance between random search and greedy search. As to how to achieve $p_{i,j}$ which is unknown to the user. It is approximated with the following steps:\n    - In the first round, $p_{i,j}=0.5,\\ \\forall\\ i,j\\in K$,\n    - After the second round, we will receive a current optimal solution $\\hat{\\mathbf{z}}^*_t$,\n    - According to the recommendation $\\hat{\\mathbf{z}}^*_t$, we can calculate a virtual utility function $V_t$ (**section 2.4**). Unlike existing PBEMOs that require mathematical tools like GP to approximate the whole utility function, our virtual utility function is not accurate except at the optimal solution.\n    - calculate $p_{i,j}=\\sigma(V_t(\\mathbf{z}_i)-V_t(\\mathbf{z}_j))$\n\nWe deeply appreciate your meticulous review and insightful suggestions. Should there be additional recommendations or insights you might offer, we would eagerly welcome them. Your time and dedication to evaluating our work are sincerely valued."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579278101,
                "cdate": 1700579278101,
                "tmdate": 1700579278101,
                "mdate": 1700579278101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]