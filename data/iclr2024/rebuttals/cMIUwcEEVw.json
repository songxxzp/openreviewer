[
    {
        "title": "RAVL: Reach-Aware Value Learning for the Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning"
    },
    {
        "review": {
            "id": "JNSsH9xBHg",
            "forum": "cMIUwcEEVw",
            "replyto": "cMIUwcEEVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_34p6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_34p6"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies offline MBRL. The authors first show a surprising experimental result: the existing offline MBRL method MOPO does not work when replacing learned models with true dynamics models. They claim that the reason for this failure is the overestimation of values on the edge-of-reach states which are only reached at the final time steps of the limited horizon rollouts. To address this issue, they propose a new method RAVL which combines EDAC and MOPO. They validate the performance of RAVL on a simple 2D environment and D4RL benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The experiments in Section 4.1 are very interesting and insightful. They found that MOPO surprisingly fails when replacing learned models with true dynamics models. This result provides a new understanding of offline MBRL: the model error is not the only issue in offline MBRL.  \n2. The paper is well-written and easy to follow, providing clear explanations and detailed descriptions of the proposed method and experimental results."
                },
                "weaknesses": {
                    "value": "1. The authors first reveal the key overestimation issue of values on edge-of-reach states in offline MBRL. However, as a direct combination of EDAC and MOPO, the proposed method RAVL is largely independent of the revealed issue. In particular, the only difference between RAVL and MOPO is that RAVL exhibits the Q-update rule in EDAC. However, such a Q-update rule is performed on all states in the buffer and is not tailored for edge-of-reach states. RAVL does not identify edge-of-reach states and correct the corresponding values to address the claimed issue.\n2. There is a large gap between the formalization in Section 4.3 and actual offline MBRL methods. First, the definition of edge-of-reach states is very limited. Concretely, such a definition only considers the case where the starting states of model rollouts are only sampled from the initial state distribution. However, in offline MBRL,  the starting state could be any state along the trajectories in the offline dataset. Such a gap leads to a problem that the Edge-of-reach states defined in Definition 1 could be **not** edge-of-reach in model rollouts with different starting states. For instance, let $(s_0, a_0, s_1, a_1, s_2, a_2)$ be a trajectory in the offline dataset. We consider two types of model rollouts with different starting states: $(s_0, \\hat{a}_0, \\hat{s}_1, \\hat{a}_1, \\hat{s}_2, \\hat{a}_2)$ and $(s_1, \\tilde{a}_1, \\tilde{s}_2, \\tilde{a}_2,  \\tilde{s}_3, \\tilde{a}_3)$. Here $\\hat{s}_2$ is edge-of-reach for the first type of model rollouts but could be not edge-of-reach for the second type of model rollouts.\nSecond, Proposition 1 considers an extremely simple case where the Q-update is performed only on a single model rollout. However, in MBRL, Q-updates are performed on multiple model rollouts where states could be overlapped. In this case, the analysis of Q-functions could be much more complicated.\n    \n3. The empirical performance of RAVL is not strong. In D4RL, the existing method MOBILE beats RAVL regarding the number of best-performance tasks, implying that the improvement of RAVL is limited."
                },
                "questions": {
                    "value": "Typos:\n\n1. In Eq.(3), $+\\gamma$ should be $-\\gamma$"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2365/Reviewer_34p6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698220681828,
            "cdate": 1698220681828,
            "tmdate": 1699636169144,
            "mdate": 1699636169144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tXUzW8D0Bm",
                "forum": "cMIUwcEEVw",
                "replyto": "JNSsH9xBHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 34p6"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time. We are pleased that you found our experiments \u201cinteresting and insightful\u201d and that the paper was \u201cwell-written and easy to follow\u201d.\n\n**(Weakness 1) Does RAVL actually target edge-of-reach states?**\n\nThis is a good question and gets to the crux of our method. Please see part A of our general response where we have included a detailed step-by-step walk-through of the intuition of why we can expect our RAVL approach to detect and penalize edge-of-reach states. We would be more than happy to provide further explanation, so please let us know if it is still unclear.\n\n**(Weakness 2, part 1) What starting states are being sampled in Definition 1?**\n\nThank you for raising this. In hindsight, the notation in Definition 1 is slightly confusing. In Definition 1 we used $\\mu_0$ to denote the starting state distribution of the rollouts (i.e. the distribution of $\\mathcal{D}_{\\text{offline}}$) as opposed to the initial state distribution of the environment. Thus, the edge-of-reach definition here is the correct one for offline model-based RL, and matches the one you describe. We apologize for causing confusion here and have adjusted the notation in Definition 1 to make this clearer.\n\n**(Weakness 2, part 2) Analysis of the Q-function update**\n\nThank you for raising this. We provide this analysis as an illustrative example of how errors can be propagated from edge-of-reach states. We agree with the reviewer that in practice, model rollouts are appended to a buffer and training is performed on mini-batches of this data. However, as demonstrated empirically, the same optimization dynamics are likely to occur over the course of training, with edge-of-reach states not receiving any updates and errors from them propagating to all states. We have further clarified this in the paper after Propositon 1.\n\n**(Weakness 3) Empirical performance of RAVL**\n\nThank you for raising this comment, however, we respectfully disagree with the assessment that the performance of RAVL is not strong, as it is state-of-the-art in 5/9 of the environments we evaluated on. In addition, we are pleased to be able to report that RAVL gives significant performance boosts on the more complex V-D4RL benchmark *(please see part B of our general response)*. We also note that SOTA scores on the D4RL benchmark are likely saturated, already being significantly over 100 (normalized expert level) on many of the environments. As such, future work is unlikely to be able to demonstrate substantial improvements on this benchmark.\n\nMore importantly, however, we have demonstrated that MOBILE (the only algorithm that beats RAVL), along with many other prominent offline model-based algorithms, is derived based on significantly incomplete theoretical foundations (and we explain why they work despite this). It is the new insight, and the resulting much more complete understanding of offline RL, that we view as the main exciting new contribution of our work. We anticipate this should be hugely valuable in the future development of robust, strong-performing offline RL methods, potentially also resulting in new improved ways of tackling the critical, previously overlooked edge-of-reach problem.\n\n**(Question 1) Typo in Equation 3**\n\nThank you for spotting the typo in Equation 3. We have now fixed this in the updated paper.\n\n---\n\nThank you again for your insightful comments and questions. Spotting the typo and your feedback on the notation in definition 1 were especially helpful in improving the paper. We hope our responses were able to address any concerns and questions you had and convince you of the contribution of our work. Please let us know if you have any remaining concerns or if anything is still unclear - we would be more than happy to discuss further. If we have been able to address your concerns, we humbly ask if you would consider raising your score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086952960,
                "cdate": 1700086952960,
                "tmdate": 1700086952960,
                "mdate": 1700086952960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N7MFdWtwG9",
                "forum": "cMIUwcEEVw",
                "replyto": "JNSsH9xBHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 34p6,\n\nThank you again for taking the time to review our paper. As author-reviewer discussion period is about to end (22nd Nov AOE, approximately two days), we were wondering whether our clarifications were able to address your questions and clear up any misunderstandings about our algorithm and the formalization in the paper.\n\nPlease let us know if you have any remaining questions or concerns. We hope that we have addressed your queries, and if so, would appreciate it if you would consider revising your score.\n\nBest wishes,\n\nThe authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566919701,
                "cdate": 1700566919701,
                "tmdate": 1700566950558,
                "mdate": 1700566950558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nY52MSGuAs",
                "forum": "cMIUwcEEVw",
                "replyto": "N7MFdWtwG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_34p6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_34p6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed response!\n\nHowever, my major concern about how RAVL identifies and handles edge-of-reach states still remains. In the authors\u2019 response, they explained that RAVL identifies edge-of-reach states by detecting high-uncertainty states (OOD states). However, the set of edge-of-reach states is only a strict subset of high-uncertainty states since high-uncertainty states also contain states that could be visited in the middle of trajectories but are uncovered in the offline dataset. Moreover, previous offline model-based and model-free methods have proposed to identify high-uncertainty states using model uncertainty and Q-uncertainty, respectively. Thus, it is difficult to discern the distinct advantage of RAVL over existing methods from the perspective of edge-of-reach states."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728870092,
                "cdate": 1700728870092,
                "tmdate": 1700728870092,
                "mdate": 1700728870092,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GjpywuDYV3",
                "forum": "cMIUwcEEVw",
                "replyto": "JNSsH9xBHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "Dear Reviewer 34p6,\n\nThe OOD (high uncertainty) states that RAVL detects are **with respect to $D_{\\text{rollouts}}$ and not $D_{\\text{offline}}$**. Therefore, the scenario the reviewer describes could not happen and RAVL specifically targets edge of reach states which are by definition OOD with respect to $s\\in D_{\\text{rollouts}}$. This allows us to use existing Q-uncertainty methods but **with respect to the model rollout distribution and not the original offline data distribution**, and is the key novelty of our method.\n\nPlease let us know if we can help clarify further.\n\nBest,\nThe authors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731629580,
                "cdate": 1700731629580,
                "tmdate": 1700733264799,
                "mdate": 1700733264799,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kuWwBpUhfb",
            "forum": "cMIUwcEEVw",
            "replyto": "cMIUwcEEVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_RQph"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_RQph"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an intriguing finding that challenges the conventional view of model-based offline reinforcement learning: the erroneous dynamical model does not account for the behavior of model-based methods; rather, it is the overestimation of the value of states that are difficult to reach that explains their behavior. This paper proposes a simple Reach-Aware Value Learning to solve the out-of-sample problem by capturing value uncertainty at edge-of-reach states. The illustration experiment is sound and makes sense."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper is meticulously written and excellently structured, providing ample background information and a well-defined problem statement.\n\n+ The findings presented in this paper are interesting, revealing how the overestimation of the value of states that are difficult to reach can have a significant impact on the optimization of offline model-based RL policies.\n\n+ The proposed solution, RAVL, is both simple to implement and highly effective in addressing this problem."
                },
                "weaknesses": {
                    "value": "+ **The problem addressed in this paper may not be influential.**\n+ **The benchmark environment used in this research is relatively simplistic.** It would be interesting to investigate whether or not the proposed method performs satisfactorily in more complex environments.\n+ **The modifications made to the experiments in this paper do not appear to be particularly substantial.**"
                },
                "questions": {
                    "value": "+ I do not fully understand the Figure 3. How is the figure generated? Why is the conclusion drawn: \"As desired, RAVL is effective at capturing the value uncertainty for state-actions which transition to edge-of-reach nextstates.\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728682394,
            "cdate": 1698728682394,
            "tmdate": 1699636169058,
            "mdate": 1699636169058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "33p84WSqCF",
                "forum": "cMIUwcEEVw",
                "replyto": "kuWwBpUhfb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RQph"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time in reviewing our submission. We are pleased that you found that our paper presented an \u201cintriguing finding\u201d and are encouraged by your comment of it being \u201cmeticulously written and excellently structured.\u201d\n\n**(Weakness 1) Influence of the paper**\n\nThank you for this question. We strongly believe *(like reviewer y8bf)* that the analysis in our paper has the potential to have a substantial impact on future work in this area.\n\nThe main contribution of our paper is to expose and explain a significant gap in the current understanding of the issues underlying offline model-based RL. This current incomplete *\u201cdynamics model error\u201d*-based view forms the basis for many current offline model-based algorithms, such as MOPO [1], MOReL [2], and MOBILE [3]. Combined, these works alone have a total of over 1000 citations and have influenced a whole sub-field of offline model-based RL. Our paper demonstrates a significant misunderstanding in these works, provides and verifies an explanation (the edge-of-reach problem), as well as providing a solution (RAVL). We believe the resulting much more complete picture of offline model-based RL should be hugely valuable in the development of more accurately motivated and stronger-performing offline algorithms.\n\nFurthermore, our detailed experiments show the edge-of-reach problem is not just an issue in theory, but that it is actually very prevalent in practice (on the simple environment, on the standard offline D4RL benchmark (see Table 1 and Figure 5), and even in latent space models *(see our new results in part B of our general response*).\n\nFinally, as discussed in Section 6 and Appendix A, our work provides a unified view of model-based and model-free approaches, which we hope will enable more ideas from model-free literature to be directly applied to model-based algorithms (as we have done with RAVL), and we anticipate that our work may inspire further future connections to be drawn between the two subfields of offline RL.\n\n**(Weakness 2) Evaluation on harder environments**\n\nThank you for raising this concern. We are pleased to present additional state-of-the-art results on the more challenging pixel-based V-D4RL benchmark *(please see part B of our general response)*.\n\n**(Weakness 3) Simplicity of our algorithm**\n\nThank you for noting that the approach in RAVL is quite simple. We agree, and we see this as a strength. Our aim was to make as few modifications to the base algorithm as possible, and our unified view of model-based and model-free approaches (introduced in Section 6 and Appendix A) enables us to directly take a robust out-of-the-box solution from model-free literature and apply it to solve the edge-of-reach problem. We found this worked extremely well and found no need to add anything to complicate it.\n\n**(Question 1) How was Figure 3 generated?**\n\nThank you for raising this question. It has brought to our attention that the caption and labels in Figure 3 were unnecessarily unclear and we have edited them in light of your comment. In short: our construction of the simple environment allows us to exactly define whether a state is edge-of-reach or not (checking whether it is within fixed bounds). We are therefore able to compare the Q-ensemble variance for edge-of-reach vs. not edge-of-reach states. Figure 3 shows a histogram of the Q-ensemble variance (RAVL\u2019s effective penalty) for all the state-action pairs evaluated over training, separated into whether the state is edge-of-reach (orange) or not edge-of-reach (ie. within-reach, blue). It shows that, as we hoped, the Q-ensemble variance (RAVL\u2019s effective penalty) is significantly higher on states which are edge-of-reach, thus verifying that our RAVL method is effective at detecting and penalizing edge-of-reach states. (In part A of our general response we reiterate the intuition on how RAVL is able to do this.)\n\n---\n\nWe would once again like to thank you for providing insightful feedback on our submission that helped us improve our paper. We hope our responses have clarified any questions you may have had. Please let us know if you have any remaining concerns or queries as we would be more than happy to discuss further. If we have been able to address your concerns, we humbly ask if you would consider raising your score.\n\n---\n\n[1] MOPO: Model-based Offline Policy Optimization Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, Tengyu Ma. NeurIPS, 2020.\n\n[2] MOReL: Model-Based Offline Reinforcement Learning Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims. NeurIPS, 2020.\n\n[3] Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, Yang Yu. ICML, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086876288,
                "cdate": 1700086876288,
                "tmdate": 1700087064164,
                "mdate": 1700087064164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rTrkvnnyzq",
                "forum": "cMIUwcEEVw",
                "replyto": "kuWwBpUhfb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_RQph"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_RQph"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response, which basically solves my questions. Therefore, I tend to recommend this paper to the ICLR community. However, the \"highlight\" may be a bit too strong for this paper, so I prefer to keep the original rating (6)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536383452,
                "cdate": 1700536383452,
                "tmdate": 1700536438585,
                "mdate": 1700536438585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4LwyNKRoyO",
            "forum": "cMIUwcEEVw",
            "replyto": "cMIUwcEEVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_y8bf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_y8bf"
            ],
            "content": {
                "summary": {
                    "value": "The paper begins by presenting the surprising finding that typical model-based offline RL algorithms fail when provided with oracle dynamics, flaunting the conventional wisdom that these algorithms aim primarily to address exploitation of model inaccuracies. This finding leads to the main conceptual contribution of the paper, which is the \u201cedge of reach\u201d problem: in these model-based offline RL algorithms, which use short-horizon rollouts to collect additional synthetic data for training, some states can only be reached in the final step of rollouts, and thus the Bellman backup targets computed at those states are prone to estimation error. Based on this understanding, the authors propose Reach-Aware Value Learning (RAVL), which eschews explicit dynamics uncertainty quantification in favor of pessimism with respect to a critic ensemble, as in Ensemble-Diversified Actor Critic (EDAC), a model-free offline RL algorithm. RAVL is evaluated on a subset of the D4RL benchmark, where it exhibits performance competitive with the SOTA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The paper demonstrates a significant misconception in the literature of model-based offline RL, i.e. that addressing model inaccuracy is a primary reason why these algorithms can succeed. This finding, along with the identification of the edge-of-reach problem, is likely to have a substantial impact on future algorithmic work in this area.\n* In addition to evaluating on a subset of the standard D4RL benchmark, the authors include a more in-depth exploration in a simple environment where the edge-of-reach issue can be cleanly studied.\n* The proposed algorithm, RAVL, addresses the edge-of-reach problem and displays compelling performance without explicitly quantifying dynamics uncertainty (which is a challenging problem)."
                },
                "weaknesses": {
                    "value": "The D4RL evaluation includes only the basic MuJoCo tasks. The authors could try a more complex environment such as AntMaze or Adroit to further demonstrate the strength of the algorithm over previous methods."
                },
                "questions": {
                    "value": "While RAVL seems to be effective at addressing the edge-of-reach problem, it may require a large ensemble (e.g. 50 models) to achieve sufficient pessimism, which could be computationally expensive. I was curious if you considered/experimented with adding a small explicit penalty to the values of states at the final step of the rollouts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755722442,
            "cdate": 1698755722442,
            "tmdate": 1699636168958,
            "mdate": 1699636168958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IMwjqQ8IfF",
                "forum": "cMIUwcEEVw",
                "replyto": "4LwyNKRoyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y8bf"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time in reviewing our submission. We really appreciate that you have taken the time to understand what we are trying to show, and are encouraged that, like us, you believe this work \u201cis likely to have a substantial impact on future algorithmic work in this area\u201d.\n\n**(Weakness 1) Evaluation on harder environments**\n\nThank you for raising this concern. We have run RAVL on the more challenging pixel-based V-D4RL benchmark and are pleased to report new state-of-the art results *(please see Part B of our general response)*.\n\n**(Question 1, part 1) Can the number of critics be reduced?**\n\nIn our original submission, we stated that RAVL required N=50 critics to achieve sufficient edge-of-reach awareness on the hopper environments (while N=10 was enough for all the other environments). We are pleased to report that we have run additional experiments and have found that N=10 is mostly sufficient for hopper as well, with only hopper-medexp needing N=30, meaning that all but one environment uses N=10.\n\n| **Environment** |        | **RAVL (N=50, in the paper)** |  **RAVL (Updated)**  |\n|:---------------:|:------:|:-----------------------------:|:--------------------:|\n| hopper-v2       | mixed  | 103.1\u00b11.0                     | 102.6\u00b11.1 **(N=10)** |\n| hopper-v2       | med    | 88.1\u00b115.4                     | 90.6\u00b111.9 **(N=10)** |\n| hopper-v2       | medexp | 110.1\u00b11.8                     | 107.0\u00b15.4 **(N=30)** |\n\n**(Question 1, part 2) Did you try adding an explicit penalty to states at the end of rollouts as an alternative approach to the edge-of-reach problem?**\n\nYes, this was an idea we had initially as well! We considered this for quite some time, but from our experiments it did not seem to work well. Our intuition on why this may not work is: Final step rollout states may not actually be edge-of-reach and hence may also appear earlier in a different rollout. This means the target for the state will sometimes have a penalty and sometimes have no penalty. From our investigations, it seems that the resulting contradictory target function or reward signal leads to significant optimization issues. As such there is an extremely delicate trade-off between balancing the penalty to be (1) large enough to address the edge-of-reach issue, but (2) small enough to not cause optimization issues. This highlights the challenge of tackling the edge-of-reach problem! We found our RAVL approach to work much better and more robustly and avoid this optimization difficulty and delicate balancing act.\n\n---\n\nWe would again like to thank you for taking the time to understand and review our work. We especially appreciated your insightful suggestions and encouraging feedback. Please let us know if you have any other comments, concerns, or questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086736586,
                "cdate": 1700086736586,
                "tmdate": 1700086736586,
                "mdate": 1700086736586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "27oS9W4FV5",
                "forum": "cMIUwcEEVw",
                "replyto": "IMwjqQ8IfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_y8bf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Reviewer_y8bf"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response and the additional experiments! For the pixel-based tasks, I think it is hard to argue that the improvement is \"significant\" in a statistical sense (considering the high standard deviation), but RAVL does improve the mean and tends to reduce the variance so I think it is still a useful addition. It is also good to see that one can use only 10 critics in most cases.\n\nI would say that my concerns are addressed and still recommend acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687804323,
                "cdate": 1700687804323,
                "tmdate": 1700687804323,
                "mdate": 1700687804323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aJ5yxM4w8R",
            "forum": "cMIUwcEEVw",
            "replyto": "cMIUwcEEVw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_1Cj8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2365/Reviewer_1Cj8"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the model-based offline reinforcement leanring algorithms, and different with prior works, this paper finds that the current model-based offline RL algorithms still fail even if the real dynamic model is accessible and attributes this failure to the edge-of-reach problem. Then the authors propose Reach-Aware Value Learning (RAVL) to mitigate this new issue."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a surprising and potential problem in model-based offline RL;"
                },
                "weaknesses": {
                    "value": "1. Though this paper points out a novel problem that may be existed in offline RL, the theoretical and empirical evidence seem to be confusing;\n2. This proposed approach, RAVL, does not appear to be reasonable for this particular edge-of-reach problem.\n\nsee below for details."
                },
                "questions": {
                    "value": "1. This paper use two similar expressions, 'out-of-sample' and 'out-of-distribution', are these two expressions the same or different?\n2. According to this paper, the edge-of-reach problem seems to be due to specific interactive schemes of some particular RL tasks, instead of applying to model-based or model-free settings in general. \n3. It's confusing that, Table.1 aims to illustrate that it is the edge-of-reach problem that  causes the failure for model-based offline RL methods, while the simple experiment in Figure.2 don't include any model-based methods (SAC is a typical model-free methods). \n4. About the proposed method, it's just the previous EDAC algorihtm which is trained with additional sythnesis rollouts through the learned dynamic model. So I can't understand how it can sovle the edge-of-reach issue. It's said to avoid overestimations at edge-of-ereach states, however, the proposed method imposes pessimisic estimation on all training data (according to Eq.3), without distinguishing whether the samples belong to edge-of-reach states or not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799760254,
            "cdate": 1698799760254,
            "tmdate": 1699636168853,
            "mdate": 1699636168853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PAhwoVhjeP",
                "forum": "cMIUwcEEVw",
                "replyto": "aJ5yxM4w8R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Cj8"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time in reviewing our submission. We are encouraged that you noted we tackled a \u201csurprising problem\u201d in model-based offline RL.\n\n**(Question 1) Clarifying out-of-sample and out-of-distribution**\n\nWe refer the reviewer to Footnote 1 on page 3 of the main paper where we clarify that we use \u201cout-of-sample\u201d and \u201cout-of-distribution\u201d to mean the same thing. We chose to use both terms to highlight connections to past work. IQL [1], for example, contains a very clear explanation of the model-free version of these issues in which they use the term \u201cout-of-sample\u201d, while many other papers (and wider literature in general) use the term \u201cout-of-distribution\u201d.\n\n**(Question 2) Generality of our finding**\n\nThank you for raising this question. Theoretically, the edge-of-reach problem may be an issue whenever the effective training dataset (in this case $\\mathcal{D}_{\\text{rollouts}}$) consists of truncated trajectories that are unable to cover or \u201creach\u201d the full state space. This is almost certainly the case with the MBPO procedure used in many prominent offline model-based methods (MOPO [2], MOReL [3], MOBILE [4]), combined with limited size offline datasets. Empirically, Table 1 and Figure 5 show that the edge-of-reach problem does indeed occur on the main offline RL benchmark (D4RL), and our new results on V-D4RL *(see part B of our general response)* indicate wider prevalence of the edge-of-reach problem even for latent-space model-based methods. We give a 1-page summary in Appendix A which concisely explains the conditions which induce the edge-of-reach problem.\n\n**(Question 3) Clarification of SAC in Figure 2**\n\nWe believe there may be a misunderstanding of the meaning of \u201cSAC\u201d in Figure 2. This figure is designed to illustrate that the MBPO procedure [4] (which is the base algorithm in most prominent offline model-based methods) causes the edge-of-reach problem. The MPBO procedure can be described as:\n\n1. generate a dataset of truncated rollouts according to the agent\u2019s policy, and then\n\n2. train the agent (most papers use SAC) on this dataset of rollouts ($\\mathcal{D}_{\\text{rollouts}}$), and then repeat.\n\nWe therefore simply use \u201cSAC\u201d to refer to the base MBPO+SAC procedure (used in MOPO, MOReL, MOBILE\u2026) without our RAVL modification. Please see Table 8 for a summary of the setups. Your question has also prompted us to add Algorithm 1, so thank you for this valuable feedback.\n\n**(Question 4) How does RAVL distinguish between states that are edge-of-reach or not?**\n\nThank you for this question. We have included a detailed step-by-step walk-through of the intuition in Part A of our general response. Please let us know whether it is still unclear. We would be happy to provide further explanation.\n\n---\n\nWe once again thank you for your valuable feedback and queries which have helped us to improve the paper. We hope our responses were able to clarify your questions. We would be glad to answer any further questions during the discussion period, so please do not hesitate to let us know if you have remaining concerns or if anything is still unclear. If we have been able to address your concerns, we would humbly ask if you would consider raising your score.\n\n---\n\n[1] Offline Reinforcement Learning with Implicit Q-Learning. Ilya Kostrikov, Ashvin Nair, Sergey Levine. ICLR 2021.\n\n[2] MOPO: Model-based Offline Policy Optimization. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, Tengyu Ma. NeurIPS, 2020.\n\n[3] MOReL: Model-Based Offline Reinforcement Learning. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims. NeurIPS, 2020.\n\n[4] Model-Bellman Inconsistency for Model-based Offline Reinforcement Learning. Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, Yang Yu. ICML, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086596320,
                "cdate": 1700086596320,
                "tmdate": 1700086596320,
                "mdate": 1700086596320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U7XnVoGtPQ",
                "forum": "cMIUwcEEVw",
                "replyto": "aJ5yxM4w8R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2365/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1Cj8,\n\nThank you again for taking the time to review our paper. As the author-reviewer discussion period is about to end (22nd Nov AOE, approximately two days time), we were wondering whether you could let us know if our clarifications were able to answer your questions and clear up any misunderstandings about our algorithm or methodology.\n\nPlease let us know if you have any remaining queries or concerns. We hope that we have addressed your questions, and if so, would appreciate it if you would consider revising your score.\n\nBest wishes,\n\nThe authors."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566826083,
                "cdate": 1700566826083,
                "tmdate": 1700566987495,
                "mdate": 1700566987495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]