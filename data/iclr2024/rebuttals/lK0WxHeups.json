[
    {
        "title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates"
    },
    {
        "review": {
            "id": "WGY58xHMjJ",
            "forum": "lK0WxHeups",
            "replyto": "lK0WxHeups",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_JhF4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_JhF4"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the impact of batch size on the iteration and gradient oracle complexities of the stochastic gradient descent (SGD) algorithm. The objective of the study is to examine how different batch sizes affect the performance of SGD. The paper is written in a reader-friendly manner, making it easily understandable.  In Tables 1 and 2, the authors present a summary of the iteration and gradient oracle complexities of the SGD method using various commonly used step sizes. By presenting this information, the authors offer valuable insights into the behavior of the algorithm. Furthermore, the authors conduct numerical experiments to compare the effectiveness of the step-decay strategy with other optimization algorithms. Through these experiments, they demonstrate the superior performance of step-decay in optimizing the objective function. This finding suggests that step-decay can be a preferable choice when implementing optimization algorithms. However, the contributions of this paper are not sufficient and some of the statements are wrong."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper provides a thorough investigation into the relationship between batch size and the complexities of the SGD algorithm. The authors present their findings in a clear and concise manner, making them accessible to readers."
                },
                "weaknesses": {
                    "value": "The paper is not well ready yet and contributions are trivial. As I checked, the analysis of SGD is quite simple and there is no technical challenge in the analysis. Besides, the main statements on step-decay are wrong. Please see the reasons below.\n\nTo calculate the iteration and gradient oracle complexity of the step-decay method, it is crucial to consider the impact of the lower bound values, denoted as $\\underline{\\alpha}$ and $T$ (representing the length of each stage for step-decay). Unfortunately, the authors of the paper have overlooked this important aspect, which is an incorrect approach. The reason why considering the lower bound values is essential lies in the relationship between $\\underline{\\alpha}$, $T$, and the total number of iterations, denoted as $K$. Specifically, we have the inequality $\\underline{\\alpha} \\leq \\alpha \\eta^{p-1}$, where $p = K/T$. This inequality implies that the lower bound value $\\underline{\\alpha}$ should be taken into account when determining the iteration and gradient oracle complexities of the step-decay method. Ignoring this relationship can lead to flawed conclusions and inaccurate assessments of the algorithm's performance. Therefore, the related complexities results on step-decay are wrong. \n\nOther weaknesses or typos:\n1. In the abstract, the authors made a claim that \"SGD using a step-decay learning rate and a small batch size reduces the SFO (Stochastic First-Order) complexity to find a local minimizer of a loss function.\" However, upon reviewing the paper, it becomes apparent that the study primarily focuses on demonstrating the convergence of SGD to a stationary point rather than specifically proving convergence to a local minimizer."
                },
                "questions": {
                    "value": "See the weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698397317610,
            "cdate": 1698397317610,
            "tmdate": 1699636490113,
            "mdate": 1699636490113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aK8uC7eEHB",
                "forum": "lK0WxHeups",
                "replyto": "WGY58xHMjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer JhF4's comments"
                    },
                    "comment": {
                        "value": "**Comment 1:**\nThe paper is not well ready yet and contributions are trivial. As I checked, the analysis of SGD is quite simple and there is no technical challenge in the analysis. Besides, the main statements on step-decay are wrong. Please see the reasons below.\n\nTo calculate the iteration and gradient oracle complexity of the step-decay method, it is crucial to consider the impact of the lower bound values, denoted as\u00a0$\\underline{\\alpha}$\u00a0and\u00a0$T$\u00a0(representing the length of each stage for step-decay). Unfortunately, the authors of the paper have overlooked this important aspect, which is an incorrect approach. The reason why considering the lower bound values is essential lies in the relationship between\u00a0$\\underline{\\alpha}$,\u00a0$T$, and the total number of iterations, denoted as\u00a0$K$. Specifically, we have the inequality\u00a0$\\underline{\\alpha} \\leq \\alpha\\eta^{p-1}$, where\u00a0$p = K/T$. This inequality implies that the lower bound value\u00a0$\\underline{\\alpha}$\u00a0should be taken into account when determining the iteration and gradient oracle complexities of the step-decay method. Ignoring this relationship can lead to flawed conclusions and inaccurate assessments of the algorithm's performance. Therefore, the related complexities results on step-decay are wrong. \n\n**Reply:**\nWe apologize for the insufficient analysis of Decay 4. Based on [1], we would like to revise our analysis of Decay 4.\n\nFirst, we modify the definition of Decay 4 as follows. Let $\\alpha_0 > 0$, $\\beta \\geq 2$, $T, P \\geq 1$, and $K = TP$.\nA step-decay learning rate is\n$$(\\alpha_k)_{k=0}^{K-1}\n= \\left(\\underbrace{\\alpha_0, \\alpha_0, \\cdots, \\alpha_0}_T,\n\\frac{\\alpha_0}{\\beta}, \\frac{\\alpha_0}{\\beta}, \\cdots, \\frac{\\alpha_0}{\\beta}, \\cdots,\n\\frac{\\alpha_0}{\\beta^{P-1}}, \\frac{\\alpha_0}{\\beta^{P-1}}, \\cdots, \\frac{\\alpha_0}{\\beta^{P-1}} \\right),$$\nwhich is monotone decreasing for $k$.\nWe assume that $\\alpha_0 \\leq \\frac{1}{L}$, which implies that, for all $k \\in [0:K-1]$, $\\alpha_k \\leq \\frac{1}{L}$.\n\nNext, we can show that\n\\begin{align*}\n\\alpha_k \\left(1 - \\frac{L \\alpha_k}{2} \\right) \\mathbb{E} \\left[ \\Vert  \\nabla f({\\theta}_{k}) \\Vert^2 \\right] \n\\leq \\mathbb{E} [ f (\\theta_k)-f (\\theta_k+1)]+\\frac{L \\sigma^2 \\alpha_k^2}{2b}\n\\end{align*}\nbased on Lemma A.1 (our paper).\n\nWe can thus modify the result of Decay 4 in Theorem 3.1 as follows:\n\nLet $P = \\log_{\\alpha_0} K/2$. Then, the sequence $(\\theta_k)$ generated by Algorithm 1 under (C1)-(C3) satisfies that, for all $K \\geq 1$,\n\\begin{align*}\n\\min_{k \\in [0:K-1]} \\mathbb{E}\\left[\\Vert \\nabla f({\\theta}_k) \\Vert^2 \\right]\n\\leq\n\\frac{D_3 (\\sqrt{K}-2 )}{\\sqrt{K}-1}+\\frac{D_4}{(\\sqrt{K}-1)b},\n\\end{align*}\nwhere $D_3 = \\frac{(\\beta -1) \\Delta}{\\alpha_0}$ and $D_4 = \\alpha_0 L \\sigma^2 (\\beta -1)$.\n\nThen, we have that $K$ and $N$ of SGD using Decay 4 needed to an $\\epsilon$-approximation are\n$$K(b) = \\frac{1}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\n$$N(b) = \\frac{b}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\nAs a result, we have the following: \n- The iterations $K(b)$ is monotone decreasing for $b < \\frac{D_4}{2D_3 - \\epsilon^2}$ and convex for $b < \\frac{3 D_4}{2(2 D_3 - \\epsilon^2)}$ .\n- The SFO complexity $N(b)$ is convex for $b > 0$ and $N'(b) > 0$ holds for all $b > 0$.\n- We have that $K_\\epsilon = O(1/\\epsilon^4)$ and $N_\\epsilon = O(1/\\epsilon^4)$. \n\nWe will revise the manuscript based on the above discussion.  \n\n[1] Wang, Xiaoyu, Sindri Magn\u00fasson, and Mikael Johansson. \"On the convergence of step decay step-size for stochastic optimization.\" Advances in Neural Information Processing Systems 34 (2021): 14226-14238.\n\n**Comment 2:**\nOther weaknesses or typos:\n1. In the abstract, the authors made a claim that \"SGD using a step-decay learning rate and a small batch size reduces the SFO (Stochastic First-Order) complexity to find a local minimizer of a loss function.\" However, upon reviewing the paper, it becomes apparent that the study primarily focuses on demonstrating the convergence of SGD to a stationary point rather than specifically proving convergence to a local minimizer. \n\n**Reply:**\nThank you for pointing out. I will revise our claim as \"SGD using a step-decay learning rate and a small batch size reduces the SFO complexity to find a stationary point of a loss function.\""
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357470700,
                "cdate": 1700357470700,
                "tmdate": 1700357470700,
                "mdate": 1700357470700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfbPPSz52U",
                "forum": "lK0WxHeups",
                "replyto": "aK8uC7eEHB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_JhF4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_JhF4"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for your detailed responses and changes made in the rebuttal. However, this paper is not in good shape yet and I will keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633883551,
                "cdate": 1700633883551,
                "tmdate": 1700633883551,
                "mdate": 1700633883551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MSYiw4jGCi",
            "forum": "lK0WxHeups",
            "replyto": "lK0WxHeups",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_1sx5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_1sx5"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the stochastic first-order oracle complexity (SFO, defined by this paper) of SGD with diminishing and constant learning rates. It shows that SGD using a step-decay learning rate and a small batch size achieves the best performance in terms of SFO complexity. Numerical experiments are provided."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-written and I enjoy reading it.\n\n2. Diminishing learning rate is commonly adopted in deep learning, and it is thus important to study it."
                },
                "weaknesses": {
                    "value": "1. The stochastic gradient is generated in a different way from practice, and I think this should be highlighted. Specifically, in each iteration, this paper assumes the stochastic gradient is chosen as an ensemble as individual gradients sampled with replacement from some distribution. However, in deep learning practice, the individual gradient is sampled without-replacement and this leads to a gap between practice and the presented theory.\n\n2. I do not think SFO is a reasonable measure. In practice, different individual gradients are calculated parallelly and the corresponding time does not accumulate across samples.\n\n3. The definition of K_{\\epsilon} and N_{epsilon} seems to be weird, since the learning rate does not appear in any side of the equation.\n\n4. I wonder what is the novelty of Theorem 3.1. Is not it a very basic analysis of SGD?\n\n5. I find the result of Decay 4 problematic. Specifically, in Theorem 3.2, isn't $\\underline{\\alpha}$ itself depends on $K(b)$? How can $K(b)$ be further calculated by $\\underline{\\alpha}$? That being said, when $T$ is independent of $\\epsilon$, T is in the same order as $\\varepsilon$ as P. Therefore, $\\underline{\\alpha}$ depends exponentially on $K$. Applying this to Theorem 3.2, it indicates $K(b)$ is also exponentially dependent over $\\varepsilon$ and contradicts Theorem 3.4."
                },
                "questions": {
                    "value": "1. On page 3, Is $N_{\\epsilon}$ just $bK_{\\epsilon}$?If yes, why not use the simpler one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591939841,
            "cdate": 1698591939841,
            "tmdate": 1699636490034,
            "mdate": 1699636490034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ugzATEZdqo",
                "forum": "lK0WxHeups",
                "replyto": "MSYiw4jGCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 1sx5's comments"
                    },
                    "comment": {
                        "value": "**Comment 1:**\nThe stochastic gradient is generated in a different way from practice, and I think this should be highlighted. Specifically, in each iteration, this paper assumes the stochastic gradient is chosen as an ensemble as individual gradients sampled with replacement from some distribution. However, in deep learning practice, the individual gradient is sampled without-replacement and this leads to a gap between practice and the presented theory.\n\n**Reply:**\nThank you for your valuable comment. Our paper assumes that, at iteration $k$, $b$ gradients of the loss functions are sampled randomly and the stochastic gradient is defined by the mean of $b$ gradients. We believe that the first-order oracle used in our paper is natural. \n\n**Comment 2:**\nI do not think SFO is a reasonable measure. In practice, different individual gradients are calculated parallelly and the corresponding time does not accumulate across samples.\n\n**Reply:**\nThank you for your valuable comment. It is important to evaluate the implementation time of SGD. In this paper, in both theory and practice, we study the relationship between the batch size and the iteration complexity/the SFO complexity without computing $b$ gradients parallelly.\n\n**Comment 3:**\nThe definition of $K_{\\epsilon}$ and $N_{\\epsilon}$ seems to be weird, since the learning rate does not appear in any side of the equation.\n\n**Reply:**\nThank you again for your comment. In the revision, we would like to replace (2) with $K_\\epsilon (n,b,\\alpha_k,\\Delta,L,\\sigma^2)$ and  $N_\\epsilon (n,b,\\alpha_k,\\Delta,L,\\sigma^2)$.\n\n**Comment 4:**\nI wonder what is the novelty of Theorem 3.1. Is not it a very basic analysis of SGD?\n\n**Reply:**\nTheorem 3.1 is used to provide Theorems 3.2 and 3.3. Our main results are Theorems 3.2 and 3.3. Hence, we may replace Theorem 3.1 by e.g., Lemma 3.1/Proposition 3.1.\n\n**Comment 5:**\nI find the result of Decay 4 problematic. Specifically, in Theorem 3.2, isn\u2019t $\\underline{\\alpha}$ itself depends on $K(b)$? How can $K(b)$ be further calculated by $\\underline{\\alpha}$? That being said, when $T$ is independent of $\\epsilon$, $T$ is in the same order as $\\epsilon$ as $P$. Therefore, $\\underline{\\alpha}$ depends exponentially on $K$. Applying this to Theorem 3.2, it indicates $K(b)$ is also exponentially dependent over $\\epsilon$ and contradicts Theorem 3.4.\n\n**Reply:**\nWe apologize for the insufficient analysis of Decay 4. Based on [1], we would like to revise our analysis of Decay 4.\n\nFirst, we modify the definition of Decay 4 as follows. Let $\\alpha_0 > 0$, $\\beta \\geq 2$, $T, P \\geq 1$, and $K = TP$.\nA step-decay learning rate is\n$$(\\alpha_k)_{k=0}^{K-1}\n= \\left(\\underbrace{\\alpha_0, \\alpha_0, \\cdots, \\alpha_0}_T,\n\\frac{\\alpha_0}{\\beta}, \\frac{\\alpha_0}{\\beta}, \\cdots, \\frac{\\alpha_0}{\\beta}, \\cdots,\n\\frac{\\alpha_0}{\\beta^{P-1}}, \\frac{\\alpha_0}{\\beta^{P-1}}, \\cdots, \\frac{\\alpha_0}{\\beta^{P-1}} \\right),$$\nwhich is monotone decreasing for $k$.\nWe assume that $\\alpha_0 \\leq \\frac{1}{L}$, which implies that, for all $k \\in [0:K-1]$, $\\alpha_k \\leq \\frac{1}{L}$.\n\nNext, we can show that\n\\begin{align*}\n\\alpha_k \\left(1 - \\frac{L \\alpha_k}{2} \\right) \\mathbb{E} \\left[ \\Vert  \\nabla f({\\theta}_{k}) \\Vert^2 \\right] \n\\leq \\mathbb{E} [ f (\\theta_k)-f (\\theta_k+1)]+\\frac{L \\sigma^2 \\alpha_k^2}{2b}\n\\end{align*}\nbased on Lemma A.1 (our paper).\n\nWe can thus modify the result of Decay 4 in Theorem 3.1 as follows:\n\nLet $P = \\log_{\\alpha_0} K/2$. Then, the sequence $(\\theta_k)$ generated by Algorithm 1 under (C1)-(C3) satisfies that, for all $K \\geq 1$,\n\\begin{align*}\n\\min_{k \\in [0:K-1]} \\mathbb{E}\\left[\\Vert \\nabla f({\\theta}_k) \\Vert^2 \\right]\n\\leq\n\\frac{D_3 (\\sqrt{K}-2 )}{\\sqrt{K}-1}+\\frac{D_4}{(\\sqrt{K}-1)b},\n\\end{align*}\nwhere $D_3 = \\frac{(\\beta -1) \\Delta}{\\alpha_0}$ and $D_4 = \\alpha_0 L \\sigma^2 (\\beta -1)$.\n\nThen, we have that $K$ and $N$ of SGD using Decay 4 needed to an $\\epsilon$-approximation are\n$$K(b) = \\frac{1}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\n$$N(b) = \\frac{b}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\nAs a result, we have the following: \n- The iterations $K(b)$ is monotone decreasing for $b < \\frac{D_4}{2D_3 - \\epsilon^2}$ and convex for $b < \\frac{3 D_4}{2(2 D_3 - \\epsilon^2)}$ .\n- The SFO complexity $N(b)$ is convex for $b > 0$ and $N'(b) > 0$ holds for all $b > 0$.\n- We have that $K_\\epsilon = O(1/\\epsilon^4)$ and $N_\\epsilon = O(1/\\epsilon^4)$. \n\nWe will revise the manuscript based on the above discussion.  \n\n[1] Wang, Xiaoyu, Sindri Magn\u00fasson, and Mikael Johansson. \u201cOn the convergence of step decay step-size for stochastic optimization.\u201d Advances in Neural Information Processing Systems 34 (2021): 14226-14238.\n\n**Comment 6:**\nOn page 3, Is $N_\\epsilon$ just $bK_\\epsilon$?If yes, why not use the simpler one?\n\n**Reply:**\nThank you. We will revise it accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700311806377,
                "cdate": 1700311806377,
                "tmdate": 1700312341324,
                "mdate": 1700312341324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sJQEaF4jfH",
                "forum": "lK0WxHeups",
                "replyto": "MSYiw4jGCi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_1sx5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_1sx5"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response. The authors promised to throughout revise the results regarding Decay 4, which is a core result in this paper and thus I think another round of review is needed for these new results. I will keep my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618506752,
                "cdate": 1700618506752,
                "tmdate": 1700618520245,
                "mdate": 1700618520245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iHHF1niibD",
            "forum": "lK0WxHeups",
            "replyto": "lK0WxHeups",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_BdrQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_BdrQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the complexity of computing stationary points with SGD \nusing a variety of step-size schedules. The authors derive convergence rates \nwith an explicit dependence on the batch-size for SGD with a constant step-size\nas well as polynomial decay and step-decay step-size schedules. These rates \nare then converted into iteration and oracle complexities and studied as a \nfunction of the mini-batch size. \nThe authors prove that the parameterized complexities are convex functions\nand use this to derive optimal batch-sizes for different schedules.\nThe submission concludes with experiments comparing different schedules on \nCIFAR-10 and CIFAR-100."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of this paper is its novel approach to hyperparameter\ntuning for SGD. While it is typical to tune (at least in theory) the step-size \nparameter to minimize the oracle complexity, maintaining an explicit dependence\non the mini-batch size in the convergence rate and using this to understand\nthe trade-offs between iteration and oracle complexity is an interesting idea.\nIn addition to this, the paper has the following strengths:\n\n- The authors provide a simple and clear analysis for SGD which covers\n    SGD with a fixed step-size, polynomial decay, and step-decay\n    schedules. \n\n- Although the optimal batch-sizes for fixed step-sizes and polynomial decay schedules\n    depend on unknown parameters of the problem, understanding the optimal values\n    may allow for new heuristics for selecting the batch-size in practice.\n\n- The experiments, although simple, generally reflect the theory and show that\n    tuning the batch-size can lead to improvements in optimization given a fixed\n    budget of gradient evaluations."
                },
                "weaknesses": {
                    "value": "This paper has several significant weaknesses that should be addressed before\npublication. In particular,\n\n- The convergence rate given for SGD with the step-decay schedule is misleading\n    and leads to incorrect iteration and oracle complexities for this method.\n\n- The paper does not address the fact that $b \\leq n$ must be maintained, where\n    $n$ is the number of functions in the finite sum. As a result, the optimal\n    batch-sizes which the authors derive may not be attainable depending on the \n    desired precision of the solution. For example, as $\\epsilon \\rightarrow 0$,\n    $b \\rightarrow \\infty$ for a fixed step-size.\n\n- The manuscript is unnecessarily \"mathy\" and many equations could be \n    omitted while maintaining the same results. See, for example, Equations 1 and 2.\n    While this makes the writing seem superficially impressive, it is difficult to\n    read and detracts from the flow of the paper. \n\n- None of the experiments serve to verify the theoretical derivations.\n    I would liked to see at least one synthetic experiment for which the problem\n    constants are known (e.g. a simple quadratic) and $b^*$ can be computed.\n    Plots similar to that in Figures 1/2 could then show that $b^*$ does, in fact,\n    obtain the optimal oracle complexity as claimed.\n\nGiven these issues, I cannot recommend accepting the submission at this time.\nHowever, I am willing to increase my score if they address these issues. At\na very minimum, I feel the problem with the complexity of the step-size schedule\nmust be resolved. See \"Questions\" below for more details."
                },
                "questions": {
                    "value": "- \"SGD using a decaying learning rate...\": some additional comment on the type\n    of learning rate decay is needed here. SGD with step-size $\\alpha_k = 1 / \\log(k)$ \n    does not converge as $O(1/\\sqrt{K})$ despite $\\alpha_k \\rightarrow 0$.\n\n- First math display in Section 1.3.2: this bound should mentioned somewhere that \n    $b > n$ isn't possible and $b = n$ reduces to full-batch gradient descent.\n    As a result, it is not always feasible to select the batch-size to minimize\n    the oracle complexity. \n\n- \"Accordingly, small batch sizes are appropriate for a decaying learning rate or a \n    step-decay learning rate\": why is this true? You have said that SFO complexity\n    has no positive stationary point, but that doesn't imply it is increasing\n    in $b$ or that a small batch-size minimize the complexity over the positive\n    integers. Can you please address this fact?\n\n- Equation (2) and Table 2: It is somewhat confusing to switch from measuring \n    convergence using the squared gradient norm in Table 1 to convergence of \n    just the gradient norm in Table 2 and Equation (2).\n\n- Theorem 3.1: I am concerned by the presentation of the convergence rates for\n    SGD with step-decay. Firstly, $\\underline{\\alpha}$ does not appear to be\n    defined anywhere. From the proof in the appendix, it seems\n    $\\underline{\\alpha} = \\alpha_{K-1} = \\alpha \\eta^{K/T-1}$.  This quantity\n    depends on $K$ --- it is exponentially decreasing every $K/T$ iterations\n    --- so that it is incorrect to write it as a constant factor.  Similarly,\n    $D_3$ depends on $T$, which may or may not have a relationship with $K$\n    depending on algorithm parameters. \n\n    Only be carefully optimizing over $T$ can a final rate of convergence be\n    obtained. Wang et al. [1] set $T = K / \\log_{\\eta}(K)$ to obtain a final\n    convergence rate of $O(\\log(T)/\\sqrt{T})$.  In contrast, treating\n    $\\underline{\\alpha}$ as a constant leads to an deceptive presentation of\n    the convergence rate in Table 1. Moreover, I am fairly certain the\n    complexity of $O(1/\\epsilon^2)$ for computing an $\\epsilon$ stationary\n    point in Table 2 is incorrect and violates lower bounds due to Drori and\n    Shamir [2]. \n\n- Theorem 3.4: In addition to the issue with the complexity of step-decay raised\n     previously, this theorem assumes that $b \\leq n$ can be chosen arbitrarily\n     large in order to obtain the desired complexity. For example, SGD with a \n     constant step-size requires $b \\geq 2 C_2 \\epsilon$, which diverges to \n     infinity as $\\epsilon \\rightarrow 0$. But this is not sensible because\n     $n$ is assumed to be a fixed, finite number of training examples.\n     If this is not the cause, then the authors must specify somewhere that they\n     assume a setting where $n$ can be taken arbitrarily large. \n\n### References\n\n[1] Wang, Xiaoyu, Sindri Magn\u00fasson, and Mikael Johansson. \"On the convergence\nof step decay step-size for stochastic optimization.\" Advances in Neural\nInformation Processing Systems 34 (2021): 14226-14238.\n\n[2] Drori, Yoel, and Ohad Shamir. \"The complexity of finding stationary points\nwith stochastic gradient descent.\" International Conference on Machine\nLearning. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811644822,
            "cdate": 1698811644822,
            "tmdate": 1699636489947,
            "mdate": 1699636489947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DHJ9DRUlAj",
                "forum": "lK0WxHeups",
                "replyto": "iHHF1niibD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BdrQ's comments"
                    },
                    "comment": {
                        "value": "**Comment 1:**\nThe convergence rate given for SGD with the step-decay schedule is misleading and leads to incorrect iteration and oracle complexities for this method.\n\n**Comment 9:**\nTheorem 3.1: I am concerned by the presentation of the convergence rates for SGD with step-decay. Firstly, $\\underline{\\alpha}$ does not appear to be defined anywhere. From the proof in the appendix, it seems $\\underline{\\alpha}=\\alpha_{K-1}=\\alpha\\eta^{K/T-1}$. This quantity depends on $K$ --- it is exponentially decreasing every $K/T$ iterations --- so that it is incorrect to write it as a constant factor. Similarly, $D_3$ depends on $T$, which may or may not have a relationship with $K$ depending on algorithm parameters. \\\\\nOnly be carefully optimizing over T can a final rate of convergence be obtained. Wang et al. [1] set $T=\\frac{K}{\\log_{\\eta}(K)}$ to obtain a final convergence rate of $O(\\frac{\\log(T)}{\\sqrt{T}})$. In contrast, treating $\\underline{\\alpha}$ as a constant leads to an deceptive presentation of the convergence rate in Table 1. Moreover, I am fairly certain the complexity of $O(\\frac{1}{\\epsilon^2})$ for computing an $\\epsilon$ stationary point in Table 2 is incorrect and violates lower bounds due to Drori and Shamir [2].\n\n**Reply:**\nWe apologize for the insufficient analysis of Decay 4. Based on [1], we would like to revise our analysis of Decay 4.\n\nFirst, we modify the definition of Decay 4 as follows. Let $\\alpha_0 > 0$, $\\beta \\geq 2$, $T, P \\geq 1$, and $K = TP$.\nA step-decay learning rate is\n$$(\\alpha_k)_{k=0}^{K-1}\n= \\left(\\underbrace{\\alpha_0, \\alpha_0, \\cdots, \\alpha_0}_T,\n\\frac{\\alpha_0}{\\beta}, \\frac{\\alpha_0}{\\beta}, \\cdots, \\frac{\\alpha_0}{\\beta}, \\cdots,\n\\frac{\\alpha_0}{\\beta^{P-1}}, \\frac{\\alpha_0}{\\beta^{P-1}}, \\cdots, \\frac{\\alpha_0}{\\beta^{P-1}} \\right),$$\nwhich is monotone decreasing for $k$.\nWe assume that $\\alpha_0 \\leq \\frac{1}{L}$, which implies that, for all $k \\in [0:K-1]$, $\\alpha_k \\leq \\frac{1}{L}$.\n\nNext, we can show that\n\\begin{align*}\n\\alpha_k \\left(1 - \\frac{L \\alpha_k}{2} \\right) \\mathbb{E} \\left[ \\Vert  \\nabla f({\\theta}_{k}) \\Vert^2 \\right]\n\\leq \\mathbb{E} [ f (\\theta_k)-f (\\theta_k+1)]+\\frac{L \\sigma^2 \\alpha_k^2}{2b}\n\\end{align*}\nbased on Lemma A.1 (our paper).\n\nWe can thus modify the result of Decay 4 in Theorem 3.1 as follows:\n\nLet $P = \\log_{\\alpha_0} K/2$. Then, the sequence $(\\theta_k)$ generated by Algorithm 1 under (C1)-(C3) satisfies that, for all $K \\geq 1$,\n\\begin{align*}\n\\min_{k \\in [0:K-1]} \\mathbb{E}\\left[\\Vert \\nabla f({\\theta}_k) \\Vert^2 \\right]\n\\leq\n\\frac{D_3 (\\sqrt{K}-2 )}{\\sqrt{K}-1}+\\frac{D_4}{(\\sqrt{K}-1)b},\n\\end{align*}\nwhere $D_3 = \\frac{(\\beta -1) \\Delta}{\\alpha_0}$ and $D_4 = \\alpha_0 L \\sigma^2 (\\beta -1)$.\n\nThen, we have that $K$ and $N$ of SGD using Decay 4 needed to an $\\epsilon$-approximation are\n$$K(b) = \\frac{1}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\n$$N(b) = \\frac{b}{(D_3 - \\epsilon^2)^2} \\left( (2D_3 - \\epsilon^2)^2  - \\frac{2 D_4 (2 D_3 -  \\epsilon^2)}{b} + \\frac{D_4^2}{b^2} \\right).$$\nAs a result, we have the following:\n- The iterations $K(b)$ is monotone decreasing for $b < \\frac{D_4}{2D_3 - \\epsilon^2}$ and convex for $b < \\frac{3 D_4}{2(2 D_3 - \\epsilon^2)}$ .\n- The SFO complexity $N(b)$ is convex for $b > 0$ and $N'(b) > 0$ holds for all $b > 0$.\n- We have that $K_\\epsilon = O(1/\\epsilon^4)$ and $N_\\epsilon = O(1/\\epsilon^4)$.\n\nWe will revise the manuscript based on the above discussion.\n\n**Comment 2:**\nThe paper does not address the fact that $b \\leq n$ must be maintained, where $n$ is the number of functions in the finite sum. As a result, the optimal batch-sizes which the authors derive may not be attainable depending on the desired precision of the solution. For example, as $\\epsilon \\to 0$, $b \\to \\infty$ for a fixed step-size.\n\n**Comment 6:**\nFirst math display in Section 1.3.2: this bound should mentioned somewhere that $b>n$ isn't possible and $b=n$ reduces to full-batch gradient descent. As a result, it is not always feasible to select the batch-size to minimize the oracle complexity. \n\n**Reply:**\nThank you for pointing out. We will add the condition $b \\leq n$ to (C3). Also, we will revise Section 1.3.2 accordingly.\n\n**Comment 3:**\nThe manuscript is unnecessarily \"mathy\" and many equations could be omitted while maintaining the same results. See, for example, Equations 1 and 2. While this makes the writing seem superficially impressive, it is difficult to read and detracts from the flow of the paper.\n\n**Reply:**\nWe kindly agree with your comment. We will revise the manuscript accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366834841,
                "cdate": 1700366834841,
                "tmdate": 1700366834841,
                "mdate": 1700366834841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ud07a6whMY",
                "forum": "lK0WxHeups",
                "replyto": "nkqDHj3WkI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_BdrQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_BdrQ"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for responding to my review. \n\nWhile I appreciate the proposed modifications to the analysis of step-decay, I think such a significant change may deserve another round of reviews.  The issues raised by Reviewer xRbi, i.e. existing mini-batch analyses for SGD with constant step-size, are also significant and reduce the novelty of this work. Considering both of these issues, I will maintain my score for now."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512592286,
                "cdate": 1700512592286,
                "tmdate": 1700512592286,
                "mdate": 1700512592286,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N1e5CJKvuh",
            "forum": "lK0WxHeups",
            "replyto": "lK0WxHeups",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_xRbi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5017/Reviewer_xRbi"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studied the effects of batch size and learning rate for nonconvex smooth optimization. The authors established iteration complexity and SFO (Stochastic First Order Oracle) complexity of the problem.\n\nDespite the result is interesting, most of the results are known (or straightforward extension) in the literature."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-written."
                },
                "weaknesses": {
                    "value": "The paper's theoretical results are known in the literature (e.g., [r1], [r2]). The hardness result [r3] says that: whatever batch size the SGD algorithm can choose, the SFO cannot be better than $O(1/\\epsilon^4)$.\n\n[r1] Ghadimi, Saeed, and Guanghui Lan. \"Stochastic first-and zeroth-order methods for nonconvex stochastic programming.\" SIAM Journal on Optimization 23, no. 4 (2013): 2341-2368.\n\n[r2] Ghadimi, S., Lan, G., & Zhang, H. (2016). Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2), 267-305.\n\n[r3] Arjevani, Yossi, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, and Blake Woodworth. \"Lower bounds for non-convex stochastic optimization.\" Mathematical Programming 199, no. 1-2 (2023): 165-214."
                },
                "questions": {
                    "value": "Can you describe how your approach is better than the references I gave above (e.g., [r1, r2])?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5017/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699491604170,
            "cdate": 1699491604170,
            "tmdate": 1699636489790,
            "mdate": 1699636489790,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BQxRXMbbM8",
                "forum": "lK0WxHeups",
                "replyto": "N1e5CJKvuh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xRbi's comments"
                    },
                    "comment": {
                        "value": "**Comment:**\nThe paper's theoretical results are known in the literature (e.g., [r1], [r2]). The hardness result [r3] says that: whatever batch size the SGD algorithm can choose, the SFO cannot be better than $O(1/\\epsilon^4)$. \n\n[r1] Ghadimi, Saeed, and Guanghui Lan. \"Stochastic first-and zeroth-order methods for nonconvex stochastic programming.\" SIAM Journal on Optimization 23, no. 4 (2013): 2341-2368.  \n\n[r2] Ghadimi, S., Lan, G., Zhang, H. (2016). Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2), 267-305.\n\n[r3] Arjevani, Yossi, Yair Carmon, John C. Duchi, Dylan J. Foster, Nathan Srebro, and Blake Woodworth. \"Lower bounds for non-convex stochastic optimization.\" Mathematical Programming 199, no. 1-2 (2023): 165-214.\n\n**Questions:**\nCan you describe how your approach is better than the references I gave above (e.g., [r1, r2])? \n\n**Reply:**\nThank you for your valuable comments.\n\nAssumptions (C1)--(C3) (our paper) are general and well used in previous papers. From the experimental results in the previous paper [1] and our experimental results (see Figures 2 and 6, and so on), it is clear that the SFO complexity of SGD depends on both the learning rate and the batch size. Therefore, we study the relationship between batch size and the SFO complexity of SGD using specific learning rates (Constant and Decay 1--4) from the viewpoints of both theory and practice. Our results show that\n\n- The number of iterations $K$ needed for nonconvex optimization of SGD is a monotone decreasing and convex function with respect to the batch size $b$ (Theorem 3.2);\n- The SFO complexity $N$ is convex with respect to the batch size $b$ (Theorem 3.3);\n- The existence of the critical batch size that is a global minimizer of $N$ depends on learning rates (Theorem 3.3).\n\nThe above results (Theorems 3.2 and 3.3) are our contributions compared with the previous results in [r1] and [r2]. \n\nReference [r3] showed that the iteration complexity of the general SGD does not perform better than the order of $\\frac{1}{\\epsilon^4}$. However, we would like to emphasize that there is a room such that, if specific learning rate and batch size are used, then SGD will perform better than the order of $\\frac{1}{\\epsilon^4}$. Our paper shows that, if we can use a constant learning rate $\\alpha < 2/L$ and the critical batch size $b^* = 2 C_2/\\epsilon^2$, then the iteration complexity breaks through the order of $\\frac{1}{\\epsilon^4}$ and becomes $O(\\frac{1}{\\epsilon^2})$. We can also check that using specific batch size performs well in practice. For example, Figure 2 (our paper) indicates that the SFO complexity of SGD using $b = 2^4$ is better than the ones using other batch sizes. \n\n[1] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of\nMachine Learning Research, 20:1\u201349, 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300367228,
                "cdate": 1700300367228,
                "tmdate": 1700373261643,
                "mdate": 1700373261643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DBOjfK9Olq",
                "forum": "lK0WxHeups",
                "replyto": "N1e5CJKvuh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_xRbi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5017/Reviewer_xRbi"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I would like thank the authors' response. However it does not change my mind.\n\n1. If your batch size is $O(1/\\epsilon^2)$ and your iteration complexity is $O(1/\\epsilon^2)$ then the total gradient complexity is still $O(1/\\epsilon^4)$.\n\n2. The authors claim that \"To the best of our knowledge, this is the first paper to provide that the iteration complexity of SGD using a constant learning rate and the critical batch size is $O(\\frac{1}{\\epsilon^2})\". This statement is very wrong. Could you please check reference [r2] carefully before claiming anything?\n\nI will end my discussion here."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5017/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326298135,
                "cdate": 1700326298135,
                "tmdate": 1700326358270,
                "mdate": 1700326358270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]