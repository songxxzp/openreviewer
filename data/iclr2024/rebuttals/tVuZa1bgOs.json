[
    {
        "title": "Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association"
    },
    {
        "review": {
            "id": "nPGflasV01",
            "forum": "tVuZa1bgOs",
            "replyto": "tVuZa1bgOs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the bias within V-L datasets and proposed a method to augment image-text pairs with multiple new emergent works. In detail, by decoupling the object and attribute in the images, locating the objects, adjusting the text, and inpainting the new images, this work rebuilt the V-L dataset in different scales and retrained baseline models. On several tasks, the proposed method is evaluated and compared with CLIP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The bias within V-L training and problems of the datasets are important to dig.\n\n+ The thought of using SOTA tools to augment the datasets is non-trivial. And the proposed pipeline looks sound."
                },
                "weaknesses": {
                    "value": "- The whole paper lacks many details of method design choices, data curation, method, training, and inference details, thus hindering the readers to fully understand the effectiveness of the proposed method. Only empirical results are not enough.\n\n- No bias analysis, which is the most important point in the introduction.\n\n- Lacking experiments using different key tools, e.g., LLM, detector/grounder, image generator, etc, which may affect the data generation a lot.\n\n- More solid analyses should be given to probe the relation between the data and the bias and performance of different tasks.\n\n- Prompt: The font color is hard to read."
                },
                "questions": {
                    "value": "1. I suggest the authors also discuss the problems in the causal view, like counterfactual samples.\n\n2. Any user study of the image generation quality, prompt quality, and rationality of the V-l relations?\n\n3. CLIP-ft: fine-tuning CLIP on the augmented data? Missing details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698412234688,
            "cdate": 1698412234688,
            "tmdate": 1699636571554,
            "mdate": 1699636571554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "57K77fmMAP",
                "forum": "tVuZa1bgOs",
                "replyto": "nPGflasV01",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "```\nQ: The whole paper lacks many details of method design choices, data curation, method, training, and inference details, thus hindering the readers to fully understand the effectiveness of the proposed method. Only empirical results are not enough.\n```\nA: Thanks for the valuable comment. We have reorganized and refined the paper from the following perspectives:\n* Update Figure 2 and the methodology with clearer presentation and more implementation details for the readers to better understand our approach.\n* Please refer to Section 5.1 for the training details.\n* Please refer to Section 4.1 for the implementation details including the data source and data processing.\n* We add human evaluation for each phases in this framework to better demonstrate the proposed method beyond the empirical results.\n\n```\nQ: Prompt: The font color is hard to read.\n```\nA: We have updated the prompt in Appendix C with carefully designed style for better readability.\n\n```\nQ: No bias analysis, which is the most important point in the introduction.\n```\nA: We want to clarify that reporting bias is the motivation of our method but not the exact problem we're studying in this paper.\n\nThe proposed method is motivated by the phenomenon that object-attribute are usually associated (which is caused by reporting bias). This association could hinder the model to understand the attributes for an object, e.g. VL model could think _lemon_ is always _yellow_ because it sees that association frequently. In the proposed method, a _richer array of object-attribute_ pairs are added into the datasets **explictly**, thus it can help the VL model's ability to understand the object-attribute, instead of just memorizing the frequent patterns.\n\n```\nQ: Lacking experiments using different key tools, e.g., LLM, detector/grounder, image generator, etc, which may affect the data generation a lot.\n```\nA: Thank you for your suggestion! Reporting bias (as its property suggested) is often overlooked in existing models and datasets. As a result, it is non-trivel to reveal and fix it, hence requires multiple language/visual tools, making it hard to conduct large-scale experiments on a mass of combination of different foundation models. Hence, we use **state-of-the-art foundation models** to construct our framework, so that the generation quality could be as good as possible at the moment. We will leave the exploration of the effect of different foundation models in the future work to find the best practice for deployment. \n\n```\nAny user study of the image generation quality, prompt quality, and rationality of the V-l relations?\n```\n\nA: Thank you for the suggestion! We add a subsection for user study in Section 4.4 in the updated paper. Please refer to the paper for details.\n\nIn summary, we did human evaluation for the foundation models outputs. We design five questions for the intermediate output:\n1. Correct object: ''In step 1 and 2, does the framework generate reasonable objects?''\n2. Implicit object: ''In step 1 and 2, do the valid objects contain the ones that are not explicitly mentioned in the original caption?''\n3. Correct attribute: ''In step 3, does the framework add description for the correct given attributes?''\n4. Valid negative: ''In step 4, can the pair of attributive objects serve as hard negative example to each other?''\n5. Valid inpainting: ''In step 7, does the model inpatint the required object in the masked area?''\n\nIt shows that we got >90\\% correctness for _correct object_ and _valid negative_, around 60\\% for _correct attribute_ and _valid inpainting_. And around 60\\% augmented examples contains objects that are not explicitly mentioned in the original caption\n\n```\nQ: More solid analyses should be given to probe the relation between the data and the bias and performance of different tasks.\nQ: I suggest the authors also discuss the problems in the causal view, like counterfactual samples.\n```\nThank you for your thoughtful feedback. As we mentioned above, our experiments involves extensive data processing and model training, which can be quite time-consuming. Due to the time limit, it may not be feasible for us to add more experimental analysis by the end of the discussion period. However, we will continue to enhance the analysis and discussion sections in the future.\n\n```\nQ: CLIP-ft: fine-tuning CLIP on the augmented data? Missing details.\n```\n\nA: CLIP-ft refers to fine-tuning on the source dataset sampled from CC3M but not on the augmented data. CLIP-ft is the baseline variant we compare with the proposed method BiAug. Specifically, we first sample dataset with different sizes from CC3M, i.e. the _source dataset_, then we apply the proposed method on the source dataset to obtain the _augmented dataset_. CLIP-ft is CLIP trained on the source one and BiAug is CLIP trained on the augmented one. They are comparable because the datasets are from the same source.\n\nWe have modified the description in Section 5.1 to avoid misunderstanding."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678811080,
                "cdate": 1700678811080,
                "tmdate": 1700678811080,
                "mdate": 1700678811080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3aLps1qpLW",
                "forum": "tVuZa1bgOs",
                "replyto": "57K77fmMAP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. After reading the reviews and responses, I tend to retain my initial rating. I think this paper could be revised and improved, especially the clarity of the complex pipeline design and experiments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728840206,
                "cdate": 1700728840206,
                "tmdate": 1700728840206,
                "mdate": 1700728840206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9VqoseN2Hi",
            "forum": "tVuZa1bgOs",
            "replyto": "tVuZa1bgOs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_NefH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_NefH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an approach to address reporting bias in vision-language datasets. The proposed method, known as bimodal augmentation (BiAug), incorporates language models, including ChatGPT, GroundingDino, and Stable-diffusion-inpainting. These models are integrated to supplement the attribute information of objects in the datasets. Ultimately, this integration leads to the creation of a new vision-language dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper stands out for its excellent writing and organization. Its clarity enhances the ease of comprehension and makes it highly accessible to readers.\n\n2. The paper's motivation is not only intriguing but also holds significance within the context of vision-language tasks. The novelty lies in addressing the crucial issue of reporting bias. While it may appear that this paper seamlessly integrates large models, the rationale behind their usage is sound and nuanced, underpinning its innovative approach."
                },
                "weaknesses": {
                    "value": "1. It appears that the current method focuses on data augmentation for vision-language (VL) datasets by altering one attribute of one object at a time. It would be beneficial if the author could elaborate on whether they have considered modifying multiple attributes and multiple objects simultaneously.\n\n2. In addition to conventional attributes like color, shape, and material, has the author explored more detailed attributes, as in [1], provided by Large Language Models (LLM)?\n\n3.  It is advisable for the authors to conduct a broader range of experiments and provide more visualization results, particularly in the context of Attribute Recognition Games (ARG) experiments.\n\n4.  The meaning of the abscissa in Fig. 5 is not immediately clear. Further clarification or labeling may be needed for readers to grasp the content effectively. Furthermore, Fig. 5's subcaption would benefit from further adjustment.\n\n5. The paper would benefit from a more extensive discussion of its limitations and potential directions for future research, offering a more comprehensive evaluation of the current method's scope and boundaries.\n\nIf the author can address my concerns, I would be willing to consider raising my scores.\n\n[1] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022"
                },
                "questions": {
                    "value": "See `Weakness' above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740556744,
            "cdate": 1698740556744,
            "tmdate": 1699636571450,
            "mdate": 1699636571450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EoWj1DgCtJ",
                "forum": "tVuZa1bgOs",
                "replyto": "9VqoseN2Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the time for reviewing our paper. Your comments and suggestions are very insightful! We're addressing your concerns as follows,\n\n```\nQ: It would be beneficial if the author could elaborate on whether they have considered modifying multiple attributes and multiple objects simultaneously.\n```\nA: This is a good point, especially we're processing a large scale of data. In fact, one attribute of one object at a time is just the example for presentation. We actually modify the all the four attributes for all valid objects to augment the captions, i.e. four attributes X multiple objects a time, please refer to Appendix C, prompt 2 for details.\n\nHowever, for the image inpaining, we do one attribute-object pair at a time, because modifying multiple objects could corrupt the image, and more importantly, we would not be able to construct the hard negative pairs to highlight the attributive difference for one object.\n\n```\nQ: In addition to conventional attributes like color, shape, and material, has the author explored more detailed attributes\n```\nA: This is also a very good point, we did explore different attributes but found it would be hard to cover every attribute in the large-scale experiments. To address the diversity of attributes, we add an **_other_** attribute to leverage the LLM\u2019s generalization capabilities to infer relevant attributes from provided captions, highlighting the most relevant commonsense knowledge besides color, shape and material. Please refer to footnote 2 in the 4th page, and the prompt 2 in Appendix C.\nBy doing this, LLM is able to generate attributive description dynamically based on the given caption, beyond the preset conventional attributes.\n\n\n```\nQ: Add ARG experiments.\n```\nA: Thanks for the suggestions! We're trying to add broader experiments by the end of discussion period if possible as the time limit. Could you also provide the reference or link to the ARG benchmark you mentioned? We'd also like to kindly note that we're focusing on the reporting bias issue (which is overlooked previously) in VL dataset, ARO is the most relevant benchmark to verify our ideas.\n\n\n```\nQ: The meaning of the abscissa in Fig. 5 is not immediately clear. Further clarification or labeling may be needed for readers to grasp the content effectively. Furthermore, Fig. 5's subcaption would benefit from further adjustment.\n```\nA: Thanks very much for the suggestion!\nThe x-axis refers to the size of the source dataset used for training the model, e.g., 40K, 100K, 200K and 300K, for demonstrating the trend of model's performance with increasing data used for fine-tuning.\nWe have added description for the x-axis in the caption of Figure 4 (Figure 5 in the original paper), and in Section 4.1 for better readability. As for the formatting in the figure, we'll carefully adjust them in the final version.\n\n```\nQ: The paper would benefit from a more extensive discussion of its limitations and potential directions for future research, offering a more comprehensive evaluation of the current method's scope and boundaries.\n```\nA: Thank you for the suggestion! We agree that discussion would be very helpful. We cannot add another section due to the page limit. Please refer to Appendix A for our discussion about limitation and future directions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678639870,
                "cdate": 1700678639870,
                "tmdate": 1700678639870,
                "mdate": 1700678639870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PUXaztx6oG",
                "forum": "tVuZa1bgOs",
                "replyto": "9VqoseN2Hi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for the time and effort you have invested in reviewing our work. Your insights and comments have been invaluable in refining our manuscript.\n\nWe are eager to learn your perspective on our responses to your comments. If there are any points or concerns that you believe have not been adequately addressed, please let us know and we are willing to provide further clarification and responses."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678904585,
                "cdate": 1700678904585,
                "tmdate": 1700678904585,
                "mdate": 1700678904585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8QnUG9tvEY",
            "forum": "tVuZa1bgOs",
            "replyto": "tVuZa1bgOs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on the problem of reporting bias in visual-language models. The authors proposed BiAug, a data augmentation framework through object-attribute decoupling. BiAug synthesizes vision-language examples and constructs hard negatives by utilizing foundation models including LLMs to extract objects, generate attribute descriptions and negative descriptions, a grounding object detector to detect objects, and an inpainting model to create new images. This paper compares with CLIP baselines on several object-attribute understanding benchmarks and zero-shot retrieval tasks. The results show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed BiAug framework utilizes several foundation models to generate synthetic and negative samples to disentangle objects and attributes. This is helpful to mitigate the reporting bias of VLMs.\n2. The experiments on ARO dataset shows the effectiveness of proposed frameworks by comparing with two CLIP baselines. The experiments also show a clear trend of enlarging the dataset size, demonstrating the promising scalability of the proposed augmentation framework."
                },
                "weaknesses": {
                    "value": "1. This paper utilizes multiple foundation models as black-box tools. It lacks of intuitive measurement of the quality of foundation models' output. For example, is there a numerical evaluation of the box quality generated in step 2 in Figure 2? In step 1 how is the quality of the \"guessed\" potential objects?\n2. This paper claims also to achieve improvement in general text-image retrieval benchmarks. However, in figure 5 table (c) and (d), the performance of BiAug and Clip-ft is rather close on the 40k subset, and with more examples, BiAug didn't show clear improvement compared with either the BiAug or CLIP-ft baseline on the 40k subset. These results can't really show a solid improvement on the two datasets.\n3. This work only compares with two baselines: CLIP and CLIP-ft. It is unclear whether BiAug can also benefit other VL models such as NegCLIP (Yuksekgonul et al., 2022) which has been trained with explicitly constructed negative samples."
                },
                "questions": {
                    "value": "See Weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5557/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699220861099,
            "cdate": 1699220861099,
            "tmdate": 1699636571353,
            "mdate": 1699636571353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOd8X5Tz0v",
                "forum": "tVuZa1bgOs",
                "replyto": "8QnUG9tvEY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful comments! We're addressing your concerns as follows,\n\n## Measurement of the quality of foundation models outputs\nThat's a very good suggestion. So we add a subsection for that in Section 4.4 in the updated paper. Please refer to the paper for details.\n\nIn summary, we did human evaluation for the foundation models outputs, as there is no benchmarks that are exactly designed for our case. To check whether these foundation models follow our instruction to work, we design five questions for the intermediate output:\n1. Correct object: ''In step 1 and 2, does the framework generate reasonable objects?''\n2. Implicit object: ''In step 1 and 2, do the valid objects contain the ones that are not explictly mentioned in the original caption?''\n3. Correct attribute: ''In step 3, does the framework add description for the correct given attributes?''\n4. Valid negative: ''In step 4, can the pair of attributive objects serve as hard negative example to each other?''\n5. Valid inpainting: ''In step 7, does the model inpatint the required object in the masked area?''\n\nIt shows that we got >90\\% correctness for _correct object_ and _valid negative_, around 60\\% for _correct attribute_ and _valid inpainting_. And around 60\\% augmented examples contains objects that are not explicitly mentioned in the original caption\n\nAs for your questions,\n```\nis there a numerical evaluation of the box quality generated in step 2 in Figure 2? \n```\nA: Grounding DINO achieves an impressive 60.7 mAP (refer to Table 2, [1]) in zero-shot object detection on the MSCOCO benchmark, without any prior training on this specific benchmark. This performance underscores its robust capability to reliably identify valid objects, which is essential for the effectiveness of our method.\n\n```\nIn step 1 how is the quality of the \"guessed\" potential objects?\n```\nA: As the results shown Table 2 in the updated paper, the quality of the guessed potential objects are very good with a correctness >90\\%.\n\n\n[1] Liu, Shilong et al. \u201cGrounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.\u201d ArXiv abs/2303.05499 (2023): n. pag.\n\n## Concern about the improvement on general VL retrieval task\nThe Filcker30K-order and COCO-order in Figure 4(Figure 5 in older verion) are not the results for general text-image retrieval task, instead it's the results on a very challenging ARO benchmark[2]. \nWe have modified the paper to avoid misunderstanding.\nIn ARO, the original Filcker and COCO captions are modified by changing partial word order, which are very similar to the original caption. This benchmark requires the VL model to pick the correct caption from a group of _hard negative_ examples. Our results show that BiAug has a more stable trend in distinguish _hard negative_ examples.\n\nThe claim that BiAug also acheives improvement on general image-text retrieval task is supported by the results in Table 3. Please refer to the paper for details.\n\n[2] Yuksekgonul, Mert et al. \u201cWhen and why vision-language models behave like bags-of-words, and what to do about it?\u201d ArXiv abs/2210.01936 (2022): n. pag.\n\n## Comparison with NegCLIP\nWe're still reproducing the NegCLIP to make the comparison. We're afraid that it cannot be finished by the end of the discussion period as it requires a lot of time to preprocess the 300K data and train the models. We will update the results as soon as possible.\n\nHowever, we want to note that BiAug and NegCLIP are not actually comparable because our focus is to mitigate the reporting bias issue by decoupling the object-attribute association for **both visual and langauge modalities**, while NegCLIP just simply shuffle the caption, without making any changes to the objects or attributes in the caption and the image."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678608668,
                "cdate": 1700678608668,
                "tmdate": 1700678608668,
                "mdate": 1700678608668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bUxtQOa1fj",
                "forum": "tVuZa1bgOs",
                "replyto": "cOd8X5Tz0v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for the response and revision of the paper. I think these responses address my concerns. I'll make decision on my final scores after reading other rebuttal materials and discussing with other reviewers"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682148382,
                "cdate": 1700682148382,
                "tmdate": 1700682148382,
                "mdate": 1700682148382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k7uaV5G86z",
            "forum": "tVuZa1bgOs",
            "replyto": "tVuZa1bgOs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz"
            ],
            "content": {
                "summary": {
                    "value": "**POST REBUTTAL NOTE FOR AUTHORS:**\n\nI appreciate the effort put into this paper and acknowledge that I have read your responses.\n\n-------------------------------------\n\n**PRE REBUTTAL REVIEW:**\n\nThis paper focuses on the phenomenon of \"reporting bias\" in VLMs. \"Reporting bias\" refers to the case where part of the text description of an image is not present due to omission of implicit/commonsense knowledge. Authors claim that mitigating \"reporting bias\" may help with two issues: 1) biased captions 2) skewing the VLM towards frequently occurring patterns. \n\nTo mitigate the \"reporting bias\", authors propose \"bi-modal augmentation\" with the goal of disentanglement of the object-attribute associations. More specifically, BiAug augments a source dataset with explicit commonsense knowledge. \n\nBiAug has three phases: 1) Cross-mode object extraction:identifies the objects that are not explicitly referred to in the caption, 2) Decouple object-attribute association: creates a pair of descriptions for the image and 3) Image synthesis: in-painting the image with the pair of descriptions found in phase (2).\n\nThe augmented datasets produced by BiAug are used to train models, which are then evaluated against benchmarks for object-attribute understanding and general vision-language retrieval tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-  \"Reporting bias\" is an important concept that affects the performance of machine learning models in understanding and processing real-world data.\n\n- The paper tackles the \"reporting bias\" problem and aims to provide a more nuanced dataset that includes objects and attributes often omitted due to reporting bias.\n\n- This paper could have implications for AI fairness: By addressing reporting bias, the research moves towards creating AI systems that can potentially reduce cultural and linguistic biases, contributing to the broader goal of AI fairness and ethics.\n\nOverall I found the problem to be very interesting and timely."
                },
                "weaknesses": {
                    "value": "- I find the framework overly complicated and built on fragile assumptions. For example, why can we assume that the grounding object detector can find objects that a different color or shape of it is still a valid object? How can we make sure that the detected grounding object is big enough for the in-painting model to process this? Also see my questions below. I believe this overall complexity hinders the generalization of the results. \n\n- Overall I did not find the paper easy to read. This is partly due to the fact that the methodology is very complex with lots of pieces. However, I think the authors need to spend more time thinking about the structure and also definitely re-writing the abstract. \n\n- Many important concepts used in the paper are not clearly explained. For example \"grounding object\" need clear definitions.\n\n- I find it hard to understand how the framework described in Figure 2 could be reliable for large scale experiments.\n\nMinor points:\n\n- I suggest having the term \"reporting bias\" italic or quoted as it is not a well-known term/concept. And it is not clear that it's a technical term. Specifically it is used in the beginning of the abstract as well as the introduction. I think both sentences in the start of the abstract and the start of the intro need to be re-written/clarified."
                },
                "questions": {
                    "value": "- Where do the samples in Figure 1 come from? Are they from a real dataset? Are they generated? \n- In Figure 2, in step 3, why \"tennis ball\" is dropped? It was part of the original text. Why \"dog\" is selected as the grounding object?\n- In section 3.2 how do you select from the list of predefined categories? For example, in figure 2, how come the \"shape\" is not selected for \"grass\"? Of course it does not make sense to modify the shape for grass, but how do you account for that and select \"color\"?\n- In 3.3 how do you mask the detected object? \n- In the example given in figure 3, what happens if non of the predicted objects are inside the image?\n- In figure 4, the \"blue boat\" and \"red boat\" samples, what was the missing reference object here according to your framework? What happened in the extraction phase? boat was already part of the original caption. \n- What is \"other\" in the preset attribute list?\n\nOverall, I found the figures and methodology very confusing. I am willing to modify my score based on the clarifications that the authors may provide. However, I am concerned with the overall writing and presentation of this work and I believe restructuring the paper to present the methodology in a more digestible and elaborate format could be very useful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5557/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz",
                        "ICLR.cc/2024/Conference/Submission5557/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699468206060,
            "cdate": 1699468206060,
            "tmdate": 1700852093206,
            "mdate": 1700852093206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eRkQn361OB",
                "forum": "tVuZa1bgOs",
                "replyto": "k7uaV5G86z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## We have refined the presentation in the updated version\n```\nQ: However, I think the authors need to spend more time thinking about the structure and also definitely re-writing the abstract.\n```\nThank you for your insightful questions and suggestions. For your main concern about the presentation of this paper, we have carefully rewritten the abstract and method, updated the figure2, and updated the demonstration of prompt for LLM, for better readability. Please see the updated paper. Please also see our clarification below to your questions about the detail of the method.\n\n```\nQ: Many important concepts used in the paper are not clearly explained. For example \"grounding object\" need clear definitions.\n```\nA: Thank you for the suggestion. I went through the paper and add necessary background information for better readability.\n\n## Concern about the relative complicated framework\nReporting bias (as its property suggested) is often overlooked in existing models and datasets. As a result, it is non-trivel to reveal and fix it, such that our method consists of different important steps. For your specific concerns, we clarify them point by point as follows,\n\n```\nQ: \u201cwhy can we assume that the grounding object detector can find objects that a different color or shape of it is still a valid object?\u201d\n```\nA: It is important to emphasize that our system utilizes the state-of-the-art grounding detector, Grounding DINO [1], for detecting valid objects. This advanced detector is particularly efficient in recognizing objects based on textual input alone, even in a zero-shot context. Notably, Grounding DINO achieves an impressive 60.7 mAP (refer to Table 2, [1]) in zero-shot object detection on the MSCOCO benchmark, without any prior training on this specific benchmark. This performance underscores its robust capability to reliably identify valid objects, which is essential for the effectiveness of our method. Though GroundingDINO is generalizable, we agree that it is not perfect and could give mistakes. However, in our qualitative/human inspections in Section4.4, we found that GroundingDINO is generally reliable. And we expect that more advanced grounding models will be developed in the community and can be easily integrated to BiAug.\n\n```\nQ: \u201cHow can we make sure that the detected grounding object is big enough for the in-painting model to process this?\u201d\n```\nA: We remove the detected boxes that are too large or too small to ensure they are valid for the following procedure. Please refer to the filtering strategies introduced in Section 4.1.\n\n```\nQ: \u201cHow the framework described in Figure 2 could be reliable for large scale experiments.\u201d\n```\nA: We apply multiple filtering strategies, as introduced in Section 4.1, to remove low-quality grounding objects/augmented captions/generated images. Filtering strategies makes the examples we finally used for training are reliable as possible. We agree that we still cannot avoid noisy examples, even BiAug is utilizing state-of-the-art language and visual foundation models. The experimental results on general visual-language retrieval task shown in Table 3, also demonstrate that the dataset augmented by BiAug is reliable enough for training a VL model, as the performance keeps increasing with finetuning on the augmented dataset.\n\n[1] Liu, Shilong et al. \u201cGrounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.\u201d ArXiv abs/2303.05499 (2023): n. pag."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678514286,
                "cdate": 1700678514286,
                "tmdate": 1700678514286,
                "mdate": 1700678514286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "07XLZbm5xM",
                "forum": "tVuZa1bgOs",
                "replyto": "k7uaV5G86z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5557/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to express our sincere gratitude for the time and effort you have invested in reviewing our work. Your insights and comments have been invaluable in refining our manuscript.\n\nWe are eager to learn your perspective on our responses to your comments. If there are any points or concerns that you believe have not been adequately addressed, please let us know and we are willing to provide further clarification and responses."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5557/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678878963,
                "cdate": 1700678878963,
                "tmdate": 1700678878963,
                "mdate": 1700678878963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]