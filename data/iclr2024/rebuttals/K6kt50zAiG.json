[
    {
        "title": "CAMBranch: Contrastive Learning with Augmented MILPs for Branching"
    },
    {
        "review": {
            "id": "Qgc2zlgYR7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
            ],
            "forum": "K6kt50zAiG",
            "replyto": "K6kt50zAiG",
            "content": {
                "summary": {
                    "value": "Update on November 19:\n\nI raise my score to 6 based on the authors' responses. I am willing to keep discussing with the authors and the other reviewers to achieve a fully discussed final score.\n\n---\nThis paper point out the shifting equivalence of the MILP problems. Then, the authors propose a contrastive learning approach based on the augmented MILP problems. Experiments demonstrate that CAMBranch achieves high performance with only 10% of the complete dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Practical motivation. In practice, I found the similar problem that generating expert demonstration for industrial-level datasets is extremely time-consuming. Thus, the motivation of this paper is practical.\n2. Clear writing. The paper is clearly structured and easy to go through flow.\n3. Simple and effective approach. The idea of the shifting equivalence is simple and effective. Previous research uses GNN to tackle the symmetry in row and column orders. In this paper, the authors explicitly use the shifting equivalence via contrastive learning to enhance the training efficiency."
                },
                "weaknesses": {
                    "value": "1. The illustration of experimental results requires to be improved. The bars missing detailed values on it. Compared with histograms, tables could be more compact for demonstration.\n2. Missing comparisons to other ML approaches. Employing auxiliary tasks is an effective way to improve the training efficiency. Empirically, I found the simple auxiliary task employed in [1] can efficiently promote the training efficiency. Thus, can you provide more results comparing this data augmentation approach to other auxiliary tasks? \n3. Marginal improvement in Figure 1. It seems the improvement of CAMBranch is not significant. Maybe a lighter GNN with fewer hidden layers and hidden nodes can achieve similar IL accuracy but requires less expert data."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd",
                        "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697458232018,
            "cdate": 1697458232018,
            "tmdate": 1700342584195,
            "mdate": 1700342584195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0SjjvhYMur",
                "forum": "K6kt50zAiG",
                "replyto": "Qgc2zlgYR7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EfKd"
                    },
                    "comment": {
                        "value": "Thanks for your valuable insights and suggestions. Here are our responses:\n\n1. \n\nThanks for pointing this out. Actually, we have displayed the table-format results in Appendix Table 5. In the revised version, we will place Table 5 into the main text.\n\n2. \n\nMaybe missing a bibliography [1]. Could you please add it? Thanks for your feedback!\n\nIn Gasse et al. (2019), experiments results have demonstrated that GCNN outperforms other ML approaches by a large margin. Therefore, we did not include these ML approaches and CAMBranch will certainly outperform them now that CAMBranch\u2019s performance is close to GCNN\u2019s. \n\nFurthermore, it's essential to note that CAMBranch is a plug-and-play method, ensuring ease of implementation without conflicts with other auxiliary methods in most cases, and can be combined with other auxiliary methods. This adaptability and compatibility contribute to its versatility and potential for integration with other methods.\n\n3. \n\nCurrently, the model architecture of CAMBranch follows Gasse et al. (2019) and it only has ~10k parameters, light enough compared with those extremely deep graph neural networks like DeeperGCN. The hidden size is only 64 and extremely small for graph neural networks. The hidden layer is only one. \n\nTo explore whether lighter GNN with less expert data can achieve similar IL accuracy or not, we conducted additional experiments. In our additional experiments, conducted with the same 10% training data (less expert data), we trained the GCNN while adjusting the hidden size to 32 and 16. Imitation results on Set Covering are shown in the following. The results highlight that merely reducing the hidden size leads to negligible performance changes. In contrast, our proposed CAMBranch significantly outperforms these approaches.\n\n|Model|acc@1|acc@5|acc@10|\n|---|---|---|---|\n|GCNN (10%, hidden size 16)|59.56 \u00b1 0.70|82.75 \u00b1 0.98|91.27 \u00b1 0.67|\n|GCNN (10%, hidden size 32)|59.65 \u00b1 0.30|83.36 \u00b1 0.39|91.63 \u00b1 0.22|\n|GCNN (10%, hidden size 64)|58.98 \u00b1 0.69|82.97 \u00b1 0.52|91.61 \u00b1 0.44|\n|CAMBranch (10%, hidden size 64)|**65.27 \u00b1 3.92**|**89.46 \u00b1 2.98**|**96.14 \u00b1 1.90**|\n\nSignificantly, when considering MILP solving, while imitation learning (IL) accuracy holds importance, it isn't the primary metric of concern. In contrast, time, Nodes, and wins are the primary metrics and can actually measure the performance of a neural branching strategy. This discussion is detailed in the Appendix E Discussion section. It's crucial to note that while Strong Branching serves as the expert strategy for imitation learning, it might not represent the optimal branching strategy. Therefore, IL accuracy in imitating Strong Branching merely stands as an intermediate result and cannot fully demonstrate the models\u2019 performance. \n\nThe true essence lies in the metrics evaluated during the inference stage\u2014solving instances fast, and generating fewer nodes. Figures 2, 3, and 4 display the results, demonstrating CAMBranch's superior performance in these crucial metrics, especially evident in handling significantly harder instances."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988305180,
                "cdate": 1699988305180,
                "tmdate": 1699988305180,
                "mdate": 1699988305180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fDfJsX67jf",
                "forum": "K6kt50zAiG",
                "replyto": "0SjjvhYMur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for pointing out the bibliography [1]. We have added it below. The simple auxiliary task employed in [1] is proposed to improve the learning accuracy of the proposed hybrid model. However, in practice, we found this auxiliary task may also improve the training efficiency with limited available data.\n\nBased on the results provided in your response, I have two further questions&suggestions:\n1. What about the training time of CAMBranch? As the main motivation of this paper is to reduce the time for data collection, I think the comparison on training overhead should also be considered together. \n2. Results on real-world MILP benchmarks are recommended. MILP instances from industry are usually more complex and challenging. Thus, the time-consuming data collection process on these benchmarks becomes the key bottleneck for ML-based optimization. Data collection on the four synthetic benchmarks is usually efficient. You might want to conduct experiments on real-world benchmarks like those in the NeurIPS ML4CO competition to further support your motivation.\n\nAnyway, the results in the paper and your response (both to me and to the other reviewers) are interesting and promising. Thus, I slightly raise my score to 6. I am willing to keep discussing with the authors and the other reviewers to achieve a fully discussed final score.\n\n[1] Gupta, Prateek, et al. \"Hybrid models for learning to branch.\" Advances in neural information processing systems 33 (2020): 18087-18097."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342313023,
                "cdate": 1700342313023,
                "tmdate": 1700342313023,
                "mdate": 1700342313023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BONwim7PkK",
                "forum": "K6kt50zAiG",
                "replyto": "Qgc2zlgYR7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EfKd"
                    },
                    "comment": {
                        "value": "Thank you for your further suggestions. Regarding the comparison of training overhead, we acknowledge that CAMBranch requires additional computations compared to the original GCNN in Gasee et al. (2019). To this end, we evaluated the computational burden for both models, measured in terms of training time per batch. Below are the results of training on Set Covering with each batch containing 64 samples. From the results, we can see that, despite the introduced computation by CAMBranch, the observed difference in computational speed is deemed acceptable\u2014merely a matter of several milliseconds. In summary, the training overhead associated with CAMBranch is within acceptable bounds.\n\n|Model|Training time per batch (s)|\n|---|---|\n|CAMBranch (10%)\t|0.09666|\n|GCNN (10%)|\t0.09216|\n\nWe appreciate your second suggestion as well. Aligning with established works in this domain, such as Gasse et al. (2019) and Gupta et al. (2020), we presented the results of our proposed method on four classical datasets. These results underscore the potential of our approach. Then, as you pointed out, the industry presents more challenging MILPs with prolonged collection processes. In such scenarios, CAMBranch's augmentation component outpaces Strong Branching solving in generating labeled expert samples. Unlike the latter, CAMBranch removes the need to solve NP-hard instances, directly generating samples based on the collected small parts of data.\n\nHere, we first consider the Capacitated Facility Location Problem, which exhibits the lowest collection efficiency among the four MILP benchmarks in our paper. Generating 100k expert samples using Strong Branching to solve the instances takes 84.79 hours. In contrast, if obtaining the same quantity of expert samples, CAMBranch requires 8.48 hours (collecting 10k samples initially) plus 0.28 hours (generating the remaining 90k samples based on the initial 10k), totaling 8.76 hours\u2014an **89.67% time savings.** The advantages will probably be more pronounced in more challenging benchmarks. Although our current results are based on the four classic benchmarks, the underlying idea of CAMBranch is promising and will promisingly offer insights to the community, which will also lead to further efforts for this community. We appreciate your valuable suggestion and will be committed to further exploration and contribution to our community based on your insights."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700372046460,
                "cdate": 1700372046460,
                "tmdate": 1700372157255,
                "mdate": 1700372157255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hRqrAwCTt8",
                "forum": "K6kt50zAiG",
                "replyto": "BONwim7PkK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_EfKd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and effort in addressing all of my comments. Most of them have been properly addressed. I will keep my current rating (6) and encourage the authors to further improve the paper according to my suggestions above."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408027534,
                "cdate": 1700408027534,
                "tmdate": 1700408027534,
                "mdate": 1700408027534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dPtd0Wt3pT",
            "forum": "K6kt50zAiG",
            "replyto": "K6kt50zAiG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_8jS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_8jS2"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose **CAMBranch** an innovative approach that seeks to enhance the efficiency of Branch and Bound (B&B) algorithms for solving Mixed Integer Linear Programming (MILP) problems. Traditional B&B methods, while reliable, can be computationally intensive, motivating the exploration of machine learning to improve branching decisions. In particular, **CAMBranch** employs a machine learning framework based on contrastive learning and uses Augmented MILPs (AMILPs) to inform its branching strategies.\n\nThe key innovation of in the work lies in its utilization of AMILPs, which are equipped with expert decision labels, making them suitable for imitation learning. This approach aims to mimic the performance of Strong Branching, a highly effective but computationally expensive B&B policy. By leveraging the relationships between MILP and AMILP, the method extracts and utilizes node features from an augmented bipartite graph to inform its branching decisions.\n\nThe strengths include its novel application of contrastive learning in the domain of optimization and its potential to significantly reduce the computational burden associated with Strong Branching. The use of AMILPs for imitation learning could lead to more efficient and informed branching strategies, potentially improving the overall performance of B&B algorithms.\n\nHowever, the success of the proposed algorithm hinges on the accuracy of the imitation learning model. If the model fails to capture the subtleties of Strong Branching, it could result in suboptimal decisions. Additionally, the integration of machine learning into B&B algorithms may introduce computational overhead, which could diminish the anticipated efficiency gains. Lastly, the generalizability of **CAMBranch** across diverse MILP problems remains to be thoroughly evaluated, as its effectiveness may vary depending on the problem's characteristics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n* Innovation in Branch and Bound (B&B) Methods: The paper discusses recent advancements in machine learning frameworks to enhance B&B branching policies for solving Mixed Integer Linear Programming (MILP), indicating a focus on innovation and improvement of existing methods.\n* Potential for Improved Efficiency: If these machine learning-based methods can successfully imitate Strong Branching, they may offer more efficient alternatives to traditional B&B methods, which can be computationally intensive."
                },
                "weaknesses": {
                    "value": "Potential weaknesses of CAMBRANCH:\n\n1. **Fidelity of Imitation Learning:** The effectiveness  is predicated on the assumption that imitation learning can closely approximate the decisions made by Strong Branching. However, Strong Branching is known for its nuanced decision-making process, which considers a multitude of factors and potential future states. Replicating this complexity through imitation learning is challenging. If the learning model fails to encapsulate the depth of Strong Branching's strategy, the resultant branching decisions could be significantly less efficient, negating the primary advantage of CAMBRANCH.\n\n2. **Computational Overhead of Machine Learning Integration:** While machine learning models offer the promise of improved decision-making, they also introduce computational overhead. Training models, especially those based on contrastive learning, require significant computational resources. Furthermore, the real-time application of these models within the iterative B&B process could lead to increased computational demands, potentially offsetting the efficiency gains from more informed branching.\n\n3. **Sensitivity to Hyperparameter Tuning:** Machine learning models, particularly those used in imitation learning, are sensitive to hyperparameter settings. The performance of CAMBRANCH could be highly dependent on the choice of learning rate, batch size, and other hyperparameters. Finding the optimal configuration can be a time-consuming process that requires extensive experimentation and computational resources.\n\n4. **Generalizability and Robustness Concerns:** The diversity of MILP problems poses a significant challenge to the generalizability of CAMBRANCH. Different MILP instances can vary drastically in terms of size, structure, and complexity. CAMBRANCH must demonstrate robust performance across a wide array of problems to be considered a viable alternative to existing B&B methods. Additionally, the model's robustness to adversarial inputs or unusual problem structures remains to be thoroughly evaluated.\n\nIn conclusion, while CAMBRANCH presents an innovative approach to improving B&B algorithms for MILP problems, its success is contingent upon overcoming the intricate challenges associated with imitation learning, computational efficiency, hyperparameter sensitivity, and the robustness of its application across diverse problem sets."
                },
                "questions": {
                    "value": "1. **How does the model's architecture influence the quality of imitation learning?**\n   - The architecture of the neural network used for imitation learning plays a crucial role in its ability to capture the decision-making process of Strong Branching. How does the choice of architecture impact the model's ability to generalize across diverse MILP instances?\n\n2. **What is the impact of contrastive learning's positive and negative sample selection on the performance?**\n   - In contrastive learning, the selection of positive and negative samples is critical for learning meaningful representations. How does CAMBranch ensure the selection of informative positive and negative samples during training? Could the incorporation of hard negative mining or other advanced sampling strategies improve the model's performance?\n\n3. **How does the algorithm address the exploration-exploitation trade-off during the B&B process?**\n   - The Branch and Bound algorithm involves a delicate balance between exploring new branches and exploiting known promising paths. How does CAMBranch navigate this trade-off? Are there mechanisms in place to prevent premature convergence on suboptimal branches or to encourage exploration when necessary?\n\n4. **How does CAMBranch handle the interpretability and explainability of its branching decisions?**"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864942301,
            "cdate": 1698864942301,
            "tmdate": 1699636323734,
            "mdate": 1699636323734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RxK9TaTSPU",
                "forum": "K6kt50zAiG",
                "replyto": "dPtd0Wt3pT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8jS2"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. Maybe there are misunderstanding and we will clarify it in the following responses.\n\n **Response to the questions**\n1. \n\nThanks for pointing this out. There may be some misunderstanding for *\"generalize across diverse MILP instances\"*. In our work, we follow Gasse et al. (2019) and focus on homogeneous MILP solving, i.e., the training and test MILP instances are the same type of problems, which is a popular setting in industry practice. Such models do not need to generalize across diverse MILP instances in the homogeneous setting but need to generalize across the same type of instances, i.e., those with different coefficients and different scales of variables and constraints.\n\nWithin this specialized context, we prefer to choose light graph neural networks due to their high efficiency during the inference phase. Following Gasse et al. (2019), we leverage the GCNN as CAMBranch\u2019s backbone model. GCNN can capture the features of MILP bipartite graphs and exhibits robust learning capabilities from expert samples during the imitation learning phases.\n\n2. \n\nIn CAMBranch, as detailed in our paper, \u201cwe leverage this principle by viewing a MILP and its corresponding AMILP as positive pairs while considering the MILP and other AMILPs within the same batch as negative pairs.\u201d This follows the simplest way of negative sampling strategy in contrastive learning, and our results already demonstrate its contribution to superior performance. Moreover, incorporating more advanced hard negative sampling strategies is a choice to further improve the model\u2019s performance. \n\n3. \n\nSince the CAMBranch pipeline operates in two stages, the exploration-exploitation trade-off isn't our concern. Specifically, we first collect expert samples for imitation learning, followed by the training of the policy model after this collection phase. Once the model is trained, it becomes directly available for inference without the necessity to address mechanisms preventing premature convergence.\n\n4.\n\nGiven that our CAMBranch methodology imitates Strong Branching, it aligns with Strong Branching decisions in most cases. The interpretability of neural branching remains an ongoing research focus and an open problem in this field, and we intend to explore this further in our future work.\n\n**Response to the weaknesses**\n\n1. As depicted in the results presented by Gasse et al. (2019), the imitation learning paradigm demonstrates remarkable superiority in this domain. Since CAMBranch follows Gasse et al. (2019), CAMBranch also demonstrates superior performance. Therefore, there is no such concern.\n\n2. Indeed, our CAMBranch model demands modest computational resources during training phases. For the inference phases, it is the same as the models in Gasse et al. (2019), both of which show faster solving speed, especially in hard instances.\n\n3. Since our CAMBranch follows Gasse et al. (2019) and the model backbone are extremely light graph neural networks, grid search for hyperparameter tuning does not need too many resouces. Actually, both CAMBranch and Gasse et al.(2019) are not sensitive to the hyperparameter tuning, further streamlining the optimization process.\n\n4. As responding in Question 1, \u201cwe focus on homogeneous MILP solving, i.e., the training and test MILP instances are the same type, which is a popular setting in industry practice.\u201d From the results in our paper, CAMBranch as well as GCNN in Gasse et al. (2019) obviously generalize well in homogeneous MILP instances. Therefore, no such concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988997723,
                "cdate": 1699988997723,
                "tmdate": 1699991364329,
                "mdate": 1699991364329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lQqRGn2SGQ",
                "forum": "K6kt50zAiG",
                "replyto": "dPtd0Wt3pT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8jS2"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8jS2,\n\nWe have revised our paper according to the other three reviewers' suggestions. \n\nWe would appreciate it if you could confirm that our responses address your concerns. We are happy to answer more if you have any remaining concerns or questions.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606823103,
                "cdate": 1700606823103,
                "tmdate": 1700606834067,
                "mdate": 1700606834067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eAKTDVd6fy",
            "forum": "K6kt50zAiG",
            "replyto": "K6kt50zAiG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes CAMBranch, which generates augmented MILPs with shifting variables and identical variable selection decisions, and uses contrastive learning ot use the augmented MILPs for learning to branch. Experiments demonstrate that CAMBranch can effectively improve the sample efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper identifies the challenge of  sampling time in Learn2Branch, which is meaningful.\n2. The augmentation strategy has a theoretical gurantee and is insightful."
                },
                "weaknesses": {
                    "value": "1. CAMBranch does not perform well on relatively easy datasets. The improvement is not significant compared with GCNN baseline on many medium and hard datasets. Fow example, results in Table 8 do not reveal that CAMBranch consistently outperforms GCNN.\n2. The augmentation method is tailored for the branching task, while not easy to be transfered to other algorithms in B & B solvers. Therefore, the application is limited.\n3. Which sample ratio, with CAMBranch, leads to a comparative results with the model trained with the full dataset? What if use the full training dataset  with this augmentation? Will it stll bring improvement?\n4. How many instances does it need to generate 20k expert samples? The 10% means using 10% MILP instances or 10% expert samples? How many expert samples can one instance produce? Experiments should be conduct to investigate the effect of the number of MILP instances and the expert samples. Also, the accumulative time used in collecting the expert samples should be reported."
                },
                "questions": {
                    "value": "1. Sometimes GCNN (10%) even outperforms GCNN (e.g., MIS in Figure 2). Why?\n2. See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq",
                        "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895874521,
            "cdate": 1698895874521,
            "tmdate": 1700669007627,
            "mdate": 1700669007627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6F5gB5Z2QL",
                "forum": "K6kt50zAiG",
                "replyto": "eAKTDVd6fy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mGEq"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. CAMBranch aims to mitigate issues of the extremely long collection time of expert samples by innovatively generating new expert samples from existing ones. Notably, CAMBranch leverages only a small fraction of the expert samples, compared to the GCNN method in Gasse et al., (2019). Its adaptable nature makes it a plug-and-play solution, seamlessly integrated into various neural branching methods for B&B applications.\n\n**Response to the weaknesses:**\n1. \n\nThanks for highlighting this. While CAMBranch might not consistently outperform GCNN in a small subset of easy and medium instances, it's important to note that the differences are marginal in these scenarios. However, the true strength of CAMBranch shines through in harder instances. \n\nFor instance, in Table 5, which is the primary focus of this paper, the Combinatorial Auction Problem at the medium level reveals that CAMBranch performs with a mere 0.3s difference compared to GCNN(10%), specifically 12.68s versus 12.38s. This trend is consistent across most cases where CAMBranch doesn't outperform GCNN (10%), as similarly observed in Table 8. \n\nHowever, when tackling genuinely challenging instances, especially in Set Covering, Capacitated Facility Location, and Maximum Independent Set (problems significantly more complex than the hard Combinatorial Auction Problem, evident from the solving times and the number of instances successfully solved), CAMBranch distinctly demonstrates its superiority in solving these intricate problems. For instance, in Set Covering, CAMBranch has a solving time of 1427.02s compared to GCNN 10%'s 2385.23s. This exemplifies the practical utility of CAMBranch since real-world instances are rarely as simple as the Combinatorial Auction Problem; most instances pose challenges of much higher complexity, such as hard Set Covering problems.\n\nTaking this into account, CAMBranch serves as a robust, plug-and-play solution, especially in scenarios where inefficient data collection hampers performance. It consistently exhibits high efficacy in MILP problem-solving while requiring fewer resources, making it a compelling choice for improving methodologies in such settings.\n\n2. \n\nCAMBranch, its plug-and-play nature facilitates easy implementation, specifically enhancing the neural branching decision part in the B&B algorithm. While the primary focus of CAMBranch is on the branching part, it's worth noting that the core idea of generating expert samples sharing identical decisions from the original ones holds potential insights for other decision parts within B&B solvers, such as node selection and cutting planes. The fundamental idea of generating new labeled expert samples from original expert decisions holds promise for broader applicability within the B&B framework.\n\n3. \n\nThe CAMBranch method is proposed to alleviate challenges arising from inefficient expert sampling.  Therefore, in such contexts, it is more meaningful to use CAMBranch when facing data-scarce issues. While CAMBranch is primarily tailored to address data scarcity issues, its adaptability allows users to combine this methodology with a full training dataset. This integration aids in the form of data augmentation, further fostering improvements in the training process. \n\n4. \n\nThanks for pointing these out. In the introduction part, we report that collecting 100k expert samples for four types of problems requires 26.65 hours, 12.48 hours, 84.79 hours, and 53.45 hours, respectively. Therefore, collecting 20k experts\uff0820% of the whole data) needs about 20% of the former time periods. \n\nThe 10% means 10% expert samples, not MILP instances. Generating MILP instances is trivial, but acquiring the expert samples is challenging since numerous NP-hard MILP instances need to be solved, making it a time-consuming endeavor.\n\nAdditionally, it's worth noting that Gasse et al. (2019) extensively explored the relationship between the number of solved MILP instances and expert samples in their Appendix Section 1. Our proposed methods have nearly identical observations to Gasse et al. (2019).\n\n**Response to the question:**\n\nYes, it is an interesting observation and we've thoroughly investigated this observation through multiple experiments, consistently confirming its occurrence. One potential reason is that Strong Branching is not always the optimal expert strategy for branching and sometimes may lead to sub-optimal decisions, even if it makes high-quality decisions most of the time. This aligns with similar observations outlined in recent studies by Scavuzzo et al. (2022), Dey et al. (2023), and Gamrath et al. (2020). Refer to our Appendix Discussion part, \u201cRecent studies by Scavuzzo (2022), Dey (2023), and Gamrath (2020) have even suggested that Strong Branching may underperform in certain cases, falling short of problem-specific rules.\u201d This intriguing observation inspires us to delve into this phenomenon, for more exploration in future work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987326948,
                "cdate": 1699987326948,
                "tmdate": 1699988389858,
                "mdate": 1699988389858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H0UtHfuIXi",
                "forum": "K6kt50zAiG",
                "replyto": "6F5gB5Z2QL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. My comments are as below.\n1. It would be better if the authors could provide further insights on why CAMBranch performs better on genuinely challenging instances than on easy instances.\n1. I still think the main contribution is limited in the branching task. The idea of generating new labeled expert samples is not novel, and what matters is the augmenting strategy.\n1. What I really challenge in Weakness 3 and 4 are as follows. When we have a collection of MILP instances, we have to determine whether we should solve all of them to obtain expert samples or solve parts of them and use CAMBranch to obtain augmented ones. Here is a trade-off between the efficiency and the performance. So I think the authors should provide more quantitative information on in which cases we should use CAMBranch. For example, futhter analysis on the relationship between the performance and the number of MILP instances, expert samples, and the number of augmented samples. If the authors can provide this, I would like to raise my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463784403,
                "cdate": 1700463784403,
                "tmdate": 1700463784403,
                "mdate": 1700463784403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pLo5B2eHhP",
                "forum": "K6kt50zAiG",
                "replyto": "whSHmFI3v7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_mGEq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses. I believe the additional experiments will provide meaningful insights. I have raised my score from  5 to 6. Since these experiments are only conducted on one datasets, I expect further results on more datasets and more hyperparameters."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668984400,
                "cdate": 1700668984400,
                "tmdate": 1700668984400,
                "mdate": 1700668984400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R24dUnvATR",
            "forum": "K6kt50zAiG",
            "replyto": "K6kt50zAiG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a data-efficient imitation learning algorithm for learning a branching policy in B&B algorithm.\nThe proposed method augments demonstration data of Strong Branching decisions using mathematically complete rule and introduces contrastive learning by using the augmented data as positive samples.\nExperiment results demonstrate that the proposed method can significantly outperform existing imitation learning-based MILP solvers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method of augmentation rule and contrastive learning idea is well-motivated, concise and effective.  \nThe performance improvement from previous method in low-data situations is significant."
                },
                "weaknesses": {
                    "value": "The presentation of main result should be improved.\n- CAMBranch (100%) result should be discussed. Despite this work aiming in data scarce situation, understanding how it behaves with the full dataset is critical information, considering the results with 10% of data is not outperforming the baseline using full data.\nEven if CAMBranch using the full data can't outperform the baseline using full data, it is still beneficial to share this limitation with the community if proper discussion is provided.\nHowever, without the result, I am negative about introducing this method to the community as a MILP solver because it is unknown whether it limits the potential of imitation learning-based MILP methods.\n- The main results appear to be promising but could be presented better. Neither Figure 2-4 nor Table 5 looks optimal in their current forms.\n- The result of Table 8 deserves to be in main text, considering the goal of this work. Also, evaluation in another domain with more dramatic performance gap could be better for the comparison. The presentation also has room for improvement; it is hard to compare the metrics of models using the same amount of data.\n\nIf this concern is resolved I am willing to vote for accepting this paper.\nHowever, as it seems that a major revision is required, including the core results of the paper, I also agree if other reviewers or AC recommend submitting this work to next venue."
                },
                "questions": {
                    "value": "What would be the intuition behind using shifted geometric mean for evaluation?  And what is the $s$ value for each metric?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3672/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700038880450,
            "cdate": 1700038880450,
            "tmdate": 1700656001789,
            "mdate": 1700656001789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1wM6GxgOw8",
                "forum": "K6kt50zAiG",
                "replyto": "R24dUnvATR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vbTP"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback and suggestions. The inspiration behind the CAMBranch project stems from real-world challenges encountered in the industry, i.e., the low sample efficiency observed during the collection of expert samples for imitation learning. To mitigate this, CAMBranch introduces a solution by generating labeled expert samples from the original datasets. This approach alleviates the need to collect a large quantity of expert samples. With a relatively small amount of data, we achieve superior and acceptable performance. The implications of this approach are highly meaningful in real-world applications.\n\n**Response to the weaknesses:**\n\n1.\n\nThanks for pointing this out. Your concern is that \u201c*it is unknown whether it limits the potential of imitation learning-based MILP methods.*\u201d While we understand that discussing CAMBranch (100%) compared to GCNN (100%) is one approach to alleviate this concern directly, we would like to propose an alternative way, i.e., to compare CAMBranch (10%) and GCNN (10%) since you can **assume the full datasets have 10k expert samples** which are also enough for training usually and the performance for GCNN (10%) is still acceptable in most of the cases. Therefore, after comparing GCNN (10%) and CAMBranch (10%), we can observe that CAMBranch (10%) shows high superiority. Thus, this observation suggests that CAMBranch will probably not limit the potential of imitation learning-based methods. Experiments for CAMBranch (100%) are also as the following. Here for combinatorial auction problem. The same observations are obtained.\n\nEasy\n|Model|Time|Wins|Nodes|\n|---|---|---|---|\nGCNN (100%)\t|1.96\t|4/100|\t87\nGCNN (10%)\t|1.99\t|2/100|\t102\nCAMBranch (10%)\t|2.03|\t1/100|\t91\nCAMBranch (100%)|\t1.73|\t93/100|\t88\n\nMedium\n|Model|Time|Wins|Nodes|\n|---|---|---|---|\nGCNN (100%)\t|11.30|\t7/100|\t695\nGCNN (10%)|\t12.38|\t3/100|\t787\nCAMBranch (10%)|\t12.68|\t2/100|\t758\nCAMBranch (100%)|\t10.04|\t88/100|\t690\n\nHard\n|Model|Time|Wins|Nodes|\n|---|---|---|---|\nGCNN (100%)|\t158.81|\t4/94|\t12089\nGCNN (10%)|\t144.40|\t2/100|\t10031\nCAMBranch (10%)|\t131.79|\t11/100|\t9074\nCAMBranch (100%)|\t109.96|\t83/100|\t8260\n\nIn fact, In practical scenarios, it is hard to measure the exact number of samples required to obtain an acceptable policy model. For some problems, maybe 10k is enough, while for others 100k, 200k, or more will lead to a better performance. In such contexts, CAMBranch is proposed to ensure that with any available dataset, one can use CAMBranch\u2019s augmented approach to train a powerful policy model. Notably, CAMBranch initiates the process with **data augmentation**, enabling the generation of **Hundreds of Thousands of labeled samples** that are totally different data one another for neural networks. CAMBranch significantly mitigates the time-intensive data collection process by providing an abundance of augmented expert samples. The results demonstrate CAMBranch\u2019s superiority and highlight its value.\n\n2.\n\nTable 5 follows the paper by Gasse et al. (2019), a format also employed by other papers within this domain. We have presented Figures 2-4 as a visual representation of the data in Table 5, as we believe it provides a clearer comparison of the gaps between different methods in different difficulty levels. We are open to exploring alternative methods to present the results and welcome any suggestions you may have. Thank you for your input.\n\n3.\n\nThank you for your advice. While we aspire to include Table 8 in the main text, space constraints have compelled us to make the paper as concise as possible. Given the abundance of intriguing observations that warrant discussion, we have chosen to place Table 8 in the Appendix to ensure a comprehensive exploration.\n\n**Response to the question:**\n\nUtilizing the shifted geometric mean for metric calculations stands as a widely accepted practice in MILP benchmarking within the community, which is also employed by Gasse et al. (2019) and Zarpellon et al. (2021), two representative papers in this domain. Shifted geometric means\u00a0have the advantage of neither being compromised by very large outliers (in contrast to arithmetic means) nor by very small outliers (in contrast to geometric means). \n\nThe solving time for different instances, even the same instance solved with different seeds, inherently has the potential for substantial variations due to the NP-hard nature. Therefore, the community use shifted geometric mean to facilitate a more equitable comparison. In such contexts, $s$ in our paper is set to 1 for time and 100 for Nodes, following the previous work.\n\n\n**References**:\n\nGasse, M., Ch\u00e9telat, D., Ferroni, N., Charlin, L., & Lodi, A. (2019). Exact combinatorial optimization with graph convolutional neural networks, NIPS.\n\nZarpellon, G., Jo, J., Lodi, A., & Bengio, Y. (2021, May). Parameterizing branch-and-bound search trees to learn branching policies. AAAI."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082837185,
                "cdate": 1700082837185,
                "tmdate": 1700191597682,
                "mdate": 1700191597682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SyCvKOUAqD",
                "forum": "K6kt50zAiG",
                "replyto": "1wM6GxgOw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3672/Reviewer_vbTP"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the author's response.\n\nThe author's response addressed the most important concern I had, including the crucial experimental result.\nI would vote to accept this work when those modifications are well-reflected in the revised version.\n\nSome responses to the author's comments:\n\n2. I apologize for the vague feedback.\nAs proposed by the reviewer EfKd, Table 5 seems to be a better format if presented with some effort on readability (e.g., bold, different line styles).\nAlternatively, the main text can include only one difficulty for the domain (e.g., hard) and provide results on other difficulties in the appendix unless the authors have any important argument on comparison results tendency over different difficulties.\nIn this case, the authors may be able to keep the graph format while showing comparisons of different metrics (i.e., time, node, and win) in one row.\nThis is a suggestion and may not be the optimal format as well, but I believe the current presentation should be improved.\n\n3. I also think the main results in Figure 2-4 should be prioritized over the results in Table 8.\nI suggest, but do not insist, moving Table 8 to the main text, especially if the authors make the main results presentation more compact (as proposed in 2. in this comment) and have more space in the paper.\nStill, as mentioned in the first review, Table 8 results can be improved by changing the evaluation domain and better presenting the results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451877074,
                "cdate": 1700451877074,
                "tmdate": 1700451877074,
                "mdate": 1700451877074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]