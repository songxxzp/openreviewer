[
    {
        "title": "More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes"
    },
    {
        "review": {
            "id": "lWNIekObYx",
            "forum": "2Oiee202rd",
            "replyto": "2Oiee202rd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_VSta"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_VSta"
            ],
            "content": {
                "summary": {
                    "value": "The authors show that adding text that describes the context of an object in an image can improve the performance of CLIP-based zero-shot classification. In addition, they show that said context can be inferred using CLIP itself."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- The paper shows some evidence towards towards the fact that CLIP representations do capture the context as well as the foreground objects, making the alignment with text prompts better when the appropriate context is included in the text.\n2- Although the improvements in performance are not large in many of the benchmarks, they come at little cost, probably making it applicable in practice."
                },
                "weaknesses": {
                    "value": "I haven\u2019t found any major weakness in this work (although it is not fully within my expertise)."
                },
                "questions": {
                    "value": "Some minor issues:\n- For some of the experiments I couldn\u2019t find if they employed ClassAttr or PureAttr.\n- In Algo 1, I assume it should be \u201cSet of classes Y\u201d rather than \u201cclass Y\u201d, and that the sum is over y \\in Y."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697915060562,
            "cdate": 1697915060562,
            "tmdate": 1699636615212,
            "mdate": 1699636615212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HfPAmxfPki",
                "forum": "2Oiee202rd",
                "replyto": "lWNIekObYx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VSta"
                    },
                    "comment": {
                        "value": "Thanks for your support! We have incorporated all the suggestions from all reviewers into the revised version. Below, we address your questions.\n\n> **Q1:** For some of the experiments I couldn\u2019t find if they employed ClassAttr or PureAttr.\n\nThank you for highlighting the need for clarity regarding the use of ClassAttr or PureAttr in our experiments. In all the experiments, we use ClassAttr if not specified (noted in Appendix E.4). To make it more clear, we have now included a specific note in Section 7 of our revised paper.\n\n> **Q2:** In Algo 1, I assume it should be \u201cSet of classes Y\u201d rather than \u201cclass Y\u201d, and that the sum is over y \\in Y.\n\nThanks for the question. In our paper, we use $Y$ to denote the random variable and $y$ to denote its realization. While, in practice, it represents a set of classes, we opted for the term \"class Y\" in the algorithm for mathematical conciseness. Your point about the summation is correct. It indeed encompasses all possible realizations. \n\n---\nThanks again for your time and effort in reviewing our paper! We are happy to have more discussions for any further questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674647730,
                "cdate": 1700674647730,
                "tmdate": 1700674647730,
                "mdate": 1700674647730,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AXlFUFtCHU",
            "forum": "2Oiee202rd",
            "replyto": "2Oiee202rd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores text-prompting in CLIP, to make the inference process more ``human-like\u201d. The authors show that their two-phased prompting process improves (i) performance, and (ii) robustness against specious features (shortcuts)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In this work, prompting is class-independent; which makes it easily applicable to numerous datasets. \n- Authors systematically evaluate the prowess of CLIP in inferring contextual attributes\n- The performance gain on domain-specific and out-of-distribution ImageNet datasets shows promise in the claims and approach of the authors\n- This work allows for building domain-specific (yet, class independent) augmentations with the possibility of human-in-loop intervention\n- The approach presented is elegant and interpretable"
                },
                "weaknesses": {
                    "value": "- If I understand correctly, Figure 3 gives only the score (x100) of the correct class in different scenarios. Is this *completely* informative? I think it can be easily misleading. What if the model provides relatively higher scores to some of the wrong classes as well? Can the authors analyse the score distribution of the wrong classes? Reporting the mean of CLIP_score@topK might be a good start to understanding the false positives.\n- I appreciate the visualisation study provided by GradCAM as a qualitative analysis but I am not confident of the calculation of \u201ccore\u201d versus \u201cspurious\u201d features [1]. \n- Can the authors also report the random accuracy for Tables 4 and 5? It is important to have a random baseline (that is, random string in place of the inferred context having the same token (and not string) length) here to isolate the effect of \u201cregisters\u201d versus actually using the context [1]. \n- The authors have not provided the code for reproducing the paper; implementation details are missing.\n\n**TL;DR**: The authors make strong claims about reducing the reliance on shortcuts, however, the missing baselines and analyses do not make me confident of their approach. However, some analyses seem misrepresented/miscommunicated. If the authors can answer my questions, I\u2019d be open to changing the score.\n\nSome clarity formatting and typographical errors to rectify:\n\n- Overall, the paper is well-written and ideas well-presented\n- The paper, at some points, deviates from standard ICLR formatting:\n  - \u201cinterleaved\u201d figures and tables (minor inconvenience)\n  - Unlabelled table: \u201cConditioning on ground-truth contextual attributes improves classification accuracy\u201d\n- Minor typographical errors:\n  - Section 5.1: \u201cIntuitively, we classify an image using both \u2013 possible classes and the ground-truth contextual attributes\u201d\n  - Remove the asterisk in the author list of \u201cDistributionally robust neural networks\u201c\n\n[1] Darcet, Timoth\u00e9e, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. \"Vision Transformers Need Registers.\" arXiv preprint arXiv:2309.16588 (2023)."
                },
                "questions": {
                    "value": "Please refer to weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors should consider adding ethical statement (especially since they are using datasets like CelebA with prompts specifically mentioning gender and race)."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674463710,
            "cdate": 1698674463710,
            "tmdate": 1699636615108,
            "mdate": 1699636615108,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tiwKPJr5Ob",
                "forum": "2Oiee202rd",
                "replyto": "AXlFUFtCHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vx4m (1)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive suggestions. They are very helpful in enhancing our paper's quality. We have incorporated all of them to our revised paper. Below, we address your questions in detail.\n\n> **Weakness 1:** If I understand correctly, Figure 3 gives only the score (x100) of the correct class in different scenarios. Is this completely informative? I think it can be easily misleading. What if the model provides relatively higher scores to some of the wrong classes as well? Can the authors analyse the score distribution of the wrong classes? Reporting the mean of CLIP_score@topK might be a good start to understanding the false positives.\n\nWe appreciate your insightful comment. To address this, **we've included Figure 5** (referenced alongside Figure 3) to scrutinize the increased CLIP score for both the ground-truth and wrong classes. This analysis is detailed in Appendix E.1.\n\nIn Figure 3, we compared {no contextual attribute, correct contextual attribute, wrong contextual attribute, random strings} and found that incorporating correct contextual attribute improves the CLIP score of the ground-truth class the most, indicating CLIP understands those attributes. However, as you rightly pointed out, it does not necessarily equate to increased accuracy since other classes could also have improved CLIP scores. That's why we test the accuracy directly in Section 5.1 and Table 2.\n\nWe agree that comparing the improvement for the correct class and the wrong classes is a good idea to complete the picture. Therefore, for each class $y$, we calculate the improvement brought by contextual attributes with \n\n$\\Delta_y \\triangleq \\mathtt{CLIP}(y, z^*; x) - \\mathtt{CLIP}_1(y; x)$. \n\nIn Figure 5, we contrast these improvements, comparing $\\Delta_{y^*}$ for the correct class against $\\Delta_{y_{wrong}}$, with the latter being the mean increase across the Top-K incorrect classes. The results confirm that the scores for the correct class see a more substantial increase when the contextual attributes are accurately described. This is because the accurate description of the class and the attribute best aligns with the corresponding image. Figure 3 and 5 together validate that the CLIP model understands the contextual attributes, and describing correct class and attributes yields higher similarity scores as described in Equation (4). Such findings also explain the increased accuracy in Table 2 when incorporating the correct contextual attributes.\n\n> **Weakness 2:** I appreciate the visualisation study provided by GradCAM as a qualitative analysis but I am not confident of the calculation of \u201ccore\u201d versus \u201cspurious\u201d features [1].\n\nWe appreciate your concern and acknowledge the insights from the concurrent work [1]. The observation of the registers is really interesting and intriguing. Although our visualization, which is based on Grad-CAM, differs from the attention maps tested in their paper, there could also be a register problem in our case. This is an interesting open question. Additionally, whether visualizations like Grad-CAM truly show how the model works remains an open question. Athough these open questions exist, the visualization still provides us with valuable insights, allowing us to better understand the model. We have tried our best to isolate the potential influence of the registers. In Figure 4, we demonstrate that the reduction in reliance on spurious features is notably observed only when the correct context is introduced. This effect is distinctly absent when incorrect contexts or random tokens are used, suggesting a more nuanced interaction than merely an increase in token count. Additionally, we have added a quantitative evaluation in **Table 3 (new results)**. These results align with our visualizations in Figure 4, indicating that the correct contextual information indeed directs the model's focus towards core features, an effect that diminishes with the use of random strings. \n\nWhile we acknowledge the limitations of current visualization techniques, we believe our multi-faceted approach, combining both qualitative and quantitative analyses, provides a comprehensive understanding of how contextual attributes influence model behavior.\n\n[1] Darcet, Timoth\u00e9e, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. \"Vision Transformers Need Registers.\" arXiv preprint arXiv:2309.16588 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673950358,
                "cdate": 1700673950358,
                "tmdate": 1700673962002,
                "mdate": 1700673962002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ue7OxbhnhM",
                "forum": "2Oiee202rd",
                "replyto": "va86dmQtCu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_vx4m"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for answering my queries. While I am satisfied with some clarifications, I believe that some experiments uncovered more serious doubts.\n\n**W1**: While the increase in the correct class is substantial, the mean increase in the wrong classes is also not trivial. Ideally, wouldn\u2019t you expect this value to be negative? I am not sure how $\\Delta_{y_{wrong}}$ supports the author's results and claims on robustness. Is it that the CLIP in-turn develops a stronger context-bias (say because the context attribute is \u201con water\u201d, the model has higher prediction scores for other classes that co-occur with water as well). **If such might be the case, which the authors haven\u2019t ruled out and seems apparent from the newer results, the work might be creating a new problem while solving one.**  \n\n**W2**: My doubts regarding the speciousness of the methodology still stand true. While the qualitative results are definitely interesting, they are not convincing enough to extrapolate the robustness claims\n\n**W3**: The results are quite nice! Appreciate their incorporation in the main paper\n\nThanks for providing the code and well-organised discussion. The authors\u2019 response to W1 has left me more skeptical of the claims of the paper.\n\nI am open to hearing the authors' response but will keep the ratings as it is till then."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735326343,
                "cdate": 1700735326343,
                "tmdate": 1700735326343,
                "mdate": 1700735326343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PSX3OLIumI",
            "forum": "2Oiee202rd",
            "replyto": "2Oiee202rd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the human perception process that the contextual attributes are separated from the foreground objects, this paper proposes a training-free, two-step zero-shot classification method PerceptionCLIP to first infer the contextual attributes (e.g., background) and then performs object clas- sification conditioning on them. A proof-of-concept investigations reveal that conditioning on ground-truth contextual attributes improves CLIP\u2019s zero-shot classification. The proposed PerceptionCLIP demonstrates performance gain and improved interpretability on several datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of imitating human perception process to improve the generalization and group robustness of the image classification model is insightful for the community.\nThe proposed method is extensively evaluated on 11 datasets."
                },
                "weaknesses": {
                    "value": "1. Collecting the contextual attributes requires either pre-knowledge for the test image or a large dataset containing captions, which hinders the generalization ability of the proposed method in the real world. For instance, contextual attributes for the CelebA dataset are manually defined, e.g., gender, age, etc. To collect the contextual attributes for the remote sensing dataset EuroSAT, the authors first retrieve similar images and captions from a large image+text dataset LAION-400M, then ask GPT-4 to summarize the contextual attributes. What if we do not have external datasets to provide captions?\n\n2. The qualitative results in Figure 4 indicate that introducing the contextual attributes reduces reliance on the spurious features and the model focuses more on the core features. It would be fairer to provide a quantitative evaluation, e.g., counting the percentage of model attention on the core features versus on spurious feature on all test set in ImageNet, and compare the ratio of different models.\n\n3. The performance gain seems marginal on most of the datasets. For instance, in Table 4, the performance gain is only around 2% on ImageNet, ImageNetV2, ImageNet-A and ImageNet-Sketch. Besides, since introducing a random attribute or even a wrong attribute can improve the accuracy in Table 2, it would be interesting to include the results of the wrong attribute and random attribute in Table 4 as well.\n\n4. The results in Table 7 are not consistent among different backbones. It is hard to get any conclusion on which method is better."
                },
                "questions": {
                    "value": "In Table 7, why lower gap between the Avg and Worst is better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841682818,
            "cdate": 1698841682818,
            "tmdate": 1699636614988,
            "mdate": 1699636614988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zNwgqdfAte",
                "forum": "2Oiee202rd",
                "replyto": "PSX3OLIumI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Reviewer_U13E"
                ],
                "content": {
                    "comment": {
                        "value": "Since the authors did not provide responses, I would like to keep the original rating."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662175743,
                "cdate": 1700662175743,
                "tmdate": 1700662175743,
                "mdate": 1700662175743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "99Z7UI07kH",
                "forum": "2Oiee202rd",
                "replyto": "PSX3OLIumI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer U13E (1)"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback. Your suggestions are very helpful. We have incorporated all of them in our revised paper. Here, we address your questions and concerns in detail.\n\n> **Weakness 1:** Collecting the contextual attributes requires either pre-knowledge for the test image or a large dataset containing captions, which hinders the generalization ability of the proposed method in the real world. For instance, contextual attributes for the CelebA dataset are manually defined, e.g., gender, age, etc. To collect the contextual attributes for the remote sensing dataset EuroSAT, the authors first retrieve similar images and captions from a large image+text dataset LAION-400M, then ask GPT-4 to summarize the contextual attributes. What if we do not have external datasets to provide captions?\n\nThanks for bringing this up. It's true that our method, like other zero-shot classification approaches, requires some prior knowledge of the data. However, the type and amount of prior knowledge needed in our case are relatively minimal and practical to obtain. We only need **basic, class-independent** contextual attributes like background or illumination for natural images and gender for facial images, which are generally known or easily inferred. This simplicity contrasts with other methods [1-2], which require more **detailed, class-specific** descriptions for every class.\n\nOne advantage of our method lies in its flexibility with contextual attributes. We only need a set of possible contextual attributes since CLIP itself can first infer the most relevant contextual attribute from this set. Such flexibility makes our approach adaptable to a wide range of datasets which was demonstrated in our paper across 13 different datasets.\n\nFor situations where contextual attributes are not readily available, we propose an **alternative**: using the LAION caption dataset and GPT-4 to automatically identify relevant attributes. This approach is grounded in the fact that OpenCLIP is trained on LAION-2B [3], and CLIP was trained on similar large-scale caption datasets. Since CLIP acquires the knowledge of contextual attributes from the training data, LAION is a good source to find contextual attributes automatically for almost all datasets. \n\n[1] Menon, Sachit, and Carl Vondrick. \"Visual classification via description from large language models.\" ICLR 2023.\n\n[2] Pratt, Sarah, et al. \"What does a platypus look like? generating customized prompts for zero-shot image classification.\" CVPR 2023.\n\n[3] https://laion.ai/blog/large-openclip/\n\n> **Weakness 2:** The qualitative results in Figure 4 indicate that introducing the contextual attributes reduces reliance on the spurious features and the model focuses more on the core features. It would be fairer to provide a quantitative evaluation, e.g., counting the percentage of model attention on the core features versus on spurious feature on all test set in ImageNet, and compare the ratio of different models.\n\nThank you for your valuable suggestion. **We've now added Table 3 in our paper**, presenting quantitative results. This table, along with a detailed discussion in Section 5.1 and Appendix E.3, addresses this point.\n\nTo calculate the percentage of model attention on the core versus spurious feature, we need the segmentation of them. We do not have such segmentation for the ImageNet dataset. Therefore, our evaluation is conducted on the Waterbirds dataset where we have the segmentation of the core feature (e.g., bird) and spurious feature (e.g., background).\n\nThe results in Table 3 highlight that when the correct context (here, background) is specified, CLIP shifts its focus more toward the core feature for classification. This effect is not observed when using incorrect or random context. Thus, both Figure 4 and Table 3 collectively demonstrate that introducing contextual attributes effectively reduces the model's reliance on spurious features and enhances its focus on core features."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669874321,
                "cdate": 1700669874321,
                "tmdate": 1700680536428,
                "mdate": 1700680536428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MMFpXqp5z5",
                "forum": "2Oiee202rd",
                "replyto": "PSX3OLIumI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer U13E (3)"
                    },
                    "comment": {
                        "value": "> **Weakness 4 & Question 1:** The results in Table 7 are not consistent among different backbones. It is hard to get any conclusion on which method is better. In Table 7, why lower gap between the Avg and Worst is better?\n\nIn Table 7 (Table 8 in our revised paper), our goal is to demonstrate how our method enhances group robustness by incorporating contextual attributes regardless of the backbones. Group robustness measures how consistently the model performs across different groups within a dataset. For example, in the Waterbirds dataset, there are four distinct groups: {waterbird on water, landbird on land, waterbird on land, landbird on water}. The first two are considered majority groups as they are more prevalent in the training data, while the latter two are minority groups. Typically, due to training set imbalances, models tend to perform better in majority groups. However, we need models that perform well in all groups. To this end, following the convention in this field [1], we assess model accuracy on each of the four groups, calculating both the average accuracy (Avg) and the worst group accuracy (Worst). The gap (Gap) is the difference between these two metrics. A higher gap indicates a bias towards certain groups, implying less consistency and robustness. Conversely, a lower gap signifies a more balanced and robust model performance across all groups. Therefore, the lower gap is better.\n\nWe would like to clarify that the purpose of Table 7 isn't to compare different backbones. Instead, we aim to show that our method is **effective across various backbones**. By comparing our method (with $\\mathcal{Z}$) against the baseline (without $\\mathcal{Z}$) for each backbone, we demonstrate that our approach consistently reduces the gap between the average and worst group accuracies, thereby achieving better group robustness.\n\n[1] Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" ICLR 2020.\n\n---\nThanks again for all your time and effort in reviewing our paper! Could you please consider increasing the score if you are satisfied with our answers? We are happy to have more discussions for any further questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670921419,
                "cdate": 1700670921419,
                "tmdate": 1700674677354,
                "mdate": 1700674677354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LvPzGT6njs",
            "forum": "2Oiee202rd",
            "replyto": "2Oiee202rd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
            ],
            "content": {
                "summary": {
                    "value": "This paper is inspired by human visual perception, where humans first discern contextual attributes, such as background and orientation, to distinguish objects, and then classify them. Similarly, when CLIP is provided with these contextual attributes, it improves in zero-shot image classification and reduces dependence on irrelevant features. Authors found that CLIP can deduce these attributes from an image itself and based on this fact to propose PerceptionCLIP. PerceptionCLIP first determines contextual attributes from an image and then classifies the object based on these attributes. Experiments are done on CLIP's zero-shot classification settings and show clear improvements over the original CLIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy-to-follow. While the concept of utilizing background information for image classification isn't groundbreaking in literature, its application to CLIP could be innovative.\n- The experiments show clear advantages of using contextual attributes over the traditional 80 templates."
                },
                "weaknesses": {
                    "value": "- The authors assert at least twice that PerceptionCLIP mirrors human perception. However, I'm not entirely convinced. Authors gave the preliminary that: \u201chumans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information.\u201d Yet, there's no evidence indicating that PerceptionCLIP actively separate foreground from background during classification, or that such separation is utilized the model. It's possible that PerceptionCLIP utilizes background attributes differently.\n- The authors refer to background information as spurious features (e.g. Figure 1). To my knowledge, it is not completely correct. Though they can sometimes overlap, they are not the same. Background information is a broader concept, while spurious features specifically refer to misleading patterns that a model might incorrectly learn as being important. In addition, the GradCAM in Figure 1 primarily emphasizes the foreground, consistent with [a], without highlighting any reduced reliance on spurious features. It's more accurate to state that it offers enhanced focus on foreground objects.\n- When I like the idea of Textual descriptions for contextual attributes Sec 4.1., I could not find how exactly you map Z using the proposed annotation function alpha to attribute text descriptions. Also, why do you say this annotation function model human preferences in captioning? Authors may also want to clarify the p value associated with the textual descriptions.  I imagine that these descriptions can also easily obtained using LLMs (e.g. ChatGPT).\n- The name of the proposed metric can be easily confused with the original CLIP score. How about naming it as Attribute-CLIP."
                },
                "questions": {
                    "value": "Post-rebuttal:\n\nI genuinely appreciate your great efforts put into this rebuttal!\n\nI read the (updated) paper one more time, the authors' responses to me thoroughly, and the responses to other reviewers.\nWhile PercentionCLIP is indeed powerful and but answer of \"why does it work\" is not fully addressed via the GradCAM visualization as raised by W2 of reviewer vx4m. \n\nI would like to keep my rating for now and may change later after discussing with other reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5829/Reviewer_k2xX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5829/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698981770397,
            "cdate": 1698981770397,
            "tmdate": 1700775988504,
            "mdate": 1700775988504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C03YePVglR",
                "forum": "2Oiee202rd",
                "replyto": "LvPzGT6njs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5829/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer k2xX (1)"
                    },
                    "comment": {
                        "value": "Thank you for your support and all your questions. Here, we address them in detail.\n\n> **Weakness 1:** The authors assert at least twice that PerceptionCLIP mirrors human perception. However, I'm not entirely convinced. Authors gave the preliminary that: \u201chumans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information.\u201d Yet, there's no evidence indicating that PerceptionCLIP actively separate foreground from background during classification, or that such separation is utilized the model. It's possible that PerceptionCLIP utilizes background attributes differently.\n\nThanks for your feedback! Let me clarify how PerceptionCLIP emulates human perception. Our approach, outlined in Algorithm 1, involves two main steps inspired by the human perceptual process. Firstly, CLIP infers contextual attributes, which is akin to how we humans first discern context, like background or orientation, to understand a scene. Secondly, this contextual information is used for classification, much like how we classify objects based on the context we've perceived.\n\nIt's important to note that in our model, separating foreground from background is just one example of using contextual attributes, not the entire story. The key aspect of human perception that we're trying to capture is the ability to use context effectively. By integrating these contextual attributes into our model's prediction process, we've seen a notable boost in zero-shot classification performance. PerceptionCLIP isn't about mimicking the exact human process of separating foreground and background, but rather about leveraging context in a way that's inspired by human perception.\n\n> **Weakness 2:** The authors refer to background information as spurious features (e.g. Figure 1). To my knowledge, it is not completely correct. Though they can sometimes overlap, they are not the same. Background information is a broader concept, while spurious features specifically refer to misleading patterns that a model might incorrectly learn as being important. In addition, the GradCAM in Figure 1 primarily emphasizes the foreground, consistent with [a], without highlighting any reduced reliance on spurious features. It's more accurate to state that it offers enhanced focus on foreground objects.\n\nThank you for pointing out the distinction between spurious features and background information. You're right in noting that spurious features are generally misleading patterns that a model might erroneously learn as significant. In our context, we use the term \"spurious feature\" to also encompass parts of the image with a recognizable concept, like the background, as referenced in several works including [1].\n\nIn Figure 1, our aim was to demonstrate the model's shift from relying on spurious (or background) features to focusing more on core (or foreground) objects. The model's reliance ratio changes from 31:69 (background vs foreground) in CLIP to 23:77 in PerceptionCLIP, suggesting that our method indeed reduces dependency on what we've termed as spurious features. This aligns with your observation about the model offering an enhanced focus on foreground objects. We appreciate your feedback and will consider clarifying this distinction in our revision.\n\n[1] Moayeri, Mazda, et al. \"Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases.\" NeurIPS 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5829/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668124229,
                "cdate": 1700668124229,
                "tmdate": 1700674701658,
                "mdate": 1700674701658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]