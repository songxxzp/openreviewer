[
    {
        "title": "Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model"
    },
    {
        "review": {
            "id": "eYQQuAJgIe",
            "forum": "cVUOnF7iVp",
            "replyto": "cVUOnF7iVp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies sparse linear regression in the different local differential privacy models (LDP). \n\nFor non-interactive LDP they propose an algorithm with estimation error $\\tilde{O}(\\frac{d\\sqrt{k}}{\\epsilon\\sqrt{n}})$, and show a lower bound $\\Omega(\\frac{\\sqrt{dk\\log d}}{\\epsilon \\sqrt{n}})$ (for sub-Gaussian covariates). In addition, they show that it is possible to improve the upper bound by a factor $\\sqrt{d}$ given public unlabeled covariates.\n\nFor interactive LDP they propose an algorithm with estimation error $\\tilde{O}(\\frac{k\\sqrt{d}}{\\epsilon\\sqrt{n}})$, and show a lower bound $\\Omega(\\frac{{\\sqrt{dk}}}{\\epsilon \\sqrt{n}})$ (for sub-Gaussian covariates)"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There are a few new non-trivial results that improve over the state of the art. The paper is well-written, I really enjoyed reading it. The contribution and comparison with prior works is clear. The idea behind Algorithm 1 is very nice and seems to be new (though I'm not an expert in the field, so I'm not 100% sure).\n\nIn addition, they found and fixed a bug in one of the results of prior work [1] on the iterative LDP settings that implied an incorrect upper bound. I briefly checked it, and indeed Hoelder's inequality in the proof of Theorem 9 is used incorrectly there, so it is good that this mistake was found and fixed."
                },
                "weaknesses": {
                    "value": "I didn't find major weaknesses. However there is one thing (which I formulate in the Questions below) that is confusing to me."
                },
                "questions": {
                    "value": "Your proof of Theorem 7 looks very similar to the proof of Theorem 9 from [1] (and you mention that). Could you please explain what are important differences, assuming linear regression settings? (I didn't check the details, so maybe I missed something). From the first glance it looks like the proof from [1] works not only for the uniform distribution, but also for 1-sub-Guaussian distributions (modulo their wrong bound in the very beginning), and if it is the case, it should also work for you settings, or did I miss anything important?\n\nAnd one minor thing: I suggest to move Table 1 to the introduction.\n\n[1] Di Wang and Jinhui Xu. On Sparse Linear Regression in the Local Differential Privacy Model.\nIEEE Transactions on Information Theory 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5338/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5338/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792144108,
            "cdate": 1698792144108,
            "tmdate": 1699636536655,
            "mdate": 1699636536655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bDOMaFkWrN",
                "forum": "cVUOnF7iVp",
                "replyto": "eYQQuAJgIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the time and effort that the reviewer has dedicated to reviewing our work.\n\n1. Answer to the question regarding Theorem 7:\n\nThe main differences between Theorem 7 and Theorem 9 are two fold. First we need finer analysis on $B_1$ term due to clipping operation. Second, due to different distributional assumption we adopt a more general lemma related to RIP condition. The main contribution of Theorem 7, as we mentioned, is rectifying the flaws in the previous result. To see greater novelty in proof techniques, please refer to Theorem 34 in the Appendix for the case without the isotropic assumption. \n\n2. Answer to the question regarding Table 1:\n\nThank you for your suggestion. In our current version, Table 1 is placed in the Appendix due to space limit. We will definitely move to the paper if additional pages are allowed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003287156,
                "cdate": 1700003287156,
                "tmdate": 1700003287156,
                "mdate": 1700003287156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OTjksGUFqj",
                "forum": "cVUOnF7iVp",
                "replyto": "bDOMaFkWrN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you very much for the clarification! The score remains unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730267679,
                "cdate": 1700730267679,
                "tmdate": 1700730267679,
                "mdate": 1700730267679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O16uFRHQN5",
            "forum": "cVUOnF7iVp",
            "replyto": "cVUOnF7iVp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_UbUb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_UbUb"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of sparse linear regression under the local differential privacy setting. The authors provide new lower bound results for this problem with a $k$-sparse underlying model parameter. In addition, the authors develop efficient upper bound algorithms for the same problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the current paper are summarized as follow:\n1. The authors provide new lower bound results for sparse linear regression under local differential private model.\n2. The authors develop new efficient algorithm for solving the same problem."
                },
                "weaknesses": {
                    "value": "The weaknesses of the current paper:\n1. It is unclear the dimension dependence in the lower bound is due to the hardness of the LDP setting or the norm of the data.\n2. It is unclear why the authors need the $\\ell_1$ norm bound in their results.\n3. It is unclear why Assumptions 1 and 2 are both required in the upper bound results.\n4. Why it is reasonable to consider the sparse model in the classical setting?\n5. The sample complexity requirement seems to be very bad in terms of $d$."
                },
                "questions": {
                    "value": "Here are some additional questions I have for the current paper:\n1. For the Remark 2, why the authors claim that the sparse linear models in the non-interactive LDP setting are ill-suited? It seems to me that the dimension dependence in Theorem 1 comes from the norm of the data, what will the result look like if you assume the data vector to be $\\ell_2$ norm bounded? In addition, the results in Raskutti et al. 2011 and Cai et al. 2021 seem to assume the data vector to be $\\ell_2$ norm bounded.\n2. For Theorem 3, why do you assume Assumptions 1 and 2 holds simultaneously? In Assumption 1, you assume $x$ with covariance $\\Sigma$, and the transformed data to be Sub Gaussian. In Assumption 2, you further assume $x_i$ has variance $\\sigma^2$. In addition, what is the assumption on $\\zeta$?\n3. If the lower bound has nothing to do with $\\ell_1$ norm bound, you should give the results in terms of the $\\ell_2$ norm bound. \n4. Whether the upper bound results can be extended to the $\\ell_2$ norm bound case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793758150,
            "cdate": 1698793758150,
            "tmdate": 1699636536555,
            "mdate": 1699636536555,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b1MrB2BnYj",
                "forum": "cVUOnF7iVp",
                "replyto": "O16uFRHQN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for the time and effort that the reviewer has dedicated to reviewing our work.\n\n1.Response for W1 and Q1: \n\n**We cannot agree with the reviewer's comment.** Our results confirm that the dependence of dimension is due to the hardness of the LDP setting rather than the norm of the data being upper bounded by $O(\\sqrt{d})$.  In fact, in the central DP case, even under some similar assumptions on data distribution, the nearly optimal rate is $O(\\sqrt{\\frac{k\\log d}{n}}+\\frac{k\\log d}{n\\epsilon})$ (see assumptions (P1'), (D1') and (D2') and Theorem 4.4 in Cai et al. 2021).  Note that even in the heavy-tailed response setting with $\\|x\\|_2\\leq O(\\sqrt{d})$, we can still get an upper bound which only depends on $\\text{Poly}(k, \\log d)$ (see Theorem 7 and Assumption 3 in [1]). Similarly, for the non-private setting, Raskutti et al. 2011  show that the optimal rate is $O(\\sqrt{\\frac{k\\log d}{n}})$ in the case where $x$ is $\\sigma^2$-subgaussian, indicating that the $\\ell_2$-norm of $x$ is upper bounded by $O(\\sqrt{d})$ with high probability. We refer the reviewer to Table 1 in Appendix for a detailed comparison with some related work. \n \n[1] Hu, Lijie, et al. \"High dimensional differentially private stochastic optimization with heavy-tailed data.\" Proceedings of the 41st ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems. 2022.\n\n2.Response to W2.\n\nIn fact, only our upper bound for the NLDP model relies on the $\\ell_1$-norm bound assumption. Specifically,  we need such an assumption in the proof of Theorem 3 to bound the $T_3$ term.\n**Please refer to Page 24 in the Appendix to see how the assumption $\\|\\theta^{*}\\|_1\\leq 1$ is applied.** We will introduce an additional factor of $\\sqrt{d}$ if we use $\\ell_2$-norm assumption instead. As we mentioned, this assumption has been previously studied\nin the literature such as Chen et al. (2023; 2022a) and Fan et al. (2021).\n\n3. Response to W3 \n\nIt is notable that Assumption 2 assumes $x$ and $y$ are sub-Gaussian, which is the most commonly used assumption in the literature of sparse linear regression even in the non-private case, such as Raskutti et al. 2011. Assumption 1 is only used for Theorem 3. The usage of $\\ell_1$-norm boundedness assumption was just discussed above; $\\kappa_\\infty$ is vital for providing $\\ell_\\infty$-norm bound on the inverse of private covariance estimator (Please refer to Page 23 in the Appendix).\n\n4. Answer to W4 \n\n\nFirstly, although $k\\ll d$ holds in general, **sparsity $k$ can heavily affect the estimation error.** For instance, when $k=O(d^{\\frac{1}{2}})$, ignoring the sparsity structure limits the previous result to an estimation error of $O(\\frac{d^\\frac{3}{2}}{\\sqrt{n}\\epsilon})$ in LDP model. In contrast, our work attains rate of $O(\\frac{d^{\\frac{5}{4} }}{\\sqrt{n}\\epsilon})$ and $O(\\frac{d} {\\sqrt{n}\\epsilon})$ for NLDP and interactive LDP, respectively. \n\nSecondly, from the theoretical perspective, the significant implication of sparse linear regression in ML, DP and many interconnected problems is the impetus to address  the constraints of previous techniques as they cannot be readily generalized to the general $k$-sparse case. We believe that our methods and some technical lemmas can also be used in other related problems. \n\nThirdly, as the previous paper already showed that the estimation error will be trivial even when $k=1$ in the case of $n\\ll d$. However, the problem remains inadequately understood in the low-dimensional regime, which is the motivation of our research. We aim to give an answer to the question: what is the optimal rate of sparse linear regression in the rich data regime ($n\\gg d$)? Sparse linear regression in the low dimensional case $n\\gg d$ has received enormous attention but remains under-studied in the DP community.  For example, [2] studies the problem of the sparse linear bandit, highlighting the critical difference between the poor data regime ($n\\ll d$) and the rich data regime ($n\\gg d$). Thus, we believe our problem is merited to study. \n\n\n[2] Hao, Botao, Tor Lattimore, and Mengdi Wang. \"High-dimensional sparse linear bandits.\" Advances in Neural Information Processing Systems 33 (2020): 10753-10763.\n\n5. Response to W5\n\nWe guess the reviewer refers to the assumption of $n\\geq O(d^4)$ in Theorem 3 and 5. Note that it is only used in the non-interactive case and we do not need such an assumption in the interactive setting. As we mentioned in Remark 3, this assumption is to ensure the private covariance matrix is invertible. We can relax such an assumption when there is some public unlabeled data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700002945838,
                "cdate": 1700002945838,
                "tmdate": 1700002945838,
                "mdate": 1700002945838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vwvh9RvzSO",
                "forum": "cVUOnF7iVp",
                "replyto": "O16uFRHQN5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "6. Response to Q2 \n\nThe reason we assume the whitened covariate $x$ is subgaussian and $x$ has bounded covariance is just for convenience and to make the paper easier to read. \n\nFor the assumption on $\\zeta$, since we have assumed that both $y$ and $x$ are sub-Gaussian, indicating that $\\zeta$ is also sub-Gaussian with variance $\\sigma^2$ due to the sub-Gaussian property and our linear model.  \n\n7. Response to Q3 \n\n**It is notable that even in the case $||\\theta^{*}||_2 \\leq 1$, all the lower bounds still hold without any changes to the proofs.** See Page 18 for the proof of Theorem 1, and Page 27 for the proof of Theorem 6. We have $\\|\\theta_z\\|_2\\leq \\|\\theta\\|_1\\leq 1$. **In the revised version, we added the above comments.**\n\n8. Response to Q4 \n\nIn fact, our upper bound in the interactive setting is still valid when $\\|\\theta^*\\|_2\\leq 1$. In contrast, our upper bound for the NLDP model relies on such an assumption. Specifically,  if the assumption on $\\theta^*$ is loosened to $\\ell_2$-norm bounded, it would be hard to bound the $T_3$ term in the proof of Theorem 3 without introducing an additional factor of $\\sqrt{d}$. **Please refer to Page 23 in the Appendix to see how the assumption $\\|\\theta^*\\|_1\\leq 1$ is applied. However, to make the paper consistent, we assume ${\\|\\theta^{*}\\|_1\\leq 1}$.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003139979,
                "cdate": 1700003139979,
                "tmdate": 1700003139979,
                "mdate": 1700003139979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VCB68tqTzO",
            "forum": "cVUOnF7iVp",
            "replyto": "cVUOnF7iVp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_qQ5H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5338/Reviewer_qQ5H"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies sparse linear regression under local differential privacy. Firstly, it establishes a lower bound under a non-interactive LDP protocol for sub-gaussian data. Secondly, it proposes the first upper bound that has a $\\sqrt{d}$ gap compared to the aforementioned lower bound. It also demonstrates that this gap can be closed if public unlabeled data is available. Lastly, in the case of sequentially interactive protocol, this paper presents a lower bound and corrects the results of the iterative hard thresholding algorithm from prior work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is thorough and clearly written. \n2. The problem is well-defined and important."
                },
                "weaknesses": {
                    "value": "The upperbound and lowerbound do not match. It is unclear which bound is tight. Also $n$ has to be greater than $O(d^4)$ to achieve a rate of $O(d)$ in Theorem 3."
                },
                "questions": {
                    "value": "1. Is the l2 norm the right metric for linear regression? For example, Cai at el (2021) consider $\\|\\theta^{priv}-\\theta^*\\|_\\Sigma$, which corresponds to minimal emprical risk. Do the results also hold under this normalized metric?\n2. Is k used in Algorithm 1?\n3. Regarding Remark 3, is it necessary to release the covariance matrix privately in LDP model? Can you privatize the two terms in OLS solution together?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806391684,
            "cdate": 1698806391684,
            "tmdate": 1699636536460,
            "mdate": 1699636536460,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TLVvUeEUJM",
                "forum": "cVUOnF7iVp",
                "replyto": "VCB68tqTzO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5338/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for the careful and detailed review as well as the constructive feedback. \n\n1.Response to Weakness \n\nWe acknowledge the gap between the upper and lower bounds established in our work. However, as sparse linear regression is a fundamental problem in both statistics and differential privacy, we do not think such gaps undermine our contributions. Here are the reasons:\n\n Only the work (Wang \\& Xu (2021))  has studied sparse linear regression in the LDP model. However, the problem is still far from well understood: (1) For lower bounds, previous results only consider the case where $k=1$ and it is technically difficult to extend to the general $k$ sparse case. **Prior to our work, there are no comparable lower bounds!** We give non-trivial proofs for our lower bounds by constructing hard instances that might be instructive for other research problems. (2) For the upper bound in the NLDP model, **we give the first algorithm with a non-trivial upper bound since there is even no previous study due to the one-round communication constraints in the model!**  Moreover, the closed-form private estimator for sparse linear regression is highly efficient, and can readily be applied to other problems. (3) Even the investigation of the upper bound in the interactive setting is still quite deficient. Previous work claims the rate is already optimal for $k=1$. **However, we found a flaw in their approach and the proof of the upper bound is partly mistaken.** We then gave a correct upper bound. \n\nIn summary, sparse linear regression in the LDP model is quite difficult and challenging even in the interactive setting but our upper and lower bounds represent a significant leap forward in addressing this insufficiently studied problem.\n\nFor the assumption that $n\\geq O(d^4)$, note that it is only used in the non-interactive case and we do not need such an assumption in the interactive setting. As we mentioned in Remark 3, this assumption is to ensure the private covariance matrix is invertible. We can relax such an assumption when there is some public unlabeled data. \n\n2.Response to Q1\n\nYes, $\\ell_2$-norm is a commonly used metric for our problem. Firstly,  many previous works employ the same metric [2] or adopt a metric that can be easily converted to parameter $\\ell_2$-norm error bound [3]. Secondly, we can easily transform the $\\ell_2$-norm bound to $||\\theta^{{priv }}-\\theta^*||_\\Sigma$ because it is less than \n\n$ \\lambda_{\\max}(\\Sigma)||\\theta^{{priv }}-\\theta^*||_2^2.$\n\nTheorem 4.2 in [4] adopts $||\\theta^{{priv }}-\\theta^*||_{\\Sigma}$. However, in their proof, they first analyzed the convergence rate of $\\ell_2$-norm error.\n \nThen armed with the boundedness and normalized rows of design matrix assumptions, they were able to develop the bound on $||\\theta^{{priv }}-\\theta^*||_{\\Sigma}$ based on $\\ell_2$-norm bound. Note that these assumptions are stronger than our Assumptions 1 and 2, therefore here we discuss the error in $\\ell_2$-norm.\n\n3.Answer to Q2 (regarding sparsity $k$ in Algorithm 1)\n\nNo, $k$ is not used in our Algorithm 1. However, the sparsity in our proposed estimator $\\hat{\\theta}^{priv}$ is preserved by setting element-wise soft-thresolding operator's parameter $\\lambda_n$ to some constant of size $O(\\frac{d\\log n \\sqrt{\\frac{1}{\\delta}}}{\\sqrt{n}\\epsilon})$. $\\lambda_n$ serves as a constraint of the $\\ell_\\infty$-norm error and introduced an extra error term of $O(\\sqrt{k})$ when it comes to computing the $\\ell_2$-norm error. Note that such phenomenon is quite common in the non-private case, such as the optimal regularization parameter in LASSO [1] \n\n4. Answer to Q3 \n\nIn the non-interactive setting, we cannot say we need to privately estimate the covariance matrix. However, for our method, we have to do this due to our closed-form estimator. \n\nIf we interpret your question correctly, \"Can you privatize the two terms in the OLS solution together?\", you are asking whether we can perturb our non-private estimator as a whole. Unfortunately, we cannot adopt the output perturbation method, i.e., adding some Gamma noise to $\\hat{\\theta}^{OLS}$. First, output perturbation does not preserve LDP. Secondly, according to the DP theory, the scale of the noise should be proportional to the $\\ell_2$-norm sensitivity, which can be bounded through clipping operation or if the data are assumed to be bounded. However, this method fails since the magnitude of noise needs to grow polynomially with $\\sqrt{\\frac{d}{n}}$, causing the estimation error much larger. Moreover, the $\\ell_2$-norm sensitivity is bounded only with some probability due to the existence of the inverse of the covariance matrix, indicating the algorithm does not satisfy DP."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5338/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700002220923,
                "cdate": 1700002220923,
                "tmdate": 1700002220923,
                "mdate": 1700002220923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]