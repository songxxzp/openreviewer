[
    {
        "title": "On Memorization in Diffusion Models"
    },
    {
        "review": {
            "id": "s1UDvzsdDO",
            "forum": "9nT8ouPui8",
            "replyto": "9nT8ouPui8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_gEHa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_gEHa"
            ],
            "content": {
                "summary": {
                    "value": "In a series of small experiments authors evaluate how different aspects of the model and dataset influence memorization of training examples in Diffusion Models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Authors tackle a very interesting problem of diffusion models that is not yet widely studied, while at the same time bears a great significance\n- There is an interesting insight on the fact that adding conditioning, even in a form of random labeling increases memorisation. This observation is worth further studying to shed some light on the nature of memorisation.\n- The work follows a nice structure and is therefore easy to understand"
                },
                "weaknesses": {
                    "value": "- The scientific contribution of this submission is limited. The problem of memorisation in diffusion models was already noticed in several works (as mentioned in the related works section). The observation that in the theoretical optimum of diffusion models they can only replicate training data is new, but it is quite expected since the simplified training objective of DDPMs [Ho et al. 2020] is to directly denoise all training examples with a simple MSE loss.\nThe main contribution of this work is therefore, a set of experiments that measure the memorization across different model sizes, widths, dataset sizes etc. Except for one experiment described in the strengths sections the results are rather intuitive and expected (e.g. diffusion models memorize more examples from smaller datasets, or when trained with wider models), and are presented in a form of report without in-depth analysis of the root-causes of memorization. The interesting hypothesis is proposed only for the analysis of the class conditioning influence, but it is denied right away in the next paragraph.\n- The memorization is only studied with respect to the direct pixel-by-pixel comparison of training and generated samples. Some works (e.g. Carlini et al. mentioned in this work) show that diffusion models can also memorize by generating simple interpolations between similar training examples.\n- The evaluation is performed using only one dataset (CIFAR10). It would be interesting how diffusion models memorize more detailed dataset e.g CelebA."
                },
                "questions": {
                    "value": "- What is the statistical significance of all of the experiments? In some plots, there is small difference between different setups, it is unclear if it should be taken into consideration.\n- What was the performance of the model when trained with large weight-decay values? What is the trade-off between the quality of samples and memorization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698523706687,
            "cdate": 1698523706687,
            "tmdate": 1699636787999,
            "mdate": 1699636787999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EYWvOpQ1US",
                "forum": "9nT8ouPui8",
                "replyto": "s1UDvzsdDO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gEHa"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable questions.\n\n---\n***Weakness 1: Limited scientific contributions.***\n\nWe would like to clarify the primary objective of our work: investigating under what conditions the diffusion models (undesirably) approximate the optimal solution represented by Eq.(2). On the one hand, such an optimal model can only replicate training data, and may even lead to privacy/copyright issues. On the other hand, such an optimal model is indeed what is theoretically expected. Therefore, it becomes imperative to gauge the extent of this theoretical hazard in typical diffusion model training scenarios. This awareness is vital for mitigating adverse consequences and refining the practical utility of these models, thus requiring quantitative studies like our work. \n\nFurthermore, our research offers extensive and empirical guidances for understanding the memorization in diffusion models and training diffusion models while preventing large memorization. Our findings aim to delineate which factors have a substantial impact on memorization and which contribute more subtly. As such, our paper primarily presents an empirical analysis, rather than a theoretical one, of the interrelationships between these factors. In addition, our study reveals surprising findings, e.g. the significant effects of random labels, which may inspire the theoretical practitioners for further exploration.\n\n---\n***Weakness 2: Only pixel-by-pixel memorization is explored.***\n\nIn our research, as presented in the introduction and Appendix, we have established through both empirical and theoretical analysis that the optimal solution for denoising score matching in diffusion models memorizes training data on a pixel-by-pixel basis. Since our motivation is to gauge the extent of this theoretical hazard in typical diffusion model training scenarios, the same pixel-by-pixel memorization is our research focus.\n\n---\n***Weakness 3: Lack of experiments on other datasets.***\n\nThank you for your valuable suggestions. In addition to CIFAR-10, we have conducted a series of additional experiments using the FFHQ dataset [1], which is a higher-dimensional face dataset similar to CelebA. These new experiments have been included in **Appendix D: More empirical results on FFHQ** of our revised paper. Due to the time constraints, our additional experiments focused on investigating the impact of data dimension / time embeddings / conditioning, on the memorization of diffusion models. It is noteworthy that the outcomes of these recent experiments support our initial findings derived from the CIFAR-10 dataset.\n\n---\n***Question 1: Whether statistical significance matters?***\n\nIn our experiments, we noted that the variance in memorization ratios across repeated trials is negligible. For instance, as depicted in Figure 2(b), with the training data size of $|\\mathcal{D}|=1$k and two classes ($C=2$), the memorization ratio for the trained diffusion model stands at $94.59\\pm0.19$\\% over three different random seeds. Similarly, for $C=5$, the memorization ratio is $92.32\\pm0.14$\\%.\n\nThroughout our research, we have trained several hundred diffusion models, necessitating substantial GPU hours. Consequently, it is impractical to execute each experiment with varying random seeds. Given that the observed variance in memorization ratios is minor (less than $0.2$%), we postulate that this minimal fluctuation is unlikely to impact the statistical significance or alter the overarching conclusions of our study.\n\n---\n***Question 2: Model performance using large weight decays? Tradeoff between generation quality and memorization.***\n\nWe note that the model performance also deteriorates when large weight decay is applied during the training process.\n\nIn reference to the tradeoff you mentioned, here we make more clarifications regarding the relationship between the quality of generated samples and memorization. When a significant proportion of generated samples are replicas of the training data, their image quality is inherently high. Additionally, this leads to a generation distribution close to that of the training data, resulting in a low FID score. The FID score is conventionally used to evaluate the quality and diversity of generated images. For instance, the optimal diffusion model, which can only replicate training data (thus exhibiting a memorization ratio is 100%), achieves an FID score of 0.56. This is substantially compared to the 1.96 FID score attained by the state-of-the-art unconditional diffusion model, EDM. \n\nGiven that the objective of our research is to explore when diffusion models memorize in terms of our definitions of EMM for a training recipe, we have consistently employed a memorization metric throughout our study. \n\n---\n**Reference:**\n\n[1] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE International Conference on Computer Vision (CVPR), 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213199883,
                "cdate": 1700213199883,
                "tmdate": 1700213199883,
                "mdate": 1700213199883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sE1lbU5HuE",
            "forum": "9nT8ouPui8",
            "replyto": "9nT8ouPui8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_MQLD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_MQLD"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion models can produce identical training images during the inference time, which is called memorization. The authors observe that a memorization behavior is expected according to the training loss. The authors observe that memorization behaviors tend to occur on smaller-sized datasets. They analyze how different data distributions, diffusion model configurations, and training options influence memorization. Besides, they also observe that conditioning training data on uninformative random labels can significantly trigger memorization in diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The discussed topic is interesting. Mitigating the memorization of diffusion models is important.\n- The empirical study is through. The authors consider the effect of data data distribution, diffusion model configurations, and training options on memorization."
                },
                "weaknesses": {
                    "value": "Lack of discussion of SOTA text-to-image diffusion models, e.g., stable diffusion."
                },
                "questions": {
                    "value": "I appreciate the thorough empirical study. I am curious is there any memorization mitigation strategy given the empirical results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721381589,
            "cdate": 1698721381589,
            "tmdate": 1699636787870,
            "mdate": 1699636787870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MPexN2j4Yw",
                "forum": "9nT8ouPui8",
                "replyto": "sE1lbU5HuE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MQLD"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable questions.\n\n---\n\n***Weakness: Lack of experiments on stable diffusion.***\n\nWe would like to clarify the primary objective of our work: investigating under what conditions the diffusion models (undesirably) approximate the optimal solution represented by Eq.(2). On the one hand, such an optimal model can only replicate training data, and may even lead to privacy/copyright issues. On the other hand, such an optimal model is indeed what is theoretically expected. Therefore, it becomes imperative to gauge the extent of this theoretical hazard in typical diffusion model training scenarios. This awareness is vital for mitigating adverse consequences and refining the practical utility of these models, thus requiring quantitative studies like our work.\n\nTo advance this research, we have trained hundreds of EDMs from scratch. However, it is computationally intractable and not feasible to similarly train such a huge number of stable diffusion models. Notably, text-to-image diffusion models, which also adhere to the objective of denoising score matching, possess the similar theoretical optimum and are likewise expected to exhibit substantial memorization of training data when meeting EMM. As updated in **Appendix D: More empirical results on FFHQ** and **Appendix E: Discussions on memorization criteria** of the revised paper, our new investigations reveal consistent findings when changing the dataset to FFHQ [1] or altering the memorization ratio to KNN distance. Consequently, we conjecture that similar memorization behaviors are likely observable in state-of-the-art text-to-image diffusion models. \n\n---\n\n***Question: Any stategies to mitigate memorization?***\n\nThank you for your inquiry. Our experiments offer empirical and quantitative guidance for practitioners aiming to train their diffusion models from scratch while preventing large memorization. Generally, it is advisable to select a training recipe, encompassing aspects of data distribution, model configuration, training procedure, conditioning that exhibits a lower EMM when other performance metrics are comparably maintained. Opting for such a training recipe is advantageous as it has lower risk of memorizaton.\n\n---\n\n**Reference**\n\n[1] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE International Conference on Computer Vision (CVPR), 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212748693,
                "cdate": 1700212748693,
                "tmdate": 1700212748693,
                "mdate": 1700212748693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "akNsALR7kS",
                "forum": "9nT8ouPui8",
                "replyto": "MPexN2j4Yw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Reviewer_MQLD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Reviewer_MQLD"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thank the authors for the responses.\n\nMy concerns are not addressed.\nSOTA text-to-image models are not considered.\nAnd no clear and promising mitigation strategies or guidelines are provided.\n\nTherefore, I would like to maintain my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643189072,
                "cdate": 1700643189072,
                "tmdate": 1700643189072,
                "mdate": 1700643189072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "phPbH7Ufqe",
                "forum": "9nT8ouPui8",
                "replyto": "sE1lbU5HuE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback"
                    },
                    "comment": {
                        "value": "Thank you for your feedback.\n\nWe have extended to fine-tuning stable diffusion [a] on Artbench-10 [b], a high dimensional art dataset with image resolution of $256\\times256$. We have explored the effects of training data size and conditioning on memorization. The results have been included in **Appendix F: More empirical results on stable diffusion** in our revised paper. We note that the observations align with our previous experiments on CIFAR-10 and FFHQ.\n\nIn response to the inquiry on mitigation strategies, from our empirical analysis, we suggest to adopt random fourier features as time embeddings in the architecture of DDPM++ or incoporate weight decay or smaller batch size during the training as these strategies typically have smaller EMMs. \n\nPlease kindly let us know if there is any further concern, and we will do our best to respond.\n\n---\n**References:**\n\n[a] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\\n[b] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models with artworks. arXiv preprint arXiv:2206.11404, 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666043925,
                "cdate": 1700666043925,
                "tmdate": 1700666804721,
                "mdate": 1700666804721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TIg3DRwChf",
            "forum": "9nT8ouPui8",
            "replyto": "9nT8ouPui8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_QP3y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_QP3y"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on understanding memorization in diffusion models. The work shows that memorization behavior is theoretically expected under the training objective of diffusion models. The paper then focuses on identifying and quantifying when memorization happens in diffusion models, by focusing on three facets i.e training distribution ($P$), the architecture ($M$) and training procedure ($T$). The paper shows results on how data diversity, model size etc has an impact on memorization. Lastly, results are shown for how much input conditioning plays a role in memorization for diffusion models using actual and varying number of random classes labels assigned for CIFAR-10."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Overall the writing quality of the paper is quite good. The writing was clear, easy to understand and instructional.\n2.  The results and experimental setup are easy to understand, and useful for the research community. The analysis itself is quite timely, with ubiquitous deployment of diffusion models and copyright lawsuits that surround them.\n3. The results regarding resolution of dataset, data diversity and model size are interesting. The results confirms the expected monotonic behavior, showing that diffusion models memorize samples more when data dimension and data diversity is small, and model size is large. \n4. The results regarding the impact of time embedding are also quite surprising. It would be interesting to analyze this further, and understand why random Fourier features impact memorization in DDPM++."
                },
                "weaknesses": {
                    "value": "1. The work focuses on a simple toy setup using a subset of CIFAR-10. While such simple setup are useful for analysis, presented in this work it does leave a taste for more. It would be good to ablate setups that plague large datasets, such as dataset duplication which was discussed to be a cause for memorization in diffusion models [3, 4, 5]. \n2. I also expected to see at least a few of these analysis, on another simple  dataset such as SVHN or CIFAR-100. \n3. The results don't discuss other relevant metrics, such as quality of generations or loss convergence. For example, high weight decay in this work is shown to have a large effect on memorization but it isn't discussed how much it comes at the cost of quality of generations.\n4.  Several results presented in this paper, especially regarding dataset and model complexity are generally expected based on previous work on other generative and discriminative models [1, 2]. \n\n[1] Feng, Qianli, et al. \"When do gans replicate? on the choice of dataset size.\"\u00a0_Proceedings of the IEEE/CVF International Conference on Computer Vision_. 2021.\n[2] Zhang, Chiyuan, et al. \"Understanding deep learning (still) requires rethinking generalization.\"\u00a0_Communications of the ACM_\u00a064.3 (2021): 107-115.\n[3] Somepalli, Gowthami, et al. \"Diffusion art or digital forgery? investigating data replication in diffusion models.\"\u00a0_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2023.\n[4] Somepalli, Gowthami, et al. \"Understanding and Mitigating Copying in Diffusion Models.\"\u00a0_arXiv preprint arXiv:2305.20086_\u00a0(2023).\n[5] Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\"\u00a0_32nd USENIX Security Symposium (USENIX Security 23)_. 2023."
                },
                "questions": {
                    "value": "1. The memorization criteria used throughout the paper should be clearly explained. What's the reasoning for using an $l_2$ threshold in the image space and comparing it to the second nearest neighbor? How was the factor 1/3 derived? The top and worst matches obtained as a result of using this criterion and its drawbacks should be discussed further. Are the results the same, if the memorization criteria is changed? For example, Somepalli et al [1] used SSCD for memorization.\n2. Results regarding weight decay and EMA aren't very informative. Is the model convergence much worse when weight decay is set high? I would suggest discussing this in more detail.\n3. It would be interesting to show how noise schedule in diffusion models impacts memorization? \n\nThings that impact clarity, but didn't affect score -\n\n1. Skip connection results figures could be better, Figure 4a & 4c some markers are too close to understand. The main observation while comes out clearly, the results about number of skip connections is hard to parse from the figure.\n2. The memorization criterion can be easily explained in words.  I had to look up the referenced paper, as the notation using $j$-th closest sample was taking a while to parse."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Reviewer_QP3y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787109192,
            "cdate": 1698787109192,
            "tmdate": 1699636787744,
            "mdate": 1699636787744,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nOJ86sFL3i",
                "forum": "9nT8ouPui8",
                "replyto": "TIg3DRwChf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QP3y [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your supportive feedback and valuable questions.\n\n---\n\n***Weakness 1 and 2: Lack of experiments or analysis on other datasets.***\n\nWe have also conducted a series of new experiments on the FFHQ dataset [6], which is a higher-dimensional face dataset. These experiments have been meticulously updated in **Appendix D** of the updated paper, titled \"**More empirical results on FFHQ**\". Given the time limit, the investigation primarily focused on revisting the effects of data dimension / time embeddings / conditioning, on the memorization of diffusion models. It is noteworthy that the outcomes of these recent experiments corroborate the findings previously observed on the CIFAR-10 dataset.\n\nOur objective is to determine the specific training size when diffusion models demonstrate similar memorization behaviors as the theoretical optimum. This specific training size is defined as EMM in our paper. However, dataset duplication will cause the ambiguity in this definition.\n\n---\n\n***Weakness 3: Lack of analysis on other metrics related to generation. For example, the effects of weight decay on quality metrics.***\n\nWe would like to clarify the primary objective of our work: investigating under what conditions the diffusion models (undesirably) approximate the optimal solution represented by Eq.(2). On the one hand, such an optimal model can only replicate training data, and may even lead to privacy/copyright issues. On the other hand, such an optimal model is indeed what is theoretically expected. Therefore, it becomes imperative to gauge the extent of this theoretical hazard in typical diffusion model training scenarios. This awareness is vital for mitigating adverse consequences and refining the practical utility of these models, thus requiring quantitative studies like our work. Consequently, the memorization metric was the focal point of our investigation. \n\nFID score is conventionally employed for assessing the quality and diversity of generations. However, in the context of our research, a low FID score is expected when diffusion models extensively memorize training data. This is because a large amount of training data replicas in generated samples would naturally result in a generation distribution close to the training distribution. Furthermore, it is anticipated that image quality metrics would also show enhanced performance, given that replicas of training data are typically of high quality. \n\nIt is crucial to note that the introduction of weight decay greater than zero alters the training objective, leading to a divergence from the original theoretical optimum, which can only replicate training data. This divergence becomes apparent for large weight decay. However, as aforementioned, the primary focus of this study is not on these other metrics but rather on the memorization aspect. \n\n---\n\n***Weakness 4: \u201cSeveral results presented in this paper, especially regarding dataset and model complexity \u2026\u201d***\n\nThe relationship between learning outcomes and the complexity of data and models represent a topic of enduring interest within the machine learning community. Nevertheless, the literature has not adequately elucidated the relationship between the memorization of diffusion models and various influencing factors. Our study aims to address this gap via a thorough analysis, examining the role of data distribution, model configuration, training procedure, and conditioning. \n\nMoreover, the motivations underpinning our research diverge significantly from those in previous studies, particularly those outlined in references [1] and [2]. While [1] delved into memorization within GANs, and [2] investigated similar phenomena in discriminative models, our focus is distinctly oriented towards diffusion models. Unlike GANs and discriminative models, which possess infinite optimal solutions, diffusion models are characterized by a closed-form solution that exclusively memorizes training data without generalization. This distinct attribute propels our inquiry into the memorization discrepancies between the trained diffusion model and its theoretical optimal solution. In contrast to [1], which concentrated on dataset size and complexity, our experimental framework extends to encompass the effects of data distribution, model configuration, training procedure, and conditioning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212619259,
                "cdate": 1700212619259,
                "tmdate": 1700212619259,
                "mdate": 1700212619259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L2Lt3IPohI",
                "forum": "9nT8ouPui8",
                "replyto": "TIg3DRwChf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QP3y [2/2]"
                    },
                    "comment": {
                        "value": "***Question 1: More explanations on selected memorization ratio.***\n\nThank you for your valuable suggestion. The Euclidean $l_2$ distance between a generated image and its nearest training data (or KNN distance) was used in [5] as a measure of image memorization. In our preliminary experiments, this $l_2$ distance was also employed as a metric for memorization. Our findings indicate that the outcomes are consistent whether utilizing this pure $l_2$ distance or the $l_2$ distance ratio, as detailed in our paper. The factor of $\\frac{1}{3}$, adopted from the paper [7], was identified by the authors as a threshold that correlates closely with the human perceptual recognition of memorization. It is acknowledged that determining an exact threshold to clearly differentiate between memorized and non-memorized generations is a complex challenge. Consequently, we have incorporated your suggestion and presented the experimental results using an alternative memorization metric for cross-validation purposes.\n\nAs updated in **Appendix E: Discussions on Memorization Criteria** in the revised paper, we re-evaluate our results in the main paper by using the above $l_2$ distance as memorization metric. It has been observed that lower KNN distances are indicative of diffusion models demonstrating a higher propensity for memorizing training data. We notice that these new results are in alignment with our original conclusions using the memorization ratio metric.\n\n---\n\n***Question 2: More explanations on experimental results regarding EMA and weight decay.***\n\nThe primary aim of our study is to systematically analyze the influence of various factors\u2014namely, data distribution, model configuration, training procedure, and conditioning on the value of EMM. Our exploration on EMA is motivated by its substantial impact on the FID score and overall image quality. This observation prompted an investigation into whether it similarly exerts a considerable effect on memorization. Our empirical findings reveal that while EMA substantially influences image quality, its effect on memorization is marginal, a conclusion that is not trivial.\n\nOur exploration on weight decay is motvated by that the training objective of diffusion models is altered when introducing weight decay. Consequently, the optimal solution is also different from Eq. 2 of our main paper. This alteration in the training objective raises the question of how weight decay affects the deviation of trained diffusion models from the optimal solution. Our experiments have shown that with a small weight decay, diffusion models are still capable of demonstrating memorization behaviors. \n\n---\n\n***Question 3: New explorations on noise schedule.***\n\nThank you for your suggestion. We have prioritized experiments involving the FFHQ dataset. Nonetheless, if time allows, we will endeavor to incorporate your suggestion regarding the investigation of noise schedules in diffusion models and include the results in the revised version of our paper.\n\n---\n\n***Minor question 1: Better demonstrations/visualizations regarding skip connection results.***\n\nYour feedback is highly appreciated. We have amended the figures in our revised paper to enhance clarity and better illustrate the results. \n\n---\n\n***Minor question 2: Better word descriptions on memorization ratio.***\n\nThank you for your suggestion. We have incorporated detailed word descriptions on the memorization metric we used into the revised version of our paper. \n\n---\n\n**Reference:**\n\n[1] Feng, Qianli, et al. \"When do gans replicate? on the choice of dataset size.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\\\n[2] Zhang, Chiyuan, et al. \"Understanding deep learning (still) requires rethinking generalization.\" Communications of the ACM 64.3 (2021): 107-115.\\\n[3] Somepalli, Gowthami, et al. \"Diffusion art or digital forgery? investigating data replication in diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\\\n[4] Somepalli, Gowthami, et al. \"Understanding and Mitigating Copying in Diffusion Models.\" arXiv preprint arXiv:2305.20086 (2023).\\\n[5] Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023.\\\n[6] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE International Conference on Computer Vision (CVPR), 2019.\\\n[7] TaeHo Yoon, Joo Young Choi, Sehyun Kwon, and Ernest K Ryu. Diffusion probabilistic models generalize when they fail to memorize. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212693499,
                "cdate": 1700212693499,
                "tmdate": 1700212693499,
                "mdate": 1700212693499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IcJgguHr9Y",
            "forum": "9nT8ouPui8",
            "replyto": "9nT8ouPui8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_sac3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_sac3"
            ],
            "content": {
                "summary": {
                    "value": "- The paper discusses that the training objective of diffusion models has a closed-form optimal solution that can only generate training-data replicating samples, and hence a memorization behaviour is expected. \n- A new metric called Effective model memorization(EMM) is introduced which quantifies the maximum number of training data points at which a diffusion model demonstrates the aforementioned memorization behaviour.\n- The impact of various factors like data distribution, model, training procedure and conditional generation on memorization behaviour are discussed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extensive experiments on the impact of various factors like data dimension & diversity, model configuration, training procedure and conditional generation on memorization behaviour.\n- The theory behind memorization behaviour of the optimal solution in diffusion models is discussed in detail and a new metric called Effective model memorization(EMM) is introduced."
                },
                "weaknesses": {
                    "value": "There is no detailed comparison with related work in these areas. The effect of various factors on memorization in diffusion models has been discussed in literature before.\n- The effect of dataset size on memorization in diffusion models has been discussed before in [1]\n- The effect of text conditioning and dataset complexity is also discussed in [2]. \n\n\n\n[1.] Somepalli, Gowthami, et al. \"Diffusion art or digital forgery? investigating data replication in diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2.]Somepalli, Gowthami, et al. \"Understanding and Mitigating Copying in Diffusion Models.\" arXiv preprint arXiv:2305.20086 (2023)."
                },
                "questions": {
                    "value": "- How do the findings discussed in the paper help us understand memorization in diffusion models happening in real world settings where datasets are huge?\n\n- How is this work different from the findings in [1],[2],[3] ?\n\n\n[1.] Somepalli, Gowthami, et al. \"Diffusion art or digital forgery? investigating data replication in diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. \n\n[2.]Somepalli, Gowthami, et al. \"Understanding and Mitigating Copying in Diffusion Models.\" arXiv preprint arXiv:2305.20086 (2023).\n\n[3.]Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Reviewer_sac3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698945650686,
            "cdate": 1698945650686,
            "tmdate": 1699636787638,
            "mdate": 1699636787638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lxjVFpg1pd",
                "forum": "9nT8ouPui8",
                "replyto": "IcJgguHr9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sac3 [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable questions.\n\n---\n\n***Weakness & Question 2: Lack of detailed comparison with [1] [2] [3]***\n\nThank you for the question. First, the foundational motivations of our research diverge significantly from those of studies [1], [2], [3]. We aim to investigate under what conditions the diffusion models (undesirably) approximate the optimal solution represented by Eq.(2). On the one hand, such an optimal model can only replicate training data, and may even lead to privacy/copyright issues. On the other hand, such an optimal model is indeed what is theoretically expected. Therefore, it becomes imperative to gauge the extent of this theoretical hazard in typical diffusion model training scenarios. This awareness is vital for mitigating adverse consequences and refining the practical utility of these models, thus requiring quantitative studies like our work.\n\nSecondly, our paper conducted a comprehensive and quantitative examination of various factors influencing the memorization of diffusion models. These factors span a wide range, including data distribution, model configuration, training configuration, and conditioning. In contrast, [1] and [3] primarily focused on demonstrating that diffusion models may replicate training data and proposing frameworks for detecting or extracting such replication. [2] investigated the impact of various factors on the memorization in text-to-image diffusion models, particularly those fine-tuned on new datasets. Compared to [2], our experiments predominantly engage with unconditional diffusion models trained from scratch, and the variables we examine differ from those in [2]. Our findings offer new insights, e.g. time embedding / random conditions / skip connections, into how these factors affect memorization in diffuion models, as detailed in our paper.\n\nWe wound also like to make more clarifications on how our research different from [1] regarding dataset size and [2] regarding text conditioning and dataset complexity. \n\n- In [1], the authors conducted a comparative analysis of the memorization tendencies in diffusion models trained on datasets of varying sizes (specifically, 300 versus 3,000 samples). In contrast, in our research, we showed when diffusion models memorize in terms of a novel metric EMM. This specific training data size reflects the capacity of model and algorithm, etc, and discloses the interactions among different factors. Additionally, we monitored the memorization ratios throughout the training procedure and showed that the memorization becomes apparent after a sufficiently extended training duration, particularly when the size of the training data is sufficiently small. \n\n- The authors in [2] undertook a comparative analysis of diffusion models conditioned on various types of captions. Our research, however, diverges in the notable discovery that random conditions can effectively induce the memorization of class-conditioned diffusion models. We also find that the number of classes plays an important role in the memorization. In terms of dataset complexity, [2] compared models trained on LAION-10k and Imagenette datasets, attributing the higher memorization observed in the latter to the structural complexity of its images. Here the dataset complexity is assessed on an instance-level basis. While in our experiments, we meticulously constructed a series of training datasets, each varying in the number of classes or the intra-class diversity, while keeping the other factor constant. In our research, the number of classes and intra-class diversity serve as population-level metrics to assess the dataset complexity.\n\nFinally, we introduced a novel metric for memorization: EMM. This metric is designed to determine the conditions under which trained diffusion models exhibit memorization behaviors akin to those of the optimal solution."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212372582,
                "cdate": 1700212372582,
                "tmdate": 1700212372582,
                "mdate": 1700212372582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oCREiwPoB0",
                "forum": "9nT8ouPui8",
                "replyto": "IcJgguHr9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sac3 [2/2]"
                    },
                    "comment": {
                        "value": "***Question 1: Extension of conclusions on small-scaled datasets to real scenarios.***\n\nFirstly, we have run a series of new experiments on the FFHQ dataset [4] during the rebuttal period, which has been updated in **Appendix D: More empirical results on FFHQ** in the revised paper. The new experimental results support our findings on CIFAR-10.\n\nSecondly, we would like to emphasize that our objective is to gauge the extent of the theoretical hazard in typical diffusion model training scenarios instead of understanding the memorization behaviors of diffusion models trained on large data. When diffusion models trained on large amounts of data, they generally do not memorize training data in a pixel-by-pixel manner. This also aligns the research findings in [3]. The authors in [3] (Table 1) mentioned that only 200~300 training images out of 1 million generations sampled by DDPM [5] and its variant [6] are extracted succussfully. The above two models are trained on a dataset of only 50k CIFAR-10 images. Therefore, much larger training data size is out of our research scope.\n\n---\n\n**References:**\n\n[1] Somepalli, Gowthami, et al. \"Diffusion art or digital forgery? investigating data replication in diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\\\n[2]Somepalli, Gowthami, et al. \"Understanding and Mitigating Copying in Diffusion Models.\" arXiv preprint arXiv:2305.20086 (2023).\\\n[3]Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023.\\\n[4] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE International Conference on Computer Vision (CVPR), 2019.\\\n[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\\n[6] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (ICML), pp. 8162\u20138171. PMLR, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212452308,
                "cdate": 1700212452308,
                "tmdate": 1700212452308,
                "mdate": 1700212452308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F6JO5CPqj4",
                "forum": "9nT8ouPui8",
                "replyto": "IcJgguHr9Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Reviewer_sac3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Reviewer_sac3"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your highlighting the differences in your work and previous work. I understand that this paper introduces a new metric called  EMM and all discussions are centred around it. But it seems that the new metric has limited application because the memorization in diffusion models according to the definition used in the work moves towards zero for any reasonable setting expected in real world settings. As dataset sizes and image resolutions go up, memorization starts decreasing at a very small scale. The setting of having completely random text conditioning is also not very real-world. Can the authors think of any other applications and benefits of the new metric ?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598431668,
                "cdate": 1700598431668,
                "tmdate": 1700598525040,
                "mdate": 1700598525040,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BGrhQ3E7Km",
            "forum": "9nT8ouPui8",
            "replyto": "9nT8ouPui8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_p3eW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6813/Reviewer_p3eW"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors present a detailed study on memorization in diffusion models by investigating various factors that may be responsible for increased memorization of training samples. In particular, the authors vary the amount of data in the training set, the time that the model was trained for, the size and the configuration of the model, and the existence of various types of embeddings in the model, and analyze their respective impacts on memorization. The study provides a deep investigation on small-scale data sets like CIFAR-10, and discusses unconditional versus conditional generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. First, the paper is very well structured with strong motivation for why memorization is natural in diffusion models, and then going on to present preliminary results on how, on small datasets, diffusion models tend to memorize. \n2. Second, the study is very comprehensive in terms of the breadth of the factors that the authors assess that could lead to memorization. In particular, I enjoyed the section on data distribution, which discusses data dimensionality and diversity with two different formulations. I enjoyed reading these two formulations because these are facets of memorization that are seldom discussed, and most prior work typically only discusses factors like model size and data size. \n3. Third, the paper is very thorough in the effects of embedding, and in particular, the finding that using Fourier embeddings versus positional embeddings can cause a significant change in memorization was surprising. \n4. Fourth, the work acts as a great guide for practitioners who might want to understand the effect of memorization and will be useful for future research."
                },
                "weaknesses": {
                    "value": "1. First, the analysis of memorization is done in complete isolation of the model's generalization or analysis of aspects of image generation or image quality such as inception score or pressure distance. And I do not think that any analysis on memorization can purely happen in the absence of the latter because we might end up analyzing models that do not make any sense for practitioners. \n2. Second, the experiments are performed on very small datasets and it is unclear how these findings actually take shape in real scenarios where you have huge datasets and you are training on millions of samples with almost similarly sized models. \n3. Third, I don't think that the authors should perform a set of experiments where they try to fine-tune a stable diffusion model on a small dataset which may still be a reasonable analysis where people might want to use a custom stable diffusion model on a particular style by further fine-tuning it on a certain type of data. However, the setting that the authors discuss while it is very helpful in creating the analysis that they do is also very, very restrictive and does not generalize to realistic settings that practitioners actually care for. And I would encourage the authors to explore that. \n4. Fourth, a lot of the paper is about showcasing a finding but does not actually explain the reasons for why a finding actually makes sense. For instance, in particular, the section on the type of embedding was rather weak in my opinion in terms of explaining the effect. Similarly, the section on why data diversity does not influence memorization so much was pretty weak and this paper can significantly be strengthened if the authors actually discuss the results in more detail and why they should happen in a particular way. And I would say that this is true for most of the sections where currently this paper reads as a reporting of a result rather than a scientific discussion of a phenomenon."
                },
                "questions": {
                    "value": "See requests in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6813/Reviewer_p3eW"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699599855095,
            "cdate": 1699599855095,
            "tmdate": 1699636787523,
            "mdate": 1699636787523,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KcCNRAQBXE",
                "forum": "9nT8ouPui8",
                "replyto": "BGrhQ3E7Km",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer p3eW"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable questions.\n\n---\n\n***Weakness 1: Isolation of analysis of memorization and generalization or image quality.***\n\nThank you for pointing this out. We would like to clarify the primary objective of our work: investigating under what conditions the diffusion models (undesirably) approximate the optimal solution represented by Eq.(2). On the one hand, such an optimal model makes no sense to practitioners (as also mentioned in the review), and may even lead to privacy/copyright issues. On the other hand, such an optimal model is indeed what is theoretically expected. Therefore, it becomes imperative to gauge the extent of this theoretical hazard in typical diffusion model training scenarios. This awareness is vital for mitigating adverse consequences and refining the practical utility of these models, thus requiring quantitative studies like our work.\n\nFurther, studying memorization behavior also helps the understanding of generalization. A recent work [1] suggests that diffusion models with memorization has potential adverse generalization performance. Therefore, it is expected that an increase in the memorization ratio within a diffusion model implies a diminution in its generalization capability.\n\nAs for the image quality, it has interleaving relationship with memorization. When a significant proportion of generated samples are replicas of the training data, their image quality is inherently high. Additionally, this leads to a generation distribution close to that of the training data, resulting in a low FID score. For instance, the optimal diffusion model, which can only replicate training data (thus exhibiting a memorization ratio is 100%), achieves an FID score of 0.56. This is substantially lower compared to the 1.96 FID score attained by the state-of-the-art unconditional diffusion model, EDM. Therefore, we do not emphasize metrics of image quality in our experiments.\n\n---\n\n***Weakness 2 and 3: Extension of conclusions on small-scaled datasets to real scenarios.***\n\nAs elucidated in the above, our objective is to gauge the extent of the theoretical hazard in typical diffusion model training scenarios instead of understanding the memorization behaviors of diffusion models trained on millions of images. Through our extensive experiments, we find that the EMMs for training recipes of diffusion models are generally small. This provides explanations why diffusion models in real scenarios demonstrate low memorization ratios.\n\nIn addition to CIFAR-10, we have conducted a series of additional experiments using the FFHQ dataset [2], which is a higher-dimensional face dataset. These new experiments have been included in **Appendix D: More empirical results on FFHQ** of our revised paper. Due to the time constraints, our additional experiments focused on investigating the impact of data dimension / time embeddings / conditioning, on the memorization of diffusion models. It is noteworthy that the outcomes of these recent experiments are in alignment with our initial findings derived from the CIFAR-10 dataset.\n\n---\n\n***Weakness 4: Lack of analysis for specific factors.***\n\nOur main body of work was developed as a comprehensive and empirical guidance towards the effects of various factors from perspectives of data distributions / model configuration / training procedure / conditioning on memorization in diffusion models. Our findings aim to delineate which factors have a substantial impact on memorization and which contribute more subtly. Therefore, our paper is positioned at an empirical analysis instead of a theoretical analysis of how different factors interact. Additionally, we provide several surprising findings, e.g. the significant effects of random labels, which may inspire the theoretical practitioners for further exploration. \n\n---\n\n**References:**\n\n[1] TaeHo Yoon, Joo Young Choi, Sehyun Kwon, and Ernest K Ryu. Diffusion probabilistic models generalize when they fail to memorize. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023.\\\n[2] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE International Conference on Computer Vision (CVPR), 2019."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212211139,
                "cdate": 1700212211139,
                "tmdate": 1700212211139,
                "mdate": 1700212211139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dwhpg2ZZoB",
                "forum": "9nT8ouPui8",
                "replyto": "BGrhQ3E7Km",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedback"
                    },
                    "comment": {
                        "value": "Thank you for your suggestion on stable diffusion. We have extended to fine-tuning stable diffusion [a] on Artbench-10 [b], a high dimensional art dataset with image resolution of $256\\times256$. We have explored the effects of training data size and conditioning on memorization. The results have been included in **Appendix F: More empirical results on stable diffusion** in our revised paper. We note that the observations align with our previous experiments on CIFAR-10 and FFHQ. \n\nPlease kindly let us know if there is any further concern, and we will do our best to respond.\n\n---\n**References:**\n\n[a] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\\n[b] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models with artworks. arXiv preprint arXiv:2206.11404, 2022."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666547270,
                "cdate": 1700666547270,
                "tmdate": 1700666868560,
                "mdate": 1700666868560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]