[
    {
        "title": "Efficient Continual Pre-training for Building Domain Specific Large Language Models"
    },
    {
        "review": {
            "id": "aEpYCnVD5u",
            "forum": "onyGT5Nbuz",
            "replyto": "onyGT5Nbuz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_rYGF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_rYGF"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates a new approach to developing domain-specific Large Language Models (LLMs) using continual pre-training. A domain-specific LLM, FinPythia-6.9B, was created for the financial sector through domain-adaptive continual pre-training. The results show that FinPythia has improved performance on financial tasks compared to the original base model. Additionally, the paper proposes effective data selection strategies at the embedding level for continual pre-training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper proves that continual pre-training can facilitate the LLM's performance on domain-specific LLMs.\n\n* The authors use embedding level selection to acquire the essential data samples and show that with 10% data, the pre-training can achieve comparable performance instead of using a large amount of data."
                },
                "weaknesses": {
                    "value": "* This paper only demonstrates that the proposed pipeline can be efficient with the data selection on one type of LLM, Pythia, which is insufficient to support the claim of efficiency advantages for other types of LLMs, e.g., LLAMA, OPT.\n\n* The comparison with other baseline methods is not fair because more data is used for training in continual pre-training. To illustrate the effectiveness of the continual pre-training, the authors should apply the proposed method to other types of LLMs."
                },
                "questions": {
                    "value": "* Could you do similar continuous pre-training and ablation studies for LLAMA-7B?\n\n* Could you do the same continuous pre-training for OPT, BLOOM, and GPT-J and report the results on financial tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Reviewer_rYGF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698341322965,
            "cdate": 1698341322965,
            "tmdate": 1700633875293,
            "mdate": 1700633875293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mKMXBlkIel",
            "forum": "onyGT5Nbuz",
            "replyto": "onyGT5Nbuz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_npUz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_npUz"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores domain-adaptive pre-training in the finance sector. It introduces a heuristic data selection method based on novelty (perplexity) and diversity (entropy of POS tagging). The results demonstrate its superior performance compared to general language models (LLM)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. DAPT/DACP is an important and practical problem\n2. The proposed data selection method is simple to use and easy to understand"
                },
                "weaknesses": {
                    "value": "In this paper, Domain-adaptive pre-training (DAPT) or DACP is not a novel concept, and the main innovation lies in the proposed data selection method. However, the paper lacks comparisons with other DAPT baselines, which is a significant drawback. For example, some prior works have explored modifications to the DAPT loss or gradient. Notably, [1] addresses continual pre-training but is not compared with in this work, nor are any other mentioned baseline systems in [1].\n\n[1]: Adapting a Language Model While Preserving its General Knowledge, Ke et al., EMNLP 2022"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Reviewer_npUz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544585824,
            "cdate": 1698544585824,
            "tmdate": 1699636373733,
            "mdate": 1699636373733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "JP3Blrqfo0",
            "forum": "onyGT5Nbuz",
            "replyto": "onyGT5Nbuz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_LVHJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_LVHJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces FinPythia-6.9B, a LLM developed through domain-adaptive continual pre-training on the financial domain. The paper shows that continual pre-training can yield consistent improvements on financial tasks over the original model. It also experiments different data selection strategies for continual pre-training and proposes a data selection strategy based on novelty and diversity measurements. The proposed efficient domain-adaptive continual pre-training technique outperforms vanilla continual pre-training's performance with just10% of corpus size and cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper contributes FinPythia-6.9B a foundation model for financial domain via continual pre-training. FinPythia-6.9B outperforms the original LLM on a series of tasks in the financial domain which showcases the feasibility of building domain-specific LLMs in a cost-effective manner.\n2. Improving continual pre-training from the data selection aspect is interesting. This paper conducts extensive experiments on different data selection methods and the gained insights can be useful to the community. Also, the proposed data selection technique is effective based on the experimental results.\n3. The paper also curates a large-scale financial corpus."
                },
                "weaknesses": {
                    "value": "1. The tasks for experimental are mainly classification tasks, which is limited as the LLM is powerful and should be evaluated on more complicated tasks or at least some generation tasks. I know the paper conducts qualitative evaluation on some QA samples. Is there any generation task in financial domain that you can use to systematically evaluate FinPythia-6.9B?\n2. This is mainly an empirical paper and does not have solid theoretical supports.\n3. ETS gives better result than ETA, but I'm wondering if knowing task data is a reasonable assumption in the real world as a foundation model is mainly designed for multiple tasks. \n4. The paper involves a lot of abbreviations that are quite similar, which makes the paper hard to read. I suggest using the full word \"task-specific\" and \"task-agnostic\" and removing the same suffix \"DACP\" in the abbreviations."
                },
                "questions": {
                    "value": "1. The paper argues that good data selection can make continual pre-training data-efficient and maintain the performance on general tasks. Speaking of this part, I think [1] needs to be discussed in the paper. That work also studies the continual pre-training problem and considers preserving the model's general ability from a different perspective.\n2. I think \"building domain-specific LLMs from scratch\" in the abstract may confuse the readers as continual pre-training is actually opposed to pre-training from scratch.\n\n[1] Adapting a Language Model While Preserving its General Knowledge, Ke et al., EMNLP 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Reviewer_LVHJ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646957814,
            "cdate": 1698646957814,
            "tmdate": 1699636373657,
            "mdate": 1699636373657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "m8Ml727wBh",
            "forum": "onyGT5Nbuz",
            "replyto": "onyGT5Nbuz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_QRSW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4091/Reviewer_QRSW"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores domain-specific continual pretraining for LLMs through continued pretraining of Pythia models on a newly curated financial dataset. The authors demonstrate that domain-adaptive continual pretraininig (DACP) can improve performance of an LLM on domain-specific tasks. They then explore methods for improving the efficiency of DACP by selecting data according to different metrics, demonstrating efficiency and performance gains using only 10% of the tokens of the original domain-specfic dataset. Finally, the authors show that general (non-domain-specific) performance is degraded less in the 10% trained models compared to the 100% DACP baseline, suggesting that in addition to gains in domain-specific performance and in training efficiency, the proposed methods help the model retain more of its original domain-nonspecific performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has an appealing overall structure (DACP vs baseline, modifications of DACP vs DACP on domain-performance, mod vs DACP on general performance). The questions addressed are of immediate topical relevance to researchers and practitioners of LLMs. The fact that the proposed modifications to DACP yield better specific performance and better generalization performance is a nice contribution."
                },
                "weaknesses": {
                    "value": "The fundamental questions of the paper are valuable to address, but the conclusions drawn from the answers provided by this paper are not as comprehensive or original as one might hope. The demonstration that training a pretrained LLM with continued pretraining on a domain-specific dataset is more efficient than training from scratch is not a surprise, nor a novelty. The results of the proposed efficient-DACP methods are generally positive (though not universally, and the deficiencies are not explained or addressed), but it is unclear to what extent those positive results might generalize to other domain-specific datasets.\n\nThe clarity of the writing could be improved. Some sections (introduction of TACP) are ambiguous regarding important details, or just difficult to read (SoftSampling).\n\nThe results of the paper are only presented on 10% and 100% of the domains-specific corpus. Why 10% in particular? It would be useful to understand the tradeoffs involved at differing percentages. The paper would be strengthened by showing the performance curves across other percentages as well. Particularly, how does this perform with as little as 1%? At what dataset size does high-quality data fail improve domain specific performance in continual pretraining? This would be very interesting to know.\n\nThe results tables (3 and 4) are very busy and somewhat difficult to parse. The stddevs presented in Table 3 are really large, which makes it difficult to be confident in the significance of the results. The relationship between the columns of Table 1 and the rows of Table 3 is unclear. Overall, the results in the tables support the arguments made in the text, but the tables are difficult to understand due to their arrangement and formatting.\n\nDifferent metrics are proposed for scoring the quality of the domain-specific data, and their performance varies dramatically across datasets. The paper would be made much stronger with analysis of the strengths and weaknesses of the different metrics, and by explanations of their divergent performance, perhaps with examples. As is, it is difficult to asses which metric one should use, and why. For example, ETA-DACP-ppl uses perplexity as a heuristic for novelty, which is assumed to be desirable. In a high-quality dataset, this might be a reasonable assumption, but in a noisy dataset this will likely score highly noisy examples, leading to a noise-enriched low quality sample. This method performs quite poorly compared to the others, suggesting the dataset may be noisy. To what extent are the other methods just avoiding training on the noisy examples in the full dataset?"
                },
                "questions": {
                    "value": "Questions/suggestions:\n\nWhy is performance on FiQA SA so bad? Table 1. This is not necessarily an issue but is an outlier compared to the other datasets so should be mentioned/explained.\n\nTACP: Make explicitly clear the similarities/differences between TACP / DACP / generic finetuning while introducing TACP. What is the difference between domain-specificity and task-specificity? Is domain-specific LM data on a topic, and Task-specific is data formatted in a specific format (ie QA style)? From the text, it is unclear how DACP and TACP differ except in the amount of {domain/task}-specific data available, and this distinction isnt made until section 2.4, but should be made in 2.3. Also, is there a citation for TACP?\n\nSoft Sampling (2.4.3): please rewrite this section to make more clear the methodology. The first couple sentences leave ambiguous whether the softness refers to you are probabilistic sampling or uniform sampling with example-loss-weighting, and the sentences that clarify that ambiguity aren't presented til midway through the next paragraph.\n\nTable 1:\nPut the columns for the 1B models and 7B models adjacent to each other, so the comparison is easily made. Consider also adding diffs for the FinPythia performance showing the delta compared to Pythia, as the diff is the point of the table.\n\nTable 3:\nThe stddev of all models, even on the averaged F1 is very high. This makes it hard to know how significant the results are.\nShouldn't the Pythia 1B column in Table 1 correspond to the same evaluation as the Pythia 1B row in Table 3? Please clarify.\nWhich row does FinPythia 1B from Table 1 correspond to in Table 3? DACP 100%?\n\nTable 4:\nHellaSwag isn't mentioned anywhere else in the text of the paper. Please introduce all datasets in use here (probably in 3.1, optionally in 4.2). Also, why not present the Pile test loss as well?\nSince the point of the table is to demonstrate low deltas over the baseline Pythia model, please show the actual deltas in the text of the table. This would help the table make its point and significantly improve readability.\n\nTable 2: this takes up a ton of room and doesn't add much. Consider trimming to 2 examples or moving to appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4091/Reviewer_QRSW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693564424,
            "cdate": 1698693564424,
            "tmdate": 1699636373568,
            "mdate": 1699636373568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]