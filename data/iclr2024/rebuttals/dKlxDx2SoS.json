[
    {
        "title": "Prompt Learning with Quaternion Networks"
    },
    {
        "review": {
            "id": "oN7sKiP8Gt",
            "forum": "dKlxDx2SoS",
            "replyto": "dKlxDx2SoS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_Rjda"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_Rjda"
            ],
            "content": {
                "summary": {
                    "value": "This paper devised a method that combines a pre-trained model to achieve high performance in VL tasks even in situations where there is no training data. It also achieved excellent benchmark results in three validation tests: base-to-novel generalization, cross-dataset transfer, and domain transfer scenarios, even compared to MaPLE, one of the latest models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Benchmark tests have been conducted and it has achieved excellent results compared to SOT methods. The benchmarks are also reasonable and provide excellent comparisons."
                },
                "weaknesses": {
                    "value": "The structure of the model shown in Figure 2 is poorly explained, making it difficult to understand the difference between it and other models. I also got the impression that there was a lack of consideration as to why MaPLE achieved such excellent results. I would have liked a more detailed chapter to explain why this model is so good."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Reviewer_Rjda"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698581264847,
            "cdate": 1698581264847,
            "tmdate": 1699636421365,
            "mdate": 1699636421365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ouj01g4JRC",
                "forum": "dKlxDx2SoS",
                "replyto": "oN7sKiP8Gt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rjda"
                    },
                    "comment": {
                        "value": "We appreciate your valuable feedback and address your concerns as follows.\n\n**W: Model structure and performance.** \nAs suggested, we will update Figure 2 to supply more detailed comparisons to help readers understand the differences between our method and other approaches. In general, both MaPLE and our proposed QNet are based on the architecture of CLIP. MaPLE primarily employs linear projections to align and merge features from different modalities. Linear projection can maintain mutual synergy and reduce the tendency to learn independent uni-modal prompts, proving effective in specific situations. However, it oversimplifies the intricate interconnections in multimodal data. In our approach, QNet handles diverse modalities by leveraging the inherent orthogonal relationship among the three imaginary axes within the quaternion hidden space. Unlike linear projection in MaPLE, our QNet utilizes the quaternion space to construct the intricate and non-linear associations between modalities, which yields better performance in multiple datasets, especially in zero-shot learning scenarios."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621337867,
                "cdate": 1700621337867,
                "tmdate": 1700621337867,
                "mdate": 1700621337867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NdUySpfynw",
            "forum": "dKlxDx2SoS",
            "replyto": "dKlxDx2SoS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_jgw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_jgw2"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to improve the performance of a multi-modal pre-trained foundation model via prompt tunning. This work proposes to use Quaternion Networks to align the semantics across modalities while finetuning. Quaternion Network projects feature quaternion hidden space, where three mutually orthogonal imaginary axes, namely i, j, and k, allocate unique weights to various distribution features from diverse perspectives. Compared to previous prompt learning works, the major difference is introducing quaternion hidden space to fuse data modalities. This work conducts experiments on more than 10 datasets which is solid to some extent.\n\nPros:\n- This work introduces quaternion hidden space to prompt learning for foundation models, which is new.\n- Experiments cover a wide range of datasets.\n\nCons:\n- Comparing the proposed method with previous prompt learning methods on computation overhead and latency is needed.\n- Quaternion hidden space seems to be more sophisticated than linear space which might be better than linear projection. However, it's not obvious why Quaternion Networks is better than the previous prompting technique; or why tunning multimodal pre-trained networks needs quaternion hidden space. \n\n\nIn-depth comparison with previous prompting methods and analysis of this quaternion network improve this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Pros:\n- This work introduces quaternion hidden space to prompt learning for foundation models, which is new.\n- Experiments cover a wide range of datasets."
                },
                "weaknesses": {
                    "value": "Cons:\n- Comparing the proposed method with previous prompt learning methods on computation overhead and latency is needed.\n- Quaternion hidden space seems to be more sophisticated than linear space which might be better than linear projection. However, it's not obvious why Quaternion Networks is better than the previous prompting technique; or why tunning multimodal pre-trained networks needs quaternion hidden space."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Reviewer_jgw2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683960463,
            "cdate": 1698683960463,
            "tmdate": 1699636421272,
            "mdate": 1699636421272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "48W1VjxvId",
                "forum": "dKlxDx2SoS",
                "replyto": "NdUySpfynw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jgw2"
                    },
                    "comment": {
                        "value": "We appreciate your valuable comments and address your concerns as follows.\n\n**Cons1: Computation overhead and latency.**\nFor a fair comparison, we calculated the parameters and GFLOPs for both MaPLe and our proposed QNet, which use multimodal prompts. Our method involves fewer trainable parameters (2.93M) than MaPLe (3.56M) yet achieves better performance across 11 datasets. Regarding computational overhead, while MaPLe operates at 167 GFLOPs, our method runs a slightly higher computational load at 179 GFLOPs. This slight increase is attributed to the more complex non-linear quaternion space compared to the linear space of MaPLe. As for latency, CoOP, Co-CoOp, Maple, and our QNet runs at 299Fps, 48Fps, 103Fps, and 50Fps under processing a hundred images per batch. Among these methods, MaPLe and CoOp exhibit higher speeds as they only require a single forward pass of prompts through the text encoder. In contrast, Co-CoOp and our QNet, which rely on instance-conditional designs, demand an independent forward pass for instance-specific prompts. Hence, Co-CoOp and our QNet consume more GPU memory when the batch size is larger than 1, leading to slightly slower running speeds.\n\n**Cons2: The superiority of quaternion spaces.** \nWe acknowledge that quaternion hidden spaces are more sophisticated than linear spaces, resulting in better performance. However, current multimodal fusion methods often rely on explicit interaction structures that struggle to fully capture the varied and complex patterns inherent in multimodal data. For example, MaPLE with linear mappings oversimplifies the associations between different modalities. Similarly, the cross-attention method assigns maximum weight to the modality that is beneficial to the current task, but fails to leverage the rich complementary information among different modalities, inadvertently discarding essential data. We have presented detailed ablation experiments in Appendix Figure B. In our approach, QNet, we use quaternion hidden space to construct more complex and non-linear relationships among diverse modalities in a more compact manner. First, we merge output image features from a pre-trained model with text context features. Then, we process these integrated features through a quaternion encoder. Within the quaternion hidden spaces, our QNet employs orthogonal imaginary axes (i.e., i, j, and k) to further tackle the fused features in a higher dimension. Our approach provides a holistic understanding and captures intricate details that might be overlooked from a single-dimensional perspective. Additionally, our QNet integrates features across hierarchical levels to enrich the representation from various perspectives. Extensive experiments on multiple datasets demonstrate the superiority of our approach over the state-of-the-art methods. Furthermore, our QNet can be used as a plug-and-play module to support various multimodal approaches."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623694719,
                "cdate": 1700623694719,
                "tmdate": 1700623694719,
                "mdate": 1700623694719,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ykE5wJSwHB",
            "forum": "dKlxDx2SoS",
            "replyto": "dKlxDx2SoS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_xbTq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4459/Reviewer_xbTq"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges and limitations of multimodal pre-trained models in capturing diverse and complementary features across different modalities. It introduces a novel approach called QNet, which utilizes quaternion networks to improve the modality fusion capacities of pre-trained models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The use of quaternion networks to capture the intricate relationships among different modalities is a novel idea that sets this paper apart from existing methods. The paper provides a thorough analysis of the proposed method, including experimental results on various datasets and comparison with existing methods. The results are presented in a clear and concise manner.\n\nThe paper is well-written and organized, making it easy to follow the proposed approach and understand the experimental results."
                },
                "weaknesses": {
                    "value": "Overall, this paper presents a sound framework. My main concern is that the authors should compare to the baseline scombining Quaternion Networks and the existing prompt learning method clearly. Besides, the benefits of QNet can be evaluated on more multimodal tasks."
                },
                "questions": {
                    "value": "Please see my comments on the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4459/Reviewer_xbTq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4459/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700383069337,
            "cdate": 1700383069337,
            "tmdate": 1700383069337,
            "mdate": 1700383069337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B2IXSyHoRh",
                "forum": "dKlxDx2SoS",
                "replyto": "ykE5wJSwHB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbTq"
                    },
                    "comment": {
                        "value": "We appreciate your valuable feedback and address your concerns as follows.\n\n**W: Expanded comparisons and evaluations.**\nOur method, QNet, significantly differs from directly combining Quaternion networks and the existing prompt learning approach, where all real and imagery axes are used as parameter axes. Instead, our QNet only uses imagery axes in which the i-axis and j-axis act as parameter axes, and the k-axis acts as a modulation axis to balance the weights of the i-axis and j-axis. Experiments in Table 5 and Table 6 demonstrate the effects of different real and imagery axes, affirming that using imagery axes can better learn multimodal information from the fused features. We also compared our method with Coop and MaPle, both with the setting of a single layer in Table 4, which further validates the effectiveness of modality fusion in QNet. As suggested, we extend our QNet to related tasks, such as video understanding. Due to limited time, we conducted preliminary experiments on the UCF-101 video dataset (see Table 1). Using the ViFi-CLIP's base-to-novel generalization setting, QNet was applied to a Kinetics-400 pre-trained ViFi-CLIP with prompt learning. QNet outperforms the IVLP method and even exceeds fully fine-tuned models like ActionCLIP, indicating its strong generalization ability across various modalities, including video tasks. Moreover, we will make more detailed comparisons with other methods and conduct more evaluations on other tasks to validate the effectiveness of QNet in the revised version. We gratefully appreciate your suggestions to improve this work.\n| Method       | Base Acc. | Novel Acc. | HM    |\n|--------------|-----------|------------|-------|\n| Vanilla CLIP | 78.50     | 63.60      | 70.30 |\n| ActionCLIP   | 85.60     | 75.30      | 80.10 |\n| XCLIP        | 95.40     | 74.00      | 83.40 |\n| A5           | 95.80     | 71.00      | 81.60 |\n| IVLP         | 95.90     | 74.10      | 83.60 |\n| **QNet**     | **96.60** | **78.20**  | **86.43** |\n\n_Table 1: Performance comparison in video action recognition generalization benchmark on UCF-101. We employ QNet and IVLP on ViFi-CLIP and compare with the prior video approaches._"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719374597,
                "cdate": 1700719374597,
                "tmdate": 1700719374597,
                "mdate": 1700719374597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]